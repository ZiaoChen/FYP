Title,Abstract,Keywords
"MiBench: A free, commercially representative embedded benchmark suite","This paper examines a set of commercially representative embedded programs and compares them to an existing benchmark suite, SPEC2000. A new version of SimpleScalar that has been adapted to the ARM instruction set is used to characterize the performance of the benchmarks using configurations similar to current and next generation embedded processors. Several characteristics distinguish the representative embedded programs from the existing SPEC benchmarks including instruction distribution, memory behavior, and available parallelism. The embedded benchmarks, called MiBench, are freely available to all researchers.",
"Particle swarm optimization: developments, applications and resources","This paper focuses on the engineering and computer science aspects of developments, applications, and resources related to particle swarm optimization. Developments in the particle swarm algorithm since its origin in 1995 are reviewed. Included are brief discussions of constriction factors, inertia weights, and tracking dynamic systems. Applications, both those already developed, and promising future application areas, are reviewed. Finally, resources related to particle swarm optimization are listed, including books, Web sites, and software. A particle swarm optimization bibliography is at the end of the paper.",
Ad hoc positioning system (APS),"Many ad hoc network protocols and applications assume the knowledge of geographic location of nodes. The absolute location of each networked node is an assumed fact by most sensor networks which can then present the sensed information on a geographical map. Finding location without the aid of GPS in each node of an ad hoc network is important in cases where GPS is either not accessible, or not practical to use due to power, form factor or line of sight conditions. Location would also enable routing in sufficiently isotropic large networks, without the use of large routing tables. We are proposing APS - a distributed, hop by hop positioning algorithm, that works as an extension of both distance vector routing and GPS positioning in order to provide approximate location for all nodes in a network where only a limited fraction of nodes have self location capability.","Routing,
Global Positioning System,
Ad hoc networks,
Meteorology,
Costs,
Computer science,
Protocols,
Application software,
Aircraft,
Monitoring"
Efficient erasure correcting codes,"We introduce a simple erasure recovery algorithm for codes derived from cascades of sparse bipartite graphs and analyze the algorithm by analyzing a corresponding discrete-time random process. As a result, we obtain a simple criterion involving the fractions of nodes of different degrees on both sides of the graph which is necessary and sufficient for the decoding process to finish successfully with high probability. By carefully designing these graphs we can construct for any given rate R and any given real number /spl epsiv/ a family of linear codes of rate R which can be encoded in time proportional to ln(1//spl epsiv/) times their block length n. Furthermore, a codeword can be recovered with high probability from a portion of its entries of length (1+/spl epsiv/)Rn or more. The recovery algorithm also runs in time proportional to n ln(1//spl epsiv/). Our algorithms have been implemented and work well in practice; various implementation issues are discussed.",Error correction coding
Automated manifold surgery: constructing geometrically accurate and topologically correct models of the human cerebral cortex,"Highly accurate surface models of the cerebral cortex are becoming increasingly important as tools in the investigation of the functional organization of the human brain. The construction of such models is difficult using current neuroimaging technology due to the high degree of cortical folding. Even single voxel mis-classifications can result in erroneous connections being created between adjacent banks of a sulcus, resulting in a topologically inaccurate model. These topological defects cause the cortical model to no longer be homeomorphic to a sheet, preventing the accurate inflation, flattening, or spherical morphing of the reconstructed cortex. Surface deformation techniques can guarantee the topological correctness of a model, but are time-consuming and may result in geometrically inaccurate models. In order to address this need the authors have developed a technique for taking a model of the cortex, detecting and fixing the topological defects while leaving that majority of the model intact, resulting in a surface that is both geometrically accurate and topologically correct.",
Coverage problems in wireless ad-hoc sensor networks,"Wireless ad-hoc sensor networks have recently emerged as a premier research topic. They have great long-term economic potential, ability to transform our lives, and pose many new system-building challenges. Sensor networks also pose a number of new conceptual and optimization problems. Some, such as location, deployment, and tracking, are fundamental issues, in that many applications rely on them for needed information. We address one of the fundamental problems, namely coverage. Coverage in general, answers the questions about quality of service (surveillance) that can be provided by a particular sensor network. We first define the coverage problem from several points of view including deterministic, statistical, worst and best case, and present examples in each domain. By combining the computational geometry and graph theoretic techniques, specifically the Voronoi diagram and graph search algorithms, we establish the main highlight of the paper-optimal polynomial time worst and average case algorithm for coverage calculation. We also present comprehensive experimental results and discuss future research directions related to coverage in sensor networks.",
On-demand multipath distance vector routing in ad hoc networks,"We develop an on-demand multipath distance vector protocol for mobile ad hoc networks. Specifically, we propose multipath extensions to a well-studied single path routing protocol known as ad hoc on-demand distance vector (AODV). The resulting protocol is referred to as ad hoc on-demand multipath distance vector (AOMDV). The protocol computes multiple loop-free and link-disjoint paths. Loop-freedom is guaranteed by using a notion of ""advertised hopcount"". Link-disjointness of multiple paths is achieved by using a particular property of flooding. Performance comparison of AOMDV with AODV using ns-2 simulations shows that AOMDV is able to achieve a remarkable improvement in the end-to-end delay-often more than a factor of two, and is also able to reduce routing overheads by about 20%.","Intelligent networks,
Ad hoc networks,
Routing protocols,
Mobile ad hoc networks,
Bandwidth,
Batteries,
Delay,
Computer science,
Computational modeling,
Network topology"
The anatomy of the grid: enabling scalable virtual organizations,,"Anatomy,
Space technology,
Distributed computing,
Peer to peer computing,
Problem-solving,
Computer science,
Internet,
Resource management,
Collaborative software,
Application software"
BRITE: an approach to universal topology generation,"Effective engineering of the Internet is predicated upon a detailed understanding of issues such as the large-scale structure of its underlying physical topology, the manner in which it evolves over time, and the way in which its constituent components contribute to its overall function. Unfortunately, developing a deep understanding of these issues has proven to be a challenging task, since it in turn involves solving difficult problems such as mapping the actual topology, characterizing it, and developing models that capture its emergent behavior. Consequently, even though there are a number of topology models, it is an open question as to how representative the generated topologies they generate are of the actual Internet. Our goal is to produce a topology generation framework which improves the state of the art and is based on the design principles of representativeness, inclusiveness, and interoperability. Representativeness leads to synthetic topologies that accurately reflect many aspects of the actual Internet topology (e.g. hierarchical structure, node degree distribution, etc.). Inclusiveness combines the strengths of as many generation models as possible in a single generation tool. Interoperability provides interfaces to widely-used simulation applications such as ns and SSF and visualization tools like otter. We call such a tool a universal topology generator.","Network topology,
Internet,
Character generation,
IP networks,
Protocols,
Engineering profession,
Computer science,
Large-scale systems,
Visualization,
Bandwidth"
Automatic lung segmentation for accurate quantitation of volumetric X-ray CT images,"Segmentation of pulmonary X-ray computed tomography (CT) images is a precursor to most pulmonary image analysis applications. This paper presents a fully automatic method for identifying the lungs in three-dimensional (3-D) pulmonary X-ray CT images. The method has three main steps. First, the lung region is extracted from the CT images by gray-level thresholding. Then, the left and right lungs are separated by identifying the anterior and posterior junctions by dynamic programming. Finally, a sequence of morphological operations is used to smooth the irregular boundary along the mediastinum in order to obtain results consistent with these obtained by manual analysis, in which only the most central pulmonary arteries are excluded from the lung region. The method has been tested by processing 3-D CT data sets from eight normal subjects, each imaged three times at biweekly intervals with lungs at 90% vital capacity. The authors present results by comparing their automatic method to manually traced borders from two image analysts. Averaged over all volumes, the root mean square difference between the computer and human analysis is 0.8 pixels (0.54 mm). The mean intrasubject change in tissue content over the three scans was 2.75%/spl plusmn/2.29% (mean/spl plusmn/standard deviation).","Lungs,
Image segmentation,
X-ray imaging,
Computed tomography,
Image sequence analysis,
Dynamic programming,
Morphological operations,
Manuals,
Arteries,
Testing"
A model based on linguistic 2-tuples for dealing with multigranular hierarchical linguistic contexts in multi-expert decision-making,"In those problems that deal with multiple sources of linguistic information we can find problems defined in contexts where the linguistic assessments are assessed in linguistic term sets with different granularity of uncertainty and/or semantics (multigranular linguistic contexts). Different approaches have been developed to manage this type of contexts, that unify the multigranular linguistic information in an unique linguistic term set for an easy management of the information. This normalization process can produce a loss of information and hence a lack of precision in the final results. In this paper, we shall present a type of multigranular linguistic contexts we shall call linguistic hierarchies term sets, such that, when we deal with multigranular linguistic information assessed in these structures we can unify the information assessed in them without loss of information. To do so, we shall use the 2-tuple linguistic representation model. Afterwards we shall develop a linguistic decision model dealing with multigranular linguistic contexts and apply it to a multi-expert decision-making problem.","Context modeling,
Decision making,
Uncertainty,
Information resources,
Information management,
Computer science,
Artificial intelligence"
Computing visual correspondence with occlusions using graph cuts,"Several new algorithms for visual correspondence based on graph cuts have recently been developed. While these methods give very strong results in practice, they do not handle occlusions properly. Specifically, they treat the two input images asymmetrically, and they do not ensure that a pixel corresponds to at most one pixel in the other image. In this paper, we present a new method which properly addresses occlusions, while preserving the advantages of graph cut algorithms. We give experimental results for stereo as well as motion, which demonstrate that our method performs well both at detecting occlusions and computing disparities.",
Network support for IP traceback,"This paper describes a technique for tracing anonymous packet flooding attacks in the Internet back toward their source. This work is motivated by the increased frequency and sophistication of denial-of-service attacks and by the difficulty in tracing packets with incorrect, or ""spoofed,"" source addresses. We describe a general purpose traceback mechanism based on probabilistic packet marking in the network. Our approach allows a victim to identify the network path(s) traversed by attack traffic without requiring interactive operational support from Internet service providers (ISPs). Moreover, this traceback can be performed ""post mortem""-after an attack has completed. We present an implementation of this technology that is incrementally deployable, (mostly) backward compatible, and can be efficiently implemented using conventional technology.","Computer crime,
Frequency,
Computer network management,
Computer science,
Communication system traffic control,
Telecommunication traffic,
Web and internet services,
Computer security,
Network servers,
Stochastic processes"
Power efficient organization of wireless sensor networks,"Wireless sensor networks have emerged recently as an effective way of monitoring remote or inhospitable physical environments. One of the major challenges in devising such networks lies in the constrained energy and computational resources available to sensor nodes. These constraints must be taken into account at all levels of the system hierarchy. The deployment of sensor nodes is the first step in establishing a sensor network. Since sensor networks contain a large number of sensor nodes, the nodes must be deployed in clusters, where the location of each particular node cannot be fully guaranteed a priori. Therefore, the number of nodes that must be deployed in order to completely cover the whole monitored area is often higher than if a deterministic procedure were used. In networks with stochastically placed nodes, activating only the necessary number of sensor nodes at any particular moment can save energy. We introduce a heuristic that selects mutually exclusive sets of sensor nodes, where the members of each of those sets together completely cover the monitored area. The intervals of activity are the same for all sets, and only one of the sets is active at any time. The experimental results demonstrate that by using only a subset of sensor nodes at each moment, we achieve a significant energy savings while fully preserving coverage.",
Spatial transformations of diffusion tensor magnetic resonance images,"The authors address the problem of applying spatial transformations (or ""image warps"") to diffusion tensor magnetic resonance images. The orientational information that these images contain must be handled appropriately when they are transformed spatially during image registration. The authors present solutions for global transformations of three-dimensional images up to 12-parameter affine complexity and indicate how their methods can be extended for higher order transformations. Several approaches are presented and tested using synthetic data. One method, the preservation of principal direction algorithm, which takes into account shearing, stretching and rigid rotation, is shown to be the most effective. Additional registration experiments are performed on human brain data obtained from a single subject, whose head was imaged in three different orientations within the scanner. All of the authors' methods improve the consistency between registered and target images over naive warping algorithms.","Tensile stress,
Magnetic resonance,
Magnetic resonance imaging,
Testing,
Humans,
Diffusion tensor imaging,
Nuclear magnetic resonance,
Biophysics,
Image registration,
Shearing"
GPS-free positioning in mobile ad-hoc networks,"We consider the problem of node positioning in ad-hoc networks. We propose a distributed, infrastructure-free positioning algorithm that does not rely on Global Positioning System (GPS). The algorithm uses the distances between the nodes to build a relative coordinate system in which the node positions are computed in two dimensions. The main contribution of this work is to define and compute relative positions of the nodes in an ad-hoc network without using GPS. We further explain how the proposed approach can be applied to wide area ad-hoc networks.","Intelligent networks,
Ad hoc networks,
Global Positioning System,
Transfer functions,
Computer networks,
Wide area networks,
Spine,
Switches,
Network servers,
Distributed processing"
Toward reference models for requirements traceability,"Requirements traceability is intended to ensure continued alignment between stakeholder requirements and various outputs of the system development process. To be useful, traces must be organized according to some modeling framework. Indeed, several such frameworks have been proposed, mostly based on theoretical considerations or analysis of other literature. This paper, in contrast, follows an empirical approach. Focus groups and interviews conducted in 26 major software development organizations demonstrate a wide range of traceability practices with distinct low-end and high-end users of traceability. From these observations, reference models comprising the most important kinds of traceability links for various development tasks have been synthesized. The resulting models have been validated in case studies and are incorporated in a number of traceability tools. A detailed case study on the use of the models is presented. Four kinds of traceability link types are identified and critical issues that must be resolved for implementing each type and potential solutions are discussed. Implications for the design of next-generation traceability methods and tools are discussed and illustrated.","Costs,
Mathematical model,
Engineering management,
Programming,
Design engineering,
Prototypes,
Computer science,
Best practices,
Standards organizations"
Consistent image registration,"Presents a new method for image registration based on jointly estimating the forward and reverse transformations between two images while constraining these transforms to be inverses of one another. This approach produces a consistent set of transformations that have less pairwise registration error, i.e., better correspondence, than traditional methods that estimate the forward and reverse transformations independently. The transformations are estimated iteratively and are restricted to preserve topology by constraining them to obey the laws of continuum mechanics. The transformations are parameterized by a Fourier series to diagonalize the covariance structure imposed by the continuum mechanics constraints and to provide a computationally efficient numerical implementation. Results using a linear elastic material constraint are presented using both magnetic resonance and X-ray computed tomography image data. The results show that the joint estimation of a consistent set of forward and reverse transformations constrained by linear-elasticity give better registration results than using either constraint alone or none at all.","Image registration,
Computed tomography,
Magnetic resonance imaging,
Magnetic materials,
X-ray imaging,
Biomedical imaging,
Cities and towns,
Topology,
Fourier series,
Magnetic resonance"
Novel Bayesian multiscale method for speckle removal in medical ultrasound images,"A novel speckle suppression method for medical ultrasound images is presented. First, the logarithmic transform of the original image is analyzed into the multiscale wavelet domain. The authors show that the subband decompositions of ultrasound images have significantly non-Gaussian statistics that are best described by families of heavy-tailed distributions such as the alpha-stable. Then, the authors design a Bayesian estimator that exploits these statistics. They use the alpha-stable model to develop a blind noise-removal processor that performs a nonlinear operation on the data. Finally, the authors compare their technique with current state-of-the-art soft and hard thresholding methods applied on actual ultrasound medical images and they quantify the achieved performance improvement.","Bayesian methods,
Speckle,
Biomedical imaging,
Ultrasonic imaging,
Medical diagnostic imaging,
Wiener filter,
Wavelet transforms,
Image analysis,
Ultrasonography,
Low pass filters"
Color invariance,"This paper presents the measurement of colored object reflectance, under different, general assumptions regarding the imaging conditions. We exploit the Gaussian scale-space paradigm for color images to define a framework for the robust measurement of object reflectance from color images. Object reflectance is derived from a physical reflectance model based on the Kubelka-Munk theory for colorant layers. Illumination and geometrical invariant properties are derived from the reflectance model. Invariance and discriminative power of the color invariants is experimentally investigated, showing the invariants to be successful in discounting shadow, illumination, highlights, and noise. Extensive experiments show the different invariants to be highly discriminative, while maintaining invariance properties. The presented framework for color measurement is well-founded in the physics of color as well as in measurement science. Hence, the proposed invariants are considered more adequate for the measurement of invariant color features than existing methods.",
Frequent subgraph discovery,"As data mining techniques are being increasingly applied to non-traditional domains, existing approaches for finding frequent itemsets cannot be used as they cannot model the requirement of these domains. An alternate way of modeling the objects in these data sets is to use graphs. Within that model, the problem of finding frequent patterns becomes that of discovering subgraphs that occur frequently over the entire set of graphs.The authors present a computationally efficient algorithm for finding all frequent subgraphs in large graph databases. We evaluated the performance of the algorithm by experiments with synthetic datasets as well as a chemical compound dataset. The empirical results show that our algorithm scales linearly with the number of input transactions and it is able to discover frequent subgraphs from a set of graph transactions reasonably fast, even though we have to deal with computationally hard problems such as canonical labeling of graphs and subgraph isomorphism which are not necessary for traditional frequent itemset discovery.","Itemsets,
Data mining,
Transaction databases,
Computer science,
Chemical compounds,
Labeling,
Image databases,
Image recognition,
Computer vision,
Association rules"
Automatic image segmentation by integrating color-edge extraction and seeded region growing,"We propose a new automatic image segmentation method. Color edges in an image are first obtained automatically by combining an improved isotropic edge detector and a fast entropic thresholding technique. After the obtained color edges have provided the major geometric structures in an image, the centroids between these adjacent edge regions are taken as the initial seeds for seeded region growing (SRG). These seeds are then replaced by the centroids of the generated homogeneous image regions by incorporating the required additional pixels step by step. Moreover, the results of color-edge extraction and SRG are integrated to provide homogeneous image regions with accurate and closed boundaries. We also discuss the application of our image segmentation method to automatic face detection. Furthermore, semantic human objects are generated by a seeded region aggregation procedure which takes the detected faces as object seeds.","Image segmentation,
Image edge detection,
Face detection,
Detectors,
Image generation,
Pixel,
Humans,
Object detection,
Computer vision,
Computer science"
Universally composable security: a new paradigm for cryptographic protocols,"We propose a novel paradigm for defining security of cryptographic protocols, called universally composable security. The salient property of universally composable definitions of security is that they guarantee security even when a secure protocol is composed of an arbitrary set of protocols, or more generally when the protocol is used as a component of an arbitrary system. This is an essential property for maintaining security of cryptographic protocols in complex and unpredictable environments such as the Internet. In particular, universally composable definitions guarantee security even when an unbounded number of protocol instances are executed concurrently in an adversarially controlled manner, they guarantee non-malleability with respect to arbitrary protocols, and more. We show how to formulate universally composable definitions of security for practically any cryptographic task. Furthermore, we demonstrate that practically any such definition can be realized using known techniques, as long as only a minority of the participants are corrupted. We then proceed to formulate universally composable definitions of a wide array of cryptographic tasks, including authenticated and secure communication, key-exchange, public-key encryption, signature, commitment, oblivious transfer, zero knowledge and more. We also make initial steps towards studying the realizability of the proposed definitions in various settings.","Cryptographic protocols,
Cryptography,
Mathematical model,
Computer security,
Application software,
Radio access networks,
Reactive power,
Job design,
Computer science"
Automated segmentation of multiple sclerosis lesions by model outlier detection,"This paper presents a fully automated algorithm for segmentation of multiple sclerosis (MS) lesions from multispectral magnetic resonance (MR) images. The method performs intensity-based tissue classification using a stochastic model for normal brain images and simultaneously detects MS lesions as outliers that are not well explained by the model. It corrects for MR field inhomogeneities, estimates tissue-specific intensity models from the data itself, and incorporates contextual information in the classification using a Markov random field. The results of the automated method are compared with lesion delineations by human experts, showing a high total lesion load correlation. When the degree of spatial correspondence between segmentations is taken into account, considerable disagreement is found, both between expect segmentations, and between expert and automatic measurements.","Multiple sclerosis,
Lesions,
Image segmentation,
Brain modeling,
Magnetic resonance,
Stochastic processes,
Context modeling,
Markov random fields,
Humans,
Magnetic field measurement"
Automated detection of pulmonary nodules in helical CT images based on an improved template-matching technique,"The purpose of this study is to develop a technique for computer-aided diagnosis (CAD) systems to detect lung nodules in helical X-ray pulmonary computed tomography (CT) images. The authors propose a novel template-matching technique based on a genetic algorithm (GA) template matching (GATM) for detecting nodules existing within the lung area; the GA was used to determine the target position in the observed image efficiently and to select an adequate template image from several reference patterns for quick template matching. In addition, a conventional template matching was employed to detect nodules existing on the lung wall area, lung wall template matching (LWTM), where semicircular models were used as reference patterns; the semicircular models were rotated according to the angle of the target point on the contour of the lung wall. After initial detecting candidates using the two template-matching methods, the authors extracted a total of 13 feature values and used them to eliminate false-positive findings. Twenty clinical cases involving a total of 557 sectional images were used in this study. 71 nodules out of 98 were correctly detected by the authors' scheme (i.e., a detection rate of about 72%), with the number of false positives at approximately 1.1/sectional image. The authors' present results show that their scheme can be regarded as a technique for CAD systems to detect nodules in helical CT pulmonary images.",
Improvements to Platt's SMO Algorithm for SVM Classifier Design,"This article points out an important source of inefficiency in Platt's sequential minimal optimization (SMO) algorithm that is caused by the use of a single threshold value. Using clues from the KKT conditions for the dual problem, two threshold parameters are employed to derive modifications of SMO. These modified algorithms perform significantly faster than the original SMO on all benchmark data sets tried.",
An online algorithm for segmenting time series,"In recent years, there has been an explosion of interest in mining time-series databases. As with most computer science problems, representation of the data is the key to efficient and effective solutions. One of the most commonly used representations is piecewise linear approximation. This representation has been used by various researchers to support clustering, classification, indexing and association rule mining of time-series data. A variety of algorithms have been proposed to obtain this representation, with several algorithms having been independently rediscovered several times. In this paper, we undertake the first extensive review and empirical comparison of all proposed techniques. We show that all these algorithms have fatal flaws from a data-mining perspective. We introduce a novel algorithm that we empirically show to be superior to all others in the literature.","Data mining,
Piecewise linear techniques,
Piecewise linear approximation,
Clustering algorithms,
Computer science,
Explosions,
Databases,
Change detection algorithms,
Indexing,
Association rules"
Automated melanoma recognition,"A system for the computerized analysis of images obtained from epiluminescence microscopy (ELM) has been developed to enhance the early recognition of malignant melanoma. As an initial step, the binary mask of the skin lesion is determined by several basic segmentation algorithms together with a fusion strategy. A set of features containing shape and radiometric features as well as local and global parameters is calculated to describe the malignancy of a lesion. Significant features are then selected from this set by application of statistical feature subset selection methods. The final kNN classification delivers a sensitivity of 87% with a specificity of 92%.","Malignant tumors,
Lesions,
Image analysis,
Microscopy,
Image recognition,
Cancer,
Skin,
Image segmentation,
Shape,
Radiometry"
A decade of reconfigurable computing: a visionary retrospective,"The paper surveys a decade of R&D on coarse grain reconfigurable hardware and related CAD, points out why this emerging discipline is heading toward a dichotomy of computing science, and advocates the introduction of a new soft machine paradigm to replace CAD by compilation.",
Basic block distribution analysis to find periodic behavior and simulation points in applications,"Modern architecture research relies heavily on detailed pipeline simulation. Simulating the full execution of an industry standard benchmark can take weeks to months to complete. To overcome this problem researchers choose a very small portion of a program's execution to evaluate their results, rather than simulating the entire program. In this paper we propose Basic Block Distribution Analysis as an automated approach for finding these small portions of the program to simulate that are representative of the entire program's execution. This approach is based upon using profiles of a program's code structure (basic blocks) to uniquely identify different phases of execution in the program. We show that the periodicity of the basic block frequency profile reflects the periodicity of detailed simulation across several different architectural metrics (e.g., IPC, branch miss rate, cache miss rate, value misprediction, address misprediction, and reorder buffer occupancy). Since basic block frequencies can be collected using very fast profiling tools, our approach provides a practical technique for finding the periodicity and simulation points in applications.","Analytical models,
Computational modeling,
Fingerprint recognition,
Application software,
Computer simulation,
Pipelines,
Frequency estimation,
Delay estimation,
Timing,
Computer science"
Deriving intrinsic images from image sequences,"Intrinsic images are a useful midlevel description of scenes proposed by H.G. Barrow and J.M. Tenenbaum (1978). An image is de-composed into two images: a reflectance image and an illumination image. Finding such a decomposition remains a difficult problem in computer vision. We focus on a slightly, easier problem: given a sequence of T images where the reflectance is constant and the illumination changes, can we recover T illumination images and a single reflectance image? We show that this problem is still imposed and suggest approaching it as a maximum-likelihood estimation problem. Following recent work on the statistics of natural images, we use a prior that assumes that illumination images will give rise to sparse filter outputs. We show that this leads to a simple, novel algorithm for recovering reflectance images. We illustrate the algorithm's performance on real and synthetic image sequences.","Image sequences,
Lighting,
Reflectivity,
Layout,
Image segmentation,
Equations,
Computer vision,
Inference algorithms,
Computer science,
Maximum likelihood estimation"
An iterative maximum-likelihood polychromatic algorithm for CT,A new iterative maximum-likelihood reconstruction algorithm for X-ray computed tomography is presented. The algorithm prevents beam hardening artifacts by incorporating a polychromatic acquisition model. The continuous spectrum of the X-ray tube is modeled as a number of discrete energies. The energy dependence of the attenuation is taken into account by decomposing the linear attenuation coefficient into a photoelectric component and a Compton scatter component. The relative weight of these components is constrained based on prior material assumptions. Excellent results are obtained for simulations and for phantom measurements. Beam-hardening artifacts are effectively eliminated. The relation with existing algorithms is discussed. The results confirm that improving the acquisition model assumed by the reconstruction algorithm results in reduced artifacts. Preliminary results indicate that metal artifact reduction is a very promising application for this new algorithm.,"Iterative algorithms,
Computed tomography,
Image reconstruction,
Attenuation,
Hospitals,
Reconstruction algorithms,
X-ray scattering,
Extraterrestrial measurements,
Nuclear medicine,
X-ray imaging"
A min-max cut algorithm for graph partitioning and data clustering,"An important application of graph partitioning is data clustering using a graph model - the pairwise similarities between all data objects form a weighted graph adjacency matrix that contains all necessary information for clustering. In this paper, we propose a new algorithm for graph partitioning with an objective function that follows the min-max clustering principle. The relaxed version of the optimization of the min-max cut objective function leads to the Fiedler vector in spectral graph partitioning. Theoretical analyses of min-max cut indicate that it leads to balanced partitions, and lower bounds are derived. The min-max cut algorithm is tested on newsgroup data sets and is found to out-perform other current popular partitioning/clustering methods. The linkage-based refinements to the algorithm further improve the quality of clustering substantially. We also demonstrate that a linearized search order based on linkage differential is better than that based on the Fiedler vector, providing another effective partitioning method.","Clustering algorithms,
Partitioning algorithms,
Helium,
Laboratories,
Computer science,
Mathematics,
Application software,
Mathematical model,
Testing,
Clustering methods"
Genomic signal processing,"Genomics is a highly cross-disciplinary field that creates paradigm shifts in such diverse areas as medicine and agriculture. It is believed that many significant scientific and technological endeavors in the 21st century will be related to the processing and interpretation of the vast information that is currently revealed from sequencing the genomes of many living organisms, including humans. Genomic information is digital in a very real sense; it is represented in the form of sequences of which each element can be one out of a finite number of entities. Such sequences, like DNA and proteins, have been mathematically represented by character strings, in which each character is a letter of an alphabet. In the case of DNA, the alphabet is size 4 and consists of the letters A, T, C and G; in the case of proteins, the size of the corresponding alphabet is 20. As the list of references shows, biomolecular sequence analysis has already been a major research topic among computer scientists, physicists, and mathematicians. The main reason that the field of signal processing does not yet have significant impact in the field is because it deals with numerical sequences rather than character strings. However, if we properly map a character string into, one or more numerical sequences, then digital signal processing (DSP) provides a set of novel and useful tools for solving highly relevant problems. For example, in the form of local texture, color spectrograms visually provide significant information about biomolecular sequences which facilitates understanding of local nature, structure, and function. Furthermore, both the magnitude and the phase of properly defined Fourier transforms can be used to predict important features like the location and certain properties of protein coding regions in DNA. Even the process of mapping DNA into proteins and the interdependence of the two kinds of sequences can be analyzed using simulations based on digital filtering. These and other DSP-based approaches result in alternative mathematical formulations and may provide improved computational techniques for the solution of useful problems in genomic information science and technology.","Genomics,
Bioinformatics,
Signal processing,
Sequences,
DNA,
Proteins,
Biomedical signal processing,
Digital signal processing,
Agriculture,
Organisms"
Data mining methods for detection of new malicious executables,"A serious security threat today is malicious executables, especially new, unseen malicious executables often arriving as email attachments. These new malicious executables are created at the rate of thousands every year and pose a serious security threat. Current anti-virus systems attempt to detect these new malicious programs with heuristics generated by hand. This approach is costly and oftentimes ineffective. We present a data mining framework that detects new, previously unseen malicious executables accurately and automatically. The data mining framework automatically found patterns in our data set and used these patterns to detect a set of new malicious binaries. Comparing our detection methods with a traditional signature-based method, our method more than doubles the current detection rates for new malicious executables.","Data mining,
Computer science,
Data security,
Testing,
Face detection,
Computer security,
Information security,
Permission,
Training data,
Protection"
An analysis of the optimum node density for ad hoc mobile networks,"An ad hoc mobile network is a collection of nodes, each of which communicates over wireless channels and is capable of movement. Wireless nodes have the unique capability of transmission at different power levels. As the transmission power is varied, a tradeoff exists between the number of hops from source to destination and the overall bandwidth available to individual nodes. Because both battery life and channel bandwidth are limited resources in mobile networks, it is important to ascertain the effects different transmission powers have on the overall performance of the network. This paper explores the nature of this transmission power tradeoff in mobile networks to determine the optimum node density for delivering the maximum number of data packets. It is shown that there does not exist a global optimum density, but rather that, to achieve this maximum, the node density should increase as the rate of node movement increases.",
Three-dimensional computer-aided diagnosis scheme for detection of colonic polyps,"We have developed a three-dimensional (3-D) computer-aided diagnosis scheme for automated detection of colonic polyps in computed tomography (CT) colonographic data sets, and assessed its performance based on colonoscopy as the gold standard. In this scheme, a thick region encompassing the entire colonic wall is extracted from an isotropic volume reconstructed from the CT images in CT colonography. Polyp candidates are detected by first computing of 3-D geometric features that characterize polyps, folds, and colonic walls at each voxel in the extracted colon, and then segmenting of connected components corresponding to suspicious regions by hysteresis thresholding based on these geometric features. We apply fuzzy clustering to these connected components to obtain the polyp candidates. False-positive (FP) detections are then reduced by computation of several 3-D volumetric features characterizing the internal structures of the polyp candidates, followed by the application of discriminant analysis to the feature space generated by these volumetric features. The locations of the polyps detected by our computerized method were compared to the gold standard of conventional colonoscopy. The performance was evaluated based on 43 clinical cases, including 12 polyps determined by colonoscopy. Our computerized scheme was shown to have the potential to detect polyps in CT colonography with a clinically acceptable high sensitivity and a low FP rate.",
IDMaps: a global Internet host distance estimation service,"There is an increasing need to quickly and efficiently learn network distances, in terms of metrics such as latency or bandwidth, between Internet hosts. For example, Internet content providers often place data and server mirrors throughout the Internet to improve access latency for clients, and it is necessary to direct clients to the nearest mirrors based on some distance metric in order to realize the benefit of the mirrors. We suggest a scalable Internet-wide architecture, called IDMaps, which measures and disseminates distance information on the global Internet. Higher level services can collect such distance information to build a virtual distance map of the Internet and estimate the distance between any pair of IP addresses. We present our solutions to the measurement server placement and distance map construction problems in IDMaps. We show that IDMaps can indeed provide useful distance estimations to applications such as nearest mirror selection.","Web and internet services,
Delay,
Web server,
Network servers,
Mirrors,
Bandwidth,
Web pages,
Engineering profession,
Computer science,
Protocols"
Dynamic and aggressive scheduling techniques for power-aware real-time systems,"In this paper we address power-aware scheduling of periodic hard real-time tasks using dynamic voltage scaling. Our solution includes three parts: (a) a static (off-line) solution to compute the optimal speed, assuming worst-case workload for each arrival, (b) an on-line speed reduction mechanism to reclaim energy by adapting to the actual workload, and (c) an online, adaptive and speculative speed adjustment mechanism to anticipate early completions of future executions by using the average-case workload information. All these solutions still guarantee that all deadlines are met. Our simulation results show that the reclaiming algorithm saves a striking 50% of the energy, over the static algorithm. Further our speculative techniques allow for an additional approximately 20% savings over the reclaiming algorithm. In this study, we also establish that solving an instance of the static power-aware scheduling problem is equivalent to solving an instance of the reward-based scheduling problem [1, 4] with concave reward functions.","Dynamic scheduling,
Real time systems,
Processor scheduling,
Computer science,
Dynamic voltage scaling,
Power systems,
Delay,
Artificial satellites,
Frequency,
Contracts"
Condor-G: a computation management agent for multi-institutional grids,"In recent years, there has been a dramatic increase in the amount of available computing and storage resources, yet few have been able to exploit these resources in an aggregated form. We present the Condor-G system, which leverages software from Globus and Condor to allow users to harness multi-domain resources as if they all belong to one personal domain. We describe the structure of Condor-G and how it handles job management, resource selection, security and fault tolerance.","Grid computing,
Resource management,
Power engineering computing,
Computer science,
Energy management,
Home computing,
Computational modeling,
Large-scale systems,
Environmental management,
Software systems"
Designing and evaluating e-business models,This article presents an e-business modeling approach that combines the rigorous approach of IT systems analysis with an economic value perspective from business sciences.,"Information systems,
Information analysis,
Ontologies,
Internet telephony,
Systems engineering and theory,
Costs,
Finance,
Visualization"
Space variant median filters for the restoration of impulse noise corrupted images,"This brief proposes a generalized framework of median based switching schemes, called multi-state median (MSM) filter. By using a simple thresholding logic, the output of the MSM filter is adaptively switched among those of a group of center weighted median (CWM) filters that have different center weights. As a result, the MSM filter is equivalent to an adaptive CWM filter with a space varying center weight which is dependent on local signal statistics. The efficacy of the proposed filter has been evaluated by extensive simulations.","Image restoration,
Nonlinear filters,
Adaptive filters,
Filtering,
Logic,
Additive noise,
Image processing,
Computer science,
Software engineering,
Australia"
Blind Source Separation by Sparse Decomposition in a Signal Dictionary,"The blind source separation problem is to extract the underlying source signals from a set of linear mixtures, where the mixing matrix is unknown. This situation is common in acoustics, radio, medical signal and image processing, hyperspectral imaging, and other areas. We suggest a two-stage separation process: a priori selection of a possibly overcomplete signal dictionary (for instance, a wavelet frame or a learned dictionary) in which the sources are assumed to be sparsely representable, followed by unmixing the sources by exploiting the their sparse representability. We consider the general case of more sources than mixtures, but also derive a more efficient algorithm in the case of a nonovercomplete dictionary and an equal numbers of sources and mixtures. Experiments with artificial signals and musical sounds demonstrate significantly better separation than other known techniques.",
Generating the knowledge base of a fuzzy rule-based system by the genetic learning of the data base,"A method is proposed to automatically learn the knowledge base by finding an appropiate data base by means of a genetic algorithm while using a simple generation method to derive the rule base. Our genetic process learns the number of linguistic terms per variable and the membership function parameters that define their semantics, while a rule base generation method learns the number of rules and their composition.","Fuzzy systems,
Knowledge based systems,
Genetic algorithms,
Learning,
Computer science,
Artificial intelligence,
Helium,
Concrete,
System performance,
Hybrid power systems"
The distribution of target registration error in rigid-body point-based registration,"Guidance systems designed for neurosurgery, hip surgery, spine surgery and for approaches to other anatomy that is relatively rigid can use rigid-body transformations to accomplish image registration. These systems often rely on point-based registration to determine the transformation and many such systems use attached fiducial markers to establish accurate fiducial points for the registration, the points being established by some fiducial localization process. Accuracy is important to these systems, as is knowledge of the level of that accuracy. An advantage of marker-based systems, particularly those in which the markers are bone-implanted, is that registration error depends only on the fiducial localization and is, thus, to a large extent independent of the particular object being registered. Thus, it should be possible to predict the clinical accuracy of marker-based systems on the basis of experimental measurements made with phantoms or previous patients. For most registration tasks, the most important error measure is target registration error (TRE), which is the distance after registration between corresponding points not used in calculating the registration transform. Here, the authors derive an approximation to the distribution of TRE; this is an extension of previous work that gave the expected squared value of TRE. They show the distribution of the squared magnitude of TRE and that of the component of TRE in an arbitrary direction. Using numerical simulations, the authors show that their theoretical results are a close match to the simulated ones.","Neurosurgery,
Surgery,
Image registration,
Hip,
Anatomy,
Accuracy,
Imaging phantoms,
Numerical simulation,
Helium,
Medical services"
Landmark-based elastic registration using approximating thin-plate splines,"The authors consider elastic image registration based on a set of corresponding anatomical point landmarks and approximating thin-plate splines. This approach is an extension of the original interpolating thin-plate spline approach and allows to take into account landmark localization errors. The extension is important for clinical applications since landmark extraction is always prone to error. The authors' approach is based on a minimizing functional and can cope with isotropic as well as anisotropic landmark errors. In particular, in the latter case it is possible to include different types of landmarks, e.g., unique point landmarks as well as arbitrary edge points. Also, the scheme is general with respect to the image dimension and the order of smoothness of the underlying functional. Optimal affine transformations as well as interpolating thin-plate splines are special cases of this scheme. To localize landmarks the authors use a semi-automatic approach which is based on three-dimensional (3-D) differential operators. Experimental results are presented for two-dimensional as well as 3-D tomographic images of the human brain.",
Vision-based mobile robot localization and mapping using scale-invariant features,"A key component of a mobile robot system is the ability to localize itself accurately and build a map of the environment simultaneously. In this paper, a vision-based mobile robot localization and mapping algorithm is described which uses scale-invariant image features as landmarks in unmodified dynamic environments. These 3D landmarks are localized and robot ego-motion is estimated by matching them, taking into account the feature viewpoint variation. With our Triclops stereo vision system, experiments show that these features are robustly matched between views, 3D landmarks are tracked, robot pose is estimated and a 3D map is built.","Mobile robots,
Stereo vision,
Robot vision systems,
Sonar navigation,
Cameras,
Robot sensing systems,
Computer science,
Robustness,
Simultaneous localization and mapping,
Machine vision"
Hierarchical model for real time simulation of virtual human crowds,"We describe a model for simulating crowds of humans in real time. We deal with a hierarchy composed of virtual crowds, groups, and individuals. The groups are the most complex structure that can be controlled in different degrees of autonomy. This autonomy refers to the extent to which the virtual agents are independent of user intervention and also the amount of information needed to simulate crowds. Thus, depending on the complexity of the simulation, simple behaviors can be sufficient to simulate crowds. Otherwise, more complicated behavioral rules can be necessary and, in this case, it can be included in the simulation data in order to improve the realism of the animation. We present three different ways for controlling crowd behaviors: by using innate and scripted behaviors; by defining behavioral rules, using events and reactions; and by providing an external control to guide crowd behaviors in real time. The two main contributions of our approach are: the possibility of increasing the complexity of group/agent behaviors according to the problem to be simulated and the hierarchical structure based on groups to compose a crowd.","Humans,
Animation,
Application software,
Computational modeling,
Virtual environment,
Autonomous agents,
Collaboration,
Discrete event simulation"
Computer-aided diagnosis in chest radiography: a survey,"The traditional chest radiograph is still ubiquitous in clinical practice, and will likely remain so for quite some time. Yet, its interpretation is notoriously difficult. This explains the continued interest in computer-aided diagnosis for chest radiography. The purpose of this survey is to categorize and briefly review the literature on computer analysis of chest images, which comprises over 150 papers published in the last 30 years. Remaining challenges are indicated and some directions for future research are given.","Computer aided diagnosis,
Diagnostic radiography,
Biomedical imaging,
Medical diagnostic imaging,
Lungs,
Physics computing,
Image analysis,
Magnetic resonance imaging,
Image segmentation,
Computed tomography"
Fuzzy c-means clustering of incomplete data,"The problem of clustering a real s-dimensional data set X={x/sub 1/,...,x/sub n/} /spl sub/ R/sup s/ is considered. Usually, each observation (or datum) consists of numerical values for all s features (such as height, length, etc.), but sometimes data sets can contain vectors that are missing one or more of the feature values. For example, a particular datum x/sub k/ might be incomplete, having the form x/sub k/=(254.3, ?, 333.2, 47.45, ?)/sup T/, where the second and fifth feature values are missing. The fuzzy c-means (FCM) algorithm is a useful tool for clustering real s-dimensional data, but it is not directly applicable to the case of incomplete data. Four strategies for doing FCM clustering of incomplete data sets are given, three of which involve modified versions of the FCM algorithm. Numerical convergence properties of the new algorithms are discussed, and all approaches are tested using real and artificially generated incomplete data sets.","Clustering algorithms,
Parameter estimation,
Pattern recognition,
Computer science,
Maximum likelihood estimation,
Convergence of numerical methods,
Testing,
Mathematics,
Fuzzy logic,
Performance analysis"
Software reflexion models: bridging the gap between design and implementation,"The artifacts constituting a software system often drift apart over time. We have developed the software reflexion model technique to help engineers perform various software engineering tasks by exploiting, rather than removing, the drift between design and implementation. More specifically, the technique helps an engineer compare artifacts by summarizing where one artifact (such as a design) is consistent with and inconsistent with another artifact (such as source). The technique can be applied to help a software engineer evolve a structural mental model of a system to the point that it is ""good enough"" to be used for reasoning about a task at hand. The software reflexion model technique has been applied to support a variety of tasks, including design conformance, change assessment, and an experimental reengineering of the million-lines-of-code Microsoft Excel product. We provide a formal characterization of the reflexion model technique, discuss practical aspects of the approach, relate experiences of applying the approach and tools, and place the technique into the context of related work.","Design engineering,
Software performance,
Reverse engineering,
Software systems,
Software engineering,
Maintenance engineering,
Computer Society,
Cognitive science,
Spreadsheet programs,
Context modeling"
Novel robust stability criteria for interval-delayed Hopfield neural networks,"In this paper, some novel criteria for the global robust stability of a class of interval Hopfield neural networks with constant delays are given. Based on several new Lyapunov functionals, delay-independent criteria are provided to guarantee the global robust stability of such systems. For conventional Hopfield neural networks with constant delays, some new criteria for their global asymptotic stability are also easily obtained. All the results obtained are generalizations of some recent results reported in the literature for neural networks with constant delays. Numerical examples are also given to show the correctness of the analysis.","Robust stability,
Hopfield neural networks,
Delay effects,
Neural networks,
Computer science,
Fluctuations,
Very large scale integration,
Asymptotic stability,
Information processing,
Application software"
Multistage hybrid active appearance model matching: segmentation of left and right ventricles in cardiac MR images,"A fully automated approach to segmentation of the left and right cardiac ventricles from magnetic resonance (MR) images is reported. A novel multistage hybrid appearance model methodology is presented in which a hybrid active shape model/active appearance model (AAM) stage helps avoid local minima of the matching function. This yields an overall more favorable matching result. An automated initialization method is introduced making the approach fully automated. The authors' method was trained in a set of 102 MR images and tested in a separate set of 60 images. In all testing cases, the matching resulted in a visually plausible and accurate mapping of the model to the image data. Average signed border positioning errors did not exceed 0.3 mm in any of the three determined contours-left-ventricular (LV) epicardium, LV and right-ventricular (RV) endocardium. The area measurements derived from the three contours correlated well with the independent standard (r=0.96, 0.96, 0.90), with slopes and intercepts of the regression lines close to one and zero, respectively. Testing the reproducibility of the method demonstrated an unbiased performance with small range of error as assessed via Bland-Altman statistic. In direct border positioning error comparison, the multistage method significantly outperformed the conventional AAM (p<0.001). The developed method promises to facilitate fully automated quantitative analysis of LV and RV morphology and function in clinical setting.","Image segmentation,
Testing,
Active shape model,
Active appearance model,
Magnetic resonance,
Area measurement,
Measurement standards,
Reproducibility of results,
Statistical analysis,
Error analysis"
Event-based analysis of video,"Dynamic events can be regarded as long-term temporal objects, which are characterized by spatio-temporal features at multiple temporal scales. Based on this, we design a simple statistical distance measure between video sequences (possibly of different lengths) based on their behavioral content. This measure is non-parametric and can thus handle a wide range of dynamic events. We use this measure for isolating and clustering events within long continuous video sequences. This is done without prior knowledge of the types of events, their models, or their temporal extent. An outcome of such a clustering process is a temporal segmentation of long video sequences into event-consistent sub-sequences, and their grouping into event-consistent clusters. Our event representation and associated distance measure can also be used for event-based indexing into long video sequences, even when only one short example-clip is available. However, when multiple example-clips of the same event are available (either as a result of the clustering process, or given manually), these can be used to refine the event representation, the associated distance measure, and accordingly the quality of the detection and clustering process.","Video sequences,
Motion pictures,
Length measurement,
Dynamic range,
Indexing,
Information analysis,
Parametric statistics,
Stochastic processes,
Computer science,
Event detection"
Fast and robust optic disc detection using pyramidal decomposition and Hausdorff-based template matching,"Reports on the design and test of an image processing algorithm for the localization of the optic disk (OD) in low-resolution (about 20 /spl mu//pixel) color fundus images. The design relies on the combination of two procedures: 1) a Hausdorff-based template matching technique on edge map, guided by 2) a pyramidal decomposition for large scale object tracking. The two approaches are tested against a database of 40 images of various visual quality and retinal pigmentation, as well as of normal and small pupils. An average error of 7% on OD center positioning is reached with no false detection. In addition, a confidence level is associated to the final detection that indicates the ""level of difficulty"" the detector has to identify the OD position and shape.","Robustness,
Optical detectors,
Testing,
Optical design,
Algorithm design and analysis,
Image processing,
Pixel,
Color,
Large-scale systems,
Image databases"
MAFIA: a maximal frequent itemset algorithm for transactional databases,"We present a new algorithm for mining maximal frequent itemsets from a transactional database. Our algorithm is especially efficient when the itemsets in the database are very long. The search strategy of our algorithm integrates a depth-first traversal of the itemset lattice with effective pruning mechanisms. Our implementation of the search strategy combines a vertical bitmap representation of the database with an efficient relative bitmap compression schema. In a thorough experimental analysis of our algorithm on real data, we isolate the effect of the individual components of the algorithm. Our performance numbers show that our algorithm outperforms previous work by a factor of three to five.","Itemsets,
Transaction databases,
Association rules,
Algorithm design and analysis,
Computer science,
Data mining,
Lattices,
Pattern analysis,
Performance analysis,
Web pages"
A Bayesian approach to digital matting,"This paper proposes a new Bayesian framework for solving the matting problem, i.e. extracting a foreground element from a background image by estimating an opacity for each pixel of the foreground element. Our approach models both the foreground and background color distributions with spatially-varying sets of Gaussians, and assumes a fractional blending of the foreground and background colors to produce the final output. It then uses a maximum-likelihood criterion to estimate the optimal opacity, foreground and background simultaneously. In addition to providing a principled approach to the matting problem, our algorithm effectively handles objects with intricate boundaries, such as hair strands and fur, and provides an improvement over existing techniques for these difficult cases.",
Retrospective correction of MR intensity inhomogeneity by information minimization,"In this paper, the problem of retrospective correction of intensity inhomogeneity in magnetic resonance (MR) images is addressed. A novel model-based correction method is proposed, based on the assumption that an image corrupted by intensity inhomogeneity contains more information than the corresponding uncorrupted image. The image degradation process is described by a linear model, consisting of a multiplicative and an additive component which are modeled by a combination of smoothly varying basis functions. The degraded image is corrected by the inverse of the image degradation model. The parameters of this model are optimized such that the information of the corrected image is minimized while the global intensity statistic is preserved. The method was quantitatively evaluated and compared to other methods on a number of simulated and real MR images and proved to be effective, reliable, and computationally attractive. The method can be widely applied to different types of MR images because it solely uses the information that is naturally present in an image, without making assumptions on its spatial and intensity distribution. Besides, the method requires no preprocessing, parameter setting, nor user interaction. Consequently, the proposed method may be a valuable tool in MR image analysis.","Magnetic resonance imaging,
Degradation,
Image analysis,
Nonuniform electric fields,
Radio frequency,
Filtering,
Magnetic resonance,
Image segmentation,
Coils,
In vitro"
Correctness of Belief Propagation in Gaussian Graphical Models of Arbitrary Topology,"Graphical models, such as Bayesian networks and Markov random fields, represent statistical dependencies of variables by a graph. Local belief propagation rules of the sort proposed by Pearl (1988) are guaranteed to converge to the correct posterior probabilities in singly connected graphs. Recently, good performance has been obtained by using these same rules on graphs with loops, a method we refer to as loopy belief propagation. Perhaps the most dramatic instance is the near Shannon-limit performance of Turbo codes, whose decoding algorithm is equivalent to loopy propagation. Except for the case of graphs with a single loop, there has been little theoretical understanding of loopy propagation. Here we analyze belief propagation in networks with arbitrary topologies when the nodes in the graph describe jointly gaussian random variables. We give an analytical formula relating the true posterior probabilities with those calculated using loopy propagation. We give sufficient conditions for convergence and show that when belief propagation converges, it gives the correct posterior means for all graph topologies, not just networks with a single loop. These results motivate using the powerful belief propagation algorithm in a broader class of networks and help clarify the empirical performance results.",
OOF: an image-based finite-element analysis of material microstructures,Determining a material's macroscopic properties given its microscopic structure is of fundamental importance to materials science. The authors describe two public-domain programs that jointly predict macroscopic behavior: OOF (Object-Oriented Finite elements) and ppm2oof (Portable Pixel Map to OOF format translator). The programs start from an image of the microstructure and end with results from finite-element calculations.,
Advanced and authenticated marking schemes for IP traceback,"Defending against distributed denial-of-service attacks is one of the hardest security problems on the Internet today. One difficulty to thwart these attacks is to trace the source of the attacks because they often use incorrect, or spoofed IP source addresses to disguise the true origin. In this paper, we present two new schemes, the advanced marking scheme and the authenticated marking scheme, which allow the victim to trace-back the approximate origin of spoofed IP packets. Our techniques feature low network and router overhead, and support incremental deployment. In contrast to previous work, our techniques have significantly higher precision (lower false positive rate) and fewer computation overhead for the victim to reconstruct the attack paths under large scale distributed denial-of-service attacks. Furthermore the authenticated marking scheme provides efficient authentication of routers' markings such that even a compromised router cannot forge or tamper markings from other uncompromised routers.","Computer crime,
Internet,
Distributed computing,
Computer science,
Computer security,
Large-scale systems,
Authentication,
Usability,
US Government agencies,
Contracts"
SETI@home-massively distributed computing for SETI,"Starting in the late 1950s, researchers have been performing progressively more sensitive searches for radio signals from extraterrestrial civilizations, but each search has been limited by the technologies available at the time. As radio frequency technologies have became more efficient and computers have become faster, the searches have grown larger and more sensitive, The SETI@home project, managed by a group of researchers at the Space Sciences Laboratory of the University of California, Berkeley, is the first attempt to use large-scale distributed computing to perform a sensitive search for radio signals from extraterrestrial civilizations.",
Simultaneous correction of ghost and geometric distortion artifacts in EPI using a multiecho reference scan,"A computationally efficient technique is described for the simultaneous removal of ghosting and geometrical distortion artifacts in echo-planar imaging (EPI) utilizing a multiecho, gradient-echo reference scan. Nyquist ghosts occur in EPI reconstructions because odd and even lines of k-space are acquired with opposite polarity, and experimental imperfections such as gradient eddy currents, imperfect pulse sequence timing, B/sub 0/ field inhomogeneity, susceptibility, and chemical shift result in the even and odd lines of k-space being offset by different amounts relative to the true center of the acquisition window. Geometrical distortion occurs due to the limited bandwidth of the EPI images in the phase-encode direction. This distortion can be problematic when attempting to overlay an activation map from a functional magnetic resonance imaging experiment generated from EPI data on a high-resolution anatomical image. The method described here corrects for geometrical distortion related to B/sub 0/ inhomogeneity, gradient eddy currents, radio-frequency pulse frequency offset, and chemical shift effect. The algorithm for removing ghost artifacts utilizes phase information in two dimensions and is, thus, more robust than conventional one-dimensional methods. An additional reference scan is required which takes approximately 2 min for a matrix size of 64/spl times/64 and a repetition time of 2 s. Results from a water phantom and a human brain at 3 T demonstrate the effectiveness of the method for removing ghosts and geometric distortion artifacts.","Eddy currents,
Chemicals,
Image reconstruction,
Timing,
Phase distortion,
Bandwidth,
Magnetic resonance imaging,
Radio frequency,
Robustness,
Imaging phantoms"
Tracking multiple moving targets with a mobile robot using particle filters and statistical data association,"One of the goals in the field of mobile robotics is the development of mobile platforms which operate in populated environments and offer various services to humans. For many tasks it is highly desirable that a robot can determine the positions of the humans in its surrounding. In this paper we present a method for tracking multiple moving objects with a mobile robot. We introduce a sample-based variant of joint probabilistic data association filters to track features originating from individual objects and to solve the correspondence problem between the detected features and the filters. In contrast to standard methods, occlusions are handled explicitly during data association. The technique has been implemented and tested on a real robot. Experiments carried out in a typical office environment show that the method is able to track multiple persons even when the trajectories of two people are crossing each other.","Target tracking,
Particle tracking,
Mobile robots,
Particle filters,
State estimation,
Computer science,
Humans,
Robot sensing systems,
Mobile computing,
Object detection"
Local feature view clustering for 3D object recognition,"There have been important recent advances in object recognition through the matching of invariant local image features. However, the existing approaches are based on matching to individual training images. This paper presents a method for combining multiple images of a 3D object into a single model representation. This provides for recognition of 3D objects from any viewpoint, the generalization of models to non-rigid changes, and improved robustness through the combination of features acquired under a range of imaging conditions. The decision of whether to cluster a training image into an existing view representation or to treat it as a new view is based on the geometric accuracy of the match to previous model views. A new probabilistic model is developed to reduce the false positive matches that would otherwise arise due to loosened geometric constraints on matching 3D and non-rigid models. A system has been developed based on these approaches that is able to robustly recognize 3D objects in cluttered natural images in sub-second times.","Object recognition,
Image recognition,
Robustness,
Solid modeling,
Image databases,
Spatial databases,
Computer science,
Layout,
Lighting,
Performance evaluation"
PDE: a Pareto-frontier differential evolution approach for multi-objective optimization problems,"The use of evolutionary algorithms (EAs) to solve problems with multiple objectives (known as multi-objective optimization problems (MOPs)) has attracted much attention. Being population based approaches, EAs offer a means to find a group of Pareto-optimal solutions in a single run. Differential evolution (DE) is an EA that was developed to handle optimization problems over continuous domains. The objective of this paper is to introduce a novel Pareto-frontier differential evolution (PDE) algorithm to solve MOPs. The solutions provided by the proposed algorithm for two standard test problems, outperform the Strength Pareto Evolutionary Algorithm, one of the state-of-the-art evolutionary algorithms for solving MOPs.","Evolutionary computation,
Humans,
Mathematical programming,
Costs,
Computer science,
Educational institutions,
Drives,
Australia,
Testing,
Decision making"
Rigid registration of 3-D ultrasound with MR images: a new approach combining intensity and gradient information,"Presents a new image-based technique to rigidly register intraoperative three-dimensional ultrasound (US) with preoperative magnetic resonance (MR) images. Automatic registration is achieved by maximization of a similarity measure which generalizes the correlation ratio, and whose novelty is to incorporate multivariate information from the MR data (intensity and gradient). In addition, the similarity measure is built upon a robust intensity-based distance measure, which makes it possible to handle a variety of US artifacts. A cross-validation study has been carried out using a number of phantom and clinical data. This indicates that the method is quite robust and that the worst registration errors are of the order of the MR image resolution.","Ultrasonic imaging,
Magnetic resonance,
Ultrasonic variables measurement,
Robustness,
Feature extraction,
Chromium,
Imaging phantoms,
Magnetic resonance imaging,
Computed tomography,
Probes"
Simgrid: a toolkit for the simulation of application scheduling,"Advances in hardware and software technologies have made it possible to deploy parallel applications over increasingly large sets of distributed resources. Consequently, the study of scheduling algorithms for such applications has been an active area of research. Given the nature of most scheduling problems one must resort to simulation to effectively evaluate and compare their efficacy over a wide range of scenarios. It has thus become necessary to simulate those algorithms for increasingly complex distributed dynamic, heterogeneous environments. We present Simgrid a simulation toolkit for the study of scheduling algorithms for distributed application. We give the main concepts and models behind Simgrid, describe its API and highlight current implementation issues. We also give some experimental results and describe work that builds on Simgrid's functionalities.","Scheduling algorithm,
Application software,
Optimal scheduling,
Processor scheduling,
Computational modeling,
Hardware,
Computer networks,
Computer science,
Distributed computing,
Grid computing"
Geometric blur for template matching,"We address the problem of finding point correspondences in images by way of an approach to template matching that is robust under affine distortions. This is achieved by applying ""geometric blur"" to both the template and the image, resulting in a fall-off in similarity that is close to linear in the norm of the distortion between the template and the image. Results in wide baseline stereo correspondence, face detection, and feature correspondence are included.","Image edge detection,
Stereo vision,
Computer vision,
Object recognition,
Uncertainty,
Lighting,
Computer science,
Robustness,
Face detection,
Concrete"
Test volume and application time reduction through scan chain concealment,"A test pattern compression scheme is proposed in order to reduce test data volume and application time. The number of scan chains that can be supported by an ATE is significantly increased by utilizing an on-chip decompressor. The functionality of the ATE is kept intact by moving the decompression task to the circuit under test. While the number of virtual scan chains visible to the ATE is kept small, the number of internal scan chains driven by the decompressed pattern sequence can be significantly increased.","Circuit testing,
Circuit faults,
Test pattern generators,
Compaction,
Application software,
Computer science,
Sequential analysis,
Fault detection,
Automatic test pattern generation,
Permission"
Combinatorial properties of frameproof and traceability codes,"In order to protect copyrighted material, codes may be embedded in the content or codes may be associated with the keys used to recover the content. Codes can offer protection by providing some form of traceability (TA) for pirated data. Several researchers have studied different notions of TA and related concepts in previous years. ""Strong"" versions of TA allow at least one member of a coalition that constructs a ""pirate decoder"" to be traced. Weaker versions of this concept ensure that no coalition can ""frame"" a disjoint user or group of users. All these concepts can be formulated as codes having certain combinatorial properties. We study the relationships between the various notions, and we discuss equivalent formulations using structures such as perfect hash families. We use methods from combinatorics and coding theory to provide bounds (necessary conditions) and constructions (sufficient conditions) for the objects of interest.",Codes
Three-dimensional multimodal brain warping using the Demons algorithm and adaptive intensity corrections,"This paper presents an original method for three-dimensional elastic registration of multimodal images. The authors propose to make use of a scheme that iterates between correcting for intensity differences between images and performing standard monomodal registration. The core of the authors' contribution resides in providing a method that finds the transformation that maps the intensities of one image to those of another. It makes the assumption that there are at most two functional dependencies between the intensities of structures present in the images to register, and relies on robust estimation techniques to evaluate these functions. The authors provide results showing successful registration between several imaging modalities involving segmentations, T1 magnetic resonance (MR), T2 MR, proton density (PD) MR and computed tomography (CT). The authors also argue that their intensity modeling may be more appropriate than mutual information (MI) in the context of evaluating high-dimensional deformations, as it puts more constraints on the parameters to be estimated and, thus, permits a better search of the parameter space.","Computed tomography,
Registers,
Robustness,
Magnetic resonance imaging,
Image segmentation,
Magnetic resonance,
Protons,
Context modeling,
Deformable models,
Mutual information"
Nonparametric genetic clustering: comparison of validity indices,"A variable-string-length genetic algorithm (GA) is used for developing a novel nonparametric clustering technique when the number of clusters is not fixed a-priori. Chromosomes in the same population may now have different lengths since they encode different number of clusters. The crossover operator is redefined to tackle the concept of variable string length. A cluster validity index is used as a measure of the fitness of a chromosome. The performance of several cluster validity indices, namely the Davies-Bouldin (1979) index, Dunn's (1973) index, two of its generalized versions and a recently developed index, in appropriately partitioning a data set, are compared.","Genetic algorithms,
Biological cells,
Pattern recognition,
Evolution (biology),
Parallel processing,
Biological information theory,
Genetic mutations,
Pattern classification,
Machine intelligence,
Computer science"
Minimum-energy mobile wireless networks revisited,"We propose a protocol that, given a communication network, computes a subnetwork such that, for every pair (u, /spl upsi/) of nodes connected in the original network, there is a a minimum-energy path between u and /spl upsi/ in the subnetwork (where a minimum-energy path is one that allows messages to be transmitted with a minimum use of energy). The network computed by our protocol is in general a subnetwork of the one computed by the protocol given by Rodoplu and Meng (see IEEE J. Selected Areas in Communications, vol.17, no.8, p.1333-44, 1999). Moreover, our protocol is computationally simpler. We demonstrate the performance improvements obtained by using the subnetwork computed by our protocol through simulation.","Wireless networks,
Protocols,
Communication networks,
Computer networks,
Military computing,
Wireless sensor networks,
Computer science,
Computational modeling,
Broadcasting,
Spread spectrum communication"
Dynamic texture recognition,"Dynamic textures are sequences of images that exhibit some form of temporal stationarity, such as waves, steam, and foliage. We pose the problem of recognizing and classifying dynamic textures in the space of dynamical systems where each dynamic texture is uniquely represented. Since the space is non-linear, a distance between models must be defined We examine three different distances in the space of autoregressive models and assess their power.","Image motion analysis,
Statistics,
Image recognition,
Layout,
Computer science,
Computer vision,
Photometry,
Stochastic processes,
Optical variables control,
Hidden Markov models"
Detection of breast masses in mammograms by density slicing and texture flow-field analysis,"We propose a method for the detection of masses in mammographic images that employs Gaussian smoothing and subsampling operations as preprocessing steps. The mass portions are segmented by establishing intensity links from the central portions of masses into the surrounding areas. We introduce methods for analyzing oriented flow-like textural information in mammograms. Features based on flow orientation in adaptive ribbons of pixels across the margins of masses are proposed to classify the regions detected as true mass regions or false-positives (FPs). The methods yielded a mass versus normal tissue classification accuracy represented as an area (A/sub z/) of 0.87 under the receiver operating characteristics (ROCs) curve with a dataset of 56 images including 30 benign disease, 13 malignant disease, and 13 normal cases selected from the mini Mammographic Image Analysis Society database. A sensitivity of 81% was achieved at 2.2 FPs/image. Malignant tumor versus normal tissue classification resulted in a higher A/sub z/ value of 0.9 under the ROC curve using only the 13 malignant and 13 normal cases with a sensitivity of 85% at 2.45 FPs/image. The mass detection algorithm could detect all the 13 malignant tumors successfully, but achieved a success rate of only 63% (19/30) in detecting the benign masses. The mass regions that were successfully segmented were further classified as benign or malignant disease by computing five texture features based on gray-level co-occurrence matrices (GCMs) and using the features in a logistic regression method. The features were computed using adaptive ribbons of pixels across the boundaries of the masses. Benign versus malignant classification using the GCM-based texture features resulted in A/sub z/=0.79 with 19 benign and 13 malignant cases.","Breast,
Cancer,
Image texture analysis,
Diseases,
Image segmentation,
Sensitivity,
Malignant tumors,
Smoothing methods,
Information analysis,
Image databases"
Biomechanical 3-D finite element modeling of the human breast using MRI data,"Breast tissue deformation modeling has recently gained considerable interest in various medical applications. A biomechanical model of the breast is presented using a finite element (FE) formulation. Emphasis is given to the modeling of breast tissue deformation which takes place in breast imaging procedures. The first step in implementing the FE modeling (FEM) procedure is mesh generation. For objects with irregular and complex geometries such as the breast, this step is one of the most difficult and tedious tasks. For FE mesh generation, two automated methods are presented which process MRI breast images to create a patient-specific mesh. The main components of the breast are adipose, fibroglandular and skin tissues. For modeling the adipose and fibroglandular tissues, we used eight noded hexahedral elements with hyperelastic properties, while for the skin, we chose four noded hyperelastic membrane elements. For model validation, an MR image of an agarose phantom was acquired and corresponding FE meshes were created. Based on assigned elasticity parameters, a numerical experiment was performed using the FE meshes, and good results were obtained. The model was also applied to a breast image registration problem of a volunteer's breast. Although qualitatively reasonable, further work is required to validate the results quantitatively.","Finite element methods,
Humans,
Magnetic resonance imaging,
Breast tissue,
Deformable models,
Mesh generation,
Skin,
Medical services,
Biomedical equipment,
Geometry"
On self-adaptive features in real-parameter evolutionary algorithms,"Due to the flexibility in adapting to different fitness landscapes, self-adaptive evolutionary algorithms (SA-EAs) have been gaining popularity in the recent past. In this paper, we postulate the properties that SA-EA operators should have for successful applications in real-valued search spaces. Specifically, population mean and variance of a number of SA-EA operators such as various real-parameter crossover operators and self-adaptive evolution strategies are calculated for this purpose. Simulation results are shown to verify the theoretical calculations. The postulations and population variance calculations explain why self-adaptive genetic algorithms and evolution strategies have shown similar performance in the past and also suggest appropriate strategy parameter values, which must be chosen while applying and comparing different SA-EAs.","Evolutionary computation,
Genetic mutations,
Genetic algorithms,
Electronic switching systems,
Genetic programming,
Automatic testing,
Helium,
Government,
Computer science,
Laboratories"
Robust online appearance models for visual tracking,"We propose a framework for learning robust, adaptive appearance models to be used for motion-based tracking of natural objects. The approach involves a mixture of stable image structure, learned over long time courses, along with 2-frame motion information and an outlier process. An online EM-algorithm is used to adapt the appearance model parameters over time. An implementation of this approach is developed for an appearance model based on the filter responses from a steerable pyramid. This model is used in a motion-based tracking algorithm to provide robustness in the face of image outliers, such as those caused by occlusions. It also provides the ability to adapt to natural changes in appearance, such as those due to facial expressions or variations in 3D pose. We show experimental results on a variety of natural image sequences of people moving within cluttered environments.","Robustness,
Target tracking,
Biological system modeling,
Stability,
Statistics,
Computer science,
Filters,
Image sequences,
Motion estimation,
Clothing"
Automated graph-based analysis and correction of cortical volume topology,The human cerebral cortex is topologically equivalent to a sheet and can be considered topologically spherical if it is closed at the brainstem. Low-level segmentation of magnetic resonance (MR) imagery typically produces cerebral volumes whose tessellations are not topologically spherical. The authors present a novel algorithm that analyzes and constrains the topology of a volumetric object. Graphs are formed that represent the connectivity of voxel segments in the foreground and background of the image. These graphs are analyzed and minimal corrections to the volume are made prior to tessellation. The authors apply the algorithm to a simple test object and to cerebral white matter masks generated by a low-level tissue identification sequence. The authors tessellate the resulting objects using the marching cubes algorithm and verify their topology by computing their Euler characteristics. A key benefit of the algorithm is that it localizes the change to a volume to the specific areas of its topological defects.,"Topology,
Cerebral cortex,
Magnetic analysis,
Image segmentation,
Brain,
Humans,
Magnetic resonance,
Signal processing,
Image processing,
Electroencephalography"
Analysis of LSB based image steganography techniques,"There have been many techniques for hiding messages in images in such a manner that the alterations made to the image are perceptually indiscernible. However, the question whether they result in images that are statistically indistinguishable from untampered images has not been adequately explored. We look at some specific image based steganography techniques and show that an observer can indeed distinguish between images carrying a hidden message and images which do not carry a message. We derive a closed form expression of the probability of detection and false alarm in terms of the number of bits that are hidden. This leads us to the notion of steganographic capacity, that is, how many bits can we hide in a message without causing statistically significant modifications? Our results are able to provide an upper bound on the this capacity. Our ongoing work relates to adaptive steganographic techniques that take explicit steps to foil the detection mechanisms. In this case we hope to show that the number of bits that can be embedded increases significantly.","Image analysis,
Steganography,
Upper bound,
Computer science,
Context,
Entropy,
Probability distribution,
Helium,
Graphics"
In vivo measurement of 3-D skeletal kinematics from sequences of biplane radiographs: Application to knee kinematics,"Current noninvasive or minimally invasive methods for evaluating in vivo knee kinematics are inadequate for accurate determination of dynamic joint function due to limited accuracy and/or insufficient sampling rates. A three-dimensional (3-D) model-based method is presented to estimate skeletal motion of the knee from high-speed sequences of biplane radiographs. The method implicitly assumes that geometrical features cannot be detected reliably and an exact segmentation of bone edges is not always feasible. An existing biplane radiograph system was simulated as two separate single-plane radiograph systems. Position and orientation of the underlying bone was determined for each single-plane view by generating projections through a 3-D volumetric model (from computed tomography), and producing an image (digitally reconstructed radiograph) similar (based on texture information and rough edges of bone) to the two-dimensional radiographs. The absolute 3-D pose was determined using known imaging geometry of the biplane radiograph system and a 3-D line intersection method. Results were compared to data of known accuracy, obtained from a previously established bone-implanted marker method. Difference of controlled in vitro tests was on the order of 0.5 mm for translation and 1.4/spl deg/ for rotation. A biplane radiograph sequence of a canine hindlimb during treadmill walking was used for in vivo testing, with differences on the order of 0.8 mm for translation and 2.5/spl deg/ for rotation.","Kinematics,
In vivo,
Radiography,
Knee,
Bones,
Image edge detection,
Testing,
Minimally invasive surgery,
Joints,
Sampling methods"
Patient-specific models for lung nodule detection and surveillance in CT images,"The purpose of this work is to develop patient-specific models for automatically detecting lung nodules in computed tomography (CT) images. It is motivated by significant developments in CT scanner technology and the burden that lung cancer screening and surveillance imposes on radiologists. We propose a new method that uses a patient's baseline image data to assist in the segmentation of subsequent images so that changes in size and/or shape of nodules can be measured automatically. The system uses a generic, a priori model to detect candidate nodules on the baseline scan of a previously unseen patient. A user then confirms or rejects nodule candidates to establish baseline results. For analysis of follow-up scans of that particular patient, a patient-specific model is derived from these baseline results. This model describes expected features (location, volume and shape) of previously segmented nodules so that the system can relocalize them automatically on follow-up. On the baseline scans of 17 subjects, a radiologist identified a total of 36 nodules, of which 31 (86%) were detected automatically by the system with an average of 11 false positives (FPs) per case. In follow-up scans 27 of the 31 nodules were still present and, using patient-specific models, 22 (81%) were correctly relocalized by the system. The system automatically detected 16 out of a possible 20 (80%) of new nodules on follow-up scans with ten FPs per case.","Lungs,
Surveillance,
Computed tomography,
Cancer,
Shape measurement,
Size measurement,
High-resolution imaging,
Protocols,
Anatomy,
Optical imaging"
An adaptive-focus statistical shape model for segmentation and shape modeling of 3-D brain structures,"This paper presents a deformable model for automatically segmenting brain structures from volumetric magnetic resonance (MR) images and obtaining point correspondences, using geometric and statistical information in a hierarchical scheme. Geometric information is embedded into the model via a set of affine-invariant attribute vectors, each of which characterizes the geometric structure around a point of the model from a local to a global scale. The attribute vectors, in conjunction with the deformation mechanism of the model, warrant that the model not only deforms to nearby edges, as is customary in most deformable surface models, but also that it determines point correspondences based on geometric similarity at different scales. The proposed model is adaptive in that it initially focuses on the most reliable structures of interest, and gradually shifts focus to other structures as those become closer to their respective targets and, therefore, more reliable. The proposed techniques have been used to segment boundaries of the ventricles, the caudate nucleus, and the lenticular nucleus from volumetric MR images.","Shape,
Brain modeling,
Deformable models,
Image segmentation,
Solid modeling,
Biological system modeling,
Radiology,
Focusing,
Statistics,
Magnetic resonance"
A shape- and texture-based enhanced Fisher classifier for face recognition,"This paper introduces a new face coding and recognition method, the enhanced Fisher classifier (EFC), which employs the enhanced Fisher linear discriminant model (EFM) on integrated shape and texture features. Shape encodes the feature geometry of a face while texture provides a normalized shape-free image. The dimensionalities of the shape and the texture spaces are first reduced using principal component analysis, constrained by the EFM for enhanced generalization. The corresponding reduced shape and texture features are then combined through a normalization procedure to form the integrated features that are processed by the EFM for face recognition. Experimental results, using 600 face images corresponding to 200 subjects of varying illumination and facial expressions, show that (1) the integrated shape and texture features carry the most discriminating information followed in order by textures, masked images, and shape images, and (2) the new coding and face recognition method, EFC, performs the best among the eigenfaces method using L/sub 1/ or L/sub 2/ distance measure, and the Mahalanobis distance classifiers using a common covariance matrix for all classes or a pooled within-class covariance matrix. In particular, EFC achieves 98.5% recognition accuracy using only 25 features.","Face recognition,
Principal component analysis,
Shape measurement,
Covariance matrix,
Shape control,
Image coding,
Computer science,
Geometry,
Lighting,
Performance evaluation"
A new pruning heuristic based on variance analysis of sensitivity information,"Architecture selection is a very important aspect in the design of neural networks (NNs) to optimally tune performance and computational complexity. Sensitivity analysis has been used successfully to prune irrelevant parameters from feedforward NNs. This paper presents a new pruning algorithm that uses the sensitivity analysis to quantify the relevance of input and hidden units. A new statistical pruning heuristic is proposed, based on the variance analysis, to decide which units to prune. The basic idea is that a parameter with a variance in sensitivity not significantly different from zero, is irrelevant and can be removed. Experimental results show that the new pruning algorithm correctly prunes irrelevant input and hidden units. The new pruning algorithm is also compared with standard pruning algorithms.","Analysis of variance,
Information analysis,
Computer architecture,
Neural networks,
Computer errors,
Sensitivity analysis,
Training data,
Computational complexity,
Computational efficiency,
Computer science"
Tuning RED for Web traffic,"We study the effects of RED on the performance of Web browsing with a novel aspect of our work being the use of a user-centric measure of performance: response time for HTTP request-response pairs. We empirically evaluate RED across a range of parameter settings and offered loads. Our results show that: (1) contrary to expectations, compared to an FIFO queue, RED has a minimal effect on HTTP response times for offered loads up to 90% of link capacity; (2) response times at loads in this range are not substantially affected by RED parameters; (3) between 90% and 100% load, RED can be carefully tuned to yield performance somewhat superior to FIFO, however, response times are quite sensitive to the actual RED parameter values selected; and (4) in such heavily congested networks, RED parameters that provide the best link utilization produce poorer response times. We conclude that for links carrying only Web traffic, RED queue management appears to provide no clear advantage over tail-drop FIFO for end-user response times.","Delay,
Traffic control,
Time measurement,
Computer science,
Internet,
Telecommunication traffic,
Particle measurements,
Communication system traffic control,
Technology management,
Resource management"
A statistical 3-D pattern processing method for computer-aided detection of polyps in CT colonography,"Adenomatous polyps in the colon are believed to be the precursor to colorectal carcinoma, the second leading cause of cancer deaths in United States. In this paper, we propose a new method for computer-aided detection of polyps in computed tomography (CT) colonography (virtual colonoscopy), a technique in which polyps are imaged along the wall of the air-inflated, cleansed colon with X-ray CT. Initial work with computer aided detection has shown high sensitivity, but at a cost of too many false positives. We present a statistical approach that uses support vector machines to distinguish the differentiating characteristics of polyps and healthy tissue, and uses this information for the classification of the new cases. One of the main contributions of the paper is the new three-dimensional pattern processing approach, called random orthogonal shape sections method, which combines the information from many random images to generate reliable signatures of shape. The input to the proposed system is a collection of volume data from candidate polyps obtained by a high-sensitivity, low-specificity system that we developed previously. The results of our tenfold cross-validation experiments show that, on the average, the system increases the specificity from 0.19 (0.35) to 0.69 (0.74) at a sensitivity level of 1.0 (0.95).","Colonic polyps,
Virtual colonoscopy,
Colonography,
Computed tomography,
Colon,
X-ray detection,
X-ray detectors,
X-ray imaging,
Shape,
Cancer"
Computer-aided characterization of mammographic masses: accuracy of mass segmentation and its effects on characterization,"Mass segmentation is used as the first step in many computer-aided diagnosis (CAD) systems for classification of breast masses as malignant or benign. The goal of this paper was to study the accuracy of an automated mass segmentation method developed in our laboratory, and to investigate the effect of the segmentation stage on the overall classification accuracy. The automated segmentation method was quantitatively compared with manual segmentation by two expert radiologists (R1 and R2) using three similarity or distance measures on a data set of 100 masses. The area overlap measures between R1 and R2, the computer and R1, and the computer and R2 were 0.76/spl plusmn/0.13,0.74 /spl plusmn/0.11, and 0.74/spl plusmn/0.13, respectively. The interobserver difference in these measures between the two radiologists was compared with the corresponding differences between the computer and the radiologists. Using three similarity measures and data from two radiologists, a total of six statistical tests were performed. The difference between the computer and the radiologist segmentation was significantly larger than the interobserver variability in only one test. Two sets of texture, morphological, and spiculation features, one based on the computer segmentation, and the other based on radiologist segmentation, were extracted from a data set of 249 films from 102 patients. A classifier based on stepwise feature selection and linear discriminant analysis was trained and tested using the two feature sets. The leave-one-case-out method was used for data sampling. For case-based classification, the area A/sub z/ under the receiver operating characteristic (ROC) curve was 0.89 and 0.88 for the feature sets based on the radiologist segmentation and computer segmentation, respectively. The difference between the two ROC curves was not statistically significant.","Testing,
Computer aided diagnosis,
Breast,
Cancer,
Laboratories,
Area measurement,
Performance evaluation,
Data mining,
Linear discriminant analysis,
Sampling methods"
On the notion of variability in software product lines,"The authors discuss the notion of variability. We have experienced that this concept has so far been underdefined, although we have observed that variability techniques become increasingly important. A clear indication of this trend is the recent emergence of software product lines. Software product lines are large, industrial software systems intended to specialize into specific software products. The authors provide a framework of terminology and concepts regarding variability. In addition, they present three recurring patterns of variability. Finally, they suggest a method for managing variability in software product lines.","Software systems,
Software engineering,
Runtime,
Delay effects,
Software reusability,
Mathematics,
Computer science,
Computer industry,
Software design,
Software architecture"
Finding failures by cluster analysis of execution profiles,"We experimentally evaluate the effectiveness of using cluster analysis of execution profiles to find failures among the executions induced by a set of potential test cases. We compare several filtering procedures for selecting executions to evaluate for conformance to requirements. Each filtering procedure involves a choice of a sampling strategy and a clustering metric. The results suggest that filtering procedures based on clustering are more effective than simple random sampling for identifying failures in populations of operational executions, with adaptive sampling from clusters being the most effective sampling strategy. The results also suggest that clustering metrics that give extra weight to industrial profile features are most effective. Scatter plots of execution populations, produced by multidimensional scaling, are used to provide intuition for these results.","Failure analysis,
Filtering,
Software testing,
Sampling methods,
Automatic testing,
Multidimensional systems,
Computer science,
Scattering,
Instruments,
Personnel"
On the convergence of the decomposition method for support vector machines,"The decomposition method is currently one of the major methods for solving support vector machines (SVM). Its convergence properties have not been fully understood. The general asymptotic convergence was first proposed by Chang et al. However, their working set selection does not coincide with existing implementation. A later breakthrough by Keerthi and Gilbert (2000, 2002) proved the convergence finite termination for practical cases while the size of the working set is restricted to two. In this paper, we prove the asymptotic convergence of the algorithm used by the software SVM/sup light/ and other later implementation. The size of the working set can be any even number. Extensions to other SVM formulations are also discussed.","Convergence,
Support vector machines,
Support vector machine classification,
Matrix decomposition,
Software algorithms,
Helium,
Upper bound,
Kernel,
Computer science"
On the behavior of information theoretic criteria for model order selection,"The Akaike (1974) information criterion (AIC) and the minimum description length (MDL) are two well-known criteria for model order selection in the additive white noise case. Our aim is to study the influence on their behavior of a large gap between the signal and the noise eigenvalues and of the noise eigenvalue dispersion. Our results are mostly qualitative and serve to explain the behavior of the AIC and the MDL in some cases of great practical importance. We show that when the noise eigenvalues are not clustered sufficiently closely, then the AIC and the MDL may lead to overmodeling by ignoring an arbitrarily large gap between the signal and the noise eigenvalues. For fixed number of data samples, overmodeling becomes more likely for increasing the dispersion of the noise eigenvalues. For fixed dispersion, overmodeling becomes more likely for increasing the number of data samples. Undermodeling may happen in the cases where the signal and the noise eigenvalues are not well separated and the noise eigenvalues are clustered sufficiently closely. We illustrate our results by using simulations from the effective channel order determination area.",
The case for resilient overlay networks,"This paper makes the case for Resilient Overlay Networks (RONs), an application-level routing and packet forwarding service that gives end-hosts and applications the ability to take advantage of network paths that traditional Internet routing cannot make use of, thereby improving their end-to-end reliability and performance. Using RON, nodes participating in a distributed Internet application configure themselves into an overlay network and cooperatively forward packets for each other. Each RON node monitors the quality of the links in the underlying Internet and propagates this information to the other nodes; this enables a RON to detect and react to path failures within several seconds rather than several minutes, and allows it to select application-specific paths based on performance. We argue that RON has the potential to substantially improve the resilience of distributed Internet applications to path outages and sustained overload.","Computer aided software engineering,
Peer to peer computing,
IP networks,
Web and internet services,
Routing protocols,
Telecommunication network reliability,
Laboratories,
Computer science,
Dissolved gas analysis,
Application software"
Speculative precomputation: long-range prefetching of delinquent loads,"This paper explores Speculative Precomputation, a technique that uses idle thread contexts in a multithreaded architecture to improve performance of single-threaded applications. It attacks program stalls from data cache misses by pre-computing future memory accesses in available thread contexts, and prefetching these data. This technique is evaluated by simulating the performance of a research processor based on the Itanium/sup TM/ ISA supporting Simultaneous Multithreading. Two primary forms of Speculative Precomputation are evaluated. If only the non-speculative thread spawns speculative threads, performance gains of up to 30% are achieved when assuming ideal hardware. However, this speedup drops considerably with more realistic hardware assumptions. Permitting speculative threads to directly spawn additional speculative threads reduces the overhead associated with spawning threads and enables significantly more aggressive speculation, overcoming this limitation. Even with realistic costs for spawning threads, speedups as high as 169% are achieved, with an average speedup of 76%.","Prefetching,
Yarn,
Bandwidth,
Hardware,
Surface-mount technology,
Computer science,
Multithreading,
Delay,
Microprocessors,
Microcomputers"
Almost difference sets and their sequences with optimal autocorrelation,"Almost difference sets have interesting applications in cryptography and coding theory. We give a well-rounded treatment of known families of almost difference sets, establish relations between some difference sets and some almost difference sets, and determine the numerical multiplier group of some families of almost difference sets. We also construct six new classes of almost difference sets, and four classes of binary sequences of period n/spl equiv/0 (mod 4) with optimal autocorrelation. We have also obtained two classes of relative difference sets and four classes of divisible difference sets (DDSs). We also point out that a result due to Jungnickel (1982) can be used to construct almost difference sets and sequences of period 4l with optimal autocorrelation.",
Efficiently mining maximal frequent itemsets,"We present GenMax, a backtracking search based algorithm for mining maximal frequent itemsets. GenMax uses a number of optimizations to prune the search space. It uses a novel technique called progressive focusing to perform maximality checking, and diffset propagation to perform fast frequency computation. Systematic experimental comparison with previous work indicates that different methods have varying strengths and weaknesses based on dataset characteristics. We found GenMax to be a highly efficient method to mine the exact set of maximal patterns.","Data mining,
Itemsets,
Frequency,
Computer science,
Association rules,
Transaction databases"
Robust super-resolution,"A robust approach for super-resolution is, presented, which is especially valuable in the presence of outliers. Such outliers may be due to motion errors, inaccurate blur models, noise, moving objects, motion blur etc. This robustness is needed since super-resolution methods are very sensitive to such errors. A robust median estimator is combined in an iterative process to achieve a super resolution algorithm. This process can increase resolution even in regions with outliers, where other super resolution methods actually degrade the image.","Image resolution,
Noise robustness,
High-resolution imaging,
Degradation,
Motion segmentation,
Computer vision,
Vectors,
Computer science,
Iterative algorithms,
Image reconstruction"
Adaptive postfiltering of transform coefficients for the reduction of blocking artifacts,"This paper proposes a novel postprocessing technique for reducing blocking artifacts in low-bit-rate transform-coded images. The proposed approach works in the transform domain to alleviate the accuracy loss of transform coefficients, which is introduced by the quantization process. The masking effect in the human visual system (HVS) is considered, and an adaptive weighting mechanism is then integrated into the postfiltering. In low-activity areas, since blocking artifacts appear to be perceptually more detectable, a large window is used to efficiently smooth out the artifacts. In order to preserve image details, a small mask, as well as a large central weight, is employed for processing those high-activity blocks, where blocking artifacts are less noticeable due to the masking ability of local background. The quantization constraint is finally applied to the postfiltered coefficients. Experimental results show that the proposed technique provides satisfactory performance as compared to other postfilters in both objective and subjective image quality.",
Incorporating varying test costs and fault severities into test case prioritization,"Test case prioritization techniques schedule test cases for regression testing in an order that increases their ability to meet some performance goal. One performance goal, rate of fault detection, measures how quickly faults are detected within the testing process. In previous work (S. Elbaum et al., 2000; G. Rothermel et al., 1999), we provided a metric, APFD, for measuring rate of fault detection, and techniques for prioritizing test cases to improve APFD, and reported the results of experiments using those techniques. This metric and these techniques, however, applied only in cases in which test costs and fault severity are uniform. We present a new metric for assessing the rate of fault detection of prioritized test cases that incorporates varying test case and fault costs. We present the results of a case study illustrating the application of the metric. This study raises several practical questions that might arise in applying test case prioritization; we discuss how practitioners could go about answering these questions.",
Hard real-time scheduling for low-energy using stochastic data and DVS processors,"Addresses scheduling for reduced energy of hard real-time tasks with fixed priorities assigned in a rate monotonic or deadline monotonic manner. The approach described can be exclusively implemented in the RTOS. It targets energy consumption reduction by using both on-line and off-line decisions, taken both at task level and at task-set level. We consider sets of independent tasks running on processors with dynamic voltage supplies (DVS). Taking into account the real behavior of a realtime system, which is often better than the worst case, our methods employ stochastic data to derive energy efficient schedules. The experimental results show that our approach achieves more important energy reductions than other policies from the same class.","Processor scheduling,
Stochastic processes,
Voltage control,
Energy consumption,
Dynamic voltage scaling,
Permission,
Computer science,
Stochastic systems,
Energy efficiency,
Digital systems"
Truthful mechanisms for one-parameter agents,"The authors show how to design truthful (dominant strategy) mechanisms for several combinatorial problems where each agent's secret data is naturally expressed by a single positive real number. The goal of the mechanisms we consider is to allocate loads placed on the agents, and an agent's secret data is the cost she incurs per unit load. We give an exact characterization for the algorithms that can be used to design truthful mechanisms for such load balancing problems using appropriate side payments. We use our characterization to design polynomial time truthful mechanisms for several problems in combinatorial optimization to which the celebrated VCG mechanism does not apply. For scheduling related parallel machines (Q/spl par/C/sub max/), we give a 3-approximation mechanism based on randomized rounding of the optimal fractional solution. This problem is NP-complete, and the standard approximation algorithms (greedy load-balancing or the PTAS) cannot be used in truthful mechanisms. We show our mechanism to be frugal, in that the total payment needed is only a logarithmic factor more than the actual costs incurred by the machines, unless one machine dominates the total processing power. We also give truthful mechanisms for maximum flow, Q/spl par//spl Sigma/C/sub j/ (scheduling related machines to minimize the sum of completion times), optimizing an affine function over a fixed set, and special cases of uncapacitated facility location. In addition, for Q/spl par//spl Sigma/w/sub j/C/sub j/ (minimizing the weighted sum of completion times), we prove a lower bound of 2//spl radic/3 for the best approximation ratio achievable by truthful mechanism.","Cost accounting,
Computer science,
Game theory,
Algorithm design and analysis,
Load management,
Polynomials,
Design optimization,
Approximation algorithms,
Power generation economics,
Operations research"
Three-dimensional texture analysis of MRI brain datasets,"A method is proposed for three-dimensional (3-D) texture analysis of magnetic resonance imaging brain datasets. It is based on extended, multisort co-occurrence matrices that employ intensity, gradient and anisotropy image features in a uniform way. Basic properties of matrices as well as their sensitivity and dependence on spatial image scaling are evaluated. The ability of the suggested 3-D texture descriptors is demonstrated on nontrivial classification tasks for pathologic findings in brain datasets.","Magnetic resonance imaging,
Image texture analysis,
Lesions,
Image edge detection,
Image analysis,
Diseases,
Magnetic analysis,
Anisotropic magnetoresistance,
Neuroscience,
Image recognition"
Interpolating implicit surfaces from scattered surface data using compactly supported radial basis functions,"Describes algebraic methods for creating implicit surfaces using linear combinations of radial basis interpolants to form complex models from scattered surface points. Shapes with arbitrary topology are easily represented without the usual interpolation or aliasing errors arising from discrete sampling. These methods were first applied to implicit surfaces by V.V. Savchenko, et al. (1995) and later developed independently by G. Turk and J.F. O'Brien (1998) as a means of performing shape interpolation. Earlier approaches were limited as a modeling mechanism because of the order of the computational complexity involved. We explore and extend these implicit interpolating methods to make them suitable for systems of large numbers of scattered surface points by using compactly supported radial basis interpolants. The use of compactly supported elements generates a sparse solution space, reducing the computational complexity and making the technique practical for large models. The local nature of compactly supported radial basis functions permits the use of computational techniques and data structures such as k-d trees for spatial subdivision, promoting fast solvers and methods to divide and conquer many of the subproblems associated with these methods. Moreover, the representation of complex models permits the exploration of diverse surface geometry. This reduction in computational complexity enables the application of these methods to the study of the shape properties of large, complex shapes.","Scattering,
Shape,
Computer science,
Interpolation,
Level set,
Sampling methods,
Computer graphics,
High performance computing,
Libraries,
Topology"
Using multiple hash functions to improve IP lookups,"High performance Internet routers require a mechanism for very efficient IP address lookups. Some techniques used to this end, such as binary search on levels, need to construct quickly a good hash table for the appropriate IP prefixes. We describe an approach for obtaining good hash tables based on using multiple hashes of each input key (which is an IP address). The methods we describe are fast, simple, scalable, parallelizable, and flexible. In particular, in instances where the goal is to have one hash bucket fit into a cache line, using multiple hashes proves extremely suitable. We provide a general analysis of this hashing technique and specifically discuss its application to binary search on levels.","Routing,
Internet,
Application software,
Hardware,
Filtering,
Algorithms,
Testing,
Databases,
Computer science,
Engineering profession"
Authenticity and integrity of digital mammography images,"Data security becomes more and more important in telemammography which uses a public high-speed wide area network connecting the examination site with the mammography expert center. Generally, security is characterized in terms of privacy, authenticity and integrity of digital data. Privacy is a network access issue and is not considered in this paper. The authors present a method, authenticity and integrity of digital mammography, here which can meet the requirements of authenticity and integrity for mammography image (IM) transmission. The authenticity and integrity for mammography (AIDM) consists of the following four modules. (1) Image preprocessing: To segment breast pixels from background and extract patient information from digital imaging and communication in medicine (DICOM) image header. (2) Image hashing: To compute an image hash value of the mammogram using the MD5 hash algorithm. (3) Data encryption: To produce a digital envelope containing the encrypted image hash value (digital signature) and corresponding patient information. (4) Data embedding: To embed the digital envelope into the image. This is done by replacing the least significant bit of a random pixel of the mammogram by one bit of the digital envelope bit stream and repeating for all bits in the bit stream. Experiments with digital IMs demonstrate the following. (1) In the expert center, only the user who knows the private key ran open the digital envelope and read the patient information data and the digital signature of the mammogram transmitted from the examination site. (2) Data integrity can he verified by matching the image hash value decrypted from the digital signature with that computed from the transmitted image. (3) No visual quality degradation is detected in the embedded image compared with the original. The authors' preliminary results demonstrate that AIDM is an effective method for image authenticity and integrity in telemammography application.",
Reducing the complexity of the register file in dynamic superscalar processors,"Dynamic superscalar processors execute multiple instructions out-of-order by looking for independent operations within a large window. The number of physical registers within the processor has a direct impact on the size of this window as most in-flight instructions require a new physical register at dispatch. A large multi-ported register file helps improve the instruction-level parallelism (ILP), but may have a detrimental effect on clock speed, especially in future wire-limited technologies. In this paper, we propose a register file organization that reduces register file size and port requirements for a given amount of ILP. We use a two-level register file organization to reduce register file size requirements, and a banked organization to reduce port requirements. We demonstrate empirically that the resulting register file organizations have reduced latency and (in the case of the banked organization) energy requirements for similar instructions per cycle (IPC) performance and improved instructions per second (IPS) performance in comparison to a conventional monolithic register file. The choice of organization is dependent on design goals.","Registers,
Clocks,
Delay effects,
Computer science,
Parallel processing,
Bandwidth,
Read-write memory,
Frequency,
Wire,
Multithreading"
"Fast EM-like methods for maximum ""a posteriori"" estimates in emission tomography","The maximum-likelihood (ML) approach in emission tomography provides images with superior noise characteristics compared to conventional filtered backprojection (FBP) algorithms. The expectation-maximization (EM) algorithm is an iterative algorithm for maximizing the Poisson likelihood in emission computed tomography that became very popular for solving the ML problem because of its attractive theoretical and practical properties. Recently, (Browne and DePierro, 1996 and Hudson and Larkin, 1991) block sequential versions of the EM algorithm that take advantage of the scanner's geometry have been proposed in order to accelerate its convergence. In Hudson and Larkin, 1991, the ordered subsets EM (OS-EM) method was applied to the hit problem and a modification (OS-GP) to the maximum a posteriori (MAP) regularized approach without showing convergence. In Browne and DePierro, 1996, we presented a relaxed version of OS-EM. (RAMLA) that converges to an ML solution. In this paper, we present an extension of RAMLA for MAP reconstruction. We show that, if the sequence generated by this method converges, then it must converge to the true MAP solution. Experimental evidence of this convergence is also shown. To illustrate this behavior we apply the algorithm to positron emission tomography simulated data comparing its performance to OS-GP.","Maximum a posteriori estimation,
Iterative algorithms,
Positron emission tomography,
Maximum likelihood estimation,
Image reconstruction,
Computed tomography,
Convergence,
Single photon emission computed tomography,
Electrical capacitance tomography,
Acceleration"
Comparison of 3-D reconstruction with 3D-OSEM and with FORE+OSEM for PET,"The combination of Fourier rebinning (FORE) and the ordered subsets expectation-maximization (OSEM), a fast statistical algorithm, appears as a promising alternative to the fully three-dimensional (3-D) iterative approach for clinical positron emission tomography (PET) data. Here, the authors evaluated the properties of FORE+OSEM and compared it with fully 3-D OSEM using both simulations and data acquired by commercial scanners. The aim is to determine to what extent the speed advantage of FORE+OSEM is paid for by a possible degradation of image quality in the case of noisy clinical PET data. A forward- and back-projection pair based on a line integral model was used in two-dimensional OSEM and 3-D OSEM (3D-OSEM) instead of a system matrix. Different variants of both approaches have been studied with simulations in terms of contrast-noise tradeoff. Two variants-FORE+OSEM with attenuation weighting (AW) [FORE+OSEM(AW)] and 3D-OSEM with attenuation-normalization weighting (ANSP) and a shifted-Poisson (SP) model [3D-OSEM(ANSP)]-were compared with measured phantom data and patient data. Based on the results from both simulations and measured data, the authors conclude that: 1) both attenuation (-normalization) weighting and the SP model improve the image quality but slow down the convergence and 2) despite its approximate nature, FORE+OSEM does not show apparent image degradation compared with 3D-OSEM for data with a noise level typical of a whole-body FDG scan.","Three dimensional displays,
Positron emission tomography,
Degradation,
Image quality,
Attenuation measurement,
Iterative algorithms,
Iterative methods,
Imaging phantoms,
Noise measurement,
Convergence"
Dynamic textures,"Dynamic textures are sequences of images of moving scenes that exhibit certain stationarity properties in time; these include sea-waves, smoke, foliage, whirlwind but also talking faces, traffic scenes etc. We present a novel characterization of dynamic textures that poses the problems of modelling, learning, recognizing and synthesizing dynamic textures on a firm analytical footing. We borrow tools from system identification to capture the ""essence"" of dynamic textures; we do so by learning (i.e. identifying) models that are optimal in the sense of maximum likelihood or minimum prediction error variance. For the special case of second-order stationary processes we identify the model in closed form. Once learned, a model has predictive power and can be used for extrapolating synthetic sequences to infinite length with negligible computational cost. We present experimental evidence that, within our framework, even low dimensional models can capture very complex visual phenomena.",
Microwave image reconstruction utilizing log-magnitude and unwrapped phase to improve high-contrast object recovery,"Reconstructing images of large high-contrast objects with microwave methods has proved difficult. Successful images have generally been obtained by using a priori information to constrain the image reconstruction to recover the correct electromagnetic property distribution. In these situations, the measured electric field phases as a function of receiver position around the periphery of the imaging field-of-view vary rapidly often undergoing changes of greater than /spl pi/ radians especially when the object contrast and illumination frequency increase. Here, the authors introduce a modified form of a Maxwell equation model-based image reconstruction algorithm which directly incorporates log-magnitude and phase of the measured electric field data. By doing so, measured phase variation can be unwrapped and distributed over more than one Rieman sheet in the complex plane. Simulation studies and microwave imaging experiments demonstrate that significant image quality enhancements occur with this approach for large high-contrast objects. Simple strategies for visualizing and unwrapping phase values as a function of the transmitter and receiver positions within our microwave imaging array are described. Metrics of the degree of phase variation expressed in terms of the amount and extent of phase wrapping are defined and found to be figures-of-merit which estimate when it is critical to deploy the new image reconstruction approach. In these cases, the new algorithm recovers high-quality images without resorting to the use of a priori information on object contrast and/or size as previously required.","Image reconstruction,
Phase measurement,
Microwave imaging,
Electric variables measurement,
Phased arrays,
Microwave theory and techniques,
Frequency measurement,
Position measurement,
Electromagnetic measurements,
Lighting"
Analysis of asymmetry in mammograms via directional filtering with Gabor wavelets,"This paper presents a procedure for the analysis of left-right (bilateral) asymmetry in mammograms. The procedure is based upon the detection of linear directional components by using a multiresolution representation based upon Gabor wavelets. A particular wavelet scheme with two-dimensional Gabor filters as elementary functions with varying tuning frequency and orientation, specifically designed in order to reduce the redundancy in the wavelet-based representation, is applied to the given image. The filter responses for different scales and orientation are analyzed by using the Karhunen-Loeve (KL) transform and Otsu's method of thresholding. The KL transform is applied to select the principal components of the filter responses, preserving only the most relevant directional elements appearing at all scales. The selected principal components, thresholded by using Otsu's method, are used to obtain the magnitude and phase of the directional components of the image. Rose diagrams computed from the phase images and statistical measures computed thereof are used for quantitative and qualitative analysis of the oriented patterns. A total of 80 images from 20 normal cases, 14 asymmetric cases, and six architectural distortion cases from the Mini-MIAS (Mammographic Image Analysis Society, London, U.K.) database were used to evaluate the scheme using the leave-one-out methodology. Average classification accuracy rates of up to 74.4% were achieved.",
Addendum: B-spline interpolation in medical image processing,"Analyzes B-spline interpolation techniques of degree 2, 4, and 5 with respect to all criteria that have been applied to evaluate various interpolation schemes in a recently published survey on image interpolation in medical imaging (Lehmann et al., 1999). It is shown that high-degree B-spline interpolation has superior Fourier properties, smallest interpolation error, and reasonable computing times. Therefore, high-degree B-splines are preferable interpolators for numerous applications in medical image processing, particularly if high precision is required. If no aliasing occurs, this result neither depends on the geometric transform applied for the tests nor the actual content of images.","Spline,
Interpolation,
Biomedical image processing,
Kernel,
Fourier transforms,
Image analysis,
Biomedical imaging,
Biomedical informatics,
Polynomials,
Digital filters"
Dynamical cognitive network - an extension of fuzzy cognitive map,"We present the dynamic cognitive network (DCN) which is an extension of the fuzzy cognitive map (FCM). Each concept in the DCNs can have its own value set, depending on how precisely it needs to be described in the network. This enables the DCN to describe the strength of causes and the degree of effects that are crucial to conducting meaningful inferences. The arcs in the DCN define dynamic, causal relationships between concepts. Structurally, DNCs are scalable and more flexible as compared to FCMs. A DCN can be as simple as a cognitive map and FCM, or as complex as a nonlinear dynamic system. To demonstrate the potential applications of DCNs, we present some simulation results. This paper represents our first attempt to develop a dynamic fuzzy inference system using causal relationships. There are many interesting and challenging theoretical and practical issues in DCNs open to further research.",
Learning inverse kinematics,"Real-time control of the end-effector of a humanoid robot in external coordinates requires computationally efficient solutions of the inverse kinematics problem. In this context, this paper investigates inverse kinematics learning for resolved motion rate control (RMRC) employing an optimization criterion to resolve kinematic redundancies. Our learning approach is based on the key observations that learning an inverse of a nonuniquely invertible function can be accomplished by augmenting the input representation to the inverse model and by using a spatially localized learning approach. We apply this strategy to inverse kinematics learning and demonstrate how a recently developed statistical learning algorithm, locally weighted projection regression, allows efficient learning of inverse kinematic mappings in an incremental fashion even when input spaces become rather high dimensional. Our results are illustrated with a 30-DOF humanoid robot.","Humanoid robots,
Manipulators,
Spatial resolution,
Motion control,
Robot kinematics,
Constraint optimization,
Computer science,
Neuroscience,
Inverse problems,
Statistical learning"
Scaling up fast evolutionary programming with cooperative coevolution,"Evolutionary programming (EP) has been applied with success to many numerical and combinatorial optimization problems in recent years. However, most analytical and experimental results on EP have been obtained using low-dimensional problems. It is interesting to know whether the empirical results obtained from the low-dimensional problems still hold for high-dimensional cases. It was discovered that neither classical EP (CEP) nor fast EP (FEP) performed satisfactorily for some large-scale problems. The paper shows empirically that FEP with cooperative coevolution (FEPCC) can speed up convergence rates on the large-scale problems whose dimension ranges from 100 to 1000. Cooperative coevolution adopts the divide-and-conquer strategy. It divides the system into many modules, and evolves each module separately and cooperatively. The results of FEPCC on the problems investigated here are something of a surprise. The time used by FEPCC to find a near optimal solution appears to scale linearly; that is, the time used seems to go up linearly as the dimensionality of the problems studied increases.",
Fast block matching algorithm based on the winner-update strategy,"Block matching is a widely used method for stereo vision, visual tracking, and video compression. Many fast algorithms for block matching have been proposed in the past, but most of them do not guarantee that the match found is the globally optimal match in a search range. This paper presents a new fast algorithm based on the winner-update strategy which utilizes an ascending lower bound list of the matching error to determine the temporary winner. Two lower bound lists derived by using partial distance and by using Minkowski's inequality are described. The basic idea of the winner-update strategy is to avoid, at each search position, the costly computation of the matching error when there exists a lower bound larger than the global minimum matching error. The proposed algorithm can significantly speed up the computation of the block matching because (1) computational cost of the lower bound we use is less than that of the matching error itself; (2) an element in the ascending lower bound list will be calculated only when its preceding element has already been smaller than the minimum matching error computed so far; (3) for many search positions, only the first several lower bounds in the list need to be calculated. Our experiments have shown that, when applying to motion vector estimation for several widely-used test videos, 92% to 98% of operations can be saved while still guaranteeing the global optimality. Moreover, the proposed algorithm can be easily modified either to meet the limited time requirement or to provide an ordered list of best candidate matches.","Motion estimation,
Video compression,
Stereo vision,
Information science,
Computer science,
Pixel,
Optimal matching,
Computational efficiency,
Testing,
HTML"
Tools for 3D-object retrieval: Karhunen-Loeve transform and spherical harmonics,"We present tools for 3D object retrieval in which a model, a polygonal mesh, serves as a query and similar objects are retrieved from a collection of 3D objects. Algorithms proceed first by a normalization step (pose estimation) in which models are transformed into a canonical coordinate frame. Second, feature vectors are extracted and compared with those derived from normalized models in the search space. Using a metric in the feature vector space nearest neighbors are computed and ranked. Objects thus retrieved are displayed for inspection, selection, and processing. For the pose estimation we introduce a modified Karhunen-Loeve transform that takes into account not only vertices or polygon centroids from the 3D models but all points in the polygons of the objects. Some feature vectors can be regarded as samples of functions on the 2-sphere. We use Fourier expansions of these functions as uniform representations allowing embedded multi-resolution feature vectors. Our implementation demonstrates and visualizes these tools.","Karhunen-Loeve transforms,
Content based retrieval,
Nearest neighbor searches,
Image retrieval,
Computer science,
Feature extraction,
Inspection,
Visualization,
Image databases,
Spatial databases"
Industrial applications of soft computing: a review,"Fuzzy logic, neural networks, and evolutionary computation are the core methodologies of soft computing (SC). SC is causing a paradigm shift in engineering and science fields since it can solve problems that have not been able to be solved by traditional analytic methods. In addition, SC yields rich knowledge representation, flexible knowledge acquisition, and flexible knowledge processing, which enable intelligent systems to be constructed at low cost. This paper reviews applications of SC in several industrial fields to show the various innovations by TR, HMIQ, and low cost in industries that have been made possible by the use of SC. Our paper intends to remove the gap between theory and practice and attempts to learn how to apply soft computing practically to industrial systems from examples/analogy, reviewing many application papers.",
Identification of high-level concept clones in source code,"Source code duplication occurs frequently within large software systems. Pieces of source code, functions, and data types are often duplicated in part or in whole, for a variety of reasons. Programmers may simply be reusing a piece of code via copy and paste or they may be ""re-inventing the wheel"". Previous research on the detection of clones is mainly focused on identifying pieces of code with similar (or nearly similar) structure. Our approach is to examine the source code text (comments and identifiers) and identify implementations of similar high-level concepts (e.g., abstract data types). The approach uses an information retrieval technique (i.e., latent semantic indexing) to statically analyze the software system and determine semantic similarities between source code documents (i.e., functions, files, or code segments). These similarity measures are used to drive the clone detection process. The intention of our approach is to enhance and augment existing clone detection methods that are based on structural analysis. This synergistic use of methods will improve the quality of clone detection. A set of experiments is presented that demonstrate the usage of semantic similarity measure to identify clones within a version of NCSA Mosaic.",
Supporting program comprehension using semantic and structural information,"Focuses on investigating the combined use of semantic and structural information of programs to support the comprehension tasks involved in the maintenance and reengineering of software systems. ""Semantic information"" refers to the domain-specific issues (both the problem and the development domains) of a software system. The other dimension, structural information, refers to issues such as the actual syntactic structure of the program, along with the control and data flow that it represents. An advanced information retrieval method, latent semantic indexing, is used to define a semantic similarity measure between software components. Components within a software system are then clustered together using this similarity measure. Simple structural information (i.e. the file organization) of the software system is then used to assess the semantic cohesion of the clusters and files with respect to each other. The measures are formally defined for general application. A set of experiments is presented which demonstrates how these measures can assist in the understanding of a nontrivial software system, namely a version of NCSA Mosaic.","Information retrieval,
Software systems,
Data mining,
Indexing,
Software measurement,
Documentation,
Natural languages,
Computer science,
Application software,
Computer languages"
LIME: a middleware for physical and logical mobility,"LIME is a middleware supporting the development of applications that exhibit physical mobility of hosts, logical mobility of agents, or both. LIME adapts a coordination perspective inspired by work on the Linda model. The context for computation, represented in Linda by a globally accessible, persistent tuple space, is represented in LIME by transient sharing of the tuple spaces carried by each individual mobile unit. Linda tuple spaces are also extended with a notion of location and with the ability to react to a given state. The hypothesis underlying our work is that the resulting model provides a minimalist set of abstractions that enable rapid and dependable development of mobile applications. In this paper, we illustrate the model underlying LIME, present its current design and implementation, and discuss initial lessons learned in developing applications that involve physical mobility.","Middleware,
Computer science,
Mobile communication,
Drives,
Programming profession,
Virtual machining,
Application software,
Investments,
Fabrics,
Data structures"
Information seeking in social context: structural influences and receipt of information benefits,"Research in the information processing, situated learning and social network traditions has consistently demonstrated the importance of social networks for acquiring information. However, we know little about how organizational relationships established by a relative position in a formal structure or social relationships established by interpersonal processes influence who is sought out for various kinds of information. Prior research suggests that people often receive some combination of five benefits when seeking information from other people: (1) solutions; (2) meta-knowledge (pointers to databases or people); (3) problem reformulation; (4) validation of plans or solutions; and (5) legitimation from contact with a respected person. This research builds on that work by assessing the influence of organizational and social structures (such as similarity of job function, hierarchy, task interdependence, physical proximity, influence, trust, friendship and gender) on receipt of these benefits from other people in a physically distributed organization. Task interdependence is the strongest and most consistent predictor of information seeking. However, social relations also affect the receipt of informational benefits, especially as they become more representational and affective. Implications are suggested for the study of social capital, computer-mediated communication and organizational learning.",
Tracking and modeling non-rigid objects with rank constraints,"This paper presents a novel solution for flow-based tracking and 3D reconstruction of deforming objects in monocular image sequences. A non-rigid 3D object undergoing rotation and deformation can be effectively approximated using a linear combination of 3D basis shapes. This puts a bound on the rank of the tracking matrix. The rank constraint is used to achieve robust and precise low-level optical flow estimation without prior knowledge of the 3D shape of the object. The bound on the rank is also exploited to handle occlusion at the tracking level leading to the possibility of recovering the complete trajectories of occluded/disoccluded points. Following the same low-rank principle, the resulting flow matrix can be factored to get the 3D pose, configuration coefficients, and 3D basis shapes. The flow matrix is factored in an iterative manner, looping between solving for pose, configuration, and basis shapes. The flow-based tracking is applied to several video sequences and provides the input to the 3D non-rigid reconstruction task. Additional results on synthetic data and comparisons to ground truth complete the experiments.","Shape,
Tracking,
Humans,
Optical noise,
Deformable models,
Robustness,
Video sequences,
Image motion analysis,
Principal component analysis,
Computer science"
G-commerce: market formulations controlling resource allocation on the computational grid,"In this paper we investigate G-commerce-computational economies for controlling resource allocation in Computational Grid settings. We define hypothetical resource consumers (representing users and Grid-aware applications) and resource producers (representing resource owners who ""sell"" their resources to the Grid). We then measure the efficiency of resource allocation under two different market conditions: commodities markets and auctions. We compare both market strategies in terms of price stability, market equilibrium, consumer efficiency, and producer efficiency. Our results indicate that commodities markets are a better choice for controlling Grid resources than previously defined auction strategies.",
Information-theoretic measures for anomaly detection,"Anomaly detection is an essential component of protection mechanisms against novel attacks. We propose to use several information-theoretic measures, namely, entropy, conditional entropy, relative conditional entropy, information gain, and information cost for anomaly detection. These measures can be used to describe the characteristics of an audit data set, suggest the appropriate anomaly detection model(s) to be built, and explain the performance of the model(s). We use case studies on Unix system call data, BSM data, and network tcpdump data to illustrate the utilities of these measures.","Intrusion detection,
Entropy,
Computer science,
Costs,
Data security,
Information security,
Data analysis,
Information analysis,
Pattern matching,
Detectors"
Linear image coding for regression and classification using the tensor-rank principle,"Given a collection of images (matrices) representing a ""class"" of objects we present a method for extracting the commonalities of the image space directly from the matrix representations (rather than from the vectorized representation which one would normally do in a PCA approach, for example). The general idea is to consider the collection of matrices as a tensor and to look for an approximation of its tensor-rank. The tensor-rank approximation is designed such that the SVD decomposition emerges in the special case where all the input matrices are the repeatition of a single matrix. We evaluate the coding technique both in terms of regression, i.e., the efficiency of the technique for functional approximation, and classification. We find that for regression the tensor-rank coding, as a dimensionality reduction technique, significantly outperforms other techniques like PCA. As for classification, the tensor-rank coding is at is best when the number of training examples is very small.","Image coding,
Principal component analysis,
Matrix decomposition,
Computer science,
Face recognition,
Image recognition,
Independent component analysis,
Tensile stress,
Decorrelation,
Computer vision"
Web engineering: an introduction,"Within a short period, the Internet and World Wide Web have become ubiquitous, surpassing all other technological developments in our history. They've also grown rapidly in their scope and extent of use, significantly affecting all aspects of our lives. Industries such as manufacturing, travel and hospitality, banking, education, and government are Web-enabled to improve and enhance their operations. E-commerce has expanded quickly, cutting across national boundaries. Even traditional legacy information and database systems have migrated to the Web. Advances in wireless technologies and Web-enabled appliances are triggering a new wave of mobile Web applications. As a result, we increasingly depend on a range of Web applications. Now that many of us rely on Web based systems and applications, they need to be reliable and perform well. To build these systems and applications, Web developers need a sound methodology, a disciplined and repeatable process, better development tools, and a set of good guidelines. The emerging field of Web engineering fulfils these needs. It uses scientific, engineering, and management principles and systematic approaches to successfully develop, deploy, and maintain high-quality Web systems and applications. It aims to bring the current chaos in Web based system development under control, minimize risks, and enhance Web site maintainability and quality.","Acoustical engineering,
Maintenance engineering,
Reliability engineering,
Internet,
Web sites,
History,
Manufacturing industries,
Banking,
Government,
Database systems"
Anonymous Gossip: improving multicast reliability in mobile ad-hoc networks,"In recent years, a number of applications of ad-hoc networks have been proposed. Many of them are based on the availability of a robust and reliable multicast protocol. We address the issue of reliability and propose a scalable method to improve packet delivery of multicast routing protocols and decrease the variation in the number of packets received by different nodes. The proposed protocol works in two phases. In the first phase, any suitable protocol is used to multicast a message to the group, while in the second concurrent phase, the gossip protocol tries to recover lost messages. Our proposed gossip protocol is called Anonymous Gossip (AG) since nodes need not know the other group members for gossip to be successful. This is extremely desirable for mobile nodes, that have limited resources, and where the knowledge of group membership is difficult to obtain. As a first step, anonymous gossip is implemented over MAODV without much overhead and its performance is studied. Simulations show that the packet delivery of MAODV is significantly improved and the variation in number of packets delivered is decreased.","Intelligent networks,
Ad hoc networks,
Multicast protocols,
Peer to peer computing,
Routing protocols,
Computer network reliability,
Computer science,
Application software,
Availability,
Robustness"
Computer-assisted bone age assessment: image preprocessing and epiphyseal/metaphyseal ROI extraction,"Clinical assessment of skeletal maturity is based on a visual comparison of a left-hand wrist radiograph with atlas patterns. Using a new digital hand atlas an image analysis methodology is being developed. To assist radiologists in bone age estimation. The analysis starts with a preprocessing function yielding epiphyseal/metaphyseal regions of interest (EMROIs). Then, these regions are subjected to a feature extraction function. Accuracy has been measured independently at three stages of the image analysis: detection of phalangeal tip, extraction of the EMROIs, and location of diameters and lower edge of the EMROIs. Extracted features describe the stage of skeletal development more objectively than visual comparison.","Bones,
Image edge detection,
Wrist,
Radiography,
Radiology,
Feature extraction,
Pediatrics,
Hospitals,
Computer aided diagnosis,
Image processing"
Lambertian reflectance and linear subspaces,"We prove that the set of all reflectance functions (the mapping from surface normals to intensities) produced by Lambertian objects under distant, isotropic lighting lies close to a 9D linear subspace. This implies that the images of a convex Lambertian object obtained under a wide variety of lighting conditions can be approximated accurately with a low-dimensional linear subspace, explaining prior empirical results. We also provide a simple analytic characterization of this linear space. We obtain these results by representing lighting using spherical harmonics and describing the effects of Lambertian materials as the analog of a convolution. These results allow us to construct algorithms for object recognition based on linear methods as well as algorithms that use convex optimization to enforce non-negative lighting functions.","Reflectivity,
National electric code,
Convolution,
Object recognition,
Algorithm design and analysis,
Power harmonic filters,
Kernel,
Jacobian matrices,
Computer science,
Optimization methods"
Hierarchical motion history images for recognizing human motion,"There has been increasing interest in computer analysis and recognition of human motion. Previously we presented an efficient real-time approach for representing human motion using a compact ""motion history image"" (MHI). Recognition was achieved by statistically matching moment-based features. To address previous problems related to global analysis and limited recognition, we present a hierarchical extension to the original MHI framework to compute dense (local) motion flow directly from the MHI. A hierarchical partitioning of motions by speed in an MHI pyramid enables efficient calculation of image motions using fixed-size gradient operators. To characterize the resulting motion field, a polar histogram of motion orientations is described. The hierarchical MHI approach remains a computationally inexpensive method for analysis of human motions.","History,
Image recognition,
Humans,
Motion analysis,
Image motion analysis,
Histograms,
Information science,
Cognitive science,
Information analysis,
Tracking"
A new ultralight anthropomorphic hand,In this paper a very lightweight artificial hand is presented that approximates the manipulation abilities of a human hand very well. A large variety of different objects can be grasped reliably and the movements of the hand appear to be very natural. This five finger hand has 13 independent degrees of freedom driven by a new type of powerful small size flexible fluidic actuator. The actuators are completely integrated in the fingers which made possible the design of a very compact and lightweight hand that can either be used as a prosthetic hand or as a humanoid robot hand. A mathematical model for the expansion of a flexible fluidic actuator is given and the mechanical construction and features of the new anthropomorphic hand are illustrated.,"Anthropomorphism,
Prosthetic hand,
Actuators,
Humans,
Fingers,
Robots,
Thumb,
Shape,
Muscles,
Computer science"
Limitations of a class of stabilization methods for delay systems,"We investigate limitations of certain stabilization methods for time-delay systems. The class of methods under consideration implements the control law through a Volterra integral equation of the second kind. Using as an example the pole placement approach of Manitius and Olbrot (1979), we illustrate how instability of the difference part of the control law leads to instability in the closed-loop system, in the case that implementation is done via numerical quadrature. The outcome of our analysis provides computable limitations to stability and a maximum allowable size of the (input) delay.","Delay systems,
Control systems,
Integral equations,
Robustness,
Computer science,
Differential equations,
State feedback,
Stability analysis,
Input variables,
Feedback control"
GestureWrist and GesturePad: unobtrusive wearable interaction devices,"In this paper we introduce two input devices for wearable computers, called GestureWrist and GesturePad. Both devices allow users to interact with wearable or nearby computers by using gesture-based commands. Both are designed to be as unobtrusive as possible, so they can be used under various social contexts. The first device, called GestureWrist, is a wristband-type input device that recognizes hand gestures and forearm movements. Unlike DataGloves or other hand gesture-input devices, all sensing elements are embedded in a normal wristband. The second device, called GesturePad, is a sensing module that can be attached on the inside of clothes, and users can interact with this module from the outside. It transforms conventional clothes into an interactive device without changing their appearance.",
Comparison between Genetic Network Programming (GNP) and Genetic Programming (GP),"Recently, many methods of evolutionary computation such as genetic algorithm (GA) and genetic programming (GP) have been developed as a basic tool for modeling and optimizing of complex systems. Generally speaking, GA has the genome of a string structure, while the genome in GP is the tree structure. Therefore, GP is suitable for constructing complicated programs, which can be applied to many real world problems. However, GP might sometimes be difficult to search for a solution because of its bloat. A novel evolutionary method named Genetic Network Programming (GNP), whose genome is a network structure is proposed to overcome the low searching efficiency of GP and is applied to the problem of the evolution of ant behavior in order to study the effectiveness of GNP. In addition, the comparison of the performances between GNP and GP is carried out in simulations on ant behaviors.","Genetic programming,
Economic indicators,
Genomics,
Bioinformatics,
Optimization methods,
Tree data structures,
Symbiosis,
Computational intelligence,
Computer networks,
Parallel algorithms"
Randomized path planning for linkages with closed kinematic chains,"We extend randomized path planning algorithms to the case of articulated robots that have closed kinematic chains. This is an important class of problems, which includes applications such as manipulation planning using multiple open-chain manipulators that cooperatively grasp an object and planning for reconfigurable robots in which links might be arranged in a loop to ease manipulation or locomotion. Applications also exist in areas beyond robotics, including computer graphics, computational chemistry, and virtual prototyping. Such applications typically involve high degrees of freedom and a parameterization of the configurations that satisfy closure constraints is usually not available. We show how to implement key primitive operations of randomized path planners for general closed kinematics chains. These primitives include the generation of random free configurations and the generation of local paths. To demonstrate the feasibility of our primitives for general chains, we show their application to recently developed randomized planners and present computed results for high-dimensional problems.",
Relaxation on a mesh: a formalism for generalized localization,"This paper considers two problems which at first sight appear to be quite distinct: localizing a robot in an unknown environment and calibrating an embedded sensor network. We show that both of these can be formulated as special cases of a generalized localization problem. In the standard localization problem, the aim is to determine the pose of some object (usually a mobile robot) relative to a global coordinate system. In our generalized version, the aim is to determine the pose of all elements in a network (both fixed and mobile) relative to an arbitrary global coordinate system. We have developed a physically inspired 'mesh-based' formalism for solving such problems. This paper outlines the formalism, and describes its application to the concrete tasks of multirobot mapping and calibration of a distributed sensor network. The paper presents experimental results for both tasks obtained using a set of Pioneer mobile robots equipped with scanning laser range-finders.","Springs,
Mobile robots,
Robot sensing systems,
Robot kinematics,
Actuators,
Motion measurement,
Computer science,
Concrete,
Calibration,
Sensor phenomena and characterization"
Topology free hidden Markov models: application to background modeling,"Hidden Markov models (HMMs) are increasingly being used in computer vision for applications such as: gesture analysis, action recognition from video, and illumination modeling. Their use involves an off-line learning step that is used as a basis for on-line decision making (i.e. a stationarity assumption on the model parameters). But, real-world applications are often non-stationary in nature. This leads to the need for a dynamic mechanism to learn and update the model topology as well as its parameters. This paper presents a new framework for HMM topology and parameter estimation in an online, dynamic fashion. The topology and parameter estimation is posed as a model selection problem with an MDL prior. Online modifications to the topology are made possible by incorporating a state splitting criterion. To demonstrate the potential of the algorithm, the background modeling problem is considered. Theoretical validation and real experiments are presented.","Hidden Markov models,
Topology,
Application software,
Parameter estimation,
Signal processing algorithms,
State estimation,
Visualization,
Computer science,
Computer vision,
Image analysis"
Optimal line-sweep-based decompositions for coverage algorithms,"Robotic coverage is the problem of moving a sensor or actuator over all points in given region. Ultimately, we want a coverage path that minimizes some cost such as time. We take the approach of decomposing the coverage region into subregions, selecting a sequence of those subregions, and then generating a path that covers each subregion in turn. We focus on generating decompositions based upon the planar line sweep. After a general overview of the coverage problem, we describe how our assumptions lead to the optimality criterion of minimizing the sum of subregion altitudes (which are measured relative to the sweep direction assigned to that subregion). For a line-sweep decomposition, the sweep direction is the same for all subregions. We describe how to find the optimal sweep direction for convex polygonal worlds. We then introduce the minimal sum of altitudes (MSA) decomposition in which we may assign a different sweep direction to each subregion. This decomposition is better for generating an optimal coverage path. We describe a method based on multiple line sweeps and dynamic programming to generate the MSA decomposition.","Robot sensing systems,
Costs,
Service robots,
Landmine detection,
Snow,
Computer science,
Inspection,
Actuators,
Spraying,
Coatings"
Automatic segmentation of subcortical brain structures in MR images using information fusion,"Reports a new automated method for the segmentation of internal cerebral structures using an information fusion technique. The information is provided both by images and expert knowledge, and consists in morphological, topological, and tissue constitution data. All this ambiguous, complementary and redundant information is managed using a three-step fusion scheme based on fuzzy logic. The information is first modeled into a common theoretical frame managing its imprecision and incertitude. The models are then fused and a decision is taken in order to reduce the imprecision and to increase the certainty in the location of the structures. The whole process is illustrated on the segmentation of thalamus, putamen, and head of the caudate nucleus from expert knowledge and magnetic resonance images, in a protocol involving 14 healthy volunteers. The quantitative validation is achieved by comparing computed, manually segmented structures and published data by means of indexes assessing the accuracy of volume estimation and spatial location. Results suggest a consistent volume estimation with respect to the expert quantification and published data, and a high spatial similarity of the segmented and computed structures. This method is generic and applicable to any structure that can be defined by expert knowledge and morphological images.","Image segmentation,
Brain,
Fuzzy logic,
Constitution,
Magnetic resonance,
Hippocampus,
Biomedical imaging,
Information management,
Magnetic heads,
Protocols"
Three-party encrypted key exchange without server public-keys,"Three-party key-exchange protocols with password authentication-clients share an easy-to-remember password with a trusted server only-are very suitable for applications requiring secure communications between many light-weight clients (end users); it is simply impractical that every two clients share a common secret. Steiner, Tsudik and Waidner (1995) proposed a realization of such a three-party protocol based on the encrypted key exchange (EKE) protocols. However, their protocol was later demonstrated to be vulnerable to off-line and undetectable on-line guessing attacks. Lin, Sun and Hwang (see ACM Operating Syst. Rev., vol.34, no. 4, p.12-20, 2000) proposed a secure three-party protocol with server public-keys. However, the approach of using server public-keys is not always a satisfactory solution and is impractical for some environments. We propose a secure three-party EKE protocol without server public-keys.",
How to go beyond the black-box simulation barrier,"The simulation paradigm is central to cryptography. A simulator is an algorithm that tries to simulate the interaction of the adversary with an honest party, without knowing the private input of this honest party. Almost all known simulators use the adversary's algorithm as a black-box. We present the first constructions of non-black-box simulators. Using these new non-black-box techniques, we obtain several results that were previously proven to be impossible to obtain using black-box simulators. Specifically, assuming the existence of collision resistent hash functions, we construct a new zero-knowledge argument system for NP that satisfies the following properties: 1. This system has a constant number of rounds with negligible soundness error. 2. It remains zero knowledge even when composed concurrently n times, where n is the security parameter. Simultaneously obtaining 1 and 2 has been recently proven to be impossible to achieve using black-box simulators. 3. It is an Arthur-Merlin (public coins) protocol. Simultaneously obtaining 1 and 3 was known to be impossible to achieve with a black-box simulator. 4. It has a simulator that runs in strict polynomial time, rather than in expected polynomial time. All previously known constant-round, negligible-error zero-knowledge arguments utilized expected polynomial-time simulators.","Computational modeling,
Polynomials,
Computer science,
Computer simulation,
Cryptography,
Security,
Knowledge management,
Access protocols"
Energy efficient fixed-priority scheduling for real-time systems on variable voltage processors,"Energy consumption has become an increasingly important consideration in designing many real-time embedded systems. Variable voltage processors, if used properly, can dramatically reduce such system energy consumption. In this paper, we present a technique to determine voltage settings for a variable voltage processor that utilizes a fixed priority assignment to schedule jobs. Our approach also produces the minimum constant voltage needed to feasibly schedule the entire job set. Our algorithms lead to significant energy saving compared with previously presented approaches.",
A cooperative search framework for distributed agents,"This paper presents an approach for cooperative search of a team of distributed agents. We consider two or more agents, or vehicles, moving in a geographic environment, searching for targets of interest and avoiding obstacles or threats. The moving agents are equipped with sensors to view a limited region of the environment they are visiting, and are able to communicate with one another to enable cooperation. The agents are assumed to have some ""physical"" limitations including possibly maneuverability limitations, fuel/time constraints and sensor range and accuracy. The developed cooperative search framework is based on two inter-dependent tasks: (1) online learning of the environment and storing of the information in the form of a ""search map""; and (2) utilization of the search map and other information to compute online a guidance trajectory for the agent to follow. The distributed learning and planning approach for cooperative search is illustrated by computer simulations.","Unmanned aerial vehicles,
Intelligent control,
Uncertainty,
Decision making,
Control systems,
Robot kinematics,
Search problems,
Computer science,
Fuels,
Time factors"
Representing classification problems in genetic programming,"Five alternative methods are proposed to perform multi-class classification tasks using genetic programming. These methods are: (1) binary decomposition, in which the problem is decomposed into a set of binary problems and standard genetic programming methods are applied; (2) static range selection, where the set of real values returned by a genetic program is divided into class boundaries using arbitrarily-chosen division points; (3) dynamic range selection, in which a subset of training examples are used to determine where, over the set of reals, class boundaries lie; (4) class enumeration, which constructs programs similar in syntactic structure to a decision tree; and (5) evidence accumulation, which allows separate branches of the program to add to the certainty of any given class. The results show that the dynamic range selection method is well-suited to the task of multi-class classification and is capable of producing classifiers that are more accurate than the other methods tried when comparable training times are allowed. The accuracy of the generated classifiers was comparable to alternative approaches over several data sets.","Genetic programming,
Classification tree analysis,
Computer science,
Dynamic range,
Problem-solving,
Decision trees,
Neural networks,
Voting,
Functional programming,
Uncertainty"
Spectral partitioning of random graphs,"Problems such as bisection, graph coloring, and clique are generally believed hard in the worst case. However, they can be solved if the input data is drawn randomly from a distribution over graphs containing acceptable solutions. In this paper we show that a simple spectral algorithm can solve all three problems above in the average case, as well as a more general problem of partitioning graphs based on edge density. In nearly all cases our approach meets or exceeds previous parameters, while introducing substantial generality. We apply spectral techniques, using foremost the observation that in all of these problems, the expected adjacency matrix is a low rank matrix wherein the structure of the solution is evident.","Partitioning algorithms,
Computer science,
Simulated annealing,
Temperature,
Algorithm design and analysis"
Optimization of wavelength assignment for QoS multicast in WDM networks,"This paper discusses quality-of-service (QoS) multicast in wavelength-division multiplexing (WDM) networks. Given a set of QoS multicast requests, we are to find a set of cost suboptimal QoS routing trees and assign wavelengths to them. The objective is to minimize the number of wavelengths in the system. This is a challenging issue. It involves not only optimal QoS multicast routing, but also optimal wavelength assignment. Existing methods consider channel setup in WDM networks in two separate steps: routing and wavelength assignment, which has limited power in minimizing the number of wavelengths. In this paper, we propose a new optimization method, which integrates routing and wavelength assignment in optimization of wavelengths. Two optimization algorithms are also proposed in minimizing the number of wavelengths. One algorithm minimizes the number of wavelengths through reducing the maximal link load in the system; while the other does it by trying to free out the least used wavelengths. Simulation results demonstrate that the proposed algorithms can produce suboptimal QoS routing trees and substantially save the number of wavelengths.","Wavelength assignment,
Intelligent networks,
WDM networks,
Wavelength routing,
Wavelength division multiplexing,
Optical wavelength conversion,
Delay,
Optical fiber networks,
Computer science,
Quality of service"
Survey of nanomanipulation systems,"Nanomanipulation as a new emerging area enables to change, interact and control the nano scale phenomenon precisely. Nanomanipulation systems are surveyed in this paper. Nanomanipulation approaches are grouped according to their starting point, utilized process, operation type, manipulation environment, interaction type, etc. Main components of such systems such as nanomanipulators, nano physics, sensors, actuators, and control are given in detail. Problems are defined and possible solutions are proposed. Moreover, possible applications in biotechnology, computer technology, material science and micro/nanotechnology are reported.","Materials science and technology,
Physics,
Biosensors,
Sensor phenomena and characterization,
Sensor systems,
Actuators,
Control systems,
Application software,
Biotechnology,
Computer applications"
LV volume quantification via spatiotemporal analysis of real-time 3-D echocardiography,"This paper presents a method of four-dimensional (4-D) (3-D+Time) space-frequency analysis for directional denoising and enhancement of real-time three-dimensional (RT3D) ultrasound and quantitative measures in diagnostic cardiac ultrasound. Expansion of echocardiographic volumes is performed with complex exponential wavelet-like basis functions called brushlets. These functions offer good localization in time and frequency and decompose a signal into distinct patterns of oriented harmonics, which are invariant to intensity and contrast range. Deformable-model segmentation is carried out on denoised data after thresholding of transform coefficients. This process attenuates speckle noise while preserving cardiac structure location. The superiority of 4-D over 3-D analysis for decorrelating additive white noise and multiplicative speckle noise on a 4-D phantom volume expanding in time is demonstrated. Quantitative validation, computed for contours and volumes, is performed on in vitro balloon phantoms. Clinical applications of this spatiotemporal analysis tool are reported for six patient cases providing measures of left ventricular volumes and ejection fraction.","Spatiotemporal phenomena,
Echocardiography,
Ultrasonic imaging,
Speckle,
Additive white noise,
Imaging phantoms,
Noise reduction,
Ultrasonic variables measurement,
Frequency,
Decorrelation"
Inferring link loss using striped unicast probes,"In this paper we explore the use of end-to-end unicast traffic as measurement probes to infer link-level loss rates. We leverage on of earlier work that produced efficient estimates for link-level loss rates based on end-to-end multicast traffic measurements. We design experiments based on the notion of transmitting stripes of packets (with no delay between transmission of successive packets within a stripe) to two or more receivers. The purpose of these stripes is to ensure that the correlation in receiver observations matches as closely as possible what would have been observed if the stripe had been replaced by a notional multicast probe that followed the same paths to the receivers. Measurements provide good evidence that a packet pair to distinct receivers introduces considerable correlation which can be further increased by simply considering longer stripes. We then use simulation to explore how well these stripes translate into accurate link-level loss estimates. We observe good accuracy with packet pairs, with a typical error of about 1%, which significantly decreases as stripe length is increased to 4 packets.",
Computational nanotechnology with carbon nanotubes and fullerenes,"The authors envision computational nanotechnology's role in developing the next generation of multifunctional materials and molecular-scale electronic and computing devices, sensors, actuators, and machines. They briefly review computational techniques and provide a few recent examples derived from computer simulations of carbon nanotube-based molecular nanotechnology. The four core areas are: molecular-scale, ultralightweight, extremely strong, functional or smart materials; molecular-scale or nanoscale electronics with possibilities for quantum computing; molecular-scale sensors or actuators; and molecular machines or motors with synthetic materials. The underlying molecular-scale building blocks in all four areas are fullerenes and carbon nanotube-based molecular materials. Only the different aspects of their physical, chemical, mechanical, and electronic properties create the many applications possible with these materials in vastly different areas.","Nanotechnology,
Carbon nanotubes,
Intelligent sensors,
Nanostructured materials,
Quantum computing,
Computer simulation,
Chemical sensors,
Mechanical sensors,
Intelligent actuators,
Micromotors"
"Building peer-to-peer systems with chord, a distributed lookup service","We argue that the core problem facing peer-to-peer Systems is locating documents in a decentralized network and propose Chord, a distributed lookup primitive. Chord provides an efficient method of locating documents while placing few constraints on the applications that use it. As proof that Chord's functionality is useful in the development of peer-to-peer applications, we outline the implementation of a peer-to-peer file sharing system based on Chord.","Peer to peer computing,
Robustness,
Scalability,
Costs,
Network servers,
Routing,
Laboratories,
Computer science,
Internet,
Authentication"
Tracking multiple people with a multi-camera system,"We present a multi-camera system based on Bayesian modality fusion to track multiple people in an indoor environment. Bayesian networks are used to combine multiple modalities for matching subjects between consecutive image frames and between multiple camera views. Unlike other occlusion reasoning methods, we use multiple cameras in order to obtain continuous visual information of people in either or both cameras so that they can be tracked through interactions. Results demonstrate that the system can maintain people's identities by using multiple cameras cooperatively.","Cameras,
Bayesian methods,
Humans,
Geometry,
Computer science,
Indoor environments,
Videoconference,
Filters,
Real time systems,
Histograms"
Vessel surface reconstruction with a tubular deformable model,"Three-dimensional (3-D) angiographic methods are gaining acceptance for evaluation of atherosclerotic disease. However, measurement of vessel stenosis from 3-D angiographic methods can be problematic due to limited image resolution and contrast. We present a method for reconstructing vessel surfaces from 3-D angiographic methods that allows for objective measurement of vessel stenosis. The method is a deformable model that employs a tubular coordinate system. Vertex merging is incorporated into the coordinate system to maintain even vertex spacing and to avoid problems of self-intersection of the surface. The deformable model was evaluated on clinical magnetic resonance (MR) images of the carotid (n=6) and renal (n=2) arteries, on an MR image of a physical vascular phantom and on a digital vascular phantom. Only one gross error occurred for all clinical images. All reconstructed surfaces had a realistic, smooth appearance. For all segments of the physical vascular phantom, vessel radii from the surface reconstruction had an error of less than 0.2 of the average voxel dimension. Variability of manual initialization of the deformable model had negligible effect on the measurement of the degree of stenosis of the digital vascular phantom.","Surface reconstruction,
Deformable models,
Image reconstruction,
Imaging phantoms,
Diseases,
Image resolution,
Merging,
Magnetic resonance,
Arteries,
Image segmentation"
Evaluation of quality of service schemes for IEEE 802.11 wireless LANs,"This paper evaluates four mechanisms for providing service differentiation in IEEE 802.11 wireless LANs, the point coordinator function (PCF) of IEEE 802.11, the enhanced distributed coordinator function (EDCF) of the proposed IEEE 802.11e extension to IEEE 802.11, distributed fair scheduling (DFS), and Blackburst using the ns-2 simulator. The metrics used in the evaluation are throughput, medium utilization, collision rate, average access delay, and delay distribution for a variable load of real time and background traffic. The PCF performance is comparably low, while the EDCF performs much better. The best performance is achieved by Blackburst. The DFS provides relative differentiation and consequently avoids starvation of low priority traffic.","Quality of service,
Wireless LAN,
Local area networks,
Computer science,
Delay effects,
Computer simulation,
Throughput,
Road accidents,
Current measurement,
Time measurement"
Reducing power requirements of instruction scheduling through dynamic allocation of multiple datapath resources,"The ""one-size-fits-all"" philosophy used for permanently allocating datapath resources in today's superscalar CPUs to maximize performance across a wide range of applications results in the overcommitment of resources in general. To reduce power dissipation in the datapath, the resource allocations can be dynamically adjusted based on the demands of applications. We propose a mechanism to dynamically, simultaneously and independently adjust the sizes of the issue queue (IQ), the reorder buffer (ROB) and the load/store queue (LSQ) based on the periodic sampling of their occupancies to achieve significant power savings with minimal impact on performance. Resource upsizing is done more aggressively (compared to downsizing) using the relative rate of blocked dispatches to limit the performance penalty. Our results are validated by the execution of SPEC 95 benchmark suite on a substantially modified version of Simplescalar simulator, where the IQ, the ROB, the LSQ and the register files are implemented as separate structures, as is the case with most practical implementations. For the SPEC 95 benchmarks, the use of our technique in a 4-way superscalar processor results in a power savings in excess of 70% within individual components and an average power savings of 53% for the IQ, LSQ and ROB combined for the entire benchmark suite with an average performance penalty of only 5%.","Dynamic scheduling,
Resource management,
Power dissipation,
Processor scheduling,
Computer science,
Application software,
Sampling methods,
Registers,
Energy efficiency,
Out of order"
Ordered treemap layouts,,
Theoretical study of lesion detectability of MAP reconstruction using computer observers,"The low signal-to-noise ratio (SNR) in emission data has stimulated the development of statistical image reconstruction methods based on the maximum a posteriori (MAP) principle. Experimental examples have shown that statistical methods improve image quality compared to the conventional filtered backprojection (FBP) method. However, these results depend on isolated data sets. Here, the authors study the lesion detectability of MAP reconstruction theoretically, using computer observers. These theoretical results can be applied to different object structures. They show that for a quadratic smoothing prior, the lesion detectability using the prewhitening observer is independent of the smoothing parameter and the neighborhood of the prior, while the nonprewhitening observer exhibits an optimum smoothing point. The authors also compare the results to those of FBP reconstruction. The comparison shows that for ideal positron emission tomography (PET) systems (where data are true line integrals of the tracer distribution) the MAP reconstruction has a higher SNR for lesion detection than FBP reconstruction due to the modeling of the Poisson noise. For realistic systems, MAP reconstruction further benefits from accurately modeling the physical photon detection process in PET.","Lesions,
Image reconstruction,
Positron emission tomography,
Smoothing methods,
Nonlinear filters,
Humans,
Signal to noise ratio,
Statistical analysis,
Image quality,
Signal resolution"
TestEra: a novel framework for automated testing of Java programs,"We present TestEra, a novel framework for automated testing of Java programs. TestEra automatically generates all non-isomorphic test cases within a given input size and evaluates correctness criteria. As an enabling technology, TestEra uses Alloy, a first-order relational language, and the Alloy Analyzer. Checking a program with TestEra involves modeling the correctness criteria for the program in Alloy and specifying abstraction and concretization translations between instances of Alloy models and Java data structures. TestEra produces concrete Java inputs as counterexamples to violated correctness criteria. The paper discusses TestEra's analyses of several case studies: methods that manipulate singly linked lists and red-black trees, a naming architecture, and a part of the Alloy Analyzer.","Automatic testing,
Java,
Data structures,
Concrete,
Software testing,
Marine technology,
Computational modeling,
Prototypes,
Laboratories,
Computer science"
Informational complexity and the direct sum problem for simultaneous message complexity,"Given m copies of the same problem, does it take m times the amount of resources to solve these m problems? This is the direct sum problem, a fundamental question that has been studied in many computational models. We study this question in the simultaneous message (SM) model of communication introduced by A.C. Yao (1979). The equality problem for n-bit strings is well known to have SM complexity /spl Theta/(/spl radic/n). We prove that solving m copies of the problem has complexity /spl Omega/(m/spl radic/n); the best lower bound provable using previously known techniques is /spl Omega/(/spl radic/(mn)). We also prove similar lower bounds on certain Boolean combinations of multiple copies of the equality function. These results can be generalized to a broader class of functions. We introduce a new notion of informational complexity which is related to SM complexity and has nice direct sum properties. This notion is used as a tool to prove the above results; it appears to be quite powerful and may be of independent interest.","Samarium,
Computer science,
Complexity theory,
Protocols,
Computational modeling,
National electric code,
Books"
Breast tissue density quantification via digitized mammograms,"Studies reported in the literature indicate that breast cancer risk is associated with mammographic densities. An objective, repeatable, and a quantitative measure of risk derived from mammographic densities will be of considerable use in recommending alternative screening paradigms and/or preventive measures. However, image processing efforts toward this goal seem to be sparse in the literature, and automatic and efficient methods do not seem to exist. Here, the authors describe and validate an automatic and reproducible method to segment dense tissue regions from fat within breasts from digitized mammograms using scale-based fuzzy connectivity methods. Different measures for characterizing mammographic density are computed from the segmented regions and their robustness in terms of their linear correlation across two different projections-cranio-caudal and medio-lateral-oblique-are studied. The accuracy of the method is studied by computing the area of mismatch of segmented dense regions using the proposed method and using manual outlining. A comparison between the mammographic density parameter taking into account the original intensities and that just considering the segmented area indicates that the former may have some advantages over the latter.","Breast tissue,
Breast cancer,
Density measurement,
Image segmentation,
Risk analysis,
Radiology,
Image processing,
Robustness,
Image analysis,
Biomedical image processing"
Training v-Support Vector Classifiers: Theory and Algorithms,"The -support vector machine (-SVM) for classification proposed by Schlkopf, Smola, Williamson, and Bartlett (2000) has the advantage of using a parameter  on controlling the number of support vectors. In this article, we investigate the relation between -SVM and C-SVM in detail. We show that in general they are two different problems with the same optimal solution set. Hence, we may expect that many numerical aspects of solving them are similar. However, compared to regular C-SVM, the formulation of -SVM is more complicated, so up to now there have been no effective methods for solving large-scale -SVM. We propose a decomposition method for -SVM that is competitive with existing methods for C-SVM. We also discuss the behavior of -SVM by some numerical experiments.",
Extraction of Specific Signals with Temporal Structure,"In this work we develop a very simple batch learning algorithm for semi-blind extraction of a desired source signal with temporal structure from linear mixtures. Although we use the concept of sequential blind extraction of sources and independent component analysis, we do not carry out the extraction in a completely blind manner; neither do we assume that sources are statistically independent. In fact, we show that the a priori information about the autocorrelation function of primary sources can be used to extract the desired signals (sources of interest) from their linear mixtures. Extensive computer simulations and real data application experiments confirm the validity and high performance of the proposed algorithm.",
Web-based peer review: the learner as both adapter and reviewer,"This study describes an effective web-based learning strategy, peer review, used by 143 computer science undergraduate students in an operating systems class at a Taiwanese university. Peer review, based on social constructivism, can be easily implemented via the authors' well-developed web-based peer review (WPR) system. Through peer review, the authors hope to form an authentic learning environment similar to an academic society in which a researcher submits a paper to a journal and receives reviews from society members before publication. Students using this learning strategy are expected to develop higher level thinking skills. The WPR system functioned in the following roles in this study: (1) an information distribution channel and management center for assignment submissions and peer review; (2) a forum for peer interaction and knowledge construction; and (3) storage for knowledge construction procedures. An evaluation of learning effects and students' perceptions about peer review during the spring of 1998 revealed that students not only performed better under peer review, but also displayed higher level thinking skills, i.e., critical thinking, planning, monitoring, and regulation. Students perceived peer review as an effective strategy that promoted their learning motivation. However, merely being an effective reviewer or an effective author may not excel in a peer review environment. The most effective individual appears to be the strategic adapter who effectively constructs a project, adjusts to peers' comments, and serves as a critical reviewer as well.",Computer science education
Spontaneous networking: an application oriented approach to ad hoc networking,"An ad hoc network must operate independent of a preestablished or centralized network management infrastructure, while still providing administrative services needed to support applications. Address allocation, name resolution, service location, authentication, and access control policies represent just some of the functionality that must be supported-without preconfiguration or centralized services. In order to solve these problems, it is necessary to leverage some aspect of the environment in which the network operates. We introduce the notion of a spontaneous network, created when a group of people come together for some collaborative activity. In this case, we can use the human interactions associated with the activity in order to establish a basic service and security infrastructure. We structure our discussion around a practical real-world scenario illustrating the use of such a network, identifying the key challenges involved and some of the techniques that can be used to address them.","Ad hoc networks,
Computer science,
Computer network management,
Application software,
Authentication,
Access control,
Collaboration,
Humans,
Pervasive computing,
TV"
Videoendoscopic distortion correction and its application to virtual guidance of endoscopy,"Modern video based endoscopes offer physicians a wide-angle field of view (FOV) for minimally invasive procedures, Unfortunately, inherent barrel distortion prevents accurate perception of range. This makes measurement and distance judgment difficult and causes difficulties in emerging applications, such as virtual guidance of endoscopic procedures. Such distortion also arises in other wide FOV camera circumstances. This paper presents a distortion correction technique that can automatically calculate correction parameters, without precise knowledge of horizontal and vertical orientation. The method is applicable to any camera-distortion correction situation. Based on a least-squares estimation, the authors' proposed algorithm considers line fits in both FOV directions and gives a globally consistent set of expansion coefficients and an optimal image center. The method is insensitive to the initial orientation of the endoscope and provides more exhaustive FOV correction than previously proposed algorithms. The distortion-correction procedure is demonstrated for endoscopic video images of a calibration test pattern, a rubber bronchial training device, and real human circumstances. The distortion correction is also shown as a necessary component of an image-guided virtual-endoscopy system that matches endoscope images to corresponding rendered three-dimensional computed tomography views.","Endoscopes,
Minimally invasive surgery,
Distortion measurement,
Cameras,
Calibration,
Testing,
Rubber,
Humans,
Rendering (computer graphics),
Computed tomography"
Encoding program executions,"Dynamic analysis is based on collecting data as the program runs. However, raw traces tend to be too voluminous and too unstructured to be used directly for visualization and understanding. We address this problem in two phases: the first phase selects subsets of the data and then compacts it, while the second phase encodes the data in an attempt to infer its structure. Our major compaction/selection techniques include gprof-style N-depth call sequences, selection based on class, compaction based on time intervals, and encoding the whole execution as a directed acyclic graph. Our structure inference techniques include run-length encoding, context free grammar encoding, and the building of finite state automata.","Encoding,
Libraries,
Data visualization,
Compaction,
Buildings,
Performance analysis,
Computer science,
Software systems,
Displays,
Navigation"
Monitoring programs using rewriting,"We present a rewriting algorithm for efficiently testing future time Linear Temporal Logic (LTL) formulae on finite execution traces. The standard models of LTL are infinite traces, reflecting the behavior of reactive and concurrent systems which conceptually may be continuously alive. In most past applications of LTL, theorem provers and model checkers have been used to formally prove that down-scaled models satisfy such LTL specifications. Our goal is instead to use LTL for up-scaled testing of real software applications, corresponding to analyzing the conformance of finite traces against LTL formulae. We first describe what it means for a finite trace to satisfy an LTL formula and then suggest an optimized algorithm based on transforming LTL formulae. We use the Maude rewriting logic, which turns out to be a good notation and being supported by an efficient rewriting engine for performing these experiments. The work constitutes part of the Java PathExplorer (JPAX) project, the purpose of which is to develop a flexible tool for monitoring Java program executions.","Logic testing,
Java,
NASA,
Data structures,
Heuristic algorithms,
Computerized monitoring,
Computer science,
Application software,
Software testing,
Engines"
Adaptive proportional delay differentiated services: characterization and performance evaluation,"We examine a proportional-delay model for Internet differentiated services. Under this model, an Internet service provider (ISP) can control the waiting-time ""spacings"" between different classes of traffic. Specifically, the ISP tries to ensure that the average waiting time of class i traffic relative to that of class i-1 traffic is kept at a constant specified ratio. If the waiting-time ratio of class i-1 to class i is greater than one, the ISP can legitimately charge users of class i traffic a higher tariff rate (compared to the rate for class i-1 traffic), since class i users consistently enjoy better performance than class i-1 users. To realize such proportional-delay differentiated services, we use the time-dependent priority scheduling algorithm. We formally characterize the feasible regions in which given delay ratios can be achieved. Moreover, a set of control parameters for obtaining the desired delay ratios can be determined by an efficient iterative algorithm. We also use an adaptive control algorithm to maintain the correctness of these parameters in response to changing system load. Experiments are carried out to illustrate the short-term, medium-term and long-term relative waiting-time performances for different service classes under Poisson, Pareto, MMPP and mixed traffic workloads. We also carry out experiments to evaluate the achieved end-to-end accumulative waiting times for different classes of traffic which traverse multiple hops under our service model.","Delay,
Traffic control,
Web and internet services,
Telecommunication traffic,
Scheduling algorithm,
Iterative algorithms,
Application software,
Computer science,
Protocols,
Adaptive control"
MBO: marriage in honey bees optimization-a Haplometrosis polygynous swarming approach,"Honey-bees are one of the most well studied social insects. They exhibit many features that distinguish their use as models for intelligent behavior. These features include division of labor, communication on the individual and group level, and cooperative behavior. In this paper, we present a unified model for the marriage in honey-bees within an optimization context. The model simulates the evolution of honey-bees starting with a solitary colony (single queen without a family) to the emergence of an eusocial colony (one or more queens with a family). From optimization point of view, the model is a committee machine approach where we evolve solutions using a committee of heuristics. The model is applied to a fifty propositional satisfiability problems (SAT) with 50 variables and 215 constraints to guarantee that the problems are centered on the phase transition of 3-SAT. Our aim in this paper is to analyze the behavior of the algorithm using biological concepts (number of queens, spermatheca size, and number of broods) rather than trying to improve the performance of the algorithm while losing the underlying biological essence. Notwithstanding, the algorithm outperformed WalkSAT, one of the state-of-the-art algorithms for SAT.","Insects,
Biological system modeling,
Ant colony optimization,
Computer science,
Educational institutions,
Drives,
Australia,
Context modeling,
Evolution (biology),
Performance analysis"
Vessel extraction in medical images by wave-propagation and traceback,"Presents an approach for the extraction of vasculature from angiography images by using a wave propagation and traceback mechanism. The authors discuss both the theory and the implementation of the approach. Using a dual-sigmoidal filter, they label each pixel in an angiogram with the likelihood that it is within a vessel. Representing the reciprocal of this likelihood image as an array of refractive indexes, the authors propagate a digital wave through the image from the base of the vascular tree. This wave ""washes"" over the vasculature, ignoring local noise perturbations. The extraction of the vasculature becomes that of tracing the wave along the local normals to the waveform. While the approach is inherently single instruction stream multiple data stream (SIMD), the authors present an efficient sequential algorithm for the wave propagation and discuss the traceback algorithm. They demonstrate the effectiveness of their integer image neighborhood-based algorithm and its robustness to image noise.","Biomedical imaging,
Data mining,
X-ray imaging,
Pattern recognition,
Angiography,
Optical imaging,
Image segmentation,
Streaming media,
Magnetic noise,
Computed tomography"
Statistical context priming for object detection,"There is general consensus that context can be a rich source of information about an object's identity, location and scale. However the issue of how to formalize centextual influences is still largely open. Here we introduce a simple probabilistic framework for modeling the relationship between context and object properties. We represent global context information in terms of the spatial layout of spectral components. The resulting scheme serves as an effective procedure for context driven focus of attention and scale-selection on real-world scenes. Based on a simple holistic analysis of an image, the scheme is able to accurately predict object locations and sizes.",
Hierarchical estimation of a dense deformation field for 3-D robust registration,"A new method for medical image registration is formulated as a minimization problem involving robust estimators. The authors propose an efficient hierarchical optimization framework which is both multiresolution and multigrid. An anatomical segmentation of the cortex is introduced in the adaptive partitioning of the volume on which the multigrid minimization is based. This allows to limit the estimation to the areas of interest, to accelerate the algorithm, and to refine the estimation in specified areas. At each stage of the hierarchical estimation, the authors refine current estimate by seeking a piecewise affine model for the incremental deformation field. The performance of this method is numerically evaluated on simulated data and its benefits and robustness are shown on a database of 18 magnetic resonance imaging scans of the head.",
An artificial intelligent algorithm for tumor detection in screening mammogram,"Cancerous tumor mass is one of the major types of breast cancer. When cancerous masses are embedded in and camouflaged by varying densities of parenchymal tissue structures, they are very difficult to be visually detected on mammograms. This paper presents an algorithm that combines several artificial intelligent techniques with the discrete wavelet transform (DWT) for detection of masses in mammograms. The AI techniques include fractal dimension analysis, multiresolution Markov random field, dogs-and-rabbits algorithm, and others. The fractal dimension analysis serves as a preprocessor to determine the approximate locations of the regions suspicious for cancer in the mammogram. The dogs-and-rabbits clustering algorithm is used to initiate the segmentation at the LL subband of a three-level DWT decomposition of the mammogram. A tree-type classification strategy is applied at the end to determine whether a given region is suspicious for cancer. The authors have verified the algorithm with 322 mammograms in the Mammographic Image Analysis Society Database. The verification results show that the proposed algorithm has a sensitivity of 97.3% and the number of false positives per image is 3.92.","Artificial intelligence,
Tumors,
Discrete wavelet transforms,
Clustering algorithms,
Fractals,
Breast neoplasms,
Breast cancer,
Cancer detection,
Algorithm design and analysis,
Markov random fields"
Robust midsagittal plane extraction from normal and pathological 3-D neuroradiology images,"This paper focuses on extracting the ideal midsagittal plane (iMSP) from three-dimensional (3-D) normal and pathological neuroimages. The main challenges in this work are the structural asymmetry that may exist in pathological brains, and the anisotropic, unevenly sampled image data that is common in clinical practice. We present an edge-based, cross-correlation approach that decomposes the plane fitting problem into discovery of two-dimensional symmetry axes on each slice, followed by a robust estimation of plane parameters. The algorithm's tolerance to brain asymmetries, input image offsets and image noise is quantitatively evaluated. We find that the algorithm can extract the iMSP from input 3-D images with 1) large asymmetrical lesions; 2) arbitrary initial rotation offsets; 3) low signal-to-noise ratio or high bias field. The iMSP algorithm is compared with an approach based on maximization of mutual information registration, and is found to exhibit superior performance under adverse conditions. Finally, no statistically significant difference is found between the midsagittal plane computed by the iMSP algorithm and that estimated by two trained neuroradiologists.","Robustness,
Pathology,
Data mining,
Anisotropic magnetoresistance,
Lesions,
Image edge detection,
Humans,
Brain,
NIST,
Robot kinematics"
Artery-vein separation via MRA-An image processing approach,"Presents a near-automatic process for separating vessels from background and other clutter as well as for separating arteries and veins in contrast-enhanced magnetic resonance angiographic (CE-MRA) image data, and an optimal method for three-dimensional visualization of vascular structures. The separation process utilizes fuzzy connected object delineation principles and algorithms. The first step of this separation process is the segmentation of the entire vessel structure from the background and other clutter via absolute fuzzy connectedness. The second step is to separate artery from vein within this entire vessel structure via iterative relative fuzzy connectedness. After seed voxels are specified inside the artery and vein in the CE-MRA image, the small regions of the bigger aspects of artery and vein are separated in the initial iterations, and further detailed aspects of artery and vein are included in later iterations. At each iteration, the artery and vein compete among themselves to grab membership of each voxel in the vessel structure based on the relative strength of connectedness of the voxel in the artery and vein. This approach has been implemented in a software package for routine use in a clinical setting and tested on 133 CE-MRA studies of the pelvic region and two studies of the carotid system from 6 different hospitals. In all studies, unified parameter settings produced correct artery-vein separation. When compared with manual segmentation/separation, the authors' algorithms were able to separate higher order branches, and therefore produced vastly more details in the segmented vascular structure. The total operator and computer time taken per study is on the average about 4.5 min. To date, this technique seems to be the only image processing approach that can be routinely applied for artery and vein separation.","Image processing,
Arteries,
Veins,
Image segmentation,
Separation processes,
Magnetic resonance,
Data visualization,
Iterative algorithms,
Software packages,
Software testing"
Time and frequency domain characteristics of polarization-mode dispersion emulators,We investigate both experimentally and theoretically a new technique to realistically emulate polarization-mode dispersion (PMD). We propose and demonstrate a PMD emulator using rotatable connectors between sections of polarization-maintaining fibers that generates an ensemble of high PMD fiber realizations by randomly rotating the connectors. It is shown that: (1) the DGD of this emulator is Maxwellian-distributed over an ensemble of fiber realizations at any fixed optical frequency; and (2) the frequency autocorrelation function of the PMD emulator resembles that in a real fiber when averaged over an ensemble of fiber realizations. A realistic autocorrelation function is required for proper emulation of higher order PMD and indicates the feasibility of using this emulator for wavelength-division-multiplexing (WDM) systems.,"Frequency domain analysis,
Polarization mode dispersion,
Optical fiber polarization,
Optical fiber communication,
Autocorrelation,
Optical fibers,
Connectors,
Emulation,
Wavelength division multiplexing,
Computer science"
"Photometric stereo with general, unknown lighting","Work on photometric stereo has shown how to recover the shape and reflectance properties of an object using multiple images taken with a fixed viewpoint and variable lighting conditions. This work has primarily relied on the presence of a single point source of light in each image. The authors show how to perform photometric stereo, assuming that all lights in a scene are isotropic and distant from the object but otherwise unconstrained. Lighting in each image may be an unknown and arbitrary combination of diffuse, point and extended sources. Our work is based on recent results showing that for Lambertian objects, general lighting conditions can be represented using low order spherical harmonics. Using this representation, we can recover shape by performing a simple optimization in a low-dimensional space. We also analyze the shape ambiguities that arise in such a representation.",
Recognition of human gaits,"We pose the problem of recognizing different types of human gait in the space of dynamical systems where each gait is represented Established techniques are employed to track a kinematic model of a human body in motion, and the trajectories of the parameters are used to learn a representation of a dynamical system, which defines a gait. Various types of distance between models are then computed These computations are non trivial due to the fact that, even for the case of linear systems, the space of canonical realizations is not linear.","Humans,
Hidden Markov models,
Trajectory,
Linear systems,
Computer science,
Tracking,
Kinematics,
Biological system modeling,
Legged locomotion,
Photometry"
Web metrics - estimating design and authoring effort,"Like any software process, Web application development would benefit from early-stage effort estimates. Using an undergraduate university course as a case study, we collected metrics corresponding to Web applications, developers and tools. Then we used those metrics to generate models for predicting design and authoring effort for future Web applications.","Web page design,
HTML,
Web pages,
Data analysis,
Design engineering,
Computer science,
Multimedia systems,
Web search,
Search engines,
Markup languages"
Algorithmic statistics,"While Kolmogorov (1965, 1983) complexity is the accepted absolute measure of information content of an individual finite object, a similarly absolute notion is needed for the relation between an individual data sample and an individual model summarizing the information in the data, for example, a finite set (or probability distribution) where the data sample typically came from. The statistical theory based on such relations between individual objects can be called algorithmic statistics, in contrast to classical statistical theory that deals with relations between probabilistic ensembles. We develop the algorithmic theory of statistic, sufficient statistic, and minimal sufficient statistic. This theory is based on two-part codes consisting of the code for the statistic (the model summarizing the regularity, the meaningful information, in the data) and the model-to-data code. In contrast to the situation in probabilistic statistical theory, the algorithmic relation of (minimal) sufficiency is an absolute relation between the individual model and the individual data sample. We distinguish implicit and explicit descriptions of the models. We give characterizations of algorithmic (Kolmogorov) minimal sufficient statistic for all data samples for both description modes-in the explicit mode under some constraints. We also strengthen and elaborate on earlier results for the ""Kolmogorov structure function"" and ""absolutely nonstochastic objects""-those objects for which the simplest models that summarize their relevant information (minimal sufficient statistics) are at least as complex as the objects themselves. We demonstrate a close relation between the probabilistic notions and the algorithmic ones: (i) in both cases there is an ""information non-increase"" law; (ii) it is shown that a function is a probabilistic sufficient statistic iff it is with high probability (in an appropriate sense) an algorithmic sufficient statistic.",
Three-dimensional Bayesian optical image reconstruction with domain decomposition,"Most current efforts in near-infrared optical tomography are effectively limited to two-dimensional reconstructions due to the computationally intensive nature of full three-dimensional (3-D) data inversion. Previously, we described a new computationally efficient and statistically powerful inversion method APPRIZE (automatic progressive parameter-reducing inverse zonation and estimation). The APPRIZE method computes minimum-variance estimates of parameter values (here, spatially variant absorption due to a fluorescent contrast agent) and covariance, while simultaneously estimating the number of parameters needed as well as the size, shape, and location of the spatial regions that correspond to those parameters. Estimates of measurement and model error are explicitly incorporated into the procedure and implicitly regularize the inversion in a physically based manner. The optimal estimation of parameters is bounds-constrained, precluding infeasible values. In this paper, the APPRIZE method for optical imaging is extended for application to arbitrarily large 3-D domains through the use of domain decomposition. The effect of subdomain size on the performance of the method is examined by assessing the sensitivity for identifying 112 randomly located single-voxel heterogeneities in 58 3-D domains. Also investigated are the effects of unmodeled heterogeneity in background optical properties. The method is tested on simulated frequency-domain photon migration measurements at 100 MHz in order to recover absorption maps owing to fluorescent contrast agent. This study provides a new approach for computationally tractable 3-D optical tomography.","Bayesian methods,
Image reconstruction,
Optical sensors,
Optical computing,
Parameter estimation,
Tomography,
Absorption,
Fluorescence,
Shape,
Optical imaging"
Dynamic prediction of critical path instructions,"Modern processors come close to executing as fast as role dependences allow. The particular dependences that constrain execution speed constitute the critical path of execution. To optimize the performance of the processor we either have to reduce the critical path or execute it more efficiently. In both cases, it can be done more effectively if we know the actual instructions that constitute that path. This paper describes critical path prediction for dynamically identifying instructions likely to be on the critical path, allowing various processor optimizations to take advantage of this information. We show several possible critical path prediction techniques and apply critical path prediction to value prediction and clustered architecture scheduling. We show that critical path prediction has the potential to increase the effectiveness of these hardware optimizations by as much as 70%, without adding greatly to their cost.","Hardware,
Capacitive sensors,
Throughput,
Out of order,
Computer science,
Modems,
Processor scheduling,
Constraint optimization,
Cost function,
Registers"
On modeling data mining with granular computing,"This paper deals with the formal and mathematical modeling of data mining. A framework is proposed for rule mining based on granular computing. It is developed in the Tarski's style through the notions of a model and satisfiability. The model is a database consisting of a finite set of objects described by a finite set of attributes. Within this framework, a concept is defined as a pair consisting of the intension, an expression in a certain language over the set of attributes, and an extension of the concept, a subset of the universe. An object satisfies the expression of a concept if the object has the properties as specified by the expression, and the object belongs to the extension of the concepts. Rules are used to describe relationships between concepts. A rule is expressed in terms of the intentions of the two concepts and is interpreted in terms of the extensions of the concepts. Two interpretations of rules are examined in detail, one is based on the logical implication and the other on the conditional probability.","Data mining,
Databases,
Mathematical model,
Computer science,
Uniform resource locators,
Algorithm design and analysis,
Testing"
"An adaptive distance vector routing algorithm for mobile, ad hoc networks","We present a new routing algorithm called adaptive distance vector (ADV) for mobile, ad hoc networks (MANETs). ADV is a distance vector routing algorithm that exhibits some on-demand characteristics by varying the frequency and the size of the routing updates in response to the network load and mobility conditions. Using simulations we show that ADV outperforms AODV and DSR especially in high mobility cases by giving significantly higher (50% or more) peak throughputs and lower packet delays. Furthermore, ADV uses fewer routing and control overhead packets than that of AODV and DSR, especially at moderate to high loads. Our results indicate the benefits of combining both proactive and on-demand routing techniques in designing suitable routing protocols for MANETs.",
BICAV: a block-iterative parallel algorithm for sparse systems with pixel-related weighting,"Component averaging (CAV) was recently introduced by Censor, Gordon, and Gordon as a new iterative parallel technique suitable for large and sparse unstructured systems of linear equations. Based on earlier work of Byrne and Censor, it uses diagonal weighting matrices, with pixel-related weights determined by the sparsity of the system matrix. CAV is inherently parallel (similar to the very slowly converging Cimmino method) but its practical convergence on problems of image reconstruction from projections is similar to that of the algebraic reconstruction technique (ART). Parallel techniques are becoming more important for practical image reconstruction since they are relevant not only for supercomputers but also for the increasingly prevalent multiprocessor workstations. This paper reports on experimental results with a block-iterative version of component averaging (BICAV). When BICAV is optimized for block size and relaxation parameters, its very first iterates are far superior to those of CAV, and more or less on a par with ART. Similar to CAV, BICAV is also inherently parallel. The fast convergence is demonstrated on problems of image reconstruction from projections, using the SNARK93 image reconstruction software package. Detailed plots of various measures of convergence, and reconstructed images are presented.","Parallel algorithms,
Image reconstruction,
Convergence,
Sparse matrices,
Subspace constraints,
Equations,
Image converters,
Supercomputers,
Workstations,
Software packages"
Self-tuned congestion control for multiprocessor networks,"One-track performance in tightly-coupled multiprocessors typically, degrades rapidly beyond network saturation. Consequently, designers must keep a network below its saturation point by reducing the load on the network. Congestion control via source throttling-a common technique to reduce the network load-presents new packets from entering the network in the presence of congestion. Unfortunately, prior schemes to implement source throttling either lack vital global information about the network to make the correct decision (whether to throttle or not) or depend on specific network parameters, network topology or communication pattern. This paper presents a global-knowledge-based, self-tuned, congestion control technique that prevents saturation at high loads across different network configurations and commutation pattern. Our design is composed of two key components. First, we use global information about a network to obtain a timely estimate of network congestion. We compare this estimate to a threshold value to determine when to throttle packet injection. The second component is a self-tuning mechanism that automatically determines appropriate threshold values based on throughput feedback. A combination of these two techniques provides high performance under heavy load does not penalize performance under light load, and gracefully adapts to changes in communication patterns.","Delay,
Degradation,
Throughput,
Multiprocessor interconnection networks,
Bandwidth,
Computer science,
Network topology,
Communication system control,
Feedback,
Application software"
Optimizing schedules for prioritized path planning of multi-robot systems,The coordination of robot motions is one of the fundamental problems for multi-robot systems. A popular approach to avoid planning in the high-dimensional composite configuration space is the prioritized and decoupled technique. In this paper we present a method for optimizing priority schemes for such prioritized and decoupled planning technique. Our approach performs a randomized search with hill-climbing to find solutions and to minimize the overall path lengths. The technique has been implemented and tested on real robots and in extensive simulation runs. The experimental results demonstrate that our method is able to greatly reduce the number of failures and to significantly reduce the overall path length for different prioritized and decoupled path planning techniques and even for large teams of robots.,"Path planning,
Multirobot systems,
Robot kinematics,
Orbital robotics,
Motion planning,
Computer science,
State-space methods,
Processor scheduling,
Optimization methods,
Testing"
Three-dimensional reconstruction of microcalcification clusters from two mammographic views,"Classification of benign/malignant microcalcification clusters is a major diagnostic challenge for radiologists. Clinical studies have revealed that the shape of the cluster, and the spatial distribution of individual microcalcifications within it, are important indicators of its malignancy. However, mammographic images of clustered microcalcifications confound their three-dimensional (3-D) distribution with image projection and breast compression. This paper presents a novel model-based method for reconstructing microcalcification clusters in 3-D from two mammographic views (cranio-caudal and medio-lateral oblique-""shoulder to the opposite hip"" or lateral-medio). The authors develop a 3-D breast representation and a parameterised breast compression model which constraints geometrically the possible 3-D positions of a calcification in a two-dimensional image. Corresponding calcifications in the two views are matched using an estimate of the calcification volume. Both the geometric constraint and the matching criterion are utilized in the final reconstruction step to build the 3-D reconstructed clusters. Validation experiments are described using 30 clusters to verify the individual steps of the model, and results consistent with known ground truth are obtained. Some of the approximations in the model and future work are discussed in the concluding section.",
New families of binary sequences with optimal three-level autocorrelation,"In this correspondence we give several new families of binary sequences of period N with optimal three-level autocorrelation, where N/spl equiv/2 (mod 4). These sequences are either balanced or almost balanced. Our construction is based on cyclotomy.",Binary sequences
Tag surface reconstruction and tracking of myocardial beads from SPAMM-MRI with parametric B-spline surfaces,"Magnetic resonance imaging (MRI) is unique in its ability to noninvasively and selectively alter tissue magnetization, and create tag planes intersecting image slices. The resulting grid of signal voids allows for tracking deformations of tissues in otherwise homogeneous-signal myocardial regions. Here, the authors propose a specific spatial modulation of magnetization (SPAMM) imaging protocol together with efficient techniques for measurement of three-dimensional (3-D) motion of material points of the human heart (referred to as myocardial beads) from images collected with the SPAMM method. The techniques make use of tagged images in orthogonal views by explicitly reconstructing 3-D B-spline surface representation of tag planes (tag planes in two orthogonal orientations intersecting the short-axis (SA) image slices and tag planes in an orientation orthogonal to the short-axis tag planes intersecting long-axis (LA) image slices). The developed methods allow for viewing deformations of 3-D tag surfaces, spatial correspondence of long-axis and short-axis image slice and tag positions, as well as nonrigld movement of myocardial beads as a function of time.","Surface reconstruction,
Myocardium,
Spline,
Image reconstruction,
Magnetic resonance imaging,
Magnetization,
Magnetic materials,
Magnetic modulators,
Protocols,
Motion measurement"
A comprehensive model of the supercomputer workload,"As with any computer system, the performance of supercomputers depends upon the workloads that serve as their input. Unfortunately, however, there are many important aspects of the supercomputer workloads that have not been modeled, or that have been modeled only incipiently. This paper attacks this problem by considering requested time (and its relation with execution time) and the possibility of job cancellation, two aspects of the supercomputer workload that have not been modeled yet. Moreover, we also improve upon existing models for the arrival instant and partition size.","Supercomputers,
Job design,
Concurrent computing,
US Department of Energy,
NASA,
Computational modeling,
Processor scheduling,
Computer science,
Upper bound,
Displays"
From play-in scenarios to code: an achievable dream,"The article presents a general, rather sweeping development scheme for complex reactive systems, combining ideas that have been known for a long time with more recent ones. The scheme makes it possible to go from a high-level user-friendly requirements capture method, which is called play-in scenarios, via a rich language for describing message sequencing to a full model of the system, and from there to final implementation. A cyclic process of verifying the system against requirements and synthesizing system parts from the requirements is central to the proposal. The article puts special emphasis on the languages, methods, and computerized tools that allow smooth but rigorous transitions between the various stages of the scheme. In contrast to database systems, the article focuses on systems that have a dominant reactive, event-driven facet. For these systems, modeling and analyzing behavior is the most crucial and problematic issue.","Object oriented modeling,
System testing,
Optimization methods,
Proposals,
Database systems,
Joining processes,
Unified modeling language,
Hardware design languages,
Computer science,
Debugging"
Algorithms for quad-double precision floating point arithmetic,"A quad-double number is an unevaluated sum of four IEEE double precision numbers, capable of representing at least 212 bits of significand. We present the algorithms for various arithmetic operations (including the four basic operations and various algebraic and transcendental operations) on quad-double numbers. The performance of the algorithms, implemented in C++, is also presented.","Floating-point arithmetic,
Physics computing,
Computational geometry,
Libraries,
Packaging,
Computer science,
Laboratories,
Mathematics,
Cryptography,
Uncertainty"
File and object replication in data grids,"Data replication is a key issue in a data grid and can be managed in different ways and at different levels of granularity: for example, at the file level or the object level. In the high-energy physics community, data grids are being developed to support the distributed analysis of experimental data. We have produced a prototype data replication tool, the Grid Data Management Pilot (GDMP) that is in production use in one physics experiment, with middleware provided by the Globus toolkit used for authentication, data movement and other purposes. We present a new, enhanced GDMP architecture and prototype implementation that uses Globus data-grid tools for efficient file replication. We also explain how this architecture can address object replication issues in an object-oriented database management system. File transfer over wide-area networks requires specific performance tuning in order to gain optimal data transfer rates. We present performance results obtained with GridFTP, an enhanced version of FTP, and discuss tuning parameters.",
On constructing the Huffman-code-based reversible variable-length codes,"In this letter, we propose a generic and efficient algorithm that can construct both asymmetrical and symmetrical reversible variable-length codes (RVLCs). Starting from a given Huffman code, the construction is based on two developed codeword selection mechanisms, for the symmetrical case and the asymmetrical case, respectively; it is shown that the two mechanisms possess simple features and can generate efficient RVLCs easily. In addition, two new asymmetrical RVLCs are constructed and shown to be very efficient for further reducing the coding overheads in MPEG-4 when operating in the reversible decoding mode.","MPEG 4 Standard,
Decoding,
Application software,
Error correction codes,
Standards development,
Robustness,
Communications Society,
Asia,
Computer science,
Continuous wavelet transforms"
A metaheuristic for the pickup and delivery problem with time windows,"In this paper, we propose a metaheuristic to solve the pickup and delivery problem with time windows. Our approach is a tabu-embedded simulated annealing algorithm which restarts a search procedure from the current best solution after several non-improving search iterations. The computational experiments on the six newly-generated different data sets marked our algorithm as the first approach to solve large multiple-vehicle PDPTW problem instances with various distribution properties.","Vehicles,
Routing,
Transportation,
Job shop scheduling,
Computer science,
Computational modeling,
Simulated annealing,
Read only memory,
Distributed computing,
Logistics"
Statistical analysis of WCET for scheduling,"To perform a schedulability test, scheduling analysis relies on a known worst-case execution time (WCET). This value may be difficult to compute and may be overly pessimistic. This paper offers an alternative analysis based on estimating a WCET from test data to within a specific level of probabilistic confidence. A method is presented for calculating an estimate given statistical assumptions. The implications of the level of confidence on the likelihood of schedulability are also presented.","Statistical analysis,
Processor scheduling,
Upper bound,
Real time systems,
Computer science,
Performance evaluation,
System testing,
Performance analysis,
Runtime,
Modems"
TCP-BuS: Improving TCP performance in wireless Ad Hoc networks,"Reliable data transmission over wireless multi-hop networks, called ad hoc networks, has proven to be non-trivial. TCP (Transmission Control Protocol), a widely used end-to-end reliable transport protocol designed for wired networks, is not entirely suitable for wireless ad hoc networks due to the inappropriateness of TCP congestion control schemes. Specifically, the TCP sender concludes that there is network congestion upon detecting packet losses or at time-outs. However, in wireless ad hoc networks, links are broken as a result of node mobility and hence some time is needed to perform route reconfiguration. During this time, packets could be lost or held back. Hence, the TCP sender could mistake this event as congestion, which is untrue. A route disconnection should be handled differently from network congestion. In this paper, we propose a new mechanism that improves TCP performance in a wireless ad hoc network where each node can buffer ongoing packets during a route disconnection and re-establishment. In addition to distinguishing network congestion from route disconnection due to node mobility, we also incorporate new measures to deal with reliable transmission of important control messages and exploitation of TCP fast recovery procedures. Our simulation compares the proposed TCP-BuS approach with general TCP and TCP-Feedback. Results reveal that TCP-BuS outperforms other approaches in terms of communication throughput under the presence of mobility.","Routing protocols,
Reliability,
Mobile ad hoc networks,
Data communication,
Mobile communication"
Applying the lattice Boltzmann equation to multiscale fluid problems,The authors discuss the theory and application of the lattice Boltzmann equation to multiscale physics in fluids. They present two examples relevant to real-life applications: airflow around an airfoil at high Reynolds numbers and reactive flow in micropores.,"Lattice Boltzmann methods,
Equations,
Physics computing,
Kinetic theory,
Computational modeling,
Extraterrestrial phenomena,
Drugs,
Combustion,
Solids,
Electric shock"
Ideal observer approximation using Bayesian classification neural networks,"It is well understood that the optimal classification decision variable is the likelihood ratio or any monotonic transformation of the likelihood ratio. An automated classifier which maps from an input space to one of the likelihood ratio family of decision variables is an optimal classifier or ""ideal observer."" Artificial neural networks (ANNs) are frequently used as classifiers for many problems. In the limit of large training sample sizes, an ANN approximates a mapping function which is a monotonic transformation of the likelihood ratio, i.e., it estimates an ideal observer decision variable. A principal disadvantage of conventional ANNs is the potential over-parameterization of the mapping function which results in a poor approximation of an optimal mapping function for smaller training samples. Recently, Bayesian methods have been applied to ANNs in order to regularize training to improve the robustness of the classifier. The goal of training a Bayesian ANN with finite sample sizes is, as with unlimited data, to approximate the ideal observer. The authors have evaluated the accuracy of Bayesian ANN models of ideal observer decision variables as a function of the number of hidden units used, the signal-to-noise ratio of the data and the number of features or dimensionality of the data. The authors show that when enough training data are present, excess hidden units do not substantially degrade the accuracy of Bayesian ANNs. However, the minimum number of hidden units required to best model the optimal mapping function varies with the complexity of the data.","Bayesian methods,
Neural networks,
Artificial neural networks,
Computer aided diagnosis,
Diseases,
Radiology,
Robustness,
Signal to noise ratio,
Training data,
Degradation"
Modeling system calls for intrusion detection with dynamic window sizes,We extend prior research on system call anomaly detection modeling methods for intrusion detection by incorporating dynamic window sizes. The window size is the length of the subsequence of a system call trace which is used as the basic unit for modeling program or process behavior. In this work we incorporate dynamic window sizes and show marked improvements in anomaly detection. We present two methods for estimating the optimal window size based on the available training data. The first method is an entropy modeling method which determines the optimal single window size for the data. The second method is a probability modeling method that takes into account context dependent window sizes. A context dependent window size model is motivated by the way that system calls are generated by processes. Sparse Markov transducers (SMTs) are used to compute the context dependent window size model. We show over actual system call traces that the entropy modeling methods lead to the optimal single window size. We also show that context dependent window sizes outperform traditional system call modeling methods.,
Computerized radiographic mass detection. I. Lesion site selection by morphological enhancement and contextual segmentation,"This paper presents a statistical model supported approach for enhanced segmentation and extraction of suspicious mass areas from mammographic images. With an appropriate statistical description of various discriminate characteristics of both true and false candidates from the localized areas, an improved mass detection may be achieved in computer-assisted diagnosis (CAD). In this study, one type of morphological operation is derived to enhance disease patterns of suspected masses by cleaning up unrelated background clutters, and a model-based image segmentation is performed to localize the suspected mass areas using a stochastic relaxation labeling scheme. We discuss the importance of model selection when a finite generalized Gaussian mixture is employed, and use the information theoretic criteria to determine the optimal model structure and parameters. Examples are presented to show the effectiveness of the proposed methods on mass lesion enhancement and segmentation when applied to mammographical images. Experimental results demonstrate that the proposed method achieves a very satisfactory performance as a preprocessing procedure for mass detection in CAD.",
Application of partition-based median type filters for suppressing noise in images,"An adaptive median based filter is proposed for removing noise from images. Specifically, the observed sample vector at each pixel location is classified into one of M mutually exclusive partitions, each of which has a particular filtering operation. The observation signal space is partitioned based an the differences defined between the current pixel value and the outputs of CWM (center weighted median) filters with variable center weights. The estimate at each location is formed as a linear combination of the outputs of those CWM filters and the current pixel value. To control the dynamic range of filter outputs, a location-invariance constraint is imposed upon each weighting vector. The weights are optimized using the constrained LMS (least mean square) algorithm. Recursive implementation of the new filter is then addressed. The new technique consistently outperforms other median based filters in suppressing both random-valued and fixed-valued impulses, and it also works satisfactorily in reducing Gaussian noise as well as mixed Gaussian and impulse noise.","Nonlinear filters,
Filtering,
Adaptive filters,
Vectors,
Gaussian noise,
Statistics,
Least squares approximation,
Noise robustness,
Computer science,
Software engineering"
Local and global localization for mobile robots using visual landmarks,"Our mobile robot system uses scale-invariant visual landmarks to localize itself and build a 3D map of the environment simultaneously. As image features are not noise-free, we carry out error analysis and use Kalman filters to track the 3D landmarks, resulting in a database map with landmark positional uncertainty. By matching a set of landmarks as a whole, our robot can localize itself globally based on the database containing landmarks of sufficient distinctiveness. Experiments show that recognition of position within a map without any prior estimate can be achieved using the scale-invariant landmarks.","Mobile robots,
Robot sensing systems,
Image databases,
Spatial databases,
Sonar navigation,
Robot vision systems,
Cameras,
Computer science,
Working environment noise,
Image analysis"
Digital filter synthesis based on minimal signed digit representation,"As the complexity of digital filters is dominated by the number of multiplications, many works have focused on minimizing the complexity of multiplier blocks that compute the constant coefficient multiplications required in filters. The complexity of multiplier blocks can be significantly reduced by using an efficient number system. Although the canonical signed digit representation is commonly used as it guarantees the minimal number of additions for a constant multiplication, we propose in this paper a digital filter synthesis algorithm that is based on the minimal signed digit (MSD) representation. The MSD representation is attractive because it provides a number of forms that have the minimal number of non-zero digits for a constant. This redundancy can lead to efficient filters if a proper MSD representation is selected for each constant. In experimental results, the proposed algorithm resulted in superior filters to those generated from the CSD representation.",
Composition patterns: an approach to designing reusable,,"Object oriented modeling,
Scattering,
Software maintenance,
Unified modeling language,
Computer science,
Software design,
Educational institutions,
Software systems,
Computer languages,
Runtime"
Responder anonymity and anonymous peer-to-peer file sharing,"Data transfer over TCP/IP provides no privacy for network users. Previous research in anonymity has focused on the provision of initiator anonymity. We explore methods of adapting existing initiator-anonymous protocols to provide responder anonymity and mutual anonymity. We present anonymous peer-to-peer file sharing (APFS) protocols, which provide mutual anonymity for peer-to-peer file sharing. APFS addresses the problem of long-lived Internet services that may outlive the degradation present in current anonymous protocols. One variant of APFS makes use of unicast communication, but requires a central coordinator to bootstrap the protocol. A second variant takes advantage of multicast routing to remove the need for any central coordination point. We compare the TCP performance of the APFS protocol to existing overt file sharing systems such as Napster. In providing anonymity, APFS can double transfer times and requires that additional traffic be carried by peers, but this overhead is constant with the size of the session.","Peer to peer computing,
TCPIP,
Unicast,
Routing protocols,
Computer science,
Data privacy,
IP networks,
Multicast protocols,
Web and internet services,
Degradation"
A parallel-polled virtual output queued switch with a buffered crossbar,"Input buffered switches with virtual output queues (VOQ) are scalable to very high speeds, but require switch matrix scheduling algorithms to achieve high throughput. Existing scheduling algorithms based on parallel request grant-accept cycles cannot natively support variable length Ethernet packets. A parallel-polled VOQ (PP-VOQ) architecture is proposed that natively supports variable length packets. Small amounts of FIFO buffering within a crossbar are used. Using simulation, the PP-VOQ with buffered crossbar switch is shown to have lower switch delay at high offered loads than an iSLIP switch for both cell and variable-length packet traffic. The PP-VOQ switch does not require internal speed-up or complex reassembly mechanisms. The priority mechanism implemented in both the iSLIP and PP-VOQ switches are demonstrated to provide guaranteed rate and bounded delay for schedulable traffic.","Switches,
Packet switching,
Delay,
Scalability,
Scheduling algorithm,
Throughput,
Ethernet networks,
Traffic control,
Impedance matching,
Computer science"
Blue Gene: A vision for protein science using a petaflop supercomputer,"In December 1999, IBM announced the start of a five-year effort to build a massively parallel computer, to be applied to the study of biomolecular phenomena such as protein folding. The project has two main goals: to advance our understanding of the mechanisms behind protein folding via large-scale simulation, and to explore novel ideas in massively parallel machine architecture and software. This project should enable biomolecular simulations that are orders of magnitude larger than current technology permits. Major areas of investigation include: how to most effectively utilize this novel platform to meet our scientific goals, how to make such massively parallel machines more usable, and how to achieve performance targets, with reasonable cost, through novel machine architectures. This paper provides an overview of the Blue Gene project at IBM Research. It includes some of the plans that have been made, the intended goals, and the anticipated challenges regarding the scientific work, the software applic ation, and the hardware design.",
Real-time tracking meets online grasp planning,"Describes a synergistic integration of a grasping simulator and a real-time visual tracking system, that work in concert to (1) find an object's pose, (2) plan grasps and movement trajectories, and (3) visually monitor task execution. Starting with a CAD model of an object to be grasped, the system can find the object's pose through vision which then synchronizes the state of the robot workcell with an online, model-based grasp planning and visualization system we have developed called GraspIt. GraspIt can then plan a stable grasp for the object, and direct the robotic hand system to perform the grasp. It can also generate trajectories for the movement of the grasped object, which are used by the visual control system to monitor the task and compare the actual grasp and trajectory with the planned ones. We present experimental results using typical grasping tasks.",
Foundational proof-carrying code,"Proof-carrying code is a framework for the mechanical verification of safety properties of machine-language programs, but the problem arises of ""quis custodiat ipsos custodes"" - i.e. who verifies the verifier itself? Foundational proof-carrying code is verification from the smallest possible set of axioms, using the simplest possible verifier and the smallest possible runtime system. I describe many of the mathematical and engineering problems to be solved in the construction of a foundational proof-carrying code system.","Safety,
Logic,
Java,
Application software,
Marketing and sales,
Virtual machining,
Program processors,
Assembly"
LEneS: task scheduling for low-energy systems using variable supply voltage processors,"The work presented in this paper addresses minimization of the energy consumption of a system during system-level design. The paper focuses on scheduling techniques for architectures containing variable supply voltage processors, running dependent tasks. We introduce our new approach for low-energy scheduling (LEneS) and compare it to two other scheduling methods. LEneS is based on a list-scheduling heuristic with dynamic recalculation of priorities, and assumes a given allocation and assignment of tasks to processors. Our approach minimizes the energy by choosing the best combination of supply voltages for each task running on its processor. The set of experiments we present shows that, using the LEneS approach, we can achieve up to 28% energy savings for the tightest deadlines, and up to 77% energy savings when these deadlines are relaxed by 50%.",
Decomposition-based motion planning: a framework for real-time motion planning in high-dimensional configuration spaces,"Research in motion planning has been striving to develop faster planning algorithms in order to be able to address a wider range of applications. In this paper a novel real-time motion planning framework, called decomposition-based motion planning, is proposed. It is particularly well suited for planning problems that arise in service and field robotics. It decomposes the original planning problem into simpler sub-problems, whose successive solution empirically results in a large reduction of the overall complexity. A particular implementation of decomposition-based planning is proposed. Experiments with an eleven degree-of-freedom mobile manipulator are presented.",
Alignment of non-overlapping sequences,"This paper shows how two image sequences that have no spatial overlap between their fields of view can be aligned both in time and in space. Such alignment is possible when the two cameras are attached closely together and are moved jointly in space. The common motion induces ""similar"" changes over time within the two sequences. This correlated temporal behavior is used to recover the spatial and temporal transformations between the two sequences. The requirement of ""coherent appearance"" in standard image alignment techniques is therefore replaced by ""coherent temporal behavior"", which is often easier to satisfy. This approach to alignment can be used not only for aligning nan-overlapping sequences, but also for handling other cases that are inherently difficult for standard image alignment techniques. We demonstrate applications of this approach to three real-world problems: (i) alignment of non-overlapping sequences for generating wide-screen movies, (ii) alignment of images (sequences) obtained at significantly different zooms, for surveillance applications, and (iii) multi-sensor image alignment for multi-sensor fusion.",
An EM algorithm for estimating SPECT emission and transmission parameters from emission data only,"A maximum-likelihood (ML) expectation-maximization (EM) algorithm (called EM-IntraSPECT) is presented for simultaneously estimating single photon emission computed tomography (SPECT) emission and attenuation parameters from emission data alone. The algorithm uses the activity within the patient as transmission tomography sources, with which attenuation coefficients can he estimated. For this initial study, EM-IntraSPECT was tested on computer-simulated attenuation and emission maps representing a simplified human thorax as well as on SPECT data obtained from a physical phantom. Two evaluations were performed. First, to corroborate the idea of reconstructing attenuation parameters from emission data, attenuation parameters (/spl mu/) were estimated with the emission intensities (/spl lambda/) fixed at their true values. Accurate reconstructions of attenuation parameters were obtained. Second, emission parameters /spl lambda/ and attenuation parameters Cl were simultaneously estimated from the emission data alone. In this case there was crosstalk between estimates of /spl lambda/ and /spl mu/ and final estimates of /spl lambda/ and /spl mu/ depended on initial values. Estimates degraded significantly as the support extended out farther from the body, and an explanation for this is proposed. In the EM-IntraSPECT reconstructed attenuation images, the lungs, spine, and soft tissue were readily distinguished and had approximately correct shapes and sizes. As compared with standard EM reconstruction assuming a fix uniform attenuation map, EM-IntraSPECT-provided more uniform estimates of cardiac activity in the physical phantom study and in the simulation study with tight support, but less uniform estimates with a broad support. The new EM algorithm derived here has additional applications, including reconstructing emission and transmission projection data under a unified statistical model.","Attenuation,
Image reconstruction,
Maximum likelihood estimation,
Imaging phantoms,
Single photon emission computed tomography,
Testing,
Physics computing,
Humans,
Thorax,
Performance evaluation"
Simulation with learning agents,"We propose that learning agents (LAs) be incorporated into simulation environments in order to model the adaptive behavior of humans. These LAs adapt to specific circumstances and events during the simulation run. They would select tasks to be accomplished among a given set of tasks as the simulation progresses, or synthesize tasks for themselves based on their observations of the environment and on information they may receive from other agents. We investigate an approach in which agents are assigned goals when the simulation starts and then pursue these goals autonomously and adaptively. During the simulation, agents progressively improve their ability to accomplish their goals effectively and safely. Agents learn from their own observations and from the experience of other agents with whom they exchange information. Each LA starts with a given representation of the simulation environment from which it progressively constructs its own internal representation and uses it to make decisions. The paper describes how learning neural networks can support this approach and shows that goal based learning may be used effectively used in this context. An example simulation is presented in which agents represent manned vehicles; they are assigned the goal of traversing a dangerous metropolitan grid safely and rapidly using goal based reinforcement learning with neural networks and compared to three other algorithms.",
Wrapping server-side TCP to mask connection failures,"We present an implementation of a fault-tolerant TCP (FT-TCP) that allows a faulty server to keep its TCP connections open until it either recovers or it is failed over to a backup. The failure and recovery of the server process are completely transparent to client processes connected with it via TCP. FT-TCP does not affect the software running on a client, does not require to change the server's TCP implementation, and does not use a proxy.","Wrapping,
Application software,
Checkpointing,
Fault tolerance,
Computer science,
Bidirectional control,
Design engineering,
Control systems,
Costs,
Computer bugs"
Effect of code coverage on software reliability measurement,"Existing software reliability-growth models often over-estimate the reliability of a given program. Empirical studies suggest that the over-estimations exist because the models do not account for the nature of the testing. Every testing technique has a limit to its ability to reveal faults in a given system. Thus, as testing continues in its region of saturation, no more faults are discovered and inaccurate reliability-growth phenomena are predicted from the models. This paper presents a technique intended to solve this problem, using both time and code coverage measures for the prediction of software failures in operation. Coverage information collected during testing is used only to consider the effective portion of the test data. Execution time between test cases, which neither increases code coverage nor causes a failure, is reduced by a parameterized factor. Experiments were conducted to evaluate this technique, on a program created in a simulated environment with simulated faults, and on two industrial systems that contained tenths of ordinary faults. Two well-known reliability models, Goel-Okumoto and Musa-Okumoto, were applied to both the raw data and to the data adjusted using this technique. Results show that over-estimation of reliability is properly corrected in the cases studied. This new approach has potential, not only to achieve more accurate applications of software reliability models, but to reveal effective ways of conducting software testing.","Software reliability,
Software measurement,
Software testing,
Predictive models,
Time measurement,
Application software,
Computer science,
System testing,
Flow graphs,
Councils"
Non-metric image-based rendering for video stabilization,"We consider the problem of video stabilization: removing unwanted image perturbations due to unstable camera motions. We approach this problem from an image-based rendering (IBR) standpoint. Given an unstabilized video sequence, the task is to synthesize a new sequence as seen from a stabilized camera trajectory. This task is relatively straightforward if one has a Euclidean reconstruction of the unstabilized camera trajectory and a suitable IBR algorithm. However, it is often not feasible to obtain a Euclidean reconstruction from an arbitrary video sequence. In light of this problem, we describe IBR techniques for non-metric reconstructions, which are often much easier to obtain since they do not require camera calibration. These rendering techniques are well suited to the video stabilization problem. The key idea behind our techniques is that all measurements are specified in the image space, rather than in the non-metric space.","Rendering (computer graphics),
Cameras,
Image reconstruction,
Video sequences,
Layout,
Software algorithms,
Laboratories,
Computer science,
Calibration,
Image sequences"
A portrait of the Semantic Web in action,"Without semantically enriched content, the Web cannot reach its full potential. The authors discuss tools and techniques for generating and processing such content, thus setting a foundation upon which to build the Semantic Web. The authors put a Semantic Web language through its paces and answer questions about how people can use it, such as: how do authors generate semantic descriptions; how do agents discover these descriptions; how can agents integrate information from different sites; and how can users query the Semantic Web.","Semantic Web,
Ontologies,
Footwear,
Web pages,
HTML,
Computer science,
Knowledge engineering,
Libraries,
Markup languages,
Microstrip"
A framework for multi-valued reasoning over inconsistent viewpoints,"In requirements elicitation, different stakeholders often hold different views of how a proposed system should behave, resulting in inconsistencies between their descriptions. Consensus may not be needed for every detail, but it can be hard to determine whether a particular disagreement affects the critical properties of the system. We describe the Xbel framework for merging and reasoning about multiple, inconsistent state machine models. Xbel permits the analyst to choose how to combine information from the multiple viewpoints, where each viewpoint is described using an underlying multi-valued logic. The different values of our logics typically represent different levels of agreement. Our multi-valued model checker, Xchek, allows us to check the merged model against properties expressed in a temporal logic. The resulting framework can be used as an exploration tool to support requirements negotiation, by determining what properties are preserved for various combinations of inconsistent viewpoints.",
Incremental learning with support vector machines,"Support vector machines (SVMs) have become a popular tool for machine learning with large amounts of high dimensional data. In this paper an approach for incremental learning with support vector machines is presented, that improves the existing approach of Syed et al. (1999). An insight into the interpretability of support vectors is also given.","Machine learning,
Support vector machines,
Computer science,
Artificial intelligence,
Training data,
Testing,
Robustness"
Cheat-proof playout for centralized and distributed online games,"We explore exploits possible for cheating in real-time, multiplayer games for both client-server and distributed, serverless architectures. We offer the first formalization of cheating in online games and propose an initial set of strong solutions. We propose a protocol that has provable anti-cheating guarantees, but suffers a performance penalty. We then develop an extended version of this protocol, called asynchronous synchronization, which avoids the penalty, is serverless, offers provable anti-cheating guarantees, is robust in the face of packet loss, and provides for significantly increased communication performance. This technique is applicable to common game features as well as clustering and cell-based techniques for massively multiplayer games. Our performance claims are backed by analysis using a simulation based on real game traces.","Protocols,
Scalability,
Performance analysis,
Communication system control,
Information security,
Computer science,
Computer architecture,
Robustness,
Performance loss,
Analytical models"
PixelFlex: a reconfigurable multi-projector display system,"This paper presents PixelFlex - a spatially reconfigurable multi-projector display system. The PixelFlex system is composed of ceiling-mounted projectors, each with computer-controlled pan, tilt, zoom and focus; and a camera for closed-loop calibration. Working collectively, these controllable projectors function as a single logical display capable of being easily modified into a variety of spatial formats of differing pixel density, size and shape. New layouts are automatically calibrated within minutes to generate the accurate warping and blending functions needed to produce seamless imagery across planar display surfaces, thus giving the user the flexibility to quickly create, save and restore multiple screen configurations. Overall, PixelFlex provides a new level of automatic reconfigurability and usage, departing from the static, one-size-fits-all design of traditional large-format displays. As a front-projection system, PixelFlex can be installed in most environments with space constraints and requires little or no post-installation mechanical maintenance because of the closed-loop calibration.","Computer displays,
Calibration,
Cameras,
Application software,
Pixel,
Three dimensional displays,
Optical distortion,
Computer science,
Focusing,
Automatic control"
Saving energy with architectural and frequency adaptations for multimedia applications,"General-purpose processors are expected to be increasingly employed for multimedia workloads on systems where reducing energy consumption is an important goal. Researchers have proposed the use of two forms of hardware adaptation - architectural adaptation and dynamic voltage (and frequency) scaling or DVS - to reduce energy. This paper develops and evaluates an integrated algorithm to control both architectural adaptation and DVS targeted to multimedia applications. It also examines the interaction between the two forms of adaptation, identifying when each will perform better in isolation and when the addition of architectural adaptation will benefit DVS. Our adaptation control algorithm is effective in saving energy and exploits most of the available potential. For the applications and systems studied, DVS is consistently better than architectural adaptation in isolation. The addition of architectural adaptation to DVS benefits some applications, but not all. Finally, in a seemingly counter-intuitive result, we find that while less aggressive architectures reduce energy for fixed frequency hardware, with DVS, more aggressive architectures are often more energy efficient.",
Distributed architectures and logical-task decomposition in multimedia surveillance systems,"In the past few years, the development of complex surveillance systems has captured the interest of both the research and industrial worlds. Strong and challenging requirements of modern society are involved in this problem, which aims to increase safety and security in several application domains such as transport, tourism, home and bank security, military applications, etc. At the same time, fast improvements in microelectronics, telecommunications, and computer science make it necessary to consider new perspectives in this field. The main objective of this paper is to investigate, discuss, and evaluate the impact of distributed processing and new communication techniques on multimedia surveillance systems, which represent the so-called third-generation surveillance systems (3 GSSs). In particular, aspects related to the distribution of intelligence among multiple-processing and wide-bandwidth resources are discussed in detail. It is shown how distribution of intelligence can be obtained by a hierarchical architecture that partitions, in a dynamic way, the main logical processing tasks (i.e., representation, recognition, and communication) performed in a 3 GSS physical architecture made up of intelligent cameras, hubs, and central control rooms. The advantages of this solution are pointed out in terms of 1) increased flexibility and reconfigurability and 2) optimal allocation of available processing and bandwidth resources. Finally, a case study is analyzed that allows one to gain a deeper insight into a distributed surveillance system.","Surveillance,
Application software,
Defense industry,
Domestic safety,
Communication system security,
Military communication,
Military computing,
Microelectronics,
Computer science,
Distributed processing"
Recognizing action events from multiple viewpoints,"A first step towards an understanding of the semantic content in a video is the reliable detection and recognition of actions performed by objects. This is a difficult problem due to the enormous variability in an action's appearance when seen from different viewpoints and/or at different times. In this paper we address the recognition of actions by taking a novel approach that models actions as special types of 3D objects. Specifically, we observe that any action can be represented as a generalized cylinder, called the action cylinder. Reliable recognition is achieved by recovering the viewpoint transformation between the reference (model) and given action cylinders. A set of 8 corresponding points from time-wise corresponding cross-sections is shown to be sufficient to align the two cylinders under perspective projection. A surprising conclusion from visualizing actions as objects is that rigid, articulated, and nonrigid actions can all be modeled in a uniform framework.","Shape,
Computer science,
Object detection,
Surveillance,
Image recognition,
Biological system modeling,
Visualization,
Application software,
Computer interfaces,
Content based retrieval"
Robust declassification,,
Incremental support vector machine construction,"SVMs (support vector machines) suffer from the problem of large memory requirement and CPU time when trained in batch mode on large data sets. We overcome these limitations, and at the same time make SVMs suitable for learning with data streams, by constructing incremental learning algorithms. We first introduce and compare different incremental learning techniques, and show that they are capable of producing performance results similar to the batch algorithm, and in some cases superior condensation properties. We then consider the problem of training SVMs using stream data. Our objective is to maintain an updated representation of recent batches of data. We apply incremental schemes to the problem and show that their accuracy is comparable to the batch algorithm.","Support vector machines,
Training data,
Computer science,
Telephony,
Marketing and sales,
Support vector machine classification,
Solids,
Partitioning algorithms"
CVSSearch: searching through source code using CVS comments,"CVSSearch is a tool that searches for fragments of source code by using CVS comments. CVS is a version control system that is widely used in the open source community. Our search tool takes advantage of the fact that a CVS comment typically describes the lines of code involved in the commit and this description will typically hold for many future versions. In other words, CVSSearch allows one to better search the most recent version of the code by looking at previous versions to better understand the current version. In this paper we describe our algorithm for mapping CVS comments to the corresponding source code, present a search tool based on this technique, and discuss preliminary feedback.","Code standards,
Data mining,
Programming profession,
Computer science,
Control systems,
Software algorithms,
Feedback,
Software maintenance,
Application software,
High level languages"
Linear ridge regression with spatial constraint for generation of parametric images in dynamic positron emission tomography studies,"Due to its simplicity, computational efficiency, and reliability, weighted linear regression (WLR) is widely used for generation of parametric imaging in positron emission tomography (PET) studies, but parametric images estimated by WLR usually have high image noise level. To improve the stability and signal-to-noise ratio of the estimated parametric images, the authors have added ridge regression, a statistical technique that reduces estimation variability at the expense of a small bias. To minimize the bias, spatially smoothed images obtained with WLR are used as a constraint for ridge regression. This new algorithm consists of two steps. First, parametric images are generated by WLR and are spatially smoothed. Ridge regression is then applied using the smoothed parametric images obtained in the first step as the constraint. Since both ""generalized"" ridge regression and ""simple"" ridge regression are used in statistical applications, we evaluated specifically in this study the relative advantages of the two when incorporated for generating parametric images from dynamic O-15 water PET studies. Computer simulations of a dynamic PET study with the spatial configuration of Hoffman's brain phantom and a real human PET study were used as the data for the evaluation. Results reveal ridge regressions improve image quality of parametric images for studies with high or middle noise level. As compared to WLR, use of generalized ridge regression offers little advantage over that of simple ridge regression.","Positron emission tomography,
Noise level,
Image generation,
Computational efficiency,
Linear regression,
Stability,
Signal to noise ratio,
Application software,
Computer simulation,
Imaging phantoms"
Human tracking in multiple cameras,"Multiple cameras are needed to cover large environments for monitoring activity. To track people successfully in multiple perspective imagery, one needs to establish correspondence between objects captured in multiple cameras. We present a system for tracking people in multiple uncalibrated cameras. The system is able to discover spatial relationships between the camera fields of view and use this information to correspond between different perspective views of the same person. We employ the novel approach of finding the limits of field of view (FOV) of a camera as visible in the other cameras. Using this information, when a person is seen in one camera, we are able to predict all the other cameras in which this person will be visible. Moreover, we apply the FOV constraint to disambiguate between possible candidates of correspondence. We present results on sequences of up to three cameras with multiple people. The proposed approach is very fast compared to camera calibration based approaches.","Humans,
Cameras,
Calibration,
Computer vision,
Computerized monitoring,
Surveillance,
Computer science,
Sensor fusion,
Application software,
Feeds"
Comparing the decompositions produced by software clustering algorithms using similarity measurements,"Decomposing source code components and relations into subsystem clusters is an active area of research. Numerous clustering approaches have been proposed in the reverse engineering literature, each one using a different algorithm to identify subsystems. Since different clustering techniques may not produce identical results when applied to the same system, mechanisms that can measure the extent of these differences are needed. Some work to measure the similarity between decompositions has been done, but this work considers the assignment of source code components to clusters as the only criterion for similarity. We argue that better similarity measurements can be designed if the relations between the components are considered. The authors propose two similarity measurements that overcome certain problems in existing measurements. We also provide some suggestions on how to identify and deal with source code components that tend to contribute to poor similarity results. We conclude by presenting experimental results, and by highlighting some of the benefits of our similarity measurements.","Clustering algorithms,
Software algorithms,
Software measurement,
Software systems,
Reverse engineering,
Documentation,
Software tools,
Area measurement,
Mathematics,
Computer science"
Reducing metric sensitivity in randomized trajectory design,"This paper addresses the trajectory design for generic problems that involve: (1) complicated global constraints that include nonconvex obstacles, (2) nonlinear equations of motion that involve substantial drift due to momentum, and (3) a high-dimensional state space. Our approach to these challenging problems is to develop randomized planning algorithms based on rapidly-exploring random trees (RRTs). RRTs use metric-induced heuristics to conduct a greedy exploration of the state space; however, performance substantially degrades when the chosen metric does not adequately reflect the true cost-to-go. In this paper, we present a version of the RRT that refines its exploration strategy in the presence of a poor metric. Experiments on problems in vehicle dynamics and spacecraft navigation indicate substantial performance improvement over existing techniques.","State-space methods,
Algorithm design and analysis,
Remotely operated vehicles,
Vehicle dynamics,
Mobile robots,
Nonlinear systems,
Orbital robotics,
Cities and towns,
Dynamic programming,
Computer science"
MOUSETRAP: ultra-high-speed transition-signaling asynchronous pipelines,"A new asynchronous pipeline design is introduced for high-speed applications. The pipeline uses simple transparent latches in its datapath, and small latch controllers consisting of only a single gate per pipeline stage. This simple stage structure is combined with an efficient transition-signaling protocol between stages. Initial pre-layout HSPICE simulations of a 10-stage FIFO on a 16-bit wide datapath indicate throughput of 3.51 GigaHertz in 0.25 /spl mu/ CMOS, using a conservative process. This performance is competitive even with that of wave pipelines, without the accompanying problems of complex timing and much design effort. Additionally, the new pipeline gracefully and robustly adapts to variable-speed environments. The stage implementations are extended to fork and join structures, to handle more complex system architectures.","Pipeline processing,
Clocks,
Latches,
Throughput,
Timing,
Delay,
Protocols,
Circuits,
Computer science,
Application software"
Coding theory framework for target location in distributed sensor networks,"Distributed, real time sensor networks are essential for effective surveillance in the digitized battlefield and for environmental monitoring. We present the first systematic theory that leads to novel sensor deployment strategies for effective surveillance and target location. We represent the sensor field as a grid (two- or three-dimensional) of points (coordinates), and use the term target at a grid point at any instant in time. We use the framework of unidentified codes to determine sensor placement for unique target location. We provide coding-theoretic-bounds on the number of sensors and present methods for determining their placement in the sensor field. We also show that sensor placement for single targets provides asymptotically complete (unambiguous) location of multiple targets.",
Performance of route caching strategies in Dynamic Source Routing,"On-demand routing protocols for mobile ad hoc networks utilize route caching in different forms in order to reduce the routing overheads as well as to improve the route discovery latency. For route caches to be effective, they need to adapt to frequent topology changes. Using an on-demand protocol called Dynamic Source Routing (DSR), we study the problem of keeping the caches up-to-date in dynamic ad hoc networks. Previous studies have shown that cache staleness in DSR can significantly degrade performance. We present and evaluate three techniques to improve cache correctness in DSR namely wider error notification, route expiry mechanism with adaptive timeout selection and the use of negative caches. Simulation results show that the combination of the proposed techniques not only result in substantial improvement of both application and cache performance but also reduce the overheads.","Routing protocols,
Ad hoc networks,
Delay,
Network topology,
Degradation,
Mobile ad hoc networks,
Bandwidth,
Computer science,
Error correction,
Spread spectrum communication"
Automated support for program refactoring using invariants,"Program refactoring-transforming a program to improve readability, structure, performance, abstraction, maintainability, or other features-is not applied in practice as much as might be desired. One deterrent is the cost of detecting candidates for refactoring and of choosing the appropriate refactoring transformation. This paper demonstrates the feasibility of automatically finding places in the program that are candidates for specific refactorings. The approach uses program invariants: when a particular pattern of invariant relationships appears at a program point, a specific refactoring is applicable. Since most programs lack explicit invariants, an invariant detection tool called Daikon is used to infer the required invariants. We developed an invariant pattern matcher for several common refactorings and applied it to an existing Java code base. Numerous refactorings were detected, and one of the developers of the code base assessed their efficacy.",
"The design and implementation of COSEN, an iterative algorithm for fully 3-D listmode data","Presents coincidence-list-ordered set expectation-maximisation (COSEM), an algorithm for iterative image reconstruction directly from list-mode coincidence acquisition data. The COSEM algorithm is based on the ordered sets EM algorithm for binned data but has several extensions that makes it suitable for rotating two planar detector tomographs. The authors develop the COSEM algorithm and extend it to include analytic calculation of detection probability, noise reducing iterative filtering schemes, and on-the-fly attenuation correction methods. The authors present an adaptation of COSEM to the VaricamVG camera and show results from clinical and phantom studies.","Algorithm design and analysis,
Iterative algorithms,
Image reconstruction,
Detectors,
Filtering algorithms,
Probability,
Noise reduction,
Iterative methods,
Attenuation,
Cameras"
Reducing DRAM latencies with an integrated memory hierarchy design,"In this paper we address the severe performance gap caused by high processor clock rates and slow DRAM accesses. We show that even with an aggressive, next-generation memory system using four Direct Rambus channels and an integrated one-megabyte level-two cache, a processor still spends over half of its time stalling for L2 misses. Large cache blocks can improve performance, but only when coupled with wide memory channels. DRAM address mappings also affect performance significantly. We evaluate an aggressive prefetch unit integrated with the L2 cache and memory, controllers. By issuing prefetches only when the Rambus channels are idle, prioritizing them to maximize DRAM row buffer hits, and giving them low replacement priority, we achieve a 43% speedup across 10 of the 26 SPEC2000 benchmarks, without degrading performance an the others. With eight Rambus channels, these ten benchmarks improve to within 10% of the performance of a perfect L2 cache.","Random access memory,
Delay,
Prefetching,
Clocks,
Dynamic scheduling,
Computer science,
High performance computing,
Degradation,
Frequency,
Banking"
Computerized analysis of multiple-mammographic views: potential usefulness of special view mammograms in computer-aided diagnosis,"Purpose: To investigate the potential usefulness of special view mammograms in the computer-aided diagnosis of mammographic breast lesions. Materials and Methods: Previously, we developed a computerized method for the classification of mammographic mass lesions on standard-view mammograms, i.e., mediolateral oblique (MLO) view and/or cranial caudal (CC) views. In this study, we evaluate the performance of our computerized classification method on an independent database consisting of 70 cases (33 malignant and 37 benign cases), each having CC, MLO, and special view mammograms (spot compression or spot compression magnification views). The mass lesion identified in each of the three mammographic views was analyzed using our previously developed and trained computerized classification method. Performance in the task of distinguishing between malignant and benign lesions was evaluated using receiver operating characteristic analysis. On this independent database, we compared the performance of individual computer-extracted mammographic features, as well as the computer-estimated likelihood of malignancy, for the standard and special views. Results: Computerized analysis of special view mammograms alone in the task of distinguishing between malignant and benign lesions yielded an A/sub z/ of 0.95, which is significantly higher (p < 0.005) than that obtained from the MLO and CC views (A/sub z/ values of 0.78 and 0.75, respectively). Use of only the special views correctly classified 19 of 33 benign cases (a specificity of 58%) at 100% sensitivity, whereas use of the CC and MLO views alone correctly classified 4 and 8 of 33 benign cases (specificities of 12% and 24%, respectively). In addition, we found that the average computer output of the three views (A/sub z/ of 0.95) yielded a significantly better performance than did the maximum computer output from the mammographic views. Conclusions: Computerized analysis of special view mammograms provides an improved prediction of the benign versus malignant status of mammographic mass lesions.","Lesions,
Image coding,
Mammography,
Computer aided diagnosis,
Radiology,
Breast cancer,
Biopsy,
Cranial,
Databases,
Performance analysis"
Towards an artificial immune system for network intrusion detection: an investigation of clonal selection with a negative selection operator,"The paper describes research towards the use of an artificial immune system (AIS) for network intrusion detection. Specifically, we focus on one significant component of a complete AIS, static clonal selection with a negative selection operator, describing this system in detail. Three different data sets from the UCI repository for machine learning are used in the experiments. Two important factors, the detector sample size and the antigen sample size, are investigated in order to generate an appropriate mixture of general and specific detectors for learning non-self antigen patterns. The results of series of experiments suggest how to choose appropriate detector and antigen sample sizes. These ideal sizes allow the AIS to achieve a good non-self antigen detection rate with a very low rate of self antigen detection. We conclude that the embedded negative selection operator plays an important role in the AIS by helping it to maintain a low false positive detection rate.","Artificial immune systems,
Intrusion detection,
Detectors,
Immune system,
Humans,
Telecommunication traffic,
Biology computing,
Computer crime,
Fault detection,
Computer science"
An index structure for efficient reverse nearest neighbor queries,"The Reverse Nearest Neighbor (RNN) problem is to find all points in a given data set whose nearest neighbor is a given query point. Just like the Nearest Neighbor (NN) queries, the RNN queries appear in many practical situations such as marketing and resource management. Thus, efficient methods for the RNN queries in databases are required. The paper introduces a new index structure, the Rdnn-tree, that answers both RNN and NN queries efficiently. A single index structure is employed for a dynamic database, in contrast to the use of multiple indexes in previous work. This leads to significant savings in dynamically maintaining the index structure. The Rdnn-tree outperforms existing methods in various aspects. Experiments on both synthetic and real world data show that our index structure outperforms previous methods by a significant margin (more than 90% in terms of number of leaf nodes accessed) in RNN queries. It also shows improvement in NN queries over standard techniques. Furthermore, performance in insertion and deletion is significantly enhanced by the ability to combine multiple queries (NN and RNN) in one traversal of the tree. These facts make our index structure extremely preferable in both static and dynamic cases.","Nearest neighbor searches,
Recurrent neural networks,
Neural networks,
Indexes,
Computer science,
Resource management,
Indexing,
Database systems,
Internet,
Sorting"
On plateaued functions,"The focus of this article is on nonlinear characteristics of cryptographic Boolean functions. First, we introduce the notion of plateaued functions that have many cryptographically desirable properties. Second, we establish a sequence of strengthened inequalities on some of the most important nonlinearity criteria, including nonlinearity, avalanche, and correlation immunity, and prove that critical cases of the inequalities coincide with characterizations of plateaued functions. We then proceed to prove that plateaued functions include as a proper subset all partially bent functions that were introduced earlier by Claude Carlet (1993). This solves an interesting problem that arises naturally from previously known results on partially bent functions. In addition, we construct plateaued, but not partially bent, functions that have many properties useful in cryptography.",Boolean functions
Segmentation and boundary detection using multiscale intensity measurements,"Image segmentation is difficult because objects may differ from their background by any of a variety of properties that can be observed in some, but often not all scales. A further complication is that coarse measurements, applied to the image for detecting these properties, often average over properties of neighboring segments, making it difficult to separate the segments and to reliably detect their boundaries. Below we present a method for segmentation that generates and combines multiscale measurements of intensity contrast, texture differences, and boundary integrity. The method is based on our former algorithm SWA, which efficiently detects segments that optimize a normalized-cut like measure by recursively coarsening a graph reflecting similarities between intensities of neighboring pixels. In this process aggregates of pixels of increasing size are gradually collected to form segments. We intervene in this process by computing properties of the aggregates and modifying the graph to reflect these coarse scale measurements. This allows us to detect regions that differ by fine as well as coarse properties, and to accurately locate their boundaries. Furthermore, by combining intensity differences with measures of boundary integrity across neighboring aggregates we can detect regions separated by weak, yet consistent edges.","Image segmentation,
Aggregates,
Image edge detection,
Pixel,
Computer science,
Optimization methods,
Object detection,
Particle measurements,
Smoothing methods,
Buildings"
A passive approach for detecting shared bottlenecks,"There is a growing interest in discovering Internet path characteristics using end-to-end measurements. However, the current mechanisms for performing this task either send probe traffic, or require the sender to cooperate by time stamping the packets or sending them back-to-back. Furthermore, most of these techniques require the packets to carry sequence numbers to detect losses, and a few of them assume the existence of multicast. This paper introduces a completely passive approach for learning Internet path characteristics. In particular, we show that by noting the time difference between consecutive packets, a passive observer can cluster the flows into groups, such that all the flows in one group share the same bottleneck. Our approach relies on the observation that the correct clustering minimizes the entropy of the inter-packet spacing seen by the observer. It does not inject any probe traffic into the network, does not require any cooperation from the senders, and works with any type of traffic whether it is TCP, UDP, or even multicast.",
Democratic Integration: Self-Organized Integration of Adaptive Cues,"Sensory integration or sensor fusionthe integration of information from different modalities, cues, or sensorsis among the most fundamental problems of perception in biological and artificial systems. We propose a new architecture for adaptively integrating different cues in a self-organized manner. In Democratic Integration different cues agree on a result, and each cue adapts toward the result agreed on. In particular, discordant cues are quickly suppressed and recalibrated, while cues having been consistent with the result in the recent past are given a higher weight in the future. The architecture is tested in a face tracking scenario. Experiments show its robustness with respect to sudden changes in the environment as long as the changes disrupt only a minority of cues at the same time, although all cues may be disrupted at one time or another.",
Quasi-randomized path planning,"We propose the use of quasi-random sampling techniques for path planning in high-dimensional configuration spaces. Following similar trends from related numerical computation fields, we show several advantages offered by these techniques in comparison to random sampling. Our ideas are evaluated in the context of the probabilistic roadmap (PRM) framework. Two quasi-random variants of PRM- based planners are proposed: 1) a classical PRM with quasi-random sampling; and 2) a quasi-random lazy-PRM. Both have been implemented, and are shown through experiments to offer some performance advantages in comparison to their randomized counterparts.","Path planning,
Sampling methods,
Image sampling,
Computer science,
Computer graphics,
Design methodology,
USA Councils,
Random number generation,
Probability"
Application-layer multicast with Delaunay triangulations,"Recently, application-layer multicast has emerged as an attempt to support group applications without the need for a network-layer multicast protocol, such as IP multicast. In application-layer multicast, applications arrange themselves as a logical overlay network and transfer data within the overlay network. In this paper, Delaunay triangulations are investigated as an overlay network topology for application-layer multicast. An advantage of Delaunay triangulations is that each application can locally derive next-hop routing information without the need for a routing protocol in the overlay. A disadvantage of a Delaunay triangulation as an overlay topology is that the mapping of the overlay to the network-layer infrastructure may be suboptimal. It is shown that this disadvantage can be partially addressed with a hierarchical organization of Delaunay triangulations. Using network topology generators, the Delaunay triangulation is compared to other proposed overlay topologies for application-layer multicast.","Network topology,
Tree graphs,
Multicast protocols,
Routing protocols,
Unicast,
Delay,
Application software,
Bandwidth,
Computer science,
Global communication"
I show you how I like you - can you read it in my face? [robotics],"We report work on a LEGO robot that displays different emotional expressions in response to physical stimulation, for the purpose of social interaction with humans. This is a first step toward our longer-term goal of exploring believable emotional exchanges to achieve plausible interaction with a simple robot. Drawing inspiration from theories of human basic emotions, we implemented several prototypical expressions in the robot's caricatured face and conducted experiments to assess the recognizability of these expressions.","Human robot interaction,
Robot sensing systems,
Displays,
Computer science,
Emotion recognition,
Prototypes,
Face recognition,
Psychology,
Robot programming,
User centered design"
New frontiers: self-assembly and nanoelectronics,"In the quest for new semiconductor materials and processes, researchers focus on self-assembly, a concept that draws from diverse disciplines like chemistry, biology, material science, and electrical engineering. The following areas are examined: information theory; thermodynamics, synergetics and self-assembly; ribosome based lithography; nanofabrication by self-assembly; molecular electronics; and smart matter.","Self-assembly,
Nanoelectronics,
Semiconductor materials,
Chemistry,
Biology,
Materials science and technology,
Electrical engineering,
Information theory,
Thermodynamics,
Lithography"
Vickrey prices and shortest paths: what is an edge worth?,"We solve a shortest path problem that is motivated by recent interest in pricing networks or other computational resources. Informally, how much is an edge in a network worth to a user who wants to send data between two nodes along a shortest path? If the network is a decentralized entity, such as the Internet, in which multiple self-interested agents own different parts of the network, then auction-based pricing seems appropriate. A celebrated result from auction theory shows that the use of Vickrey pricing motivates the owners of the network resources to bid truthfully. In Vickrey's scheme, each agent is compensated in proportion to the marginal utility he brings to the auction. In the context of shortest path routing, an edge's utility is the value by which it lowers the length of the shortest path, i.e., the difference between the shortest path lengths with and without the edge. Our problem is to compute these marginal values for all the edges of the network efficiently. The naive method requires solving the single-source shortest path problem up to n times, for an n-node network. We show that the Vickrey prices for all the edges can be computed in the same asymptotic time complexity as one single-source shortest path problem. This solves an open problem posed by N. Nisan and A. Ronen (1999).","Protocols,
Shortest path problem,
Pricing,
Computer networks,
Computer science,
Internet,
IP networks,
Computer graphics,
Operations research,
Application software"
Adaptation of pitch and spectrum for HMM-based speech synthesis using MLLR,"Describes a technique for synthesizing speech with arbitrary speaker characteristics using speaker independent speech units, which we call ""average voice"" units. The technique is based on an HMM-based text-to-speech (TTS) system and maximum likelihood linear regression (MLLR) adaptation algorithm. In the HMM-based TTS system, speech synthesis units are modeled by multi-space probability distribution (MSD) HMMs which can model spectrum and pitch simultaneously in a unified framework. We derive an extension of the MLLR algorithm to apply it to MSD-HMMs. We demonstrate that a few sentences uttered by a target speaker are sufficient to adapt not only voice characteristics but also prosodic features. Synthetic speech generated from adapted models using only four sentences is very close to that from speaker dependent models trained using 450 sentences.","Hidden Markov models,
Speech synthesis,
Maximum likelihood linear regression,
Probability distribution,
Computer science,
Character generation,
Human computer interaction,
Loudspeakers,
Vectors,
Smoothing methods"
Interrelated two-way clustering: an unsupervised approach for gene expression data analysis,"DNA arrays can be used to measure the expression levels of thousands of genes simultaneously. Most research is focusing on interpretation of the meaning of the data. However, the majority of methods are supervised, with less attention having been paid to unsupervised approaches which are important when domain knowledge is incomplete or hard to obtain. In this paper we present a new framework for unsupervised analysis of gene expression data which applies an interrelated two-way clustering approach to the gene expression matrices. The goal of clustering is to find important gene patterns and perform cluster discovery on samples. The advantage of this approach is that we can dynamically use the relationships between the groups of genes and samples while iteratively clustering through both gene-dimension and sample-dimension. We illustrate the method on gene expression data from a study of multiple sclerosis patients. The experiments demonstrate the effectiveness of this approach.","Gene expression,
Data analysis,
DNA,
Information analysis,
Colon,
Supervised learning,
Entropy,
Computer science,
Data engineering,
Pharmaceuticals"
Transport level mechanisms for bandwidth aggregation on mobile hosts,"Present mobile computing does not support the simultaneous use of multiple heterogeneous network interfaces for a single transport layer connection. We describe a solution for channel aggregation at the transport layer, which provides increased bandwidth to mobile nodes. We present R-MTP (reliable multiplexing transport protocol), a rate-based reliable transport protocol capable of multiplexing data from a single application data stream across multiple network interfaces. Due to the lossy nature of wireless links in mobile environments, R-MTP tracks packet interarrival time for discrimination between congestion-based and transmission-based losses as well as better bandwidth estimation. The challenges to such a reliable protocol lie in the coordination of packets across streams with varying channel characteristics. Our experimental results validate R-MTP's bandwidth estimation and loss characterization techniques. Successful bandwidth aggregation is demonstrated in ideal and lossy environments.","Bandwidth,
Transport protocols,
Network interfaces,
Propagation losses,
Communications technology,
Availability,
Communication channels,
Delay estimation,
Mobile communication,
Computer science"
CARS: a new code generation framework for clustered ILP processors,"Clustered ILP processors are characterized by a large number of non-centralized on-chip resources grouped into clusters. Traditional code generation schemes for these processors consist of multiple phases for cluster assignment, register allocation and instruction scheduling. Most of these approaches need additional re-scheduling phases because they often do not impose finite resource constraints in all phases of code generation. These phase-ordered solutions have several drawbacks, resulting in the generation of poor performance code. Moreover the iterative/back-tracking algorithms used in some of these schemes have large turning times. In this paper we present CARS, a code generation framework for Clustered ILP processors, which combines the cluster assignment, register allocation, and instruction scheduling phases into a single code generation phase, thereby eliminating the problems associated with phase-ordered solutions. The CARS algorithm explicitly takes into account all the resource constraints at each cluster scheduling step to reduce spilling and to avoid iterative re-scheduling steps. We also present a new on-the-fly register allocation scheme developed for CARS. We describe an implementation of the proposed code generation framework and the results of a performance evaluation study using the SPEC95/2000 and MediaBench benchmarks.","Registers,
Processor scheduling,
Clustering algorithms,
Iterative algorithms,
Computer science,
Educational institutions,
Scheduling algorithm,
Dynamic scheduling,
Microarchitecture,
Microprocessors"
"Random, Ephemeral Transaction Identifiers in dynamic sensor networks","Recent advances in miniaturization and low-cost, low-power design have led to active research in large-scale, highly distributed systems of small, wireless, low-power unattended sensors and actuators. We explore the use of Random, Ephemeral TRansaction Identifiers (RETRI) in such systems, and contrast it with the typical design philosophy of using static identifiers in roles such as node addressing or efficient data naming. Instead of using statically assigned identifiers that are guaranteed to be unique, nodes randomly select probabilistically unique identifiers for each new transaction. We show how this randomized scheme can significantly improve the system's energy efficiency in contexts where that efficiency is paramount, such as energy-constrained wireless sensor networks. Benefits are realized if the typical data size is small compared to the size of an identifier, and the number of transactions seen by an individual node is small compared to the number of nodes that exist in the entire system. Our scheme is designed to scale well: identifier sizes grow with a system's density not its overall size. We quantify these benefits using an analytic model that predicts our scheme's efficiency. We also describe an implementation as applied to packet fragmentation and an experiment that validates our model.","Wireless sensor networks,
Large-scale systems,
Sensor systems,
Actuators,
Power system modeling,
Acoustic sensors,
Temperature sensors,
Broadcasting,
Sensor phenomena and characterization,
Computer science"
Diffuse optical tomography of highly heterogeneous media,"The authors investigate the performance of diffuse optical tomography to image highly heterogeneous media, such as breast tissue, as a function of background heterogeneity. To model the background heterogeneity, they have employed the functional information derived from Gadolinium-enhanced magnetic resonance images of the breast. The authors demonstrate that overall image quality and quantification accuracy worsens as the background heterogeneity increases. Furthermore they confirm the appearance of characteristic artifacts at the boundaries that scale with background heterogeneity. These artifacts are very similar to the ones seen in clinical examinations and can be misinterpreted as actual objects if not accounted for. To eliminate the artifacts and improve the overall image reconstruction, the authors apply a data-correction algorithm that yields superior reconstruction results and is virtually independent of the degree of the background heterogeneity.","Tomography,
Nonhomogeneous media,
US Department of Transportation,
Optical scattering,
Image reconstruction,
Biomedical optical imaging,
Optical saturation,
Magnetic resonance,
Breast,
Optical imaging"
View-invariance in action recognition,"Automatically understanding human actions using motion trajectories derived from video sequences is a very challenging problem. Since an action takes place in 3-D, and is projected on 2-D image, depending on the viewpoint of the camera, the projected 2-D trajectory may vary. Therefore, the same action may have very different trajectories, and trajectories of different actions may look the same. This may create a problem in interpretation of trajectories at the higher level. However, if the representation of actions only captures characteristics, which are view-invariant, then the higher level interpretation can proceed without any ambiguity. In most of the current work on action recognition, the issue of view invariance has been ignored. Therefore, proposed methods do not succeed in more general situations. In this paper, we first present a view-invariant representation of action consisting of dynamic instants and intervals, which is computed using spatiotemporal curvature of a trajectory. Then this representation is used by our system to learn human actions without any training. The system is able to incrementally learn different actions starting with no model. It can discover instances of the same action performed by different people, and in different viewpoints.",
Look-ahead based fuzzy decision tree induction,"Decision tree induction is typically based on a top-down greedy algorithm that makes locally optimal decisions at each node. Due to the greedy and local nature of the decisions made at each node, there is considerable possibility of instances at the node being split along branches such that instances along some or all of the branches require a large number of additional nodes for classification. In this paper, we present a computationally efficient way of incorporating look-ahead into fuzzy decision tree induction. Our algorithm is based on establishing the decision at each internal node by jointly optimizing the node splitting criterion (information gain or gain ratio) and the classifiability of instances along each branch of the node. Simulations results confirm that the use of the proposed look-ahead method leads to smaller decision trees and as a consequence better test performance.","Decision trees,
Testing,
Greedy algorithms,
Classification tree analysis,
Fuzzy systems,
Uncertainty,
Statistics,
Computational modeling,
Computer science,
Entropy"
A new type system for secure information flow,,
Design and evaluation of menu systems for immersive virtual environments,"Interfaces for system control tasks in virtual environments (VEs) have not been extensively studied. The paper focuses on various types of menu systems to be used in such environments. We describe the design of the TULIP menu, a menu system using Pinch Gloves/sup TM/, and compare it to two common alternatives: floating menus and pen and tablet menus. These three menus were compared in an empirical evaluation. The pen and tablet menu was found to be significantly faster, while users had a preference for TULIP. Subjective discomfort levels were also higher with the floating menus and pen and tablet.",
Development of the standard CubeSat deployer and a CubeSat class PicoSatellite,"Cal Poly students are participating in the development of a new class of picosatellite, the CubeSat. CubeSats are ideal as space development projects for universities around the world. In addition to their significant role in educating space scientists and engineers, CubeSats provide a low-cost platform for testing and space qualification of the next generation of small payloads in space. A key component of the project is the development of a standard CubeSat deployer. This deployer is capable of releasing a number of CubeSats as secondary payloads on a wide range of launchers. The standard deployer requires all CubeSats to conform to common physical requirements, and share a standard deployer interface. CubeSat development time and cost can be significantly reduced by the development of standards that are shared by a large number of spacecraft.","Standards development,
Payloads,
Satellites,
Space vehicles,
Aerospace engineering,
Costs,
Standardization,
Aerospace industry,
Manufacturing,
Computer science"
Modeling the branching characteristics and efficiency gains in global multicast trees,"We investigate two issues. First, what level of efficiency gain does multicast offer over unicast? Second, how does the shape of multicast trees impact multicast efficiency? We address the first issue by developing a metric to measure multicast efficiency for a number of real and synthetic datasets. We find that group sizes as small as 20 to 40 receivers offer a 60-70% reduction in the number of links traversed compared to separately delivered unicast streams. Addressing the second issue, we have found that almost all multicast trees have similar characteristics in terms of key parameters such as depth, degree frequency and average degree. A final contribution of our work is that we have taken multicast group membership data and multicast path data and compiled datasets which can be used to generate large, realistic multicast trees.","Unicast,
Bandwidth,
Shape,
Frequency,
Internet,
Character generation,
Assembly,
Topology,
Computer science,
Tree data structures"
The study and optimization of new micropattern gaseous detectors for high-rate applications,"We performed a new series of systematic studies of gain and rate characteristics of several micropattern gaseous detectors. Extending earlier studies, characteristics ere measured at various pressures and gas mixtures at a wide range of primary charges, and also when the whole area of the detectors was irradiated with a high-intensity X-ray beam. Several new effects were discovered, common to all tested detectors, which define fundamental limits of operation. The results of these studies allow us to identify several concrete wags of improving the performance of micropattern detectors and to suggest that in some applications, resistive plate chambers may constitute a valid alternative. Being protected from damaging discharges by the resistive electrodes, these detectors feature high gain, high rate capability (10/sup 5/ Hz/mm/sup 2/), good position resolution (better than 30 /spl mu/m), and excellent timing (50 ps /spl sigma/).",
Precise sub-pixel estimation on area-based matching,"Area-based matching is a common procedure in various fields such as image-based measurements, stereo image processing, and fluidics. Sub-pixel estimation using parabola fitting over three points with their similarity measures is also a common method to increase the resolution of matching. However, few investigations or studies concerning the characteristics of this estimation have been reported. In this paper we have analyzed the sub-pixel estimation error by using an approximate image function and three kinds of similarity measures for matching. The results illustrate some inherently problematic phenomena such as so called ""pixel-locking"". In addition to this, we propose a new algorithm to greatly reduce sub-pixel estimation error. This method is independent from the similarity measure and quite simple to implement. The advantage of our novel method is confirmed through experiments using three different types of images.","Estimation error,
Optical imaging,
Area measurement,
Fluidics,
Histograms,
Pixel,
Transfer functions,
Apertures,
Information science,
Image processing"
Self-tuned remote execution for pervasive computing,"Pervasive computing creates environments saturated with computing and communication capability, yet gracefully integrated with human users. Remote execution has a natural role to play, in such environments, since it lets applications simultaneously leverage the mobility of small devices and the greater resources of large devices. In this paper, we describe Spectra, a remote execution system designed for pervasive environments. Spectra monitors resources such as battery, energy and file cache state which are especially important for mobile clients. It also dynamically balances energy use and quality goals with traditional performance concerns to decide where to locate functionality. Finally, Spectra is self-tuning-it does not require applications to explicitly specify intended resource usage. Instead, it monitors application behavior, learns functions predicting their resource usage, and uses the information to anticipate future behavior.",
Adaptive multipath source routing in ad hoc networks,"In this paper, we propose a new multipath routing protocol for ad hoc wireless networks-multipath source routing (MSR), which is an extension of DSR (dynamic source routing). Based on the measurement of RTT, we propose a scheme to distribute load among multiple paths. The simulation results show that our approach improves the packet delivery ratio and the throughput of TCP and UDP, and reduces the end-to-end delay and the average queue size, while adding little overhead. As a result, MSR decreases the network congestion and increases the path fault tolerance quite well.",
An experience in collaborative software engineering education,"Large-scale software development requires the interaction of specialists from different fields who must communicate their decisions and coordinate their activities. As global software development becomes mainstream, software engineers face new challenges for which they have received little or no training. To help a new generation of software developers better understand the industry's globalization and familiarize them with distributed, collaborative development, we designed a course entitled the Distributed Software Engineering Laboratory. In the class, pairs of students from different countries work as a virtual organization overseeing the whole software development process. We describe the lessons we have learned in this course and propose a framework useful in dealing with some of the difficulties participants face.","Collaborative software,
Engineering education,
Programming,
Large-scale systems,
Industrial training,
Computer industry,
Globalization,
Collaborative work,
Software engineering,
Laboratories"
NPACI: rocks: tools and techniques for easily deploying manageable Linux clusters,,"Linux,
Hardware,
Security,
Robustness,
Supercomputers,
Monitoring,
Large-scale systems,
Engines,
Power generation economics,
Personnel"
Traffic modeling and characterization for UMTS networks,"In this paper, we present a synthetic traffic model for the Universal Mobile Telecommunication Systems (UMTS) based on measured trace data. The analysis and scaling process of the measured trace data with respect to different bandwidth classes constitutes the basic concept of the UMTS traffic characterization. Furthermore, we introduce an aggregated traffic model for UMTS networks that is analytically tractable. The key idea of this aggregated traffic model lies in customizing the batch Markovian arrival process (BMAP) such that different packet sizes of IP packets are represented by rewards (ie, batch sizes of arrivals) of the BMAP. The effectiveness of the customized BMAP for modeling UMTS traffic is illustrated using the synthetic traffic model previously presented.","Telecommunication traffic,
Traffic control,
3G mobile communication,
IP networks,
Electrical capacitance tomography,
ISDN,
Bandwidth,
Radio access networks,
Modems,
Computer science"
Towards compressing Web graphs,"We consider the problem of compressing graphs of the link structure of the World Wide Web. We provide efficient algorithms for such compression that are motivated by random graph models for describing the Web. The algorithms are based on reducing the compression problem to the problem of finding a minimum spanning free in a directed graph related to the original link graph. The performance of the algorithms on graphs generated by the random graph models suggests that by taking advantage of the link structure of the Web, one may achieve significantly better compression than natural Huffman-based schemes. We also provide hardness results demonstrating limitations on natural extensions of our approach.","Web sites,
Tree graphs,
Testing,
Web pages,
Search engines,
Computer science,
Engineering profession,
Electronic mail,
Prototypes,
Compression algorithms"
An index-based approach for similarity search supporting time warping in large sequence databases,"This paper proposes a new novel method for similarity search that supports time warping in large sequence databases. Time warping enables finding sequences with similar patterns even when they are of different lengths. Previous methods for processing similarity search that supports time warping fail to employ multi-dimensional indexes without false dismissal since the time warping distance does not satisfy the triangular inequality. Our primary goal is to innovate on search performance without permitting any false dismissal. To attain this goal, we devise a new distance function D/sub tw-lb/ that consistently underestimates the time warping distance and also satisfies the triangular inequality D/sub tw-lb/ uses a 4-tuple feature vector that is extracted from each sequence and is invariant to time warping. For efficient processing of similarity search, we employ a multi-dimensional index that uses the 4-tuple feature vector as indexing attributes and D/sub tw-lb/ as a distance function. The extensive experimental results reveal that our method achieves significant speedup up to 43 times with real-world S&P 500 stock data and up to 720 times with very large synthetic data.","Databases,
Euclidean distance,
Computer science,
Length measurement,
Data engineering,
Indexing,
Exchange rates,
Temperature,
Marketing and sales,
Data mining"
Practical programmable packets,"We present SNAP (safe and nimble active packets), a new scheme for programmable (or active) packets centered around a new low-level packet language. Unlike previous active packet approaches, SNAP is practical: namely, adding significant flexibility over IP without compromising safety and security or efficiency. In this paper we show how to compile from the well-known active picket language PLAN to SNAP, showing that SNAP retains PLAN's flexibility; give proof sketches of its novel approach to resource control; and present experimental data showing SNAP attains performance very close to that of a software IP router.","Safety,
Data security,
Resource management,
Contracts,
Protection,
Information science,
Software performance,
Explosives,
IP networks,
Application software"
Multilevel approach to full-chip gridless routing,"Presents a novel gridless detailed routing approach based on multilevel optimization. The multilevel framework with recursive coarsening and refinement in a ""V-shaped"" flow allows efficient scaling of the gridless detailed router to very large designs. The downward pass of recursive coarsening builds the representations of routing regions at different levels, while the upward pass of iterative refinement allows a gradual convergence to a globally optimized solution. The use of a multicommodity flow-based routing algorithm for the initial routing at the coarsest level and a modified maze algorithm for the refinement at each level considerably improves the quality of gridless routing results. Compared with the recently published gridless detailed routing algorithm using wire planning, the multilevel gridless routing algorithm is 3/spl times/ to 75/spl times/faster. We also compared the multilevel framework with a recently developed three-level routing approach and a traditional hierarchical routing approach.","Routing,
Tiles,
Iterative algorithms,
Wire,
Partitioning algorithms,
Very large scale integration,
Computer science,
Delay,
Silicon,
Approximation algorithms"
Separation of duties for access control enforcement in workflow environments,"Separation of duty, as a security principle, has as its primary objective the prevention of fraud and errors. This objective is achieved by disseminating the tasks and associated privileges for a specific business process among multiple users. This principle is demonstrated in the traditional example of separation of duty found in the requirement of two signatures on a check. Previous work on separation of duty requirements often explored implementations based on role-based access control (RBAC) principles. These implementations are concerned with constraining the associations between RBAC components, namely users, roles, and permissions. Enforcement of the separation of duty requirements, although an integrity requirement, thus relies on an access control service that is sensitive to the separation of duty requirements. A distinction between separation of duty requirements that can be enforced in administrative environments, namely static separation of duty, and requirements that can only be enforced in a run-time environment, namely dynamic separation of duty, is required. It is argued that RBAC does not support the complex work processes often associated with separation of duty requirements, particularly with dynamic separation of duty. The workflow environment, being primarily concerned with the facilitation of complex work processes, provides a context in which the specification of separation of duty requirements can be studied. This paper presents the conflicting entities administration paradigm for the specification of static and dynamic separation of duty requirements in the workflow environment.",
Fine-grained layered multicast,"Traditional approaches to receiver-driven layered multicast have advocated the benefits of cumulative layering, which can enable coarse-grained congestion control that complies with TCP-friendliness equations over large time scales. In this paper, we quantify the costs and benefits of using non-cumulative layering and present a new, scalable multicast congestion control scheme which provides a fine-grained approximation to the behavior of TCP additive increase/multiplicative decrease (AIMD). In contrast to the conventional wisdom, we demonstrate that fine-grained rate adjustment can be achieved with only modest increases in the number of layers and aggregate bandwidth consumption, while using only a small constant number of control messages to perform either additive increase or multiplicative decrease.","Subscriptions,
Encoding,
Bandwidth,
Computer science,
Scheduling algorithm,
Equations,
Aggregates,
Jacobian matrices,
Frequency,
Engineering profession"
An OS interface for active routers,"This paper describes an operating system (OS) interface for active routers. This interface allows code loaded into active routers to access the router's memory, communication, and computational resources on behalf of different packet flows. In addition to motivating and describing the interface, the paper also reports our experiences implementing the interface in three different OS environments: Scout, the OSKit, and the esokernel.","Operating systems,
Computer architecture,
Computer science,
Computer interfaces,
Aerospace industry,
Programming environments,
Transfer functions,
Communication system control,
Cities and towns,
Electronic mail"
Human tracking with mixtures of trees,"Tree-structured probabilistic models admit simple, fast inference. However they are not well suited to phenonena such as occlusion, where multiple components of an object may disappear simultaneously. We address this problem with mixtures of trees, and demonstrate an efficient and compact representation of this mixture, which admits simple learning and inference algorithms. We use this method to build an automated tracker for Muybridge sequences of a variety of human activities. Tracking is difficult, because the temporal dependencies rule out simple inference methods. We show how to use our model for efficient inference, using a method that employs alternate spatial and temporal inference. The result is a cracker that (a) uses a very loose motion model, and so can track many different activities at a variable frame rate and (b) is entirely, automatic.","Humans,
Assembly,
Biological system modeling,
Tracking,
Computer science,
Inference algorithms,
Torso,
Object recognition"
Document restoration using 3D shape: a general deskewing algorithm for arbitrarily warped documents,"We present a framework for restoring arbitrarily warped and deformed documents to their original planar shape. The impetus for this work is the need for tools and techniques to help digitally preserve and restore fragile manuscripts. Current digitization is performed under the assumption that the documents are flat, with subsequent image-processing and restoration algorithms either relying on this assumption or attempting to overcome it without shape information. Although most manuscripts were originally flat, many become deformed from damage and deterioration. Physical flattening is not possible without risking further, possibly irreversible, damage. Our framework addresses this restoration problem with two primary contributions. First, we present a working 3D digitization setup that acquires a 3D model with accurate shape-to-texture registration under multiple lighting conditions. Second, we show how the 3D model and a mass-spring particle system can be used together as a framework for digital flattening. We show that this restoration process can correct document deformations and can significantly improve subsequent document analysis.","Shape,
Image restoration,
Optical materials,
Optical character recognition software,
Optical sensors,
Software libraries,
Optical distortion,
Facsimile,
Computer science,
Springs"
Evaluation of center-line extraction algorithms in quantitative coronary angiography,"Objective testing of centerline extraction accuracy in quantitative coronary angiography (QCA) algorithms is a very difficult task. Standard tools for this task are not yet available. The authors present a simulation tool that generates synthetic angiographic images of a single coronary artery with predetermined centerline and diameter function. This simulation tool was used creating a library of images for the objective comparison and evaluation of QCA algorithms. This technique also provides the means for understanding the relationship between the algorithms performance and limitations and the vessel's geometrical parameters. In this paper, two algorithms are evaluated and the results are presented.","Angiography,
Quantum cellular automata,
Biomedical engineering,
Arteries,
Brightness,
Active contours,
Skeleton,
Testing,
Image generation,
Libraries"
Calculation of penalties due to polarization effects in a long-haul WDM system using a Stokes parameter model,"We derive a Stokes parameter model to calculate the penalties due to the combination of polarization mode dispersion (PMD), polarization dependent loss (PDL), and polarization dependent gain (PDG) in long-haul, dense wavelength division multiplexed (WDM) systems. In this model, we follow the Stokes parameters for the signal and the noise in each channel instead of following the full time domain behavior of each channel. This approach allows us to determine the statistical distribution of penalties with up to 10/sup 5/ fiber realizations and 40 channels. We validate this model to the extent possible by comparison to full numerical simulations. Using this model, we find that the interaction of PMD and PDL is the major source of penalties and that the effect of PDG is negligible in WDM systems with more than ten channels.","Wavelength division multiplexing,
Optical fiber polarization,
Stokes parameters,
Page description languages,
Polarization mode dispersion,
Optical fibers,
Computer science,
Statistical distributions,
Numerical simulation,
WDM networks"
Elastic registration of fMRI data using Bezier-spline transformations,"A three-dimensional (3-D) elastic registration algorithm has been developed to find a veridical transformation that maps activation patterns from functional magnetic resonance imaging (fMRI) experiments onto a 3-D high-resolution anatomical dataset. The proposed algorithm uses trilinear Bezier-splines and a 3-D voxel-based optimization technique to determine the transformation that maps the functional data onto the coordinate system of the anatomical dataset. Simple conditions are presented which guarantee that the data are mapped one-to-one on each other. Two voxel-based similarity measures, the linear correlation coefficient and the entropy correlation coefficient, are used. Their performance with respect to the registration of fMRI data is compared. Tests on simulated and real data have been performed to evaluate the accuracy of the method. Our results demonstrate that subvoxel accuracy can be achieved even for noisy low-resolution multislice datasets with local distortions up to 10 mm. Although the method is optimized for the registration of functional and anatomical MR images, it can also be used for solving other elastic registration problems.",
Fractal analysis of bone X-ray tomographic microscopy projections,"Fractal analysis of bone X-ray images has received much interest recently for the diagnosis of bone disease. Here, the authors propose a fractal analysis of bone X-ray tomographic microscopy (XTM) projections. The aim of the study is to establish whether or not there is a correlation between three-dimensional (3-D) trabecular changes and two-dimensional (2-D) fractal descriptors. Using a highly collimated beam, 3-D bone X-ray tomographic images were obtained. Trabecular bone loss was simulated using a mathematical morphology method. Then, 2-D projections were generated in each of the three orthogonal directions. Finally, the model of fractional Brownian motion (fBm) was used on bone XTM 2-D projections to characterize changes in bone structure that occur during disease, such a simulation of bone loss. Results indicate that fBm is a robust texture model allowing quantification of simulations of trabecular bone changes.","Fractals,
Tomography,
Microscopy,
X-ray imaging,
Bone diseases,
Cancellous bone,
Image analysis,
Two dimensional displays,
Collimators,
Morphology"
Demand-driven service differentiation in cluster-based network servers,"Service differentiation that provides prioritized service qualities to multiple classes of client requests can effectively utilize available server resources. This paper studies how demand-driven service differentiation in terms of end-user performance can be supported in cluster-based network servers. Our objective is to deliver better services to high priority request classes without over-sacrificing low priority classes. To achieve this objective, we propose a dynamic scheduling scheme, called DDSD that adapts to fluctuating request resource demands by periodically repartitioning servers. This scheme also employs priority-based admission control to drop excessive user requests and achieve soft performance guarantees. For each scheduling period, our scheme monitors the system status and uses a queuing model to approximate server behaviors and guide resource allocation. Our experiments show that the proposed technique achieves demand-driven service differentiation while maximizing resource utilization and that it can substantially outperform static server partitioning.","Intelligent networks,
Network servers,
Resource management,
Bandwidth,
Admission control,
Web server,
Computer science,
Dynamic scheduling,
Computer network management,
Quality management"
Recursive diagonal torus: an interconnection network for massively parallel computers,"Recursive Diagonal Torus (RDT), a class of interconnection network is proposed for massively parallel computers with up to 2/sup 16/ nodes. By making the best use of a recursively structured diagonal mesh (torus) connection, the RDT has a smaller diameter (e.g., it is 11 for 2/sup 10/ nodes) with a smaller number of links per node (i.e., 8 links per node) than those of the hypercube. A simple routing algorithm, called vector routing, which is near-optimal and easy to implement is also proposed. Although the congestion on upper rank tori sometimes degrades the performance under the random traffic, the RDT provides much better performance than that of a 2D/3D torus in most cases and, under hot spot traffic, the RDT provides much better performance than that of a 2D/3D/4D torus. The RDT router chip which provides a message multicast for maintaining cache consistency is available. Using the 0.5 /spl mu/m BICMOS SOG technology, versatile functions, including hierarchical multicasting, combining acknowledge packets, shooting down/restart mechanism, and time-out/setup mechanisms, work at a 60 MHz clock rate.","Multiprocessor interconnection networks,
Computer networks,
Concurrent computing,
Routing,
Mesh networks,
Hypercubes,
Degradation,
Multicast algorithms,
Network topology,
Computer science"
Implementation of an authenticated dictionary with skip lists and commutative hashing,We present the software architecture and implementation of an efficient data structure for dynamically maintaining an authenticated dictionary. The building blocks of the data structure are skip lists and one-way commutative hash functions. We also present the results of a preliminary experiment on the performance of the data structure. Applications of our work include certificate revocation in a public key infrastructure and the publication of data collections on the Internet.,"Dictionaries,
Data structures,
Authentication,
Internet,
Public key,
Computer science,
Software architecture,
Application software,
Algorithm design and analysis,
High performance computing"
Shared kernel models for class conditional density estimation,We present probabilistic models which are suitable for class conditional density estimation and can be regarded as shared kernel models where sharing means that each kernel may contribute to the estimation of the conditional densities of an classes. We first propose a model that constitutes an adaptation of the classical radial basis function (RBF) network (with full sharing of kernels among classes) where the outputs represent class conditional densities. In the opposite direction is the approach of separate mixtures model where the density of each class is estimated using a separate mixture density (no sharing of kernels among classes). We present a general model that allows for the expression of intermediate cases where the degree of kernel sharing can be specified through an extra model parameter. This general model encompasses both the above mentioned models as special cases. In all proposed models the training process is treated as a maximum likelihood problem and expectation-maximization algorithms have been derived for adjusting the model parameters.,"Kernel,
Maximum likelihood estimation,
Neural networks,
Density functional theory,
Probability,
Pattern recognition,
Computer science,
Unsupervised learning,
Radial basis function networks"
A new approach of geodesic reconstruction for drusen segmentation in eye fundus images,"Segmentation of bright blobs in an image is an important problem in computer vision and particularly in biomedical imaging. In retinal angiography, segmentation of drusen, a yellowish deposit located on the retina, is a serious challenge in proper diagnosis and prevention of further complications. Drusen extraction using classic segmentation methods does not lead to good results. We present a new segmentation method based on new transformations we introduced in mathematical morphology. It is based on the search for a new class of regional maxima components of the image. These maxima correspond to the regions inside the drusen. We present experimental results for drusen extraction using images containing examples having different types and shapes of drusen. We also apply our segmentation technique to two important cases of dynamic sequences of drusen images. The first case is for tracking the average gray level of a particular drusen in a sequence of angiographic images during a fluorescein exam. The second case is for registration and matching of two angiographic images from widely spaced exams in order to characterize the evolution of drusen.","Image reconstruction,
Image segmentation,
Retina,
Angiography,
Morphology,
Shape,
Computer vision,
Biomedical imaging,
Image edge detection,
Aging"
Aggregated multicast: an approach to reduce multicast state,"IP multicast suffers from a scalability problem with the number of concurrently active multicast groups because it requires a router to keep the forwarding state for every multicast tree passing through it and the number of forwarding entries grows with the number of groups. In this paper, we propose an approach to reduce the multicast forwarding state. In our approach, multiple groups are forced to share a single delivery tree. We discuss the advantages and some implementation issues of our approach, and conclude that it is feasible and promising. We then propose metrics to quantify state reduction and analyze the bounds on state reduction of our approach. Finally, we use simulations to verify our analytical bounds and quantify the state reduction. These initial simulation results suggest that our method can reduce multicast state significantly.","Scalability,
Computer science,
Aggregates,
Analytical models,
Internet,
Delay,
Unicast,
Filters,
Multicast algorithms,
Routing"
Real-time feature tracking and outlier rejection with changes in illumination,"We develop an efficient algorithm to track point features supported by image patches undergoing affine deformations and changes in illumination. The algorithm is based on a combined model of geometry and photometry, that is used to track features as well as to detect outliers in a hypothesis testing framework. The algorithm runs in real time on a personal computer; and is available to the public.","Lighting,
Layout,
Robustness,
Target tracking,
Real time systems,
Application software,
Computer science,
Photometry,
Testing,
Microcomputers"
Constructing adaptive software in distributed systems,"Adaptive software that can react to changes in the execution environment or user requirements by switching algorithms at run time is powerful yet difficult to implement, especially in distributed systems. This paper describes a software architecture for constructing such adaptive software and a graceful adaptive protocol that allows adaptations to be made in a coordinated manner across hosts transparently to the application. A realization of the architecture based on Cactus, a system for constructing highly configurable distributed services and protocols, is also presented. The approach is illustrated by outlining examples of adaptive components from a group communication service.","Software systems,
Computer architecture,
Protocols,
Application software,
Software architecture,
Middleware,
Computer science,
Software performance,
Security,
Buildings"
"Markov chains, classifiers, and intrusion detection",,"Intrusion detection,
Detectors,
Predictive models,
Measurement,
Computer science,
Information security,
Pattern matching,
Prototypes"
Bitwidth cognizant architecture synthesis of custom hardware accelerators,"Program-in chip-out (PICO) is a system for automatically synthesizing embedded hardware accelerators from loop nests specified in the C programming language. A key issue confronted when designing such accelerators is the optimization of hardware by exploiting information that is known about the varying number of bits required to represent and process operands. In this paper, we describe the handling and exploitation of integer bitwidth in PICO. A bitwidth analysis procedure is used to determine bitwidth requirements for all integer variables and operations in a C application. Given known bitwidths for all variables, complex problems arise when determining a program schedule that specifies on which function unit (FU) and at what time each operation executes. If operations are assigned to FUs with no knowledge of bitwidth, bitwidth-related cost benefit is lost when each unit is built to accommodate the widest operation assigned. By carefully placing operations of similar width on the same unit, hardware costs are decreased. This problem is addressed using a preliminary clustering of operations that is based jointly on width and implementation cost. These clusters are then honored during resource allocation and operation scheduling to create an efficient width-conscious design. Experimental results show that exploiting integer bitwidth substantially reduces the gate count of PICO-synthesized hardware accelerators across a range of applications.","Hardware,
Acceleration,
Costs,
Laboratories,
Resource management,
Energy consumption,
Computer science,
Logic arrays,
Control system synthesis,
Computer languages"
Online facility location,"We consider the online variant of facility location, in which demand points arrive one at a time and we must maintain a set of facilities to service these points. We provide a randomized online O(1)-competitive algorithm in the case where points arrive in random order. If points are ordered adversarially, we show that no algorithm can be constant-competitive, and provide an O(log n)-competitive algorithm. Our algorithms are randomized and the analysis depends heavily on the concept of expected waiting time. We also combine our techniques with those of M. Charikar and S. Guha (1999) to provide a linear-time constant approximation for the offline facility location problem.","Costs,
Network servers,
Web pages,
Algorithm design and analysis,
Linear approximation,
Cables,
Computer science,
Application software,
Extraterrestrial measurements"
High-Performance Wide-Area Optical Tracking: The HiBall Tracking System,"Since the early 1980s, the Tracker Project at the University of North Carolina at Chapel Hill has been working on wide-area head tracking for virtual and augmented environments. Our long-term goal has been to achieve the high performance required for accurate visual simulation throughout our entire laboratory, beyond into the hallways, and eventually even outdoors. In this article, we present results and a complete description of our most recent electro-optical system, the HiBall Tracking System. In particular, we discuss motivation for the geometric configuration and describe the novel optical, mechanical, electronic, and algorithmic aspects that enable unprecedented speed, resolution, accuracy, robustness, and flexibility.",
Dynamic network flow optimization models for air vehicle resource allocation,"A weapon system consisting of a swarm of air vehicles whose mission is to search for, classify, attack, and perform battle damage assessment, is considered. It is assumed that the target field information is communicated to all the elements of the swarm as it becomes available. A network flow optimization problem is posed whose readily obtained solution yields the optimum resource allocation among the air vehicles in the swarm. Hence, the periodic reapplication of the centralized optimization algorithm yields the benefit of cooperative feedback control.","Vehicle dynamics,
Vehicles,
Resource management,
Weapons,
Vehicle detection,
Computer science,
Operations research,
Neodymium,
Aerospace control,
Force control"
Adaptive approaches to relieving broadcast storms in a wireless multihop mobile ad hoc network,"In a multihop mobile ad hoc network, broadcasting is an elementary operation to support many applications. In (Ni et al., 1999), it is shown that naively broadcasting by flooding may cause serious redundancy, contention, and collision in the network, which we refer to as the broadcast storm problem. Several threshold-based schemes are shown to perform better than flooding in (Ni et al., 1999). However, how to choose thresholds also poses a dilemma between reachability and efficiency under different host densities. We propose several adaptive schemes, which can dynamically adjust thresholds based on local connectivity information. Simulation results show that these adaptive schemes can offer better reachability as well as efficiency as compared to the results in (Ni et al., 1999).","Broadcasting,
Storms,
Intelligent networks,
Spread spectrum communication,
Mobile ad hoc networks,
Computer science,
Mobile communication,
Relays,
Routing,
Delay"
Steiner-optimal data replication in tree networks with storage costs,"We consider the problem of placing copies of objects at multiple locations in a distributed system, whose interconnection network is a tree, in order to minimize the cost of servicing read and write requests to the objects. We assume that the tree nodes have limited storage and the number of copies permitted may be limited. The set of nodes that have a copy of the object, called replica nodes, constitute the replica set of the object. Read requests of a node are serviced from the closest replica node. Write requests of a node are propagated to all the replicas of the object using a minimum cost Steiner tree that includes the writer and all replica nodes. The total cost associated with a replica set equals the cost of servicing all the read and write requests, plus the storage cost at all the replica nodes. We are interested in finding a replica set with minimum total cost, i.e. a Steiner-optimal replica set. Given a tree with n nodes, we provide an O(n/sup 6/p/sup 2/)-time algorithm for finding a Steiner-optimal replica set of size p, taking into consideration the read, write, and storage costs. Our algorithm can also find a Steiner-optimal replica set for a tree with n nodes in time O(n/sup 8/). We also demonstrate that the policy used to propagate write requests to all the replica nodes in the network affects the cost and configuration of the optimal replica set for the object.","Intelligent networks,
Cost function,
Surface-mount technology,
Computer science,
Web sites,
Web pages,
Internet,
System performance,
Multiprocessor interconnection networks"
Parallel quantum-inspired genetic algorithm for combinatorial optimization problem,"This paper proposes a new parallel evolutionary algorithm called parallel quantum-inspired genetic algorithm (PQGA). Quantum-inspired genetic algorithm (QGA) is based on the concept and principles of quantum computing such as qubits and superposition of states. Instead of binary, numeric, or symbolic representation, by adopting the qubit chromosome as a representation, QGA can represent a linear superposition of solutions due to its probabilistic representation. QGA is suitable for parallel structures because of rapid convergence and good global search capability. That is, QGA is able to possess the two characteristics of exploration and exploitation simultaneously. The effectiveness and the applicability of PQGA are demonstrated by experimental results on the knapsack problem, which is a well-known combinatorial optimization problem. The results show that PQGA is superior to QGA as well as other conventional genetic algorithms.","Genetic algorithms,
Quantum computing,
Evolutionary computation,
Convergence,
Stochastic processes,
Evolution (biology),
Quantum mechanics,
Computer science,
Biological cells,
Optimization methods"
"Making knowledge visible through intranet knowledge maps: concepts, elements, cases","Establishes the conceptual and empirical basis for an innovative instrument of corporate knowledge management: the knowledge map. It begins by briefly outlining the rationale for knowledge mapping, i.e. providing a common context to access expertise and experience in large companies. It then conceptualizes five types of knowledge maps that can be used in managing organizational knowledge. They are: knowledge sources, assets, structures, applications and development maps. In order to illustrate these five types of maps, a series of examples is presented (from a multimedia agency, a consulting group, a market research firm and a medium-sized services company), and the advantages and disadvantages of the knowledge mapping technique for knowledge management are discussed. The paper concludes with a series of quality criteria for knowledge maps and proposes a five-step procedure to implement knowledge maps in a corporate intranet.","Computer aided software engineering,
Knowledge management,
Instruments,
Market research,
Context,
Information technology,
Information management,
Product development,
Consumer electronics,
Graphics"
A nonparametric statistical comparison of principal component and linear discriminant subspaces for face recognition,"The FERET evaluation compared recognition rates for different semi-automated and automated face recognition algorithms. We extend FERET by considering when differences in recognition rates are statistically distinguishable subject to changes in test imagery. Nearest Neighbor classifiers using principal component and linear discriminant subspaces are compared using different choices of distance metric. Probability distributions for algorithm recognition rates and pairwise differences in recognition rates are determined using a permutation methodology. The principal component subspace with Mahalanobis distance is the best combination; using L2 is second best. Choice of distance measure for the linear discriminant subspace matters little, and performance is always worse than the principal components classifier using either Mahalanobis or L1 distance. We make the source code for the algorithms, scoring procedures and Monte Carlo study available in the hopes others will extend this comparison to newer algorithms.","Nearest neighbor searches,
Probes,
Face recognition,
Monte Carlo methods,
Image recognition,
Computer science,
Probability distribution,
Automatic testing,
Protocols,
Statistics"
Space-time segmentation using level set active contours applied to myocardial gated SPECT,"This paper presents a new variational method for the segmentation of a moving object against a still background, over a sequence of [two-dimensional or three-dimensional (3-D)] image frames. The method is illustrated in application to myocardial gated single photon emission computed tomography (SPECT) data, and incorporates a level set framework to handle topological changes while providing closed boundaries. The key innovation is the introduction of a geometrical constraint into the derivation of the Euler-Lagrange equations, such that the segmentation of each individual frame can be interpreted as a closed boundary of an object (an isolevel of a set of hyper-surfaces) while integrating information over the entire sequence. This results in the definition of an evolution velocity normal to the object boundary. Applying this method to 3-D myocardial gated SPECT sequences, the left ventricle endocardial and epicardial limits can be computed in each frame. This space-time segmentation method was tested on simulated and clinical 3-D myocardial gated SPECT sequences and the corresponding ejection fractions were computed.","Level set,
Active contours,
Myocardium,
Image segmentation,
Single photon emission computed tomography,
Technological innovation,
Equations,
Testing,
Computational modeling,
Monitoring"
Using component metacontent to support the regression testing of component-based software,"Component based software technologies are viewed as essential for creating the software systems of the future. However, the use of externally-provided components has serious drawbacks for a wide range of software engineering activities, often because of a lack of information about the components. Previously (A. Orso et al., 2000), we proposed the use of component metacontents: additional data and methods provided with a component, to support software engineering tasks. The authors present two new metacontent based techniques that address the problem of regression test selection for component based applications: a code based approach and a specification based approach. First, we illustrate the two techniques. Then, we present a case study that applies the code based technique to a real component based system. On the system studied, on average, 26% of the overall testing effort was saved over seven releases, with a maximum savings of 99% for one version.","Software testing,
Batteries,
Computer science,
System testing,
Software engineering,
Intellectual property,
Information retrieval,
Runtime,
Data engineering,
Electronic switching systems"
Self-Adaptive Genetic Algorithms with Simulated Binary Crossover,"Self-adaptation is an essential feature of natural evolution. However, in the context of function optimization, self-adaptation features of evolutionary search algorithms have been explored mainly with evolution strategy (ES) and evolutionary programming (EP). In this paper, we demonstrate the self-adaptive feature of real-parameter genetic algorithms (GAs) using a simulated binary crossover (SBX) operator and without any mutation operator. The connection between the working of self-adaptive ESs and real-parameter GAs with the SBX operator is also discussed. Thereafter, the self-adaptive behavior of real-parameter GAs is demonstrated on a number of test problems commonly used in the ES literature. The remarkable similarity in the working principle of real-parameter GAs and self-adaptive ESs shown in this study suggests the need for emphasizing further studies on self-adaptive GAs.","evolution strategies,
Self-adaptation,
genetic algorithms,
simulated binary crossover,
blend crossover,
real-coded GAs"
Do we really have to consider covariance matrices for image features?,"Many studies have been made in the past for optimization using covariance matrices of feature points. We first describe how to compute the covariance matrix of a feature point from the gray levels by integrating existing methods. Then, we experimentally examine if thus computed covariance matrices really reflect the accuracy of the feature points. To test this, we do subpixel template matching and compute the homography and the fundamental matrix. Our conclusion is rather surprising, pointing out important elements often overlooked.","Covariance matrix,
Uncertainty,
Computer vision,
Least squares approximation,
Knowledge engineering,
Computer science,
Testing,
Three dimensional displays,
Computational modeling,
Noise generators"
Improved cathode design for long-pulse MILO operation,"An improved cathode design for a magnetically insulated transmission line oscillator (MILO) has resulted in extending the radiated microwave pulse duration from 200 ns to over 400 ns. This was accomplished by maximizing the emission uniformity in the launch-point region of the cathode which, in turn, minimized anode plasma formation. The extended RF pulse duration has allowed us to find evidence of a new pulse-shortening mechanism late in the beam pulse: anode plasma formation in the load region.","Cathodes,
Anodes,
Radio frequency,
Insulation,
Electrons,
Plasma simulation,
Power transmission lines,
Particle beams,
Computer simulation,
Microwave oscillators"
Complete 3-D reconstruction of dental cast shape using perceptual grouping,"To achieve the complete three-dimensional (3-D) data retrieval of the shape of dentition, dental casts were measured from four directions; occlusal, right, left, and labial sides using a line laser scanner. Reconstruction of the entire shape, including undercuts and tooth crowding area, was attempted by applying a perceptual grouping algorithm, which is one of pattern-recognition theories. In the data measured from occlusal, right and left sides, the rows of measurements were parallel to the frontal plane, and three-directionally combined data (3-DC data) was accomplished by affine transformation. While, in the labial side, transformation to the frontal plane was done since rows of the measured data were parallel to the sagittal plane. To combine the labial data with the 3-DC data and reconstruct the complete image, rearrangement of the order of the data in the file was attempted by applying the perceptual grouping. That is, the minimum total length of data combining was examined by considering the factor of proximity and continuity between the data. The most appropriate order of data combining and recognition of islands were accomplished. Using a computer graphic (CG) with a wire-frame model, complicated regions such as anterior segments showing tooth crowding and undercut area were found to be successfully reconstructed without any data defects. The accuracy of reconstruction was ascertained by comparing the characteristic distances between apexes of molars in the reconstructed model with the real cast. The difference was within 0.3 mm, and present method for dental cast reconstruction is considered to be satisfactory for the present purpose such as orthodontics.","Three dimensional displays,
Dentistry,
Image reconstruction,
Teeth,
Morphology,
Shape measurement,
Prosthetics,
Biomedical optical imaging,
Information retrieval,
Laser theory"
Pulse recording by free-running sampling,"Pulses from a position-sensitive photomultiplier (PS-PMT) are recorded by free-running ADCs at a sampling rate of 40 MHz. A four-channel acquisition board has been developed which is equipped with four 12-bit ADCs connected to one field programmable gate array (FPGA). The FPGA manages data acquisition and the transfer to the host computer. It can also work as a digital trigger, so a separate hardware trigger can be omitted. The method of free-running sampling provides a maximum of information, besides the pulse charge and amplitude also pulse shape and starting time are contained in the sampled data. This information is crucial for many tasks such as distinguishing between different scintillator materials, determination of radiation type, pile-up recovery, coincidence detection or time-of-flight applications. The absence of an analog integrator allows very high count rates to be dealt with. Since this method is to be employed in positron emission tomography (PET), the position of an event is also important. The simultaneous readout of four channels allows localization by means of center-of-gravity weighting. First results from a test setup with LSO scintillators coupled to the PS-PMT are presented here.","Sampling methods,
Field programmable gate arrays,
Pulse shaping methods,
Positron emission tomography,
Photomultipliers,
Data acquisition,
Hardware,
Shape,
Radiation detectors,
Testing"
The capacity of the quantum multiple-access channel,"We define classical quantum multiway channels for transmission of classical information, after the previous work by Allahverdyan and Saakian (see Quantum Computing and Quantum Communications (Lecture Notes in Computer Science). Berlin, Germany: Springer-Verlag, vol.1509, 1999). Bounds on the capacity region are derived in a uniform way, which are analogous to the classically known ones, simply replacing Shannon (1961) entropy with von Neumann (1955) entropy. For the single receiver case (multiple-access channel) the elect capacity region is determined. These results are applied to the case of noisy channels, with arbitrary input signal states. A second issue of this work is the presentation of a calculus of quantum information quantities, based on the algebraic formulation of quantum theory.",Information rates
Exploiting the map metaphor in a tool for software evolution,"Software maintenance and evolution are the dominant activities in the software lifecycle. Modularization can separate design decisions and allow them to be independently revolved, but modularization often breaks down and complicated global changes are required. Tool support can reduce the costs of these unfortunate changes, but current tools are limited in their ability to manage information for large-scale software evolution. We argue that the map metaphor can serve as an organizing principle for the design of effective tools for performing global software changes. We describe the design of Aspect Browser, developed around the map metaphor, and discuss a case study of removing a feature from a 500000 line program written in Fortran and C.",
Some approaches and paradigms for verifying and validating simulation models,"In this paper we discuss verification and validation of simulation models. The different approaches to deciding model validity are described, two different paradigms that relate verification and validation to the model development process are presented, the use of graphical data statistical references for operational validity is discussed, and a recommended procedure for model validation is given.","Computational modeling,
Accreditation,
Random variables,
Testing,
Computer simulation,
Educational institutions,
Computer science,
Problem-solving,
Decision making,
Application software"
A compression algorithm for DNA sequences,"We present a DNA compression algorithm, GenCompress, based on approximate matching that gives the best compression results on standard benchmark DNA sequences. We present the design rationale of GenCompress based on approximate matching, discuss details of the algorithm, provide experimental results, and compare the results with the two most effective compression algorithms for DNA sequences (Biocompress-2 and Cfact).","Compression algorithms,
DNA,
Sequences,
Genomics,
Bioinformatics,
Genetic mutations,
Biological information theory,
Data compression,
Computer science,
Arithmetic"
Feature selection from huge feature sets,"The number of features that can be completed over an image is, for practical purposes, limitless. Unfortunately, the number of features that can be computed and exploited by most computer vision systems is considerably less. As a result, it is important to develop techniques for selecting features from very large data sets that include many irrelevant or redundant features. This work addresses the feature selection problem by proposing a three-step algorithm. The first step uses a variation of the well known Relief algorithm to remove irrelevance; the second step clusters features using K-means to remove redundancy; and the third step is a standard combinatorial feature selection algorithm. This three-step combination is shown to be more effective than standard feature selection algorithms for large data sets with lots of irrelevant and redundant features. It is also shown to he no worse than standard techniques for data sets that do not have these properties. Finally, we show a third experiment in which a data set with 4096 features is reduced to 5% of its original size with very little information loss.","Computer vision,
Principal component analysis,
Probes,
Computer science,
Supervised learning,
Particle measurements,
Size measurement,
Data mining,
Object recognition,
Biometrics"
MASE: a novel infrastructure for detailed microarchitectural modeling,,"Microarchitecture,
Computational modeling,
Hardware,
Timing,
Computer simulation,
Space technology,
Computer aided instruction,
Computer architecture,
Computer science,
Emulation"
A framework for the emerging mobile commerce applications,"We envision many new e-commerce applications will be possible and significantly benefit from emerging wireless and mobile networks. These applications can collectively be termed wireless e-commerce or mobile commerce. To allow designers, developers, and researchers to strategize and effectively implement mobile commerce applications, we propose a 4-level integrated framework for mobile commerce. Since there are potentially an unlimited number of mobile commerce applications we only identify a few important classes of applications such as mobile financial applications, mobile advertising, mobile inventory management, proactive service management, product location and search, and wireless re-engineering. We also address the networking requirements of these applications and discuss how these requirements can be supported by existing and emerging wireless networks. It is our hope that this work will become the framework for further research in mobile commerce.","Business,
Application software,
Middleware,
Mobile computing,
Inventory management,
Electronic commerce,
Wireless application protocol,
Computer networks,
Information systems,
Computer science"
Reducing quantum computations to elementary unitary operations,"Quantum computations are intimately connected with unitary operators. This article shows that standard techniques from numerical linear algebra can be used to represent quantum computations as sequences of simple quantum operations, called quantum Givens operators, on single quantum bits.",
Schedulability analysis and utilization bounds for highly scalable real-time services,"The proliferation of high-volume time-critical Web services such as online trading calls for a scalable server design that allows meeting individual response-time guarantees of real time transactions. A main challenge is to honor these guarantees despite unpredictability in incoming server load. The extremely high volume of real-time service requests mandates constant-time scheduling and schedulability analysis algorithms (as opposed to polynomial or logarithmic ones in the number of current requests). The paper makes two major contributions towards developing an architecture and theoretical foundations for scalable real-time servers operating in dynamic environments. First, we derive a tight utilization bound for schedulability of aperiodic tasks (requests) that allows implementing a constant time schedulability test on the server. We demonstrate that Liu and Layland's schedulable utilization bound of ln 2 does not apply to aperiodic tasks, and prove that an optimal arrival-time independent scheduling policy will meet all aperiodic task deadlines if utilization is maintained below 1/1+/spl radic/(1/2). Second, we show that aperiodic deadline-monotonic scheduling is the optimal arrival-time-independent scheduling policy for aperiodic tasks. This result is used to optimally prioritize server requests. Evaluation of a utilization control loop that maintains server utilization below the bound shows that the approach is effective in meeting all individual deadlines in a high performance real-time server.","Processor scheduling,
Computer science,
Time factors,
Algorithm design and analysis,
Scheduling algorithm,
Polynomials,
Testing,
Timing,
Performance analysis,
Runtime"
A study of networks simulation efficiency: fluid simulation vs. packet-level simulation,"Network performance evaluation through traditional packet-level simulation is becoming increasingly difficult as today's networks grow in scale along many dimensions. As a consequence, fluid simulation has been proposed to cope with the size and complexity of such systems. This study focuses on analyzing and comparing the relative efficiencies of fluid simulation and packet-level simulation for several network scenarios. We use the ""simulation event"" rate to measure the computational effort of the simulators and show that this measure is both adequate and accurate. For some scenarios, we derive analytical results for the simulation event rate and identify the major factors that contribute to the simulation event rate. Among these factors, the ""ripple effect"" is very important since it can significantly increase the fluid simulation event rate. For a tandem queueing system, we identify the boundary condition to establish regions where one simulation paradigm is more efficient than the other. Flow aggregation is considered as a technique to reduce the impact of the ""ripple effect"" in fluid simulation. We also show that WFQ scheduling discipline can limit the ""ripple effect"", making fluid simulation particularly well suited for WFQ models. Our results show that tradeoffs between parameters of a network model determines the most efficient simulation approach.","Computational modeling,
Discrete event simulation,
Telecommunication traffic,
Traffic control,
Power system modeling,
Computer simulation,
Analytical models,
Computer networks,
Computer science,
Boundary conditions"
Computerized radiographic mass detection. II. Decision support by featured database visualization and modular neural networks,"For pt.I see ibid., vol.20, no.4, p.289-301 (2001). Based on the enhanced segmentation of suspicious mass areas, further development of computer-assisted mass detection may be decomposed into three distinctive machine learning tasks: (1) construction of the featured knowledge database; (2) mapping of the classified and/or unclassified data points in the datahase; and (3) development of an intelligent user interface. A decision support system may then be constructed as a complementary machine observer that should enhance the radiologists performance in mass detection, We adopt a mathematical feature extraction procedure to construct the featured knowledge database from all the suspicious mass sites localized by the enhanced segmentation. The optimal mapping of the data points is then obtained by learning the generalized normal mixtures and decision boundaries, where a probabilistic modular neural network (PMNN) is developed to carry out both soft and hard clustering. A visual explanation of the decision making is further invented as a decision support, based on an interactive visualization hierarchy through the probabilistic principal component projections of the knowledge database and the localized optimal displays of the retrieved raw data. A prototype system is developed and pilot tested to demonstrate the applicability of this framework to mammographic mass detection.",
Maximum-likelihood expectation-maximization reconstruction of sinograms with arbitrary noise distribution using NEC-transformations,"The maximum-likelihood (ML) expectation-maximization (EM) [ML-EM] algorithm is being widely used for image reconstruction in positron emission tomography. The algorithm is strictly valid if the data are Poisson distributed. However, it is also often applied to processed sinograms that do not meet this requirement. This may sometimes lead to suboptimal results: streak artifacts appear and the algorithm converges toward a lower likelihood value. As a remedy, the authors propose two simple pixel-by-pixel methods [noise equivalent counts (NEC)-scaling and NEC-shifting] in order to transform arbitrary sinogram noise into noise which is approximately Poisson distributed (the first and second moments of the distribution match those of the Poisson distribution). The convergence speed associated with both transformation methods is compared, and the NEC-scaling method is validated with both simulations and clinical data. These new methods extend the ML-EM algorithm to a general purpose nonnegative reconstruction algorithm.","Acoustic noise,
Image reconstruction,
Positron emission tomography,
Convergence,
Reconstruction algorithms,
Attenuation,
Nuclear medicine,
Image converters,
Helium,
Noise robustness"
Ground plane segmentation for mobile robot visual navigation,"We describe a method of mobile robot monocular visual navigation, which uses multiple visual cues to detect and segment the ground plane in the robot's field of view. Corner points are tracked through an image sequence and grouped into coplanar regions using a method which we call an H-based tracker. The H-based tracker employs planar homographies and is initialised by 5-point planar projective invariants. This allows us to detect ground plane patches and the colour within such patches is subsequently modelled. These patches are grown by colour classification to give a ground plane segmentation, which is then used as an input to a new variant of the artificial potential field algorithm.","Mobile robots,
Navigation,
Layout,
Cameras,
Image sequences,
Image reconstruction,
Transmission line matrix methods,
Computer science,
Image segmentation,
Charge coupled devices"
The shortest processing time first (SPTF) dispatch rule and some variants in semiconductor manufacturing,"Looking for appropriate dispatch rules for semiconductor fabrication facilities (wafer fabs), practitioners often intend to use the Shortest Processing Time First (SPTF) rule because it is said to reduce cycle times. In our study, we show, however, that this positive effect on cycle times can be achieved in single machine systems but not necessarily in complete wafer fabs. In addition, we discuss variants of the SPTF rule.","Semiconductor device modeling,
Operating systems,
Semiconductor device manufacture,
Computer aided manufacturing,
Manufacturing processes,
Computer science,
Fabrication,
Electronics industry,
Production control,
Throughput"
An Introduction to 3-D User Interface Design,"Three-dimensional user interface design is a critical component of any virtual environment (VE) application. In this paper, we present a broad overview of 3-D interaction and user interfaces. We discuss the effect of common VE hardware devices on user interaction, as well as interaction techniques for generic 3-D tasks and the use of traditional 2-D interaction styles in 3-D environments. We divide most user-interaction tasks into three categories: navigation, selection/manipulation, and system control. Throughout the paper, our focus is on presenting not only the available techniques but also practical guidelines for 3-D interaction design and widely held myths. Finally, we briefly discuss two approaches to 3-D interaction design and some example applications with complex 3-D interaction requirements. We also present an annotated online bibliography as a reference companion to this article.",
Ten years of genetic fuzzy systems: current framework and new trends,"Although fuzzy systems demonstrated their ability to solve different kinds of problems in various applications, there is an increasing interest on augmenting them with learning capabilities. Two of the most successful approaches to hybridise fuzzy systems with adaptation methods have been made in the realm of soft computing: neuro-fuzzy systems and genetic fuzzy systems hybridise the approximate reasoning method of fuzzy systems with the learning capabilities of neural networks and evolutionary algorithms. The article focuses on genetic fuzzy systems, paying special attention to genetic fuzzy rule based systems, giving a brief overview of the field.","Fuzzy systems,
Biological cells,
Fuzzy neural networks,
Knowledge based systems,
Genetic algorithms,
Computer science,
Computer networks,
Fuzzy reasoning,
Neural networks,
Evolutionary computation"
Reverse engineering to achieve maintainable WWW sites,"The growth of the World Wide Web and the accelerated development of web sites and associated web technologies has resulted in a variety of maintenance problems. The maintenance problems associated with web sites and the WWW are examined. It is argued that currently web sites and the WWW lack both data abstractions and structures that could facilitate maintenance. A system to analyse existing web sites and extract duplicated content and style is described here. In designing the system, existing reverse engineering techniques have been applied, and a case for further application of these techniques is made in order to prepare sites for their inevitable evolution in future.",
Development of intelligent wheelchair system with face and gaze based interface,"Electric wheelchairs are important locomotion devices for disabled and senior people, and they are becoming even more important in our aging society. Therefore safer and more comfortable ""intelligent wheelchairs"" are expected to be developed for practical use. Usually, the control of wheelchair is realized by pushing a joystick to the direction to move toward, but quick and safe operation is needed to cope with the surrounding dynamic environments. From this point of view, an ""intelligent wheelchair"" should have the ability to reduce the load on the user. We propose a wheelchair system which has an intuitive interface using head and gaze motion of a user. The user needs only to look where he/she wants to go, and can start and stop by nodding and shaking his/her head. The system also estimates whether the user is concentrating on the operation based on the relationship between the head and gaze movement. The feasibility of the developed wheelchair system is confirmed through experiments in both indoor and outdoor environments.","Intelligent systems,
Wheelchairs,
Intelligent sensors,
Navigation,
Mobile robots,
Intelligent robots,
Humans,
Information science,
Aging,
Senior citizens"
Wireless commerce: marketing issues and possibilities,"Wireless commerce is viewed as the extension of Internet-based e-commerce beyond the static terminal of the PC or the Web-TV to the flexible anytime, anywhere, anyplace context of the mobile environment. While many in the industry and popular press seem to equate the world of Internet e-commerce and wireless commerce as one and the same, there are unique characteristics of the wireless technology and its usage that renders it distinctly different from and complementary to Internet based e-commerce. This difference also has important implications for the marketing of goods, services and content and for conducting marketing research using wireless technology. We first discuss the unique characteristics of wireless technology and its usage. On the basis of these characteristics, we identify and set out a series of propositions that relate to the issues of marketing and marketing research using wireless technology. In making our arguments, we view how wireless technology's contribution complement the capabilities brought about by Internet based e-commerce. Finally, we outline a marketing research agenda that will allow testing of some of the propositions put forth in the paper.","Business,
Application software,
Personal digital assistants,
Testing,
Portable computers,
Educational institutions,
Electronic commerce,
Mobile computing,
Internet telephony,
Taxonomy"
Estimating error rates in processor-based architectures,The paper investigates a new technique to predict error rates in digital architectures based on microprocessors. Three studied cases are presented concerning three different processors. Two of them are included in the instruments of a satellite project. The actual space applications of these two instruments were implemented using the capabilities of a dedicated system. Results of the fault injection and radiation testing experiments and discussions about the potentialities of this technique are presented.,"Error analysis,
Circuit testing,
Single event upset,
Microprocessors,
Instruments,
Satellites,
Laboratories,
Circuit faults,
Helium,
Space vehicles"
Power efficient and sparse spanner for wireless ad hoc networks,"Due to the limited resources available in the wireless ad hoc networking nodes, the scalability is crucial for network operations. One effective approach is to maintain only a sparse spanner of a linear number of links while still preseving the power-efficient route for any pair of nodes. For any spanner G, its power stretch factor is defined as the maximum ratio of the minimum power needed to support any link in this spanner to the least necessary. In this paper, we first consider several well-known proximity graphs including the relative neighborhood graph, Gabriel graph and Yao graph. These graphs are sparse and can be constructed locally in an efficient way. We show that the power stretch factor of the Gabriel graph is always one, and the power stretch factor of the Yao graph is bounded by a constant while the power stretch factor of the relative neighborhood graph could be as large as the network size minus one. Notice that all of these graphs do not have constant degrees. We further propose another sparse spanner that has both constant degree and constant power stretch factor. An efficient local algorithm is presented for the construction of this spanner.","Ad hoc networks,
Mobile ad hoc networks,
Network topology,
Energy consumption,
Protocols,
Scalability,
Routing,
Joining processes,
Power measurement,
Computer science"
Who links to whom: mining linkage between Web sites,"Previous studies of the Web graph structure have focused on the graph structure at the level of individual pages. In actuality the Web is a hierarchically nested graph, with domains, hosts and Web sites introducing intermediate levels of affiliation and administrative control. To better understand the growth of the Web we need to understand its macro-structure, in terms of the linkage between Web sites. We approximate this by studying the graph of the linkage between hosts on the Web. This was done based on snapshots of the Web taken by Google in Oct 1999, Aug 2000 and Jun 2001. The connectivity between hosts is represented by a directed graph, with hosts as nodes and weighted edges representing the count of hyperlinks between pages on the corresponding hosts. We demonstrate how such a ""hostgraph"" can be used to study connectivity properties of hosts and domains over time, and discuss a modified ""copy model"" to explain observed link weight distributions as a function of subgraph size. We discuss changes in the Web over time in the size and connectivity of Web sites and country domains. We also describe a data mining application of the hostgraph: a related host finding algorithm which achieves a precision of 0.65 at rank 3.",
A signal estimation approach to functional MRI,"In the last half decade, fast methods of magnetic resonance imaging have led to the possibility, for the first time, of noninvasive dynamic brain imaging. This has led to an explosion of work in the Neurosciences. From a signal processing viewpoint the problems are those of nonlinear spatio-temporal system identification. Here, the authors develop new methods of identification using novel spatial regularization. They also develop a new model comparison technique and use that to compare their method with existing techniques on some experimental data.","Estimation,
Magnetic resonance imaging,
High-resolution imaging,
Blood,
Biomedical imaging,
Spatial resolution,
Radiology,
Hemodynamics,
Fluctuations,
Nonlinear dynamical systems"
Automated CT image evaluation of the lung: a morphology-based concept,"Computed tomography (CT) provides the most reliable method to detect emphysema in vivo. Commonly used methods only calculate the area of low attenuation [pixel index (PI)], while a radiologist considers the bullous morphology of emphysema. The PI is a good, well-known measure of emphysema. But it is not able to detect emphysema in cases in which emphysema and fibrosis occur at the same time. This is because fibrosis leads to a low number of low-attenuation pixels, while emphysema leads to a high number of pixels. The PI takes the average of both and, consequently, may present a result within the normal range. The main focus of this paper is to present a new algorithm of thoracic CT image evaluation based on pulmonary morphology of emphysema. The PI is extended, in that it is enabled to differentiate between small, medium, and large bullae (continuous low-attenuation areas). It is not a texture-based algorithm. The bullae are sorted by size into four size classes: class 1 being within the typical size of lung parenchyma; classes 24 presenting small, medium, and large bullae. It is calculated how much area the different classes take up of all low-attenuation pixels. The bullae index (BI) is derived from the percentage of areas covered, respectively, by small, medium, and large bullae. From the relation of the area of bullae belonging to class 4, to that of those belonging to class 2, a measure of the emphysema type (ET) is calculated. It classifies the lung by the type of emphysema in bullous emphysema or small-sized, diffuse emphysema, respectively. The BI is as reliable as the PI. In cases in which the PI indicates normal values while in fact emphysema is coexisting with fibrosis, the BI, nevertheless, detects the destruction caused by the emphysema. The BI combined with the ET reflects the visual assessment of the radiological expert. In conclusion, the BI is an objective and reliable index in order to quantify emphysematous destruction, hence, avoiding interobserver variance. This is particularly interesting for follow-up. The classification of the ET is a helpful and unique approach to achieving an exact diagnosis of emphysema.","Computed tomography,
Lungs,
Image analysis,
Morphology,
Testing,
In vivo,
Information technology,
Attenuation,
Focusing,
Area measurement"
Training product unit networks using cooperative particle swarm optimisers,"The cooperative particle swarm optimiser (CPSO) is a variant of the particle swarm optimiser (PSO) that splits the problem vector, for example a neural network weight vector, across several swarms. The paper investigates the influence that the number of swarms used (also called the split factor) has on the training performance of a product unit neural network. Results are presented, comparing the training performance of the two algorithms, PSO and CPSO, as applied to the task of training the weight vector of a product unit neural network.","Particle swarm optimization,
Neural networks,
Computer science,
Genetic algorithms,
Random sequences,
Acceleration"
Comparison of relativistic runaway electron avalanche rates obtained from Monte Carlo simulations and kinetic equation solution,"New computer simulations of the relativistic runaway electron avalanche mechanism were carried out to remove prior approximations and reassess the space and temporal scales predicted by previous Boltzmann calculations. Two Monte Carlo techniques, a finite-difference solution, and a finite-volume solution of the kinetic equation for high-energy electrons were employed. Results obtained for the length and time scales at sufficiently high electric fields by the different methods are consistent with each other and with analytical estimates. The physical reason for the remaining discrepancy at small fields is discussed.","Electrons,
Kinetic theory,
Equations,
Monte Carlo methods,
Avalanche breakdown,
Ionization,
Drag,
Computer simulation,
Finite difference methods,
Clouds"
Computer-aided reconstruction of cathode images obtained by high speed photography of high current vacuum arcs,"The images of the cathode obtained by high-speed photography are ellipsoidal because they are taken at a small angle (about 10/spl deg/) to the cathode surface. For this reason, it is very difficult, if at all possible, to draw conclusions about the spatial distribution of cathode spots and current density over the cathode surface. One may perform inverse parallel projection, but previously it was necessary to free the image from the noise of the interelectrode plasma radiation and other noise sources. In this paper, we propose a method of image filtering based on contours of equal intensity evaluation, with subsequent determination of the cathode spots distribution.","Image reconstruction,
Cathodes,
Photography,
Vacuum arcs,
Electrodes,
Filtering,
Current density,
Plasma density,
Plasma materials processing,
Surface discharges"
An evolutionary approach to materialized views selection in a data warehouse environment,"A data warehouse (DW) contains multiple views accessed by queries. One of the most important decisions in designing a DW is selecting views to materialize for the purpose of efficiently supporting decision making. The search space for possible materialized views is exponentially large. Therefore heuristics have been used to search for a near optimal solution. In this paper, we explore the use of an evolutionary algorithm for materialized view selection based on multiple global processing plans for queries. We apply a hybrid evolutionary algorithm to solve three related problems. The first is to optimize queries. The second is to choose the best global processing plan from multiple global processing plans. The third is to select materialized views from a given global processing plan. Our experiment shows that the hybrid evolutionary algorithm delivers better performance than either the evolutionary algorithm or heuristics used alone in terms of the minimal query and maintenance cost and the evaluation cost to obtain the minimal cost.","Data warehouses,
Costs,
Evolutionary computation,
Query processing,
Australia,
Data mining,
Warehousing,
Distributed databases,
Computer science,
Decision making"
Nine points of light: acquiring subspaces for face recognition under variable lighting,"Previous work has demonstrated that the image variations of many objects (human faces in particular) under variable lighting can be effectively modeled by low dimensional linear spaces. Basis images spanning this space are usually obtained in one of two ways: A large number of images of the object under different conditions is acquired, and principal component analysis (PCA) is used to estimate a subspace. Alternatively, a 3D model (perhaps reconstructed from images) is used to render virtual images under either point sources from which a subspace is derived using PCA or more recently under diffuse synthetic lighting based on spherical harmonics. In this paper we show that there exists a configuration of nine point light source directions such that by taking nine images of each individual under these single sources, the resulting subspace is effective at recognition under a wide range of lighting conditions. Since the subspace is generated directly from real images, potentially complex intermediate steps such as PCA and 3D reconstruction can be completely avoided; nor is it necessary to acquire large numbers of training images or physically construct complex diffuse (harmonic) light fields. We provide both theoretical and empirical results to explain why these linear spaces should be good for recognition.","Face recognition,
Humans,
Principal component analysis,
Lighting,
Jacobian matrices,
Optical reflection,
Kernel,
Frequency,
Computer science,
Ear"
On the sensitivity of Web proxy cache performance to workload characteristics,"This paper describes the design and use of a synthetic Web proxy workload generator (ProWGen) to investigate the sensitivity of proxy cache replacement policies to selected Web workload characteristics. Trace-driven simulations with synthetic workloads from ProWGen show the relative sensitivity of three popular cache replacement algorithms-LRU, LFU-aging and GD-size-to Zipf slope, temporal locality, and correlation (if any) between file size and popularity, and the relative insensitivity of these algorithms to one-timers and heavy tail index. Performance differences between the three policies are also highlighted.","Web server,
Internet,
Tail,
Delay,
Computer science,
Character generation,
Computational modeling,
Spine,
Scalability,
Network servers"
"Improved inapproximability results for MaxClique, chromatic number and approximate graph coloring","The author presents improved inapproximability results for three problems: the problem of finding the maximum clique size in a graph, the problem of finding the chromatic number of a graph, and the problem of coloring a graph with a small chromatic number with a small number of colors. J. Hastad's (1996) result shows that the maximum clique size in a graph with n vertices is inapproximable in polynomial time within a factor n/sup 1-/spl epsi// or arbitrarily small constant /spl epsi/>0 unless NP=ZPP. We aim at getting the best subconstant value of /spl epsi/ in Hastad's result. We prove that clique size is inapproximable within a factor n/2((log n))/sup 1-y/ corresponding to /spl epsi/=1/(log n)/sup /spl gamma// for some constant /spl gamma/>0 unless NP/spl sube/ZPTIME(2((log n))/sup O(1)/). This improves the previous best inapproximability factor of n/2/sup O(log n//spl radic/log log n)/ (corresponding to /spl epsi/=O(1//spl radic/log log n)) due to L. Engebretsen and J. Holmerin (2000). A similar result is obtained for the problem of approximating chromatic number of a graph. We also present a new hardness result for approximate graph coloring. We show that for all sufficiently large constants k, it is NP-hard to color a k-colorable graph with k/sup 1/25 (log k)/ colors. This improves a result of M. Furer (1995) that for arbitrarily small constant /spl epsi/>0, for sufficiently large constants k, it is hard to color a k-colorable graph with k/sup 3/2-/spl epsi// colors.","Polynomials,
Computer science,
Error correction,
Error correction codes,
Approximation algorithms"
Using public domain metrics to estimate software development effort,"The authors investigate the accuracy of cost estimates when applying most commonly used modeling techniques to a large-scale industrial data set which is professionally maintained by the International Software Standards Benchmarking Group (ISBSG). The modeling techniques applied are ordinary least squares regression (OLS), analogy based estimation, stepwise ANOVA, CART, and robust regression. The questions addresses in the study are related to important issues. The first is the appropriate selection of a technique in a given context. The second is the assessment of the feasibility of using multi-organizational data compared to the benefits from company-specific data collection. We compare company-specific models with models based on multi-company data. This is done by using the estimates derived for one company that contributed to the ISBSG data set and estimates from using carefully matched data from the rest of the ISBSG data. When using the ISBSG data set to derive estimates for the company, generally poor results were obtained. Robust regression and OLS performed most accurately. When using the company's own data as the basis for estimation, OLS, a CART-variant, and analogy performed best. In contrast to previous studies, the estimation accuracy when using the company's data is significantly higher than when using the rest of the ISBSG data set. Thus, from these results, the company that contributed to the ISBSG data set, would be better off when using its own data for cost estimation.","Programming,
Costs,
Robustness,
Computer science,
Data engineering,
Large-scale systems,
Computer industry,
Least squares approximation,
Analysis of variance,
Software standards"
The space of all stereo images,"A theory of stereo image formation is presented that enables a complete classification of all possible stereo views, including non-perspective varieties. Towards this end, the notion of epipolar geometry is generalized to apply to multiperspective images. It is shown that any stereo pair must consist of rays lying on one of three varieties of quadric surfaces. A unified representation is developed to model all classes of stereo views, based on the concept of a quadric view. The benefits include a unified treatment of projection and triangulation operations for all stereo views. The framework is applied to derive new types of stereo image representations with unusual and useful properties.","Geometry,
Surface treatment,
Image representation,
Layout,
Image sensors,
Sensor phenomena and characterization,
Particle measurements,
Computer science,
Graphics,
Stereo image processing"
Learning occupancy grids with forward models,"Presents a way to acquire occupancy grid maps with mobile robots. Virtually all existing occupancy grid mapping algorithms decompose the high-dimensional mapping problem into a collection of one-dimensional problems, where the occupancy of each grid cell is estimated independently of others. This induces conflicts that can lead to inconsistent maps. The paper shows how to solve the mapping problem in the original, high-dimensional space, thereby maintaining all dependencies between neighboring cells. As a result, maps generated by our approach are often more accurate than those generated using traditional techniques. Our approach relies on a rigorous statistical formulation of the mapping problem using forward models. It employs the expectation maximization algorithm for estimating maps, and a Laplacian approximation to determine uncertainty.","Mobile robots,
Uncertainty,
Sonar navigation,
Robot sensing systems,
Inverse problems,
Computer science,
Path planning,
Collision avoidance,
Sonar measurements,
Noise measurement"
Greedy mapping of terrain,"We study a greedy mapping method that always moves the robot from its current location to the closest location that it has not visited (or observed) yet, until the terrain is mapped. Although one does not expect such a simple mapping method to minimize the travel distance of the robot, we present analytical results that show (perhaps surprisingly) that the travel distance of the robot is reasonably small. This is interesting because greedy mapping has a number of desirable properties. It is simple to implement and integrate into complete robot architectures. It does not need to have control of the rebut at all times, takes advantage of prior knowledge about parts of the terrain (if available), and can be used by several robots cooperatively.","Terrain mapping,
Mobile robots,
Robot sensing systems,
Robot control,
Robustness,
Robotics and automation,
Resumes,
Educational institutions,
Computer science,
Computer architecture"
Attributed concept maps: fuzzy integration and fuzzy matching,"A concept map, typically depicted as a connected graph, is composed of a collection of propositions. Each proposition forming a semantic unit consists of a small set of concept nodes interconnected to one another with relation links. Concept maps possess a number of appealing features which make them a promising tool for teaching, learning, evaluation, and curriculum planning. We extend concept maps by associating their concept nodes and relation links with attribute values which indicate the relative significance of concepts and relationships in knowledge representation. The resulting maps are called attributed concept maps (ACM). Assessing students will be conducted by matching their ACMs with those prebuilt by experts. The associated techniques are referred to as map matching techniques. The building of an expert ACM has in the past been done by only one specialist. We integrate a number of maps developed by separate experts into a single map, called the master map (MM), which will serve as a prototypical map in map matching. Both map integration and map matching are conceptualized in terms of fuzzy set discipline. Experimental results have shown that the proposed ideas of ACM, MM, fuzzy map integration, and fuzzy map matching are well suited for students with high performances and difficult subject materials.","Fuzzy sets,
Joining processes,
Knowledge representation,
Prototypes,
Councils,
Computer science education,
Gold,
Psychology"
Model-based synthesis of fault trees from Matlab-Simulink models,"We outline a new approach to safety analysis in which concepts of computer HAZOP are fused with the idea of software fault tree analysis to enable a continuous assessment of an evolving programmable design developed in Matlab-Simulink. We also discuss the architecture of a tool that we have developed to support the new method and enable its application in complex environments. We show that the method and the tool enable the integrated hardware and software analysis of a programmable system and that in the course of that analysis they automate and simplify the development of fault trees for the system. Finally, we propose a demonstration of the method and the tool and we outline the experimental platform and aims of that demonstration.","Mathematical model,
Fault trees,
MATLAB,
Software safety,
Application software,
Hazards,
Performance analysis,
Railway safety,
Automation,
Computer science"
Handling soft modules in general nonslicing floorplan using Lagrangian relaxation,"In the early stage of floorplan design, many modules have large flexibilities in shape (soft modules). Handling soft modules in general nonslicing floorplan is a complicated problem. Many previous works have attempted to tackle this problem using heuristics or numerical methods, but none of them can solve it optimally and efficiently. In this paper, we show how this problem can be solved optimally by geometric programming using the Lagrangian relaxation technique. The resulting Lagrangian relaxation subproblem is so simple that the optimal size of each module can be computed in linear time. We implemented this method in a simulated annealing framework based on the sequence pair representation. The geometric program is invoked in every iteration of the annealing process to compute the optimal size of each module to give the best packing. The execution time is much faster (at least 15 times faster for data sets with more than 50 modules) than that of the most updated previous work by Murata and Kuh (1998). For a benchmark data with 49 modules, we take 3.7 h in total for the whole annealing process using a 600-MHz Pentium III processor while the convex programming approach described by Murata and Koh needs seven days using a 250-MHz DEC Alpha. Our technique will also be applicable to other floorplanning algorithms that use constraint graphs to find module positions in the final packing.","Lagrangian functions,
Shape,
Computational modeling,
Simulated annealing,
Very large scale integration,
Integrated circuit technology,
Optimization methods,
Computer science,
Topology"
Tracking nonrigid motion and structure from 2D satellite cloud images without correspondences,"Tracking both structure and motion of nonrigid objects from monocular images is an important problem in vision. In this paper, a hierarchical method which integrates local analysis (that recovers small details) and global analysis (that appropriately limits possible nonrigid behaviors) is developed to recover dense depth values and nonrigid motion from a sequence of 2D satellite cloud images without any prior knowledge of point correspondences. This problem is challenging not only due to the absence of correspondence information but also due to the lack of depth cues in the 2D cloud images (scaled orthographic projection). In our method, the cloud images are segmented into several small regions and local analysis is performed for each region. A recursive algorithm is proposed to integrate local analysis with appropriate global fluid model constraints, based on which a structure and motion analysis system, SMAS, is developed. We believe that this is the first reported system in estimating dense structure and nonrigid motion under scaled orthographic views using fluid model constraints. Experiments on cloud image sequences captured by meteorological satellites (GOES-8 and GOES-9) have been performed using our system, along with their validation and analyses. Both structure and 3D motion correspondences are estimated to subpixel accuracy. Our results are very encouraging and have many potential applications in earth and space sciences, especially in cloud models for weather prediction.","Tracking,
Satellites,
Clouds,
Image analysis,
Image motion analysis,
Image sequence analysis,
Motion analysis,
Performance analysis,
Motion estimation,
Image segmentation"
Region division assignment of orthogonal variable-spreading-factor codes in W-CDMA,"This work focuses on the problem of efficient assignment of the orthogonal-variable-spreading-factor (OVSF) codes for multimedia communications in W-CDMA systems. Due to the problem of code blocking in OVSF-code assignment considered by Minn and Siu, and the different probability of requests for each data rate, the OVSF-code assignment algorithms proposed so far cannot efficiently serve the high-rate requests when traffic load is high. Even though Minn and Siu proposed the algorithm to completely eliminate the code blocking, a system implemented with such an algorithm has a lower number of simultaneous voice conversations than the system with the code blocking problem. To efficiently provide the services for all supported rates, we propose an OVSF-code assignment scheme called the region division assignment (RDA). The performance of the proposed RDA scheme is evaluated by simulation for voice and video sources based on the multiple access protocol in UMTS/IMT-2000.","Multiaccess communication,
Multimedia communication,
Multimedia systems,
Telecommunication traffic,
3G mobile communication,
Computer science,
Access protocols,
Base stations,
Mobile communication"
Agent-based and system dynamics modeling: a call for cross study and joint research,"In recent years, the ""Science of Complexity"", as promoted by the Santa Fe institute, has been recognized by mainstream scholars in prominent scientific disciplines ranging from physics over economics and computer science to the social sciences. In various reviews, it appears, however, as if Complexity Theory and techniques such as agent-based modeling are unique in their capacity of modeling nonlinear systems. These reviews overlook that such systems have been modeled and simulated at least since the late 1950s, e.g., by the research track known as System Dynamics, a discipline with a rich body of literature. This paper gives an overview of the general modeling principles of both tracks, describes their areas of applicability, and discusses their relative strengths and weaknesses. It tries to identify areas in which the two modeling traditions complement each other, and where they overlap. The paper concludes that cross study and joint research are overdue.","Mathematical model,
Computer simulation,
Iron,
Read only memory,
Computer science,
Nonlinear systems,
Identity-based encryption,
Ontologies"
Integrating the teaching of computer organization and architecture with digital hardware design early in undergraduate courses,"This paper describes a new way to teach computer organization and architecture concepts with extensive hands-on hardware design experience very early in computer science curricula. While describing the approach, it addresses relevant questions about teaching computer organization, computer architecture and hardware design to students in computer science and related fields. The justification to concomitantly teach two often separately addressed subjects is twofold. First, to provide a better insight into the practical aspects of computer organization and architecture. Second, to allow addressing only highly abstract design levels yet achieving reasonably performing implementations, to make the integrated teaching approach feasible. The approach exposes students to many of the essential issues incurred in the analysis, simulation, design and effective implementation of processors. Although the former separation of such connected disciplines has certainly brought academic benefits in the past, some modern technologies allow capitalizing on their integration. The practical implementation of the teaching approach comprises lecture as well as laboratory courses, starting in the third semester of an undergraduate computer science curriculum. In four editions of the first two courses, most students have obtained successful processor implementations. In some cases, considerably complex applications, such as bubble sort and quick sort procedures were programmed in assembly and or machine code and run at the hardware description language simulation level in the designed processors.",Computer science education
Models for replica synchronisation and consistency in a data grid,"Data grids are currently proposed solutions to large-scale data management problems, including efficient file transfer and replication. Large amounts of data and the world-wide distribution of data stores contribute to the complexity of the data management challenge. Recent architecture proposals and prototypes deal with replication of read-only files but do not address the replica synchronisation problem. We propose a new data grid service, called the Grid Consistency Service (GCS), that sits on top of existing data grid services and allows for replica update synchronisation and consistency maintenance. We give models for different levels of consistency, provided to the Grid user and discuss how they can be included into a replica consistency service for a data grid.","Computer science,
Informatics,
Large-scale systems,
Proposals,
Prototypes,
Physics"
"Astronomical image and signal processing: looking at noise, information and scale","We present methods used to measure the information in an astronomical image, in both a statistical and a deterministic way. We discuss the wavelet transform and noise modeling, and describe how to measure the information and the implications for object detection, filtering, and deconvolution. The perspectives opened up by the range of noise models, catering for a wide range of eventualities in physical science imagery and signals, and the new two-pronged but tightly coupled understanding of the concept of information have given rise to better quality results in applications such as noise filtering, deconvolution, compression, and object (feature) detection. We have illustrated some of these new results in this article. The theoretical foundations of our perspectives have been sketched out. The practical implications, too, are evident from the range of important signal processing problems which we can better address with this armoury of methods. The results described in this work are targeted at information and at relevance. While we have focused on experimental results in astronomical image and signal processing, the possibilities are apparent in many other application domains.","Signal processing,
Extraterrestrial measurements,
Object detection,
Information filtering,
Information filters,
Deconvolution,
Wavelet transforms,
Noise measurement,
Image coding,
Computer vision"
Variable length queries for time series data,"Finding similar patterns in a time sequence is a well-studied problem. Most of the current techniques work well for queries of a prespecified length, but not for variable length queries. We propose a new indexing technique that works well for variable length queries. The central idea is to store index structures at different resolutions for a given dataset. The resolutions are based on wavelets. For a given query, a number of subqueries at different resolutions are generated. The ranges of the subqueries are progressively refined based on results from previous subqueries. Our experiments show that the total cost for our method is 4 to 20 times less than the current techniques including linear scan. Because of the need to store information at multiple resolution levels, the storage requirement of our method could potentially be large. In the second part of the paper we show how the index information can be compressed with minimal information loss. According to our experimental results, even after compressing the size of the index to one fifth, the total cost of our method is 3 to 15 times less than the current techniques.","Databases,
Costs,
Euclidean distance,
Marine vehicles,
Computer science,
Indexing,
Indexes,
Stock markets,
Video compression,
Multidimensional systems"
A scalable location management scheme in mobile ad-hoc networks,"In ad-hoc networks, geographical routing protocols take advantage of location information so that stateless and efficient routing is feasible. However such routing protocols are heavily dependent on the existence of scalable location management services. We present a novel scheme to perform scalable location management. With any location management schemes, a specific node, A, in the network trusts a small subset of nodes, namely its location servers, and periodically updates them with its location. Our approach adopts a similar strategy, but a different and original approach to select such location servers. First, we present a selection algorithm used to designate location servers of a node by its identifier. Second, we propose a hierarchical addressing model for mobile ad-hoc networks, where node locations could be represented at different accuracy levels. With this approach, different location servers may carry location information of different levels of accuracy and only a small set of location servers needs to be updated when the node moves. Through rigorous theoretical analysis, we are able to show that the control message overhead is bounded under our scheme. Finally, simulation results are presented to demonstrate the performance of our location management scheme.","Intelligent networks,
Ad hoc networks,
Routing protocols,
Network servers,
Databases,
Computational modeling,
Computer simulation,
Computer network management,
Mobile computing,
Computer science"
Detecting design flaws via metrics in object-oriented systems,"The industry is nowadays confronted with large-scale monolithic and inflexible object-oriented software. Because of their high business value, these legacy systems must be re-engineered. One of the important issues in re-engineering is the detection and location of design flaws, which prevent the efficient maintenance and further development of the system. In this paper, we present a metrics-based approach for detecting design problems, and we describe two concrete techniques for the detection of two well-known design flaws found in the literature. We apply our technique to an industrial case study and discuss the findings. The proposed technique indeed found real flaws in the system and the experiment suggests that, based on the same approach, further detection techniques for other common design flaws could be defined.","Computer industry,
Concrete,
Software systems,
Computer science,
Large-scale systems,
Vents,
Encapsulation,
Power generation economics,
Employment,
Intelligent systems"
Case study: extreme programming in a university environment,"Extreme programming (XP) is a new and controversial software process for small teams. A practical training course at the University of Karlsruhe led to the following observations about the key practices of XP. First, it is unclear how to reap the potential benefits of pair programming, although pair programming produces high-quality code. Second, designing in small increments appears to be problematic but ensures rapid feedback about the code. Third, while automated testing is helpful, writing test cases before coding is a challenge. Last, it is difficult to implement XP without coaching. This paper also provides some guidelines for those starting out with XP.","Computer aided software engineering,
Writing,
Programming,
Documentation,
Automatic testing,
Computer science,
Feedback,
Guidelines,
Software engineering,
Inspection"
A new algorithm to correct fish-eye- and strong wide-angle-lens-distortion from single images,"The use of super-wide angle and fish-eye lenses causes strong distortions in the resulting images. A methodology for the correction of distortions in these cases using only single images and linearity of imaged objects is presented. Contrary to most former algorithms, the algorithm discussed here does not depend on information about the real world co-ordinates of matching points. Moreover reference points determination and camera calibration is not required in this case. The algorithm is based on circle fitting. It requires only the possibility of the extraction of distorted image points from straight lines in the 3D scene. Further, the actual distortion must approximately fit the chosen distortion model. For most fish-eye lenses appropriate distortion correction results can be obtained.",
A subspace approach to layer extraction,"Representing images with layers has many important applications, such as video compression, motion analysis, and 3D scene analysis. This paper presents an approach to reliably extracting layers from images by taking advantages of the fact that homographies induced by planar patches in the scene form a low dimensional linear subspace. Layers in the input images will be mapped in the subspace, where it is proven that they form well-defined clusters and can be reliably identified by a simple mean-shift based clustering algorithm. Global optimality is achieved since all valid regions are simultaneously taken into account, and noise can be effectively reduced by enforcing the subspace constraint. Good layer descriptions are shown to be extracted in the experimental results.","Subspace constraints,
Tiles,
Motion analysis,
Image analysis,
Layout,
Computer vision,
Motion estimation,
Computer science,
Video compression,
Clustering algorithms"
A configurable and extensible transport protocol,"The ability to configure transport protocols from collections of smaller software modules allows the characteristics of the protocol to be customized for a specific application or network technology. This paper describes an approach to building such customized protocols using Cactus, a system in which micro-protocols implementing individual attributes of transport can be combined into a composite protocol that realizes the desired overall functionality. In contrast with similar systems, Cactus supports non-hierarchical module composition and event-driven execution, both of which increase flexibility and allow finer-grain modules implementing orthogonal properties. To illustrate this approach, the design and implementation of a configurable transport protocol called CTP is presented. CTP allows customization of a number of properties including reliable transmission, congestion detection and control, jitter control, and message ordering. This suite of micro-protocols has been implemented using Cactus/C 2.0 on Red Hat Linux 6.2, with initial experimental results indicating that the ability to target the guarantees more precisely to the needs of applications can in fact result in better performance.","Transport protocols,
Linux,
Prototypes,
Computer science,
Application software,
Jitter,
Wireless networks,
Standards development,
Bandwidth,
Streaming media"
Computational electromagnetics of metal nanoparticles and their aggregates,The absorption and scattering of light by silver and gold nanoparticles and their aggregates provides a powerful mechanism for detecting important molecules in a variety of applications. The authors describe recent advances in determining nanoparticle optical properties using computational electromagnetics.,"Computational electromagnetics,
Nanoparticles,
Optical scattering,
Electromagnetic wave absorption,
Electromagnetic scattering,
Light scattering,
Silver,
Gold,
Aggregates,
Optical computing"
Multiresolution feature extraction for unstructured meshes,"We present a framework to extract mesh features from unstructured two-manifold surfaces. Our method computes a collection of piecewise linear curves describing the salient features of surfaces, such as edges and ridge lines. We extend these basic techniques to a multiresolution setting which improves the quality of the results and accelerates the extraction process. The extraction process is semi-automatic, that is, the user is required to input a few control parameters and to select the operators to be applied to the input surface. Our mesh feature extraction algorithm can be used as a preprocessor for a variety of applications in geometric modeling including mesh fairing, subdivision and simplification.","Feature extraction,
Signal resolution,
Image edge detection,
Signal processing algorithms,
Energy resolution,
Piecewise linear techniques,
Solid modeling,
Computer science,
Iterative algorithms,
Filters"
Some good quantum error-correcting codes from algebraic-geometric codes,"It is shown that quantum error correction can be achieved by the use of classical binary codes or additive codes over F/sub 4/. In this correspondence, with the help of some algebraic techniques the theory of algebraic-geometric codes is used to construct an asymptotically good family of quantum error-correcting codes and other classes of good quantum error-correcting codes. Our results are compared with the known best quantum codes.",Error correction coding
Computer controlled piezo micromanipulation system for biomedical applications,This paper presents the development of a computer-controlled piezo manipulation system for biomedical applications such as intra-cytoplasmic sperm injection (ICSI). The hardware set-up and control strategies are described in detail to illustrate the advantages of this approach compared to manually based injection methods. When used for ICSI an improved success rate in terms of oocyte survival and fertilisation is achieved.,"medical robotics,
biomedical equipment,
piezoelectric actuators,
microactuators,
computerised control,
obstetrics,
micromanipulators"
Variability in the execution of multimedia applications and implications for architecture,"Multimedia applications are an increasingly important workload for general-purpose processors. This paper analyzes frame-level execution time variability for several multimedia applications on general-purpose architectures. There are two reasons for such an analysis. First, it has been conjectured that complex features of such architectures (e.g., out-of-order issue) result in unpredictable execution times, making them unsuitable for meeting real-time requirements of multimedia applications. Our analysis tests this conjecture. Second, such an analysis can be used to effectively employ recently proposed adaptive architectures. We find that while execution time varies from frame to frame for many multimedia applications, the variability is mostly caused by the application algorithm and the media input. Aggressive architectural features induce little additional variability (and unpredictability) in execution time, in contrast to conventional wisdom. The presence of frame-level execution time variability motivates frame-level architectural adaptation (e.g., to save energy). Additionally, our results show that execution time generally varies slowly, implying it is possible to dynamically predict the behavior of future frames on a variety of hardware configurations for effective adaptation.","Application software,
Computer architecture,
Testing,
Multimedia systems,
Handheld computers,
Computer science,
Out of order,
Hardware,
Telephony,
Portable computers"
Adaptive multicast topology inference,"The use of end-to-end multicast traffic measurements has been recently proposed as a means to infer network internal characteristics as packet link loss rate and delay. We propose an algorithm that infers the multicast tree topology based on these end-to-end measurements. It is different from previous approaches which make only partial use of the available information, this algorithm adaptively combines different performance measures to reconstruct the topology. We establish its consistency and evaluate its accuracy through simulation. We show that in general it requires many fewer probes to correctly identify the topology than other methods.","Multicast algorithms,
Network topology,
Delay estimation,
Multicast protocols,
Telecommunication traffic,
Current measurement,
Statistics,
Computer science,
Loss measurement,
Probes"
Nonparametric regression estimation using penalized least squares,We present multivariate penalized least squares regression estimates. We use Vapnik-Chervonenkis (see Statistical Learning Theory 1998) theory and bounds on the covering numbers to analyze convergence of the estimates. We show strong consistency of the truncated versions of the estimates without any conditions on the underlying distribution.,Statistics
Three-dimensional tracking of coronary arteries from biplane angiographic sequences using parametrically deformable models,"A new method for coronary artery tracking in biplane digital subtraction is presented. The dynamic tracking of nonrigid objects from two views is achieved using a generalization of parametrically deformable models. Three-dimensional (3-D) Fourier descriptors used for shape representation are obtained from the two-dimensional (2-D) descriptors of the projections. A new constraint inferred from epipolar geometry is applied to the contour model. Direct 3-D tracking is compared with the classical approach in two steps: independent 2-D tracking in each of the two projection planes; 3-D reconstruction using the epipolar constraint. Convergence quality and accuracy of the 3-D reconstruction are analyzed for several sequences showing different displacement amplitudes, deformation rates and image contrasts.","Arteries,
Deformable models,
Three dimensional displays,
Shape,
Two dimensional displays,
Geometry,
Solid modeling,
Convergence,
Image analysis,
Image sequence analysis"
Jitter control in QoS networks,"We study jitter control in networks with guaranteed quality of service (QoS) from the competitive analysis point of view: we propose on-line algorithms that control jitter and compare their performance to the best possible (by an off-line algorithm) for any given arrival sequence. For delay jitter, where the goal is to minimize the difference between delay times of different packets, we show that a simple on-line algorithm using a buffer of B slots guarantees the same delay jitter as the best off-line algorithm using buffer space B/2. We prove that the guarantees made by our on-line algorithm hold, even for simple distributed implementations, where the total buffer space is distributed along the path of the connection, provided that the input stream satisfies a certain simple property. For rate jitter, where the goal is to minimize the difference between inter-arrival times, we develop an on-line algorithm using a buffer of size 2B+h for any h/spl ges/1, and compare its jitter to the jitter of an optimal off-line algorithm using buffer size B. We prove that our algorithm guarantees that the difference is bounded by a term proportional to B/h.","Jitter,
Intelligent networks,
Quality of service,
Propagation delay,
Computer science,
Communication system traffic control,
Performance analysis,
Algorithm design and analysis,
Helium,
IP networks"
Change blindness in information visualization: a case study,,"Blindness,
Computer aided software engineering,
Layout,
Displays,
Information analysis,
Laboratories,
Data visualization,
Contracts,
Cognitive science,
Context"
Botanical visualization of huge hierarchies,,"Data visualization,
Tree graphs,
Data structures,
Solid modeling,
Data mining,
Mathematics,
Computer science,
Books,
Three dimensional displays,
Two dimensional displays"
Molding the flow of light,"A new class of materials, called photonic crystals, affects a photon's properties in much the some way that a semiconductor affects an electron's properties. The ability to mold and guide light leads naturally to novel applications in several fields, including optoelectronics and telecommunications. The authors present an introductory survey of the basic concepts and ideas, including results for never before possible photon phenomena. The paper considers how computer calculations and design play particularly important and complementary roles in experimental investigations of photonic crystals.","Photonic crystals,
Maxwell equations,
Crystalline materials,
Semiconductor materials,
Electrons,
Optical computing,
Frequency,
Conducting materials,
Optical control,
Dielectrics"
HMRSVP: a hierarchical mobile RSVP protocol,"We propose a hierarchical Mobile RSVP (HMRSVP) that can achieve mobility independent QoS guaranteed services in mobile computing environments. The HMRSVP integrates RSVP with Mobile IP regional registration and makes advance resource reservations only when an inter-region movement may possibly happen. We first show that, by NS simulation, that our HMRSVP can achieve the same QoS guarantees as MRSVP with fewer resource reservations. Then, we show that HMRSVP outperforms MRSVP in terms of reservation blocking, forced termination and session completion probabilities.","Protocols,
Mobile computing,
Signal resolution,
Computer science,
Educational institutions,
Computational modeling,
Intserv networks,
Web and internet services,
Encapsulation,
Resource management"
Collaborative exploration for map construction,"We consider the problem of map learning while maintaining ground-truth pose estimates. Map learning is important in tasks that require a model of the environment or some of its features. As a robot collects data, uncertainty about its position accumulates and corrupts its knowledge of the positions from which observations are taken. We address this problem by employing cooperative localization; that is, deploying a second robot to observe the other as it explores, thereby establishing a virtual tether, and enabling an accurate estimate of the robot's position while it constructs the map. The paper presents our approach to this problem in the context of learning a set of visual landmarks useful for pose estimation. In addition to developing a formalism and concept, we validate our results experimentally and present quantitative results demonstrating the performance of the method.","Collaboration,
Robot sensing systems,
Computer science,
Machine learning,
Uncertainty,
Sonar measurements,
Computational efficiency,
Kalman filters,
Filtering,
Collaborative work"
A magnification lens for interactive volume visualization,"Volume visualization of large data sets suffers from the same problem that many other visualization modalities suffer from: either one can visualize the entire data set and lose small details or visualize a small region and lose the context. The authors we present a magnification lens technique for volume visualization. While the notion of a magnification-lens is not new, and other techniques attempt to simulate the physical properties of a magnifying lens, our contribution is in developing a magnification lens that is fast, can be implemented using a fairly small software overhead, and has a natural, intuitive appearance. The issue with magnification lens is the border, or transition region. The lens center and exterior have a constant zoom factor, and are simple to render. It is the border region that blends between the external and interior magnification, and has a nonconstant magnification. We use the ""perspective-correct textures"" capability, available in most current graphics systems, to produce a lens with a tessellated border region that approximates linear compression with respect to the radius of the magnification lens. We discuss how a ""cubic"" border can mitigate the discontinuities resulting from the use of a linear function, without significant performance loss. We discuss various issues concerning development of a three-dimensional magnification lens.","Lenses,
Data visualization,
Rendering (computer graphics),
Displays,
Pixel,
Navigation,
Scientific computing,
Laboratories,
Image processing,
Computer science"
New buyers' arrival under dynamic pricing market microstructure: the case of group-buying discounts on the Internet,"The current research studies the dynamics of one instance of dynamic pricing-group-buying discounts-used by MobShop.com, whose products' selling prices drop as more buyers place their orders. We use an econometric model to analyze changes in the number of orders for Mobshop-listed products over various periods of time. We find that the number of existing orders has a significant positive effect on new orders placed during each 3-hour period, indicating the presence of a positive participation externality effect. We also find evidence for expectations of falling prices, a price drop effect. Results also reveal a significant ending effect, as more orders were placed during the last 3-hour period of the auction cycles.",
Online reconfiguration in replicated databases based on group communication,"Over the last years, many replica control protocols have been developed that take advantage of the ordering and reliability semantics of group communication primitives to simplify database system design and to improve performance. Although current solutions are able to mask site failures effectively, many of them are unable to cope with recovery of failed sites, merging of partitions, or joining of new sites. This paper addresses this important issue. It proposes efficient solutions for online system reconfiguration providing new sites with a current state of the database without interrupting transaction processing in the rest of the system. Furthermore, the paper analyzes the impact of cascading reconfigurations, and argues that they call be handled in an elegant way by extended forms of group communication.","Transaction databases,
Communication system control,
Computer science,
Control systems,
Database systems,
Protocols,
Merging,
Availability,
Distributed databases,
Fault tolerance"
Asymptotically good quantum codes exceeding the Ashikhmin-Litsyn-Tsfasman bound,"It is known that quantum error correction can be achieved using classical binary codes or additive codes over F/sub 4/. Asymptotically good quantum codes have been constructed from algebraic-geometry codes and a bound on (/spl delta/, R) was computed from the Tsfasman-Vladut-Zink bound of the theory of classical algebraic-geometry codes. In this correspondence, by the use of a concatenation technique we construct a family of asymptotically good quantum codes exceeding the bound in a small interval.",Error correction coding
Anisotropic 2-D and 3-D averaging of fMRI signals,A novel method for denoising functional magnetic resonance imaging temporal signals is presented in this note. The method is based on progressively enhancing the temporal signal by means of adaptive anisotropic spatial averaging. This average is based on a new metric for comparing temporal signals corresponding to active fMRI regions. Examples are presented both for simulated and real two and three-dimensional data. The software implementing the proposed technique is publicly available for the research community.,"Anisotropic magnetoresistance,
Magnetic resonance imaging,
Protocols,
Noise reduction,
Spatial resolution,
Electroencephalography,
Engineering profession,
Brain modeling,
Humans,
Blood"
"Dynamic, reliability-driven scheduling of parallel real-time jobs in heterogeneous systems","In this paper, a heuristic dynamic scheduling scheme for parallel real-time jobs in a heterogeneous system is presented. The parallel real-time jobs studied in this paper are modelled by directed acyclic graphs (DAG). We assume a scheduling environment where parallel real-time jobs arrive at a heterogeneous system following a Poisson process. The scheduling algorithms developed in this paper take the reliability measure into account, in order to enhance the reliability of the heterogeneous system without any additional hardware cost. In addition, scheduling time and dispatch time are both incorporated into our scheduling scheme so as to make the scheduling result more realistic and precise. Admission control is in place so that a parallel real-time job whose deadline cannot be guaranteed is rejected by the system. The performance of the proposed scheme is evaluated via extensive simulations. The simulation results show that the heuristic algorithm performs significantly better than two other algorithms that do not consider reliability cost. Furthermore, results suggest that shortening the scheduling time results in a higher guarantee ratio. Hence, if parallel scheduling algorithm is devised and employed to shorten the scheduling time, the performance of the heterogeneous system will be further enhanced.","Dynamic scheduling,
Real time systems,
Processor scheduling,
Scheduling algorithm,
Costs,
Heuristic algorithms,
Computer science,
Reliability engineering,
Concurrent computing,
Information technology"
Robustness in complex systems,"The paper argues that a common design paradigm for systems is fundamentally flawed, resulting in unstable, unpredictable behavior as the complexity of the system grows. In this flawed paradigm, designers carefully attempt to predict the operating environment and failure modes of the system in order to design its basic operational mechanisms. However, as a system grows in complexity, the diffuse coupling between the components in the system inevitably leads to the butterfly effect, in which small perturbations can result in large changes in behavior We explore this in the context of distributed data structures, a scalable, cluster-based storage server We then consider a number of design techniques that help a system to be robust in the face of the unexpected, including overprovisioning, admission control, introspection, adaptivity through closed control loops. Ultimately, however, all complex systems eventually must contend with the unpredictable. Because of this, we believe systems should be designed to cope with failure gracefully.","Robustness,
Routing,
Throughput,
Storms,
Computer science,
Design engineering,
Data structures,
Robust control,
Control systems,
Jacobian matrices"
Fast edge-preserving noise reduction for ultrasound images,"Edge-preserving noise reduction is an essential operation for computer-aided ultrasound image processing and understanding. This paper describes a novel filter which is a two-dimensional (2-D) extension of the one-dimensional (1-D) Savitzky-Golay filter. The new filter, referred to as the 2-D weighted Savitzky-Golay filter, is based on the least squares fitting of a polynomial function to image intensities. The performance of the proposed filter has been compared with that of the commonly used median filter in reducing speckle noise on a synthetic image and an ultrasound thyroid image. Experimental results indicate that on these particular examples, the new filter can achieve at least the same level of noise reduction and edge preservation as that of the median filter, but with far less computation time. Since its complexity scales linearly with the problem size, the new filter is suitable for filtering problems with large windows. In addition, its performance also shows to be less sensitive to the size of the filtering window compared to the median filter.","Noise reduction,
Ultrasonic imaging,
Speckle,
Low pass filters,
Nonlinear filters,
Least squares methods,
Polynomials,
Filtering,
Image processing,
Two dimensional displays"
Two-body segmentation from two perspective views,"We consider a scene containing two independently and generally moving objects, viewed by two general perspective views. Using matching points arising from both objects simultaneously we derive a geometrical constraint, applicable to points from both objects, we call the segmentation matrix. We then use this constraint in order to recover the fundamental matrices associated with, each object, or simply to segment the scene into the two objects. Moreover, when the two bodies move in pure translation relative to each other we can both segment the scene and recover the affine calibration (homography at infinity) of the camera geometry. Unlike algorithms suggested in the past we need only two images, we work with general projective cameras (rather than affine or orthographic) and with general body motion, and no prior information beyond point matches is required.","Image segmentation,
Layout,
Cameras,
Computer science,
Calibration,
Symmetric matrices,
H infinity control,
Impedance matching,
Geometry,
Solid modeling"
Web engineering: creating a discipline among disciplines,"Web engineering is a discipline among disciplines, cutting across computer science, information systems, and software engineering, as well as benefiting from several non-IT specializations. Intertwining so many disciplines presents a unique problem for organization and development. The authors discuss Web engineering's classification, define its characteristics, and contrast its present issues with previous problems in information technology.","Information technology,
Application software,
Software engineering,
Design engineering,
Graphics,
Computer science,
Technology management,
Protocols,
Web page design,
Web sites"
An interactive broadcasting protocol for video-on-demand,"Broadcasting protocols reduce the cost of video-on-demand services by distributing more efficiently videos that are likely to be simultaneously watched by several viewers. Unfortunately, they do not allow the customer to pause, move fast forward or backward while watching a video. We present an interactive pagoda broadcasting protocol that provides these functions at a very reasonable cost. Our protocol is based on the pagoda broadcasting protocol and requires a set-top box buffer large enough to keep in storage all video data until the customer has watched the entire video. As a result, rewind and pause interactions do not require any server intervention. To minimize the bandwidth requirements of fast forward interactions, the server only transmits the segments that are not available on any of the server broadcasting channels. We evaluate the overhead of these fast forward operations through a probabilistic model. Our data indicate that the most costly fast forward operations are those starting at the beginning of the video and jumping to the beginning of the second half of the video while most fast-forward operation taking place during the second half of the video require little or no additional data.",
Data broadcasting and seamless channel transition for highly demanded videos,"One way to broadcast a popular video is to use a number of dedicated channels, each responsible for broadcasting some portion of the video periodically in a predefined way. The stress on the channels can be alleviated, and new viewers do not have to wait long to start their playback. Many approaches falling in this category have been proposed. One such scheme that interests us is the fast broadcasting (FB) scheme, which can broadcast a video using k channels by incurring at most O(D/2/sup k/) waiting time on new-coming viewers, where D is the length of the video. We consider a set of videos, each being broadcast by the FB scheme. Since the demand levels on these videos may change with time, it is sometimes inevitable to change the numbers of channels assigned to some videos. We propose a novel seamless channel transition enhancement on top of the FB scheme to dynamically change the number of channels assigned to a video on-the-fly. Clients currently viewing this video will not experience any disruption because of the transition. A channel allocation scheme is also proposed based on the arrival rates of videos to minimize the average waiting experienced by all viewers. From the system manager's point of view, the enhancement will make the FB scheme more attractive.","Bandwidth,
Multimedia communication,
Stress,
Channel allocation,
Computer science,
Cable TV,
Digital video broadcasting,
Urban areas,
Computer architecture,
Telecommunications"
Concast: design and implementation of an active network service,"Concast is a network layer service that provides many-to-one channels: multiple sources send messages toward one destination, and the network delivers a single ""merged"" copy to that destination. As we have defined it, the service is generic but the relationship between the sent and received messages can be customized for particular applications. We describe the concast service and show how it can be implemented in a back ward-compatible manner in the Internet. We describe its use to solve a problem that has eluded scalable end-system-only solutions: collecting feedback in multicast applications. Our preliminary analysis of concasting effectiveness shows that it provides significant benefits, even with partial deployment. We argue that concast has the characteristics needed for a programmable service to be widely accepted and deployed in the Internet.",
Penalized discriminant analysis of [/sup 15/O]-water PET brain images with prediction error selection of smoothness and regularization hyperparameters,"The authors propose a flexible, comprehensive approach for analysis of [/sup 15/O]-water positron emission tomography (PET) brain images using a penalized version of linear discriminant analysis (PDA). They applied it to scans from 20 subjects (eight scans/subject) performing a finger movement task and analyzed: (1) two classes to obtain a covariance-normalized baseline-activation image, and (2) eight classes for the mean within subject temporal structure which contained baseline-activation and time-dependent changes in a two-dimensional canonical subspace. The authors imposed spatial smoothness on the resulting image(s) by expanding it in five tensor-product B-spline (TPS) bases of varying smoothness, and further regularized with a ridge-type penalty on the noise covariance matrix. The discrimination approach of PDA provides a probabilistic framework within which prediction error (PE) estimates are derived. The authors used these to optimize over TPS bases and a ridge hyperparameter (expressed as equivalent degrees of freedom, EDF). They obtained unbiased, low variance PE estimates using modern resampling tools (.632+ Bootstrap and cross validation), and compared PDA of (1) TPS-projected, mean-normalized and unnormalized scans and (2) mean-normalized scans with and without additional presmoothing. By examining the tradeoffs between PE and EDF, as a function of basis selection and image smoothing the authors demonstrate the utility of PDA, the PE framework, and the relationship between singular value decomposition and smooth TPS bases in the analysis of functional neuroimages.","Image analysis,
Positron emission tomography,
Brain,
Linear discriminant analysis,
Fingers,
Performance analysis,
Spline,
Covariance matrix,
Smoothing methods,
Singular value decomposition"
Likelihood maximization for list-mode emission tomographic image reconstruction,"The maximum a posteriori (MAP) Bayesian iterative algorithm using priors that are gamma distributed, due to Lange, Bahn and Little, is extended to include parameter choices that fall outside the gamma distribution model. Special cases of the resulting iterative method include the expectation maximization maximum likelihood (EMML) method based on the Poisson model in emission tomography, as well as algorithms obtained by Parra and Barrett and by Huesman et al. that converge to maximum likelihood and maximum conditional likelihood estimates of radionuclide intensities for list-mode emission tomography. The approach taken here is optimization-theoretic and does not rely on the usual expectation maximization (EM) formalism. Block-iterative variants of the algorithms are presented. A self-contained, elementary proof of convergence of the algorithm is included.","Image reconstruction,
Iterative algorithms,
Detectors,
Event detection,
Iterative methods,
Single photon emission computed tomography,
Bayesian methods,
Maximum likelihood detection,
Maximum likelihood estimation,
Positron emission tomography"
Performance analysis of iCAR (integrated cellular and ad-hoc relay system),"iCAR is a new wireless architecture based on the integration of cellular and modern ad-hoc relaying technologies. We analyze its performance and compare it with conventional cellular system. In particular, we prove that due to the ability of ad-hoc relay stations (ARS) to relay traffic from one cell to another cell dynamically. iCAR has a lower system-wide call blocking probability than any corresponding cellular system without ARS, even if traffic can be evenly distributed among cells. We also study two typical scenarios and present some numeric results.",
Data flow analysis for software prefetching linked data structures in Java,"Describes an effective compile-time analysis for software prefetching in Java. Previous work in software data prefetching for pointer-based codes asses simple compiler algorithms and does not investigate prefetching for object-oriented language features that male compile-time analysis difficult. We develop a new data flow analysis to detect regular accesses to linked data structures in Java programs. We use intra- and inter-procedural analysis to identify profitable prefetching opportunities for greedy and jump-pointer prefetching, and we implement these techniques in a compiler for Java. Our results show that both prefetching techniques improve four of our ten programs. The largest performance improvement is 48% with jump-pointers, but consistent improvements are difficult to obtain.","Data analysis,
Prefetching,
Data structures,
Java,
Delay,
Algorithm design and analysis,
Program processors,
Computer science,
Software performance,
Object oriented modeling"
Performance of scheduling scientific applications with adaptive weighted factoring,,"Computational modeling,
Grid computing,
Load management,
Application software,
Processor scheduling,
Runtime,
Computer applications,
Dynamic scheduling,
Concurrent computing,
Computer science"
Differentiated reliability (DiR) in WDM rings without wavelength converters,"The concept of differentiated reliability, (DiR) was recently introduced by the authors to provide multiple reliability degrees (or classes) at the same network layer using a common protection mechanism, e.g., path switching. According to the DiR concept, each connection at the layer under consideration is guaranteed a minimum reliability degree, defined as the maximum failure probability allowed for that connection. The reliability degree chosen for a given connection is thus determined by the application requirements, and not by the actual network topology, design constraints, robustness of the network components, and span of the connection. In the paper the DiR concept is applied to designing the wavelength division multiplexing (WDM) layer of a ring network in which wavelength conversion is not available. To solve the routing and wavelength assignment problem at the WDM layer an efficient algorithm is proposed that resorts to reusable protection wavelengths while guaranteeing the required reliability degree of each connection. Lower bounds on the network bandwidth required by two approaches-respectively based on non-reusable and reusable protection wavelengths-reveal interesting properties of the DiR concept and the proposed algorithm.","Wavelength division multiplexing,
Protection,
Telecommunication network reliability,
Costs,
Computer network reliability,
Network topology,
Contracts,
Reliability engineering,
Computer science,
Robustness"
Sequential and parallel algorithms for mixed packing and covering,"We describe sequential and parallel algorithms that approximately solve linear programs with no negative coefficients (aka mixed packing and covering problems). For explicitly given problems, our fastest sequential algorithm returns a solution satisfying all constraints within a 1/spl plusmn//spl epsi/ factor in O(mdlog(m)//spl epsi//sup 2/) time, where m is the number of constraints and d is the maximum number of constraints any variable appears in. Our parallel algorithm runs in time polylogarithmic in the input size times /spl epsi//sup -4/ and uses a total number of operations comparable to the sequential algorithm. The main contribution is that the algorithms solve mixed packing and covering problems (in contrast to pure packing or pure covering problems, which have only ""/spl les/"" or only ""/spl ges/"" inequalities, but not both) and run in time independent of the so-called width of the problem.","Parallel algorithms,
Lagrangian functions,
Iterative algorithms,
Polynomials,
Differential equations,
Computer science,
Partitioning algorithms,
Linear approximation,
Constraint optimization,
Ellipsoids"
FedEx - a fast bridging fault extractor,"Test pattern generation and diagnosis algorithms that target realistic bridging faults must be provided with a realistic fault list. In this work we describe FedEx, a bridging fault extractor that extracts a circuit from the mask layout, identifies the two-node bridges that can occur, their locations, layers, and relative probability of occurrence. Our experimental results show that FedEx is memory efficient and fast.","Circuit faults,
Fault diagnosis,
Bridge circuits,
Circuit testing,
Manufacturing,
Software tools,
Integrated circuit manufacture,
Distributed computing,
Wires,
Computer science"
A rapid look-up table method for reconstructing MR images from arbitrary K-space trajectories,"Look-up tables (LUTs) are a common method for increasing the speed of many algorithms. Their use can be extended to the reconstruction of nonuniformly sampled k-space data using either a discrete Fourier transform (DFT) algorithm or a convolution-based gridding algorithm. A table for the DFT would be precalculated arrays of weights describing how each data point affects all of image space. A table for a convolution-based gridding operation would be a precalculated table of weights describing how each data point affects a small k-space neighborhood. These LUT methods were implemented in C++ on a modest personal computer system; they allowed a radial k-space acquisition sequence, consisting of 180 view's of 256 points each, to be gridded in 36.2 ms, or, in approximately 800 ns/point. By comparison, a similar implementation of the gridding operation, without LUTs, required 45 times longer (1639.2 ms) to grid the same data. This was possible even while using a 4/spl times/4 Kaiser-Bessel convolution kernel, which is larger than typically used. These table-based computations will allow real time reconstruction in the future and can currently be run concurrently with the acquisition allowing for completely real-time gridding.","Table lookup,
Image reconstruction,
Discrete Fourier transforms,
Magnetic resonance imaging,
Radiology,
Biomedical engineering,
Hospitals,
Hardware,
Application software,
Microcomputers"
An efficient bottom-up distance between trees,,"Pattern matching,
Tree graphs,
Software measurement,
Costs,
Algorithm design and analysis,
Signal processing algorithms,
Pattern analysis,
Extraterrestrial measurements,
Computer science,
Application software"
A model for moldable supercomputer jobs,"The performance of supercomputer schedulers is influenced by the workloads that serve as their input. Realistic workloads are therefore critical to evaluate how supercomputer schedulers perform in practice. There has been much written in the literature about rigid parallel jobs, i.e. jobs that require partitions of a fixed size to run. However the majority of the parallel jobs in production today are moldable, i.e. jobs that can execute on a variety of partition sizes. In this paper we describe a workload model for moldable jobs, which is based on a user survey and good analytical models. Our model can serve as the basis for the development of performance-efficient strategies for selection of the job partition size, as well as the basis for enhancing supercomputer schedulers to directly accept moldable request.","Supercomputers,
Processor scheduling,
Job production systems,
Concurrent computing,
Computer science,
Performance evaluation,
Analytical models,
US Department of Energy,
NASA,
Turning"
"2D vs 3D, implications on spatial memory","Since the introduction of graphical user interfaces (GUI) and two-dimensional (2D) displays, the concept of space has entered the information technology (IT) domain. Interactions with computers were re-encoded in terms of fidelity to the interactions with real environment and consequently in terms of fitness to cognitive and spatial abilities. A further step in this direction was the creation of three-dimensional (3D) displays which have amplified the fidelity of digital representations. However, there are no systematic results evaluating the extent to which 3D displays better support cognitive spatial abilities. The aim of this research is to empirically investigate spatial memory performance across different instances of 2D and 3D displays. Two experiments were performed. The displays used in the experimental situation represented hierarchical information structures. The results of the test show that the 3D display does improve performances in the designed spatial memory task.","Three dimensional displays,
Graphical user interfaces,
Two dimensional displays,
Information science,
Computer displays,
Space technology,
Information technology,
Testing,
Performance evaluation,
Production"
Fast Matrix Multiplies Using Graphics Hardware,"We present a technique for large matrix-matrix multiplies using low cost graphics hardware. The result is computed by literally visualizing the computations of a simple parallel processing algorithm. Current graphics hardware technology has limited precision and thus limits immediate applicability of our algorithm. We include results demonstrating proof of concept, correctness, speedup, and a simple application. This is therefore forward looking research: a technique ready for technology on the horizon.","Hardware,
Computer graphics,
Concurrent computing,
Rendering (computer graphics),
Computer science,
Visualization,
Parallel processing,
Distributed computing,
Permission,
Costs"
A trend analysis of exploitations,"We have conducted an empirical study of a number of computer security exploits and determined that the rates at which incidents involving the exploit are reported to CERT can be modeled using a common mathematical framework. Data associated with three significant exploits involving vulnerabilities in phf, imap, and bind can all be modeled using the formula C=I+S/spl times//spl radic/M where C is the cumulative count of reported incidents, M is the time since the start of the exploit cycle, and I and S are the regression coefficients determined by analysis of the incident report data. Further analysis of two additional exploits involving vulnerabilities in mountd and statd confirm the model. We believe that the models will aid in predicting the severity of subsequent vulnerability exploitations, based on the rate of early incident reports.","Data analysis,
Predictive models,
Computer science,
Educational institutions,
System software,
Software engineering,
Risk management,
Regression analysis,
Performance analysis,
Data mining"
FastMesh: efficient view-dependent meshing,"The article presents an optimized view-dependent meshing framework for adaptive and continuous level-of-detail (LOD) rendering in real-time. Multiresolution triangle mesh representations are an important tool for adapting triangle mesh complexity in real-time rendering environments. Ideally, for interactive visualization, a triangle mesh is simplified to the maximal tolerated perceptual error, and thus mesh simplification is view-dependent. The paper introduces an efficient hierarchical multiresolution triangulation framework based on a half-edge triangle mesh data structure, and presents an optimized computation of several view-dependent error metrics within that framework, providing conservative error bounds. The presented approach called FastMesh, is highly efficient both in space and time cost, and it spends only a fraction of the time required for rendering to perform the error calculations and dynamic mesh updates.","Computer errors,
Computer graphics,
Pipelines,
Data structures,
Computer science,
Data visualization,
Costs,
Rendering (computer graphics),
Error correction,
Chromium"
A scalable algorithm for clustering sequential data,"In recent years, we have seen an enormous growth in the amount of available commercial and scientific data. Data from domains such as protein sequences, retail transactions, intrusion detection, and Web-logs have an inherent sequential nature. Clustering of such data sets is useful for various purposes. For example, clustering of sequences from commercial data sets may help marketer identify different customer groups based upon their purchasing patterns. Grouping protein sequences that share similar structure helps in identifying sequences with similar functionality. Over the years, many methods have been developed for clustering objects according to their similarity. However these methods tend to have a computational complexity that is at least quadratic on the number of sequences. In this paper we present an entirely different approach to sequence clustering that does not require an all-against-all analysis and uses a near-linear complexity K-means based clustering algorithm. Our experiments using data sets derived from sequences of purchasing transactions and protein sequences show that this approach is scalable and leads to reasonably good clusters.","Clustering algorithms,
Proteins,
Algorithm design and analysis,
Partitioning algorithms,
Contracts,
Computer science,
Computational complexity,
Intrusion detection,
US Department of Energy,
Military computing"
Estimating probabilistic timing performance for real-time embedded systems,"In system-level design of real-time embedded systems, being able to capture the interactions among the tasks with respect to timing constraints and determine the overall system timing performance is a major challenge. Most previous works in the area are either based on a fixed execution time model or are only concerned with the probabilistic timing behavior of each individual task. The few papers that deal with overall system probabilistic behavior have used improper assumptions. In this paper, given that the execution time of each task is a discrete random variable, a novel concept of state is introduced based on a new metric that is derived that measures the probability of a task set being able to be scheduled. Several approaches to evaluating the metric are also presented. Applying this metric in the system-level design exploration process, one can readily compare the probabilistic timing performance of alternative designs.","Timing,
Real time systems,
Embedded system,
System-level design,
Aircraft navigation,
Control systems,
Computer science,
Random variables,
Time measurement,
Aerospace control"
A joint physics-based statistical deformable model for multimodal brain image analysis,"A probabilistic deformable model for the representation of multiple brain structures is described. The statistically learned deformable model represents the relative location of different anatomical surfaces in brain magnetic resonance images (MRIs) and accommodates their significant variability across different individuals. The surfaces of each anatomical structure are parameterized by the amplitudes of the vibration modes of a deformable spherical mesh. For a given MRI in the training set, a vector containing the largest vibration modes describing the different deformable surfaces is created. This random vector is statistically constrained by retaining the most significant variation modes of its Karhunen-Loeve expansion on the training population. By these means, the conjunction of surfaces are deformed according to the anatomical variability observed in the training set. Two applications of the joint probabilistic deformable model are presented: isolation of the brain from MRI using the probabilistic constraints embedded in the model and deformable model-based registration of three-dimensional multimodal (magnetic resonance/single photon emission computed tomography) brain images without removing nonbrain structures. The multi-object deformable model may be considered as a first step toward the development of a general purpose probabilistic anatomical atlas of the brain.","Deformable models,
Image analysis,
Magnetic resonance imaging,
Anatomical structure,
Image segmentation,
Brain modeling,
Shape,
Single photon emission computed tomography,
Biology,
Biomedical imaging"
End-to-end statistical delay service under GPS and EDF scheduling: a comparison study,"Generalized processor sharing (GPS) has gained much popularity as a simple and effective scheduling mechanism for the provisioning of quality of service (QoS) in emerging high-speed networks. For supporting deterministic end-to-end delay guarantees, GPS is known to be sub-optimal in comparison to the earliest deadline first (EDF) scheduling discipline; nevertheless it is often prefered over EDF due to its simplicity. In this paper, using analytical frameworks developed in the literature, we reassess the merits of GPS as compared to EDF in the setting of statistical delay service. Our contributions are threefold. The statistical frameworks in the literature enable the aggregate losses (i.e., delay bound violations) at an EDF scheduler to be estimated-our first contribution, therefore, is to develop a mechanism that allows the aggregate losses to translate to per-flow guarantees. This is achieved by means of a simple packet discard scheme that drops packets fairly then delay violations are imminent at the EDF scheduler. The discard mechanism has a constant complexity and is feasible for implementation in current packet switches. The ability to derive the per-flow guarantees from the aggregate allows a direct comparison between EDF and GPS-our next contribution, therefore, is to show for various traffic mixes with given per-flow loss constraints that EDF offers consistently larger schedulable regions than GPS, both in the single-hop and multi-hop setting. As our final contribution, we argue that the use of GPS for statistical delay support is inherently problematic. We demonstrate that achieving the maximal schedulable regions under GPS could necessitate dynamic resynchronization of the GPS weights, an operation considered infeasible for practical implementation.","Global Positioning System,
Processor scheduling,
Quality of service,
Optimal scheduling,
Aggregates,
Delay estimation,
Packet switching,
Switches,
Computer science,
Drives"
Mobility-adaptive protocols for managing large ad hoc networks,"We propose a new protocol for efficiently managing large ad hoc networks, i.e., networks in which all nodes can be mobile. We observe that, since nodes in such networks are not necessarily equal in that they may have different resources, not all of them should be involved in basic network operations such as packet forwarding, flooding, etc. In the proposed protocol, a small subset of the network nodes is selected based on their status and they are organized to form a backbone (whence the name ""backbone protocol"" or simply B-protocol to our proposed solution). The B-protocol operates in two phases: first the ""most suitable"" nodes are selected to serve as backbone nodes, then the selected nodes are linked to form a backbone which is guaranteed to be connected if the original network is. The effectiveness of the B-protocol in constructing and maintaining in face of node mobility and node/link failure a connected backbone that uses only a small fraction of the nodes and of the links of the original networks is demonstrated via simulation. The obtained results show that both the selected backbone nodes and the links between them in the backbone are considerably smaller than the nodes and the links in the flat network.","Ad hoc networks,
Spine,
Computer network management,
Computer science,
Electronic mail,
Routing protocols,
Multicast protocols,
Bandwidth,
Microstrip,
Wireless application protocol"
Prediction of software reliability: a comparison between regression and neural network non-parametric models,"In this paper, neural networks have been proposed as an alternative technique to build software reliability growth models. A feedforward neural network was used to predict the number of faults initially resident in a program at the beginning of a test/debug process. To evaluate the predictive capability of the developed model, data sets from various projects were used. A comparison between regression parametric models and neural network models is provided.","Software reliability,
Neural networks,
Predictive models,
Software testing,
Feedforward neural networks,
Artificial neural networks,
Application software,
Equations,
Computer science,
Parametric statistics"
A novel code assignment scheme for W-CDMA systems,"The third-generation wideband CDMA (W-CDMA) systems support the relative higher and variable bit rate transmissions for applications with various quality-of-service (QoS) requirements. Code assignment and reassignment schemes in W-CDMA are essential, with the aim of boosting utilization of codes. A dynamic code assignment scheme (DCA) is proposed for traffic with various QoS classes introduced by Universal Mobile Telecommunication System (UMTS). From the simulation results, the proposed DCA can reduce the new call blocking probability and thus improve the system utilization for W-CDMA.","Multiaccess communication,
3G mobile communication,
Multicarrier code division multiple access,
Hardware,
Wideband,
Quality of service,
Transceivers,
Computer science,
Electronic mail,
Bit rate"
The architecture of the Remos system,"Remos provides resource information to distributed applications. Its design goals of scalability, flexibility, and portability are achieved through an architecture that allows components to be positioned across the network, each collecting information about its local network. To collect information from different types of networks and from hosts on those networks, Remos provides several collectors that use different technologies, such as SNMP or benchmarking. By matching the appropriate collector to each particular network environment and by providing an architecture for distributing the output of these collectors across all querying environments, Remos collects appropriately detailed information at each site and distributes this information where needed in a scalable manner. Prediction services are integrated at the user-level, allowing history-based data collected across the network to be used to generate the predictions needed by a particular user. Remos has been implemented and tested in a variety of networks and is in use in a number of different environments.","Computer science,
Scalability,
Computer architecture,
Educational institutions,
System testing,
US Government,
Condition monitoring,
Real time systems,
Bandwidth,
Delay"
Code placement and replacement strategies for wideband CDMA OVSF code tree management,"The use of OVSF codes in WCDMA systems has offered opportunities to provide variable data rates to flexibly support applications with different bandwidth requirements. Two rarely addressed issues in such environments are the code placement problem and code replacement problem. The former may have significant impact on code utilization and thus code blocking probability, while the latter may affect the code reassignment cost if dynamic code assignment is to be conducted. The general objective is to make the OVSF code tree as compact as possible in order to support new calls, either with less blocking or with less reassignment cost. This paper is perhaps the first one which addresses these issues in WCDMA. Three simple strategies, which can be adopted by both code placement and code replacement, are proposed: random, leftmost, and crowded-first. Among them the crowded-first strategy looks most promising, which is shown to be able to reduce, for example, the code blocking probability by 77% and the number of reassignments by 81% as opposed to the random strategy when the system is 80% fully loaded and the max SF=256.","Wideband,
Multiaccess communication,
Costs,
3G mobile communication,
Telecommunication standards,
Computer science,
Data engineering,
Bandwidth,
Engineering management,
Chaotic communication"
On the wavelength assignment problem in multifiber WDM star and ring networks,"This paper studies-the off-line wavelength assignment problem in star and ring networks that deploy multiple fibers between nodes and use wavelength division multiplexing (WDM) for transmission. The results in this paper show that the ability to switch between fibers increases wavelength utilization. In particular, sharper per-fiber bounds on the number of required wavelengths are derived for the multifiber version of the assignment problem in star and ring networks. Additionally, the complexity of the problem is studied and several constrained versions of the problem are also considered for star and ring networks. A summary of contributions is provided.","Wavelength assignment,
Intelligent networks,
Wavelength division multiplexing,
Switches,
Wavelength routing,
WDM networks,
Optical fiber networks,
Bandwidth,
Computer science"
Expander-based constructions of efficiently decodable codes,"We present several novel constructions of codes which share the common thread of using expander (or expander-like) graphs as a component. The expanders enable the design of efficient decoding algorithms that correct a large number of errors through various forms of ""voting"" procedures. We consider both the notions of unique and list decoding, and in all cases obtain asymptotically good codes which are decodable up to a ""maximum"" possible radius and either: (a) achieve a similar rate as the previously best known codes but come with significantly faster algorithms, or (b) achieve a rate better than any prior construction with similar error-correction properties. Among our main results are: i) codes of rate /spl Omega/(/spl epsi//sup 2/) over constant-sized alphabet that can be list decoded in quadratic time from (1-/spl epsi/) errors; ii) codes of rate /spl Omega/(/spl epsi/) over constant-sized alphabet that can be uniquely decoded from (1/2-/spl epsi/) errors in near-linear time (this matches AG-codes with much faster algorithms); iii) linear-time encodable and decodable binary codes of positive rate (in fact, rate /spl Omega/(/spl epsi//sup 2/)) that can correct up to (1/4-/spl epsi/) fraction errors.","Decoding,
Error correction codes,
Binary codes,
Computer errors,
Encoding,
Yarn,
Algorithm design and analysis,
Error correction,
Computer science,
Communication channels"
A model checking approach to evaluating system level dynamic power management policies for embedded systems,"System Level Power Management policies are typically based on moving the system to various power management states, in order to achieve minimum wastage of power The major challenge in devising such strategies is that the input task arrival rates to a system is usually unpredictable, and hence the power management strategies have to be designed as on-line algorithms. These algorithms are aimed at optimizing wasted power in the face of nondeterministic task arrivals. Previous works on evaluating power management strategies for optimality, have used trace driven simulations, and competitive analysis. In this work we build upon the competitive analysis based paradigm. Our work views a power management strategy as a winning strategy in a two player game, between the power management algorithm, and a non-deterministic adversary. With the power of non-determinism, we can generate the worst possible scenarios in terms of possible traces of tasks. Such scenarios not only disprove conjectured bounds on the optimality of a power management strategy, but also guides the designer towards a better policy. One could also prove such bounds automatically. To achieve these, we exploit model checkers used in formal verification. However, specific tools which are focused mainly on this kind of power management strategies are under development, which would alleviate some of the state explosion problems inherent in model checking techniques.","Power system modeling,
Energy management,
Power system management,
Embedded system,
Algorithm design and analysis,
Stochastic systems,
Analytical models,
Power dissipation,
Embedded computing,
Computer science"
A fault model for subtype inheritance and polymorphism,"Although program faults are widely studied, there are many aspects of faults that we still do not understand, particularly about OO software. In addition to the simple fact that one important goal during testing is to cause failures and thereby detect faults, a full understanding of the characteristics of faults is crucial to several research areas. The power that inheritance and polymorphism brings to the expressiveness of programming languages also brings a number of new anomalies and fault types. This paper presents a model for the appearance and realization of OO faults and defines and discusses specific categories of inheritance and polymorphic faults. The model and categories can be used to support empirical investigations of object-oriented testing techniques, to inspire further research into object-oriented testing and analysis, and to help improve design and development of object-oriented software.",
Divide-and-conquer learning and modular perceptron networks,"A novel modular perceptron network (MPN) and divide-and-conquer learning (DCL) schemes for the design of modular neural networks are proposed. When a training process in a multilayer perceptron falls into a local minimum or stalls in a flat region, the proposed DCL scheme is applied to divide the current training data region into two easier to be learned regions. The learning process continues when a self-growing perceptron network and its initial weight estimation are constructed for one of the newly partitioned regions. Another partitioned region will resume the training process on the original perceptron network. Data region partitioning, weight estimating and learning are iteratively repeated until all the training data are completely learned by the MPN. We evaluated and compared the proposed MPN with several representative neural networks on the two-spirals problem and real-world dataset. The MPN achieved better weight learning performance by requiring much less data presentations during the network training phases, and better generalization performance, and less processing time during the retrieving phase.","Multilayer perceptrons,
Neural networks,
Training data,
Computer science,
Resumes,
Information retrieval,
Neurons,
Backpropagation algorithms,
Pursuit algorithms,
Councils"
Considering power variations of DVS processing elements for energy minimisation in distributed systems,"Dynamic voltage scaling (DVS) is a powerful technique to reduce power dissipation in embedded systems. In this paper we investigate the problem of considering DVS-processing element (DVS-PE) power variations dependent on the executed tasks, during the synthesis of distributed embedded systems, and its impact on the energy savings. Unlike previous approaches, which minimise the energy consumption by exploiting the available slack time without considering the PE power profiles, a new and fast heuristic for the voltage scaling problem is proposed, which improves the voltage selection for each task dependent on the individual power dissipation caused by that task. Experimental results show that energy reductions with up to 80.7% were achieved by integrating the proposed DVS algorithm, which considers the PE power profiles, into the co-synthesis of distributed systems.","Voltage control,
Embedded system,
Power dissipation,
Dynamic voltage scaling,
Energy consumption,
Computer science,
Power system reliability,
Frequency,
Permission,
Switching circuits"
A bound on mutual information for image registration,"An upper bound is derived for the mutual information between a fixed image and a deformable template containing a fixed number of gray-levels. The bound can be calculated by maximizing the entropy of the template under the constraint that the conditional entropy of the template, given the fixed image, be zero. This bound provides useful insight into the properties of mutual information as a similarity metric for deformable image registration. Specifically, it indicates that maximizing mutual information may not necessarily produce an optimal solution when the deformable transform is too flexible.","Mutual information,
Image registration,
Entropy,
Upper bound,
Signal processing,
Random variables,
Delay estimation,
Delay effects,
Analysis of variance,
Interpolation"
Collaborative representations: supporting face to face and online knowledge-building discourse,"The present widespread interest in the use of electronic media for learning presents an unprecedented opportunity for leveraging the computational medium's strengths for learning. However, existing software tools provide only primitive support for online knowledge-building discourse. Further work is needed in supporting coordinated use of disciplinary representations, discourse representations, and knowledge representations. The paper introduces the concept of representational guidance for discourse along with results of an initial study of this phenomenon in face to face situations. The paper then considers the requirements for supporting asynchronous online knowledge-building discourse, finding existing computer mediated communication tools to be particularly deficient in supporting artifact-centered discourse. A solution is proposed that coordinates discourse representations with disciplinary and knowledge representations.",
Variable grouping in multivariate time series via correlation,"The decomposition of high-dimensional multivariate time series (MTS) into a number of low-dimensional MTS is a useful but challenging task because the number of possible dependencies between variables is likely to be huge. This paper is about a systematic study of the ""variable groupings"" problem in MTS. In particular, we investigate different methods of utilizing the information regarding correlations among MTS variables. This type of method does not appear to have been studied before. In all, 15 methods are suggested and applied to six datasets where there are identifiable mixed groupings of MTS variables. This paper describes the general methodology, reports extensive experimental results, and concludes with useful insights on the strength and weakness of this type of grouping method.",
An interior point iterative maximum-likelihood reconstruction algorithm incorporating upper and lower bounds with application to SPECT transmission imaging,"The algorithm we consider here is a block-iterative (or ordered subset) version of the inferior point algorithm for transmission reconstruction. Our algorithm is an interior point method because each vector of the iterative sequence {x/sup k/}, k= 0, 1, 2,..., satisfies the constraints a/sub j/","Reconstruction algorithms,
Iterative algorithms,
Image reconstruction,
Maximum likelihood estimation,
Attenuation,
Iterative methods,
Pixel,
Computer simulation,
Torso,
Imaging phantoms"
Duality-based subsequence matching in time-series databases,"The authors propose a subsequence matching method, Dual Match, which exploits duality in constructing windows and significantly improves performance. Dual Match divides data sequences into disjoint windows and the query sequence into sliding windows, and thus, is a dual approach of the one by C. Faloutsos et al. (1994), which divides data sequences into sliding windows and the query sequence into disjoint windows. We formally prove that our dual approach is correct, i.e., it incurs no false dismissal. We also prove that, given the minimum query length, there is a maximum bound of the window size to guarantee correctness of Dual Match and discuss the effect of the window size on performance. FRM causes a lot of false alarms by storing minimum bounding rectangles rather than individual points representing windows to avoid excessive storage space required for the index. Dual Match solves this problem by directly storing points, but without incurring excessive storage overhead. Experimental results show that, in most cases, Dual Match provides large improvement in both false alarms and performance over FRM, given the same amount of storage space. In particular, for low selectivities (less than 10/sup -4/), Dual Match significantly improves performance up to 430-fold. On the other hand, for high selectivities(more than 10/sup -2/), it shows a very minor degradation (less than 29%). For selectivities in between (10/sup -4//spl sim/10/sup -2/), Dual Match shows performance slightly better than that of FRM. Dual Match is also 4.10/spl sim/25.6 times faster than FRM in building indexes of approximately the same size. Overall, these results indicate that our approach provides a new paradigm in subsequence matching that improves performance significantly in large database applications.","Databases,
Euclidean distance,
Tin,
Moon,
Computer science,
Information technology,
Degradation,
Data mining,
Exchange rates,
Biomedical measurements"
Transparent access to multiple bioinformatics information sources,"This paper describes the Transparent Access to Multiple Bioinformatics Information Sources project, known as TAMBIS, in which a domain ontology for molecular biology and bioinformatics is used in a retrieval-based information integration system for biologists. The ontology, represented using a description logic and managed by a terminology server, is used both to drive a visual query interface and as a global schema against which complex intersource queries are expressed. These source-independent declarative queries are then rewritten into collections of ordered source-dependent queries for execution by a middleware layer. In bioinformatics, the majority of data sources are not databases but tools with limited accessible interfaces. The ontology helps manage the interoperation between these resources. The paper emphasizes the central role that is played by the ontology in the system. The project distinguishes itself from others in the following ways: the ontology, developed by a biologist, is substantial; the retrieval interface is sophisticated; the description logic is managed by a sophisticated terminology server. A full pilot application is available as a Java applet integrating five sources concerned with proteins. This pilot is currently undergoing field trials with working biologists and is being used to answer real questions in biology, one of which is used as a case study throughout the paper.",
A location-aided power-aware routing protocol in mobile ad hoc networks,"In multi-hop wireless ad-hop networks, designing energy-efficient routing protocols is critical since nodes are power-constrained. However, it is also an inherently hard problem due to two important factors: First, the nodes may be mobile, demanding the energy-efficient routing protocol to be fully distributed and adaptive to the current states of nodes; Second, the wireless links may be uni-directional due to asymmetric power configurations of adjacent nodes. In this paper, we propose a location-aided power-aware routing protocol that dynamically makes local routing decisions so that a near-optimal power-efficient end-to-end route is formed for forwarding data packets. The protocol is fully distributed such that only location information of neighboring nodes are exploited in each routing node. Through rigorous theoretical analysis for our distributed protocol based on greedy algorithms, we are able to derive critical global properties with respect to end-to-end energy-efficient routes. Finally, preliminary simulation results are presented to verify the performance of our protocol.","Routing protocols,
Intelligent networks,
Mobile ad hoc networks,
Ad hoc networks,
Energy efficiency,
Relays,
Algorithm design and analysis,
Greedy algorithms,
Energy consumption,
Computer science"
Characteristic features of research article titles in computer science,"Previous researchers have given conflicting views as to what makes a ""good"" research article (RA) title. In this paper, characteristic features of research article titles, including length, punctuation usage, word frequency, and preposition usage are investigated using a corpus of 600 research articles from the six journals of the IEEE Computer Society. Results show, while some of the intuitive observations made in the literature about title writing are accurate for computer science journals, other observations have ignored the effects of discipline and field variation. Subsequently, these observations are either unjustified or misleading.",
SHriMP views: an interactive environment for exploring Java programs,"The paper describes a demonstration of the SHriMP visualization tool. SHriMP provides a flexible and customizable environment for exploring software programs. It supports the embedding of multiple views, both graphical and textual within a nested graph display of a program's software architecture. SHriMP has recently been redesigned and reimplemented using Java Bean components. These APIs allow SHriMP to be easily integrated with other software understanding tools. The article demonstrates the use of SHriMP for exploring and browsing Java programs.","Java,
Visualization,
Packaging,
Software tools,
Software prototyping,
Prototypes,
Computer science,
Computer displays,
Software architecture,
Software maintenance"
Generalized mosaicing,"We present an approach that significantly enhances the capabilities of traditional image mosaicing. The key observation is that as a camera moves, it senses each scene point multiple times. We rigidly attach to the camera an optical filter with spatially varying properties, so that multiple measurements are obtained for each scene point under different optical settings. Fusing the data captured in the multiple images yields an image mosaic that includes additional information about the scene. This information can come in the form of extended dynamic range, high spectral quality, or enhancements to other dimensions of imaging. We refer to this approach as generalized mosaicing. The approach was tested using a filter with spatially varying transmittance and a standard 8-bit black/white video camera, to achieve image mosaicing with dynamic range comparable to imaging with a 16-bit camera. In another experiment, we attached a spatially varying spectral filter to the same camera to obtain mosaics that represent the spectral distribution (rather than the usual RGB measurements) of each scene point. We also discuss how generalized mosaicing can be used to explore other imaging dimensions.",
An application of zero-inflated Poisson regression for software fault prediction,"Poisson regression model is widely used in software quality modeling. When the response variable of a data set includes a large number of zeros, Poisson regression model will underestimate the probability of zeros. A zero-inflated model changes the mean structure of the pure Poisson model. The predictive quality is therefore improved. In this paper, we examine a full-scale industrial software system and develop two models, Poisson regression and zero-inflated Poisson regression. To our knowledge, this is the first study that introduces the zero-inflated Poisson regression model in software reliability. Comparing the predictive qualities of the two competing models, we conclude that for this system, the zero-inflated Poisson regression model is more appropriate in theory and practice.","Application software,
Software quality,
Predictive models,
Software systems,
Economic forecasting,
Software reliability,
Software testing,
Fault diagnosis,
Software engineering,
Computer science"
Multispeaker speech activity detection for the ICSI meeting recorder,"As part of a project into speech recognition in meeting environments, we have collected a corpus of multichannel meeting recordings. We expected the identification of speaker activity to be straightforward given that the participants had individual microphones, but simple approaches yielded unacceptably erroneous labelings, mainly due to crosstalk between nearby speakers and wide variations in channel characteristics. Therefore, we have developed a more sophisticated approach for multichannel speech activity detection using a simple hidden Markov model (HMM). A baseline HMM speech activity detector has been extended to use mixtures of Gaussians to achieve robustness for different speakers under different conditions. Feature normalization and crosscorrelation processing are used to increase the channel independence and to detect crosstalk. The use of both energy normalization and crosscorrelation based postprocessing results in a 35% relative reduction of the frame error rate. Speech recognition experiments show that it is beneficial in this multispeaker setting to use the output of the speech activity detector for presegmenting the recognizer input, achieving word error rates within 10% of those achieved with manual turn labeling.",
HeRMES: high-performance reliable MRAM-enabled storage,"Magnetic RAM (MRAM) is a new memory technology with access and cost characteristics comparable to those of conventional dynamic RAM (DRAM) and the non-volatility of magnetic media such as disk. Simply replacing DRAM with MRAM will make main memory non-volatile, but it will not improve file system performance. However, effective use of MRAM in a file system has the potential to significantly improve performance over existing file systems. The HeRMES file system will use MRAM to dramatically improve file system performance by using it as a permanent store for both file system data and metadata. In particular, metadata operations, which make up over 50% of all file system requests [14], are nearly free in HeRMES because they do not require any disk accesses. Data requests will also be faster, both because of increased metadata request speed and because using MRAM as a non-volatile cache will allow HeRMES to better optimize data placement on disk. Though MRAM capacity is too small to replace disk entirely, HeRMES will use MRAM to provide high-speed access to relatively small units of data and metadata, leaving most file data stored on disk.","File systems,
Random access memory,
Costs,
Nonvolatile memory,
Read-write memory,
Data structures,
Computer science,
DRAM chips,
Pain,
Tiles"
A new text-independent method for phoneme segmentation,A new approach for text-independent speech segmentation is proposed. The novelty consists in a preprocessing based on critical-band perceptual analysis and an original algorithm for the individuation of phoneme boundaries. The results are promising since the method gives /spl sim/74% of correct segmentation without presenting over-segmentation.,"Speech analysis,
Speech processing,
Computer science,
Algorithm design and analysis,
Speech recognition,
Pattern recognition,
Environmental factors,
Humans,
System testing,
Signal processing"
Faster image template matching in the sum of the absolute value of differences measure,"Given an m/spl times/m image I and a smaller n/spl times/n image P, the computation of an (m-n+1)/spl times/(m-n+1) matrix C where C(i, j) is of the form C(i,j)=/spl Sigma//sub k=0//sup n-1//spl Sigma//sub k'=0//sup n-1/f(I(i+k,j+k'), P(k,k')), 0/spl les/i, j/spl les/m-n for some function f, is often used in template matching. Frequent choices for the function f are f(x,y)=(x-y)/sup 2/ and f(x,y)=|m-y|. For the case when f(x,y)=(x-y)/sup 2/, it is well known that C is computable in O(m/sup 2/ log n) time. For the case f(x,y)=|-y|, on the other hand, the brute force O((m-n+1)/sup 2/n/sup 2/) time algorithm for computing C seems to be the best known. This paper gives an asymptotically faster algorithm for computing C when f(x,y)=|x-y|, one that runs in time O(min{s,n//spl radic/log n}m/sup 2/ log n) time, where s is the size of the alphabet, i.e., the number of distinct symbols that appear in I and P. This is achieved by combining two algorithms, one of which runs in O(sm/sup 2/ log n) time, the other in O(m/sup 2/n/spl radic/log n) time. We also give a simple Monte Carlo algorithm that runs in O(m/sup 2/ log n) time and gives unbiased estimates of C.",
Locality vs. criticality,"Current memory hierarchies exploit locality of references to reduce load latency and thereby improve processor performance. Locality based schemes aim at reducing the number of cache misses and tend to ignore the nature of misses. This leads to a potential mis-match between load latency requirements and latencies realized using a traditional memory system. To bridge this gap, we partition loads as critical and non-critical. A load that needs to complete early to prevent processor stalls is classified as critical, while a load that can tolerate a long latency is considered non-critical. In this paper, we investigate if it is worth violating locality to exploit information on criticality to improve processor performance. We present a dynamic critical load classification scheme and show that 40% performance improvements are possible on average, if all critical loads are guaranteed to hit in the LI cache. We then compare the two properties, locality and criticality, in the context of several cache organization and prefetching schemes. We find that the working set of critical loads is large, and hence practical cache organization schemes based on criticality are unable to reduce the critical load miss ratios enough to produce performance gains. Although criticality-based prefetching can help for some resource constrained programs, its benefit over locality-based prefetching is small and may not be worth the added complexity.","Delay,
Prefetching,
Random access memory,
Memory management,
Hardware,
Computer science,
Microprocessors,
Bridges,
Performance gain,
Cache memory"
Variable partitioning for dual memory bank DSPs,"DSPs with dual memory banks offer high memory bandwidth, which is required for high-performance applications. However, such DSP architectures pose problems for C compilers, which are mostly not capable of partitioning program variables between memory banks. As a consequence, time-consuming assembly programming is required for an efficient coding of time-critical algorithms. This paper presents a new technique for automatic variable partitioning between memory banks in compilers, which leads to a higher utilization of available memory bandwidth in the generated machine code. We present experimental results obtained by integrating the proposed technique into an existing C compiler for the AMS Gepard, an industrial DSP core.",
On axiomatic characterization of fuzzy approximation operators. II. The rough fuzzy set based case,"In two previous papers we have developed axiomatic characterizations of approximation operators which are defined by the classical diamond and box operator of the modal logic on the one hand and are defined by the ""fuzzified"" diamond and box operator in applying to crisp sets, i.e. by using the concept of fuzzy rough sets on the other hand. The paper presented is a continuation of the first paper mentioned above by applying the classical diamond and box operators to fuzzy sets, i.e. by using the concepts of rough fuzzy sets.","Fuzzy sets,
Computer aided software engineering,
Fuzzy logic,
Rough sets,
Computer science,
Collaboration"
Hormone-controlled metamorphic robots,"Metamorphic robots with shape-changing capabilities provide a powerful and flexible approach to complex tasks in unstructured environments. However, due to their dynamic topology and decentralized configuration, metamorphic robots demand control mechanisms that go beyond those used by conventional robots. This paper builds on our previous results of hormone-based control, and develops a novel distributed control algorithm called CELL that can select, synchronize, and execute gaits and other reconfiguration actions without assuming any global configuration knowledge. This algorithm is flexible enough to deal with changes of configuration, and can resolve conflicts between locally selected actions and manage multiple active hormones for producing coherent global effects.",
Security implications of typical Grid Computing usage scenarios,"A Computational Grid is a collection of heterogeneous computers and resources spread across multiple administrative domains with the intent of providing users easy access to these resources. There are many ways to access the resources of a Computational Grid, each with unique security requirements and implications for both the resource user and the resource provider. A comprehensive set of Grid usage scenarios is presented and analyzed with regard to security requirements such as authentication, authorization, integrity, and confidentiality. The main value of these scenarios and the associated security discussions is to provide a library of situations against which an application designer can match, thereby facilitating security-aware application use and development from the initial stages of the application design and invocation. A broader goal of these scenarios is to increase the awareness of security issues in Grid Computing. The purpose of this paper is to review the various Grid usage scenarios and analyze their security requirements and implications.","Grid computing,
Application software,
National security,
Computer security,
Computer science,
Laboratories,
Distributed computing,
Authentication,
Software libraries,
Software design"
A Gabor feature classifier for face recognition,"This paper describes a novel Gabor feature classifier (GFC) method for face recognition. The GFC method employs an enhanced Fisher discrimination model on an augmented Gabor feature vector, which is derived from the Gabor wavelet transformation of face images. The Gabor wavelets, whose kernels are similar to the 2D receptive field profiles of the mammalian cortical simple cells, exhibit desirable characteristics of spatial locality and orientation selectivity. As a result, the Gabor transformed face images produce salient local and discriminating features that are suitable for face recognition. The feasibility of the new GFC method has been successfully tested on face recognition using 600 FERET frontal face images, which involve different illumination and varied facial expressions of 200 subjects. The effectiveness of the novel GFC method is shown in terms of both absolute performance indices and comparative performance against some popular face recognition schemes such as the eigenfaces method and some other Gabor wavelet based classification methods. In particular, the novel GFC method achieves 100% recognition accuracy using only 62 features.",
Requirement-based automated black-box test generation,"Testing large software systems is very laborious and expensive. Model-based test generation techniques are used to automatically generate tests for large software systems. However, these techniques require manually created system models that are used for test generation. In addition, generated test cases are not associated with individual requirements. In this paper, we present a novel approach of requirement-based test generation. The approach accepts a software specification as a set of individual requirements expressed in textual and SDL formats (a common practice in the industry). From these requirements, system model is automatically created with requirement information mapped to the model. The system model is used to automatically generate test cases related to individual requirements. Several test generation strategies are presented. The approach is extended to requirement-based regression test generation related to changes on the requirement level. Our initial experience shows that this approach may provide significant benefits in terms of reduction in number of test cases and increase in quality of a test suite.","Automatic testing,
Software testing,
System testing,
Software systems,
Software maintenance,
Costs,
Automation,
Computer science,
Software tools,
Time to market"
Evaluating the reverse engineering capabilities of Web tools for understanding site content and structure: a case study,"This paper describes an evaluation of the reverse engineering capabilities of three Web tools for understanding site content and structure. The evaluation is based on partitioning Web sites into three classes (static, interactive, and dynamic), and is structured using an existing reverse engineering environment framework (REEF). This case study also represents an initial evaluation of the applicability of the REEF in the related but qualitatively different domain of Web sites. The case study highlights several shortcomings of current Web tools in the context of aiding understanding to support evolution. For example, most Web tools are geared towards new page design and development, not to understanding detailed page content or overall site structure. The evaluation also identified some aspects of the REEF that might benefit from refinement to better reflect Web tool capabilities that support common evolution tasks. For example, Web server log file analysis as a specialized form of data gathering and subsequence information presentation.",
High-Performance Remote Access to Climate Simulation Data: A Challenge Problem for Data Grid Technologies,"In numerous scientific disciplines, terabyte and soon petabyte-scale data collections are emerging as critical community resources. A new class of Data Grid infrastructure is required to support management, transport, distributed access to, and analysis of these datasets by potentially thousands of users. Researchers who face this challenge include the Climate Modeling community, which performs long-duration computations accompanied by frequent output of very large files that must be further analyzed. We describe the Earth System Grid prototype, which brings together advanced analysis, replica management, data transfer, request management, and other technologies to support high-performance, interactive analysis of replicated data. We present performance results that demonstrate our ability to manage the location and movement of large datasets from the users desktop. We report on experiments conducted over SciNET at SC2000, where we achieved peak performance of 1.55Gb/s and sustained performance of 512.9Mb/s for data transfers between Texas and California.",
Permutation testing made practical for functional magnetic resonance image analysis,"We describe an efficient algorithm for the step-down permutation test, applied to the analysis of functional magnetic resonance images. The algorithm's time bound is nearly linear, making it feasible as an interactive tool. Results of the permutation test algorithm applied to data from a cognitive activation paradigm are compared with those of a standard parametric test corrected for multiple comparisons. The permutation test identifies more weakly activated voxels than the parametric test, always activates a superset of the voxels activated by this parametric method, almost always yields significance levels greater than or equal to those produced by the parametric method, and tends to enlarge activated clusters rather than adding isolated voxels. Our implementation of the permutation test is freely available as part of a widely distributed software package for analysis of functional brain images.","Magnetic resonance,
Image analysis,
Brain,
Neuroimaging,
Statistical analysis,
Magnetic analysis,
Software testing,
Error correction,
Hospitals,
Algorithm design and analysis"
System-level exploration for Pareto-optimal configurations in parameterized systems-on-a-chip,Provides a technique for efficiently exploring the configuration space of a parameterized system-on-a-chip (SOC) architecture to find all Pareto-optimal configurations. These configurations represent the range of meaningful power and performance tradeoffs that are obtainable by adjusting parameter values for a fixed application mapped onto the SOC architecture. The approach extensively prunes the potentially large configuration space by taking advantage of parameter dependencies. The authors have successfully incorporated the technique into the parameterized SOC tuning environment (Platune) and applied it to a number of applications.,
Using components for rapid distributed software development,"Software development has not reached the maturity of other engineering disciplines; it is still challenging to produce software that works reliably, is easy to use and maintain, and arrives within budget and on time. In addition, relatively small software systems for highly specific applications are in increasing demand. This need requires a significantly different approach to software development from that used by their large, monolithic, general-purpose software counterparts such as Microsoft Word. The paper discusses the use of components for rapid distributed software development. It reports on the the experience of a large testbed called Educational Software Components of Tomorrow (www.escot.org), supported by the US National Science Foundation.","Programming,
Software testing,
Application software,
Scheduling,
State feedback,
Pipelines,
Object oriented modeling,
Performance analysis,
Guidelines,
Analytical models"
Improving subjective estimates using paired comparisons,"Despite the existence of structured methods for software sizing and effort estimation, the so-called ""expert"" approach seems to be the prevalent way to produce estimates in the software industry. This article presents a method based on paired comparisons, which social science researchers use for measuring when there is no accepted measurement scale or when a measurement instrument does not exist. Although not new, the idea has received little attention in the literature.","Computer industry,
Project management,
Binary trees,
Software measurement"
ERS transform for the automated detection of bronchial abnormalities on CT of the lungs,"The identification of bronchi on Computed Tomography (CT) images of the lungs provides valuable clinical information in patients with suspected airways diseases including bronchiectasis, emphysema, or constrictive obliterative bronchiolitis. The automated recognition of the airways is, therefore, an important part of a diagnosis aid system for resolving potential ambiguities associated with intensity-based feature extractors. On CT images, near-perpendicular cross sections of bronchi normally appear as elliptical rings and this paper presents a novel technique for their recognition. The proposed method, the edge-radius-symmetry (ERS) transform, is based on the analysis of the distribution of edges in local polar coordinates. Pixels are ranked according to local edge (E) strength, radial (R), uniformity and local symmetry (S). A discrete implementation of the technique is provided which reduces the computational cost of the ERS transform by using a geometric approximation of the intensity patterns. The identification of the adjacent pulmonary vessels with template matching then allows for the automated measurement of bronchial dilatation and bronchial wall thickening. Computationally, the method compares favorably with other methods such as the Hough transform. Noise-sensitivity of the technique was evaluated on a set of synthetic images and 9 patients under investigation for suspected airways disease. Agreement for the automated scoring of the presence and severity of bronchial abnormalities was demonstrated to be comparable to that of an experienced radiologist (kappa statistics /spl kappa/>0.5).","Lungs,
Computed tomography,
Respiratory system,
Diseases,
Discrete transforms,
Clinical diagnosis,
Feature extraction,
Data mining,
Image recognition,
Computational efficiency"
Multicast with cache (Mcache): an adaptive zero-delay video-on-demand service,"This paper presents a closed-loop (demand-driven) approach towards VoD services, called multicast with caching (Mcache). Servers use multicast to reduce bandwidth usage by serving multiple requests using a single data stream. However, this requires clients to delay receiving the movie until the multicast starts. Using regional cache servers, Mcache removes initial playout delays at the clients, because the clients can receive the prefix of a requested clip from regional caches while waiting for the multicast to start. In addition, the multicast containing the later portion of the movie can wait until the prefix is played out. While this use of caches has been proposed before, the novelty of our scheme lies in that the requests coming after the multicast starts can still be batched together to be served by multicast patches without any playout delays. The use of patches has been proposed to be used either with unicast or with playout delays. Mcache effectively hires the idea of a multicast patch with caches to provide a truly adaptive VoD service whose bandwidth usage is up to par with the best known open-loop schemes under high request rates while using only minimal bandwidth under low request rates. In addition, efficient use of multicast and caches removes the need for a priori knowledge of client request rates and client disk storage requirements which some of the existing schemes assume. This makes Mcache ideal for the current heterogeneous Internet environments where those parameters are hard to predict.",
High dimensional similarity search with space filling curves,"We present a new approach for approximate nearest neighbor queries for sets of high dimensional points under any L/sub t/-metric, t=1,...,/spl infin/. The proposed algorithm is efficient and simple to implement. The algorithm uses multiple shifted copies of the data points and stores them in up to (d+1) B-trees where d is the dimensionality of the data, sorted according to their position along a space filling curve. This is done in a way that allows us to guarantee that a neighbor within an O(d/sup 1+1/t/) factor of the exact nearest, can be returned with at most (d+1)log, n page accesses, where p is the branching factor of the B-trees. In practice, for real data sets, our approximate technique finds the exact nearest neighbor between 87% and 99% of the time and a point no farther than the third nearest neighbor between 98% and 100% of the time. Our solution is dynamic, allowing insertion or deletion of points in O(d log/sub p/ n) page accesses and generalizes easily to find approximate k-nearest neighbors.","Filling,
Nearest neighbor searches,
Neural networks,
Approximation algorithms,
Mathematics,
Computer science,
Data mining,
Image processing,
Pattern recognition,
Sorting"
Dense surface point distribution models of the human face,"In this paper we show how a dense surface model of the human face can be built from a population of examples. A technique that combines active shape models (ASMs) with iterative closest point (ICP) can be used to fit the model to new faces. The model is built by aligning the surfaces using a sparse set of hand-placed landmarks, then using thin-plate spline warping to make a dense correspondence with a base mesh. All of the mesh vertices are then used as landmarks to build a 3D point distribution model. The dense surface point distribution model is more sensitive than the landmark model to correlated facial characteristics such as gender, age and the presence of congenital abnormalities.","Humans,
Face,
Educational institutions,
Active shape model,
Biomedical informatics,
Dentistry,
Spline,
Coatings,
Deformable models,
Computer science"
Volume delineation by fusion of fuzzy sets obtained from multiplanar tomographic images,"Techniques of three-dimensional (3-D) volume delineation from tomographic medical imaging are usually based on 2-D contour definition. For a given structure, several different contours can be obtained depending on the segmentation method used or the user's choice. The goal of this work is to develop a new method that reduces the inaccuracies generally observed. A minimum volume that is certain to be included in the volume concerned (membership degree /spl mu/=1), and a maximum volume outside which no part of the volume is expected to be found (membership degree /spl mu/=0), are defined semi-automatically. The intermediate fuzziness region (0","Fuzzy sets,
Tomography,
Active shape model,
Biomedical imaging,
Image segmentation,
Imaging phantoms,
Fuzzy logic,
Two dimensional displays,
Pathology,
Testing"
Why more choices cause less cooperation in iterated prisoner's dilemma,"The classic iterated prisoner's dilemma (IPD) has only 2 choices, cooperate or defect. However, most real-world situations offer intermediate responses, between full cooperation and full defection. Previous studies observed that with intermediate levels, mutual cooperation is less likely to emerge, and even if it does it is less stable. Exactly why has been a mystery. This paper demonstrates two mechanisms that sabotage the emergence of full mutual cooperation. First, to increase cooperation requires behavioral (phenotypic) diversity to explore different possible outcomes, and once evolution has converged somewhat on a particular degree of cooperation, it is unlikely to shift. Secondly, more choices allows a richer choice of stable strategies that are not simply cooperating with each other to exclude an invader, but which are symbiotic. Such non-symmetric and symbiotic players in the space of strategies act as roadblocks on the path to full cooperation.","Computer science,
Evolution (biology),
Symbiosis,
History,
Biology,
Artificial intelligence,
Sorting,
Scheduling algorithm"
Text extraction from gray scale document images using edge information,"In this paper we present a well designed method that makes use of edge information to extract textual blocks from gray scale document images. It aims at detecting textual regions on heavy noise infected newspaper images and separate them from graphical regions. The algorithm traces the feature points in different entities and then groups those edge points of textual regions. From using the technology of line approximation and layout categorization, it can successfully retrieve directional placed text blocks. Finally feature based connected component merging was introduced to gather homogeneous textual regions together within the scope of its bounding rectangles. We can obtain correct page decomposition with efficient computation and reduced memory size by handling line segments instead of small pixels. The proposed method has been tested on a large group of newspaper images with multiple page layouts, promising results approved the effectiveness of our method.","Data mining,
Image segmentation,
Image texture analysis,
Image edge detection,
Optical character recognition software,
Computer science,
Drives,
Design methodology,
Layout,
Merging"
An FDI approach for sampled-data systems,"In this paper, problems related to fault detection and isolation (FDI) in sampled-data (SD) systems are studied. A tool to analyze SD systems from the viewpoint of FDI and based on it a direct design approach of FDI system are developed. Key of these studies is the introduction of an operator which is used to describe the sampling effect. With the aid of the developed tool, we also study the perfect decoupling problem, the influence of sampling on the performance of FDI system and the relationship between the direct and indirect design approaches. The application of the approach proposed is finally illustrated and compared with the indirect design approaches through examples.","Fault detection,
Application software,
Sampling methods,
Low pass filters,
Signal generators,
Vectors,
Computer science education,
Control system synthesis,
Process control,
Educational technology"
A replacement for Voronoi diagrams of near linear size,"For a set P of n points in R/sup d/, we define a new type of space decomposition. The new diagram provides an /spl epsi/-approximation to the distance function associated with the Voronoi diagram of P, while being of near linear size, for d/spl ges/2. This contrasts with the standard Voronoi diagram that has /spl Omega/ (n/sup [d/2]/) complexity in the worst case.","Chromium,
Neural networks,
Polynomials,
Computer science,
Mesh generation,
Graphics,
Surface reconstruction,
Clustering algorithms,
Artificial intelligence,
Nearest neighbor searches"
Ellipsoid ART and ARTMAP for incremental clustering and classification,"We introduce ellipsoid-ART (EA) and ellipsoid-ARTMAP (EAM) as a generalization of hypersphere ART (HA) and hypersphere-ARTMAP (HAM) respectively. As was the case with HA/HAM, these novel architectures are based on ideas rooted in fuzzy-ART (FA) and fuzzy-ARTMAP (FAM). While FA/FAM aggregate input data using hyper-rectangles, EA/EAM utilize hyper-ellipsoids for the same purpose. Due to their learning rules, EA and EAM share virtually all properties and characteristics of their FA/FAM counterparts. Preliminary experimentation implies that EA and EAM are to be viewed as good alternatives to FA and FAM for data clustering and classification tasks respectively.","Ellipsoids,
Subspace constraints,
Neural networks,
Feedforward systems,
Computer science,
Buildings,
Aggregates,
Resonance,
Feedforward neural networks,
Clustering algorithms"
Functional abstraction driven design space exploration of heterogeneous programmable architectures,"Rapid design space exploration (DSE) of a programmable architecture is feasible using an automatic toolkit (compiler, simulator, assembler) generation methodology driven by an architecture description language (ADL). While many contemporary ADLs can effectively capture one class of architecture, they are typically unable to capture a wide spectrum of processor and memory features present in DSP, VLIW, EPIC and Superscalar processors. The main bottleneck has been the lack of an abstraction underlying the ADL that permits reuse of the abstraction primitives to compose the heterogeneous architectures. We present the functional abstraction needed to capture such wide variety of programmable architectures. We illustrate the usefulness of this approach by specifying two very different architectures using functional abstraction. Our DSE results demonstrate the power of reuse in composing heterogeneous architectures using functional abstraction primitives allowing for a reduction in the time for specification and exploration by at least an order of magnitude.","Space exploration,
VLIW,
Digital signal processing,
Computer architecture,
Computer science,
Random access memory,
Program processors,
Permission,
Computational modeling,
Computer simulation"
Noninvasive estimation of the aorta input function for measurement of tumor blood flow with [/sup 15/O] water,"Quantitative measurement of tumor blood flow with [/sup 15/O] water can be used to evaluate the effects of tumor treatment over time. Since quantitative flow measurements require an input function, we developed the profile fitting method (PFM) to measure the input function from positron emission tomography images of the aorta. First, a [/sup 11/C] CO scan was acquired and the aorta region was analyzed. The aorta diameter was determined by fitting the image data with a model that includes scanner resolution, the measured venous blood radioactivity concentration, and the spillover of counts from the background. The diameter was used in subsequent fitting of [/sup 15/O] water dynamic images to estimate the aorta and background radioactivity concentrations. Phantom experiments were performed to test the model. Image quantification biases (up to 15%) were found for small objects, particularly for those in a large elliptical phantom. However, the bias in the PFM concentration estimates was much smaller (2%-6%). A simulation study showed that PFM had less bias and/or variability in flow parameter estimates than an ROT method. PFM was applied to human [/sup 11/C] CO and [/sup 15/O] water dynamic studies with left ventricle input functions used as the gold standard. PFM parameter estimates had higher variability than found in the simulation but with minimal bias. These studies suggest that PFM is a promising technique for the noninvasive measurement of the aorta [/sup 15/O] water input function.",
Word segmentation in handwritten Korean text lines based on gap clustering techniques,"We propose a word segmentation method for handwritten Korean text lines. It uses gap information to separate a text line into word units, where the gap is defined as a white-run obtained after a vertical projection of the line image. Each gap is classified into a between-word gap or a within-word gap using a clustering technique. We take up three gap metrics - the bounding box (BB), run-length/Euclidean (RLE) and convex hull (CH) distances - which are known to have superior performance in Roman-style word segmentation, and three clustering techniques - the average linkage method, the modified MAX method and sequential clustering. An experiment with 498 text-line images extracted from live mail pieces has shown that the best performance is obtained by the sequential clustering technique using all three gap metrics.","Image segmentation,
Couplings,
Handwriting recognition,
Text recognition,
Pattern recognition,
Machine intelligence,
Computer science,
Postal services,
Character recognition,
Humans"
Effective use of Boolean satisfiability procedures in the formal verification of superscalar and VLIW microprocessors,We compare SAT-checkers and decision diagrams on the evaluation of Boolean formulas produced in the formal verification of both correct and buggy versions of superscalar and VLIW microprocessors. We identify one SAT-checker that significantly outperforms the rest. We evaluate ways to enhance its performance by variations in the generation of the Boolean correctness formulas. We reassess optimizations previously used to speed up the formal verification and probe future challenges.,"Formal verification,
VLIW,
Data structures,
Boolean functions,
Permission,
Computer science,
Probes,
Automatic test pattern generation,
Microprocessors,
Circuits"
Magnetically levitated micro PM motors by two types of active magnetic bearings,"This paper describes magnetically levitated micro permanent magnet (PM) motors by two types of active magnetic bearings. The micro PM motors consist of a cylindrical rotor (/spl phi/2.0 mm/spl times/10 mm), a pair of electromagnets, a pair of photodiodes, and an analog PD controller. The motors are characterized by the small rotor levitated without any mechanical contacts and one-axis controlled active magnetic bearing. Horseshoe-shaped and cylindrical electromagnets are applied to the active magnetic bearing. The rotor successfully rotates, levitating in the center of the electromagnets. In this paper, dynamic characteristics of the two types of micro PM motors, such as relationships between rotation speed and driving current, rotation speed and time, and acceleration and driving current, are discussed. As a result, it is found that the magnetically levitated micro PM motors by two types of active magnetic bearings are very different from each other and very promising.",
Bending invariant representations for surfaces,"Isometric surfaces share the same geometric structure also known as the first fundamental form. For example, bending of a given surface, that includes length preserving deformations without tearing or stretching the surface, are considered to be isometric. We present a method to construct a bending invariant canonical form for such surfaces. This invariant representation is an embedding of the intrinsic geodesic structure of the surface in a finite dimensional Euclidean space, in which geodesic distances are approximated by Euclidean ones. The canonical representation is constructed by first measuring the intergeodesic distances between points on the surfaces. Next, multi-dimensional scaling (MDS) techniques are applied to extract a finite dimensional flat space in which geodesic distances are represented as Euclidean ones. The geodesic distances are measured by the efficient fast marching on triangulated domains numerical algorithm. Applying this transform to various objects with similar geodesic structures (similar first fundamental form) maps isometric objects into similar canonical forms. We show a simple surface classification method based on the bending invariant canonical form.","Level measurement,
Face detection,
Computer science,
Cities and towns,
Computer vision,
Face recognition,
Heuristic algorithms,
Shape,
Quaternions,
Eigenvalues and eigenfunctions"
Techniques for language identification for hybrid Arabic-English document images,"Because of the different characteristics of Arabic language and Romance and Anglo Saxon languages, recognition of documents written in hybrids of these languages requires that the language of the text is to be identified prior to the recognition phase. In this paper, three efficient techniques that can be used to discriminate between text written in Arabic script and text written in English script are presented and evaluated. These techniques address the language identification problem on the word level and on text level. The characteristics of horizontal projection profiles as well as runlength histograms for text written in both languages are the basic features underlying these techniques. Solving this problem is very important in building bilingual document image analysis systems which are capable of processing documents containing hybrid Arabic/Romance and Anglo Saxon languages.","Natural languages,
Optical character recognition software,
Character recognition,
Text analysis,
Computer science,
Image recognition,
Text recognition,
Educational institutions,
TV,
Image analysis"
LPMiner: an algorithm for finding frequent itemsets using length-decreasing support constraint,"Over the years, a variety of algorithms for finding frequent item sets in very large transaction databases has been developed. The key feature in most of these algorithms is that they use a constant support constraint to control the inherently exponential complexity of the problem. In general, item sets that contain only a few items tend to be interesting if they have a high support, whereas long item sets can still be interesting even if their support is relatively small. Ideally, we desire to have an algorithm that finds all the frequent item sets whose support decreases as a function of their length. In this paper, we present an algorithm called LPMiner (Long Pattern Miner) that finds all item sets that satisfy a length-decreasing support constraint. Our experimental evaluation shows that LPMiner is up to two orders of magnitude faster than the FP-growth algorithm for finding item sets at a constant support constraint, and that its run-time increases gradually as the average length of the transactions (and the discovered item sets) increases.","Itemsets,
Transaction databases,
Runtime,
Data mining,
Contracts,
Computer science,
Data engineering,
Association rules,
US Department of Energy,
High performance computing"
Deblurring subject to nonnegativity constraints when known functions are present with application to object-constrained computerized tomography,"The reconstruction of tomographic images is often treated as a linear deblurring problem. When a high-density, man-made metal object is present somewhere in the image field, it is a deblurring problem in which the unknown function has a component that is known except for some location and orientation parameters. The authors first address general linear deblurring problems in which a known function having unknown parameters is present. They then show how the resulting iterative solution can be applied to tomographic imaging in the presence of man-made foreign objects, and they apply the result, in particular, to X-ray computed tomography imaging used in support of brachytherapy treatment of advanced cervical cancer.","Application software,
Computed tomography,
Image reconstruction,
Optical imaging,
X-ray imaging,
Cervical cancer,
Kernel,
Brachytherapy,
Deconvolution,
Books"
Computational modeling of arterial biomechanics,"Factors such as local stresses seem to be important in the development of arterial diseases such as atherosclerosis. The authors describe how computational tools, frequently in conjunction with advanced imaging tools, help us understand these biomechanical effects in health and diseases.",
A Macroscopic Analytical Model of Collaboration in Distributed Robotic Systems,"In this article, we present a macroscopic analytical model of collaboration in a group of reactive robots. The model consists of a series of coupled differential equations that describe the dynamics of group behavior. After presenting the general model, we analyze in detail a case study of collaboration, the stick-pulling experiment, studied experimentally and in simulation by Ijspeert et al. [Autonomous Robots, 11, 149171]. The robots' task is to pull sticks out of their holes, and it can be successfully achieved only through the collaboration of two robots. There is no explicit communication or coordination between the robots. Unlike microscopic simulations (sensor-based or using a probabilistic numerical model), in which computational time scales with the robot group size, the macroscopic model is computationally efficient, because its solutions are independent of robot group size. Analysis reproduces several qualitative conclusions of Ijspeert et al.: namely, the different dynamical regimes for different values of the ratio of robots to sticks, the existence of optimal control parameters that maximize system performance as a function of group size, and the transition from superlinear to sublinear performance as the number of robots is increased.",
Software engineering in the academy,Institutions that teach software are responsible for producing professionals who will build and maintain systems to the satisfaction of their beneficiaries. The article presents some ideas on how best to honor this responsibility. It presents five goals of a curriculum: principles: lasting concepts that underlie the whole field; practices: problem-solving techniques that good professionals apply consciously and regularly; applications: areas of expertise in which the principles and practices find their best expression; tools: state-of-the-art products that facilitate the application of these principles and practices; mathematics: the formal basis that makes it possible to understand everything else.,
Shape from texture and integrability,"We describe a shape from texture method that constructs a maximum a posteriori estimate of surface coefficients using both the deformation of individual texture elements-as in local methods-and the overall distribution of elements-as in global methods. The method described applies to a much larger family of textures than any previous method, local or global. We demonstrate an analogy with shape from shading, and use this to produce a numerical method. Examples of reconstructions for synthetic images of surfaces are provided, and compared with ground truth. The method is defined for orthographic views, but can be generalised to perspective views simply.",
Trust-region methods for real-time tracking,"Optimization methods based on iterative schemes can be divided into two classes: linesearch methods and trust-region methods. While linesearch techniques are commonly found in various vision applications, not much attention is paid to trust-region methods. Motivated by the fact that linesearch methods can be considered as special cases of trust-region methods, we propose to apply trust-region methods to visual tracking problems. Our approach integrates trust-region methods with the Kullback Leibler distance to track a rigid or non-rigid object in real-time. If not limited by the speed of a camera, the algorithm can achieve frame rate above 60 fps. To justify our method, a variety of experiments/comparisons are carried out for the trust-region tracker and a linesearch-based mean-shift tracker with same initial conditions. The experimental results support our conjecture that a trust-region tracker should perform superiorly to a linesearch one.","Optimization methods,
Cameras,
Shape,
Target tracking,
Information science,
Iterative methods,
Color,
Pixel,
Image reconstruction,
Humans"
On-line signature verification using model-guided segmentation and discriminative feature selection for skilled forgeries,"The paper describes an online signature verification system using model-guided segmentation and discriminative feature selection for skilled forgeries. The system is based on segment-to-segment comparison between the input signature and the reference model. To obtain a consistent segmentation, we propose a model-guided segmentation, which segments an input signature by the correspondence with the reference model. To reject skilled forgeries effectively, we use a discriminative feature selection. It is motivated from the observation that a skilled forger can imitate the shape of the genuine signature better than even the owner, that is some features distinguish skilled forgeries from genuine signatures, though some features distinguish only random forgeries. For random forgeries and skilled forgeries respectively, we select the discriminative features among all the features according to the distance between references and forgeries. In the experiment, we collected 1000 genuine signatures and 1000 skilled forgeries. The result showed that the proposed method gave more stable segmentation, and the discriminative feature selection eliminated about 62% of the errors.","Handwriting recognition,
Forgery,
Computer science,
Shape,
Authentication,
Banking,
Access control,
Feature extraction,
Testing"
The build-time software architecture view,"Research and practice in the application of software architecture has reaffirmed the need to consider software systems from several distinct points of view. Previous work by P. Kruchten (1995) and C. Hofmeister et al. (2000) suggests that four or five points of view may be sufficient: the logical view (i.e., the domain object model), the (static) code view, the process/concurrency view, the deployment/execution view, plus scenarios and use-cases. We have found that some classes of software systems exhibit interesting and complex build-time properties that are not explicitly addressed by previous models. In this paper, we present the idea of build-time architectural views. We explain what they are, how to represent them, and how they fit into traditional models of software architecture. We present three case studies of software systems with interesting build-time architectural views, and show how modelling their build-time architectures can improve developer understanding of what the system is and how it is created. Finally, we introduce a new architectural style, the ""code robot"" that is often present in systems with interesting build-time views.","Software architecture,
Taxonomy,
Software systems,
Computer science,
Application software,
Concurrent computing,
Computer industry,
Java,
Robots,
Mathematical model"
Enabling and measuring electronic customer relationship management readiness,"This work provides a comprehensive customer-focused evaluation framework that businesses can use to assess their electronic customer relationship management (e-CRM) readiness. The framework is intended to provide a big picture of the overall composition of e-CRM, to facilitate gap analysis, and to support a monitoring and feedback process. Knowledge management, trust, and technology are identified as key enablers of e-CRM. Finally, we propose weighting and rating scales to aid in assessing customer relationship management readiness, and provide examples of their use.","Customer relationship management,
Monitoring,
Computer science,
Business communication,
Feedback,
Knowledge management,
Supply chain management,
Supply chains,
Consumer electronics,
Couplings"
Contextual clustering for analysis of functional MRI data,"Presents a contextual clustering procedure for statistical parametric maps (SPM) calculated from time varying three-dimensional images. The algorithm can be used for the detection of neural activations from functional magnetic resonance images (fMRI). An important characteristic of SPM is that the intensity distribution of background (nonactive area) is known whereas the distributions of activation areas are not. The developed contextual clustering algorithm divides an SPM into background and activation areas so that the probability of detecting false activations by chance is controlled, i.e., hypothesis testing is performed. Unlike the much used voxel-by-voxel testing, neighborhood information is utilized, an important difference. This is achieved by using a Markov random field prior and iterated conditional modes (ICM) algorithm. However, unlike in the conventional use of ICM algorithm, the classification is based only on the distribution of background. The results from the authors' simulations and human fMRI experiments using visual stimulation demonstrate that a better sensitivity is achieved with a given specificity in comparison to the voxel-by-voxel thresholding technique. The algorithm is computationally efficient and can be used to detect and delineate objects from a noisy background in other applications.","Magnetic resonance imaging,
Scanning probe microscopy,
Clustering algorithms,
Testing,
Magnetic analysis,
Magnetic resonance,
Performance evaluation,
Markov random fields,
Classification algorithms,
Computational modeling"
Salient iso-surface detection with model-independent statistical signatures,"Volume graphics has not been accepted for widespread use. One of the inhibiting reasons is the lack of general methods for data-analysis and simple interfaces for data exploration. An error-and-trial iterative procedure is often used to select a desirable transfer function or mine the dataset for salient iso-values. New semi-automatic methods that are also data-centric have shown much promise. However, general and robust methods are still needed for data-exploration and analysis. In this paper, we propose general model-independent statistical methods based on central moments of data. Using these techniques we show how salient iso-surfaces at material boundaries can be determined. We provide examples from the medical and computational domain to demonstrate the effectiveness of our methods.","Histograms,
Information science,
Shape measurement,
Transfer functions,
Image generation,
Laboratories,
Bridges,
Humans,
Bayesian methods,
Computer vision"
The science and education of mechatronics engineering,"We try to get to the heart of multidisciplinary engineering, of which mechatronics is an excellent example, and point out how the integration of disciplines leads to new degrees of freedom in design and corresponding research directions that otherwise would not have been investigated. This is the major contribution achieved by a multidisciplinary approach to engineering science; it leads to a new important research field and at the same time helps to push research in related fields into new fruitful directions. We point to a number of areas that have benefited from the interdisciplinary perspective and a focus on interactions between disciplines including: engineering curriculum; mechatronics research; control of nonlinear mechanical systems; real time control systems modelling; and time varying control systems.",
Synthesizing distributed systems,"In system synthesis, we transform a specification into a system that is guaranteed to satisfy the specification. When the system is distributed, the goal is to construct the system's underlying processes. Results on multi-player games imply that the synthesis problem for linear specifications is undecidable for general architectures, and is nonelementary decidable for hierarchical architectures, where the processes are linearly ordered and information among them flows in one direction. In this paper, we present a significant extension of this result. We handle both linear and branching specifications, and we show that a sufficient condition for decidability of the synthesis problem is a linear or cyclic order among the processes, in which information flows in either one or both directions. We also allow the processes to have internal hidden variables, and we consider communications with and without delay. Many practical applications fall into this class.",
Privacy-preserving cooperative scientific computations,,
Cost functions and model combination for VaR-based asset allocation using neural networks,"We introduce an asset-allocation framework based on the active control of the value-at-risk of the portfolio. Within this framework, we compare two paradigms for making the allocation using neural networks. The first one uses the network to make a forecast of asset behavior, in conjunction with a traditional mean-variance allocator for constructing the portfolio. The second paradigm uses the network to directly make the portfolio allocation decisions. We consider a method for performing soft input variable selection, and show its considerable utility. We use model combination (committee) methods to systematize the choice of hyperparameters during training. We show that committees using both paradigms are significantly outperforming the benchmark market performance.",
Inferring network characteristics via moment-based estimators,In this work we develop simple inference models based on finite capacity single server queues for estimating the buffer size and the intensity of cross traffic at the bottleneck link of a path between two hosts. Several pairs of moment-based estimators are proposed to estimate these two quantities. The best scheme is then identified through simulation.,"Traffic control,
Bandwidth,
Quality of service,
Diffserv networks,
IP networks,
Web and internet services,
Joining processes,
Computer science,
Monitoring,
Helium"
Global nearness diagram navigation (GND),"Presents the global nearness diagram navigation system for mobile robots. The GND generates motion commands to drive a robot safely between locations, whilst avoiding collisions. This system has all the advantages of using the reactive scheme nearness diagram (ND), while having the ability to reason and plan globally (reaching global convergence to the navigation problem). This framework has been extensively tested using a holonomic mobile base equipped with a laser range-finder. Experiments in unknown, unstructured, dynamic and complex environments are reported to validate the system.","Navigation,
Hybrid power systems,
Robots,
Neodymium,
Convergence,
Motion planning,
Computer science,
Systems engineering and theory,
Mobile computing,
Testing"
Asymptotic behavior of the minimum mean squared error threshold for noisy wavelet coefficients of piecewise smooth signals,"This paper investigates the asymptotic behavior of the minimum risk threshold for wavelet coefficients with additive, homoscedastic, Gaussian noise and for a soft-thresholding scheme. We start from N samples from a signal on a continuous time axis. For piecewise smooth signals and for N/spl rarr//spl infin/, this threshold behaves as C/spl radic/(2logN)/spl sigma/, where /spl sigma/ is the noise standard-deviation. The paper contains an original proof for this asymptotic behavior as well as an intuitive explanation. The paper also discusses the importance of this asymptotic behavior for practical cases when we estimate the minimum risk threshold.","Wavelet coefficients,
Additive noise,
Gaussian noise,
Mean square error methods,
Noise reduction,
Vectors,
Wavelet transforms,
Statistics,
Computer science,
Random variables"
Performance evaluation of an agent-based resource management infrastructure for grid computing,Resource management is an important infrastructure in the grid computing environment. Scalability and adaptability are two key challenges in the implementation of such complex software systems. We introduce a new model for resource management in a metacomputing environment using a hierarchy of homogeneous agents that has the capability of service discovery. The performance of the agent system can be improved using different combinations of optimisation strategies. A modelling and simulation environment has been developed in this work that enables the performance of the system to be investigated. A simplified model of the resource management infrastructure is given as a case study and simulation results are included that show the impact of the choice of performance optimisation strategies on the overall system performance.,
Language games for autonomous robots,"Integration and grounding are key AI challenges for human-robot dialogue. The author and his team are tackling these issues using language games and have experimented with them on progressively more complex platforms. A language game is a sequence of verbal interactions between two agents situated in a specific environment. Language games both integrate the various activities required for dialogue and ground unknown words or phrases in a specific context, which helps constrain possible meanings.",
"On the support of MSE-optimal, fixed-rate, scalar quantizers","This paper determines how the support regions of optimal and asymptotically optimal fixed-rate scalar quantizers (with respect to mean-squared error) depend on the number of quantization points N and the probability density of the variable being quantized. It shows that for asymptotic optimality it is necessary and sufficient that the support region grow fast enough that the outer (or overload) distortion decreases as o(1/N/sup 2/). Formulas are derived for the minimal support of asymptotically optimal quantizers for generalized gamma densities, including Gaussian and Laplacian. Interestingly, these turn out to be essentially the same as for the support of optimal fixed-rate uniform scalar quantizers. Heuristic arguments are then used to find closed-form estimates for the support of truly optimal quantizers for generalized gamma densities. These are found to be more accurate than the best prior estimates, as computed by numerical algorithms. They demonstrate that the support of an optimal quantizer is larger than the minimal asymptotically optimal support by a factor depending on the density but not N, and that the outer distortion of optimal quantizers decreases as 1/N/sup 3/.",Optimization methods
Numerical inversion of Laplace transform using Haar wavelet operational matrices,"In this paper, a unified derivation of the operational matrices of various orthogonal functions including the Haar wavelet is first given. Based on the derived operational matrix, this paper presents a new method for performing numerical inversion of the Laplace transform. Only matrix multiplications and ordinary algebraic operations are involved in the method. The proposed method is much simpler as compared with the dictionary-type method and the contour-integration method.","Laplace equations,
Differential equations,
Discrete wavelet transforms,
Continuous wavelet transforms,
Differential algebraic equations,
Integral equations,
Computer science,
Transfer functions,
Pulse circuits"
Deadline fair scheduling: bridging the theory and practice of proportionate pair scheduling in multiprocessor systems,"The authors present Deadline Fair Scheduling (DFS), a proportionate-fair CPU scheduling algorithm for multiprocessor servers. A particular focus of our work is to investigate practical issues in instantiating proportionate-fair (P-fair) schedulers into conventional operating systems. We show via a simulation study that characteristics of conventional operating systems such as the asynchrony in scheduling multiple processors, frequent arrivals and departures of tasks, and variable quantum durations can cause proportionate-fair schedulers to become non-work-conserving. To overcome this drawback, we combine DFS with an auxiliary work-conserving scheduler to ensure work-conserving behavior at all times. We then propose techniques to account for processor affinities while scheduling tasks in multiprocessor environments. We implement the resulting scheduler in the Linux kernel and evaluate its performance using various applications and benchmarks. Our experimental results show that DFS can achieve proportionate allocation, performance isolation and work-conserving behavior at the expense of a small increase in the scheduling overhead. We conclude that practical considerations such as work-conserving behavior and processor affinities when incorporated into a P-fair scheduler such as DFS can result in a practical approach for scheduling tasks in a multiprocessor operating system.",
Testing random variables for independence and identity,"Given access to independent samples of a distribution A over [n] /spl times/ [m], we show how to test whether the distributions formed by projecting A to each coordinate are independent, i.e., whether A is /spl epsi/-close in the L/sub 1/ norm to the product distribution A/sub 1//spl times/A/sub 2/ for some distributions A/sub 1/ over [n] and A/sub 2/ over [m]. The sample complexity of our test is O/spl tilde/(n/sup 2/3/m/sup 1/3/poly(/spl epsi//sup -1/)), assuming without loss of generality that m/spl les/n. We also give a matching lower bound, up to poly (log n, /spl epsi//sup -1/) factors. Furthermore, given access to samples of a distribution X over [n], we show how to test if X is /spl epsi/-close in L/sub 1/ norm to an explicitly specified distribution Y. Our test uses O/spl tilde/(n/sup 1/2/poly(/spl epsi//sup -1/)) samples, which nearly matches the known tight bounds for the case when Y is uniform.",
Improved cut sequences for partitioning based placement,"Recursive partitioning based placement has a long history, but there has been little consensus on how cut sequences should be chosen. In this paper, we present a dynamic programming approach to cut sequence generation. If certain assumptions hold, these sequences are optimal. After study of these optimal sequences, we observe that an extremely simple method can be used to construct sequences that are near optimal. Using this method, our bisection based placement tool Feng Shui outperforms the previously presented Capo tool by 11% on a large benchmark. By integrating our cut sequence method into Capo, we are able to improve performance by 5%, bringing the results of Feng Shui and Capo closer together.",
EM algorithms of Gaussian mixture model and hidden Markov model,"The HMM (hidden Markov model) is a probabilistic model of the joint probability of a collection of random variables with both observations and states. The GMM (Gaussian mixture model) is a finite mixture probability distribution model. Although the two models have a close relationship, they are always discussed independently and separately. The EM (expectation-maximum) algorithm is a general method to improve the descent algorithm for finding the maximum likelihood estimation. The EM of HMM and the EM of GMM have similar formulae. Two points are proposed in this paper. One is that the EM of GMM can be regarded as a special EM of HMM. The other is that the EM algorithm of GMM based on symbols is faster in implementation than the EM algorithm of GMM based on samples (or on observation) traditionally.",
Real-time out-of-core visualization of particle traces,"Visualization of particle traces provides intuitive and efficient means for the exploration and analysis of complex vector fields. The paper presents a method suitable for the real-time visualization of arbitrarily large time-varying vector fields in virtual environments. We describe an out-of-core scheme in which two distinct pre-processing and rendering components enable real-time data streaming and visualization. The presented approach yields low-latency application start-up times and small memory footprints. The described system was used to implement a ""volumetric fog lance,"" which can emit up to 60000 particles into a flow field while maintaining an interactive frame rate of 60 frames per second. All algorithms were specifically designed to support commodity hardware. The proof-of-concept system is running on a low-cost Linux workstation equipped with a 120 GB E-IDE RAID (redundant array of inexpensive disk) system.","Data visualization,
Computational modeling,
Computational fluid dynamics,
Motion pictures,
Hardware,
Computer graphics,
Interactive systems,
Image processing,
Computer science,
Image analysis"
Fuzzy temporal rules for mobile robot guidance in dynamic environments,"The paper describes a fuzzy control system for the avoidance of moving objects by a robot. The objects move with no type of restriction, varying their velocity and making turns. Due to the complex nature of this movement, it is necessary to realize temporal reasoning with the aim of estimating the trend of the moving object. A new paradigm of fuzzy temporal reasoning, which we call fuzzy temporal rules (FTRs), is used for this control task. The control system has over 117 rules, which reflects the complexity of the problem to be tackled. The controller has been subjected to an exhaustive validation process and examples are shown of the results obtained.",
Direct addressed caches for reduced power consumption,"A direct addressed cache is a hardware-software design for an energy-efficient microprocessor data cache. Direct addressing allows software to access cache data without a hardware cache tag check. These tag-unchecked loads and stores save the energy of a tag check when the compiler can guarantee an access will be to the same line as an earlier access. We have added support for tag-unchecked loads and stores to C and Java compilers. For Mediabench C programs, the compiler eliminates 16-76% of data cache tag accesses, with half of the benchmarks avoiding over 40% of the data tag checks. For SPECjvm98 Java programs, the compiler eliminates 18-63% of data cache tag checks. These tag check reductions translate into data cache energy savings of 9-40%, and overall processor and cache energy savings of 2-8%.",
Real-time traffic transmission over the Internet,"Multimedia applications require the transmission of real-time streams over a network. These streams often exhibit variable bandwidth requirements, and require high bandwidths and guarantees from the network. This creates problems when such streams are delivered over the Internet. To solve these problems, recently, a small set of differentiated services has been introduced. Among these, Premium Service is suitable for transmitting real-time stored stream (full knowledge of the stream characteristics). It uses a bandwidth allocation mechanism (BAM) based on the stream peak rate. Due to the variable bandwidth requirement, the peak rate BAM can waste large amount of bandwidth. In this paper we propose a new BAM that uses less bandwidth than the peak rate BAM, while providing the same service. Our BAM does not affect the real-time stream quality of service (QoS) and does not require any modification to the Premium Service Architecture. We also introduce several frame dropping mechanisms that further reduce bandwidth consumption subject to a QoS constraint when coupled with the above BAM. The proposed BAM and the dropping mechanisms are evaluated using Motion JPEG and MPEG videos and are shown to be effective in reducing bandwidth requirements. Further, since VCR operations are very useful in video streaming, we propose a mechanism that introduces these operations in our BAM. Through simulations we show the effectiveness of this mechanism.","Bandwidth,
Magnesium compounds,
Streaming media,
Videos,
Quality of service,
Telecommunication traffic,
Bit rate,
IP networks,
Web and internet services,
Computer science"
Dynamic voltage scheduling technique for low-power multimedia applications using buffers,"As multimedia applications are used increasingly in many embedded systems, power efficient design for the applications becomes more important than ever. This paper proposes a simple dynamic voltage scheduling technique, which suits the multimedia applications well. The proposed technique fully utilizes the idle intervals with buffers in a variable speed processor. The main theme of this paper is to determine the minimum buffer size to achieve the maximum energy saving in three cases: single-task, multiple subtasks, and multi-task. Experimental results show that the proposed technique is expected to obtain significant power reduction for several real-world multimedia applications.",
A network of dynamically coupled chaotic maps for scene segmentation,"In this paper, a computational model for scene segmentation based on a network of dynamically coupled chaotic maps is proposed. Time evolutions of chaotic maps that correspond to an object in the given scene are synchronized with one another, while this synchronized evolution is desynchronized with respect to time evolution of chaotic maps corresponding to other objects in the scene. In this model, the coupling range of each active element increases dynamically according to predefined rules until a saturated state is achieved, i.e., locally coupled chaotic maps corresponding to an object in the initial state will be coupled globally in the final state. Consequently, the advantage of both global coupling and local coupling are incorporated in a single scheme. In comparison to continuous models, this proposed model is suitable for computational implementation. Another significant benefit is that the good performance and transparent dynamics of the model are obtained by utilizing one-dimensional chaotic map instead of complex neuron as each element.","Chaos,
Layout,
Neurons,
Evolution (biology),
Electroencephalography,
Biological system modeling,
Power system modeling,
Frequency synchronization,
Computer science,
Brain"
An efficient method to identify untestable path delay faults,"Several methods to reduce the run time and memory requirements of a procedure used to efficiently identify untestable path delay faults are proposed in this work. Based on the correlation between the conditions required for sensitizing subpaths in the fan-out-free regions of a circuit, equivalence relations between the subpaths are defined. Equivalence relations are used to reduce the number of subpaths considered in the identification of untestable paths. Dynamic pruning of the potential search space for identifying pairs of subpaths that cannot be sensitized together is used to achieve additional speedup. Results on benchmark circuits show the effectiveness of the proposed methods.",
Low-energy intra-task voltage scheduling using static timing analysis,"We propose an intra-task voltage scheduling algorithm for low-energy hard real-time applications. Based on a static timing analysis technique, the proposed algorithm controls the supply voltage within an individual task boundary. By fully exploiting all the slack times, a scheduled program by the proposed algorithm always completes its execution near the deadline, thus achieving a high energy reduction ratio. In order to validate the effectiveness of the proposed algorithm, we built a software tool that automatically converts a DVS-unaware program into an equivalent low-energy program. Experimental results show that the low-energy version of an MPFG-4 encoder/decoder (converted by the software tool) consumes less than 7-25% of the original program running on a fixed-voltage system with a power-down mode.","Voltage,
Timing,
Energy consumption,
MPEG 4 Standard,
Very large scale integration,
Clocks,
Computer science,
Power engineering and energy,
Scheduling algorithm,
Software tools"
Designing networks for selfish users is hard,"We consider a directed network in which every edge possesses a latency function specifying the time needed to traverse the edge given its congestion. Selfish, noncooperative agents constitute the network traffic and wish to travel from a source s to a sink t as quickly as possible. Since the route chosen by one network user affects the congestion (and hence the latency) experienced by others, we model the problem as a noncooperative game. Assuming each agent controls only a negligible portion of the overall traffic, Nash equilibria in this noncooperative game correspond to s-t flows in which all flow paths have equal latency. We give optimal inapproximability results and approximation algorithms for several network design problems of this type. For example, we prove that for networks with n nodes and continuous, nondecreasing latency functions, there is no approximation algorithm for this problem with approximation ratio less than n/2 (unless P = NP). We also prove this hardness result to be best possible by exhibiting an n/2-approximation algorithm. For networks in which the latency of each edge is a linear function of the congestion, we prove that there is no (4/3 - /spl epsi/)-approximation algorithm for the problem (for any /spl epsi/ > 0, unless P = NP); the existence of a 4/3-approximation algorithm follows easily from existing work, proving this hardness result sharp.",
Segmentation of large brain lesions,This paper describes a region-growing algorithm for the segmentation of large lesions in T/sub 1/-weighted magnetic resonance (MR) images of the head. The algorithm involves a gray level similarity criterion to expand the region and a size criterion to prevent from over-growing outside the lesion. The performance of tile algorithm is evaluated and validated on a series of pathologic three-dimensional MR images of the head.,"Lesions,
Image segmentation,
Magnetic heads,
Multiple sclerosis,
Magnetic resonance,
Neuroscience,
Magnetic resonance imaging,
Hemorrhaging,
Inspection,
Protocols"
Persistent objects in the Fleet system,"Fleet is a middleware system implementing a distributed repository for persistent Java objects. Fleet is primarily targeted for supporting highly critical applications: in particular, the objects it stores maintain correct semantics despite the arbitrary failure (including hostile corruption) of a limited number of Fleet servers and, for some object types, of clients allowed to invoke methods on those objects. Fleet is designed to be highly available, dynamically extensible with new object types, and scalable to large numbers of servers and clients. In this paper, we describe the design of Fleet objects, including how new objects are introduced into the system, how they are named, and their default semantics.","Java,
Protocols,
Middleware,
Fault tolerance,
Scalability,
Buildings,
Computer science,
Contracts,
Software systems,
Computer architecture"
Radio wave propagation over a nonconstant immittance plane,"The problem of electromagnetic wave propagation over a horizontal, nonconstant immittance plane, whose immittance value is a function of incident grazing angle, is considered. An equivalent specification of the immittance surface is in terms of its angle-dependent reflection coefficient. Expressions are provided for the field on a vertical line given the field on a previous vertical line. The vertical line field is initialized at the plane containing the source where its aperture current distribution is specified. Both two-dimensional and three-dimensional fields are considered, and the expressions are valid for either polarization. The form of expressions is particularly suited for implementing with the Fourier split-step algorithm of the parabolic wave equation. Extension to inhomogeneous atmosphere to account for mild atmospheric inhomogeneities is presented. Several examples are considered where the immittance arises from small-scale and large-scale surface roughnesses. A numerical procedure is described wherein incomplete or approximate reflection coefficient data are made to conform to the assumptions made in the development of the expressions. This is demonstrated for a surface reflection coefficient which is governed by the Miller-Brown-Vegh roughness reduction factor. Numerical results are presented for propagation under ducting conditions over a rough surface for frequencies from HF through microwave.","Surface impedance,
Surface waves,
Rough surfaces,
Surface roughness,
Sea surface,
Admittance,
Surface treatment"
What value covariance information in estimating vision parameters?,"Many parameter estimation methods used in computer vision are able to utilise covariance information describing the uncertainty of data measurements. This paper considers the value of this information to the estimation process when applied to measured image point locations. Covariance matrices are first described and a procedure is then outlined whereby covariances may be associated with image features located via a measurement process. An empirical study is made of the conditions under which covariance information enables generation of improved parameter estimates. Also explored is the extent to which the noise should be anisotropic and inhomogeneous if improvements are to be obtained over covariance-free methods. Critical in this is the devising of synthetic experiments under which noise conditions can be precisely controlled. Given that covariance information is, in itself, subject to estimation error tests are also undertaken to determine the impact of imprecise covariance information upon the quality of parameter estimates. Finally, an experiment is carried out to assess the value of covariances in estimating the fundamental matrix from real images.",
A unicast-based approach for streaming multicast,"Network layer multicast is know as the most efficient way to support multicast sessions. However, for security, QoS and other considerations, most of the real-time application protocols can be better served by upper layer (transport or application) multicast. We propose a scheme called M-RTP for multicast RTP sessions. The idea behind this scheme is to set up the multicast RTP session over a set of unicast RTP sessions, established between the various participants (source and destinations) of the multicast session. We then address the issue of finding a set of paths with maximum bottleneck for an M-RTP session. We show that this problem is NP-complete, and propose several heuristics to solve it.","Unicast,
Multicast protocols,
Routing,
Telecommunication traffic,
Computer science,
Operating systems,
Application software,
Transport protocols,
Internet,
Multicast algorithms"
An evolutionary approach for the tuning of a chess evaluation function using population dynamics,"Using the game of chess, we propose an approach for the tuning of evaluation function parameters based on evolutionary algorithms. We introduce an iterative method for population member selection and show how the resulting win, loss, or draw information from competition can be used in conjunction with the statistical analysis of the population to develop evaluation function parameter values. A population of evaluation function candidates are randomly generated and exposed to the proposed learning techniques. An analysis to the success of learning is given and the undeveloped and developed players are examined through competition against a commercial chess program.",
Visualizing object-oriented software in virtual reality,"The paper describes a system, Imsovision, for visualizing object-oriented software in a virtual reality Environment. A visualization language (COOL) is defined that maps C++ source code to a visual representation. Our aim is to develop a language with few metaphors and constructs, but with the ability to represent a variety of elements with no ambiguity or loss of meaning. In addition, the visualization has to maximally use the potential of the used media. The design of the OO software system and its attributes are represented in the visualization. Class information, relationships between classes, and metric information is displaced. VRML is used for the visualization and it is rendered in the CAVE environment.",
Numerical aspects of spatio-temporal current density reconstruction from EEG-/MEG-data,"The determination of the sources of electric activity inside the brain from electric and magnetic measurements on the surface of the head is known to be an ill-posed problem. In this paper, a new algorithm which takes temporal a priori information modeled by the smooth activation model into account is described and compared with existing algorithms such as Tikhonov-Phillips.",
Maintaining mutual consistency for cached Web objects,"Existing Web proxy caches employ cache consistency mechanisms to ensure that locally cached data is consistent with that at the server. We argue that techniques for maintaining consistency of individual objects are not sufficient; a proxy should employ additional mechanisms to ensure that related Web objects are mutually consistent with one another. We formally define the notion of mutual consistency and the semantics provided by a mutual consistency mechanism to end users. We then present techniques for maintaining mutual consistency in the temporal and value domains. A novel aspect of our techniques is that they can adapt to the variations in the rate of change of the source data, resulting in judicious use of proxy and network resources. We evaluate our approaches using real-world Web traces and show that: (i) careful tuning can result in substantial savings in the network overhead incurred without any substantial loss in fidelity, of the consistency guarantees, and (ii) the incremental cost of providing mutual consistency guarantees over mechanisms to provide individual consistency guarantees is small.","Computer science,
Delay,
Web server,
Costs,
Engineering profession,
Electromagnetic compatibility,
HTML,
Stock markets,
Frequency"
"OO design patterns, design structure, and program changes: an industrial case study","A primary expected benefit of object-oriented (OO) methods is the creation of software systems that are easier to adapt and maintain. OO design patterns are especially geared to improve adaptability, since patterns generally increase the complexity of an initial design in order to ease future enhancements. For design patterns to really provide benefit, they must reduce the cost of future adaptation. The evidence of improvements in adaptability through the use of design patterns and other design structures consists primarily of intuitive arguments and examples. There is little empirical evidence to support claims of improved flexibility of these preferred structures. In this case study, we analyze 39 versions of an evolving industrial OO software system to see if there is a relationship between patterns, other design attributes, and the number of changes. We found a strong relationship between class size and the number of changes-larger classes were changed more frequently. We also found two relationships that we did not expect: (1) classes that participate in design patterns are not less change prone-these pattern classes are among the most change prone in the system, and (2) classes that are reused the most through inheritance tend to be more change prone. These unexpected results hold up after accounting for class size, which had the strongest relationship with changes.","Computer aided software engineering,
Programming profession,
Costs,
Software design,
Books,
Computer industry,
Computer science,
Pattern analysis,
Software maintenance,
Documentation"
Enterprise application integration-the case of the Robert Bosch Group,"Today, most organizations are using packaged software for their key business processes. Enterprise resource planning (ERP), supply chain management (SCM), customer relationship management (CRM) and electronic commerce (EC) systems enable organizations to improve their focus of using information systems (IS) to support their operational and financial goals. This article argues that the need to integrate these packaged software applications with each other as well as with existing or legacy business applications drives the need for a standardized integration architecture to more flexibly implement new business processes across different organizations and applications. To illustrate the components of such an architecture, a case study undertaken at the Robert Bosch Group provided necessary empirical evidence. The Robert Bosch Group has evaluated different enterprise application integration (EAI) systems to achieve a standardized integration architecture. The article describes a reference architecture and criteria for the classification of EAI systems which are derived from different integration approaches.","Computer aided software engineering,
Application software,
Computer architecture,
Electronics packaging,
Software packages,
Enterprise resource planning,
Management information systems,
Companies,
Costs,
Business communication"
A time-dependent charge-collection efficiency for diffusion,"The diffusion equation has some applications relevant to charge collection from ion tracks in silicon devices. Textbook solutions for the diffusion equation are available only for a few simple boundary geometries and special types of boundary conditions. A broader class of geometries was previously treated via a charge-collection efficiency function, but this applies only to total (integrated in time from zero to infinity) collected charge. The earlier work took advantage of the fact that Laplace's equation can be solved for a broad class of geometries. This paper extends the earlier work so that it applies to charge collected up to an arbitrary time. A time-dependent charge-collection efficiency function can be estimated for any geometry such that Laplace's equation has been solved. In particular, the analysis permits a comparison between diffusion calculations and a computer simulation of charge collection from an ion track. This comparison supports an earlier model in which charge collection, including a so-called ""prompt"" component, is driven by diffusion. The analysis applies to arbitrary track locations and directions. It also provides the option of treating a device geometry as two-dimensional in rectangular coordinates (if desired) while simultaneously treating the track as a line instead of a plane. Simulation codes having such flexibility regarding geometry are difficult to use, so the analysis makes the study of geometry effects accessible to a larger number of investigators.",
Spectral graphs for quasi-cyclic LDPC codes,"Quasi-cyclic codes are described in terms of code equations on the spectral components of constituent cyclic codes. These define a constraint graph in the spectral domain. Here spectral graphs define low density parity check codes (LDPC) codes for which minimum distance can be bounded with algebraic and graph-based arguments. Examples include [42,22,8] and [155,64,20] regular LPDC codes.","Parity check codes,
Equations,
Sparse matrices,
Matrix decomposition,
Computer science,
Constraint theory,
Testing,
Iterative decoding,
Iterative algorithms,
Frequency"
Resource reservation with pointer forwarding schemes for the mobile RSVP,We propose a pointer forwarding scheme for the mobile resource reservation protocol (MRSVP) to reduce the resource reservation cost on the wireless Internet. We show that the pointer forwarding scheme could significantly degrade the reservation cost when a mobile host performs locality movement.,
New self-dual codes over GF(4) with the highest known minimum weights,"The purpose of this correspondence is to construct new Hermitian self-dual codes over GF(4) of lengths 22, 24, 26, 32, and 34 which have the highest known minimum weights. In particular, for length 22, we construct eight new extremal self-dual [22,11,8] codes over GF(4) which do not have a nontrivial automorphism of odd order. The existence of such codes has been left open since 1991 by Huffman.",Dual codes
Capturing outline of fonts using genetic algorithm and splines,"In order to obtain a good spline model from large measurement data, we frequently have to deal with knots as variables, which becomes a continuous, non-linear and multivariate optimization problem with many local optima. Hence, it is very difficult to obtain a global optima. We present a method to convert the original problem into a discrete combinatorial optimization problem and solve it by a genetic algorithm. We also incorporate a corner detection algorithm to detect significant points which are necessary to capture a pleasant looking spline fitting for shapes such as fonts. A parametric B-Spline has been approximated to various characters and symbols. The chromosomes have been constructed by considering the candidates of the locations of knots as genes. The best model among the candidates is searched by using the Akaike Information Criterion (AIC). The method determines the appropriate number and location of knots automatically and simultaneously. Some examples are given to show the results obtained from the algorithm.","Genetic algorithms,
Spline,
Shape,
Polynomials,
Detection algorithms,
Computer science,
Petroleum,
Minerals,
Electronic mail,
Optimization methods"
Multiple-input translinear element networks,"We describe a new class of translinear circuits that accurately embody product-of-power-law relationships in the current signal domain. We call such circuits multiple-input translinear element (MITE) networks. A MITE is a circuit element, which we defined recently that produces an output current that is exponential in a weighted sum of its input voltages. We describe intuitively the basic operation of MITE networks and provide a systematic matrix technique for analyzing the nonlinear relationships implemented by any given circuit. We also show experimental data from three MITE networks that were fabricated in a 1.2-/spl mu/m double-poly CMOS process.",
Fast packet classification for two-dimensional conflict-free filters,"Routers can use packet classification to support advanced functions. Routers with packet classification capability can forward packets based on multiple header fields, such as source address, protocol type, or application port numbers. The destination-based forwarding can be thought of as one-dimensional packet classification. While several efficient solutions are known for the one-dimensional IP lookup problem, the multi-dimensional packet classification has proved to be far more difficult. While an O(log w) time scheme is known for the IP lookup, Srinivisan et al. (1999) show a lower bound of /spl Omega/(/spl omega//sup k-1/) for k-dimensional filter lookup, where /spl omega/ is the number of bits in a header field. In particular, this lower bound precludes the possibility of a binary search like scheme even for 2-dimensional filters. In this paper, we examine this lower bound more closely, and discover that the lower bound depends crucially on conflicts in the filter database. We then show that for two-dimensional conflict-free filters, a binary search scheme does work! Our lookup scheme requires O(log/sup 2/ /spl omega/) hashes in the worst-case, and uses O(n log/sup 2/ /spl omega/) memory. Alternatively, our algorithm can be viewed as making O (log /spl omega/) calls to a prefix lookup scheme. It has been observed in practice that filter databases have very few conflicts, and these conflicts can be removed by adding additional filters (one per conflict). Thus, our scheme may also be quite practical. Our simulation and experimental results show that the proposed scheme also performs as good as or better than existing schemes.","Filters,
Virtual private networks,
Filtering,
Routing,
Bandwidth,
Databases,
Packet switching,
Computer science,
Access control,
Access protocols"
Channelization problem in large scale data dissemination,"In many large scale data dissemination systems, a large number of information flows must be delivered to a large number of information receivers. However, because of differences in interests among receivers, not all receivers are interested in all of the information flows. Multicasting provides the opportunity to deliver a subset of the information flows to a subset of the receivers. With a limited number of multicast groups available, the channelization problem is to find an optimal mapping of information flows to a fixed number of multicast groups, and a subscription mapping of receivers to multicast groups so as to minimize a function of the total bandwidth consumed and the amount of unwanted information received by receivers. We formally define two versions of the channelization problem and subscription problem (a subcomponent of the channelization problem). We analyze the complexity of each version of the channelization problem and show that they are both NP-complete. We also find that the subscription problem is NP-complete when one flow can be assigned to multiple multicast groups. We also study and compare different approximation algorithms to solve the channelization problem, finding that one particular heuristic, flow-based-merge, finds good solutions over a range of problem configurations.",
Component metrics to measure component quality,"Recently, component-based software development is getting accepted in industry as a new effective software development paradigm. Since the introduction of component-based software engineering (CBSE) in later 90's, the CBSD research has focused largely on component modeling, methodology, architecture and component platform. However, as the number of components available on the market increases, it becomes more important to devise metrics to quantify the various characteristics of components. In this paper, we propose metrics for measuring the complexity, customizability, and reusability of software components. Complexity metric can be used to evaluate the complexity of components. Customizability is used to measure how efficiently and widely the components can be customized for organization specific requirement. Reusability can be used to measure the degree of features that are reused in building applications. We expect that these metrics can be effectively used to quantify the characteristics of components.",
Checking equivalence for partial implementations,"We consider the problem of checking whether a partial implementation can (still) be extended to a complete design which is equivalent to a given full specification. Several algorithms trading off accuracy and computational resources are presented: starting with a simple 0,1,X-based simulation, which allows approximate solutions, but is not able to find all errors in the partial implementation, we consider more and more exact methods finally covering all errors detectable in the partial implementation. The exact algorithm reports no error if and only if the current partial implementation conforms to the specification, i.e. it can be extended to a full implementation which is equivalent to the specification. We give a series of experimental results demonstrating the effectiveness and feasibility of the methods presented.",
Minimum and maximum utilization bounds for multiprocessor RM scheduling,"This paper deals with the problem of finding utilization bounds for multiprocessor rate monotonic scheduling with partitioning. The minimum and maximum utilization bounds among all the reasonable allocation algorithms are calculated. We prove that the utilization bound associated with the reasonable allocation heuristic Worst Fit (WF) is equal to that minimum. In addition, we prove that the utilization bound associated with the heuristics First Fit Decreasing (FFD) and Best Fit Decreasing (BFD) is equal to the maximum, of value (n+1)(2/sup 1/2/-1), where n is the number of processors.","Processor scheduling,
Scheduling algorithm,
Testing,
Partitioning algorithms,
Computer science,
Real time systems,
Simulated annealing,
Performance evaluation,
Polynomials"
A novel low power CAM design,The Content Addressable Memory (CAM) is a class of memory that allows access by data instead of physical address. In this paper a novel low power CAM cell called the toggling match line CAM is introduced. It is shown to provide over 40% reduction in power consumed on read misses as compared to the basic CAM design.,"Computer aided manufacturing,
CADCAM,
Energy consumption,
Writing,
Associative memory,
Read-write memory,
Computer science,
Data engineering,
Power engineering and energy,
Modems"
The performance measurement of cryptographic primitives on palm devices,"We developed and evaluated several cryptographic system libraries for Palm OS(R) which include stream and block ciphers, hash functions and multiple-precision integer arithmetic operations. We noted that the encryption speed of SSC2 outperforms both ARC4 (Alleged RC4) and SEAL 3.0 if the plaintext is small. On the other hand, SEAL 3.0 almost doubles the speed of SSC2 when the plaintext is very large. We also observed that the optimized Rijndael with 8KB of lookup tables is 4 times faster than DES. In addition, our results show that implementing the cryptographic algorithms as system libraries does not degrade their performance significantly. Instead, they provide great flexibility and code management to the algorithms. Furthermore, the test results presented provide a basis for performance estimation of cryptosystems implemented on the PalmPilot/sup TM/.",
Measuring HMM similarity with the Bayes probability of error and its application to online handwriting recognition,"We propose a novel similarity measure for hidden Markov models (HMMs). This measure calculates the Bayes probability of error for HMM state correspondences and propagates it along the Viterbi path in a similar way to the HMM Viterbi scoring. It can be applied as a tool to interpret misclassifications, as a stop criterion in iterative HMM training or as a distance measure for HMM clustering. The similarity measure is evaluated in the context of online handwriting recognition on lower case character models which have been trained from the UNIPEN database. We compare the similarities with experimental classifications. The results show that similar and misclassified class pairs are highly correlated. The measure is not limited to handwriting recognition, but can be used in other applications that use HMM based methods.","Hidden Markov models,
Handwriting recognition,
Computer errors,
Application software,
Probability,
Viterbi algorithm,
Writing,
Integrated circuit modeling,
Computer science,
Databases"
Disassembly sequencing using a motion planning approach,"Our motion planning based approach treats the parts in the assembly as robots and operates in the composite configuration space of the parts' individual configuration spaces. Randomized techniques inspired by recent motion planning methods are used to sample configurations in this space. Since typical assemblies consist of many parts, the corresponding composite C-spaces have high dimensionality. Also, since many important configurations for the disassembly sequence will involve closely packed parts, the disassembly problem suffers from the so-called narrow passage problem. We bias the sampling by computing potential movement directions based on the geometric characteristics of configurations known to be reachable from the assembled configuration. We construct a disassembly tree which is rooted at the starting assembled configuration. Our experimental results with several non-trivial puzzle-like assemblies show the potential of this approach.","Robotic assembly,
Motion planning,
Sampling methods,
Mechanical engineering,
Orbital robotics,
Recycling,
Engineering profession,
Computer science,
Artificial intelligence,
Robot kinematics"
TCP Westwood: analytic model and performance evaluation,"We present a performance model of TCP Westwood (TCPW), a new TCP protocol with a sender-side modification of the window congestion control scheme. TCP Westwood controls the window using end-to-end connection bandwidth share estimation, obtained by monitoring the ACK reception rate. An analytic model using Markov Chain techniques is developed in this paper, and then used to assess the performance improvements obtained using TCPW. The model takes into account the estimation and filtering method used in TCPW, as well as the following system parameters, bottleneck link bandwidth, buffer space at the bottleneck router, end-to-end propagation time, and error rate. The model reveals substantial TCPW gains over Reno whenever losses due to link or other errors are taken into consideration. The analytic model accuracy is confirmed by comparing to simulation results.",
Improving trigram language modeling with the World Wide Web,"We propose a method for using the World Wide Web to acquire trigram estimates for statistical language modeling. We submit an N-gram as a phrase query to Web search engines. The search engines return the number of Web pages containing the phrase, from which the N-gram count is estimated. The N-gram counts are then used to form Web-based trigram probability estimates. We discuss the properties of such estimates, and methods to interpolate them with traditional corpus based trigram estimates. We show that the interpolated models improve speech recognition word error rate significantly over a small test set.",
A new algorithm based on saturation and desaturation in the xy chromaticity diagram for enhancement and re-rendition of color images,"This paper presents a new algorithm for color contrast enhancement which adopts the xy chromaticity diagram and consists of two steps. All the ""chromatic"" colors are first maximally saturated within a certain gamut; the opportune desaturation operation which follows is based on the center of gravity law for color mixture. This prevents the introduction of unnatural colors and, in combination with a suitable manipulation of the brightness value, turns out to increase the image sharpness and to provide more appealing results Our technique can also be applied to re-rendition a color image since it can easily simulate the change of the illuminant(s) of the scene portrayed in the image, this could find useful applications in image and video editing. Some examples of the performance of the algorithm are reported and discussed.","Color,
Gravity,
Brightness,
Layout,
Space technology,
Image converters,
Computational modeling,
Application software,
Computer science,
Image enhancement"
Organ motion detection in CT images using opposite rays in fan-beam projection systems,"Motion artifacts have been identified as a problem in medical tomography systems. While computed tomography (CT) imaging has been getting faster, there remains a need to detect and compensate for motions in clinical follow-up of neurological patients (multiple sclerosis, tumors, stroke, etc.), in cardiac imaging, and in any area in which failing to detect a motion artifact may lead to misdiagnosis. The authors have developed a novel algorithm to detect motion in brain images. The algorithm deals with detecting and isolating motion in the object domain using only the information available in the sinogram domain. The new ""opposite ray algorithm"" (ORA) addresses the issue of motion in the interior elements of the object. The ORA combines information from projections that are opposite in space and separated in time to isolate and identify the motion. A sinogram of motion is created, integrated and reconstructed to isolate the moving component. The algorithm can be used with conventional clinical scanners employing quarter-detector offset. The significant effect of quarter-detector offset on the ORA is investigated. The effects that a finite beamwidth and noise have on the ORA are also investigated. Both the similarity index and a correlation coefficient are used to evaluate the algorithm. The algorithm is successful when applied to cases exhibiting translational and translational-rotational motion. A similarity index of 0.88 is obtained in a typical case with both translational and rotational motion. Further development is recommended in the deformation case.",
Client-transparent fault-tolerant Web service,"Most of the existing fault tolerance schemes for Web servers detect server failure and route future client requests to backup servers. These techniques typically do not provide transparent handling of requests whose processing was in progress when the failure occurred. Thus, the system may fail to provide the user with confirmation for a requested transaction or clear indication that the transaction was not performed. We describe a client-transparent fault tolerance scheme for Web servers that ensures correct handling of requests in progress at the time of server failure. The scheme is based on a standby backup server and simple proxies. The error handling mechanisms of TCP are used to multicast requests to the primary and backup as well as to reliably deliver replies from a server that may fail while sending the reply. Our scheme does not involve OS kernel changes or use of user-level TCP implementations and requires minimal changes to the Web server software.","Fault tolerance,
Web services,
Web server,
Protocols,
Web and internet services,
Computer science,
Fault detection,
Kernel,
Banking,
Availability"
A class of loop self-scheduling for heterogeneous clusters,,
Electrical behavior of a simple helical flux compression generator for code benchmarking,"A variety of basic magnetic flux compression (MFC) generator geometries have been tested during the last three decades. Though size and operating regimes differ widely, it is apparent that the helical flux compression generator is the most promising concept with respect to current amplification and compactness. Though the geometry of the helical generator (dynamically expanding armature in the center of a current carrying helix) seems to be basic, it turns out that the understanding of all involved processes is rather difficult. This fact is apparent from the present lack of a computer model that is solely based on physical principles and manages without heuristic factors. A simple generator was designed to address flux and current losses of the helical generator. The generator's maximum current amplitude is given as a function of the seed current and the resulting ""seed-current"" spread is compared to the output of state-of-the-art computer models. Temporally resolved current and current time derivative signals are compared as well. The detailed generator geometry is introduced in order to facilitate future computer code bench marking or development. The impact of this research on the present understanding of magnetic flux losses in helical MFC generators is briefly discussed.","Laboratories,
Magnetic flux,
Computational geometry,
Life testing,
Physics computing,
Signal resolution,
Magnetic losses,
Weapons,
Power generation,
Power engineering and energy"
A fixed-delay broadcasting protocol for video-on-demand,"Broadcasting protocols reduce the cost of video-on-demand services by distributing more efficiently videos that are likely to be simultaneously watched by many viewers. Rather than answering individual customer requests, they broadcast the contents of each video according to a fixed schedule. We present a fixed-delay pagoda broadcasting protocol that requires all users to wait for a small fixed delay before watching the video they have selected. The protocol uses this delay to reduce the bandwidth required to transmit the first minutes of each video. As a result, our protocol provides the lowest waiting times of all protocols using segments of equal duration and channels of equal bandwidth. In addition, its performance is not very far from the theoretical minimum. We also show how to modify our protocol to restrict the set-top, box receiving bandwidth to two times the video consumption rate.",
A Bezier curve-based approach to shape description for Chinese calligraphy characters,"In this paper, we propose a method to vectorize Chinese characters in calligraphy documents. Our system can prevent the zigzag phenomena when the characters are enlarged. The system contains two modules: contour segment extraction and description. In the former, high curvature points on contours are detected as corner points, which divide the contour into several segments. In the latter, a contour segment can be described either by a straight line or a cubic Bezier curve. According to relations between the contour segment and the Bezier curve, control points are adjusted to fit the contour segment better. When the curve fitness cost is small enough, the shape is described well. The processing time of our curve fitting is about five seconds per A4 image, which has 4488 contour segments. Experimental results demonstrate that our system is efficient and promising.",
Exploring robustness in group key agreement,"Secure group communication is crucial for building distributed applications that work in dynamic environments and communicate over unsecured networks (e.g. the Internet). Key agreement is a critical part of providing security services for group communication systems. Most of the current contributory key agreement protocols are not designed to tolerate failures and membership changes during execution. In particular, nested or cascaded group membership events (such as partitions) are not accommodated. We present the first robust contributory key agreement protocols, resilient to any sequence of events while preserving the group communication membership and ordering guarantees.",
Simulation and forecasting complex financial time series using neural networks and fuzzy logic,"We describe the application of several neural network architectures to the problem of simulating and predicting the dynamic behavior of complex economic time series. We use several neural network models and training algorithms to compare the results and decide which one is best for this application. We also compare the simulation results with fuzzy logic models and the traditional approach of using a statistical model. In this case, we use real time series of prices of consumer goods to test our models. Real prices of tomato and green onion in the US show complex fluctuations in time and are very complicated to predict with traditional statistical approaches. For this reason, we have chosen neural networks and fuzzy logic to simulate and predict the evolution of these prices in the US market.","Predictive models,
Neural networks,
Fuzzy logic,
Economic forecasting,
Biological neural networks,
Computational modeling,
Fluctuations,
Chaos,
Neurons,
Computer science"
A new system for online quantitative evaluation of optical see-through augmentation,"A crucial aspect in the implementation of an augmented reality (AR) system is determining its accuracy. The accuracy of a system determines the applications it can be used for. The aim of our research is measuring the overall accuracy of an arbitrary AR system. Once measurements of a system are made, they can be analyzed for determining the structure and sources of errors. From the analysis it may also be possible to improve the methods used to calibrate and register the virtual to the real. This paper describes an online system for measuring the registration accuracy of optical see-through augmentation. By online, we mean that the user can measure the registration error they are experiencing while they are using the system. We overcome the difficulty of not having retinal access by having the user indicate the projection of a perceived object on a planar measurement device. Our method provides information which can be used to analyze the structure of the system error in two or three dimensions. The results of the application of our method to two monocular optical see-through AR systems are shown.","Cameras,
Calibration,
Optical imaging,
Visualization,
Computer science,
Application software,
Information analysis,
Retina,
Computer displays,
Humans"
Sharing partitionable workloads in heterogeneous NOWs: greedier is not better,,
The impact of software evolution on code coverage information,"Many tools and techniques for addressing software maintenance problems rely on code coverage information. Often, this coverage information is gathered for a specific version of a software system, and then used to perform analyses on subsequent versions of that system without being recalculated. As a software system evolves, however, modifications to the software alter the software's behavior on particular inputs, and code coverage information gathered on earlier versions of a program may not accurately reflect the coverage that would be obtained on later versions. This discrepancy may affect the success of analyses dependent on code coverage information. Despite the importance of coverage information in various analyses, in our search of the literature we find no studies specifically examining the impact of software evolution on code coverage information. Therefore, we conducted empirical studies to examine this impact. The results of our studies suggest that even relatively small modifications can greatly affect code coverage information, and that the degree of impact of change on coverage may be difficult to predict.",
Optimal allocation of packet-level and byte-level FEC in video multicasting over wired and wireless networks,"Multicast is an efficient technique to deliver video content over a network. We consider such a multicast system to serve both wireless and wireline users when there are errors over the wired network and the wireless hop. Since packets are likely to be dropped in the wired networks while bit errors are more likely over the wireless hop, a combination of both packet-level and byte-level FEC is required to recover these errors. Given the estimated error and bandwidth characteristics reported by end users, the server needs to allocate optimally the packet-level and byte-level FEC to achieve maximum video quality. We study two schemes pertaining to whether or not the wireless gateway is able to transcode the video packets from the wired network before forwarding them to the wireless users. We first develop a model to analyze the system; and then propose an efficient algorithm for the FEC computation. We finally compare the schemes in terms of the optimal parameters used in the FEC, and the video quality achieved.",
Discrete cosine transforms on quantum computers,"A classical computer does not allow the calculation of a discrete cosine transform on N points in less than linear time. This trivial lower bound is no longer valid for a computer that takes advantage of quantum mechanical superposition, entanglement, and interference principles. In fact, we show that it is possible to realize the discrete cosine transforms and the discrete sine transforms of size N/spl times/N and types I, II, III and IV with as little as O(log/sup 2/N) operations on a quantum computer; whereas the known fast algorithms on a classical computer need O(N logN) operations.",
An Architectural model for service-based software with ultra rapid evolution,"There is an urgent industrial need for new approaches to software evolution that will lead to far faster implementation of software changes. For the past 40 years, the techniques, processes and methods of software development have been dominated by supply side issues, and as a result the software industry is oriented towards developers rather than users. Existing software maintenance processes are simply too slow to meet the needs of many businesses. To achieve the levels of functionality, flexibility and time to market of changes and updates required by users, a radical shift is required in the development of software, with a more demand-centric view leading to software which will be delivered as a service, within the framework of an open marketplace. Although there are some signs that this approach is being adopted by industry, it is in a very limited and restricted form. We summarise research that has resulted in a long term strategic view of software engineering innovation. Based on this foundation, we describe more recent work that has resulted in an innovative demand-led model for the future of software. We describe a service architecture in which components may be bound instantly, just at the time they are needed and then the binding may be disengaged. Such ultra late binding requires that many non-functional attributes of the software are capable of automatic negotiation and resolution. Some of these attributes have been demonstrated and amplified through a prototype implementation based on existing and available technology.","Software maintenance,
Computer industry,
Computer science,
Computer architecture,
Gold,
Programming,
Time to market,
Software engineering,
Technological innovation,
Software prototyping"
End-to-end integration testing design,"Integration testing has always been a challenge especially if the system under test is large with many subsystems and interfaces. This paper proposes an approach to design End-to-End (E2E) integration testing, including test scenario specification, test case generation and tool support. Test scenarios are specified as thin threads, each of which represents a single function from an end user's point of view. Thin threads can be organized hierarchically into a tree with each branch consisting of a set of related thin threads representing a set of related functionality. A test engineer can use thin-thread trees to generate test cases systematically, as well as carry out other related tasks such as risk analysis and assignment, regression testing, ripple effect analysis. A prototype tool has been developed to support E2E testing in a distributed environment on the J2EE platform.",
Class-based cache management for dynamic Web content,"Caching dynamic pages at a server site is beneficial in reducing server resource demands and it also helps dynamic page caching at proxy sites. Previous work has used fine-grain dependence graphs among individual dynamic pages and underlying data sets to enforce result consistency. This paper proposes a complementary solution for applications that require coarse-grain cache management. The key idea is to partition dynamic pages into classes based on URL patterns so that an application can specify page identification and data dependence, and invoke invalidation for a class of dynamic pages. To make this scheme time-efficient with small space requirement, lazy invalidation is used to minimize slow disk accesses when IDs of dynamic pages are stored in memory with a digest format. Selective precomputing is further proposed to refresh stale pages and smoothen load peaks. A data structure is developed for efficient URL class searching during lazy or eager invalidation. This paper also presents design and implementation of a caching system called Cachuma which integrates the above techniques, runs in tandem with standard Web servers, and allows Web sites to add dynamic page caching capability with minimal changes. The experimental results show that the proposed techniques are effective in supporting coarse-grain cache management and reducing server response times for tested applications.",
Specified-resolution wavelet analysis of activation patterns from BOLD contrast fMRI,"Functional magnetic resonance (MR) MR imaging (fMRI) with blood-oxygenation-level-dependent (BOLD) contrast localizes neuronal processing of cognitive paradigms. As magnetic resonance signal responses are small, functional mapping requires statistical analysis of temporally averaged image data. Although voxels activating at the paradigm frequency can be identified from the Fourier power spectrum, such analyses collapse the temporal information that is useful to establish consistency of responses during the paradigm. The design of a set of nonorthogonal wavelets of specified frequency resolution within the power spectrum was investigated for extracting desired frequency responses from the noisy signal intensity of individual voxels. These wavelets separate the low-frequency cognitive response to the paradigm from the respiratory and cardiac responses at higher frequencies. The retention of the temporal information, possible by wavelet analysis, allows the MR signal changes to be compared to changes in behavioral responses over the duration of an entire paradigm. The amplitude and time delay of the wavelet specified by the paradigm identify quantitatively the size of the MR signal change and the temporal delay of the hemodynamic BOLD response, respectively. This specified-resolution wavelet analysis was demonstrated for individual voxels and maps through the frontal eye fields using a visually guided saccade paradigm.",
An agent-based approach for supporting cross-enterprise workflows,"In order to support global competitiveness and rapid market responsiveness, virtual enterprises need to efficiently integrate different organization's workflows to provide customized services. Currently, most of the integrations are case based which have high setup cost and involve time consuming low level programming. Cross-enterprise workflow that is able to streamline and coordinate business processes across organizations in dynamic Web environment provides a low cost and flexible solution. We develop an agent based cross-enterprise workflow management system (WFMS) architecture which can dynamically integrate the workflows and compose a workflow execution community customized to different workflow specifications.","Aerodynamics,
Costs,
Manufacturing processes,
Business process re-engineering,
Australia,
Virtual enterprises,
Finance,
Supply chains,
Computer science,
Delay"
VizCraft: a problem-solving environment for aircraft configuration design,"The VizCraft problem-solving environment aids aircraft designers during conceptual design of a high-speed civil transport (HSCT). It integrates simulation codes that evaluate a design with visualizations for analyzing a design individually or in contrast to other designs. VizCraft provides a graphical user interface to a widely used suite of simulation and analysis codes for HSCT design, and it provides tools for visualizing the outputs of these codes. So, VizCraft provides an environment that combines visualization and computation, encouraging the designer to think in terms of the overall problem-solving task, not simply using the visualization to view the computation's results.","Problem-solving,
Aerodynamics,
Data visualization,
Computational modeling,
Analytical models,
Geometry,
Aircraft propulsion,
Graphical user interfaces,
Solid modeling,
Multidimensional systems"
Testbed Evaluation of Virtual Environment Interaction Techniques,"As immersive virtual environment (VE) applications become more complex, it is clear that we need a firm understanding of the principles of VE interaction. In particular, designers need guidance in choosing three-dimensional interaction techniques. In this paper, we present a systematic approach, testbed evaluation, for the assessment of interaction techniques for VEs. Testbed evaluation uses formal frameworks and formal experiments with multiple independent and dependent variables to obtain a wide range of performance data for VE interaction techniques. We present two testbed experiments, covering techniques for the common VE tasks of travel and object selection/manipulation. The results of these experiments allow us to form general guidelines for VE interaction and to provide an empirical basis for choosing interaction techniques in VE applications. Evaluation of a real-world VE system based on the testbed results indicates that this approach can produce substantial improvements in usability.",
A method for verifying real-time properties of Ada programs,"This paper describes a method for transforming concurrent Ada programs by way of abstractions into input for the UPPAAL model checker for the purpose of analyzing the real-time properties of programs. The method depends on being able to compute the best and worst case execution times of procedures called by the various tasks in a concurrent program. It employs abstractions of actions to simplify the control structure of a task, abstractions of complex data structures to more abstract variables and abstractions to simplify clocks. The method is illustrated on an Ada implementation of a kernel implementing ICPP scheduling. A TLA specification of a typical client user task is derived that can be interpreted as an UPPAAL timed automaton.","Automata,
Kernel,
Sparks,
Delay,
Clocks,
System recovery,
Computer science,
Data structures,
Computer languages,
Concurrent computing"
RIPPLES: tool for change in legacy software,"Key parts of software change are concept location and change propagation. We introduce a tool called RIPPLES that supports both. It uses the Abstract System Dependence Graph (ASDG) of the program, enriched by conceptual dependencies. A case study of NCSA Mosaic demonstrates the use of the tool. Precision and recall are used to evaluate the quality of support provided by RIPPLES.",
Evaluation of a novel two-step server selection metric,"Choosing the best-performing server for a particular client from a group of replicated proxies is a difficult task. We offer a novel, two-step technique for server selection that chooses a small subset of five servers, and isolates testing to that subset for ten days. We present an empirical evaluation of both our method and previously proposed metrics based on traces to 193 commercial proxies. We show that our technique performs better than any of the other metrics we studied -often one to two seconds better for a one-megabyte file-while requiring considerably less work over time. Metrics such as round-trip time and tests using small files usually select servers that are two to three times worse than the best server. Network-layer metrics such as minimizing router and autonomous system count poorly predict which server provides the best performance. These metrics often select servers with transfer times four to six times that of the best-performing server.","Web server,
Network servers,
File servers,
System testing,
Streaming media,
Web and internet services,
Mirrors,
Proposals,
Computer science,
Protocols"
Concise representation of frequent patterns based on disjunction-free generators,"Many data mining problems require the discovery of frequent patterns in order to be solved. Frequent itemsets are useful in the discovery of association rules, episode rules, sequential patterns and clusters. The number of frequent itemsets is usually huge. Therefore, it is important to work out concise representations of frequent itemsets. We describe three basic lossless representations of frequent patterns in a uniform way and offer a new lossless representation of frequent patterns based on disjunction-free generators. The new representation is more concise than two of the basic representations and more efficiently computable than the third representation. We propose an algorithm for determining the new representation.",
A new congestion-driven placement algorithm based on cell inflation,"We describe a new congestion-driven placement based on cell inflation. In our approach, we have used the method of probability-estimation to evaluate the routing of nets. We also take use of the strategy of cell inflation to eliminate the routing congestion. Further reduction in congestion is obtained by the scheme of cell moving. We have tested our algorithm on a set of sample circuits from American industry and the results obtained have shown great improvement of routability.","Routing,
Wire,
Computer science,
Circuit testing,
Integrated circuit layout,
Libraries,
Costs,
Wiring,
Minimization"
Appearance-based object recognition using multiple views,"Object recognition from a single view fails when the available features are not sufficient to determine the identity of a single object, either because of similarity with another object or because of feature corruption due to clutter and occlusion. Active object recognition systems have addressed this problem successfully, but they require complicated systems with adjustable viewpoints that are not always available. In this paper we investigate the performance gain available by combining the results of a single view object recognition system applied to imagery obtained from multiple fixed cameras. In particular, we address performance in cluttered scenes with varying degrees of information about relative camera pose. We argue that a property common to many computer vision recognition systems, which we term a weak target error, is responsible for two interesting limitations of multi-view performance enhancement: the lack of significant improvement in systems whose single-view performance is weak, and the plateauing of performance improvement as additional multi-view constraints are added.","Object recognition,
Cameras,
Feature extraction,
Image databases,
Spatial databases,
Voting,
Computer science,
Performance gain,
Layout,
Target recognition"
K2/Kleisli and GUS: Experiments in integrated access to genomic data sources,"The integrated access to heterogeneous data sources is a major challenge for the biomedical community. Several solution strategies have been explored: link-driven federation of databases, view integration, and warehousing. In this paper we report on our experiences with two systems that were developed at the University of Pennsylvania: K2, a view integration implementation, and GUS, a data warehouse. Although the view integration and the warehouse approaches each have advantages, there is no clear winner. Therefore, in selecting the best strategy for a particular application, users must consider the data characteristics, the performance guarantees required, and the programming resources available. Our experiences also point to some practical tips on how database updates should be published, and how XML can be used to facilitate the processing of updates in a warehousing environment.",
Low-complexity and low-memory entropy coder for image compression,"A low-complexity and low-memory entropy coder (LLEC) is proposed for image compression. The two key elements in the LLEC are zerotree coding and Golomb-Rice (1966, 1991) codes. Zerotree coding exploits the zerotree structure of transformed coefficients for higher compression efficiency. G-R codes are used to code the remaining coefficients in a variable-length codes/variable-length integer manner resulting in JPEG similar computational complexity. The proposed LLEC does not use any Huffman table, significant/insignificant list, or arithmetic coding, and therefore its memory requirement is minimized with respect to any known image entropy coder. In terms of compression efficiency, the experimental results show that discrete cosine transform (DCT)- and discrete wavelet transform (DWT)-based LLEC outperforms baseline JPEG and embedded zerotree wavelet coding (EZW) at the given bit rates, respectively. For example, LLEC outperforms baseline JPEG by an average of 2.2 dB on the Barbara image and is superior to EZW by an average of 0.2 dB on the Lena image. When compared with set partition in hierarchical trees, LLEC is inferior by 0.3 dB, on average, for both Lena and Barbara. In addition, LLEC has other desirable features, such as parallel processing support, region of interest coding, and as a universal entropy coder for DCT and DWT.","Image coding,
Discrete wavelet transforms,
Transform coding,
Discrete cosine transforms,
Computational complexity,
Entropy coding,
Arithmetic,
Computer science,
PSNR,
Hardware"
Towards a standard schema for C/C++,"Developing a standard schema at the abstract syntax tree (AST) level for C/C++ to be used by reverse engineering and reengineering tools is a complex and difficult problem. In this paper we present a catalogue of issues that need to be considered in order to design a solution. Three categories of issues are discussed. Lexical structure is the first category and pertains to characteristics of the source code, such as spaces and comments. The second category, syntax, includes both the mundane and hard problems in the C++ programming language. The final category is semantics and covers aspects such as naming and reference resolution. Example solutions to these challenges are provided from the Datrix schema from Bell Canada and the Columbus schema from University of Szeged. The paper concludes with a discussion of lessons learnt and plans for future work on a C/C++AST standard schema.",
A physically based relation between extracted threshold voltage and surface potential flat band voltage for MOSFET compact modeling,"Compact MOS models based on surface potential are now firmly established, but for practical applications there is no reliable link between measured values of threshold voltage and the flat-band voltage on which such models are based. This brief presents an analytical relationship which may be implemented in compact models to provide a reliable and accurate threshold parameter input. Results are compared with a conventional threshold voltage model for several SOI CMOS technologies. This technique has been developed for use with body-tied SOI transistors, and hence it can also be applied to bulk devices.",MOSFETs
Image reconstruction using the wavelet transform for positron emission tomography,"The authors conducted positron emission tomography (PET) image reconstruction experiments using the wavelet transform. The Wavelet-Vaguelette decomposition was used as a framework from which expressions for the necessary wavelet coefficients might be derived, and then the wavelet shrinkage was applied to the wavelet coefficients for the reconstruction (WVS). The performances of WVS were evaluated and compared with those of the filtered back-projection (FBP) using software phantoms, physical phantoms, and human PET studies. The results demonstrated that WVS gave stable reconstruction over the range of shrinkage parameters and provided better noise and spatial resolution characteristics than FBP.","Image reconstruction,
Wavelet transforms,
Positron emission tomography,
Imaging phantoms,
Noise reduction,
Performance evaluation,
Wavelet coefficients,
Software performance,
Humans,
Nuclear medicine"
Peer-to-peer networks,"Will peer-to-peer computing be the next killer Internet application? Like most over-hyped concepts, P2P is loosely defined and covers a set of rather disparate ideas. Perhaps the only common theme is a client-oriented view of the world; you might think of P2P as ""Power to the People"". The servers are subservient to the clients, which do most of the work. P2P has several important technology challenges and applications, varying from the sublime to the ridiculous. The article presents a quick overview and suggests some emerging research areas and opportunities.","Peer to peer computing,
Web server,
Collaborative work,
Relays,
Distributed computing,
Internet,
World Wide Web,
Garnets,
Network servers,
Collaboration"
Component clustering based on maximal association,"Presents a supervised clustering framework for recovering the architecture of a software system. The technique measures the association between the system components (such as files) in terms of data and control flow dependencies among the groups of highly related entities that are scattered throughout the components. The application of data mining techniques allows us to extract the maximum association among the groups of entities. This association is used as a measure of closeness among the system files in order to collect them into subsystems using an optimization clustering technique. A two-phase supervised clustering process is applied to incrementally generate the clusters and control the quality of the system decomposition. In order to address the complexity, issues, the whole clustering space is decomposed into subspaces based on the association property. At each iteration, the subspaces are analyzed to determine the most eligible subspace for the next cluster, which is then followed by an optimization search to generate a new cluster.","Software systems,
Data mining,
Clustering algorithms,
Computer architecture,
Computer science,
Fluid flow measurement,
Scattering,
Application software,
Control systems,
Software architecture"
A pursuit-evasion BUG algorithm,"We consider the problem of searching for an unpredictable moving target, using a robot that lacks a map of the environment, lacks the ability to construct a map, and has imperfect navigation ability. We present a complete algorithm, which yields a motion strategy for the robot that guarantees the elusive target will be detected, if such a strategy exists. It is assumed that the robot has an omnidirectional sensing device that is used to detect moving targets and also discontinuities in depth data in a 2D environment. We also show that the robot has the same problem solving power as a robot that has a complete map and perfect navigation abilities. The algorithm has been implemented in simulation, and some examples are shown.","Pursuit algorithms,
Robot sensing systems,
Navigation,
Military computing,
Computer science,
Motion detection,
Computational modeling,
Solid modeling,
Computational geometry,
Embedded computing"
Multiway range trees: scalable IP lookup with fast updates,"In this paper, we introduce a new IP lookup scheme with worst-case search and update time of O(log n), where n is the number of prefixes in the forwarding table. Our scheme is based on a new data structure, a multiway range tree. While existing lookup schemes are good for IPv4, they do not scale well in both lookup speed and update costs when addresses grow longer as in the IPv6 proposal. Thus our lookup scheme is the first lookup scheme to offer fast lookups and updates for IPv6 while remaining competitive for IPv4.","Internet,
Routing protocols,
Scalability,
Tree data structures,
Proposals,
Electronic switching systems,
Data structures,
Switches,
Multicast protocols,
Computer science"
"E-services: problems, opportunities, and digital platforms","Services that are delivered over the Internet-e-services-pose unique problems yet offer unprecedented opportunities. In this paper, we classify e-services along the dimensions of their level of digitization and the nature of their target markets (business-to-business, business-to-consumer, consumer-to-consumer). Using the case of application services, we analyze how they differ from traditional software procurement and development. Next, we extend the concept of modular platforms to this domain and identify how knowledge management can be used to rapidly assemble new application services. We also discuss how such traceability-based knowledge management can facilitate e-service evolution and version-based market segmentation.","Web and internet services,
Knowledge management,
Application software,
Computer industry,
Taxonomy,
Educational institutions,
Procurement,
Assembly,
Software engineering,
Marketing and sales"
Computational studies of protein folding,,
Java and numerical computing,"Java represents both a challenge and an opportunity to practitioners of numerical computing. The article analyzes the current state of Java in numerical computing and identifies some directions for the realization of its full potential. Many research projects have demonstrated the technology to achieve very high performance in floating-point computations with Java. Its incorporation into commercially available JVMs is more an economic and market issue than a technical one. The combination of Java programming features, pervasiveness, and performance could make it the language of choice for numerical computing. Furthermore, all Java programmers can potentially benefit from the techniques developed for optimizing Java's numerical performance. The authors hope the article will encourage more numerical programmers to pursue developing their applications in Java. This, in turn, will motivate vendors to develop better execution environments, harnessing Java's true potential for numerical computing.",
Capturing dynamic program behaviour with UML collaboration diagrams,"The UML provides means to specify both static and dynamic aspects of object oriented software systems and can be used to assist in all phases of a software development process. With growing support by CASE tools, its applications become more and more widespread. In addition to the automatic generation of class code from diagrams, the recovery of static structure from source code has become common too. However, we focus on the extraction of behavioural information from program code. We introduce a restricted meta model for Java code and present a novel approach to extract the required data, which will then be rendered as UML collaboration diagrams.",
Faster SAT and smaller BDDs via common function structure,"The increasing popularity of SAT and BDD techniques in verification and synthesis encourages the search for additional speed-ups. Since typical SAT and BDD algorithms are exponential in the worst-case, the structure of real-world instances is a natural source of improvements. While SAT and BDD techniques are often presented as mutually exclusive alternatives, our work points out that both can be improved via the use of the same structural properties of instances. Our proposed methods are based on efficient problem partitioning and can be easily applied as pre-processing with arbitrary SAT solvers and BDD packages without source code modifications. Our contribution is validated on the ISCAS circuits and the DIMACS benchmarks. Empirically, our technique often outperforms existing techniques by a factor of two or more. Our results motivate search for stronger dynamic ordering heuristics and combined static/dynamic techniques.",
Hierarchy-based access control in distributed environments,"Access control is a fundamental concern in any system that manages resources, e.g., operating systems, file systems, databases and communications systems. The problem we address is how to specify, enforce, and implement access control in distributed environments. This problem occurs in many applications such as management of distributed project resources, e-newspaper and pay TV subscription services. Starting from an access relation between users and resources, we derive a user hierarchy, a resource hierarchy, and a unified hierarchy. The unified hierarchy is then used to specify the access relation in a way that is compact and that allows efficient queries. It is also used in cryptographic schemes that enforce the access relation. We introduce three specific cryptography based hierarchical schemes, which can effectively enforce and implement access control and are designed for distributed environments because they do not need the presence of a central authority (except perhaps for setup).","Access control,
Permission,
Operating systems,
Project management,
Computer science,
Resource management,
Engineering management,
File systems,
Databases,
Subscriptions"
PKI and digital certification infrastructure,"Secure VPN technology is only possible with the use of appropriate security systems such as encryption, digital signatures, digital certificates, public/private key pairs, non-repudiation, and time-stamping. A PKI comprises a system of certificates, certificate authorities, subjects, relying partners, registration authorities, and key repositories that provide for safe and reliable communications. This paper discusses these key technologies focusing particularly on standardisation as well as looking at some of the challenges pending its widespread operation in the industry.","Certification,
Protocols,
Public key cryptography,
Public key,
Communication system security,
Digital signatures,
Standards development,
Internet,
Computer science,
Virtual private networks"
A compositional logic for protocol correctness,,"Logic,
Protocols,
Body sensor networks,
Computer science,
Error correction,
Information security,
Authentication,
Joining processes,
Cryptography"
A simulation based study of on-demand routing protocols for ad hoc wireless networks,"Ad hoc networks are wireless, mobile networks that can be set up anywhere and anytime without the aid of any established infrastructure or centralized administration. Because of the limited range of each host's wireless transmission, to communicate with hosts outside its transmission range, a host needs to enlist the aid of its nearby hosts in forwarding packets to the destination. However, since there is no stationary infrastructure such as base stations, each host has to act as a router for itself. A routing protocol for ad hoc networks is executed on every host and is therefore subject to the limit of the resources at each mobile host. A good routing protocol should minimize the computing load on the host as well as the traffic overhead on the network. Therefore, a number of routing protocols have been proposed for ad hoc wireless networks. We focus upon on-demand schemes. We study and compare the performance of the following three routing protocols AODV, CBRP and DSR. A variety of workload and scenarios, as characterized by mobility, load and size of the ad hoc network were simulated. Our results indicate that despite its improvement in reducing route request packets, CBRP has a higher overhead than DSR because of its periodic hello messages while AODV's end-to-end packet delay is the shortest when compared to DSR and CBRP.","Routing protocols,
Wireless networks,
Ad hoc networks,
Computational modeling,
Computer simulation,
Wireless communication,
Base stations,
Computer networks,
Laboratories,
Computer science"
Statistical skew modeling for general clock distribution networks in presence of process variations,"Clock skew modeling is important in the performance evaluation and prediction of clock distribution networks. This paper addresses the problem of statistical skew modeling for general clock distribution networks in the presence of process variations. The only available statistical skew model is not suitable for modeling the clock skews of general clock distribution networks in which clock paths are not identical. The old model is also too conservative for estimating the clock skew of a well-balanced clock network that has identical but strongly correlated clock paths (for instance, a well-balanced H-tree). In order to provide a more accurate and more general statistical skew model for general clock distributions, we propose a new approach to estimating the mean values and variances of both clock skews and the maximal clock delay of general clock distribution networks. Based on the new approach, a closed-form model is also obtained for well-balanced H-tree clock distribution networks. The paths delay correlation caused by the overlapped parts of path lengths is considered in the new approach, so the mean values and the variances of both clock skews and the maximal clock delay are accurately estimated for general clock distribution networks. This enables an accurate estimate of yields of both clock skew and maximal clock delay to be made for a general clock distribution network.","Clocks,
Intelligent networks,
Delay estimation,
Predictive models,
Yield estimation,
Helium,
Very large scale integration,
Probability,
Timing,
Information science"
Explicit word error minimization using word hypothesis posterior probabilities,"We introduce a new concept, the time frame error rate. We show that this error rate is closely correlated with the word error rate and use it to overcome the mismatch between Bayes' decision rule which aims at minimizing the expected sentence error rate and the word error rate which is used to assess the performance of speech recognition systems. Based on the time frame errors we derive a new decision rule and show that the word error rate can be reduced consistently with it on various recognition tasks. All stochastic models are left completely unchanged. We present experimental results on five corpora, the Dutch Arise corpus, the German Verbmobil '98 corpus, the English North American Business '94 20k and 64k development corpora, and the English Broadcast News '96 corpus. The relative reduction of the word error rate ranges from 2.3% to 5.1%.",
A self-organizing approach to data forwarding in large-scale sensor networks,"The large number of networked sensors, frequent sensor failures and stringent energy constraints pose unique design challenges for data forwarding in wireless sensor networks. In this paper, we present a new approach to data forwarding in sensor networks that effectively addresses these design issues. Our approach organizes sensors into a dynamic, self-optimizing multicast tree-based forwarding hierarchy, which is data centric and robust to node failures. We demonstrate the effectiveness of our design through simulations.","Intelligent networks,
Large-scale systems,
Wireless sensor networks,
Multicast protocols,
Robustness,
Sensor phenomena and characterization,
Ad hoc networks,
Scalability,
Computer science,
Computer networks"
Set reconciliation with nearly optimal communication complexity,"We consider the problem of efficiently reconciling two similar sets held by different hosts while minimizing the communication complexity. This type of problem arises naturally from gossip protocols used for the distribution of information, but has other applications as well. We describe an approach to such reconciliation based on the encoding of sets as polynomials. The resulting protocols exhibit tractable computational complexity and nearly optimal communication complexity. Moreover, these protocols can be adapted to work over a broadcast channel, allowing many clients to reconcile with one host based on a single broadcast.","Complexity theory,
Protocols,
Broadcasting,
Polynomials,
Computer science,
Computational complexity,
Computational efficiency,
Galois fields,
Gold"
"Quantum factoring, discrete logarithms, and the hidden subgroup problem",Among the most remarkable successes of quantum computation are Shor's efficient quantum algorithms for the computational tasks of integer factorization and the evaluation of discrete logarithms. This article reviews the essential ingredients of these algorithms and draws out the unifying generalization of the so-called hidden subgroup problem.,"Quantum computing,
Zinc,
Computer displays,
Error probability,
Modular construction,
Discrete Fourier transforms,
Polynomials,
Fourier transforms,
Equations,
Registers"
A proposed taxonomy for nailfold capillaries based on their morphology,"Certain diseases cause permanent changes to the shapes and densities of nailfold capillaries and, therefore, nailfold capillaroscopy is important as a tool for diagnosing and monitoring these diseases. The first aim of the project is to resolve differences in terminology that have developed over the years in previous work. We propose a taxonomy for nailfold capillaries that cover six descriptive classes: cuticulis, open, tortuous, crossed, bushy, and bizarre. The first three are parametric in that they may be distinguished by the ratio of capillary length to width and by the curvature of the capillary limbs. The last three are characterized by their topology; a crossed capillary has a closed area that is not connected to the image background. Bushy and bizarre capillaries have atypical shapes that are characterized by the convex hull of their skeleton. These descriptive classes may be modified according to anomalies in width and length. The second aim is to automate the classification of capillaries by encapsulating the taxonomy in an algorithm; our computer program rivals the most experienced clinicians in classifying capillaries consistently with an overall agreement of 85%, with the clinicians' majority view. This was particularly valuable in classifying borderline shapes objectively and consistently.","Taxonomy,
Morphology,
Diseases,
Monitoring,
Shape control,
Terminology,
Topology,
Skeleton,
Pattern classification,
Microscopy"
Computer-assisted reasoning,"Over the past few years, a novel approach to understanding complex and uncertain problems has emerged. The central insight is to conceive of any model run on a computer as a computational experiment. Instead of constructing and running only the single model that we believe best represents the system in question (after making various assumptions and a priori decisions), we can examine large numbers of models that depict alternative plausible future states of the system. This ensemble of plausible models can provide information not captured by any single best-estimate model. Furthermore, working with such an ensemble enables methodological approaches leading to more powerful and appropriate means than have heretofore been available for reasoning about these problems. By looking in many mirrors, each necessarily flawed (albeit in different ways), we can see truths that no single mirror can reveal. The authors show how they have implemented this approach, giving examples of problems where it has been fruitful.","Visualization,
Humans,
Uncertainty,
Power engineering computing,
Petroleum,
Military computing,
Testing,
Design engineering,
Knowledge engineering,
Power engineering and energy"
VERA: an extensible router architecture,"We recognize two trends in router design: increasing pressure to extend the set of services provided by the router and increasing diversity in the hardware components used to construct the router. The consequence of these two trends is that it is becoming increasingly difficult to map the services onto the underlying hardware. Our response to this situation is to define a virtual router architecture, called VERA, that hides the hardware details from the forwarding functions. This paper presents the details of VERA and reports preliminary experiences implementing various aspects of the architecture.",
Resolution is not automatizable unless W[P] is tractable,We show that neither Resolution nor tree-like Resolution is automatizable unless the class W[P] from the hierarchy of parameterized problems is fixed-parameter tractable by randomized algorithms with one-sided error.,
Don't trust your file server,"All too often, decisions about whom to trust in computer systems are driven by the needs of system management rather than data security. In particular data storage is often entrusted to people who have no role in creating or using the data-through outsourcing of data management, hiring of outside consultants to administer servers, or even collocation servers in physically insecure machine rooms to gain better network, connectivity. This paper outlines the design of SUNDR, a network file system designed to run on untrusted servers. SUNDR servers can safely be managed by people who have no permission to read or write data stored in the file system. Thus, people can base their trust decisions on who needs to use data and their administrative decisions on how best to manage the data. Moreover, with SUNDR, attackers will no longer be able to wreak havoc by compromising servers and tampering with data. They will need to compromise clients while legitimate users are logged on. Since clients do not need to accept incoming network connections, they can more easily be firewalled and protected from compromise than servers.","File servers,
Network servers,
Protection,
File systems,
Data security,
Memory,
Computer science,
Computer security,
Data warehouses,
Engineering profession"
A new computationally adaptive formulation of block-matching motion estimation,"This paper presents a new computationally adaptive formulation of the full-search block-matching (FSBM) motion estimation. A key feature of our approach is to progressively adjust the number of computations required per block to the picture variation by applying a data-driven thresholding. Unlike the related methods, the proposed algorithm utilizes a simple threshold updating mechanism which neither requires a large hardware overhead to eliminate redundant search candidates, nor affects the performance of the motion estimation. Compared to the FSBM, it executes four times fewer operations whilst preserving the same quality of results. The algorithm implicitly features array implementation and can be easily realized in VLSI. A supportive hardware design is outlined.","Motion estimation,
Hardware,
Very large scale integration,
PSNR,
Computer architecture,
Computer science education,
Computer science,
Computational complexity,
Video sequences,
Layout"
Development of an automated method for detecting mammographic masses with a partial loss of region,"Recently, we have been developing several automated algorithms for detecting masses on mammograms. For our algorithm, we devised an adaptive thresholding technique for detecting masses, but our system failed to detect masses with a partial loss of region that were located on the edge of the film. This is a common issue in all of the algorithms developed so far by other groups. In order to deal with this problem, we propose a new method in the present study. The partial loss masses are identified by their similarity to a sector-form model in the template matching process. To calculate the similarity, four features are applied: 1) average pixel value; 2) standard deviation of pixel values; 3) standard correlation coefficient defined by the sector-form model; and 4) concentration feature determined from the density gradient. After employing the new method to 335 digitized mammograms, the detection sensitivity for the partial loss masses jumped from 70% to 90% when the number of false positives was kept constant (0.2/image). Moreover, a combination of the existing method and the new method improved the true-positive rate up to 97%. Such results indicate that the new technique may improve the performance of our computer-aided diagnosis system for mammographic masses effectively.",
The automatic measurement of facial beauty,"We develop an automatic facial beauty scoring system based on ratios between facial features. After isolating the face, eyes, eyebrows and mouth in a portrait photograph, we represent a face abstractly as an 8-element vector of ratios between these features. We use a variant of the K-nearest neighbor algorithm, in the context of a parameterized metric space optimized using a genetic algorithm, to learn a beauty assignment function from a training set of photographs rated by humans. We assess performance on a test set of photographs, concluding that when facial ratios are accurately extracted in the computer vision phase, the results of the program are highly correlated with median-human ratings of beauty.","Eyes,
Mouth,
Humans,
Facial features,
Genetic algorithms,
Computer vision,
Feature extraction,
Shape,
Computer science,
Eyebrows"
A computational laboratory for evolutionary trade networks,"This study presents, motivates, and illustrates the use of a computational laboratory (CL) for the investigation of evolutionary trade network formation among strategically interacting buyers, sellers, and dealers. The CL, referred to as the Trade Network Game Laboratory (TNG Lab), is targeted for the Microsoft Windows desktop. The TNG Lab is both modular and extensible and has a clear easily operated graphical user interface. It permits visualization of the formation and evolution of trade networks by means of real-time animations. Data tables and charts reporting descriptive performance statistics are also provided in real time. The capabilities of the TNG Lab are demonstrated by means of labor-market experiments.","Computer networks,
Laboratories,
Animation,
Power generation economics,
Data visualization,
Computational modeling,
Software tools,
Graphical user interfaces,
System testing,
Costs"
Fast LV motion estimation using subspace approximation techniques,"Cardiac motion estimation is very important in understanding cardiac dynamics and in noninvasive diagnosis of heart disease. Magnetic resonance (MR) imaging tagging is a technique for measuring heart deformations. In cardiac tagged MR images, a set of dark lines are noninvasively encoded within myocardial tissue providing the means for measurement of deformations of the heart. The points along tag lines measured in different frames and in different directions carry important information for determining the three-dimensional nonrigid movement of left ventricle. However, these measurements are sparse and, therefore, multidimensional interpolation techniques are needed to reconstruct a dense displacement field. In this paper, a novel subspace approximation technique is used to accomplish this task. The authors formulate the displacement estimation as a variational problem and then project the solution into spline subspaces. Efficient numerical methods are derived by taking advantages of B-spline properties. The proposed technique significantly improves the authors' previous results reported in A.A. Amini et al., ibid., vol. 17, p. 344-56 (1998) with respect to computational time. The method is applied to a temporal sequence of two-dimensional images and is validated with simulated and in vivo heart data.",
Feature extraction and classification of dynamic contrast-enhanced T2*-weighted breast image data,"The relatively low specificity of dynamic contrast-enhanced T1-weighted magnetic resonance imaging (MR) imaging of breast cancer has lead several groups to investigate different approaches to data acquisition, one of them being the use of rapid T2*-weighted imaging. Analyses of such data are difficult due to susceptibility artifacts and breathing motion. One-hundred-twenty-seven patients with breast tumors underwent MR examination with rapid, single-slice T2*-weighted imaging of the tumor. Different methods for classifying the image data set using leave-one-out cross validation were tested. Furthermore, a semi-automatic region of interest (ROI) definition tool was presented and compared with manual ROI definitions from a previous study. Finally, pixel-by-pixel analysis was done and compared with ROI analysis. The analyses were done with and without noise reduction. The minimum enhancement parameter was the most robust and accurate of the parameters tested. The semi-automatic ROI definition method was fast and produced similar results as the manually defined ROIs. Noise reduction improved both sensitivity and specificity, but the improvement was not statistically significant. The pixel-based analysis methods used in the present study did not improve classification results. In conclusion, analysis of T2*-weighted breast images can be done in a rapid and robust manner by using semi-automatic ROI definition tools in combination with noise reduction. Minimum enhancement gives an indication of malignancy in T2*-weighted imaging.","Feature extraction,
Magnetic resonance imaging,
Noise reduction,
Testing,
Noise robustness,
Breast cancer,
Data acquisition,
Data analysis,
Image motion analysis,
Magnetic analysis"
A first course in digital design using VHDL and programmable logic,"Present industry practice has created a high demand for systems designers with knowledge and experience in using programmable logic in the form of CPLDs and FPGAs in addition to hardware description languages. Many universities offer this type of training in advanced digital engineering courses. This paper describes our experience in integrating VHDL and programmable logic devices based on Xilinx Foundation tools and Altera into a first course in logic design. In the main, student reaction to the course was positive. The course seems to have the right blend of being current (using VHDL and FPGAs) and being hands-on (using bread-boarding). We conclude by stating that in our experience, modeling using VHDL and mapping designs to FPGAs can be effectively integrated into a first course in logic design.","Logic design,
Programmable logic arrays,
Programmable logic devices,
Design engineering,
Logic devices,
Educational institutions,
Field programmable gate arrays,
Sequential circuits,
Gas industry,
Hardware"
Distributed query processing using partitioned inverted files,,"Query processing,
Distributed databases,
Workstations,
Indexes,
Distributed computing,
Information retrieval,
Computer networks,
Filtering,
Computer science,
Information management"
The effect of 3D widget representation and simulated surface constraints on interaction in virtual environments,"The paper reports empirical results from two studies of effective user interaction in immersive virtual environments. The use of 2D interaction techniques in 3D environments has received increased attention recently. We introduce two new concepts to the previous techniques: the use of 3D widget representations; and the imposition of simulated surface constraints. The studies were identical in terms of treatments, but differed in the tasks performed by subjects. In both studies, we compared the use of two-dimensional (2D) versus three-dimensional (3D) interface widget representations, as well as the effect of imposing simulated surface constraints on precise manipulation tasks. The first study entailed a drag-and-drop task, while the second study looked at a slider-bar task. We empirically show that using 3D widget representations can have mixed results on user performance. Furthermore, we show that simulated surface constraints can improve user performance on typical interaction tasks in the absence of a physical manipulation surface. Finally, based on these results, we make some recommendations to aid interface designers in constructing effective interfaces for virtual environments.","Mice,
Virtual environment,
Computational modeling,
Computer simulation,
Surface treatment,
Fingers,
Computer science,
Feedback,
Two dimensional displays,
Virtual reality"
Extractors from Reed-Muller codes,"Finding explicit extractors is an important derandomization goal that has received a lot of attention in the past decade. Previous research has focused on two approaches, one related to hashing and the other to pseudorandom generators. A third view, regarding extractors as good error correcting codes, was noticed before. Yet, researchers had failed to build extractors directly from a good code without using other tools from pseudorandomness. We succeed in constructing an extractor directly from a Reed-Muller code. To do this, we develop a novel proof technique. Furthermore, our construction is the first to achieve a degree close to linear. In contrast, the best previous constructions brought the log of the degree within a constant of optimal, which gives polynomial degree. This improvement is important for certain applications. For example, it follows that approximating the VC dimension to within a factor of N/sup 1-/spl delta// is AM-hard for any positive /spl delta/.","Entropy,
Computer science,
Character generation,
Polynomials,
Error correction codes,
Virtual colonoscopy,
History,
Buildings,
Circuits"
A simple KNN algorithm for text categorization,"Text categorization (also called text classification) is the process of identifying the class to which a text document belongs. This paper proposes to use a simple non-weighted features KNN algorithm for text categorization. We propose to use a feature selection method that finds the relevant features for the learning task at hand using feature interaction (based on word interdependencies). This will allow us to reduce considerably the number Of selected features from which to learn, making our KNN algorithm applicable in contexts where both the volume of documents and the size of the vocabulary are high, like with the World Wide Web. Therefore, the KNN algorithm that we propose becomes efficient for classifying text documents in that context (in terms of its predictability and interpretability), as is demonstrated. Its simplicity (WRT its implementation and fine-tuning) becomes its main assets for in-the-field applications.","Text categorization,
Vocabulary,
Computer science,
Web sites,
Solids,
Testing,
Frequency conversion,
Unsolicited electronic mail"
Probabilistic and statistical fuzzy set foundations of competitive exception learning,"After recapitulating various basic notions from classical probability theory and statistics, this theory is generalized to a probabilistic and statistical framework defined on fuzzy sets. Using the new framework, the competitive exception learning algorithm is presented, described and discussed.",
Robust Full Bayesian Learning for Radial Basis Networks,"We propose a hierarchical full Bayesian model for radial basis networks. This model treats the model dimension (number of neurons), model parameters, regularization parameters, and noise parameters as unknown random variables. We develop a reversible-jump Markov chain Monte Carlo (MCMC) method to perform the Bayesian computation. We find that the results obtained using this method are not only better than the ones reported previously, but also appear to be robust with respect to the prior specification. In addition, we propose a novel and computationally efficient reversible-jump MCMC simulated annealing algorithm to optimize neural networks. This algorithm enables us to maximize the joint posterior distribution of the network parameters and the number of basis function. It performs a global search in the joint space of the parameters and number of parameters, thereby surmounting the problem of local minima to a large extent. We show that by calibrating the full hierarchical Bayesian prior, we can obtain the classical Akaike information criterion, Bayesian information criterion, and minimum description length model selection criteria within a penalized likelihood framework. Finally, we present a geometric convergence theorem for the algorithm with homogeneous transition kernel and a convergence theorem for the reversible-jump MCMC simulated annealing method.",
Design of a new 6-DOF parallel haptic device,"In this paper, a new 6-DOF parallel haptic master device is proposed. Many existing haptic devices require large power due to having floating actuator and also have small workspaces. The proposed new mechanism is relatively light, as it employs nonfloating actuators and has large workspace. Kinematic analysis and kinematic optimal design problem is performed for this mechanism. Dexterous workspace, global isotropic index, and global maximum force transmission ratio are considered as kinematic design indices. To deal with such multicriteria optimization problem, composite design index is employed. Actuator sizing for this mechanism is also carried out.",
Parallel-perspective stereo mosaics,"In this paper we present a novel method for automatically and efficiently generating stereoscopic mosaics by seamless registration of optical data collected by a video camera mounted on an airborne platform that undergoes dominant translational motion. There are four critical points discussed in this paper: (1) Using a parallel-perspective representation, a pair of geometrically registered stereo mosaics can be constructed before we explicitly recover any 3D information under rather general motion. (2) A PRISM (parallel ray interpolation for stereo mosaicing) technique is proposed to make stereo mosaics seamless in the presence of motion parallax and for rather arbitrary scenes. A fast PRISM algorithm is presented and issues on stitching point selection and occlusion handling are discussed. (3) The epipolar geometry of parallel-perspective stereo mosaics generated under constrained 6 DOF motion is formulated, which shows optimal baselines, easy search for correspondence and constant depth resolution. (4) The proposed methods for the generation of stereo mosaics and then the reconstruction of a 3D map are efficient in both computation and storage. Experimental results on long video sequences are given.","Cameras,
Robot vision systems,
Computational geometry,
Stereo image processing,
Computer science,
Geometrical optics,
Interpolation,
Layout,
Image reconstruction,
Concurrent computing"
On-line handwritten signature verification using wavelets and back-propagation neural networks,"This paper investigates dynamic handwritten signature verification (HSV) using the wavelet transform with verification by the backpropagation neural network (NN). It is yet another avenue in the approach to HSV that is found to produce excellent results when compared with other methods of dynamic, or on-line, HSV. Using a database of dynamic signatures collected from 41 Chinese writers and 7 from Latin script we extract features (including pen pressure, x and y velocity, angle of pen movement and angular velocity) from the signature and apply the Daubechies-6 wavelet transform using coefficients as input to a NN which learns to verify signatures with a False Rejection Rate (FRR) of 0.0% and False Acceptance Rate (FAR) less of than 0.1.","Handwriting recognition,
Neural networks,
Wavelet transforms,
Angular velocity,
Backpropagation,
Feature extraction,
Testing,
Data mining,
Computer networks,
Information science"
CoDesign - a collaborative pattern design system based on agent,"For effective collaborative working between the parties in a pattern design project team, it is essential that a highly-efficient and feasible cooperative platform is available. This paper presents an agent-based infrastructure for the automated collaborative design of textile industrial patterns. Cooperative awareness among distributed participants is one of the most important issues associated with collaborative pattern design. Several techniques based on awareness intensity management are proposed. In addition, the Pattern Knowledge Library enables intelligent design on the World Wide Web and thus greatly enhances the functionality of computer-supported cooperative design (CSCD). This prototype is intended to serve as a useful cooperative system for designers and should allow faster, better and more economic collaborative design of patterns.","Collaboration,
Collaborative work,
Textile industry,
Project management,
Libraries,
Prototypes,
Intelligent agent,
Computer science,
Cooperative systems,
Environmental economics"
Arguments for cross-layer optimizations in Bluetooth scatternets,"Bluetooth, an innovation in short-range radio technology, has gone through the first stage of standardization, and commercial products based on /spl nu/1.0 specifications will be appearing. While much work has gone into developing the radio technology and hardware for this system, little effort has been focused on additional infrastructure that is necessary for applications in this environment. We examine the issue of supporting ubiquitous computing applications (Weiser, 1991) in a Bluetooth network. The Bluetooth standard defines a multi-hop routing structure, called a scatternet, to address the limitations caused by short-range and small fanout of the underlying link technology. We identify several characteristics, the combination of which makes scatternets different from previously considered networks. Importantly, Bluetooth links are connection-oriented with low-power link modes. We show that the unique aspects of the technology require a redesign of the protocol structure for link formation, IP routing and service discovery. When existing approaches to these protocols are applied to scatternets, the multiple protocol layers would operate without knowledge of each other, resulting in inefficient use of power in many cases. We suggest an alternative approach where there is a single protocol layer providing a level of indirection within the scope of a scatternet. That is, we argue for extensive cross-layer optimizations.","Bluetooth,
Personal area networks,
Ubiquitous computing,
Protocols,
Scattering,
Costs,
Computer science,
Technological innovation,
Standardization,
Hardware"
Optimizing execution of component-based applications using group instances,"Research on programming models for developing applications in the Grid has proposed component-based models as a viable approach, in which an application is composed of multiple interacting computational objects. We have been developing a framework, called filter-stream programming, for building data-intensive applications that query, analyze and manipulate very large data sets in a distributed environment. In this model, the processing structure of an application is represented as a set of processing units, referred to as filters. We develop the problem of scheduling instances of a filter group. A filter group is a set of filters collectively performing a computation for an application. In particular we seek the answer to the following question: should a new instance be created, or an existing one reused? We experimentally investigate the effects of instantiating multiple filter groups on performance under varying application characteristics.",
QoS-aware discovery of wide-area distributed services,"Global computational grids bring together distributed computation/communication resources. Beyond this, we envision the emergence of global 'service grids', which provide a 'market' of application-level distributed services for clients to discover and to request. We study the issue of wide-area service discovery in service grids. We start with an existing basic wide-area service discovery framework. The framework adopts a scalable architecture consisting of a hierarchy of Discovery Servers. We then identify problems with the basic framework, and propose our enhancement of query responsiveness and QoS awareness. The key techniques we introduce include: (1) the addition of QoS feedback capability to clients; and (2) the caching and propagation of discovery results with QoS feedback in the discovery server hierarchy. With these techniques, the enhanced service discovery framework will be faster in finding qualified service providers. Furthermore, it will select a 'good' (with respect to the QoS to be delivered) service provider for each querying client, based on QoS feedback.","Grid computing,
Feedback,
Delay,
Quality of service,
Distributed computing,
Computer science,
Laboratories,
Information management,
Quantum cascade lasers,
Frequency"
FV encoding for low-power data I/O,"The power consumed by I/O pins of a CPU is significant due to high capacitances associated with the pins. While highly effective techniques for reducing address bus switching exist, similarly effective techniques for data bus have not been developed. We have discovered a characteristic of values transmitted over the data bus according to which a small number of distinct values, called frequent values, account for 58-68% of transmissions over the external data bus. To exploit this characteristic we have developed a method for dynamic identification of frequent values and their use in encoding data values using FV (frequent value) encoding scheme. Our experiments show that FV encoding of 32 frequent values yields an average reduction of 42.7% (with onchip data cache) and 67.63% (without on-chip data cache) in data bus switching activity for SPEC95 benchmarks.",
Deriving safety requirements using scenarios,"Elicitation of requirements for safety critical aero-engine control systems is dependent on the capture of core design intent and the systematic derivation of requirements addressing hazardous deviations from that intent. Derivation of these requirements is inextricably linked to the safety assessment process. Conventional civil aerospace practice (as advocated by guidelines such as ARP4754 and ARP4671) promotes the application of Functional Hazard Assessment (FHA) to sets of statements of functional intent. Systematic hazard analysis of scenario-based requirements representations is less well understood. This paper discusses the principles and problems of hazard analysis and proposes an approach to conducting hazard analysis on use case requirements representations. Using the approach, it is possible to justifiably derive hazard-mitigation use cases as first class requirements from systematic hazard analysis of core design intent scenarios. An industrial example is used to illustrate the technique.",
A graph pattern matching approach to software architecture recovery,"This paper presents a technique for recovering the high level design of legacy software systems based on pattern matching and user defined architectural patterns. Architectural patterns are represented using a description language that is mapped to an attributed relational graph and allows to specify the legacy system components and their data and control flow interactions. Such pattern descriptions are viewed as queries that are applied against an entity-relation graph that represents information extracted from the source code of the software system. A multi-phase branch and bound search algorithm with a forward checking mechanism controls the matching process of the two graphs by which, the query is satisfied and its variables are instantiated. An association based scoring mechanism is used to rank the alternative results generated by the matching process. Experimental results of applying the technique on the Xfig system are also presented.",
A partial order approach to noisy fitness functions,"If the fitness values are perturbed by noise then they do not have a definitive total order. As a consequence, traditional selection procedures in evolutionary algorithms may lead to obscure solutions. A potential remedy is as follows: Construct a partial order on the set of noisy fitness values and apply those evolutionary algorithms that have been designed for finding the minimal elements of partially ordered sets. These minimal elements are the only reasonable candidates for the unperturbed true solution. A method for reducing the number of candidate solutions is suggested. From a theoretical point of view it is worth mentioning that all convergence results for evolutionary algorithms with partially ordered fitness sets remain valid for the approach considered here.","Evolutionary computation,
Gaussian distribution,
Gaussian noise,
Noise measurement,
Noise shaping,
Computer science,
Algorithm design and analysis,
Measurement units,
Shape,
Size measurement"
On optimal permutation codes,"Permutation codes are vector quantizers whose codewords are related by permutations and, in one variant, sign changes. Asymptotically, as the vector dimension grows, optimal Variant I permutation code design is identical to optimal entropy-constrained scalar quantizer (ECSQ) design. However, contradicting intuition and previously published assertions, there are finite block length permutation codes that perform better than the best ones with asymptotically large length; thus, there are Variant I permutation codes whose performances cannot be matched by any ECSQ. Along similar lines, a new asymptotic relation between Variant I and Variant II permutation codes is established but again demonstrated to not necessarily predict the performances of short codes. Simple expressions for permutation code performance are found for memoryless uniform and Laplacian sources. The uniform source yields the aforementioned counterexamples.",Optimization methods
The quest for petascale computing,"Although the challenges to achieving petascale computing within the next decade are daunting, several software and hardware technologies are emerging that could help us reach this goal. The authors review these technologies and consider new algorithms capable of exploiting a petascale computer's architecture. One petaflop per second is a rate of computation corresponding to 10/sup 15/ floating-point operations per second. To be of use in scientific computing, a computer capable of this prodigious speed needs a main memory of tens or hundreds of terabytes and enormous amounts of mass storage. Sophisticated compilers and high memory and I/O bandwidth are also essential to exploit the architecture efficiently. To mask the hardware and software complexities from the scientific end user, it would be advantageous to access and use a petascale computer through an advanced problem-solving environment. Immersive visualization environments could play an important role in analyzing and navigating the output from petascale computations. Thus, petascale computing is capable of driving the next decade of research in high-performance computing and communications and will require advances across all aspects of it.","Petascale computing,
Computer architecture,
Hardware,
Supercomputers,
High performance computing,
Scientific computing,
Application software,
Economic forecasting,
Moore's Law,
Bandwidth"
How powerful is adiabatic quantum computation?,"The authors analyze the computational power and limitations of the recently proposed 'quantum adiabatic evolution algorithm'. Adiabatic quantum computation is a novel paradigm for the design of quantum algorithms; it is truly quantum in the sense that it can be used to speed up searching by a quadratic factor over any classical algorithm. On the question of whether this new paradigm may be used to efficiently solve NP-complete problems on a quantum computer, we show that the usual query complexity arguments cannot be used to rule out a polynomial time solution. On the other hand, we argue that the adiabatic approach may be thought of as a kind of 'quantum local search'. We design a family of minimization problems that is hard for such local search heuristics, and establish an exponential lower bound for the adiabatic algorithm for these problems. This provides insights into the limitations of this approach. It remains an open question whether adiabatic quantum computation can establish an exponential speed-up over traditional computing or if there exists a classical algorithm that can simulate the quantum adiabatic process efficiently.",
A proposal for a flexible service plan that is attractive to users and Internet service providers,"The current Internet service provider market does not offer different types of service plans for Internet access. The predominant pricing plan is a flat-rated plan. Since the number of new Internet users is still growing very fast, there is no real competition in the Internet market. Consequently, there is no incentive for Internet service providers (ISPs) to focus on certain user groups by offering more attractive pricing plans in order to differentiate themselves. However, as soon as the number of new Internet users stagnates, ISPs have to specialize on certain market segments. Then, the question raises of what is an attractive service plan that is attractive for users, but allows ISPs to build a sustainable business. Based on empirical results of the INDEX project, we discuss a service plan for Internet access that might be appreciated by Internet users as well as by ISPs. This service plan combines the advantages of flat-rate pricing and usage-based pricing. Using this service plan, users will benefit by receiving a basic service, but are given the choice of higher quality whenever they demand. From the ISP perspective, it will help to focus on certain user groups and limit the peak load on their network.","Proposals,
Web and internet services,
Pricing,
Bandwidth,
Electronic mail,
DSL,
Computer science,
Laboratories,
Quality of service,
Spine"
Fault identification in networks by passive testing,"We employ the finite state machine (FSM) model for networks to investigate fault identification using passive testing. First we introduce the concept of passive testing. Then, we introduce the FSM model with necessary assumptions and justification. We introduce the fault model and the fault detection algorithm using passive testing. Extending this result, we develop the theorems and algorithms for fault identification. An example is given illustrating our approach. Then, extensions to our approach are introduced to achieve better fault identification. We then illustrate our technique through a simulation of a practical X.25 example. Finally future extensions and potential trends are discussed.","Fault diagnosis,
Intelligent networks,
Testing,
Protocols,
Fault detection,
Automata,
Computer network management,
Educational institutions,
Computer science,
Open systems"
Using artificial anomalies to detect unknown and known network intrusions,"Intrusion detection systems (IDSs) must be capable of detecting new and unknown attacks, or anomalies. We study the problem of building detection models for both pure anomaly detection and combined misuse and anomaly detection (i.e., detection of both known and unknown intrusions). We propose an algorithm to generate artificial anomalies to coerce the inductive learner into discovering an accurate boundary between known classes (normal connections and known intrusions) and anomalies. Empirical studies show that our pure anomaly detection model trained using normal and artificial anomalies is capable of detecting more than 77% of all unknown intrusion classes with more than. 50% accuracy per intrusion class. The combined misuse and anomaly detection models are as accurate as a pure misuse detection model in detecting known intrusions and are capable of detecting at least 50% of unknown intrusion classes with accuracy measurements between 75% and 100% per class.","Intrusion detection,
Computer science,
Machine learning algorithms,
Educational institutions,
Data analysis,
Event detection,
Pattern matching,
Government,
Web and internet services,
Training data"
Implicit surfaces that interpolate,"Implicit surfaces are often created by summing a collection of radial basis functions. Researchers have begun to create implicit surfaces that exactly interpolate a given set of points by solving a simple linear system to assign weights to each basis function. Due to their ability to interpolate, these implicit surfaces are more easily controllable than traditional ""blobby"" implicits. There are several additional forms of control over these surfaces that make them attractive for a variety of applications. Surface normals may be directly specified at any location over the surface, and this allows the modeller to pivot the normal while still having the surface pass through the constraints. The degree of smoothness of the surface can be controlled by changing the shape of the basis functions, allowing the surface to be pinched or smooth. On a point-by-point basis the modeller may decide whether a constraint point should be exactly interpolated or approximated. Applications of these implicits include shape transformation, creating surfaces from computer vision data, creation of an implicit surface from a polygonal model, and medical surface reconstruction.","Surface reconstruction,
Computer science,
Shape control,
Animation,
Educational institutions,
Linear systems,
Face detection,
Application software,
Computer vision,
Computer graphics"
Simulation evaluation of a heterogeneous Web proxy caching hierarchy,"This paper uses trace-driven simulations to evaluate the performance of different cache management techniques for multi-level Web proxy caching hierarchies. In particular the experiments consider heterogeneous cache replacement policies within a two-level caching hierarchy, and size-based partitioning across the levels of a caching hierarchy. Three different synthetic Web proxy workloads are used in the study, reflecting complete overlap, partial overlap, and no overlap in the workloads seen by the child-level proxies. The simulation results demonstrate that heterogeneous replacement policies and size-based partitioning each offer modest improvements in caching performance. The sensitivity of the results to the degree of workload overlap is also discussed.","Web server,
Computational modeling,
Computer science,
Internet,
Network servers,
Telecommunication traffic,
Delay,
Laboratories,
Filters,
Collaboration"
Minimal subset evaluation: rapid warm-up for simulated hardware state,"This paper introduces minimal subset evaluation (MSE) as a way to reduce time spent on large-structure warm-up during the fast-forwarding portion of processor simulations. Warm up is commonly used prior to full-detail simulation to avoid cold-start bias in large structures like caches and branch predictors. Unfortunately, warm up can be very time consuming, often representing 50% or more of total simulation time. Previous techniques have used the entire fast forward interval to obtain accurate warm up, which may be prohibitive for large parameter-space searches, or chosen a short but ad-hoc warm-up length that reduces simulation time but may sacrifice accuracy. MSE probabilistically determines a minimally sufficient fraction of the set of fast forward transactions that must be executed for warm up to accurately produce state as it would have appeared had the entire fast forward interval been used for warm up. The paper describes the mathematical underpinnings of MSE and demonstrates its effectiveness for both single-large-sample and multiple-sample simulation styles. In our experiments, MSE yields errors of less than 1% in IPC measurements with cycle-accurate simulation, while reducing simulation times by an average factor of two or more.",
Distributed Database Management Systems and the Data Grid,"Currently, Grid research as well as distributed database research deals with data replication but both tackle the problem from different points of view. The aim of this paper is to outline both approaches and try to find commonalities between the two worlds in order to have a most efficient Data Grid that manages data stored in objectoriented databases. Our target object-oriented database management system is Objectivity/DB which is currently the database of choice in some existing High Energy Physics (HEP) experiments as well as in next generation experiments at CERN. The characteristics of Data Grids are described, especially within the High Energy Physics community, and needs for Data Grids are defined. The Globus toolkit is the Grid middle-ware on which we base our discussions on Grid research.",
A decision procedure for an extensional theory of arrays,"A decision procedure for a theory of arrays is of interest for applications in formal verification, program analysis and automated theorem proving. This paper presents a decision procedure for an extensional theory of arrays and proves it correct.","Logic arrays,
Laboratories,
Design automation,
Application software,
Formal verification,
Libraries,
Equations"
"Bandwidth-efficient multicast routing for multihop, ad-hoc wireless networks","In this paper, we propose and investigate a bandwidth-efficient multicast routing protocol for ad-hoc networks. The proposed protocol achieves low communication overhead, namely, it requires a small number of control packet transmissions for route setup and maintenance. The proposed protocol also achieves high multicast efficiency, namely, it delivers multicast packets to receivers with a small number of transmissions. In order to achieve low communication overhead and high multicast efficiency, the proposed protocol employs the following mechanisms: (1) on-demand invocation of the route setup and route recovery processes to avoid periodic transmissions of control packets, (2) a new route setup process that allows a newly joining node to find the nearest forwarding node to minimize the number of forwarding nodes, and (3) a route optimization process that detects and removes unnecessary forwarding nodes to eliminate redundant and inefficient routes. Our simulation results show that the proposed protocol achieves high multicast efficiency with low communication overhead compared with other existing multicast routing protocols, especially in the ease where the number of receivers in a multicast group is large.","Spread spectrum communication,
Wireless networks,
Multicast protocols,
Routing protocols,
Mobile computing,
Mobile communication,
Bandwidth,
Computer science,
Wire,
Land mobile radio cellular systems"
Sensor performance specifications,"The author explains sensor specifications which quantify the ability of sensor to provide measurements of physical variables. Sensor manufacturer typically provide information about sensor specifications in the form of a specification sheet. The concepts described will help one interpret the information on a specification sheet. They can be applied to all branches of science and engineering, not just control systems. However, the systems approach of control engineers provides a unique perspective on these topics and associated issues.","Control systems,
Length measurement,
Velocity measurement,
Computer displays,
Systems engineering and theory,
Sensor phenomena and characterization,
Dairy products,
Business,
Accidents,
Petroleum"
Computers in imaging and guided surgery,"The authors review the main technical issues in computer-integrated surgery (CIS) systems. They illustrate with examples of working systems the state of the art in the field and provide perspectives on deployment and future developments. They discuss the structure of CIS systems. At the core is a computer (or network of computers) running various modeling and analysis processes, including image and sensor processing, creation and manipulation of patient-specific anatomical models, surgical planning, visualization, monitoring, and control of surgical processes. After receiving information about the patient from medical imaging devices, some CIS systems act directly on the patient using specialized robots or other computer controlled therapy devices.","Surgery,
Computational Intelligence Society,
Computer networks,
Medical control systems,
Image analysis,
Image sensors,
Process planning,
Visualization,
Patient monitoring,
Biomedical monitoring"
Exploring and exploiting wire-level pipelining in emerging technologies,"Pipelining is a technique that has long since been considered fundamental by computer architects. However, the world of nanoelectronics is pushing the idea of pipelining to new and lower levels-particularly the device level. How this affects circuits and the relationship between their timing, architecture, and design will be studied in the context of an inherently self-latching nanotechnology termed quantum cellular automata (QCA). Results indicate that this nanotechnology offers the potential for ""free"" multi-threading and ""processing-in-wire"". All of this could be accomplished in a technology that could be almost three orders of magnitude denser than an equivalent design fabricated in a process at the end of the CMOS curve.",
Binary multiplication radix-32 and radix-256,"Multipliers are used at many different places in microprocessor design. As the non-memory sub-blocks of the microprocessor with the largest size and delay, multipliers have a big impact on the cycle time of the microprocessor. Targeting deeper pipelines and higher clock frequencies, there is a growing demand for multiplier designs that can be split into shorter stages. For this purpose, the use of Booth recoding has been a popular method to cut down the number of partial products in a multiplier to reduce the delay of the partial product accumulation and to simplify the partition of the multiplier into several shorter stages. The complexity to pre-compute an increasing number of digit multiples of the multiplicand within the multiplier unit limits the use of Booth recoding mainly to radices 4 and 8. We propose novel encoding schemes for the implementation of higher radix multiplication. In particular we consider multiplication radix-32 and radix-256. The features provide more flexible multiplier designs that can be implemented in shorter pipeline stages. We compare the proposed designs with multipliers that use traditional Booth recoding.",
Customizing PRM roadmaps at query time,"We propose an approach for building and querying probabilistic roadmaps. In the roadmap construction stage, we build coarse roadmaps by performing only an approximate validation of the roadmap nodes and/or edges. In the query stage, the roadmap is validated and refined only in the area of interest for the query, and moreover is customized in accordance with any specified query preferences. This approach, which postpones some of the validation checks (e.g., collision checks) to the query phase, yields more efficient solutions to many problems. An important benefit of our approach is that it gives one the ability to customize the same roadmap in accordance with multiple, variable, query preferences. For example our approach enables one to find a path which maintains a particular clearance, or makes at most some specified number of sharp turns. Our preliminary results on problems drawn from diverse application domains show that this new approach dramatically improves performance, and shows remarkable flexibility when adapting to different query requirements.",
Successive approximation of abstract transition relations,"Recently, we have improved the efficiency of the predicate abstraction scheme presented by Das, Dill and Park (1999). As a result, the number of validity checks needed to prove the necessary verification condition has been reduced. The key idea is to refine an approximate abstract transition relation based on the counter-example generated. The system starts with an approximate abstract transition relation on which the verification condition (in our case, this is a safety property) is model-checked. If the property holds then the proof is done; otherwise the model checker returns an abstract counter-example trace. This trace is used to refine the abstract transition relation if possible and start anew. At the end of the process, the system either proves the verification condition or comes up with an abstract counter-example trace which holds in the most accurate abstract transition relation possible (with the user-provided predicates as a basis). If the verification condition fails in the abstract system, then either the concrete system does not satisfy it or the abstraction predicates chosen are not strong enough. This algorithm has been used on a concurrent garbage collection algorithm and a secure contract-signing protocol. This method improved the performance on the first problem significantly, and allowed us to tackle the second problem, which the previous method could not handle.",
A dynamic heuristic broadcasting protocol for video-on-demand,Most existing distribution protocols for video-on-demand are tailored for a specific range of video access rates and perform poorly beyond that range. We present a dynamic heuristic broadcasting protocol that performs as well as stream tapping with unlimited extra tapping at low video access rates and has the same average bandwidth requirements as the best existing broadcasting protocols at high video access rates. We also show how our protocol can handle compressed video and adapt itself to the individual bandwidth requirements of each video.,
Component identification method with coupling and cohesion,"Since the introduction of component-based development (CBD), an effective component identification technique is an important factor for successful CBD projects. As in the CORBA component model of OMG, a component consists of one or more related objects, carrying out a homogeneous functionality. Most of the CBD methodologies utilize UML as the basic notational convention. A component diagram or its variation is used to depict components. However, current CBD methodologies lack a systematic component identification algorithm that can be effectively used to group related use-cases and classes into components. In this paper, we introduce a component identification method that considers component coupling, cohesion, dependency, interface, granularity, and architecture. We also provide a case study on a large-scale real CBD project, in which the proposed method was applied.",
Lightweight extraction of object models from bytecode,"A program's object model captures the essence of its design. For some programs, no object model was developed during design; for others, an object model exists but may be out-of-sync with the code. This paper describes a tool that automatically extracts an object model from the class-files of a Java program. Unlike existing tools, it handles container classes by inferring the types of elements stored in a container and eliding the container itself. This feature is crucial for obtaining models that show the structure of the abstract state and bear some relation to conceptual models. Although the tool performs only a simple, heuristic analysis that is almost entirely local, the resulting object model is surprisingly accurate. The paper explains what object models are and why they are useful; describes the analysis, its assumptions, and limitations; evaluates the tool for accuracy, and illustrates its use on a suite of sample programs.",
Energy-efficient instruction dispatch buffer design for superscalar processors,"The instruction dispatch buffer (DB, also known as an issue queue) used in modem superscalar processors is a considerable source of energy dissipation. We consider design alternatives that result in significant reductions in the power dissipation of the DB (by as much as 60%) through the use of: (a) fast comparators that dissipate energy mainly on a tag match, (b) zero byte encoding of operands to imply the presence of bytes with all zeros and, (c) bitline segmentation. Our results are validated by the execution of SPEC 95 benchmarks on true hardware level, cycle-by-cycle simulator for a superscalar processor and SPICE measurements for actual layouts of the DB and its variants in a 0.5 micron CMOS process.","Energy efficiency,
Process design,
Computer science,
Permission,
Registers,
Logic,
Energy dissipation,
Power dissipation,
Encoding,
Hardware"
Forward error correction codes to reduce intercarrier interference in OFDM,"Orthogonal Frequency Division Multiplexing (OFDM) is sensitive to the carrier frequency offset (CFO), which destroys orthogonality and causes intercarrier interference (ICI). Recently, a simple rate 1/2 repeat coding scheme has been shown to be effective in suppressing ICI. That such a simple coding scheme is so effective raises an interesting question. Can more powerful error correcting codes with less redundancy be used just as effectively for the same purpose? In this paper, we propose the use of rate-compatible punctured convolutional (RCPC) codes.",
Subspace Information Criterion for Model Selection,"The problem of model selection is considerably important for acquiring higher levels of generalization capability in supervised learning. In this article, we propose a new criterion for model selection, the subspace information criterion (SIC), which is a generalization of Mallows's CL. It is assumed that the learning target function belongs to a specified functional Hilbert space and the generalization error is defined as the Hilbert space squared norm of the difference between the learning result function and target function. SIC gives an unbiased estimate of the generalization error so defined. SIC assumes the availability of an unbiased estimate of the target function and the noise covariance matrix, which are generally unknown. A practical calculation method of SIC for least-mean-squares learning is provided under the assumption that the dimension of the Hilbert space is less than the number of training examples. Finally, computer simulations in two examples show that SIC works well even when the number of training examples is small.",
A method of attenuation map and emission activity reconstruction from emission data,"An iterative algorithm that reconstructs the attenuation map and emission activity distribution from SPECT emission data is proposed. The algorithm is based on the quasilinearized attenuated Radon transform. At each iteration of the algorithm, emission activity distribution is reconstructed using the attenuation map from the previous iteration. Then, the attenuation map is estimated by using the current emission image. To effectively locate the attenuation map within a possible class of solutions the attenuation map is constrained by an optimal basis set, which is derived from a cross-correlation of a priori images known as a ""knowledge set."" Computer simulations show that the proposed algorithm has the capability to reconstruct the emission activity distribution and the transmission map without the use of transmission measurements. The proposed method has been tested using clinically acquired data.",
A probabilistic framework for surface reconstruction from multiple images,"The paper presents a novel probabilistic framework for 3D surface reconstruction from multiple stereo images. The method works on a discrete voxelized representation of the scene. An iterative scheme is used to estimate the probability that a scene point lies on the true 3D surface. The novelty of our approach lies in the ability to model and recover surfaces which may be occluded in some views. This is done by explicitly estimating the probabilities that a 3D scene point is visible in a particular view from the set of given images. This relies on the fact that for a point on a lambertian surface, if the pixel intensities of its projection along two views differ, then the point is necessarily occluded in one of the views. We present results of surface reconstruction from both real and synthetic image sets.","Surface reconstruction,
Image reconstruction,
Layout,
Stereo vision,
Cameras,
Stereo image processing,
Computer vision,
Machine vision,
Computer science,
Educational institutions"
Hidden dependencies in program comprehension and change propagation,"Large software systems are difficult to understand and maintain. Program dependency analysis plays a key role in both understanding and maintenance. This paper discusses hidden dependencies among software components that make both understanding and maintenance hard. A hidden dependency is a relationship between two seemingly independent components, and it is caused by the data flow in a third software component. The paper uses abstract system dependency graphs to define hidden dependencies. It discusses the impact of hidden dependencies on the process of change propagation and also discusses an algorithm that warns about the possible presence of hidden dependencies.",
Surface knowledge: toward a predictive theory of materials,"To understand how materials function, we must understand how molecules, from the environment, interact with their surfaces. This article describes new methodologies for studying surface dynamics at the atomic level over a range of time scales. We focus here on advances in this field, specifically, the development of methodologies, that combine the density-functional theory with elasticity theory, thermodynamics, or statistical mechanics.",
Dependency preserving probabilistic modeling of switching activity using Bayesian networks,We propose a new switching probability model for combinational circuits using a logic-induced-directed-acyclic-graph (LIDBG) and prove that such a graph corresponds to a Bayesian network guaranteed to map all the dependencies inherent in the circuit. This switching activity can be estimated by capturing complex dependencies (spatiotemporal and conditional) among signals efficiently by local message-passing based on the Bayesian networks. Switching activity estimation of ISCAS and MCNC circuits with random input streams yield high accuracy (average mean error=0.002) and low computational time (average time=3.93 seconds).,"Bayesian methods,
Random variables,
Switching circuits,
Computer science,
Microelectronics,
Computational modeling,
Circuit simulation,
Probability distribution,
Permission,
Yield estimation"
Evaluating long-term spectral subtraction for reverberant ASR,"Even a modest degree of room reverberation can greatly increase the difficulty of automatic speech recognition. We have observed large increases in speech recognition word error rates when using a far-field (3-6 feet) microphone in a conference room, in comparison with recordings from head-mounted microphones. In this paper, we describe experiments with a proposed remedy based on the subtraction of an estimate of the log spectrum from a long-term (e.g., 2 s) analysis window, followed by overlap-add resynthesis. Since the technique is essentially one of enhancement, the processed signal it generates can be used as input for complete speech recognition systems. Here we report results with both the HTK and the SRI Hub-5 recognizer. For simpler recognizer configurations and/or moderate-sized training, the improvements are huge, while moderate improvements are still observed for more complex configurations under a number of conditions.",
Let's talk! Socially intelligent agents for language conversation training,"This paper promotes socially intelligent animated agents for the pedagogical task of English conversation training for native speakers of Japanese. As a novel feature, social role awareness is introduced to animated conversational agents, that are by non-strong affective reasoners, but otherwise often lack the social competence observed in humans. In particular, humans may easily adjust their behavior depending on their respective role in a social setting, whereas their synthetic pendants tend to be driven mostly by emotions and personality. Our main contribution is the incorporation of a ""social filter program"" to mental models of animated agents. This program may qualify an agent's expression of its emotional state by the social contest, thereby enhancing the agent's believability as a conversational partner. Our implemented system is web-based and demonstrates socially aware animated agents in a virtual coffee shop environment. An experiment with our conversation system shows that users consider socially aware agents as more natural than agents that violate conventional practices.","Intelligent agent,
Animation,
Humans,
Robustness,
Marketing and sales,
Natural languages,
Filters,
Cognitive science,
Character generation,
Guidelines"
Planning agents in JAMES,"Testing is an obligatory step in developing multiagent systems. For testing multiagent systems in virtual, dynamic environments, simulation systems are required that support a modular, declarative construction of experimental frames, that facilitate the embedding of a variety of agent architectures and that allow an efficient parallel, distributed execution. We introduce the system JAMES (a Java based agent modeling environment for simulation). In JAMES, agents and their dynamic environment are modeled as reflective, time-triggered state automata. Its possibilities to compose experimental frames based on predefined components, to express temporal interdependencies, to capture the phenomenon of proactiveness and reflectivity of agents are illuminated by experiments with planning agents. The underlying planning system is a general-purpose system, about which no empirical results exist besides traditional static benchmark tests. We analyze the interplay between heuristics for selecting goals, viewing range, commitment strategies, explorativeness, and trust in the persistence of the world and uncover properties of the the agent, the planning engine, and the chosen test scenario: TILEWORLD.","System testing,
Multiagent systems,
Strategic planning,
Concrete,
Benchmark testing,
Computer science,
Modular construction,
Java,
Automata,
Engines"
A new approach to target recognition for LADAR data,"We discuss target detection in LADAR intensity images. Thirteen features, eleven of which come from an asymmetric co-occurrence matrix, are extracted from region-of-interest windows in each image. Two methods of feature selection are applied to the extracted vectors. Random selection leads to a pair of selected features for a nearest-neighbor rule (1-nn) detector. Extended backpropagation leads to six selected features using a modified multilayered perceptron (MLP) network. The 1-nn detector achieves a test-error rate of about 16% at a false-alarm rate of 8%. The MLP has a test-error rate of about 12% with a false-alarm rate of 6%.","Target recognition,
Laser radar,
Detectors,
Testing,
Layout,
Vehicles,
Computer science,
Inspection,
Pixel,
Object detection"
Optimal assignment of high threshold voltage for synthesizing dual threshold CMOS circuits,"Development of the process technology for dual threshold (dual V/sub th/) CMOS circuit has opened up the possibility of using it to reduce static power in low voltage high performance circuits. It has been demonstrated that by using transistors of a low threshold voltage for gates on the critical path, and by using a high threshold voltage for gates in the off-critical path it is possible to significantly reduce leakage power consumption of a circuit without performance degradation. In this paper we have a new algorithm to realize dual CMOS circuits. Our algorithm produces significantly better results for ISCAS benchmark circuits compared to reported results.",
Efficient solution of GSPNs using canonical matrix diagrams,"The solution of a generalized stochastic Petri net (GSPN) is severely restricted by the size of its underlying continuous-time Markov chain. In recent work (G. Ciardo and A.S. Miner, 1999), matrix diagrams built from a Kronecker expression for the transition rate matrix of certain types of GSPNs were shown to allow for more efficient solution; however, the GSPN model requires a special form, so that the transition rate matrix has a Kronecker expression. In this paper, we extend the earlier results to GSPN models with partitioned sets of places. Specifically, we give a more restrictive definition for matrix diagrams and show that the new form is canonical. We then present an algorithm that builds a canonical matrix diagram representation for an arbitrary non-negative matrix, given encodings for the sets of rows and columns. Using this algorithm, a Kronecker expression is not required to construct the matrix diagram. The efficient matrix diagram algorithms for numerical solution presented earlier are still applicable. We apply our technique to several example GSPNs.","Stochastic systems,
Encoding,
Sparse matrices,
Computer science,
Stochastic processes,
Partitioning algorithms,
Petri nets,
Performance analysis,
Explosions,
Approximation error"
Estimating 3D body pose using uncalibrated cameras,"An approach for estimating 3D body pose from multiple, uncalibrated views is proposed. First, a mapping from image features to 2D body joint locations is computed using a statistical framework that yields a set of several body pose hypotheses. The concept of a ""virtual camera"" is introduced that makes this mapping invariant to translation, image-plane rotation, and scaling of the input. As a consequence, the calibration matrices (intrinsics) of the virtual cameras can be considered completely known, and their poses are known up to a single angular displacement parameter Given pose hypotheses obtained in the multiple virtual camera views, the recovery of 3D body pose and camera relative orientations is formulated as a stochastic optimization problem. An Expectation-Maximization algorithm is derived that can obtain the locally most likely (self-consistent) combination of body pose hypotheses. Performance of the approach is evaluated with synthetic sequences as well as real video sequences of human motion.",
Evaluating power consumption of parameterized cache and bus architectures in system-on-a-chip designs,"Architectures with parameterizable cache and bus can support large tradeoffs between performance and power. We provide simulation data showing the large tradeoffs by such an architecture for several applications and demonstrating that the cache and bus should be configured simultaneously to find the optimal solutions. Furthermore, we describe analytical techniques for speeding up the cache/bus power and performance evaluation by several orders of magnitude over simulation, while maintaining sufficient accuracy with respect to simulation-based approaches.",
Performance comparison of data distribution management strategies,,"Distributed decision making,
Computational modeling,
Computer simulation,
Discrete event simulation,
Subscriptions,
Filtering,
Computer science,
Large-scale systems,
Analytical models,
Performance analysis"
Resettably-sound zero-knowledge and its applications,"Resettably-sound proofs and arguments maintain soundness even when the prover can reset the verifier to use the same random coins in repeated executions of the protocol. We show that resettably-sound zero-knowledge arguments for NP exist if collision-free hash functions exist. In contrast, resettably-sound zero-knowledge proofs are possible only for languages in P/poly. We present two applications of resettably-sound zero-knowledge arguments. First, we construct resettable zero-knowledge arguments of knowledge for NP, using a natural relaxation of the definition of arguments (and proofs) of knowledge. We note that, under the standard definition of proof of knowledge, it is impossible to obtain resettable zero-knowledge arguments of knowledge for languages outside BPP. Second, we construct a constant-round resettable zero-knowledge argument for NP in the public-key model, under the assumption that collision-free hash functions exist. This improves upon the sub-exponential hardness assumption required by previous constructions. We emphasize that our results use non-black-box zero-knowledge simulations. Indeed, we show that some of the results are impossible to achieve using black-box simulations. In particular, only languages in BPP have resettably-sound arguments that are zero-knowledge with respect to black-box simulation.",
TCP-friendly SIMD congestion control and its convergence behavior,"The increased diversity of Internet application requirements has spurred interest in flexible congestion control mechanisms. Window-based congestion control schemes use increase rules to probe available bandwidth, and decrease rules to back off when congestion is detected. The control rules are parameterized so as to ensure that the resulting protocol is TCP-friendly in terms of the relationship between throughput and packet loss rate. We propose a novel window-based congestion control algorithm called SIMD (Square-Increase/Multiplicative-Decrease). Contrary to previous memoryless controls, SIMD utilizes history information in its control rules. It uses multiplicative decrease but the increase in window size is in proportion to the square of the time elapsed since the detection of the last loss event. Thus, SIMD can efficiently probe available bandwidth. Nevertheless, SIMD is TCP-friendly as well as TCP-compatible through RED routers. Furthermore, SIMD has much better convergence behavior than TCP-friendly AIMD and binomial algorithms proposed previously.",
Simultaneous localization and mapping in domestic environments,This paper describes an accurate and robust algorithm for simultaneous localization and map building (SLAM). The objective of SLAM is to enable a mobile robot to build an internal representation (map) of an unexplored environment while simultaneously using that map to navigate. An extended Kalman filter (EKF) approach is used to process the information acquired by the sonar sensors mounted on the robot. A method for recovering from failures of the SLAM algorithm is presented for increasing the robustness of the general EKF method. Real experiments are presented considering a Nomadic SuperScout mobile robot navigating in a domestic environment.,
A robust control approach to the swing up control problem for the Acrobot,"Studies the swing up control for the Acrobot, i.e., to move the Acrobot from its stable downward position to its unstable inverted position and balance it about the vertical. The combination of the partial linearization control for the swing up phase proposed by Spong (1995) and the robust control for the capture and balance phase is utilized in this paper. The key idea is first to treat the speed of the second link when it rotates across the vertical as an uncertainty, and then to design a robust controller based on the quadratic stabilization method to cope with such uncertainty. It is shown that that the robust controller is generally superior to the LQR controller in capturing and balancing the Acrobot, and the difficulty of tuning the gains in the swing up phase can be ameliorated.","Robust control,
Uncertainty,
Control systems,
Communication system control,
Actuators,
Mechanical systems,
Robots,
Computer science,
Systems engineering and theory,
Costs"
Automatic scoliosis detection based on local centroids evaluation on moire topographic images of human backs,"This paper presents a technique for automating human scoliosis detection by computer based on moire topographic images of human backs. Scollosis is a serious disease often suffered by teenagers. For prevention, screening is performed at schools in Japan employing a moire method in which doctors inspect moire images of subjects' backs visually. The inspection of a large number of moire images collected by the school screening causes exhaustion of doctors and leads to misjudgment. Computer-aided diagnosis of scoliosis has, therefore, been requested eagerly by orthopedists. To automate the inspection process, unlike existent three-dimensional techniques, displacement of local centroids is evaluated two-dimensionally between the left-hand side and the right-hand side of the moire images in the present technique. The technique was applied to real moire images to draw a distinction between normal and abnormal cases. According to the leave-out method, the entire 120 image data (60 normal and 60 abnormal) were separated into three data sets. The linear discriminant function based on Mahalanobis distance was defined on the two-dimensional feature space employing one of the data sets containing 40 moire images and classified 80 images in the remaining two sets. The technique finally achieved the average classification rate of 88.3%.",
A fast high-resolution track trigger for the H1 experiment,"After 2001, the upgraded ep collider HERA will provide an about five times higher luminosity for the two experiments H1 and ZEUS. To cope with the expected higher event rates, the H1 collaboration is building a track-based trigger system, the Fast Track Trigger (FTT). It will be integrated in the first three levels (L1-L3) of the H1 trigger scheme to provide higher selectivity for events with charged particles. The FTT will allow reconstruction of three-dimensional tracks in the central drift chamber down to 100 MeV/c within the L2 latency of /spl sim/23 /spl mu/s. To reach the necessary momentum resolution of /spl sim/5% (at 1 GeV/c), sophisticated reconstruction algorithms have to be implemented using high-density field-programmable gate arrays and their embedded content addressable memories. The final track parameter optimization will be done using noniterative fits implemented in digital signal processors. While at the first trigger level rough track information will be provided, at L2 tracks with high resolution are available to form trigger decisions on topological and other track-based criteria like multiplicities and momenta. At the third trigger level, a farm of commercial processor boards will be used to compute physics quantities such as invariant masses.",
First International Newspaper Segmentation contest,"This paper presents the results of the First International Newspaper Segmentation contest that was organized on the frame of ICDAR 2001 conference. The aim of this contest was to evaluate all existing algorithms for document image segmentation that can be applied to Newspaper page segmentation. We evaluated the performance of three different newspaper segmentation algorithms on tracing all basic entities that appear in newspaper pages from the beginning of the previous century up to the present. The selected entities are text regions, lines and images/drawings. Both training and test sets come from Greek and English newspapers. The performance evaluation method is based on counting the number of matches between the entities detected by the algorithms and the entities of the ground truth. In order to rank the global performance of each participant, we employed a metric that combines the average values of detection rate and recognition accuracy.","Image segmentation,
Testing,
Graphics,
Computer science,
Training data,
Layout"
Parallel and adaptive reduction of hyperspectral data to intrinsic dimensionality,,"Hyperspectral imaging,
Hyperspectral sensors,
Principal component analysis,
Remote sensing,
Adaptive algorithm,
NASA,
Space technology,
Parallel algorithms,
Jacobian matrices,
Aerospace engineering"
Asymmetric cone-beam transmission tomography,"Transmission scans are an important part of the process of obtaining patient attenuation maps for accurate SPECT (single photon emission computed tomography) imaging. In order to avoid truncation of the projection data, asymmetric cone-beam imaging geometries were investigated. By using the Beacon/sup TM/ SPECT system, it was shown that in the acquisition process transmission and emission imaging geometries can be different. Utilizing this new system, cone-beam transmission scans can be performed while the parallel collimators are attached for emission data acquisition. Patient and phantom studies were performed and compared with X-ray CT. A whole-body helical scan to obtain a whole-body attenuation map is also proposed. For helical asymmetric cone-beam geometries, at least two detectors are required in order to provide sufficient cone-beam data. If two detectors are used, they must have opposite geometries. Computer simulations were performed to demonstrate the feasibility of the proposed asymmetric helical cone-beam scanning geometry.",
A bitstream reconfigurable FPGA implementation of the WSAT algorithm,"A field programmable gate array (FPGA) implementation of a coprocessor which uses the WSAT algorithm to solve Boolean satisfiability problems is presented. The input is a SAT problem description file from which a software program directly generates a problem-specific circuit design which can be downloaded to a Xilinx Virtex FPGA device and executed to find a solution. On an XCV300, problems of 50 variables and 170 clauses can be solved. Compared with previous approaches, it avoids the need for resynthesis, placement, and routing for different constraints. Our coprocessor is eminently suitable for embedded applications where energy, weight and real-time response are of concern.","Field programmable gate arrays,
Circuit synthesis,
Coprocessors,
Algorithm design and analysis,
Hardware,
Routing,
Boolean functions,
Computer science,
Circuit testing,
Real time systems"
Simulation based performance analysis of web servers,"This paper presents a general framework for modeling distributed computing environments for performance analysis by means of Timed Hierarchical Coloured Petri Nets. The proposed framework was used to build and analyze a Coloured Petri Net model of a HTTP web server. Analysis of the performance of the web server model reveals how the web server will respond to changes in the arrival rate of requests, and alternative configurations of the web server model are examined. These are the results of a research project conducted in cooperation between the CPN Centre and Hewlett-Packard Corporation on capacity planning and performance analysis of distributed computing environments.","Analytical models,
Performance analysis,
Web server,
Distributed computing,
Computational modeling,
Petri nets,
Capacity planning,
Network servers,
Computer science,
Internet"
Separating features in source code: an exploratory study,"Most software systems are inflexible. Reconfiguring a system's modules to add or to delete a feature requires substantial effort. This inflexibility increases the costs of building variants of a system, amongst other problems. New languages and tools that are being developed to provide additional support for separating concerns show promise to help address this problem. However applying these mechanisms requires determining how to enable a feature to be separated from the codebase. We investigate this problem through an exploratory study conducted in the context of two existing systems: gnu.regexp and jFTPd. The study consisted of applying three different separation of concern mechanisms: Hyper/J/sup TM/ AspectJ/sup TM/ and a lightweight, lexically-based approach, to separate features in the two packages. We report on the study, providing contributions in two areas. First, we characterize the effect different mechanisms had on the structure of the codebase. Second, we characterize the restructuring process required to perform the separations. These characterizations can help researchers to elucidate how the mechanisms may be best used, tool developers to design support to aid the separation process, and early adopters to apply the techniques.",
Towards a cost model for distributed and replicated data stores,"Large, Petabyte-scale data stores need detailed design considerations about distributing and replicating particular parts of the data store in a cost-effective way. Technical issues need to be analysed and, based on these constraints, an optimisation problem can be formulated. In this paper we provide a novel cost model for building a world-wide distributed Petabyte data store which will be in place starting from 2005 at CERN and its collaborating, world-wide distributed institutes. We elaborate on a framework for assessing potential system costs and influences which are essential for the design of the data store.",
Hierarchical unsupervised learning of facial expression categories,"We consider the problem of unsupervised classification of temporal sequences of facial expressions in video. This problem arises in the design of an adaptive visual agent, which must be capable of identifying appropriate classes of visual events without supervision to effectively complete its tasks. We present a multilevel dynamic Bayesian network that learns the high-level dynamics of facial expressions simultaneously, with models of the expressions themselves. We show how the parameters of the model can be learned in a scalable and efficient way. We present preliminary results using real video data and a class of simulated dynamic event models. The results show that our model correctly classifies the input data comparably to a standard event classification approach, while also learning the high-level model parameters.","Unsupervised learning,
Games,
Bayesian methods,
Computer science,
Discrete event simulation,
Intelligent agent,
Smart cameras,
Face recognition,
Image motion analysis,
Data mining"
Quantitative comparative evaluation of 2D vector field visualization methods,"Presents results from a user study that compared six visualization methods for 2D vector data. Two methods used different distributions of short arrows, two used different distributions of integral curves, one used wedges located to suggest flow lines, and the final one was line-integral convolution (LIC). We defined three simple but representative tasks for users to perform using visualizations from each method: (1) locating all critical points in an image, (2) identifying critical point types, and (3) advecting a particle. The results show different strengths and weaknesses for each method. We found that users performed better with methods that: (1) showed the sign of vectors within the vector field, (2) visually represented integral curves, and (3) visually represented the locations of critical points. These results provide quantitative support for some of the anecdotal evidence concerning visualization methods. The tasks and testing framework also provide a basis for comparing other visualization methods, for creating more effective methods and for defining additional tasks to further understand tradeoffs among methods. They may also be useful for evaluating 2D vectors on 2D surfaces embedded in 3D and for defining analogous tasks for 3D visualization methods.",
Similarity search without tears: the OMNI-family of all-purpose access methods,"Designing a new access method inside a commercial DBMS is cumbersome and expensive. We propose a family of metric access methods that are fast and easy to implement on top of existing access methods, such as sequential scan, R-trees and Slim-trees. The idea is to elect a set of objects as foci, and gauge all other objects with their distances from this set. We show how to define the foci set cardinality, how to choose appropriate foci, and how to perform range and nearest-neighbor queries using them, without false dismissals. The foci increase the pruning of distance calculations during the query processing. Furthermore we index the distances from each object to the foci to reduce even triangular inequality comparisons. Experiments on real and synthetic datasets show that our methods match or outperform existing methods. They are up to 10 times faster, and perform up to 10 times fewer distance calculations and disk accesses. In addition, it scales up well, exhibiting sub-linear performance with growing database size.","Computer science,
Statistics,
Nearest neighbor searches,
Query processing,
Image databases,
Multimedia systems,
Multimedia databases,
Database systems,
Fingerprint recognition,
Proteins"
Measuring similarity of interests for clustering Web-users,"There has been an increased demand for understanding of Web-users due to the Web development and the increased number of Web-based applications. Informative knowledge extracted from Web user access patterns has been used for many applications, such as the prefetching of pages between clients and proxies. This paper presents an approach for measuring similarity of interests among Web users, based on the interest items collected from Web user's access logs. A matrix-based algorithm is then developed to cluster Web users such that the users in the same cluster are closely related with respect to the similarity measure. As an application example, a Web document prefetching technique is proposed that utilises the similarity measure and clusters obtained. Experiments have been conducted and the results have shown that our clustering method is capable of clustering Web users with similar interests, and the prefetching method is practical.","Prefetching,
Navigation,
Computer science,
Application software,
Clustering algorithms,
Clustering methods,
Web page design,
Web pages,
Cities and towns,
Australia"
A video text detection and recognition system,,
"Non-interference, who needs it?",,"Cryptography,
Information security,
Algorithm design and analysis,
Cryptographic protocols,
Educational institutions,
Multilevel systems,
Computer science,
Regulators,
Channel capacity,
Operating systems"
Formal and use-case driven requirement analysis in UML,"We have recently proposed a formalization of the use of UML in requirement analysis. This paper applies that formalization to a library system as a case study. We intend to show how the approach supports a use case-driven, step-wised and incremental development in building models for requirement analysis. The actual process of building the models shows the importance and feasibility of the formalization itself.",
A novel delay-oriented shortest path routing protocol for mobile ad hoc networks,"In wireless ad hoc mobile network, a host which desires to communicate with another host may need some intermediate nodes to relay data packets. To maximize the channel resource utilization and minimize the network transfer delay along the path, the shortest path with minimum hops approach is often adapted. However, by considering employing the medium access control (MAC) protocol, the minimum transfer delay from source to destination may be achieved by choosing a longer path but with less contention delay. We propose an efficient delay-oriented routing protocol for mobile ad hoc wireless networks. The expected access contention delay of the IEEE 802.11 protocol is analyzed to support the routing decision. Simulation results show that the derived path length in the proposed delay-oriented routing protocol is slightly higher than that of the conventional shortest path with minimum hops approach but it can significantly reduce both the average transfer delay and packet loss rate.",
REportal: a Web-based portal site for reverse engineering,"We present a Web-based portal site for the reverse engineering of software systems, called REportal (Reverse Engineering portal). REportal enables authorized users to upload their code to a secure Web site and then, through the guidance of wizards, to browse and analyze their code. Currently, the portal services include code analysis, browsing, querying and design extraction for C, C++ and Java programs. The REportal services are implemented by several reverse engineering tools that our team has developed over the years. With this work, we aim to assist professional software engineers, educators and other researchers who need to analyze code. Specifically, we present a technology that provides a simple and easily accessible user interface to a number of reverse engineering tools. More importantly, this technology saves the user from the time and effort required to install, administer and integrate these tools.",
"Single mask, large force, and large displacement electrostatic linear inchworm motors",We have demonstrated a family of large force and large displacement electrostatic linear inchworm motors that can operate with moderate to high voltages. The inchworm motor design decouples actuator force from total travel and allows the use of electrostatic gap-closing actuators to achieve large force and large displacement while consuming low power. A typical inchworm motor measures 3 mm/spl times/1 mm/spl times/50 /spl mu/m and can lift over 130 times its own weight. One motor has achieved a travel of 80 /spl mu/m and a calculated force of 260 /spl mu/N at 33 V. The force density of that motor was 87 /spl mu/N/mm/sup 2/ at 33 V and the energy efficiency was estimated at 8%. Another motor displaced the shuttle at an average velocity of almost 4 mm/s and achieved an estimated power density of 190 W/m/sup 3/. Motors were cycled 23.6 million times for over 13.5 hours without stiction. This family of motors is fabricated on Silicon-on-Insulator wafers using only a single mask.,"Micromechanical devices,
Electrostatic actuators,
Voltage,
Thermal force,
Force sensors,
Thermal sensors,
Hydraulic actuators,
Computer science,
Thermal engineering,
Couplings"
Modelling faces dynamically across views and over time,"A comprehensive novel multi-view dynamic face model is presented in this paper to address two challenging problems in face recognition and facial analysis: modelling faces with large pose variation and modelling faces dynamically in video sequences. The model consists of a sparse 3D shape model learnt from 2D images, a shape-and-pose-free texture model, and an affine geometrical model. Model fitting is performed by optimising (1) a global fitting criterion on the overall face appearance while it changes across views and over time, (2) a local fitting criterion on a set of landmarks, and (3) a temporal fitting criterion between successive frames in a video sequence. By temporally estimating the model parameters over a sequence input, the identity and geometrical information of a face is extracted separately. The former is crucial to face recognition and facial analysis. The latter is used to aid tracking and aligning faces. We demonstrate the results of successfully applying this model on faces with large variation of pose and expression over time.","Solid modeling,
Face recognition,
Video sequences,
Active appearance model,
Active shape model,
Support vector machines,
Computer science,
Image sequence analysis,
Data mining,
Aging"
Agent-based simulation and greenhouse gas emissions trading,"The need for new theoretical and experimental approaches to understand dynamic and heterogeneous behavior in complex economic and social systems is increasing. An approach using agent-based simulation and artificial markets on a computer system is considered to be an effective approach. Computational simulation with dynamically interacting heterogeneous agents is expected to re-produce complex phenomena in economics, and helps us to experiment various controlling methods, evaluate systematic designs, and extract the fundamental elements which produce interesting phenomena for future analytical work. In previous work, we investigated the stability of a virtual commodities market and the aggregated behavior of dynamic online auctions with heterogeneous agents. In this paper, we introduce a simple framework to develop agent-based simulations systematically and consider an application of agent-based simulation for a dynamical model of international greenhouse gas emissions trading.",
A mobile agent framework for follow-me applications in ubiquitous computing environment,"The paper presents a new mobile agent framework, f-Desktop, which is designed for describing follow-me applications in a ubiquitous computing environment. It provides the migration mechanisms for agent software which are secure against illegal call-up transaction requests from computers which masquerade as the owner of the agent software. Moreover, a follow-me application built with the f-Desktop framework possesses a high adaptability to computing environment and a high connection continuity against changes of location. In the f-Deskrop framework, a user can define his desktop computing environment, desktop or desktop environment in short, as a set of follow-me applications. Then, the user can take the desktop computing environment along to other computers.","Mobile agents,
Ubiquitous computing,
Application software,
Pervasive computing,
Computer displays,
Resumes,
Computer networks,
Costs,
Computer science,
Software architecture"
Transformations for the synthesis and optimization of asynchronous distributed control,"Asynchronous design has been the focus of renewed interest. However, a key bottleneck is the lack of high-quality CAD tools for the synthesis of large-scale systems which also allow design-space exploration. This paper proposes a new synthesis method to address this issue, based on transformations. The method starts with a scheduled and resource-bounded Control-Data Flow Graph (CDFG). Global transformations are first applied to the entire CDFG, unoptimized controllers are then extracted, and, finally, local transforms are applied to the individual controllers. The result is a highly-optimized set of interacting distributed controllers. The new transforms include aggressive timing- and area-oriented optimizations, several of which have not been previously supported by existing asynchronous CAD tools. As a case study the method is applied to the well-known differential equation solver synthesis benchmark. Results comparable to a highly-optimized manual design by Yun et al. (1997) can be obtained by applying the new automated transformations. Such an implementation cannot be obtained using existing asynchronous CAD tools.",
"Better rules, fewer features: a semantic approach to selecting features from text","The choice of features used to represent a domain has a profound effect on the quality of the model produced; yet, few researchers have investigated the relationship between the features used to represent text and the quality of the final model. We explored this relationship for medical texts by comparing association rules based on features with three different semantic levels: (1) words (2) manually assigned keywords and (3) automatically selected medical concepts. Our preliminary findings indicate that bi-directional association rules based on concepts or keywords are more plausible and more useful than those based on word features. The concept and keyword representations also required 90% fewer features than the word representation. This drastic dimensionality reduction suggests that this approach is well suited to large textual corpora of medical text, such as parts of the Web.","Breast cancer,
Association rules,
Bidirectional control,
Computer science,
Predictive models,
Natural languages,
Diseases,
Breast neoplasms,
Medical treatment,
Data mining"
Instruction generation for hybrid reconfigurable systems,"We present an algorithm for simultaneous template generation and matching. The algorithm profiles the graph and iteratively contracts edges to create the templates. The algorithm is general and can be applied to any type of graph, including directed graphs and hypergraphs. We discuss how to target the algorithm towards the novel problem of instruction generation and selection for a hybrid (re)configurable systems. In particular, we target the strategically programmable system, which embeds complex computational units like ALUs, IP blocks, etc. into a configurable fabric. We argue that an essential compilation step for these systems is instruction generation, as it is needed to specify the functionality of the embedded computational units. Additionally, instruction generation can be used to create soft macros tightly sequenced pre-specified operations placed in the configurable fabric.",
Flux maximizing geometric flows,"Several geometric active contour models have been proposed for segmentation in computer vision. The essential idea is to evolve a curve (in 2D) or a surface (in 3D) under constraints from image forces so that it clings to features of interest in an intensity image. Recent variations on this theme take into account properties of enclosed regions and allow for multiple curves or surfaces to be simultaneously represented. However, it is not clear how to apply these techniques to images of low contrast elongated structures, such as those of blood vessels. To address this problem we derive the gradient flow which maximizes the rate of increase of flux of an auxiliary vector field through a curve or surface. The calculation leads to a simple and elegant interpretation which is essentially parameter free. We illustrate its advantages with level-set based segmentations of 2D and 3D MRA images of blood vessels.","Blood vessels,
Biomedical imaging,
Image segmentation,
Active contours,
Computer vision,
Shape,
Computer science,
Machine intelligence,
Blood flow,
Solid modeling"
Dynamic image sequence analysis using fuzzy measures,"In this paper, we present an image understanding system using fuzzy sets and fuzzy measures. This system is based on a symbolic object-oriented image interpretation system. We apply a simple, powerful three-dimensional (3-D) recursive filter to tracking moving objects in a dynamic image sequence. This filter has a time-varying 3-D frequency-planar passband that is adapted in a feedback system to automatically track moving objects. However, as objects in the image sequence are not well-defined and are engaged in dynamic activities, their shapes and trajectories in most cases can be described only vaguely. In order to handle these uncertainties, we use fuzzy measures to capture subtle variations and manage the uncertainties involved. This enables us to develop an image understanding system that produces a very natural output. We demonstrate the effectiveness of our system with complex real traffic scenes.","Image sequence analysis,
Fuzzy sets,
Filters,
Image sequences,
Fuzzy systems,
Time varying systems,
Frequency,
Passband,
Feedback,
Shape"
Centaurus: a framework for intelligent services in a mobile environment,"In an age where wirelessly networked appliances and devices are becoming commonplace, there is a necessity for connecting them to work together for a mobile user. The design outlined in the paper provides an infrastructure and communication protocol for providing 'smart' services to these mobile devices. This flexible framework allows any medium to be used for communication between the system and the portable device, including infra-red, and BlueTooth. Using Extensible Markup Language (XML) for information passing, gives the system a uniform and easily adaptable interface. We explain our trade-offs in implementation and through experiments we show that the design is feasible and that it indeed provides a flexible structure for providing services. Centaurus provides a uniform infrastructure for heterogeneous services, both hardware and software services, to be made available to the users everywhere where they are needed.",
Fast hand gesture recognition for real-time teleconferencing applications,"Work on real-time hand-gesture recognition for SAVI (stereo active vision interface) is presented. Based on the detection of frontal faces, image regions near the face are searched for the existence of skin-tone blobs. Each blob is evaluated to determine if it it is a hand held in a standard pose. A verification algorithm based on the responses of elongated oriented filters is used to decide whether a hand is present or not. Once a hand is detected, gestures are given by varying the number of fingers visible. The hand is segmented using an algorithm which detects connected skin-tone blobs in the region of interest, and a medial axis transform (skeletonization) is applied. Analysis of the resulting skeleton allows detection of the number of fingers visible, thus determining the gesture. The skeletonization is sensitive to strong shadows which may alter the detected morphology of the hand. Experimental results are given indicating good performance of the algorithm.","Teleconferencing,
Face detection,
Computer science,
Fingers,
Hidden Markov models,
Computer vision,
Face recognition,
Hardware,
Cameras,
Image segmentation"
Cooperative caching middleware for cluster-based servers,"Considers the use of cooperative caching to manage the memories of cluster-based servers. Over the last several years, a number of researchers have proposed content-aware servers that implement locality-conscious request distribution to address this memory management problem. During this development, it has become conventional wisdom that cooperative caching cannot match the performance of these servers. Unfortunately, while content-aware servers provide very high performance, their request distribution algorithms are typically bound to specific applications. The advantage of building distributed servers on top of a block-based cooperative caching layer is the generality of such a layer; it can be used as a building block for diverse services, ranging from file systems to web servers. In this paper, we reexamine the question of whether a server built on top of a generic block-based cooperative caching algorithm can perform competitively with content-aware servers. Specifically, we compare the performance of a cooperative caching-based Web server against L2S, a highly optimized locality- and load-conscious server. Our results show that, by modifying the replacement policy of traditional cooperative caching algorithms, we can achieve much of the performance provided by locality-conscious servers. Our modification increases network communication to reduce disk accesses, a reasonable trade-off considering the current trend of relative performance between LANs and disks.","Cooperative caching,
Middleware,
Memory management,
Web server,
Network servers,
Clustering algorithms,
File servers,
Computer science,
Content management,
File systems"
Extreme programming for software engineering education?,"The eXtreme Programming (XP) software development methodology, has received considerable attention in recent years. The adherents of XP anecdotally extol its benefits, particularly as a method that is highly responsive to changing customer's desires. While XP has acquired numerous vocal advocates, the interactions and dependencies between XP practices have not been adequately studied. Good software engineering practice requires expertise in a complex set of activities that involve the intellectual skills of planning, designing, evaluating, and revising. The authors explore the practices of XP in the context of software engineering education. To do so, one must examine the practices of XP as they influence the acquisition of software engineering skills. The practices of XP, in combination or isolation, may provide critical features to aid or hinder the development of increasingly capable practitioners. This paper evaluates the practices of XP in the context of acquiring these necessary software engineering skills.","Programming profession,
Software engineering,
Educational programs,
Computer science,
Testing,
Assembly systems,
Job shop scheduling,
Functional programming,
Code standards,
Stress"
Local control for mesh morphing,"Mesh morphing techniques are capable of producing a sequence of meshes, gradually changing from a source to a target shape. However, current techniques do not allow to describe the local behavior of the morph. A solution to this problem is presented. The main idea is to describe mesh geometry in a differential way, thus, insertion of local features from one shape into another does not suffer from difference in absolute coordinates. Besides interesting possibilities for animation the technique proves to be a powerful modeling tool.","Shape,
Interpolation,
Nose,
Ear,
Merging,
Computer science,
Animation,
Layout,
Topology,
Fuses"
Validation of ultrasonic image boundary recognition in abdominal aortic aneurysm,"An aneurysm of the abdominal aorta (AAA) is characterized by modified wall properties, and a balloon-like area usually filled by a thrombus. A rupture of an aortic aneurysm can be fatal, yet there is no way to accurately predict such an occurrence. The study of the wall and thrombus cross-sectional distension, due to a pressure wave, is important as a way of assessing the degradation of the mechanical properties of the vessel wall and the risk of a rupture. Echo ultrasound transverse cross-sectional imaging is used here to study the thrombus and the aortic wall distension, requiring their segmentation within the image. Polar coordinates are defined, and a search is performed for minimizing a cost function, which includes a description of the boundary (based on a limited series of sine and cosine functions) and information from the image intensity gradients along the radii. The method is based on filtering by a modified Canny-Deriche edge detector and then on minimization of an energy function based on five parts. Since echoes from blood in the lumen and the thrombus produce similar patterns and speckle noise, a modified version for identifying the lumen-thrombus border was developed. The method has been validated by various ways, including parameter sensitivity testing and comparison to the performance of an expert. It is robust enough to track the lumen and total arterial cross-sectional area changes during the cardiac cycle. In 34 patients where sequences of images were acquired, the border between the thrombus and the arterial wall was detected with errors less than 2%, while the lumen-thrombus border was detected with a mean error of 4%. Thus, a noninvasive measurement of the AAA cross-sectional area is presented, which has been validated and found to be accurate.",
Multi-view software evolution: a UML-based framework for evolving object-oriented software,"It is well-known that uncontrolled change to software can lead to increasing evolution costs caused by deteriorating structure and compromised system qualities. For complex systems, the need to carefully manage system evolution is critical. In this paper we outline an approach to managing evolution of object-oriented (OO) software. The approach is based on a goal-directed, cyclic process, in which OO models are transformed and quantitatively evaluated in each cycle. Evaluation criteria guide developers in choosing between alternative transformations in each cycle. The process, transformations, and evaluation techniques can be used to develop systems from a set of baseline models.","Object oriented modeling,
Software systems,
Software quality,
Costs,
Unified modeling language,
Computer science,
Software design,
Computer architecture,
Documentation,
Communication industry"
Distributed multi-robot task allocation for emergency handling,"We describe a prototype task, emergency handling, for multi-robot coordination. The experiments reported measure the effects of individualism and opportunism in a physically-implemented multi-robot system. We use sound at multiple frequencies to simulate emergencies by producing several locally-sensable gradients in the environment. Our results show that opportunism affords a significant performance improvement over individualism. Our experiments also demonstrate the viability of sound for producing detectable local gradients in the environment.","Robot kinematics,
Prototypes,
Dynamic scheduling,
Laboratories,
Computer science,
Frequency,
Computational modeling,
Indoor environments,
Hazardous materials,
Production"
Geometrical fundamentals of polycentric panoramas,"This paper proposes polycentric panoramas as a general model of panoramic images. The model formalizes essential characteristics of panoramic geometry. It is able to describe a wide range of panoramic images, including those potentially of future interest, or previously introduced such as single-center, multi-perspective, or concentric panoramas. This paper presents geometrical fundamentals towards stereo applications based on sets of polycentric panoramas. We discuss the image acquisition model, epipolar geometry and a 3D reconstruction approach for this general model of polycentric panoramas. Our theorems on epipolar curve and 3D reconstruction hold for any pair of polycentric panoramas. Corollaries demonstrate that the proposed mathematical model clarifies the understanding and characterization of more specific models. Epipolar curves of special cases are illustrated on panoramic images acquired by a high resolution line-camera.","Geometry,
Layout,
Computer science,
Computational Intelligence Society,
Visualization,
Navigation,
Image reconstruction,
Cameras,
Stability,
Solid modeling"
"A ""dual-tree"" scheme for fault-tolerant multicast","To protect against possible network node or link failure and achieve high reliability of communications, pre-planned failure recovery schemes are needed in modern high-speed communication networks. A couple of schemes have been previously reported for multicast communications. We present a scheme based on a ""dual-tree"" structure in which a secondary tree for fault-tolerance purpose is built as a complement to a primary multicast tree. The secondary tree provides alternative delivery paths that can be activated when link or node failure is detected in the primary multicast tree. Simulation experiments show that this scheme has shorter restoration time and cause less multicast tree cost increase after restoration than some schemes proposed previously.","Fault tolerance,
Protection,
Multicast protocols,
Unicast,
Quality of service,
Costs,
Routing protocols,
NASA,
Computer science,
Computer network reliability"
Toward accurate models of achievable routing,"Models of achievable routing, i.e., chip wireability, rely on estimates of available and required routing resources. Required routing resources are estimated from placement or (a priori) using wire length estimation models. Available routing resources are estimated by calculating a nominal ""supply"" then take into account such factors as the efficiency of the router and the impact of vias. Models of achievable routing can be used to optimize interconnect process parameters for future designs or to supply objectives that guide layout tools to promising solutions. Such models must be accurate in order to be useful and must support empirical verification and calibration by actual routing results. In this paper, we discuss the validation of such models and we apply our validation process to three existing models. We find notable inaccuracies in the existing models when matched against real data. We then present a thorough analysis of the assumptions underlying these models. Based on this analysis, we discuss requirements for predictors of routing resources and make suggestions for a new model of achievable routing.","Routing,
Wire,
Computer science,
Predictive models,
Wiring,
Design optimization,
Calibration,
Very large scale integration,
Silicon,
Information systems"
The MD-join: an operator for complex OLAP,"OLAP queries (i.e. group-by or cube-by queries with aggregation) have proven to be valuable for data analysis and exploration. Many decision support applications need very complex OLAP queries, requiring a fine degree of control over both the group definition and the aggregates that are computed. For example, suppose that the user has access to a data cube whose measure attribute is Sum(Sales). Then the user might wish to compute the sum of sales in New York and the sum of sales in California for those data cube entries in which Sum(Sales)>$1,000,000. This type of complex OLAP query is often difficult to express and difficult to optimize using standard relational operators (including standard aggregation operators). In this paper, we propose the MD-join operator for complex OLAP queries. The MD-join provides a clean separation between group definition and aggregate computation, allowing great flexibility in the expression of OLAP queries. In addition, the MD-join has a simple and easily optimizable implementation, while the equivalent relational algebra expression is often complex and difficult to optimize. We present several algebraic transformations that allow relational algebra queries that include MD-joins to be optimized.","Aggregates,
Data analysis,
Databases,
Marketing and sales,
Algebra,
Decision support systems,
Computer science,
Application software,
Data warehouses,
Business"
A three-dimensional human agent metaphor for modeling and simulation,"The use of metaphor can be a potential aid to the novice modeler in several ways. Metaphor can imbue abstract ideas with concrete properties, thereby making the abstract ideas more accessible. The analogies suggested by metaphor might also aid reasoning about modeling and implementation problems. Another potential benefit of metaphor in modeling is the improvement of mental retention of model architecture and functionality. Traditionally, models and programs have been produced in a two dimensional (2D) or textual medium. However these media may be inferior to a three-dimensional (3D) medium in the development and use of metaphor, as the concrete properties that metaphors often provide are real-world phenomena, which are naturally 3D. We developed an example of the use of metaphors in modeling and 3D simulation. The example consists of a simplified operating system task scheduler along with associated hardware devices, developed in a VRML environment using VRML PROTO nodes. These nodes are designed as modular objects based on real-world metaphors. We were able to construct a set of metaphors and prototypes that may, if extended ease the modeling and design of agent oriented systems for novices. A proposed extension of one metaphor presented in the research is the synthetic human agent.","Humans,
Object oriented modeling,
Object oriented programming,
Concrete,
Computer science,
Two dimensional displays,
Operating systems,
Prototypes,
Functional programming,
Mathematical programming"
The need for small learning rates on large problems,"In gradient descent learning algorithms such as error backpropagation, the learning rate parameter can have a significant effect on generalization accuracy. In particular, decreasing the learning rate below that which yields the fastest convergence can significantly improve generalization accuracy, especially on large, complex problems. The learning rate also directly affects training speed, but not necessarily in the way that many people expect. Many neural network practitioners currently attempt to use the largest learning rate that still allows for convergence, in order to improve training speed. However, a learning rate that is too large can be as slow as a learning rate that is too small, and a learning rate that is too large or too small can require orders of magnitude more training time than one that is in an appropriate range. The paper illustrates how the learning rate affects training speed and generalization accuracy, and thus gives guidelines on how to efficiently select a learning rate that maximizes generalization accuracy.","Convergence,
Neural networks,
Computer science,
Nominations and elections,
Computer errors,
Guidelines,
Approximation algorithms"
Heuristic vision-based computation of planar antipodal grasps on unknown objects,"A key issue in robotics is the development of the ability to grasp unknown objects. This ability requires a grasp determination mechanism that, based on the analysis of the description of the object, determines how it can be stably grasped. In this paper, a grasp determination method is presented that computes a set of grasps that comply with the force-closure condition. Its input is a set of contours, extracted from the vision data, describing the shape of the object. The internal holes of the object are taken into account, so the algorithm can find grasps on them. The algorithm also finds expansion and squeezing grasps, which are executed by opening and closing the gripper fingers.","Computer vision,
Grippers,
Fingers,
Robots,
Cameras,
Friction,
Computer science,
Data mining,
Shape,
Robustness"
On workflow enabled e-learning services,Workflow technology provides a suitable platform to define and manage the coordination and allocation of business process activities. We introduce the Flex-eL (Flexible electronic Learning) environment that has been built upon workflow technology. The workflow functionality of Flex-eL manages the coordination of learning and assessment activities of the course process between students and teaching staff. It provides a unique environment for teachers to design and develop process-centric courses and to monitor student progress. It allows students to learn at their own pace while observing the learning guidelines and checkpoints modelled into the course process by teaching staff. We also report on the successful deployment of the concept and system for a university course and our experiences from the implementation.,
Locality-aware predictive scheduling of network processors,,
Blind separation of second-order nonstationary and temporally colored sources,"This paper presents a method of blind source separation that jointly exploits the nonstationarity and temporal structure of sources. The method needs only multiple time-delayed correlation matrices of the observation data, each of which is evaluated at a different time-windowed data frame, to estimate the demixing matrix. We show that the method is quite robust with respect to the spatially correlated but temporally white noise. We also discuss the extension of some existing second-order blind source separation methods. Extensive numerical experiments confirm the validity of the proposed method.",
Unsupervised training of acoustic models for large vocabulary continuous speech recognition,"For speech recognition systems, the amount of acoustic training data is of crucial importance. In the past, large amounts of speech were recorded and transcribed manually for training. Since untranscribed speech is available in various forms these days, the unsupervised training of a speech recognizer on recognized transcriptions is studied. A low-cost recognizer trained with only one hour of manually transcribed speech is used to recognize 72 hours of untranscribed acoustic data. These transcriptions are then used in combination with confidence measures to train an improved recognizer. The effect of confidence measures which are used to detect possible recognition errors is studied systematically. Finally, the unsupervised training is applied iteratively. Using this method, the recognizer is trained with very little manual effort while losing only 14.3% relative on the Broadcast News '96 and 18.6% relative on the Broadcast News '98 evaluation test sets.","Vocabulary,
Speech recognition,
Broadcasting,
Error analysis,
Linear discriminant analysis,
Training data,
Computer science,
Acoustic signal detection,
Testing,
Personnel"
H/sub 2/ near-optimal model reduction,"This note considers the problem of finding a stable reduced-order model for a given stable model so that its H/sub 2/ model reduction cost differs by less than a prescribed error from the optimal cost, which may or may not be achievable. It is shown that this new version of the long-standing H/sub 2/ optimal model reduction problem can be reduced to a well-posed smooth constrained minimization problem whose global solution is guaranteed to exist. In addition, a globally convergent algorithm in the form of an ordinary differential equation is derived.",
Normalization by evaluation for typed lambda calculus with coproducts,"Solves the decision problem for the simply typed lambda calculus with a strong binary sum, or, equivalently, the word problem for free Cartesian closed categories with binary co-products. Our method is based on the semantic technique known as ""normalization by evaluation"", and involves inverting the interpretation of the syntax in a suitable sheaf model and, from this, extracting an appropriate unique normal form. There is no rewriting theory involved and the proof is completely constructive, allowing program extraction from the proof.","Calculus,
Equations,
Logic,
Informatics,
Computer languages,
Terminology"
Extension of finite-support extrapolation using the generalized series model for MR spectroscopic imaging,"In magnetic resonance (MR) imaging, limited data sampling in /spl kappa/-space leads to the well-known Fourier truncation artifact, which includes ringing and blurring. This problem is particularly severe for MR spectroscopic imaging, where only 16-24 points are typically acquired along each spatial dimension. Several methods have been proposed to overcome this problem by incorporating prior information in the image reconstruction. These include the generalized series (GS) model and the finite-support extrapolation method. This paper shows the connection between finite-support extrapolation and the GS model. In particular, finite-support extrapolation is a limiting case of the GS model, when the only available prior information is the support region. The support region refers to those image portions with nonzero intensities, and it can be estimated in practice as the nonbackground region of an image. By itself, the support region constitutes a rather weak constraint that may not lead to considerable resolution gain. This situation can be improved by using additional prior information, which can be incorporated systematically with the GS model. Examples of such additional prior information include intensity estimates of anatomical structures inside the support region.","Extrapolation,
Spectroscopy,
Image reconstruction,
Magnetic resonance imaging,
Magnetic resonance,
Sampling methods,
Anatomical structure,
Fourier transforms,
Data acquisition,
Signal to noise ratio"
An efficiency and scalability model for heterogeneous clusters,,"Scalability,
Clustering algorithms,
Performance analysis,
Size measurement,
Delay,
Sun,
Concurrent computing"
Improving degradation and fairness for mobile adaptive multimedia wireless networks,"In this paper, we propose proportional degradation services for multiple classes of adaptive multimedia services in wireless/mobile networks. First, we introduce two novel quality of service (QoS) parameters for the bandwidth degradation: the degradation ratio (DR) and the degradation degree (DD). Then, we propose two measurement-based call admission control (CAC) schemes to satisfy QoS requirements, and to utilize the resource efficiently. They are the window averaging (WA) scheme and the leaky-bucket integration (LBI) scheme. Finally, a K-level proportional bandwidth adaptation algorithm is also proposed to fairly adapt calls' bandwidth, to minimize DR and DD, to minimize DD with higher priority than to minimize DR, and to guarantee the ratios of service degradation among classes.","Degradation,
Adaptive systems,
Wireless networks,
Bandwidth,
Quality of service,
Mobile computing,
Computer science,
Call admission control,
Fluctuations,
Frequency"
Fair bandwidth allocation for multi-class of adaptive multimedia services in wireless/mobile networks,"Adaptive multimedia services are very attractive in wireless/mobile networks since they can mitigate the fluctuation of resources. We consider some quality of service (QoS) aspects for multiple classes of adaptive multimedia services in wireless/mobile networks. They are QoS parameters for the bandwidth degradation, a call admission control scheme and a bandwidth allocation algorithm to satisfy QoS, and to utilize resources efficiently. Moreover, they are for multiple classes of users with fairness among classes and fairness within one class. The fairness among classes is achieved by partitioning the bandwidth resource according to arrival rates. The fairness within one class is achieved by fairly distributing the degree of bandwidth degradation among users. Simulation results show that the proposed scheme achieves good fairness for bandwidth and degree of degradation.","Channel allocation,
Intelligent networks,
Bandwidth,
Streaming media,
Quality of service,
Degradation,
Fluctuations,
Call admission control,
Fading,
Computer science"
The role of trust and deception in virtual societies,"The authors argue that it is important to analyse the role of trust and deception in interactions between agents in virtual societies. In particular, in hybrid situations where artificial agents interact with human agents it is important that those artificial agents can reason about the trustworthiness and deceptive actions of the human counterpart. In order to support this interaction between agents in virtual societies, a theory on trust and deception must be developed. In the literature, a wide variety of theories on trust (less so on deception!) have been developed but not specifically for virtual communities. Based on these earlier scientific results, we make a first attempt to develop a general theory on trust and deception for virtual communities, and we discuss a number of examples to illustrate which objectives such a theory should fulfil.","Electronic commerce,
Artificial intelligence,
Humans,
Machine intelligence,
Face detection,
Information systems,
Information analysis,
Counting circuits,
Intelligent agent,
Computer science"
Quality-assuring scheduling-using stochastic behavior to improve resource utilization,"We present a unified model for admission and scheduling, applicable for various active resources such as CPU or disk to assure a requested quality in situations of temporary overload. The model allows us to predict and control the behavior of applications based on given quality requirements. It uses the variations in the execution time, i.e., the time any active resource is needed We split resource requirements into a mandatory part which must be available and an optional part which should be available as often as possible but at least with a certain percentage. In combination with a given distribution for the execution time we can move away from worst-case reservations and drastically reduce the amount of reserved resources for applications which can tolerate occasional deadline misses. This increases the number of admittable applications. For example, with negligible loss of quality our system can admit more than two times the disk bandwidth than a system based on the worst-case. Finally, we validated the predictions of our model by measurements using a prototype real-time system and observed a high accuracy between predicted and measured values.","Stochastic processes,
Resource management,
Processor scheduling,
Real time systems,
Predictive models,
Computer science,
Electronic mail,
Application software,
Prototypes,
Accuracy"
Studying protein folding on the grid: experiences using CHARMM on NPACI resources under Legion,"One benefit of a computational grid is the ability to run high-performance applications over distributed resources simply and securely. We demonstrate this benefit with an experiment in which we studied the protein folding process with the CHARMM molecular simulation package over a grid managed by Legion, a grid operating system. High-performance applications can take advantage of grid resources if the grid operating system provides both low level functionality as well as high-level services. We describe the nature of services provided by Legion for high-performance applications. Our experiences indicate that human factors continue to play a crucial role in the configuration of grid resources, underlying resources can be problematic, grid services must tolerate underlying problems or inform the user, and high-level services must continue to evolve to meet user requirements. Our experiment not only helped a scientist perform an important study, but also showed the viability of an integrated approach such as Legion's for managing a grid.","Proteins,
Grid computing,
Operating systems,
Computer science,
Distributed computing,
Biology computing,
Computer networks,
Computational biology,
Application software,
Biological system modeling"
A profile-based energy-efficient intra-task voltage scheduling algorithm for hard real-time applications,"Intra-task voltage scheduling (IntraVS), which adjusts the supply voltage within an individual task boundary, is an effective technique for developing low-power applications. In this paper, we propose a novel intra-task voltage scheduling algorithm for hard real-time applications based on average-case execution information. Unlike the original IntraVS algorithm where voltage scaling decisions are based on the worst-case execution cycles, the proposed algorithm improves the energy efficiency by controlling the execution speed based on average-case execution cycles while still meeting the real-time constraints. The experimental results using an MPEG-4 decoder program show that the proposed algorithm reduces the energy consumption by up to 34% over the original IntraVS algorithm.","Energy efficiency,
Scheduling algorithm,
Dynamic voltage scaling,
Voltage control,
Clocks,
Application software,
Processor scheduling,
Energy consumption,
Computer science,
Circuits"
Performance-driven multi-level clustering with application to hierarchical FPGA mapping,"In this paper, we study the problem of performance-driven multi-level circuit clustering with application to hierarchical FPGA designs. We first show that the performance-driven multi-level clustering problem is NP-hard (in contrast to the fact that single-level performance-driven clustering can be solved in polynomial time optimally). Then, we present an efficient heuristic for two-level clustering for delay minimization. It can also provide area-delay trade-off by controlling the amount of node duplication. The algorithm is applied to Altera's latest APEX FPGA architecture which has a two-level hierarchy. Experimental results with combinational circuits show that with our performance-driven two-level clustering solution we can improve the circuit performance produced by the Quartus Design System from Altera by an average of 15% for APEX devices measured in terms of delay after final layout. To our knowledge this is the first in-depth study for the performance-driven multi-level circuit clustering problem.","Field programmable gate arrays,
Delay,
Integrated circuit interconnections,
Clustering algorithms,
Minimization,
Application software,
Logic arrays,
Computer science,
Polynomials,
Permission"
Hierarchical dummy fill for process uniformity,"To improve manufacturability and performance predictability, we seek to make a layout uniform with respect to prescribed density criteria, by inserting ""fill"" geometries into the layout. Previous approaches for flat layout density control are not scalable due to the necessity of solving very large linear programs, the large data volume of the solution, and the impact of hierarchy-breaking on verification. In this paper, we give the first methods for hierarchical layout density control for process uniformity. Our approach trades off naturally between runtime, solution quality, and output data volume. We also allow generation of compressed GDSII of fill geometries. Our experiments show that this hybrid hierarchical filling approach saves data volume and is scalable, while yielding solution quality that is competitive with existing Monte-Carlo and linear programming based approaches.","Etching,
Computer science,
Filling,
Chemical vapor deposition,
Copper,
Capacitance,
Proximity effect,
Computer aided manufacturing,
Computational geometry,
Process control"
Method of generating coded description of human body motion from motion-captured data,"By using a motion capture system we can measure and record human body motion of intangible cultural properties like ballet dance or other performing arts. Since the measurement accuracy of the system is high, volume of the data is usually very large. Compact and abstracted description of the motion is also required. We developed a method to generate the coded description from motion captured data. The description is based on dance notation, called Labanotation, which is well known among Western dance communities. Labanotation score is produced from the coded description, and it can be readily used in choreography and dance education. This compact coded description of the motion may also be used as an index for motion captured data stored in the database.","Humans,
Art,
Motion measurement,
Anthropometry,
Cultural differences,
Computer science,
Performance evaluation,
Volume measurement,
Educational products,
Databases"
DRES: network resource management using deferred reservations,"We consider the problem of resource reservation for networks. Current approaches for resource reservation in integrated service networks adopt an all-or-nothing approach, where partially acquired resources must be released if resources are not available at all routers on the chosen path. Furthermore, under high load, end-systems must retry requests repeatedly leading to inefficient allocation and increased traffic. We propose a new approach called Deferred REServation (DRES) that substantially improves performance (reduces the overall flow rejection probability and increases link utilization) over the all-or-nothing reservation approach. Flow admissibility is increased by deferring requests at routers for a limited period of time until resources are available. Analytical and simulation results confirm the performance benefits of our approach.","Resource management,
Analytical models,
Streaming media,
Internet,
Signal processing,
Computer science,
Intserv networks,
Telecommunication traffic,
Performance analysis,
Teleconferencing"
"On projection matrices P/sup k//spl rarr/P/sup 2/, k=3,...,6, and their applications in computer vision","Projection matrices from projective spaces P/sup 3/ to P/sup 2/ have long been used in multiple-view geometry to model the perspective projection created by the pin-hole camera. In this work we introduce higher-dimensional mappings P/sup k//spl rarr/P/sup 2/, k=4,5,6 for the representation of various applications in which the world we view is no longer rigid. We also describe the multi-view constraints from these new projection matrices and methods for extracting the (non-rigid) structure and motion for each application.","Cameras,
Tensile stress,
Layout,
Application software,
Transmission line matrix methods,
Computer vision,
Integrated circuit modeling,
Computer science,
Electronic mail,
Computational geometry"
Fault-based testing in the absence of an oracle,"Although testing is the most popular method for assuring software quality, there are two recognized limitations, known as the reliable test set problem and the oracle problem. Fault-based testing is an attempt by Morell to alleviate the reliable test set problem. In this paper, we propose to enhance fault-based testing to address the oracle problem as well. We present an integrated method that combines metamorphic testing with fault-based testing using real and symbolic inputs.","Software testing,
Genetic mutations,
Fault detection,
Software quality,
Councils,
Computer science,
Information systems,
Error correction,
Power generation"
Fitting nature's basic functions. I. Polynomials and linear least squares,"The problem of fitting a mathematical model which depends on an n-vector of unknown parameters, to a measured data set is ubiquitous in science and engineering. This paper is the first installment of a series that will demonstrate modern techniques for fitting combinations of basic mathematical functions to measured real-world data. Fitting a straight line, linear least squares and the best linear unbiased estimate are discussed.","Polynomials,
Least squares methods,
Temperature,
Predictive models,
Data engineering,
Measurement errors,
Gaussian processes,
Prototypes,
Equations,
Least squares approximation"
Creating ensembles of classifiers,"Ensembles of classifiers offer promise in increasing overall classification accuracy. The availability of extremely large datasets has opened avenues for application of distributed and/or parallel learning to efficiently learn models of them. In this paper, distributed learning is done by training classifiers on disjoint subsets of the data. We examine a random partitioning method to create disjoint subsets and propose a more intelligent way of partitioning into disjoint subsets using clustering. It was observed that the intelligent method of partitioning generally performs better than random partitioning for our datasets. In both methods a significant gain in accuracy may be obtained by applying bagging to each of the disjoint subsets, creating multiple diverse classifiers. The significance of our finding is that a partition strategy for even small/moderate sized datasets when combined with bagging can yield better performance than applying a single learner using the entire dataset.",
Coupling of design patterns: common practices and their benefits,"Object-oriented (OO) design patterns define collections of interconnected classes that serve a particular purpose. A design pattern is a structural unit in a system built out of patterns, not unlike the way a function is a structural unit in a procedural program or a class is a structural unit in an OO system designed without patterns. When designers treat patterns as structural units, they become concerned with issues such as coupling and cohesion at a new level of abstraction. We examine the notion of pattern coupling to classify how designs may include coupled patterns. We find many examples of coupled patterns; this coupling may be ""tight"" or ""loose"", and provides both benefits and costs. We qualitatively assess the goodness of pattern coupling in terms of effects on maintainability, factorability, and reusability when patterns are coupled in various ways.","Production facilities,
Computer science,
Unified modeling language,
Object oriented modeling,
Books,
Software quality,
Software maintenance,
Data analysis,
Pattern analysis,
Application software"
Middleware for mobile computing: awareness vs. transparency,"Summary form only given. Middleware solutions for wired distributed systems cannot be used in a mobile setting, as mobile applications impose new requirements that run counter to the principle of transparency on which current middleware systems have been built. We propose the use of reflection capabilities and meta-data to pave the way for a new generation of middleware platforms designed to support mobility.","Middleware,
Mobile computing,
Application software,
Batteries,
Context awareness,
Reflection,
XML,
Software engineering,
Computer science,
Educational institutions"
An architecture for location dependent query processing,"As the mobility of people increases, anytime, anywhere computing has become a must. With the advances in wireless communications, the geographical position of the mobile user can be estimated accurately enough to be used to access data dependent on it. Location awareness then enables new kinds of services specific to localized information. While this provides great flexibility for the end user, it creates many challenges for the content providers and wireless operators. We propose an architecture, location dependent services manager (LDSM), between the mobile user and the service/content providers to service location dependent applications which access location dependent data. The proposed middleware does location translation which we call location leveling, to adjust the location granularities and to solve the location mismatch problem. Quality of query results and the complicated queries involving more than one service provider is also supported in this middleware. We give an overview of the proposed architecture, examine the query processing issues and discuss the future work plans.","Query processing,
Application software,
Computer architecture,
Computer science,
Cities and towns,
Databases,
Telecommunication computing,
Mobile radio mobility management,
Content management,
Middleware"
Rademacher penalization applied to fuzzy ARTMAP and boosted ARTMAP,"We deal with the performance bounding of fuzzy ARTMAP and other ART-based neural network architectures, such as boosted ARTMAP, according to the theory of structural risk minimization. Structural risk minimization research indicates a trade-off between training error and hypothesis complexity. This trade-off directly motivated boosted ARTMAP. In this paper, we present empirical evidence for boosted ARTMAP as a viable learning technique, in general, in comparison to fuzzy ARTMAP and other ART-based neural network architectures. We also show direct empirical evidence for decreased hypothesis complexity in conjunction with the improved empirical performance for boosted ARTMAP as compared with fuzzy ARTMAP. Application of the Rademacher penalty to boosted ARTMAP on a specific learning problem further indicates its utility as compared with fuzzy ARTMAP.","Risk management,
Fuzzy neural networks,
Neural networks,
Training data,
Machine learning algorithms,
Fuzzy sets,
Computer science,
Fuzzy logic,
Labeling,
Supervised learning"
Lost and found in software space,"Two problems often occur when implementing large software projects in a group: the problem of orientation and the problem of finding competent partners for tight cooperation. The paper presents a spatial representation of the shared source code under development called software space. It shows how awareness about another person's activities can be provided, which helps to find appropriate partners. Software space dynamically adapts to the user's preferences and thus improves during usage. The paper illustrates how the adapted representation can assist users in finding their way in software space.","Collaborative software,
Navigation,
Collaborative work,
Programming profession,
Space technology,
Information technology,
Information systems,
Programming environments,
HTML,
Usability"
A modular reduction of regular logic to classical logic,"In this paper we first define a reduction /spl delta/ that transforms an instance /spl Gamma/ of Regular-SAT into a satisfiability equivalent instance /spl Gamma//sup /spl delta// of SAT. The reduction /spl delta/ has interesting properties: (i) the size of /spl Gamma//sup /spl delta// is linear in the size of /spl Gamma/, (ii) /spl delta/ transforms regular Horn formulas into Horn formulas, and (iii) /spl delta/ transforms regular 2-CNF formulas into 2-CNF formulas. Second, we describe a new satisfiability algorithm that determines the satisfiability of a regular 2-CNF formula /spl Gamma/ in time O(|/spl Gamma/|log|/spl Gamma/|); this algorithm is inspired by the reduction /spl delta/. Third, we introduce the concept of renamable-Horn regular CNF formula and define another reduction /spl delta/' that transforms a renamable-Horn instance /spl Gamma/ of Regular-SAT into a renamable-Horn instance /spl Gamma//sup /spl delta/'/ of SAT. We use this reduction to show that both membership and satisfiability of renamable-Horn regular CNF formulas can be decided in time O(|/spl Gamma/|log|/spl Gamma/|).",
A multistage perceptual quality assessment for compressed digital angiogram images,"This paper describes a multistage perceptual quality assessment (MPQA) model for compressed images. The motivation for the development of a perceptual quality assessment is to measure (in)visible differences between original and processed images. The MPQA produces visible distortion maps and quantitative error measures informed by considerations of the human visual system (HVS). Original and decompressed images are decomposed into different spatial frequency bands and orientations modeling the human cortex. Contrast errors are calculated for each frequency and orientation, and masked as a function of contrast sensitivity and background uncertainty. Spatially masked contrast error measurements are then made across frequency bands and orientations to produce a single perceptual distortion visibility map (PDVM). A perceptual quality rating (PQR) is calculated from the PDVM and transformed into a one to five scale, PQR/sub 1-5/, for direct comparison with the mean opinion score, generally used in subjective ratings. The proposed MPQA model is based on existing perceptual quality assessment models, while it is differentiated by the inclusion of contrast masking as a function of background uncertainty. A pilot study of clinical experiments on wavelet-compressed digital angiogram has been performed on a sample set of angiogram images to identify diagnostically acceptable reconstruction. Our results show that the PQR/sub 1-5/ of diagnostically acceptable lossy image reconstructions have better agreement with cardiologists' responses than objective error measurement methods, such as peak signal-to-noise ratio. A Perceptual thresholding and CSF-based Uniform quantization (PCU) method is also proposed using the vision models presented in this paper. The vision models are implemented in the thresholding and quantization stages of a compression algorithm and shown to produce improved compression ratio performance with less visible distortion than that of the embedded zerotrees wavelet (EZWs).",
Web site maintenance with software-engineering tools,"Internet information providers need to continually analyse and update their offerings in order to survive in today's marketplace. As Web sites and their information content grow larger, this becomes a more and more difficult task. Software engineers have had to deal with the problem of understanding and maintaining complex and interconnected documents (in the form of program source code) for a long time, and a number of tools exist to support them in this task. The paper describes commonalities in the maintenance of software systems and Web sites and presents a case study in the application of a software reengineering tool for Web site analysis.","Software maintenance,
Internet,
Web pages,
Computer science,
Information analysis,
History,
Visualization,
Statistical analysis,
Unified modeling language,
Application software"
Hierarchical brushing in a collection of video data,"The amount of digital video and associated metadata being generated and stored is increasing rapidly. Given the complex spatial and temporal structure of video information it is a formidable challenge to provide compact and human-readable representations of such content. The solution has to be versatile in order to satisfy different user needs, such as browsing, zooming (looking for something specific), discovering or recording access patterns performing partial searches that can be re-used later, etc. We present a new approach that addresses many of the difficulties. Our representation provides at-a-glance high-level overview of the video collection and also serves as a navigational tool in that collection. While the user navigates to finer-level representations in the video collection our visualization allows him to maintain a sense of location and context within the collection. We introduce the movieDNA as an abstraction to visualize interesting features in a video. Our compact, yet flexible, representation is applicable to any type of linear data. In order to access even larger amounts of data in one view and to express several levels of granularity and compactness we extend the movieDNA to a hierarchical movieDNA. Hierarchical brushing enables browsing, navigating and visualizing several semantic levels of video content. We discuss hierarchical brushing of video content for various user scenarios and present examples demonstrating the versatility of our approach.","Navigation,
Data visualization,
Streaming media,
Nails,
Internet,
Videoconference,
Disk recording,
Video recording,
Computer vision,
Speech recognition"
On the impossibility of basing trapdoor functions on trapdoor predicates,"We prove that, somewhat surprisingly, there is no black-box reduction of (poly-to-one) trapdoor functions to trapdoor predicates (equivalently, to public-key encryption schemes). Our proof follows the methodology that was introduced by R. Impagliazzo and S. Rudich (1989), although we use a new, weaker model of separation.","Public key cryptography,
Polynomials,
Public key,
Computer science"
"Intensionality, extensionality, and proof irrelevance in modal type theory","We develop a uniform type theory that integrates intensionality, extensionality and proof irrelevance as judgmental concepts. Any object may be treated intensionally (subject only to /spl alpha/-conversion), extensionally (subject also to /spl beta//spl eta/-conversion), or as irrelevant (equal to any other object at the same type), depending on where it occurs. Modal restrictions developed by R. Harper et al. (2000) for single types are generalized and employed to guarantee consistency between these views of objects. Potential applications are in logical frameworks, functional programming and the foundations of first-order modal logics. Our type theory contrasts with previous approaches that, a priori, distinguished propositions (whose proofs are all identified - only their existence is important) from specifications (whose implementations are subject to some definitional equalities).","Computer science,
Functional programming,
Logic programming,
Heart"
Video-based online face recognition using identity surfaces,"A multi-view dynamic face model is designed to extract the shape-and-pose-free texture patterns effaces. The model provides a precise correspondence to the task of recognition since the 3D shape information is used to warp the multi-view faces onto the model mean shape in frontal-view. The identity surface of each subject is constructed in a discriminant feature space from a sparse set of face texture patterns, or more practically, from one or more learning sequences containing the face of the subject. Instead of matching templates or estimating multi-modal density functions, face recognition can be performed by computing the pattern distances to the identity surfaces or trajectory distances between the object and model trajectories. Experimental results depict that this approach provides an accurate recognition rate while using trajectory distances achieves a more robust performance since the trajectories encode the spatio-temporal information and contain accumulated evidence about the moving faces in a video input.","Face recognition,
Face detection,
Facial animation,
Active shape model,
Principal component analysis,
Active appearance model,
Image recognition,
Pattern matching,
Support vector machines,
Computer science"
Frequency assignment with complex co-site constraints,"The adaption of meta-heuristic algorithms such as tabu search and simulated annealing to large frequency assignment problems with complex co-site constraints is described. The constraints considered include frequency separation constraints, intermodulation product constraints and spurious emission and response constraints. Requests for frequencies can also be prioritised. The importance of fast evaluation of the quality of the current assignment by updating techniques is stressed. Additionally, a useful lower bound for assessing the quality of assignments is described.",Radio spectrum management
Using multirail networks in high-performance clusters,,"Intelligent networks,
Bandwidth,
Computer networks,
Laboratories,
Delay,
Switches,
Libraries,
Communication switching,
Supercomputers,
Rails"
Design of a predictive filter cache for energy savings in high performance processor architectures,"Filter cache has been proposed as an energy saving architectural feature. A filter cache is placed between the CPU and the instruction cache (I-cache) to provide the instruction stream. Energy savings result from accesses to a small cache. There is however loss of performance when instructions are not found in the filter cache. The majority of the energy savings from the filter cache are due to the temporal reuse of instructions in small loops. We examine subsequent fetch addresses to predict whether the next fetch address is in the filter cache dynamically. In case a miss is predicted, we reduce miss penalty by accessing the I-cache directly. Experimental results show that our next fetch prediction reduces performance penalty by more than 91% and is more energy efficient than a conventional filter cache. Average I-cache energy savings of 31 % can be achieved by our filter cache design with around 1 % performance degradation.","Degradation,
Computer architecture,
Frequency estimation,
Digital signal processing,
Information filtering,
Information filters,
Computer science,
Electronic mail,
Performance loss,
Parallel processing"
Process-Oriented Metrics for Software Architecture Adaptability,"Proposes the POMSAA (Process-Oriented Metrics for Software Architecture Adaptability) framework, which aims to provide numeric scores representing the adaptability of a software architecture, as well as the intuitions behind these scores. In this framework, the intuitions behind the architectural adaptability scores are traced back to the ""whys"" of the architecture, namely the requirements for which the architecture exists in the first place. POMSAA achieves the needed tracing by adopting the NFR (Non-Functional Requirements) framework, which is a process-oriented qualitative framework for representing and reasoning about non-functional requirements. We show how to use POMSAA to: (1) calculate and re-calculate the metrics; (2) detect weaknesses and strategic strengths; (3) understand the reasons for those weaknesses and strengths; and (4) make the needed changes to the architecture to increase its adaptability.",
Software engineering by source transformation - experience with TXL,"Many tasks in software engineering can be characterized as source to source transformations. Design recovery, software restructuring, forward engineering, language translation, platform migration and code reuse can all be understood as transformations from one source text to another. TXL, the Tree Transformation Language, is a programming language specifically designed to support rule-based source to source transformation. Originally conceived as a tool for exploring programming language dialects, TXL has evolved into a general purpose software transformation system that has proven well suited to a wide range of software maintenance and reengineering tasks, including the design recovery, analysis and automated reprogramming of billions of lines of commercial Cobol, PL/I and RPG code for the Year 2000. The authors introduce the basic features of modern TXL and its use in a range of software engineering applications, with an emphasis on how each task can be achieved by source transformation.","Pattern matching,
Electronic switching systems,
Information science,
Councils,
Computer science,
Optimized production technology"
Code assignment for IMT-2000 on forward radio link,"High-speed multimedia data transmission is a sign significant feature in the third-generation mobile communication systems. Wideband CDMA is widely studied and accepted as the transmission scheme over the air. The single-code and multi-code transmission schemes over the CDMA were also investigated by many researchers. OVSF codes were proposed by the 3GPP as the forward channelization codes to the IMT-2000, the property the OVSF code tree and the code manipulations are important in achieving the high spectral efficiency, which is also the key issue of resource management. This paper proposed an efficient and fast channelization code assignment scheme (FEX scheme), by utilizing the code exchange and garbage collection technique, to reduce the blocking rate of the system. The simulations demonstrated the effectiveness of this scheme by comparing to an ""ideal system"". The results also revealed the importance of the code exchange and multi-code property to greatly reduce the blocking rate of the system.","Radio link,
Multiaccess communication,
Wideband,
RAKE receivers,
Computer science,
Electronic mail,
Multimedia systems,
Data communication,
Mobile communication,
Telecommunications"
Average-sense optimality and competitive optimality for almost instantaneous VF codes,"One-shot coding and repeated coding are considered for the class of almost instantaneous variable-to-fixed length (AIVF) codes, C/sub AIVF/, which includes some nonproper VF codes in addition to the class of proper VF codes, C/sub PVF/. An algorithm is given to construct the average-sense optimal (a-optimal) AIVF code in one-shot coding that attains the maximum average parse length in C/sub AIVF/. The algorithm can also be used to obtain an AIVF code with multiple parse trees, which can attain good performance for repeated coding. Generally, the a-optimal code for one-shot coding and the good code for repeated coding are more efficient than the Tunstall (1967) code in A-ary cases if A/spl ges/3 although they coincide with the Tunstall code in the binary case. The competitively optimal (c-optimal) VF code is also considered for one-shot coding, and it is shown that the c-optimal code does not always exist in C/sub PVF/ and in C/sub AIVF/. Furthermore, whenever the c-optimal code exists, the Tunstall code is c-optimal in C/sub PVF/ and the a-optimal code obtained by our algorithm is c-optimal in C/sub AIVF/ if A=2 or 3, but the a-optimal code is not always c-optimal in C/sub AIVF/ if A/spl ges/4.",
SOVIA: a user-level sockets layer over virtual interface architecture,,"Sockets,
Hardware,
Protocols,
Computer architecture,
Libraries,
Communication industry,
Operating systems,
Delay,
Bandwidth,
TCPIP"
Performance benefits of NIC-based barrier on myrinet/GM,,"Delay,
Protocols,
Computer networks,
Concurrent computing,
Distributed computing,
Network interfaces,
Message passing,
Laboratories,
Information science,
Engineering profession"
Multi-frame infinitesimal motion model for the reconstruction of (dynamic) scenes with multiple linearly moving objects,"We introduce new small-motion multi-frame equations applicable to the reconstruction of dynamic scenes in which points are allowed to move along straight-line paths with constant velocity. The motion equations apply to both static and dynamic points, thus prior segmentation is not necessary. We present a reconstruction algorithm of camera motion, scene structure, and point trajectories embedded into a multi-frame factorization principle which requires the minimum of 11 images and 7 points (out of which at feast 3 are dynamic).",
Fast fuzzy clustering of infrared images,"Clustering is an important technique for unsupervised image segmentation. The use of fuzzy c-means clustering can provide more information and better partitions than traditional c-means. In image processing, the ability to reduce the precision of the input data and aggregate similar examples can lead to significant data reduction and correspondingly less execution time. This paper discusses brFCM (bit reduction by Fuzzy C-Means), a data reduction fuzzy c-means clustering algorithm. The algorithm is described and several key implementation issues are discussed. Performance speedup and correspondence to a typical FCM implementation are presented from a data set of 172 infrared images. Average speedups of 59 times that of traditional FCM were obtained using brFCM, while producing identical cluster output relative to FCM.","Infrared imaging,
Clustering algorithms,
Partitioning algorithms,
Image segmentation,
Quantization,
Image processing,
Layout,
Computer science,
Testing,
Image generation"
Restoration scheme of mobility databases by mobility learning and prediction in PCS networks,"This paper proposes a restoration scheme based on mobility learning and prediction in the presence of the failure of mobility databases in personal communication systems (PCSs). In PCSs, mobility databases must maintain the current location information of users to provide a fast connection for them. However, the malfunction of mobility databases may cause some location information to be lost. As a result, without an explicit restoration procedure, incoming calls to users may be rejected. Therefore, an explicit restoration scheme against the malfunction of mobility databases is needed to guarantee continuous service availability to users. Introducing mobility learning and prediction into the restoration process allows systems to locate users after a failure of mobility databases. In failure-free operations, the movement patterns of users are learned by a neuro-fuzzy inference system (NFIS). After a failure, an inference process of the NFIS is initiated and the users' future location is predicted. This is used to locate lost users after a failure. This proposal differs from previous approaches using a checkpoint because it does not need a backup process nor additional storage space to store checkpoint information. In addition, simulations show that our proposal can reduce the cost needed to restore the location records of lost users after a failure when compared to the checkpointing scheme.",
The service grid: supporting scalable heterogeneous services in wide-area networks,Scalable delivery of network services to client applications is a difficult problem. We present a service architecture that is designed to accommodate the scalable delivery of heterogeneous services to clients with different QoS requirements. A prototype was built using the Legion wide area computing infrastructure to explore the design of application- and service-based replica selection and creation policies in a wide area network. The preliminary results demonstrate that application- and service-based policies outperform generic policies such as random and round robin. The results also show that overhead control is a key issue for latency sensitive requests.,"Intelligent networks,
Network servers,
Prototypes,
Distributed computing,
Computer architecture,
Computer science,
Design engineering,
Cities and towns,
Computer networks,
Explosives"
A new code-disjoint sum-bit duplicated carry look-ahead adder for parity codes,A new code-disjoint self-checking carry look-ahead adder is proposed. To reduce the necessary area and the power dissipation only the sum bits of the adder cells are duplicated. This is possible since the input parity is determined by use of internal nodes of the adder cells. The adder is modeled by a SYNOPSIS CAD tool from the EUROCHIP-Project with a standard library. With respect to duplication and comparison the necessary area and the power dissipation can be reduced up to 38 % and up to 29 % respectively compared to an increase of the maximal delay of only 12 %.,"Equations,
Power dissipation,
Parity check codes,
Computer science,
Fault tolerance,
Libraries,
Delay,
Fault detection,
Signal design,
Computer errors"
CRAFT: a framework for evaluating software clustering results in the absence of benchmark decompositions [Clustering Results Analysis Framework and Tools],"Software clustering algorithms are used to create high-level views of a system's structure using source code-level artifacts. Software clustering is an active area of research that has produced many clustering algorithms. However, we have so far seen very little work that investigates how the results of these algorithms can be evaluated objectively in the absence of a benchmark decomposition or without the active participation of the original designers of the system. Ideally, for a given system, art agreed upon reference (benchmark) decomposition of the system's structure would exist, allowing the results of various clustering algorithms to be compared against it. Since such benchmarks seldom exist, we seek alternative methods to gain confidence in the quality of results produced by software clustering algorithms. In this paper, we present a tool that supports the evaluation of software clustering results in the absence of a benchmark decomposition.","Clustering algorithms,
Software algorithms,
Software tools,
Software maintenance,
Algorithm design and analysis,
Software systems,
Documentation,
Mathematics,
Computer science,
Software quality"
Adaptive fault-tolerant wormhole routing in 2D meshes,"We present an adaptive fault-tolerant wormhole routing algorithm for 2D meshes. The main feature is that with the algorithm, a normal routing message, when blocked by some faulty processes would detour along the f-polygons around the fault region. The proposed algorithm can tolerate convex faults with only three virtual channels per physical channel regardless of the overlapping of f-polygons of different fault regions. The proposed algorithm is deadlock-free.","Fault tolerance,
Routing,
System recovery,
Multiprocessing systems,
Solid modeling,
Computer science,
Information systems,
Fault tolerant systems,
Delay,
Throughput"
Secure and invisible data hiding in 2-color images,"In an earlier paper, we propose a steganography scheme for hiding a piece of critical information in a host binary image. That scheme ensures that in each m/spl times/n image block of the host image, as many as [log/sub 2/(mn+1)] bits can be hidden in the block by changing at most 2 bits in the block. As a sequel of that work, in this paper we propose a revised scheme that can maintain higher quality of the host image by sacrificing some data hiding space. The new scheme can still offer a good data hiding ratio. It ensures that for any bit that is modified in the host image, the bit is adjacent to another bit which has a value equal to the former's new value. Thus, the hiding effect is quite invisible.","Data encapsulation,
Cryptography,
Steganography,
Information security,
Computer science,
Data security,
Pixel,
Image processing,
Image coding,
Terminology"
Light affine lambda calculus and polytime strong normalization,"Light linear logic (LLL) and its variant, intuitionistic light affine logic (ILAL), are logics of polytime computation. All polynomial-time functions are representable by proofs of these logics (via the proofs-as-programs correspondence), and, conversely, that there is a specific reduction (cut-elimination) strategy which normalizes a given proof in polynomial time (the latter may well be called the polytime ""weak"" normalization theorem). In this paper, we introduce an untyped term calculus, called the light affine lambda calculus (/spl lambda//sub LA/), generalizing the essential ideas of light logics into an untyped framework. It is a simple modification of the /spl lambda/-calculus, and has ILAL as a type assignment system. Then, in this generalized setting, we prove the polytime ""strong"" normalization theorem: any reduction strategy normalizes a given /spl lambda//sub LA/ term (of fixed depth) in a polynomial number of reduction steps, and indeed in polynomial time.","Calculus,
Polynomials,
Logic programming,
Functional programming"
Parallel programming with message passing and directives,"The authors discuss methods for expressing and tuning the performance of parallel programs, using two programming models in the same program: distributed and shared memory. Such methods are important for anyone who uses these large machines for parallel programs as well as for those who study combinations of the two programming models. The article outlines applications in hydrology, computational chemistry, general science, seismic processing, aeronautics, and computational physics. Emphasizing both I/O and computation, they apply several numerical methods including finite element analysis, wave equation integration, linear algebra subroutines, fast Fourier transforms (FFTs), filters, and a variety of PDEs (partial differential equations) and ODEs (ordinary differential equations).","Parallel programming,
Message passing,
Physics computing,
Partial differential equations,
Hydrology,
Computer applications,
Chemistry,
Science - general,
Finite element methods,
Linear algebra"
A fully abstract game semantics of local exceptions,"A fully abstract game semantics for an extension of Idealized Algol with locally declared exceptions is presented. It is based on ""Hyland-Ong games"" (J.M.E. Hyland & C.-H.L. Ong, 1995), but as well as relaxing the constraints which impose functional behavior (as in games models of other computational effects, such as continuations and references), new structure is added to plays in the form of additional pointers which track the flow of control. The semantics is proved to be fully abstract by a factorization of strategies into a ""new-exception generator"" and a strategy with local control flow. It is shown, using examples, that there is no model of exceptions which is a conservative extension of the semantics of Idealized Algol without the new pointers.",
A new sufficient condition for stable fuzzy control system and its design method,"Fuzzy inference has a multigranular architecture consisting of symbols and continuous values, and this architecture has worked well to incorporate experts' know-how into fuzzy controls. The paper focuses on the important characteristic of fuzzy control, ""symbolic expression"" of a fuzzy control system. The paper first introduces a ""chain of rules"" to clarify the description of the behavior of the control systems. A coincidence of the symbolic and continuous behaviors is defined. With these definitions, the paper proposes a new condition, ""relaxed nonseparate condition"" and a new fuzzy inference method. The relaxed condition together with the new inference method guarantees the coincidence of symbolic and continuous behavior of the control system. The paper proposes a new design method of fuzzy controller that guarantees this coincidence. Simulations are done to show the feasibility of the relaxed nonseparate condition and proposed design method.",
Noah: low-cost file access prediction through pairs,"Prediction is a powerful tool for performance and usability. It can reduce access latency for I/O systems, and can improve usability for mobile computing systems by automating the file hoarding process. We present recent research that has resulted in a file successor predictor that matches the performance of state-of-the-art context-modeling predictors, while requiring a small fraction of their space requirements. Noah is an online algorithm for predicting successor file access events, effectively identifying strong pairings (successor relationships) among files. Noah can accurately predict approximately 80% of all file access events while tracking only two candidate successors, of which only one requires regular dynamic updates.","Delay,
Mobile computing,
Predictive models,
Prediction algorithms,
Computer science,
Lifting equipment,
Power engineering and energy,
Usability,
Power engineering computing,
Context modeling"
Predictable and efficient virtual addressing for safety-critical real-time systems,"Conventionally, the use of virtual memory in safety-critical real-time systems has been avoided, one reason being the difficulties it provides to timing analysis. The difficulties arise due to the Memory Management Unit (MMU) on commercial processors being optimised to improve average performance, to the detriment of simple worst-case analysis. However within safety-critical systems, there is a move towards implementations where processes of differing integrity levels are allocated to the same processor. This requires adequate partitioning between processes of different integrity levels. One method for achieving this in the context of commercial processor is via use of the MMU and its support for virtual memory. The focus of this paper is upon the provision of virtual memory for processes of all integrity levels without complicating the timing analysis of safety-critical processes with hard deadlines. Also, for lower integrity processes without hard deadlines, the flexibility of the virtual memory provided does not restrict the process functionality, The virtual memory system proposed is generic and can be implemented on many commercial architectures e.g. PowerPC, ARM and MIPS. This paper details the PowerPC implementation.","Real time systems,
Timing,
Aerospace electronics,
Memory management,
Performance analysis,
Runtime,
Operating systems,
Computer science,
Software safety,
Aircraft"
Integrating occlusion culling with view-dependent rendering,"We present an approach that integrates occlusion culling within the view-dependent rendering framework. View-dependent rendering provides the ability to change level of detail over the surface seamlessly and smoothly in real-time. The exclusive use of view-parameters to perform level-of-detail selection causes even occluded regions to be rendered in high level of detail. To overcome this serious drawback we have integrated occlusion culling into the level selection mechanism. Because computing exact visibility is expensive and it is currently not possible to perform this computation in real time, we use a visibility estimation technique instead. Our approach reduces dramatically the resolution at occluded regions.","Rendering (computer graphics),
Computer graphics,
Hardware,
Computer science,
Application software,
Geometry,
Image quality,
Lighting,
Niobium"
Integrating pair programming into a software development process,"Anecdotal and statistical evidence indicates that pair programmers - two programmers working side-by-side at one computer collaborating on the same design, algorithm, code or test - outperform individual programmers. One of the programmers (the driver) has control of the keyboard/mouse and actively implements the program. The other programmer (the observer) continuously observes the work of the driver to identify tactical (syntactic, spelling, etc.) defects, and also thinks strategically about the direction of the work. On demand, the two programmers can brainstorm any challenging problem. Because the two programmers periodically switch roles, they work together as equals to develop software. This practice of pair programming can be integrated into any software development process. As an example, this paper describes the changes that were made to the Personal Software Process (PSP) to leverage the power of two programmers working together, thereby formulating the Collaborative Software Process (CSP). The paper also discusses the expected results of incorporating pair programming into a software development process in which traditional, individual programming is currently used.","Programming profession,
Automatic testing,
Collaborative work,
Switches,
Collaborative software,
Computer science,
Algorithm design and analysis,
Books,
Problem-solving,
Software engineering"
An overview of emerging results in networked multi-vehicle systems,"Autonomous vehicle systems have been the topic of much research due to their ability to perform dangerous, repetitive and automated tasks in remote or hazardous environments. The potential for multi-vehicle systems cooperating together to accomplish given tasks is starting to draw together researchers from several fields, including robotics, control systems, and computer science. Multiple vehicles can be more effective than a single one, for example in information gathering tasks. By spreading out over the terrain to be searched, a cluster of autonomous helicopters, for example, can locate a target quite rapidly, or a group of coordinated autonomous underwater vehicles can search a coastal area for mines. In other cases, the coordinated operation of multiple vehicles can provide new capabilities. This is the case, for example, of the PATH strategy of platooning several vehicles as they travel along the highway, which may yield up to a four-fold increase in transportation capacity while enhancing safety. Another example is the Mobile Offshore Base, where semi-submersible modules are aligned to form a military base and runway at sea. The unprecedented length of the at-sea runway (up to a mile long) warrants the use of several modules. In each of these cases, there is a need for inter-vehicle communications so that each vehicle can know the status of the operation, the position of its counterparts, and whether the specific mission goals have changed. Thus the control and communication problems become inexorably tied. However, few results are available to analyze performance and stability of a closed loop system where some of the loops are closed by communicated variables. Using the above examples as a motivation, this paper examines emerging results in networked multi-vehicle systems. Recent work has taken many different approaches, such as hybrid systems, distributed control, differential games, control architectures, and artificial intelligence. The focus of this paper is on the control systems perspective. We attempt to present some current issues common to networked multi-vehicle systems, and to show how they have been solved to date in the perspective of the case studies.","Remotely operated vehicles,
Control systems,
Vehicle safety,
Road safety,
Mobile robots,
Robot kinematics,
Robotics and automation,
Automatic control,
Robot control,
Computer science"
Diagnosis for scan-based BIST: reaching deep into the signatures,"For partitioning-based diagnosis in a scan-based BIST environment, an exact analysis scheme, capable of identifying all scan cells that receive incorrect data, is proposed. In contrast to previously suggested approaches, the scheme we propose identifies all failing scan cells with no ambiguity whatsoever. Not only do we resolve failing scan cells unambiguously, but we do so at the earliest possible instance through reexamination of already computed signatures. Intensive utilization of this highly precise diagnostic state information leads to prognostic information regarding the usefulness of running upcoming tests which in turn leads to reductions in diagnosis time in excess of 30% compared to previous approaches.","Built-in self-test,
Testing,
Computer science,
Costs,
Hardware,
Data engineering,
Fault location,
Application software,
Fasteners,
Fault diagnosis"
A state-transition model of trust management and access control,,"Access control,
Computer science,
Cryptography,
Heart,
Resource management,
Energy management,
Computer languages,
Concurrent computing"
Implementing KDB-trees to support high-dimensional data,"The problem of retrieving large volumes of high dimensional data is an important and timely issue in the area of database management. The guiding idea of the paper is to develop a general-purpose point access method that attacks the limitations of KDB-trees in high-dimensional spaces, while preserving their relatively good performance in low-dimensional situations. The proposed structure, called high-dimensional KDB-tree, eliminates downward propagation of splits associated with the original KDB-tree structure, which results in low storage utilization and rapid deterioration of the retrieval performance. Additional improvements in the storage and retrieval performance are achieved by removing certain redundant information from the interior nodes. Experimental results show that, in high-dimensional spaces, the proposed structure outperforms the original KDB-trees by a significant margin, while incurring no loss of performance in low-dimensional spaces. The structure also outperforms two other variants of KDB-trees investigated in the paper.","Information retrieval,
Engines,
Computer science,
Spatial databases,
Technology management,
Performance loss,
US Department of Energy,
Data mining,
Multimedia systems,
Explosives"
LIPT: a lossless text transform to improve compression,"We propose an approach to develop a dictionary based reversible lossless text transformation, called LIFT (length index preserving transform), which can be applied to a source text to improve the existing algorithm's ability to compress. In LIFT, the length of the input word and the offset of the words in the dictionary are denoted with alphabets. Our encoding scheme makes use of the recurrence of same length words in the English language to create context in the transformed text that the entropy coders can exploit. LIFT also achieves some compression at the preprocessing stage and retains enough context and redundancy for the compression algorithms to give better results. Bzip2 with LIFT gives 5.24% improvement in average BPC over Bzip2 without LIPT, and PPMD with LIPT gives 4.46% improvement in average BPC over PPMD without LIFT, for our test corpus.","Dictionaries,
Natural languages,
Encoding,
Entropy,
Testing,
Internet,
Frequency,
Computer science,
Compression algorithms,
Explosions"
3-D interpretation of single line drawings based on entropy minimization principle,"The human visual system can interpret two-dimensional (2-D) line drawings like the Necker cube as three-dimensional (3-D) wire frames. On this human ability Thomas Marill presented two important papers. First one proposed the 3-D interpretation model based on the principle to minimize the standard deviation of the angles between line segments in 3-D wire frame (MSDA), and reported the results of simulation experiments. Second one proposed the principle to minimize the description length on the internal representation in visual system. Motivated by Marill's principle to minimize the description length, we propose a principle to minimize the entropy of angle distribution between line segments in a 3-D wire frame (MEAD), which is more general than the MSDA one. And we implement the principle MEAD using a genetic algorithm (GA) as a simulation program. The results of simulation experiments show that the proposed principle of MEAD is more appropriate than the MSDA and another principle.","Entropy,
Shape,
Humans,
Wire,
Engineering drawings,
Visual system,
Genetic algorithms,
Information science,
Computer graphics,
Microcomputers"
New techniques for efficient sliding thin-slab volume visualization,"High-resolution three-dimensional (3-D) volumetric images obtained by today's radiologic imaging scanners are rich in detailed diagnostic information. Despite the many visualization techniques available to assess such images, there remains information that is challenging to uncover, such as the location of small structures (e.g., mediastinal lymph nodes, narrowed-airway regions). Recently, sliding thin-slab (STS) visualization was proposed to improve the visualization of interior structures. These STS techniques sometimes depend on user opacity specifications or extra preprocessing, and other rendering approaches that use the general STS mechanism are conceivable. The authors introduce two techniques for STS volume visualization. The first, a depth (perspective) rendering process, produces an unobstructed, high-contrast 3-D view of the information within a thin volume of image data. Results are a function of relative planar locations. Thus, rendered views accurately depict the internal properties that were initially captured as position and intensity. The second method produces a gradient-like view of the intensity changes in a thin volume. Results can effectively detect the occurrence and location of dramatic tissue variations, often not visually recognized otherwise. Both STS techniques exploit the concept of temporal coherence to form sequences of consecutive slabs, using information from previously computed slabs. This permits efficient real-time computation on a general-purpose computer. Further, these techniques require no preprocessing, and results are not dependent on user knowledge. Results using 3-D computed tomography chest images show the computational efficiency and visual efficacy of the new STS techniques.",
A comparison and combination of methods for OOV word detection and word confidence scoring,"This paper examines an approach for combining two different methods for detecting errors in the output of a speech recognizer. The first method attempts to alleviate recognition errors by using an explicit model for detecting the presence of out-of-vocabulary (OOV) words. The second method identifies potentially misrecognized words from a set of confidence features extracted from the recognition process using a confidence scoring model. Since these two methods are inherently different, an approach which combines the techniques can provide significant advantages over either of the individual methods. In experiments in the JUPITER weather domain, we compare and contrast the two approaches and demonstrate the advantage of the combined approach. In comparison to either of the two individual approaches, the combined approach achieves over 25% fewer false acceptances of incorrectly recognized keywords (from 55% to 40%) at a 98% acceptance rate of correctly recognized keywords.",
Behavior-to-placed RTL synthesis with performance-driven placement,"Interconnect delay should be considered together with computation delay during architectural synthesis in order to achieve timing closure in deep submicrometer technology. In this paper, we propose an architectural synthesis technique for distributed-register architecture, which separates interconnect delay for data transfer from component delay for computation. The technique incorporates performance-driven placement into the architectural synthesis to minimize performance overhead due to interconnect delay. Experimental results show that our methodology achieves performance improvement of up to 60% and 22% on the average.","Delay effects,
Clocks,
Timing,
Computer architecture,
Distributed computing,
Logic,
Delay estimation,
Computer science,
Delay systems,
Inductance"
Augmented CPU reservations: towards predictable execution on general-purpose operating systems,"One problem with performing soft real-time computations on general-purpose operating systems is that these OSs may spend significant amounts of time in the kernel instead of performing work on behalf of the application that is nominally scheduled: the OS effectively steals time from the running application. Stolen time can be a significant obstacle to predictable program execution on real-time versions of Linux and Windows 2000, where it can cause applications to miss essentially all of their deadlines. We propose augmented CPU reservations, a novel mechanism for using fine-grained accounting information about the amount of stolen time to help the scheduler allow applications to meet their deadlines. We have designed and implemented Rez-C and Rez-FB, two schedulers that provide augmented reservations, and we have tested them in Windows 2000, showing that they can increase the predictability of CPU reservations. We also experimentally quantify the severity of stolen time caused by a variety of devices such as hard disk controllers, a network interface, and a software modem under real-time versions of Windows 2000 and Linux.","Operating systems,
Yarn,
Real time systems,
Kernel,
Linux,
Processor scheduling,
Testing,
Clocks,
Computer science,
Hard disks"
Unwrapping phase images by propagating probabilities across graphs,"Phase images are derived from source images by applying a modulus operation to each pixel value. Phase unwrapping is the problem of inferring the original, unwrapped values from the wrapped values, using prior knowledge about the smoothness of the image. One approach to solving this problem is to infer the gradient vector field of the unwrapped image and then integrate the gradient field. The gradient in a particular direction at a pixel is equal to the observed pixel difference plus an unknown integer number of shifts. We introduce a technique for inferring these shifts using the low-complexity probability propagation algorithm, applied in a graphical model that prefers shifts that match the phase image and that constrains the shifts to satisfy the properties of a gradient field. We present results for a phase image from the region of the Sandia National Laboratories.","Graphical models,
Laboratories,
Synthetic aperture radar,
Phase measurement,
Linear programming,
Decoding,
Computer science,
Pixel,
Magnetic resonance imaging,
Radar tracking"
A comparison of encodings and algorithms for multiobjective minimum spanning tree problems,"Finding minimum-weight spanning trees (MST) in graphs is a classic problem in operations research with important applications in network design. The basic MST problem can be solved efficiently, but the degree constrained and multiobjective versions are NP-hard. Current approaches to the degree-constrained single objective MST include Raidl's (2000) evolutionary algorithm (EA) which employs a direct tree encoding and associated operators, and Knowles and Corne's (2000) encoding based on a modified version of Prim's (1957) algorithm. Approaches to the multiobjective MST include various approximate constructive techniques from operations research, along with Zhou and Gen's (1999) evolutionary algorithm using a Prufer (1918) based encoding. We apply (appropriately modified) the best of recent methods for the (degree-constrained) single objective MST problem to the multiobjective MST problem, and compare with a method based on Zhou and Gen's approach. Our evolutionary computation approaches, using the different encodings, involve a new population-based variant of Knowles and Corne's PAES algorithm. We find the direct encoding to considerably outperform the Prufer encoding. We find that a simple iterated approach, based on Prim's algorithm modified for the multiobjective MST, also significantly outperforms the Prufer encoding.","Encoding,
Tree graphs,
Evolutionary computation,
Costs,
Algorithm design and analysis,
Operations research,
Computer science,
Cybernetics,
Design engineering,
Application software"
It's all about process: project-oriented teaching of software engineering,"Process considerations are a central part of the material for a software engineering course; they are also central to accomplishing full-lifecycle, team-based systems development projects in such a course. This paper discusses the ways in which we have achieved an effective process structure within an academic context of full-year project courses. The key features are a kernel project plan and a process management mechanism. The project plan is a schedule including eight milestones with fixed due dates and quite explicit deliverables. The management is accomplished through an advanced full-year course, whose participants guide the project teams through the process.","Education,
Software engineering,
Project management,
Engineering management,
Management information systems,
Databases,
Computer science,
Kernel,
Feedback,
Decision making"
Faults in processor control subsystems: testing correctness and performance faults in the data prefetching unit,"The processor control subsystems have for a long time been recognized as a bottleneck in the process of achieving complete fault coverage through various functional test propagation approaches. The difficult-to-test corner cases are further accentuated in fault-resilient control subsystems as no functional effect is incurred as a result of the fault, even though performance suffers. We investigate the construction of software programs, capable of providing full fault coverage at minimal hardware cost, for one such fault resilient subsystem in processor architecture: the data prefetching unit. Experimental results confirm the efficacy of the proposed method.","Prefetching,
System testing,
Hardware,
Costs,
Design for testability,
Control systems,
Delay,
Buffer storage,
Computer science,
Data engineering"
Artificial consciousness: Utopia or real possibility?,"Since the beginnings of computer technology, researchers have speculated about the possibility of building smart machines that could compete with human intelligence. Given the current pace of advances in artificial intelligence and neural computing, such an evolution seems to be a more concrete possibility. Many people now believe that artificial consciousness is possible and that, in the future, it will emerge in complex computing machines. However, a discussion of artificial consciousness gives rise to several philosophical issues: can computers think or do they just calculate? Is consciousness a human prerogative? Does consciousness depend on the material that comprises the human brain, or can computer hardware replicate consciousness? Answering these questions is difficult because it requires combining information from many disciplines including computer science, neurophysiology, philosophy, and religion. Further, we must consider the influence of science fiction, especially science fiction films, when addressing artificial consciousness. As a product of the human imagination, such works express human desires and fears about future technologies and may influence the course of progress. At a societal level, science fiction simulates future scenarios that can help prepare us for crucial transitions by predicting the consequences of significant technological advances. The paper considers robots in science fiction, the Turing test, computer chess and artificial consciousness.",
Minimum total-squared-correlation design of DS-CDMA binary signature sets,"The Welch lower bound (see Welch, L.R., IEEE Trans. Inform. Theory, vol.20, p.397-9, 1974; Massey, J. L. and Mittelholzer, T., ""Sequences II, Methods in Communication, Security, and Computer Sciences"", p.63-78, Springer-Verlag, New York, 1993) on the total-squared-correlation (TSC) of signature sets is known to be tight for real-valued signatures and loose for binary signatures whose number is not a multiple of 4. We derive new bounds on the TSC of binary signature sets for any number of signatures K and any signature length L. For almost all K, L in {1, 2, ..., 200}, we develop simple algorithms for the design of optimum binary signature sets that achieve the new bound.","Multiaccess communication,
Algorithm design and analysis,
Character generation,
Frequency conversion,
Communication channels,
System performance,
Interference,
Signal design,
Gain measurement,
Length measurement"
A multiobjective genetic algorithm for feature selection and granularity learning in fuzzy-rule based classification systems,We propose a new method to automatically learn the knowledge base of a fuzzy rule-based classification system (FRBCS) by selecting an adequate set of features and by finding an appropiate granularity for them. This process uses a multiobjective genetic algorithm and considers a simple generation method to derive the fuzzy classification rules.,"Genetic algorithms,
Fuzzy sets,
Computer science,
Fuzzy systems,
Neural networks,
Knowledge based systems,
System performance,
Diversity reception,
Fuzzy neural networks,
Input variables"
Examining Mobile-IP performance in rapidly mobile environments: the case of a commuter train,"Trains travel at speeds ranging from 0 to 80 m/s (0 to 288 km/hr). Providing in-train wireless Internet access to multimedia applications will require the use of a mobile networking protocol, such as Mobile-IP, to achieve uninterrupted connectivity. Although Mobile-IP represents a promising solution. its performance under ""extreme"" mobility is questionable. We simulated a train scenario and identified the limitations of the current Mobile-IP standard in terms of throughput, handoff and packet loss of a train moving tit different velocities. We investigated the performance of UDP- and TCP-sessions, and examined the effect of different base station interleaving distances on throughput and packet loss. The results presented are part of an investigative research into adaptive mobile networking protocols in rapidly mobile networks.","Computer aided software engineering,
Throughput,
Internet,
IP networks,
Home automation,
Information science,
Access protocols,
Vehicles,
Streaming media,
Delay"
Guaranteeing Pfair supertasks by reweighting,"We reconsider the ""supertask"" approach, in which a set of Pfair tasks is scheduled as a single task. We define a ""safe"" weight threshold for a given supertask in a multiprocessor system with either hard or soft deadlines and either strict or relaxed rate constraints.","Processor scheduling,
Scheduling algorithm,
Computer science,
Actuators,
Flow graphs,
Costs"
UPPAAL - present and future,"UPPAAL is a tool for modelling, simulation and verification of real-time systems, developed jointly by BRICS at Aalborg University and the Department of Computer Systems at Uppsala University. The tool is appropriate for systems that can be modelled as a collection of non-deterministic processes with finite control structure and real-valued clocks, communicating through channels or shared variables. Typical application areas include real-time controllers and communication protocols in particular, those where timing aspects are critical. In this paper, we review the status of the currently distributed version of the tool as well as facilities to be found in upcoming releases.",
Genre based navigation on the Web,"We report on our ongoing study of using the genre of Web pages to facilitate information exploration. By genre, we mean socially recognized regularities of form and purpose in documents (e.g., a letter, a memo, a research paper). Our study had three phases. First, through a user study, we identified genres which most/least frequently meet searchers' information needs. We found that certain genres are better suited for certain types of needs. We identified five major groups of document genres that might be used in an interactive search tool that would allow genre based navigation. We tried to balance the following dual objectives: i) each group should be recognizable by a computer algorithm as easily as possible; ii) each group has a better chance of satisfying particular types of information needs. Finally, we developed a novel user interface for Web searching that allows genre based navigation through three major functionalities: 1) limiting search to specified genres, 2) visualizing the hierarchy of genres discovered in the search results and 3) accepting user feedback on the relevancy of the specified genres.","Navigation,
Publishing,
Internet,
Web pages,
User interfaces,
Visualization,
Feedback,
Search engines,
Medical treatment,
Nominations and elections"
The dynamic probe class library-an infrastructure for developing instrumentation for performance tools,"The complexity of both parallel architectures and parallel applications poses several problems for the development of performance analysis and optimization tools. In this paper, we describe the motivations and the main aspects of the design of the Dynamic Probe Class Library (DPCL), an object based C++ class library that provides an infrastructure to reduce the cost of writing instrumentation for performance tools. Additionally, we present some of the performance tools built on top of DPCL, which demonstrate the power and flexibility of the library.","Probes,
Libraries,
Instruments,
Costs,
Application software,
Writing,
Performance analysis,
Computer science,
Educational institutions,
Parallel architectures"
LDA/SVM driven nearest neighbor classification,"Nearest neighbor classification relies on the assumption that class conditional probabilities are locally constant. This assumption becomes false in high dimensions with finite samples due to the curse of dimensionality. The nearest neighbor rule introduces severe bias under these conditions. We propose a locally adaptive neighborhood morphing classification method to try to minimize bias. We use local support vector machine teaming to estimate an effective metric for producing neighborhoods that are elongated along less discriminant feature dimensions and constricted along most discriminant ones. As a result, the class conditional probabilities can be expected to be approximately constant in the modified neighborhoods, whereby better classification performance can be achieved. The efficacy of our method is validated and compared against other competing techniques using a number of data sets.",
Mobile code security by Java bytecode instrumentation,"Mobile code provides significant opportunities and risks. Java bytecode is used to provide executable content to Web pages and is the basis for dynamic service configuration in the Jini framework. While the Java Virtual Machine includes a bytecode verifier that checks bytecode programs before execution, and a bytecode interpreter that performs run-time tests, mobile code may still behave in ways that are harmful to users. We present techniques that insert run-time tests into Java code, illustrating them for Java applets and Jini proxy bytecodes. These techniques may be used to contain mobile code behavior or, potentially, insert code that is appropriate to profiling or other monitoring efforts. The main techniques are class modification, involving subclassing non-final classes, and method-level modifications that may be used when control over objects from final classes is desired.",
Two on-line Japanese character databases in Unipen format,"This paper presents the UP-Kuchibue and UP-Nakayosi databases containing on-line handwritten Japanese characters. These databases are the international versions of two databases, Kuchibue and Nakayosi, collected in the Nakagawa Laboratory at the University of Agriculture & Technology in Tokyo. They contain more than 3 million characters written by 283 Japanese writers. UP-Kuchibue and UP-Nakayosi are stored in the common Unipen format. Unipen is a western plain ASCII format which allows easy access for international researchers and facilitates international benchmarks.","Databases,
Handwriting recognition,
Agriculture,
Writing,
Laboratories,
Benchmark testing,
Mice,
Computer science,
Automation,
Computer interfaces"
An efficient method for generating location updates for processing of location-dependent continuous queries,"Recent advances in mobile computing and mobile communication technology have led to the emergence of many innovative mobile computing applications. Some of them require providing support to location-dependent continuous queries (LDCQs) on moving objects. The result of a location-dependent query depends on the current locations of the moving objects. When the query is specified as continuous, the requesting client can get continuously changing results. In order to provide correct and timely results to requesting clients, the locations of moving objects have to be closely monitored. In this paper, we propose an adaptive monitoring method (AMM) for managing the locations of moving objects to maintain the correctness of the results of query evaluation without significantly increasing the wireless bandwidth requirements. Extensive simulation experiments have been conducted to investigate the performance of the proposed method as compared to plain dead-reckoning (PDR).","Mobile computing,
Bandwidth,
Portable computers,
Mobile communication,
Monitoring,
Databases,
Data models,
Uncertainty,
Computer science,
Communications technology"
Remote laboratory for a brushless DC motor,"The objective of this study is to investigate remote-learning methods in the context of mechatronics education, and in particular, for the study of brushless DC motors, which are extensively employed in robots, information devices, home appliances and other areas. While hypermedia-based courseware and computer-assisted instruction are widely used in conventional desk-type learning, very few examples exist of remote learning that involve experiments. The authors therefore developed a prototype client-server system for remotely conducting experiments on brushless DC motors, including Web-based courseware and other software. The server computer is connected to the motor laboratory, and the visual image and sounds of the experiment are transmitted to the client computer in real time. The remotely located user can operate the motors and conduct experiments through the client computer. Through demonstrations to a class, the authors conclude that the remote lab combined with a simulation of the motor's dynamic behavior can be a quite effective teaching aid for the study of precision motors.",
Efficient evaluation of classification and recognition systems,"In this paper, a new framework for evaluating a variety of computer vision systems and components is introduced. This framework is particularly well suited for domains such as classification or recognition systems, where blind application of the i.i.d. assumption would reduce an evaluation's accuracy, such as with classification or recognition systems. With few exceptions, most previous work on vision system evaluation does not include confidence intervals, since they are difficult to calculate, and are often coupled with strict requirements. We show how a set of previously overlooked replicate statistics tools can be used to obtain tighter confidence intervals of evaluation estimates while simultaneously reducing the amount of data and computation required to reach such sound evaluatory conclusions. In the included application of the new methodology, the well-known FERET face recognition system evaluation is extended to incorporate standard errors and confidence intervals.","Statistics,
Computer vision,
Terminology,
Layout,
Computer science,
Machine vision,
Application software,
Face recognition,
Computer errors,
Face detection"
Probabilistic roadmaps-putting it all together,"Given a robot and a workspace, probabilistic roadmap planners (PRMs) build a roadmap of paths sampled from the workspace. A roadmap node is a single collision-free robot configuration, randomly generated. A roadmap edge is a sequence of collision-free robot configurations which interpolate the path from one roadmap node to another. Queries to the roadmap are (start, goal) pairs. If both the start and goal of a pair can be connected to the same connected component of the roadmap, the query is solved. Many promising variants of the PRM have been proposed, each with their own strengths and weaknesses. We propose a meta-planner for using many PRMs in such a way that the strengths are combined and the weaknesses offset. Our meta-planner will perform the combination in the following manner: i) provide a framework in which different motion planners are available and to which new ones are easily added; ii) characterize subregions (possibly overlapping) based on sample characteristics and connection results; iii) assign subregions to one or more planners which are judged promising; and iv) provide stopping criteria for roadmap construction. We present experimental results for four characterization measures. A general technique we call 'filtering' is presented for keeping roadmaps compact.","Robot kinematics,
Motion planning,
Computer science,
Orbital robotics,
Management training,
Road accidents,
Mathematics,
Path planning,
Educational robots,
Drugs"
Fast reconciliations in fluid replication,"Mobile users can increasingly depend on high speed connectivity. Despite this, using distributed file services across the wide area is painful. Fast approaches sacrifice one or more of safety, visibility, and consistency in the name of performance. Instead, we propose fluid replication, the ability to create replicas where and when needed. These replicas, called WayStations, maintain consistency with home servers through periodic reconciliations. Two techniques make reconciliation fast; this is crucial to the success of fluid replication. First, we defer propagation of updates, and only invalidate files during a reconciliation. Second, rather than depend on operation logs, we provide the subtrees in which all updates have occurred. These subtrees, named by their least common ancestors, or LCAs, can be constructed incrementally, and reduce the burden of checking serializability during a reconciliation. While these techniques provide better performance, they are not without risk. Bulk invalidation can lead to false sharing, optimistic updates are subject to conflict, and deferred updates may cause performance problems if they are needed elsewhere. To address these concerns, we performed a trace-based evaluation of our algorithms.","Network servers,
Safety,
Mobile computing,
Costs,
Peer to peer computing,
File systems,
Computer science,
Performance evaluation,
Wireless networks,
Ethernet networks"
A general framework for networked multimedia applications enabling access to laboratory equipment: the LABNET project experience,"The issue of accessing remote complex laboratories in a networked environment and performing experiments and measurements in which several applications and diverse devices are involved is addressed. A proposal is described for a common architecture for enhanced multimedia remote device control. The aim is to have the ability to set up a ""gate"" device that is the structured sum of real devices (from routers to microscopes) and virtual ones (e.g. software applications). The architecture exploits the most recent WWW ""products"", using the flexibility of embedding Java within a middleware framework. The authors do not claim that this will be ""the"" common architecture in such environment, but this exercise is useful for highlighting a number of problems through the proposal of different sets of solutions. The definition and initial implementation of this concept is part of the activities carried on within the framework of the LABNET project, at the CNIT Multimedia Communications Laboratory in Naples, Italy. The project, which started in April 2000, aims at the implementation of tools for network access to real laboratories and the management of experiments at a distance. The main structure of the project and the on-going activities are also briefly described.","Computer architecture,
Proposals,
Laboratories,
Performance evaluation,
Microscopy,
Application software,
World Wide Web,
Java,
Middleware,
Multimedia communication"
Team-Soar: a computational model for multilevel decision making,"Computational models which symbolically represent the abstraction of reality on computers, have won popularity as research tools in organizational studies. However, very few, if any computational models have been used to test theories in the same ways that human experiments generally do. The author introduces a simulation experiment using a computational model of team called ""Team-Soar,"" which mimics a human team experiment that was performed to test a theory of team decision making. The results support the major propositions of the multilevel theory in the same fashion as the ones of the human team experiment. The simulation experiment displays the Team-Soar model's effectiveness as a theory prover.","Computational modeling,
Decision making,
Humans,
Testing,
Performance evaluation,
Power engineering computing,
Voting,
Power engineering and energy,
Buildings,
Construction industry"
Extracting navigation states from a hand-drawn map,"Being able to interact and communicate with robots in the same way we interact with people has long been a goal of AI and robotics researchers. In this paper, we propose a novel approach to communicating a navigation task to a robot, which allows the user to sketch an approximate map on a PDA and then sketch the desired robot trajectory relative to the map. State information is extracted from the drawing in the form of relative, robot-centered spatial descriptions, which are used for task representation and as a navigation language between the human user and the robot. Examples are included of two hand-drawn maps and the linguistic spatial descriptions generated from the maps.",
A flexible method for segmentation in concept assignment,"Software comprehension is one of the most expensive activities in software maintenance and many tools have been developed to help the maintainer reduce the time and cost of the task. Of the numerous tools and methods available, one group has received relatively little attention: those using plausible reasoning to address the concept assignment problem. This problem is defined as the process of assigning descriptive terms to their implementation in source code, the terms being nominated by a maintainer and usually relating to computational intent. We present a new concept assignment method for COBOL II: Hypothesis-Based Concept Assignment (HB-CA). It employs a simple knowledge base to model concepts, source code indicators, and inter-concept relationships. An implementation of a prototype tool is described, and the results from a comprehensive evaluation using COBOL II sources summarised.","Software maintenance,
Costs,
Software tools,
Software systems,
Gold,
Computer science,
Software prototyping,
Prototypes,
Software algorithms,
Software engineering"
Creating and exploiting flexibility in Steiner trees,This paper presents the concept of flexibility-a geometric property associated with Steiner trees. Flexibility is related to the routability of the Steiner tree. We present an optimal algorithm which takes a Steiner tree and outputs a more flexible Steiner tree. Our experiments show that a net with a flexible Steiner tree increases its routability. Experiments with a global router show that congestion is improved by approximately 20%.,"Routing,
Delay,
Circuits,
Permission,
Computer science,
Stability"
An iterative rounding 2-approximation algorithm for the element connectivity problem,"In the survivable network design problem (SNDP), given an undirected graph and values r/sub ij/ for each pair of vertices i and j, we attempt to find a minimum-cost subgraph such that there are r/sub ij/ disjoint paths between vertices i and j. In the edge connected version of this problem (EC-SNDP), these paths must be edge-disjoint. In the vertex connected version of the problem (VC-SNDP), the paths must be vertex disjoint. K. Jain et al. (1999) propose a version of the problem intermediate in difficulty to these two, called the element connectivity problem (ELC-SNDP, or ELC). These variants of SNDP are all known to be NP-hard. The best known approximation algorithm for the EC-SNDP has performance guarantee of 2 (K. Jain, 2001), and iteratively rounds solutions to a linear programming relaxation of the problem. ELC has a primal-dual O (log k) approximation algorithm, where k=max/sub i,j/ r/sub ij/. VC-SNDP is not known to have a non-trivial approximation algorithm; however, recently L. Fleischer (2001) has shown how to extend the technique of K. Jain ( 2001) to give a 2-approximation algorithm in the case that r/sub ij//spl isin/{0, 1, 2}. She also shows that the same techniques will not work for VC-SNDP for more general values of r/sub ij/. The authors show that these techniques can be extended to a 2-approximation algorithm for ELC. This gives the first constant approximation algorithm for a general survivable network design problem which allows node failures.","Iterative algorithms,
Approximation algorithms,
Algorithm design and analysis,
Linear programming,
Robustness,
Computer science,
Polynomials"
Designing Grid-based problem solving environments and portals,"Building problem solving environments in the emerging national-scale Computational Grid infrastructure is a challenging task. Accessing advanced Grid services, such as authentication, remote access to computers, resource management, and directory services, is usually not a simple matter for problem solving environment developers. The Commodity Grid project is working to overcome this difficulty by creating what we call Commodity Grid Toolkits (CoG Kits) that define mappings and interfaces between the Grid and particular commodity frameworks familiar to problem solving environment developers. We explain why CoG Kits are important for problem solving environment developers, describe the design and implementation of a Java CoG Kit, and use examples to illustrate how CoG Kits can enable new approaches to application development based on the integrated use of commodity and Grid technologies.","Problem-solving,
Portals,
Java,
Grid computing,
Distributed computing,
Resource management,
Buildings,
Authentication,
Explosives,
Internet"
Comparison of local plane fitting methods for range data,"In this research, we introduce a reasonable noise model for range data which is obtained by a laser radar range finder, and derive two simple approximate solutions of optimal local plane fitting the range data under the noise model. We compare our methods with general least-squares based methods, such as Z-function fitting, the eigenvalue method, the maximum likelihood estimation method, and the renormalization method, an iterative method to obtain the optimal fitting of planes of range data under the noise model. All the methods are compared and evaluated using both synthetic range data and real range data with ground truth. From the experimental evaluation results, the proposed methods are shown to be effective, and the general least-squares-based methods are shown to be unsuitable for the assumed noise model.",
Comparison of methods for estimation of Kyoto Protocol products of forests from multitemporal Landsat,"The Kyoto Protocol requires nations to report on their reforestation, afforestation, and deforestation (RAD). Using 1990 as a baseline, nations are also required to monitor changes in carbon stocks leading up to the reporting period 2008 to 2012. A study was conducted using three dates of boreal summer Landsat 5 imagery to estimate above-ground carbon for a forested test site near Hinton, Alberta. The carbon estimates were compared with those derived from Canada's national forest inventory. The remote sensing estimates for areas that had not changed were consistent year to year within 3%. The experiment was repeated with the addition of leaf-on and leaf-off image pairs and Landsat-7 imagery. A comparison was made of the classification accuracies achieved for forest classes with single date and paired leaf-on and leaf-off image sets. Spatial properties were incorporated into the image analysis by first creating a multitemporal segmentation. This paper reports on the classification methods used, compares the classification accuracies achieved, and gives recommendations for the creation of Kyoto Protocol products for temperate forests derived from remotely sensed imagery.","Protocols,
Remote monitoring,
Remote sensing,
Satellites,
Testing,
Read only memory,
Forestry,
Computer science,
Image analysis,
Image segmentation"
"Planar graphs, negative weight edges, shortest paths, and near linear time","The authors present an O(n log/sup 3/ n) time algorithm for finding shortest paths in a planar graph with real weights. This can be compared to the best previous strongly polynomial time algorithm developed by R. Lipton et al., (1978 )which ran in O(n/sup 3/2/) time, and the best polynomial algorithm developed by M. Henzinger et al. (1994) which ran in O/spl tilde/(n/sup 4/3/) time. We also present significantly improved algorithms for query and dynamic versions of the shortest path problems.",
Implementation techniques for efficient data-flow analysis of large programs,"Many software engineering tools such as program slicers must perform data-flow analysis in order to extract necessary information from the program source. These tools typically borrow much of their implementation from optimizing compilers. However, since these tools are expected to analyze programs in their entirety, rather than functions in isolation, the time and space performance of the dataflow analyses are of major concern. We present techniques that reduce the time and space required to perform dataflow analysis of large programs. We have used these techniques to implement an efficient program slicing tool for C programs and have computed slices of programs with more than 100,000 lines of code.","Data analysis,
Performance analysis,
Data engineering,
Program processors,
Information analysis,
Optimizing compilers,
Software tools,
Programming profession,
Computer science,
Data mining"
Real-time cloth simulation with sparse particles and curved faces,"In this paper, we present a novel technique for real-time cloth simulation. The method combines dynamic simulation and geometric techniques. Only a small number of particles (a few hundred at maximum) are controlled using dynamic simulation to simulate global cloth behaviors such as waving and bending. The cloth surface is then smoothed based on the elastic forces applied to each particle and the distance between each pair of adjacent particles. Using this geometric smoothing, local cloth behaviors such as twists and wrinkles are efficiently simulated. The proposed method is very simple, and is easy to implement and integrate with existing particle-based systems. We also describe a particle-based simulation system for efficient simulation with sparse particles. The proposed method has animated a skirt with rich details in real-time.","Computational modeling,
Solid modeling,
Animation,
Smoothing methods,
Clothing,
Mesh generation,
Intelligent systems,
Information science,
Real time systems,
Force control"
Recognizing and tracking of 3D-shaped micro parts using multiple visions for micromanipulation,"Presents a visual feedback system that controls a micromanipulator using multiple microscopic vision information. The micromanipulation stations basically have an optical microscope. However the single field-of-view of optical microscope essentially limits the workspace of the micromanipulator and low depth-of-field makes it difficult to handle 3D-shaped micro objects. The system consists of a stereoscopic microscope, three CCD cameras, the micromanipulator and personal computer. The use of a stereoscopic microscope which has long working distance and high depth-of-field with selective field-of-view improves the recognizability of 3D-shaped micro objects and provides a method for overcoming several essential limitations in micromanipulation. Thus, visual feedback information is very important in handling micro objects for overcoming those limitations and provides a mean for the closed-loop operation. We propose this method for recognition and tracking of 3D-shaped micro parts and generating motion commands for the micromanipulator.","Optical microscopy,
Micromanipulators,
Optical feedback,
Optical sensors,
Control systems,
Sensor systems,
Optical noise,
Microcomputers,
Humans,
Haptic interfaces"
Enabling personalized recommendation on the Web based on user interests and behaviors,"The dramatic growth of the Web has brought about the rapid accumulation of data and the increasing possibility of information sharing. As the population on the Web grows, the analysis of user interests and behaviors will provide hints on how to improve the quality of service. We define user interests and behaviors based on the documents read by the user. A method for mining such user interests and behaviors is then presented. In this way, each user is associated with a set of interests and behaviors, which is stored in the user profile. In addition, we define six types of user profiles and a distance measure to classify users into clusters. Finally, three kinds of recommendation services using the clustered results are realized. For performance evaluation, we implement these services on the Web to make experiments on real data/users. The results show that the average acceptance rates of these services range from 71.5% to 94.6%.","Information filtering,
Information filters,
Web pages,
Search engines,
Collaboration,
Costs,
Indexing,
Computer science,
Quality of service,
Information retrieval"
Symbolic model checking for self-stabilizing algorithms,"A distributed system is said to be self-stabilizing if it converges to safe states regardless of its initial state. In this paper we present our results of using symbolic model checking to verify distributed algorithms against the self-stabilizing property. In general, the most difficult problem with model checking is state explosion; it is especially serious in verifying the self-stabilizing property, since it requires the examination of all possible initial states. So far applying model checking to self-stabilizing algorithms has not been successful due to the problem of state explosion. In order to overcome this difficulty, we propose to use symbolic model checking for this purpose. Symbolic model checking is a verification method which uses Ordered Binary Decision Diagrams (OBDDs) to compactly represent state spaces. Unlike other model checking techniques, this method has the advantage that most of its computations do not depend on the initial states. We show how to verify the correctness of algorithms by means of SMV, a well-known symbolic model checker. By applying the proposed approach to several algorithms in the literature, we demonstrate empirically that the state spaces of self-stabilizing algorithms can be represented by OBDDs very efficiently. Through these case studies, we also demonstrate the usefulness of the proposed approach in detecting errors.",
Flexible Web document analysis for delivery to narrow-bandwidth devices,"We propose a set of baseline heuristics for identifying genuinely tabular information and news links in HTML documents. A prototype implementation of these heuristics is described for delivering content from news providers' home pages to a narrow-bandwidth device such as a portable digital assistant or cellular phone display. Its evaluation on 75 Web sites is provided, along with a discussion of topics for future research.","Text analysis,
HTML,
Portals,
Computer displays,
Cellular phones,
Laboratories,
Computer science,
Multimedia systems,
Educational institutions,
Prototypes"
Clusterfile: a flexible physical layout parallel file system,,"File systems,
Concurrent computing,
Application software,
Control systems,
Scattering,
Clustering algorithms,
Computer science,
Pattern matching,
Electronic switching systems,
Peer to peer computing"
Sparse PCA. Extracting multi-scale structure from data,"Sparse Principal Component Analysis (S-PCA) is a novel framework for learning a linear, orthonormal basis representation for structure intrinsic to an ensemble of images. S-PCA is based on the discovery that natural images exhibit structure in a low-dimensional subspace in a sparse, scale-dependent form. The S-PCA basis optimizes an objective function which trades off correlations among output coefficients for sparsity in the description of basis vector elements. This objective function is minimized by a simple, robust and highly scalable adaptation algorithm, consisting of successive planar rotations of pairs of basis vectors. The formulation of S-PCA is novel in that multi-scale representations emerge for a variety of ensembles including face images, images from outdoor scenes and a database of optical flow vectors representing a motion class.","Principal component analysis,
Data mining,
Independent component analysis,
Layout,
Higher order statistics,
Statistical distributions,
Computer science,
Educational institutions,
Robustness,
Image databases"
Realizations of fast 2-D/3-D image filtering and enhancement,"This paper proposes a novel algorithm for multidimensional image enhancement based on a fuzzy domain enhancement method, and an implementation of a recursive and separable low-pass filter. Considering a smoothed image as a fuzzy data set, each pixel in an image is processed independently, using fuzzy domain transformation and enhancement of both the dynamic range and the local gray level variations. The algorithm has the advantages of being fast and adaptive, so it can be used in real-time image processing applications and for multidimensional data with low computational cost. It also has the ability to reduce noise and unwanted background that may affect the visualization quality of two-dimensional (2-D)/three-dimensional (3-D) data. Examples for the applications of the algorithm are given for mammograms, ultrasound 3-D images, and photographic images.","Filtering,
Low pass filters,
Multidimensional systems,
Fuzzy sets,
Image enhancement,
Pixel,
Dynamic range,
Image processing,
Computational efficiency,
Noise reduction"
Software cultures and evolution,"To work effectively with legacy code, software engineers need to understand a legacy computer program's culture - the combination of the programmer's background, the hardware environment and the programming techniques that guided its creation. Software systems typically pass through a series of stages. During the initial development stage, software developers create a first functioning version of the code. An evolution stage follows, during which developmental efforts focus on extending system capabilities to meet user needs. During the servicing stage, only minor repairs and simple functional changes are possible. In the phase-out stage, the system is essentially frozen, but it still produces value. Finally, during the close-down stage, the developers withdraw the system and possibly replace it. Most of the tasks in the evolution and servicing phases require program comprehension /sup n/derstanding how and why a software program functions in order to work with it effectively. Effective comprehension requires viewing a legacy program not simply as a product of inefficiency or stupidity, but instead as an artifact of the circumstances in which it was developed. This information can be an important factor in determining appropriate strategies for the software program's transition from the evolution stage to the servicing or phase-out stage.","Printers,
Hardware,
Programming profession,
Software systems,
Humans,
Solid modeling,
Genetic programming,
Art,
Ethics,
Computer science"
xADL: enabling architecture-centric tool integration with XML,"In order to support architecture-centric tool integration within the ArchStudio 2.0 Integrated Development Environment (IDE), we adopted Extensible Markup Language (XML) to represent the shared architecture-in-progress. Since ArchStudio is an architectural style-based development environment that incorporates an extensive number of tools, including commercial off-the-shelf products, we developed a new, vendor-neutral, ADL-neutral interchange format called Extensible Architecture description Language (xADL), as well as a ""vocabulary"" specific to the C2 style (xC2). This paper outlines our vision for representing architectures as hypertext, the design rationale behind xADL and xC2, and summarizes our engineering experience with this strategy.","XML,
Computer science,
Markup languages,
HTML,
Connectors,
Web sites,
Humans,
Automation,
SGML,
Uniform resource locators"
Stakeholder discovery and classification based on systems science principles,"It is the goal of the research work presented to elaborate on improvements to software development methods so that quality attributes can be handled more systematically. By quality attributes, we mean the large group of typically systemic properties of a software system, such as availability, security, etc., but also reusability, maintainability and many more. We define quality attributes as stakeholder-centric conditions on the behavior or structure of a system. The importance of the notion of a stakeholder cannot surprise, but the lack of a general theory on how to define and identify the relevant set of stakeholders does. Drawing from systems theory, we claim that four basic, generic types of stakeholders are sufficient to be able to derive a specialized set of stakeholders for any considered system and domain of inquiry. It is only when we understand the generic concepts and principles behind quality properties of systems, that we can properly derive methods and build tools to cope with them.","Software systems,
Information technology,
Software engineering,
Web services,
Computer science,
Usability,
Product safety,
Software safety,
Computer security,
Software quality"
A high-availability clustering architecture with data integrity guarantees,,"Peer to peer computing,
Hardware,
Computer architecture,
Heart beat,
Availability,
Mission critical systems,
Linux,
Foot,
Network servers,
Writing"
Fundamental trade-offs in aggregate packet scheduling,"We investigate the fundamental trade-offs in aggregate packet scheduling for the support of guaranteed delay service. Besides the simple FIFO packet scheduling algorithm, we consider two new classes of aggregate packet scheduling algorithms: the static earliest time first (SETF) and dynamic earliest time first (DETF). Through these two classes of aggregate packet scheduling, we show that, with additional time stamp information encoded in the packet header for scheduling purpose, we can significantly increase the maximum allowable network utilization level, while at the same time reducing the worst-case edge-to-edge delay bound. Furthermore, we demonstrate how the number of the bits used to encode the time stamp information affects the trade-off between the maximum allowable network utilization level and the worst-case edge-to-edge delay bound. In addition, the more complex DETF algorithms have far better performance than the simpler SETF algorithms. These results illustrate the fundamental trade-offs in aggregate packet scheduling algorithms and shed light on their provisioning power in support of guaranteed delay service.","Aggregates,
Scheduling algorithm,
Delay effects,
IP networks,
Diffserv networks,
Algorithm design and analysis,
Computer science,
Scalability,
H infinity control,
Encoding"
Reconstructing shape from motion using tactile sensors,"We present a new method to reconstruct the shape of an unknown object using tactile sensors without requiring object immobilization. Instead, the robot manipulates the object without prehension. The robot infers the shape, motion and the center of mass of the object based on the motion of the contact points as measured by tactile sensors. Our analysis is supported by simulation and experimental results.","Tactile sensors,
Probes,
Robot sensing systems,
Humans,
Fingers,
Shape measurement,
Computer science,
Motion measurement,
Analytical models,
Equations"
Distributed admission control for anycast flows with QoS requirements,"We study a distributed admission control (DAC) procedure for anycast flows with QoS requirements. We focus on algorithms that perform destination selection, which is critical in anycast. Several algorithms are proposed. These algorithms differ from each other in their dependence on system status information. We also address the issue of resource reservation and re-trial control in the DAC procedure. Performance data obtained by mathematical analysis and computer simulation show that in terms of admission probabilities, DAC systems that are based on local status information can perform closely to those that utilize global and dynamic status information. We note that the latter is much more expensive and difficult to realize.","Admission control,
Centralized control,
Scalability,
Computer science,
Computer simulation,
Unicast,
Distributed control,
Information analysis,
Performance analysis,
Computer networks"
Instructional design based on reusable learning objects: applying lessons of object-oriented software engineering to learning systems design,There is currently a lot of interest in the concept of learning objects. Learning objects are discrete units of learning resources based on agreed standards. The idea behind learning objects is to promote greater reuse of resources within new instructional systems development. The main work in learning objects has primarily focussed on defining the technical requirements and standards for computer based learning objects. The technology itself is not likely to bring the benefits promised by reusable objects without a change in methods used by practicing instructional designers. The instructional design implications of the learning object approach is examined to determine the adaptation required in instructional design methodologies. Object-oriented software engineering is proposed as a useful basis for new thinking in instructional design methodology.,"Design methodology,
Automobile manufacture,
Computer industry,
Programming,
Learning systems,
Software engineering,
Manufacturing industries,
Manufacturing automation,
Design engineering,
Systems engineering and theory"
A system-level energy minimization approach using datapath width optimization,"This paper presents a novel system-level approach that minimizes the energy consumption of embedded core-based systems through datapath width optimization. It is based on the idea of minimizing energy consumed by redundant bits, which are unused during execution of programs by means of optimizing the datapath width of processors. To minimize the redundant bits of variables in a given application program, the effective size of each variable is determined by variable size analysis, and Valen-C language is used to preserve the precision of computation. Analysis results of variables show that there are average 39% redundant bits in the C source program of MPEG-2 video decoder. In our experiments for several embedded applications, energy savings without performance penalty are reported range from about 10.8% to 48.3%.","Energy consumption,
Minimization,
Batteries,
Embedded system,
Permission,
Power system modeling,
Computer science,
Data engineering,
Power engineering and energy,
Decoding"
Characteristics of production database workloads and the TPC benchmarks,"There has been very little empirical analysis of any real production database workloads. Although the Transaction Processing Performance Council benchmarks C (TPC-C) and D (TPC-D) have become the standard benchmarks for on-line transaction processing and decision support systems, respectively, there has not been any major effort to systematically analyze their workload characteristics, especially in relation to those of real production database workloads. In this paper, we examine the characteristics of the production database workloads of ten of the world's largest corporations, and we also compare them to TPC-C and TPC-D. We find that the production workloads exhibit a wide range of behavior. In general, the two TPC benchmarks complement one another in reflecting the characteristics of the production workloads, but some aspects of real workloads are still not represented by either of the benchmarks. Specifically, our analysis suggests that the TPC benchmarks tend to exercise the following aspects of the system differently than the production workloads: concurrency control mechanism, workload-adaptive techniques, scheduling and resource allocation policies, and I/O optimizations for temporary and index files. We also re-examine Amdahl's rule of thumb for a typical data processing system and discover that both the TPC benchmarks and the production workloads generate on the order of 0.5 to 1.0 bit of logical I/O per instruction, surprisingly close to the much earlier figure.",
Predictive Approaches for Choosing Hyperparameters in Gaussian Processes,"Gaussian processes are powerful regression models specified by parameterized mean and covariance functions. Standard approaches to choose these parameters (known by the name hyperparameters) are maximum likelihood and maximum a posteriori. In this article, we propose and investigate predictive approaches based on Geisser's predictive sample reuse (PSR) methodology and the related Stone's cross-validation (CV) methodology. More specifically, we derive results for Geisser's surrogate predictive probability (GPP), Geisser's predictive mean square error (GPE), and the standard CV error and make a comparative study. Within an approximation we arrive at the generalized cross-validation (GCV) and establish its relationship with the GPP and GPE approaches. These approaches are tested on a number of problems. Experimental results show that these approaches are strongly competitive with the existing approaches.",
Heuristic algorithms for multi-constrained quality of service routing,"Multi-constrained quality of service (QoS) routing finds a route in the network that satisfies multiple independent quality of service constraints. This problem is NP-hard and a number of heuristic algorithms have been proposed to solve the problem. This paper studies two heuristics, the limited granularity heuristic and the limited path heuristic, for solving general k-constrained problems. Analytical and simulation studies are conducted to compare the time/space requirements of the heuristics and the effectiveness of the heuristics in finding the paths that satisfy the QoS constraints. We prove analytically that for an N nodes and E edges network with k (a small constant) independent QoS constraints, the limited granularity heuristic must maintain a table of size O(|N|/sup k-1/) in each node to be effective, which results in a time complexity of O(|N|/sup k/|E|). We also prove that the limited path heuristic can achieve very high performance by maintaining O(|N|/sup 2/lg(|N|)) entries in each node, which indicates that the performance of the limited path heuristic is not sensitive to the number of constraints. We conclude that although both the limited granularity heuristic and the limited path heuristic can efficiently solve 2-constrained QoS routing problems, the limited path heuristic is superior to the limited granularity heuristic in solving k-constrained QoS routing problems when k>3. Our simulation study further confirms this conclusion.","Heuristic algorithms,
Quality of service,
Routing,
Delay,
Bandwidth,
Performance analysis,
Computer science,
Analytical models,
Aggregates,
Costs"
An Automatic Design Optimization Tool and its Application to Computational Fluid Dynamics,"In this paper we describe the Nimrod/O design optimization tool, and its application in computational fluid dynamics. Nimrod/O facilitates the use of an arbitrary computational model to drive an automatic optimization process. This means that the user can parameterise an arbitrary problem, and then ask the tool to compute the parameter values that minimize or maximise a design objective function. The paper describes the Nimrod/O system, and then discusses a case study in the evaluation of an aerofoil problem. The problem involves computing the shape and angle of attack of the aerofoil that maximises the lift to drag ratio. The results show that our general approach is extremely flexible and delivers better results than a program that was developed specifically for the problem. Moreover, it only took us a few hours to set up the tool for the new problem and required no software development.",
A benefit function mapping heuristic for a class of meta-tasks in grid environments,"The Computational Grid is an appealing high performance computational platform. Problem in implementing Computational Grid environment is how to effectively use various resources in the system, such as compute cycle, memory, communication network, and data repositories. A benefit function resource mapping heuristic for Computational Grid environments is presented to map a set of independent tasks (Meta-task) to resources. The algorithm considers the influence of input data repositories' location and QOS of tasks to result of mapping. This semi-dynamic algorithm is more suitable for the dynamic adaptability and domain autonomy in the grid, and the benefit function heuristic adopted in the algorithm can assure the QOS of tasks more effectively.","Grid computing,
Computer networks,
Distributed computing,
Clustering algorithms,
Computer science,
Communication networks,
Availability,
Aggregates,
Supercomputers,
Operating systems"
Fast algorithms for DCT-domain video transcoding,"Video transcoding is an efficient way for rate adaptation and format conversion in various networked video applications. Many transcoder architectures have been proposed to achieve fast processing. Recently, thanks to its relatively low complexity and acceptable quality, the DCT-domain transcoder (DDT) was proposed to overcome the drift in the open-loop transcoders and the high-complexity in the cascaded transcoders. We show that there is still some improvement space for computation reduction in the DDT. We propose a method to fast extract partial low-frequency coefficients in the DCT-domain motion compensation (DCT-MC) operation. We investigate fast algorithms of the DDT based on the proposed fast coefficients extraction scheme. The simulation results show that the proposed methods can achieve significant computation reduction while maintaining close PSNR performance compared to the DDT.","Transcoding,
Decoding,
Computational complexity,
Discrete cosine transforms,
Computer architecture,
Video compression,
Motion compensation,
Computer science,
Application software,
Computational modeling"
Efficient update diffusion in byzantine environments,"We present a protocol for diffusion of updates among replicas in a distributed system where up to b replicas may suffer Byzantine failures. Our algorithm ensures that no correct replica accepts spurious updates introduced by faulty replicas, by requiring that a replica accepts an update only after receiving it from at least b+1 distinct replicas (or directly from the update source). Our algorithm diffuses updates more efficiently than previous such algorithms and, by exploiting additional information available in some practical settings, sometimes more efficiently than known lower bounds predict.","Delay,
Computer science,
Prediction algorithms,
Communication system security,
Information security,
Broadcasting,
Protocols,
Upper bound"
"Active Disk File System : A Distributed, Scalable File System","Consistent improvements in processor and memory technology have led to disks having greater processing power and cache memory in a compact size. This increased processing power and memory allows disks to execute more than just the basic disk operations, sometimes, even run user defined codes. Offloading part of the application processing to the disks can help reducing the latency of data manipulation as well as the amount of data transferred across the network. Such disks are called active disks. In this paper, a file system for the active-disk-based data server (ADFS) is proposed. All data files stored on active disks are provided with operations, forming objects. For some applications such as database, application-specific operations can be run by disk processors so that only the results are returned to clients, rather than whole data file is read by the clients. Therefore, ADFS is able to reduce the application-processing overhead of the system. In ADFS, part of file system functionality of the file manager can be delegated to active disks. Our implementation is based on CORBA specification. A set of file system interfaces is implemented for each module of each file system component. Part of the file system functionality, such as lookup, is embedded in the modules for active disks. We tested the performance of ADFS with 64 files in three-layer directory system. The performance results are obtained in a limited manner and presented in this paper. Our ADFS is basically stateful, but the delegation of file system functionality to disks allows our file system to be more scalable and helps to overcome some of the limitations of current prevailing file systems, like NFS, by greatly reducing the work of the central file manager. In addition, it is conceivable that the central file manager could be eliminated altogether.","File systems,
Utility programs,
File servers,
Disk drives,
Network servers,
Databases,
Information retrieval,
Computer science,
Rivers,
Cache memory"
Enterprise model as a basis of administration on role-based access control,"Access control is one of the important security issues for large enterprise organizations. The role-based access control (RBAC) model is well known and recognized as a good security model for the enterprise environment. Though RBAC is a good model, the administration of RBAC including building and maintaining access control information remains a difficult problem in large companies. The RBAC model itself does not tell the solution. Little research has been done on the practical ways of finding information that fills RBAC components such as role, role hierarchy, permission-role assignment, user-role assignment, and so on from the real world. We suggest model-based administration of RBAC in an enterprise environment. Model-based administration methods allow the security administrator to manage access control by a GUI that supports a graphical enterprise model. If the security administrator creates or changes some of the components of the graphical enterprise model, then it is translated to RBAC schema information by the administration tool. We focus on a practical way of deriving access control information from the real world. It is a core of model-based administration. We show the derivation method and implementation experiences.","Access control,
Information security,
Companies,
Permission,
Power system security,
Computer science,
Graphical user interfaces,
Power system modeling,
Personnel,
Writing"
Backoff protocols for distributed mutual exclusion and ordering,"Presents a simple and efficient protocol for mutual exclusion in synchronous message-passing distributed systems subject to failures. Our protocol borrows design principles from prior work in backoff protocols for multiple access channels such as the Ethernet. Our protocol is adaptive in that the expected amortized system response time - informally, the average time a process waits before entering the critical section - is a function only of the number of clients currently contending and is independent of the maximum number of processes that might contend. In particular, in the contention-free case, a process can enter the critical section after only one round-trip message delay. We use this protocol to derive a protocol for ordering operations on a replicated object in an asynchronous distributed system subject to failures. This protocol is always safe, is probabilistically live during periods of stability and is suitable for deployment in practical systems.","Access protocols,
Ethernet networks,
Disruption tolerant networking,
Fault detection,
Computer science,
Stability,
Interference,
Delay effects,
Upper bound,
Fault tolerance"
Blind removal of image non-linearities,This paper presents a technique for blindly removing image non-linearities in the absence of any calibration information or explicit knowledge of the imaging device. The basic approach exploits the fact that a non-linearity introduces specific higher-order correlations in the frequency domain (beyond second-order). These correlations can be detected using tools from polyspectral analysis. The non-linearities can then be estimated and removed by simply minimizing these correlations.,"Calibration,
Lenses,
Layout,
Speech analysis,
Frequency domain analysis,
Optical imaging,
Image processing,
Higher order statistics,
Computer science,
Educational institutions"
Neural network ensembles and their application to traffic flow prediction in telecommunications networks,"It is well-known that large neural networks with many unshared weights can be very difficult to train. A neural network ensemble consisting of a number of individual neural networks usually performs better than a complex monolithic neural network. One of the motivations behind neural network ensembles is the divide-and-conquer strategy, where a complex problem is decomposed into different components each of which is tackled by an individual neural network. A promising algorithm for training neural network ensembles is the negative correlation learning algorithm which penalizes positive correlations among individual networks by introducing a penalty term in the error function. A penalty coefficient is used to balance the minimization of the error and the minimization of the correlation. It is often very difficult to select an optimal penalty coefficient for a given problem because as yet there is no systematic method available for setting the parameter. This paper first applies negative correlation learning to the traffic flow prediction problem, and then proposes an evolutionary approach to deciding the penalty coefficient automatically in negative correlation learning. Experimental results on the traffic flow prediction problem will be presented.","Neural networks,
Telecommunication traffic,
Intelligent networks,
Management training,
Economic forecasting,
State estimation,
Aging,
Training data,
Application software,
Computer science"
Dynamic reconfiguration in high-speed computer clusters,,"Routing,
Fault detection,
Topology,
System recovery,
Testing,
Heuristic algorithms,
Process control,
Fans,
Resumes,
Tires"
Novel fast fading compensator for OFDM using space diversity with space-domain interpolator,This paper proposes a novel space diversity assisted OFDM receiver with Doppler spread compensator. A linear array antenna is set up on the vehicle. The proposed compensator estimates the received signal at the point that does not move with respect to the ground during the observation period. Computer simulation results show that the proposed scheme can mitigate the performance degradation due to Doppler spread.,"Fading,
OFDM,
Linear antenna arrays,
Vehicles,
Antenna arrays,
Space technology,
Computer simulation,
Receiving antennas,
Degradation,
Information science"
Integrating white- and black-box techniques for class-level regression testing,"In recent years, several techniques have been proposed for class-level regression testing. Most of these techniques focus either on white- or black-box testing, although an integrated approach can have several benefits. As similar tasks have to be carried out for both white- and black-box testing, an integrated approach can improve efficiency and cost effectiveness. The article explains an approach for class-level regression testing, integrating existing techniques. Particularly, those of Rothermel et al. (2000) and Hong et al. (1995) for white-box regression testing and black-box testing, respectively, have been integrated into a single technique. The benefits of the resulting technique are shown by example.","Software testing,
Costs,
Flow graphs,
Computer science,
Software tools,
Visualization,
Automata,
Printing"
Mixtures of trees for object recognition,"Efficient detection of objects in images is complicated by variations of object appearance due to intra-class object differences, articulation, lighting, occlusions, and aspect variations. To reduce the search required for detection, we employ the bottom-up approach where we find candidate image features and associate some of them with parts of the object model. We represent objects as collections of local features, and would like to allow any of them to be absent, with only a small subset sufficient for detection;furthermore, our model should allow efficient correspondence search. We propose a model, Mixture of Trees, that achieves these goals. With a mixture of trees, we can model the individual appearances of the features, relationships among them, and the aspect, and handle occlusions. Independences captured in the model make efficient inference possible. In our earlier work, we have shown that mixtures of trees can be used to model objects with a natural tree structure, in the context of human tracking. Now we show that a natural tree structure is not required, and use a mixture of trees for both frontal and view-invariant face detection. We also show that by modeling faces as collections of features we can establish an intrinsic coordinate frame for a face, and estimate the out-of-plane rotation of a face.","Object recognition,
Object detection,
Face detection,
Humans,
Detectors,
Space exploration,
Software,
Computer science,
Context modeling,
Tree data structures"
Architecture-based exception handling,"Architecture-based development environments are becoming an effective solution towards the construction of robust distributed systems. Through the abstract description of complex software systems configurations in terms of the interconnection of software elements at the interface level, software reuse and evolution get promoted. In addition, as shown by research results from the software architecture domain, it becomes feasible to provide formal notations for the precise description of configuration behavior, together with associated CASE tools for their automated analyses. However, little attention has been paid to software fault tolerance and in particular exception handling in that context, although this is crucial for achieving software robustness. We investigate the design and implementation of exception handling support for architecture-based development environments. After a survey of the issues raised by exception handling at the level of software architecture description, we introduce an exception handling facility for architecture-based software systems, addressing the resulting extension to architecture description languages and the mapping to implementation of software architectures embedding exception handling.","Software architecture,
Fault tolerance,
Software systems,
Connectors,
Robustness,
Computer aided software engineering,
Architecture description languages,
Computer architecture,
Protocols,
Topology"
A 2D systems approach to iterative learning control based on nonlinear adaptive control techniques,"We consider the iterative learning control problem from a 2D systems/adaptive control viewpoint. In particular, it is shown how some fundamental results from nonlinear adaptive control can be successfully applied in the iterative learning control domain under very weak assumptions. Some areas for further research are also briefly discussed.",
Resolving mobile database overflow with most idle replacement,"In a personal communications service (PCS) network, mobility databases called visitor location registers (VLRs) are utilized to temporarily store the subscription data and the location information for the roaming users. Because of user mobility, it is possible that the VLR is full when a mobile user arrives. Under such a circumstance, the incoming user has no VLR record and thus cannot receive PCS services. This issue is called VLR overflow. To resolve the VLR overflow problem, a VLR record can be selected for replacement when the VLR is full and then the reclaimed storage is used to hold the record of the requesting user. This paper considers the most idle replacement policy to provide services to mobile users without VLR records. In this policy, the record with the longest idle time is selected for replacement. We propose an analytic model to investigate the performance of this replacement policy. The analytic results are validated against simulation experiments. The results indicate that our approach effectively resolves the VLR overflow problem.","Databases,
Personal communication networks,
Subscriptions,
Roaming,
Councils,
Computer science,
Performance analysis,
Analytical models,
Mobile radio mobility management,
Wireless communication"
Making Networked Virtual Environments Work,"Collaborative virtual environments (CVEs) are a promising technology enabling remote participants to share a common place through three-dimensional graphical scenes. Within the COVEN project (Normand, 1999), we have run prolonged series of Internet trials that have allowed us to gather valuable data to formulate usability guidelines and networking requirements. However, running such trials in a real setting and making sure that the application and networking infrastructures will be stable enough is still a challenge. In this paper, we describe some of our experiences, together with the technical choices that have permitted many hours of successful Internet trials. We also make a thorough analysis of different correlated logging data. This analysis allows us to propose and confirm a model of a CVE application's network behavior, together with a number of interesting results that disprove some common assumptions. Furthermore, we use the model and the logging data to highlight the benefits of IP multicasting and for predicting traffic behaviors and bandwidth use on top of different logical network topologies.",
Performance analysis of preemptive handoff scheme for integrated wireless mobile networks,"We propose and analyze a preemptive handoff scheme for an integrated real-time and non real-time service wireless mobile network. The total channels of each cell are divided into three parts, one is for real-time service calls only, the second is for non real-time service calls only, and the last one is for overflowed handoff requests that can not be served in the first two parts. Out of third part, a few channels are reserved exclusively for the real-time handoffs service. To give the real-time service handoff requests higher priority over non-real-time service handoff requests, the real-time service handoff request is allowed to preempt the non-real-time service call when it finds no channel available on its arrival. The interrupted non-real-time service call returns back to the non-real-time service handoff request queue. The system is modeled by a multi-dimensional Markov chain and a numerical analysis is presented to estimate blocking probabilities of originating calls, forced termination probability of real-time service handoff requests calls, and average transmission delay of non-real-time service calls. This scheme is also simulated using extensive runs and both results are observed to agree fairly well. It is seen that the forced termination probability of real-time service calls is significantly reduced with our scheme while the probability of packet loss of non-real-time transmission is made to be negligibly small, as a non- real-time service handoff request can be transferred from the queue of current base station to another one.","Performance analysis,
Telecommunication traffic,
Delay estimation,
Traffic control,
Mobile computing,
Computer science,
Numerical analysis,
Delay effects,
Propagation losses,
Base stations"
Evolving cyclic control for a hexapod robot performing area coverage,"For a robot to search an entire area, it must follow a path that allows the range of its sensors to cover all parts of the area. This problem is a subset of path planning called area coverage. Most work done in this type of path planning has concentrated on ways of dividing the area up to avoid obstacles while covering the area. This is an important step in the process, but often takes for granted the movement of the robot within clear areas. This is not a problem if the robot has sufficient calibration to ensure the accuracy of calculated turns or if it has accurate enough navigational devices to keep track of its location. However, simple legged robots usually lack both of these attributes. It is difficult to make turns that fit a specified arch and sufficient on board navigational devices are expensive and/or too large to carry. In this paper, we use cyclic genetic algorithms to learn the control cycles required to make an actual hexapod robot perform area coverage.","Robot sensing systems,
Path planning,
Mobile robots,
Dead reckoning,
Testing,
Navigation,
Actuators,
Computer science,
Educational institutions,
Calibration"
Region segmentation via deformable model-guided split and merge,"An improved method for deformable shape-based image segmentation is described. Image regions are merged together and/or split apart, based on their agreement with an a priori distribution on the global deformation parameters for a shape template. Perceptually-motivated criteria are used to determine where/how to split regions, based on the local shape properties of the region group's bounding contour. A globally consistent interpretation is determined in part by the minimum description length principle. Experiments show that model-guided split and merge yields a significant improvement in segmention over a method that uses merging alone.","Deformable models,
Shape,
Image segmentation,
Humans,
Merging,
Image retrieval,
Content based retrieval,
Computer science,
Object detection,
Indexing"
Countering denial-of-service attacks using congestion triggered packet sampling and filtering,"Denial-of-service (DoS) attacks have received a great amount of attention in research communities and general public alike, due to recent, high-profile attacks against major Internet e-commerce sites. We present a countermeasure against such attacks, called the congestion-triggered packet sampling/packet filtering (CTPS/PF) architecture. With CTPS/PF, a packet sampling mechanism that is integrated with the congestion control mechanism at routers is used to detect DoS attacks, and packet filters are activated only when sampling results warrant action. One important concern in deploying any form of traffic analysis in the critical data-forwarding paths of the Internet is performance. Our sample processing algorithm takes into account the confidence indicators of statistic results to raise alarms with relatively small numbers of samples. Moreover, the per-sample processing complexity is only O(1). Our simulation study reveals that the CTPS/PF architecture is able to detect the presence of DoS attacks and take proper action within hundreds of milliseconds to tens of seconds. Moreover, the average sampling overhead during a congestion period is in the vicinity of 1 sample per second.","Computer crime,
Sampling methods,
Information filtering,
Information filters,
Internet,
Computer science,
Electronic mail,
Multiprotocol label switching,
Traffic control,
Performance analysis"
Stages of autonomy determination,"We discuss stages of autonomy determination for software agents that manage and manipulate knowledge in organizations that house other software agents and human knowledge workers. We suggest recognition of potential autonomies in the belief-desire-intention (BDI) paradigm and actual reasoning about autonomy choice decision theoretically. We show how agents might revise their autonomies in light of one another's autonomy and might also experience new, derived autonomies. We discuss the conditions under which an entire group of agents might have a collective autonomy attitude toward agents outside their group. We believe group attitudes are a novel concept and form a strong basis for developing theories of dynamic organizational structure. We briefly sketch an outline of a case study that motivates reasoning about autonomies.","Knowledge management,
Software agents,
Humans,
Collaboration,
Multiagent systems,
Technology management,
Sociology,
Computer science,
Biology computing,
Power engineering computing"
Segmentation of multispectral MR images using a hierarchical self-organizing map,"The application of a hierarchical self-organizing map (HSOM) to the problem of segmentation of multispectral magnetic resonance (MR) images is investigated. The HSOM is composed of several layers of self-organizing maps (SOMs) organized in a pyramidal fashion. SOMs have previously been used for the segmentation of multispectral MR images, but the results often suffer from under-segmentation or over-segmentation. By combining the concepts of self-organization and topographic mapping with multi-scale image segmentation, the HSOM is shown to overcome the major drawbacks of the SOM. The segmentation results of the HSOM are compared with those of the SOM and the k-means clustering algorithm on multispectral MR images of the human brain representing both normal conditions and pathological conditions, such as multiple sclerosis. The multi-scale segmentation results of the HSOM are shown to have interesting consequences from the viewpoint of the clinical diagnosis of pathological conditions.","Image segmentation,
Magnetic resonance imaging,
Biomedical imaging,
Protons,
Volume relaxation,
Magnetic resonance,
Humans,
Magnetic fields,
Phased arrays,
Computer science"
Multimedia distance learning without the wait,"Web-linked digital video disks provide high-quality interactive multimedia presentations and virtual laboratories with Web-access for timely materials and collaboration. The authors present techniques for presentation, interaction, and assessment. Experimental modules using these techniques have been developed using multimedia systems as content. New assessment methods allow materials to be dynamically modified to satisfy different learner backgrounds and objectives. A new software tool has been developed for gathering user statistics to best adapt the curriculum as well as evaluate the overall learning efficiency across diverse learner populations. The project is being disseminated at the University of Massachusetts, Springfield Technical Community College, Smith College, ENST/Paris and the National Technological University.","Computer aided instruction,
Internet,
DVD,
Timing,
Multimedia systems,
Educational institutions,
Bandwidth,
Web server,
HTML,
Laboratories"
Online prediction of the running time of tasks,"We describe and evaluate the Running Time Advisor (RTA), a system that can predict the running time of a compute-bound task on a typical shared, unreserved commodity host. The prediction is computed from linear time series predictions of host load and takes the form of a confidence interval that neatly expresses the error associated with the measurement and prediction processes, error that must be captured to make statistically valid decisions based on the predictions. Adaptive applications make such decisions in pursuit of consistent high performance, choosing, for example, the host where a task is most likely to meet its deadline. We begin by describing the system and summarizing the results of our previously published work on host load prediction (P.A. Dinda, 1999; 2000)We then describe our algorithm for computing predictions of running time from host load predictions. Finally, we evaluate the system using over 100000 randomized testcases run on 39 different hosts.","Distributed computing,
Application software,
Computer science,
Time sharing computer systems,
Time measurement,
Prediction algorithms,
System testing,
Visualization,
Computer applications,
Processor scheduling"
Multi-object tracking using dynamical graph matching,"We describe a tracking algorithm to address the interactions among objects, and to track them individually and confidently via a static camera. It is achieved by constructing an invariant bipartite graph to model the dynamics of the tracking process, of which the nodes are classified into objects and profiles. The best match of the graph corresponds to an optimal assignment for resolving the identities of the detected objects. Since objects may enter/exit the scene indefinitely, or when interactions occur/conclude they could form/leave a group, the number of nodes in the graph changes dynamically. Therefore it is critical to maintain an invariant property to assure that the numbers of nodes of both types are kept the same so that the matching problem is manageable. In addition, several important issues are also discussed, including reducing the effect of shadows, extracting objects' shapes, and adapting large abrupt changes in the scene background. Finally, experimental results are provided to illustrate the efficiency of our approach.","Shape,
Target tracking,
Cameras,
Layout,
Real time systems,
Particle tracking,
Information science,
Bipartite graph,
Object detection,
Predictive models"
Mobile antennas for reception of S-DARS,"Despite the difficult specification written for the mobile antenna and LNA to receive inclined elliptical orbit S-DARS satellite signals, several suppliers have successfully demonstrated various solutions, easily attaining the -19 dB/K G/T requirement over a wide (130/spl deg/) elevation beamwidth. Antenna size is generally in the order of 1/4 to 1/2 /spl lambda/, and the integrated antenna and LNA at S-band is about the size of a computer mouse. This seems compact enough for most customers to mount on the roof of a small car, or other vehicle. The terrestrial antenna is integrated into the same radome as the satellite antenna, with little or no impact upon the appearance of the antenna module.","Mobile antennas,
Satellite broadcasting,
Satellite antennas,
Receiving antennas,
Transmitting antennas,
Low-noise amplifiers,
Bandwidth,
Radio spectrum management,
USA Councils,
Repeaters"
Image segmentation using local spectral histograms,"We propose a new algorithm for image segmentation. We use the spectral histogram, which is a vector consisting of marginal distributions of responses from chosen filters as a generic feature for texture as well as intensity images. Motivated by a new segmentation energy functional, we derive an iterative and deterministic approximation algorithm for segmentation. Based on the relationships between different scales and neighboring windows, we also develop an algorithm which can automatically detect homogeneous regions in an input image, which may consist of texture regions. To reduce the boundary uncertainty due to the large spatial window used for spectral histograms, we propose a novel local feature by building precise probability models based on current segmentation results. We have applied our algorithm to intensity, texture, and natural images and obtained good results with accurate texture boundaries.","Image segmentation,
Histograms,
Iterative algorithms,
Statistics,
Filters,
Windows,
Computer science,
Uncertainty,
Partitioning algorithms,
Energy resolution"
Automatic simplification of particle system dynamics,"We present a novel framework for automatically simplifying the dynamics computation of particle systems to improve simulation speeds. Our approach is based on a physically-based subdivision scheme to generate a hierarchy of approximated motion models or simulation levels of detail (SLOD). At each time step, the SLODs are updated on-the-fly, and the appropriate SLOD is chosen adaptively to reduce computational costs. We have tested a prototype implementation on the simulation of a water fountain and a galaxy system. The preliminary results show a significant performance gain on these scenarios with little loss in the visual appearance of the simulation, indicating the potential to generalize this approach to other dynamical systems.","Computational modeling,
Acceleration,
Rendering (computer graphics),
Physics computing,
Computer graphics,
Layout,
Discrete event simulation,
Solid modeling,
Biological system modeling,
Computer science"
NMR signal enhancement via a new time-frequency transform,"In this paper, a reliable method to reduce the noise from nuclear magnetic resonance (NMR) signals using a recently developed linear critically sampled time-frequency transform is proposed. In addition to its low computational requirements, this transform has many theoretical advantages that make it a good candidate for NMR signal enhancement. NMR signals in the transform domain are concentrated in a few coefficients while the noise is well distributed. Performing a thresholding technique in the transform domain, therefore, significantly enhances the signal. A comparison with other signal enhancement techniques shows that this technique has a superior performance, thus confirming the theoretical expectations.","Nuclear magnetic resonance,
Time frequency analysis,
Wavelet transforms,
Magnetic resonance,
Biological system modeling,
Noise reduction,
Magnetic noise,
Spectroscopy,
Magnetic resonance imaging,
Parameter estimation"
Mosaics of video sequences with moving objects,"We propose an efficient method for creating a mosaic of a video sequence in the presence of moving objects. This method includes two principal processes. The first one removes the moving objects from the background, and as a side effect, obtains the global motion. This global motion provides a good initial estimation to the next stage. Second, we employ a feature-based technique to derive the precise global motion with eight projective parameters. The performance of our work is demonstrated by experiments.",
Ligand binding with OBPRM and user input,"We present a framework for studying ligand binding which is based on techniques recently developed in the robotics motion planning community. We are interested in locating binding sites on the protein for ligand molecule. Our work investigates the performance of a fully automated motion planner, as well as the effects of supplementary user input collected using a haptic device. Our results applying an obstacle-based probabilistic roadmap motion planning algorithm (OBPRM) to some protein-ligand complexes are encouraging. The framework successfully identified potential building sites for all complexes studied. We find that user input helps the planner, and haptic device helps the user to understand the protein structure by enabling them to feel the difficult-to-visualize forces.","Proteins,
Testing,
Drugs,
Potential energy,
Computer science,
Robotics and automation,
Motion planning,
Haptic interfaces,
Electrostatic measurements,
Position measurement"
Using robots in an undergraduate artificial intelligence course: an experience report,"In this paper, we report our experience using robots in the artificial intelligence course we taught in Fall 2000. Our objective was to use robots to reinforce the traditional concepts of search and expert systems. We wanted the robots to be simple to build, yet powerful enough to illustrate AI concepts. In this paper, we discuss our choice of robot, describe the projects we assigned and list the problems our students encountered carrying out those projects. We surveyed our class regarding the use of robots in this course at the end of the semester. We discuss the results of this survey, which we believe, make a strong case for using robots in the AI course.","Educational robots,
Intelligent robots,
Artificial intelligence,
Robot sensing systems,
Mobile robots,
Books,
Expert systems,
Art,
Educational institutions,
Wheels"
On representations of algebraic-geometry codes,"We show that all algebraic-geometric codes possess a succinct representation that allows for list decoding algorithms to run in polynomial time. We do this by presenting a root-finding algorithm for univariate polynomials over function fields when their coefficients lie in finite-dimensional linear spaces, and proving that there is a polynomial size representation, given which the root-finding algorithm runs in polynomial time.",
"Teaching introductory programming, problem solving and information technology with robots at West Point","As part of an ongoing initiative to continually revise and improve its introductory computer science courses, the Electrical Engineering and Computer Science Department at the United States Military Academy has added the use of LEGO Mindstorm robots as part of the active-learning environment used to teach Information Technology (IT) and programming basics. It is critical for the Army and the Nation that its future leaders understand and are capable of taking advantage of IT. All cadets at the United States Military Academy at West Point are required to take a course on IT and problem solving using computer programming. This course is an important first, and sometimes only, opportunity to expose undergraduate students to technology and concepts that will be a part of their daily lives and future careers. The LEGO Mindstorm robots are used in the introductory computer science course to teach fundamental, computer programming concepts and introduce the concepts of autonomous vehicles, embedded computer systems and computer simulation. The positive short-term impact on the students taking the course has been substantial and while the long-term impact has yet to be measured, it also has the potential to be substantial. Members of the faculty at West Point developed a computer simulation of the robot environment as well as a Java programming language translator for the LEGO programmable brick, called the RCX. These two tools are combined into a unique programming and teaching environment that we named Jago. Jago enables the robots and robot simulator to be used to teach fundamental programming concepts visually, which some students can more easily grasp and all are clearly excited to use. Based on these results we have incorporated Jago into the core IT course taught at West Point.",
On the equivalence of classes of hybrid dynamical models,"We establish equivalences among five classes of hybrid systems, that we have encountered in previous research: mixed logical dynamical systems, linear complementarity systems, extended linear complementarity systems, piecewise affine systems, and max-min-plus-scaling systems. These results are of paramount importance for transferring properties and tools from one class to another.","Computer science,
Control systems,
Control system analysis,
Control design,
Linear systems,
Logic,
Automata,
State estimation,
Fault detection,
Safety"
Detecting network intrusions via a statistical analysis of network packet characteristics,"With the growing threat of abuse of network resources, it becomes increasingly important to be able to detect malformed packets on a network and estimate the damage they fan cause. In this paper, we collect and analyze all of the IP and TCP packets seen on a network that either violate existing standards or should not appear in modern internets. Our goal is to determine what these suspicions packets mean and evaluate what proportion of such packets can cause actual damage. Thus, we divide unusual packets obtained during our experiments into several categories depending on the severity of their consequences, including indirect consequences as a result of information gathering, and show the results. The traces analyzed were gathered at Ohio University's main Internet link, providing a massive amount of statistical data.","Intrusion detection,
Statistical analysis,
TCPIP,
Monitoring,
Data analysis,
Performance analysis,
Computer science,
Computer crashes,
Modems,
IP networks"
Virtual source based multicast routing in WDM networks with sparse light splitting,"Wavelength-division multiplexed (WDM) networks using wavelength-routing are considered to be potential candidates for the next generation wide-area backbone networks. This paper concerns with the problem of multicast routing in WDM networks. A node in the network may have both wavelength conversion and splitting capabilities which is called as virtual source. It is assumed that virtual sources are limited in number and are distributed evenly in the network. This paper proposes a new approach, which makes use of these special capable virtual sources to construct a multicast tree.",
Approximation algorithms for data distribution with load balancing of web servers,,"Approximation algorithms,
Load management,
Web server,
Network servers,
Memory management,
Delay,
Mirrors,
Clustering algorithms,
Computer science,
Costs"
Internet-based learning by doing,"This paper presents the current trends in Internet-based training by experimental work. The authors show how to apply the ""learning by doing"" paradigm in Internet-based distance learning, both for academic educational environments and life-long training systems, taking into account available computer and network resources. Firstly, the different phases in the learning process are introduced. The aim of this introduction is to show to the readers the importance of the learning by doing paradigm, which is not implemented in many Internet-based educational environments. Then, they identify the most important trends in this field which can be classified into two main groups. The first one consists of accessing the real equipment through an Internet interface. The second is based on simulation, very often, Java-based simulation. Both approaches are discussed, including brief descriptions of currently available systems that implement them. Finally, these approaches are compared from different points of view. They point out the most significant variables to bear in mind and, as the readers may find a tradeoff between some of them, they also provide a graphical guide to help them in their choice.",Continuing education
Delay analysis for CBR traffic under static-priority scheduling,"We examine the delay performance of packets from constant-bit-rate (CBR) traffic whose delay is affected by non-real-time traffic. The delay performance is analyzed by solving the nD/D/1 queue with vacations. We obtain an exact and closed form solution, hence obviating the need of any approximations or numerical Laplace inversions. We then provide various numerical results for low-bit-rate transmission links, in which packets can experience large delay. From our quantitative evaluation, we conclude that there exists an optimum packet size for a given delay bound. In extremely slow links, such as modem links, transmission control protocol (TCP) packets should be segmented to reduce the CBR delay. We therefore investigate the delay impact of TCP packet sizes as well.",
Three Generations of Automatically Designed Robots,"The difficulties associated with designing, building, and controlling robots have led their development to a stasis: Applications are limited mostly to repetitive tasks with predefined behavior. Over the last few years we have been trying to address this challenge through an alternative approach: Rather than trying to control an existing machine or create a general-purpose robot, we propose that both the morphology and the controller should evolve at the same time. This process can lead to the automatic design of special-purpose mechanisms and controllers for specific short-term objectives. Here we provide a brief review of three generations of our recent research, which underlies the robots shown on the cover of this issue: Automatically designed static structures, automatically designed and manufactured dynamic electromechanical systems, and modular robots automatically designed through a generative DNA-like encoding.","computer aided design,
robotics,
artificial life,
coevolution,
physical simulation,
automatic manufacturing"
Performance evaluation of a robust method for mathematical expression recognition,"We proposed two methods for mathematical expression recognition. One is based on projection profile cutting and the other uses top-down and bottom-up strategies to analyze the two-dimensional structure of expressions. The paper describes the improvement of the latter method in terms of structural analysis robustness and application to matrix recognition. To evaluate the performance of our method, intensive experiments were carried out on a large variety of mathematical expression images which were collected from many mathematical journals.","Robustness,
Character recognition,
Image recognition,
Computer science,
Design methodology,
Labeling"
A novel collision detection method based on enclosed ellipsoid,"The paper introduces an efficient collision detection method for convex polyhedra in a three-dimensional workspace. In cooperation with the enclosing and the enclosed ellipsoids of convex polyhedra, potential collisions can be detected more accurate than those methods using only bounding ellipsoids for object representation, and more efficient than the polyhedral methods. An approach for computing the enclosed ellipsoid of a convex polyhedron by compressing, stretching and scaling operations on its best-fit enclosing ellipsoid is also introduced in this content. Graphical simulation of two robot manipulators moving in a share workspace was conducted to demonstrate the effectiveness of the proposed algorithm for collision detection.","Ellipsoids,
Object detection,
Solid modeling,
Robot kinematics,
Tree data structures,
Manipulator dynamics,
Information science,
Computational modeling,
Computer simulation,
Operating systems"
Noise properties of low-dose CT projections and noise treatment by scale transformations,"Projection data acquired for image reconstruction of low-dose computed tomography (CT) are degraded by many factors. These factors complicate noise analysis on the projection data and render a very challenging task for noise reduction. In this study, we first investigate the noise property of the projection data by analyzing a repeatedly acquired experimental phantom data set, in which the phantom was scanned 900 times at a fixed projection angle. The statistical analysis shows that the noise can be regarded as normally distributed with a nonlinear signal-dependent variance. Based on this observation, we then utilize scale transformations to modulate the projection data so that the data variance can be stabilized to be signal independent. By analyzing the relationship between the data standard deviation and the data mean level, we propose a segmented logarithmic transform for the stabilization of the non-stationary noise. After the scale transformations, the noise variance becomes approximately a constant. A two-dimensional Wiener filter is then designed for an analytical treatment of the noise. Experimental results show that the proposed method has a better noise reduction performance without circular artifacts, by visual judgment, as compared to conventional filters, such as the Harming filter.","Computed tomography,
Noise reduction,
Wiener filter,
Imaging phantoms,
Image reconstruction,
Degradation,
Rendering (computer graphics),
Data analysis,
Statistical analysis,
Noise level"
Educating software engineering students to manage risk,"In 1996, the University of Southern California (USC) switched its core two-semester software engineering course from a hypothetical-project, homework-and-exam course based on the Bloom taxonomy of educational objectives (knowledge, comprehension, application, analysis, synthesis and evaluation). The revised course is a real-client team-project course based on the CRESST (Center for Research on Evaluation, Standards and Student Testing) model of learning objectives (content understanding, problem solving, collaboration, communication and self-regulation). We used the CRESST cognitive demands analysis to determine the necessary student skills required for software risk management and the other major project activities, and have been refining the approach over the last four years of experience, including revised versions for one-semester undergraduate and graduate project courses at Columbia University. This paper summarizes our experiences in evolving the risk management aspects of the project courses. These have helped us mature more general techniques, such as risk-driven specifications, domain-specific simplifier and complicator lists, and the SAIV (schedule as an independent variable) process model. The largely positive results in terms of review pass/fail rates, client evaluations, product adoption rates and hiring manager feedback are summarized as well.","Software engineering,
Risk management,
Engineering management,
Taxonomy,
Application software,
Communication standards,
Automatic testing,
Problem-solving,
Collaboration,
Risk analysis"
Comparing adaptation techniques for on-line handwriting recognition,This paper describes an online handwriting recognition system with focus on adaptation techniques. Our hidden Markov model (HMM)-based recognition system for cursive German script can be adapted to the writing style of a new writer using either a retraining depending on the EM (expectation maximization)-approach or an adaptation according to the MAP (maximum a posteriori) or MLLR (maximum likelihood linear regression)-criterion. The performance of the resulting writer-dependent system increases significantly even if the amount of adaptation data is very small (about 6 words). So this approach is also applicable for online systems in hand-held computers such as PDAs. Special attention was paid to the performance comparison of the different adaptation techniques with the availability of different amounts of adaptation data ranging from a few words tip to 100 words per writer.,"Handwriting recognition,
Hidden Markov models,
Writing,
Maximum likelihood linear regression,
Personal digital assistants,
Error analysis,
Character recognition,
Databases,
Computer science,
Linear regression"
Yoopeedoo (UPEDU): a process for teaching software process,"The software engineering process is a growing concern for many software development organizations. The need for well-educated software engineers is bringing new software engineering programs to universities. In many programs, software process education adds up to a few hours of lectures in an introductory software engineering course. This paper presents the structure and the content for a full, one-semester course on software processes, which has been designed in close collaboration with industry. The course is based on a software process called UPEDU (Unified Process for EDUcation), pronounced Yoopeedoo, and has been customized from the Rational Unified Process (RUP) for the educational environment. Many artifacts derived from a project case study are used as examples or templates. The content of the course is oriented towards the cognitive skills needed to perform the various activities required in the software process.","Education,
Coordinate measuring machines,
Software engineering,
Capability maturity model,
Programming,
Educational institutions,
Computer industry,
ISO standards,
IEC standards,
Software tools"
Natural and simulated annealing,,"Simulated annealing,
Cost function,
Temperature,
Random number generation,
Optimization methods,
Data structures,
Space exploration,
Processor scheduling,
Scheduling algorithm,
Mirrors"
Retiming synchronous data-flow graphs to reduce execution time,"Many common iterative or recursive DSP applications be can represented by synchronous data-flow graphs (SDFGs). A great deal of research has been done attempting to optimize such applications through retiming. However, despite its proven effectiveness in transforming single-rate data-flow graphs to equivalent DFGs with smaller clock periods, the use of retiming for attempting to reduce the execution time of synchronous DFGs has never been explored. In this paper, we do just this. We develop the basic definitions and results necessary to express and study SDFGs. We review the problems faced when attempting to retime an SDFG in order to minimize clock period and then present algorithms for doing this. Finally, we demonstrate the effectiveness of our methods on several examples.","Digital signal processing,
Clocks,
Delay,
Computer science,
Signal processing algorithms,
Time factors,
Process design,
Signal design,
Signal processing,
Programming environments"
RPack: routability-driven packing for cluster-based FPGAs,Routing tools consume a significant portion of the total design time. Considering routability at earlier steps of the CAD flow would both yield better quality and faster design process. We present a routability-driven clustering method for cluster-based FPGAs. Our method packs LUTs into logic clusters while incorporating routability metrics into a cost function. The objective is to minimize this routability cost function. Our cost function is consistently able to indicate improved routability. Our method yields up to 50% improvement over existing clustering methods in terms of the number of routing tracks required. The average improvement obtained is 16.5%. Reduction in number of tracks yields reduced routing area.,"Field programmable gate arrays,
Table lookup,
Routing,
Integrated circuit interconnections,
Logic arrays,
Computer science,
Design automation,
Clustering methods,
Cost function,
Logic circuits"
Adapting the Web interface: an adaptive Web browser,"The growing number of mobile computing devices with diverse characteristics creates a requirement for seamless (device independent) access to computing resources of distributed systems. One of the most common applications in distributed systems is the Web browser, which is not only used to access resources on the Internet but also as an interface to many information system applications. The authors address types of adaptation that can be applied to a Web browser in response to diverse context changes, including changes in available computing resources, input and output device capabilities, network characteristics, location and use context. We also present a design and implementation of a Web browser that adapts to changes in its network and computing environment by exploiting context metadata.","Computer networks,
Availability,
Application software,
Mobile computing,
Distributed computing,
User interfaces,
Prototypes,
Access protocols,
Pervasive computing,
Computer science"
Optimized readout of small gamma cameras for high resolution single gamma and positron emission imaging,"Using a novel resistive readout scheme and off-the-shelf PCI ADC data acquisition cards, a compact readout and control system for mini gamma cameras based on position sensitive photomultiplier tubes (PMTs) has been developed. This economical resistive readout incorporates a special fractional subtraction technique to greatly expand the useful field of view of the PMTs to the very edges of their active area as compared to standard resistive readout methods such as charge division or Anger logic. This allows one to obtain the same quality images as those obtained with much more expensive and bulky individual wire readout data acquisition systems. The four output signals from the readout circuit can be digitized by a PCI ADC card in a computer to achieve high counting rates. Results will be shown for this readout applied to a dual modality breast biopsy system currently undergoing clinical trials and an ultra high resolution single gamma small animal imaging system for studies on mice and rats.","Cameras,
Data acquisition,
Control systems,
Photomultipliers,
Subtraction techniques,
Logic,
Wire,
Circuits,
Breast biopsy,
Clinical trials"
Data visualization: parallel coordinates and dimension reduction,"Visualization techniques deal with multidimensional multivariable data sets. We introduce visualization methods for multidimensional data sets, including an effective dimension reduction method for the multivariate genetic algorithm data set.",
Autonomous decentralized database system for assurance in heterogeneous e-business,"Due to the advancement in the Information Technology, different kind of companies with heterogeneous needs have had the necessity to cooperate among them to get more benefit in a continuous changing market. That is heterogeneous e-business. In order to cope with such kind of e-business, a platform which can effectively realize heterogeneous needs, real time, flexibility and fault-tolerance is needed. This paper focuses on the database field that is indispensable for the e-business. Thus, it proposes Autonomous Decentralized Database System, which is composed of two techniques. In the first one, the concept of Allowable Volume (AV) is introduced, while in the second one, a Mobile Agent (MA) that adjusts the AV of each site is shown. Since the MA is always moving among the sites, and each site can always negotiate AV with the MA autonomously, the system can realize assurance. Furthermore, the effectiveness of the proposed system is shown by simulation.",
Subband adaptive generalized sidelobe canceller for broadband beamforming,"We propose a novel subband adaptive broadband beamforming architecture based on the generalised sidelobe canceller (GSC), in which we decompose each of the tapped delay-line signals feeding the adaptive part of the GSC and the reference signal into subbands and perform adaptive minimisation of the mean squared error in each subband independently. Besides its lower computational complexity, this new subband adaptive GSC outperforms its fullband counterpart in terms of convergence speed because of its prewhitening effect. Simulations based on different kinds of blocking matrices with different orders of derivative constraints are presented to support these findings.","Array signal processing,
Sensor arrays,
Filters,
Delay,
Computational complexity,
Computational modeling,
Computer science,
Computer architecture,
Convergence,
Sonar"
The natural work-stealing algorithm is stable,"In this paper we analyse a very simple dynamic work-stealing algorithm. In the work-generation model, there are n generators which are arbitrarily distributed among a set of n processors. During each time-step, with probability /spl lambda/, each generator generates a unit-time task which it inserts into the queue of its host processor. After the new tasks are generated, each processor removes one task from its queue and services it. Clearly, the work-generation model allows the load to grow more and more imbalanced, so, even when /spl lambda/<1, the system load can be unbounded. The natural work-stealing algorithm that we analyse works as follows. During each time step, each empty processor sends a request to a randomly selected other processor. Any non-empty processor having received at least one such request in turn decides (again randomly) in favour of one of the requests. The number of tasks which are transferred from the non-empty processor to the empty one is determined by the so-called work-stealing function f. We analyse the long-term behaviour of the system as a function of /spl lambda/ and f. We show that the system is stable for any constant generation rate /spl lambda/<1 and for a wide class of functions f. We give a quantitative description of the functions f which lead to stable systems. Furthermore, we give upper bounds on the average system load (as a function of f and n).","Algorithm design and analysis,
Load management,
Computer science,
Heuristic algorithms,
Load modeling,
Upper bound,
Distributed computing,
Contracts,
Parallel programming,
Kernel"
A web-based testing system with dynamic question generation,"The use of the web for a variety of educational needs has exploded. It provides one approach in the increasing use of electronic media. HTML-the ""Lingua Franca"" of the Internet-provides great platform independence. However, for mathematically intensive applications as seen in Engineering, HTTP and HTML are not able to compete with nonweb computer based programs. At least not without server-side or client-side enhancement. In the paper, we describe one solution to the problem of presenting students with dynamically generated browser-based exams with significant engineering mathematics content. This proof of concept is called WTML, Web Testing Markup Language. WTML is an extension of HTML. The web-server is extended to process the WTML and support features necessary for mathematics exams. The result is then supplied to the client (student's browser). The student completes an exam and returns it for grading and database recording. The system is used in a calculus class and then assessed.","System testing,
Mathematics,
HTML,
Web server,
Internet,
Markup languages,
Spatial databases,
Calculus,
Computer aided instruction,
Automatic testing"
Power electronics and basic electronics real experiments through the World Wide Web,"The increasing diffusion and development of information and communication technologies has allowed the introduction of new techniques, such as virtual laboratories, in the world of research and education. The main feature of these laboratories is the remote control of the applications that are developed in the local laboratory and the remote data acquisition and treatment that can be carried out from any point of the planet. This paper focuses on the new equipment, based on specific software and hardware, dedicated to the power electronics and basic electronics teaching through remote HTTP network access. Besides, a novel methodology based on real-time data transfer and interactive applications is presented here. This methodology has been developed on a set of virtual instruments designed over Lab View.",
Haptic Representation of Elastic Objects,"In this paper, a method for visually and haptically representing elastic objects in a virtual environment is proposed. A linear FEM (finite-element method) model is employed to define the elasticity of an object. By computing the inverse stiffness matrix in advance, the force and the displacement according to the interaction are obtained in real time using the precomputed inverse stiffness matrix. Based on the fast-computation method of FEM, an approach for implementing an elastic virtual object is proposed. In this approach, a force interpolation method is introduced to realize a smooth change of the feedback force on the surface of objects. Moreover, the force update process and deformation process are executed asynchronously to attain a high update rate of force.",
Spike-Timing-Dependent Hebbian Plasticity as Temporal Difference Learning,"A spike-timing-dependent Hebbian mechanism governs the plasticity of recurrent excitatory synapses in the neocortex: synapses that are activated a few milliseconds before a postsynaptic spike are potentiated, while those that are activated a few milliseconds after are depressed. We show that such a mechanism can implement a form of temporal difference learning for prediction of input sequences. Using a biophysical model of a cortical neuron, we show that a temporal difference rule used in conjunction with dendritic backpropagating action potentials reproduces the temporally asymmetric window of Hebbian plasticity observed physiologically. Furthermore, the size and shape of the window vary with the distance of the synapse from the soma. Using a simple example, we show how a spike-timing-based temporal difference learning rule can allow a network of neocortical neurons to predict an input a few milliseconds before the input's expected arrival.",
Flight simulation in synthetic environments,"The VRAC, has the mission of applying virtual reality technology to the challenges of engineering and science. We typically create virtual worlds that allow the user to interact with 3D real-time graphics. We refer to the virtual worlds enabling this interaction as synthetic environments. A synthetic environment produces sufficient sensory cues to make users believe they are in a different geographic location with a different velocity and orientation than they have in the real world. Classical flight simulators are a subset of synthetic environments. Although they do not usually use stereoscopic displays, they do use large FOV visual systems, digital audio, force feedback via the flight controls, and force feedback on the simulator as a whole via a motion base. We believe the addition of stereoscopic 3D graphics will be particularly valuable in hovering, formation flying, and landing. VRAC uses two surround screen environments, the C2 and the C6. The C2 came on line in 1995. It is a room with four projected surfaces, three walls and the floor. The C6, which came on line in June 2000, is a cube-shaped room, the first of its kind in the US, with 3D graphics projected on all four walls, the floor, and the ceiling. The C2 and C6 are linked by a dedicated fiber network. This will enable collaboration between geographically separated virtual worlds. Many researchers believe that collaboration between geographically separate synthetic environments is the key to future breakthroughs in the use of SE technology. The C2/C6 combination will be an ideal testbed to support this kind of collaboration. In addition, the C2 and C6 are connected to a 250-person auditorium that has dual screens capable of simultaneously displaying real-time images from two sources. Our vision is simultaneous display of real-time graphics from the C2 and the C6 in the auditorium, enabling up to 250 people to study and, perhaps, participate in the real-time interactions. As quality of service, bandwidth, and latency challenges are met by emerging technology, we also look forward to interconnecting the C2, the C6, and the auditorium with off campus simulators.","Aerospace simulation,
Graphics,
Collaboration,
Force feedback,
Floors,
Virtual reality,
Aerospace engineering,
Auditory displays,
Visual system,
Aerospace control"
Design and implementation of a composable reflective middleware framework,"With the evolution of the global information infrastructure, service providers will need to provide effective and adaptive resource management mechanisms that can serve more concurrent clients and deal with applications that exhibit quality-of-service (QoS) requirements. Flexible, scalable and customizable middleware can be used as an enabling technology for next-generation systems that adhere to the QoS requirements of applications that execute in highly dynamic distributed environments. To enable application-aware resource management, we are developing a customizable and composable middleware framework called CompOSE|Q (Composable Open Software Environment with QoS), based on a reflective meta-model. In this paper, we describe the architecture and runtime environment for CompOSE|Q and briefly assess the performance overhead of the additional flexibility. We also illustrate how flexible communication mechanisms can be supported efficiently in the CompOSE|Q framework.","Middleware,
Resource management,
Quality of service,
Application software,
Intelligent transportation systems,
Access protocols,
Distributed computing,
Software safety,
Computer science,
Runtime environment"
Teaching real-time with a scheduler simulator,"In this paper we describe a scheduler simulator for real-time tasks, RTsim, that can be used as a tool to teach real-time scheduling algorithms. It simulates a variety of preprogrammed scheduling policies for single and multiprocessor systems and simple algorithm variants introduced by its user. Using RTsim students can conduct experiments that will allow them to understand the effects of each policy given different load conditions and learn which policy is better for different workloads. We show how to use RTsim as a learning tool and the results achieved with its application on the real-time systems course taught at the B.Sc. on Computer Science at Paulista State University-Unesp-at Rio Preto.","Education,
Processor scheduling,
Real time systems,
Scheduling algorithm,
Computational modeling,
Computer science,
Hardware,
Control systems,
Petri nets,
Computer simulation"
A Comparative Study of Feature-Salience Ranking Techniques,"We assess the relative merits of a number of techniques designed to determine the relative salience of the elements of a feature set with respect to their ability to predict a category outcome-for example, which features of a character contribute most to accurate character recognition. A number of different neural-net-based techniques have been proposed (by us and others) in addition to a standard statistical technique, and we add a technique based on inductively generated decision trees. The salience of the features that compose a proposed set is an important problem to solve efficiently and effectively, not only for neural computing technology but also in order to provide a sound basis for any attempt to design an optimal computational system. The focus of this study is the efficiency and the effectiveness with which high-salience subsets of features can be identified in the context of ill-understood and potentially noisy real-world data. Our two simple approaches, weight clamping using a neural network and feature ranking using a decision tree, generally provide a good, consistent ordering of features. In addition, linear correlation often works well.",
Multiple-input multiple-output (MIMO) radio channel measurements,"We present results from the first field test to characterize the mobile multiple-input multiple-output (MIMO) radio channel. We measured the capacity normalized to a single antenna system, and fading correlation between antennas of a system with 4 antennas on a laptop computer and 4 antennas at a rooftop base station. The field test results show that close to the theoretical 4 times the capacity of a single antenna system can be supported in a 30 kHz channel with dual-polarized, spatially-separated base station and terminal antennas under a variety of test runs, including suburban drives, highway drives and pedestrian routes. Therefore, these results show that it may be possible to provide in excess of 1 Mbps in a 200 kHz mobile radio channel (for the 3G wireless TDMA system EDGE) with the appropriate base station antennas.","MIMO,
Base stations,
Antenna measurements,
System testing,
Antenna theory,
Fading,
Portable computers,
Road transportation,
Land mobile radio,
Time division multiple access"
Token based group mutual exclusion for asynchronous rings,"We propose a group mutual exclusion algorithm for unidirectional rings. Our algorithm does not require the processes to have any id. Moreover, processes maintain no special data structures to implement any queues. The space requirement of processes depends only on the number of shared resources, and is equal to 4/spl times/log(m+1)+2 bits. The size of messages is 2/spl times/log(m+1) bits only. Every resource request generates O(n/sup 2/) messages in the worst case, but zero messages in the best case.","Concurrent computing,
Access protocols,
Data structures,
Message passing,
Clocks,
Computer science,
Quality of service,
Web and internet services,
Web server,
Synchronization"
Route optimization of multicast sessions in sparse light-splitting optical networks,"In this paper, we investigate the multicast routing problem in sparse splitting networks (MR-SSN). The MR-SSN problem is to find a route from the source node of a session to all destinations of the session such that the total number of fibers used in establishing the session is minimized while the multicast capable nodes are evenly distributed throughout the network. We present a heuristic based on Tabu search that requires only one transmitter for the source node and one wavelength for a multicast session in this paper. We test our heuristic on a wide range of network topologies and random sessions and conclude that the difference between our solution and ILP optimal solution in terms of the number of fibers used for establishing a multicast session is within 10% nearly all the time and within 5% in about half of the time.","Intelligent networks,
Optical fiber networks,
Routing,
Optical transmitters,
Optical devices,
Optical fiber testing,
Network topology,
Erbium-doped fiber amplifier,
Solids,
Computer science"
Handwritten Chinese character segmentation using a two-stage approach,"Correct segmentation of handwritten Chinese characters is crucial to the successful recognition. However, because of the many difficulties involved, little work has been done in this area. In this paper, a two-stage approach is addressed to segment unconstrained handwritten Chinese character strings. A string is first coarsely segmented according to the background skeleton and vertical projection after a proper image preprocessing. At the fine segmentation stage that follows, the strokes that may contain segmentation points are first identified. The feature points are then extracted from candidate strokes and taken as segmentation point candidates through each of which a segmentation path may be formed. Geometric features are extracted and fuzzy decision rules learned from examples are used to evaluate the segmentation paths. By using this two-stage segmentation approach, we can achieve both good performance and efficiency in segmenting unconstrained handwritten Chinese characters.","Image segmentation,
Character recognition,
Pattern recognition,
Handwriting recognition,
Signal processing,
Image processing,
Computer science,
Electronic mail,
Image recognition,
Skeleton"
Formulations of Support Vector Machines: A Note from an Optimization Point of View,"In this article, we discuss issues about formulations of support vector machines (SVM) from an optimization point of view. First, SVMs map training data into a higher- (maybe infinite-) dimensional space. Currently primal and dual formulations of SVM are derived in the finite dimensional space and readily extend to the infinite-dimensional space. We rigorously discuss the primal-dual relation in the infinite-dimensional spaces. Second, SVM formulations contain penalty terms, which are different from unconstrained penalty functions in optimization. Traditionally unconstrained penalty functions approximate a constrained problem as the penalty parameter increases. We are interested in similar properties for SVM formulations. For two of the most popular SVM formulations, we show that one enjoys properties of exact penalty functions, but the other is only like traditional penalty functions, which converge when the penalty parameter goes to infinity.",
Generation and analysis of hard to round cases for binary floating point division,"We investigate two sets of hard to round p/spl times/p bit fractions arising from division of a normalized p bit floating point dividend by a normalized p bit floating point divisor. These sets are characterized by the p/spl times/p bit fraction's quotient bit string, beginning with or just after the round bit, having the maximum number (p-1) of repeating like bits, specifically 00...01 or 11...10 for the directed rounding ""RD-hard"" set and 100...01 or 11...10 for the round-to-nearest ""RN/sub -/hard"" set. We show both the p/spl times/p bit RD-hard and RN-hard sets to be of size at least 2/sup p-2/ and at most 2/sup p-1/. Two dimensional quotient vs. divisor plots empirically reveal both the RD-hard and RN-hard sets of p/spl times/p bit fractions to be jointly widely distributed. Analysis of patterns and linear sequences of fractions visible in the quotient vs. divisor plots leads to simplified procedures for generating test suites of hard to round fractions. Our strongest computational result is the derivation of formulas that allow 2/sup (p/2)+O(1)/ RD-hard and RN-hard p/spl times/p bit fractions to be enumerated based on sequential incrementation of respective numerators and denominators.","Computer aided software engineering,
Testing,
Iterative algorithms,
Computer science,
Pattern analysis,
Sequential analysis,
Arithmetic,
Hardware,
Algorithm design and analysis,
Table lookup"
Quality of service routing algorithms for bandwidth-delay constrained applications,"In this paper we present bandwidth-delay constrained routing algorithms that use knowledge of the ingress-egress node pairs in the network in reducing the rejection rates for setting up new paths. Simulation is used to evaluate the algorithms and compare their performance against some existing algorithms for bandwidth constraints that have been modified to handle delay constraints. The results show that the proposed algorithms outperform all others under a wide range of workload, topology and system parameters.","Quality of service,
Routing,
Delay,
Bandwidth,
Application software,
Web and internet services,
Multiprotocol label switching,
Computer science,
Computational modeling,
Topology"
Evolving core promoter signal motifs,"DNA annotation is an important problem in molecular biology. The task in annotation is to determine functional roles of DNA regions. We are interested in core promoter regions in eukaryotes; these are approximately 50 bp in length, and are believed to act as the critical factor in attracting the transcription initiation complex to the DNA at that site, so promoting the transcription of a gene immediately downstream. The pattern of nucleotides which constitute a core promoter region is very complex, resulting in relatively poor performance from classification methods so far in terms of distinguishing true core promoter regions from other regions of DNA. The methods described both use evolutionary algorithms to optimise simple models of core promoter regions. They seem to perform comparably to the best of many published methods so far, but has the advantage of simplicity and transparency. That is, we are able to evolve easily understandable models of the core promoter region which perform comparably with the best of existing techniques. There seem to be several ways in which the scheme can be further developed.",
Using SSM proxies to provide efficient multiple-source multicast delivery,"We consider the possibility that single-source multicast (SSM) will become a universal multicast service, enabling large-scale distribution of content from a few well-known sources to a general audience. Operating under this assumption, we explore the problem of building the traditional IP model of any-source multicast on top of SSM. Toward this end, we design an SSM proxy service that allows any sender to efficiently deliver content to a multicast group. We demonstrate the performance improvements this service offers over standard SSM and describe extensions for access control, dynamic proxy discovery, and multicast proxy distribution.","Multicast protocols,
Delay,
Large-scale systems,
Computer science,
Buildings,
Access control,
Internet,
Routing protocols,
TV,
Relays"
Prediction of moving objects in dynamic environments using Kalman filters,"In this work, we describe a framework for predicting future positions and orientation of moving obstacles in a time-varying environment using Kalman filtering techniques. No constraint are placed on the obstacles motion. The proposed algorithm can be used in a variety of applications, one of which is robot motion planning in time varying environments. The advantage of using this model compared to reported ones in the literature is it's ability to start the prediction process from the first time step without the need to wait for few time steps before starting the prediction process. Early experimental results are very encouraging.",
Code placement in hardware software Co synthesis to improve performance and reduce cost,"This paper introduces an algorithm for code placement in cache, and maps it to memory using a second algorithm. The target architecture is a multiprocessor system with IS' level cache and a common main memory. These algorithms guarantee that as many instruction codewords as possible of the high priority tasks remain in cache all of the time so that other tasks do not overwrite them. This method improves the overall performance, and might result in cheaper systems if more powerful processors are not needed. Amount of memory increase necessary to facilitate this scheme is in the order of 13%. The average percentage of highest priority tasks always in memory can vary from 3% to 100% depending upon how many tasks (and their sizes) are allocated to each processor.",
On complexity analysis of supervised MLP-learning for algorithmic comparisons,"This paper presents the complexity analysis of a standard supervised MLP-learning algorithm in conjunction with the well-known backpropagation, an efficient method for evaluation of derivatives, in either batch or incremental learning mode. In particular, we detail the cost per epoch (i.e., operations required for processing one sweep of all the training data) using ""approximate"" FLOPs (floating point operations) in a typical backpropagation for solving neural networks nonlinear least squares problems. Furthermore, we identify erroneous complexity analyses found in the past NN literature. Our operation-count formula would be very useful for a given MLP architecture to compare learning algorithms.","Algorithm design and analysis,
Backpropagation algorithms,
Costs,
Neural networks,
Computer architecture,
Computer science,
Industrial engineering,
Operations research,
Training data,
Least squares methods"
Fast incremental CRC updates for IP over ATM networks,"In response to the increasing network speeds, many operations in IP routers and similar devices are being made more efficient. With the advances in other areas of packet processing, the verification and regeneration of cyclic redundancy check (CRC) codes of the data link layer is likely to become a bottleneck in the near future. In this paper, we present a mechanism to defer CRC verification without compromising reliability. This opens the possibility of incremental updates of the CRC. We introduce a new high-speed technique and present efficient implementations, speeding up CRC processing by a factor of 15. Although the paper and analysis focuses on IP over ATM, the scheme applies to a much wider set of network protocols.","Cyclic redundancy check,
Asynchronous transfer mode,
Cyclic redundancy check codes,
Protocols,
Optical packet switching,
Internet,
IP networks,
High speed optical techniques,
Ethernet networks,
Computer science"
The challenge of collaborative learning in engineering and math,"The CoWeb is a collaborative learning technology used in many classes (over 100) at Georgia Institute of Technology (Georgia Tech), USA. The authors present evidence of the success of the tool in supporting learning at a low cost. They also provide anecdotes about the active resistance they have received to use of the CoWeb in engineering, mathematics and some computer science classes. Evidence from interviews and questionnaires points to some of the sources for this resistance.","Collaborative work,
Collaboration,
Educational institutions,
Costs,
Mathematics,
Chemical technology,
Computer science,
Educational technology,
Computer science education,
Accreditation"
Pipelined heap (priority queue) management for advanced scheduling in high-speed networks,"Quality-of-service (QoS) guarantees in networks are increasingly based on per-flow queueing and sophisticated scheduling. Most advanced scheduling algorithms rely on a common computational primitive: priority queues. Large priority queues are built using calendar queue or heap data structures. To support advanced scheduling at OC-192 (10 Gbps) rates and above, pipelined management of the priority queue is needed. We present a pipelined heap manager that we have designed as a core integratable into ASICs, in synthesizable Verilog form. We discuss how to use it in switches and routers, its advantages over calendar queues, and we present cost-performance tradeoffs. Our design can be configured to any heap size. We have verified and synthesized our design and present cost and performance analysis information.","Intelligent networks,
High-speed networks,
Quality of service,
Processor scheduling,
Hardware,
Switches,
Scheduling algorithm,
Aggregates,
Technology management,
Computer science"
Navigation and comprehension of programs by novice programmers,"The purpose of this research is to examine the influence of different methods of program navigation on the mental representation and comprehension of novice procedural programmers. As a programmer tries to comprehend a program, a particular navigation method may assist or inhibit the process by highlighting, or making more accessible, certain kinds of information. Presumably, a method of navigation that highlights a certain type of information will help the programmer to better comprehend that information. In this research we study the effect of sequential, control flow, and data flow navigation methods on novices programmers' overall comprehension of a program and on the ability to comprehend specific types of information. Our results indicate that novice comprehension is facilitated by a sequential or control flow view of the program and is inhibited by a data flow view.",
Floating-point behavioral synthesis,"Traditionally, the data processed by a synthesized digital design is fixed (occasionally variable) width integer, and the functional units available are concomitantly simple ladders, subtractors, multipliers, multiplexers, and so on). The aims of paper work are two-fold: 1) to provide a library of high-level floating-point functions (trigonometric, transcendental, complex) to support the synthesis of behavioral designs incorporating complicated sets of floating-point operations and 2) to incorporate this into an optimizing behavioral synthesis environment. Floating-point units are large and cumbersome and an optimization technique that allows the internal substructures of these units to be shared (in both space and time) produces a dramatic decrease in the overall hardware resources required to support a design. The floating-point modules themselves are each implemented in several ways: as an iterative series, by table lookup, and using the coordinate rotation digital computer algorithm. The choice of implementation is left to the optimizer, which makes individual binding choices based on global knowledge of the overall design. This paper describes the library and the optimization algorithm and demonstrates the overall system use with an exemplar: a floating-point quadratic equation solver capable of delivering complex roots, realized using 30% of a Xilinx 40125XV field-programmable gate array.","Design optimization,
Hardware,
Field programmable gate arrays,
Delay,
Software libraries,
Iterative algorithms,
Integrated circuit synthesis,
Application specific integrated circuits,
Computer science,
Control system synthesis"
Compact SOP representations for multiple-output functions-an encoding method using multiple-valued logic,"This paper shows a method to represent a multiple output function: Encoded characteristic function for non-zero outputs (ECFN). The ECFN uses (n+u) binary variables to represent an n-input m-output function, where u=[log/sub 2/m]. The size of the sum-of-products expressions (SOPs) depends on the encoding method of the outputs. For some class of functions, the optimal encoding produces SOPs with O(n) products, while the worst encoding produces SOPs with O(2/sup n/) products. We formulate encoding problem and show a heuristic optimization method. Experimental results using standard benchmark functions show the usefulness of the method.","Encoding,
Programmable logic arrays,
Logic design,
Computer science,
Microelectronics,
Optimization methods,
Time division multiplexing,
Network synthesis,
Field programmable gate arrays,
Emulation"
Next-Generation Visual Supercomputing Using PC Clusters with Volume Graphics Hardware Devices,"To seek a low-cost, extensible solution for the large-scale data visualization problem, a visual computing system is designed as a result of a collaboration between industry and government research laboratories in Japan, also with participation by researchers in U.S. This scalable system is a commodity PC cluster equipped with the VolumePro 500 volume graphics cards and a specially designed image compositing hardware. Our performance study shows such a system is capable of interactive rendering 5123 and 10243 volume data and highly scalable. In particular, with such a system, simulation and visualization can be performed concurrently which allows scientists to monitor and tune their simulations on the fly. In this paper, both the system and hardware designs are presented.","Hardware,
Data visualization,
Rendering (computer graphics),
Large-scale systems,
Computer graphics,
Computational modeling,
Government,
Collaborative software,
Prototypes,
Runtime"
An effective schedulability analysis for fault-tolerant hard real-time systems,"We propose worst-case response time schedulability analysis for fault-tolerant hard real-time systems which takes into account the effects of temporary faults. The major contribution of our approach is to consider the recovery of tasks running with higher priorities. This characteristic is very useful since faulty tasks certainly have a shorter period of time to meet their deadlines. Due to its flexibility and simplicity, the proposed approach provides an effective schedulability analysis, where system predictability can be fully guaranteed.","Fault tolerant systems,
Real time systems,
Processor scheduling,
Fault tolerance,
Computational modeling,
Delay,
Computer science,
Performance analysis,
Hardware,
Redundancy"
VLSI block placement using less flexibility first principles,"A deterministic algorithm for VLSI block placement was developed through human's accumulated experience in solving ""packing"" problem. Rectangle packing problem is just a simplified case of the polygon-shape stone plate packing problem that the ancient masons needed to face. Several ""packing"" principles derived from the so-called ""less flexibility first"" experience of the masons. A k-d tree data structure is used for manipulating the packed rectangles under the derived packing principles. Experiment results demonstrate that the algorithm is effective and promising in building block layout application.","Very large scale integration,
Computer science,
Tree data structures,
Process design,
Humans,
Floors,
Wire"
Efficient historical R-trees,"The historical R-tree (HR-tree) is a spatio-temporal access method aimed at the retrieval of window queries in the past. The concept behind the method is to keep an R-tree for each timestamp in history, but to allow consecutive trees to share branches when the underlying objects do not change. New branches are only created to accommodate updates from the previous timestamp. Although existing implementations of HR-trees process timestamp (window) queries very efficiently, they are hardly applicable in practice due to excessive space requirements and poor interval query performance. This paper addresses these problems by proposing the HR+-tree, which occupies a small fraction of the space required for the corresponding HR-tree (for typical conditions about 20%), while improving interval query performance several times. Our claims are supported by extensive experimental evaluation.","History,
Spatial databases,
Information retrieval,
Windows,
Indexing,
Computer science,
System performance,
Concrete"
Using cost benefit analysis for enterprise resource planning project evaluation: a case for including intangibles,"The paper demonstrates how cost benefit analysis can be applied to large-scale ERP projects, and that these methods can incorporate intangible benefits, e.g., user satisfaction. Detailed information on the business case utilized by a large computer manufacturer in the decision to implement the SAP system R/3 is presented. Techniques that demonstrate how intangibles may be included in the standard cost benefit analyses are presented. The paper concludes with a discussion on the state of valuing ERP projects and questions to be answered in the future.","Cost benefit analysis,
Enterprise resource planning,
Computer aided software engineering,
Investments,
Management information systems,
Information technology,
Electrical capacitance tomography,
Resource management,
Information analysis,
Decision support systems"
Natural-language processing support for developing policy-governed software systems,"Organizations are policy-driven entities. Policy bases can be very large and the relationships between policies can be complex. In addition, policy can change on a frequent basis. Checking for gaps in policy or analyzing the ramifications of changing policy is necessary to both identify and rectify gaps or unintended policy prior to the policy base being refined into requirements for a system. A policy workbench is an integrated set of computer based tools for developing, reasoning about, and maintaining policy. A workbench takes as input a computationally equivalent form of policy statements. We have developed a prototype of a tool that maps natural language policy statements to an equivalent computational form. The authors describe the architecture of a natural language input-processing tool (NLIPT). It has an extractor, which generates a meaning list representative of the natural language input; an index-term generator, which identifies the key terms used to index relevant policy schema in the policy base; a structural modeler, which structures a schema for input; and a logic modeler, which maps the schema to an equivalent logical form. We experimented with a prototype of the extractor which successfully parsed a sample of ninety-nine Naval Postgraduate School security policy statements with ninety-six percent accuracy.","Software systems,
Natural languages,
Prototypes,
Information security,
Computer science,
Marine vehicles,
Logic,
Java,
Virtual machining,
Information systems"
Non-photorealistic rendering using watercolor inspired textures and illumination,"The authors present a watercolor inspired method for the rendering of surfaces. Our approach mimics the watercolor process by building up an illuminated scene through the compositing of several layers of semitransparent paint. The key steps consist of creating textures for each layer using LIC (Line Integral Convolution) of Perlin Noise (K. Perlin, 1985), and then calculating the layer thickness distribution using an inverted subtractive lighting model. The resulting watercolor-style images have color coherence that results from the mixing of a limited palette of paints. The new lighting model helps to better convey large shape changes, while texture orientations give hints of less dominant features. The rendered images therefore possess perceptual clues to more effectively communicate shape and texture information.","Lighting,
Paints,
Layout,
Shape,
Rendering (computer graphics),
Color,
Pigments,
Painting,
Computer science,
Noise shaping"
Molecular nanotechnology,"Molecular nanotechnology is an interdisciplinary field combining the sciences of molecular chemistry and physics with the engineering principles of mechanical design, structural analysis, computer science, electrical engineering, and systems engineering. Molecular manufacturing is a method conceived for the processing and rearrangement of atoms to fabricate custom products. It relies on the use of a large number of molecular robotic subsystems working in parallel and using commonly available chemicals. Built to atomic specification, the products would exhibit order-of-magnitude improvements in strength, toughness, speed, and efficiency, and be of high quality and low cost. This article provides an overview of molecular nanotechnology, reviews progress in the field since its origins, and outlines the implications of its eventual emergence as the dominant manufacturing technique of the 21st century.","Nanotechnology,
Design engineering,
Chemistry,
Physics,
Computer science,
Electrical engineering,
Systems engineering and theory,
Manufacturing processes,
Parallel robots,
Chemicals"
Symmetric slope compensation in a long-haul WDM system using the CRZ format,"We numerically compared symmetric and asymmetric dispersion slope compensation schemes in a long-haul, chirped-return-to-zero, wavelength-division-multiplexed system. Symmetric compensation has significant advantages over asymmetric compensation. We elucidate the physical reasons and the system implications.","Wavelength division multiplexing,
Optical fiber dispersion,
Chirp,
Phase modulation,
Degradation,
Optical noise,
Computer science,
Laboratories,
Optical fiber communication,
Linear systems"
A super-scheduler for embedded reconfigurable systems,"Emerging reconfigurable systems attain high performance with embedded optimized cores. For mapping designs on such special architectures, synthesis tools, that are aware of the special capabilities of the underlying architecture are necessary. We propose an algorithm to perform simultaneous scheduling and binding, targeting embedded reconfigurable systems. The algorithm differs from traditional scheduling methods in its capability of efficiently utilizing embedded blocks within the reconfigurable system. The algorithm can be used to implement several other scheduling techniques, such as ASAP, ALAP, and list scheduling. Hence we refer to it as a super-scheduler. The algorithm is a path-based scheduling algorithm. At each step, an individual path from the input DFG is scheduled. The experiments with several DFGs extracted from MediaBench suite indicate promising results. The scheduler presents the capability to perform the trade-off between maximally utilizing the high-performance embedded blocks and exploiting parallelism in the schedule.","Scheduling algorithm,
Reconfigurable architectures,
Computer architecture,
Fabrics,
High level synthesis,
Computer science,
Parallel processing,
System-on-a-chip,
System performance,
Digital signal processing chips"
Fast inhomogeneous plane wave algorithm for the analysis of electromagnetic scattering,"The fast inhomogeneous plane wave algorithm has been developed to accelerate the solution of three-dimensional electromagnetic scattering problems in free space. By expanding the kernel of the Green's function using the Weyl identity and choosing a proper steepest descent path, the diagonalization of the translation matrix is achieved after the interpolation and extrapolation techniques are applied. The proposed algorithm is implemented on top of the scalable multipole engine, a portable implementation of the dynamic multilevel fast multipole algorithm for distributed-memory computers. The computational time per matrix vector multiplication is reduced to O(NlogN) and the memory requirement is reduced to O(N), where N is the number of unknowns in the discretized integral equation. The algorithm is validated by applying it to the solution of the electromagnetic scattering from the perfect electric conducting scatterers. This approach can be easily extended to more general problems with complicated Green's function expressed in terms of the plane wave spectral integrals, such as the ones encountered in the multilayered medium studies.","Electromagnetic scattering,
Nonhomogeneous media,
Mathematical model,
Green's function methods,
Integral equations,
Heuristic algorithms,
Surface waves"
An ROC analysis for subpixel detection,"ROC (Receiver Operating Characteristic) analysis has been widely used to evaluate detection performance. It is based on the Neyman-Pearson detection theory, which solves binary hypothesis testing problems. In mixed pixel classification many algorithms that are developed to estimate abundance fractions (of image endmembers) generally produce gray scale images. As a result, they are not directly applied to hypothesis testing problems. Instead of using the standard ROC curve generated by the detection power versus the false alarm probability, a 3-dimensional (3D) ROC curve is developed in this paper for subpixel detection. It is a 3D plot derived from the mean-detection probability versus the mean-false alarm rate with the third dimension specified by abundance fractions produced by subpixel detection algorithms. In order to illustrate the utility of the proposed 3D ROC analysis in subpixel detection, several linear unmixing-based algorithms are used for performance evaluation.","Testing,
Pixel,
Performance analysis,
Computer science,
Chemical analysis,
Image segmentation,
Signal processing,
Image processing,
Laboratories,
Chemical processes"
A component-based approach to building formal analysis tools,"Automatic-verification capability tends to be packaged into stand-alone tools, as opposed to components that are easily integrated into a larger software-development environment. Such packaging complicates integration because it involves translating internal representations into a form compatible with the stand-alone tool. By contrast, lightweight-analysis components package analysis capability in a form that does not involve such a translation. Borrowing ideas from GenVoca and object-oriented design patterns, we developed a domain model and an automatic generation framework for lightweight-analysis components. The generated components operate directly over the internal form of a specification without requiring a change in representation. Moreover, the domain model identifies several ""useful subsets"" that can be used to customize analysis capability to a particular application. We validated this domain model by generating lightweight analyzers for temporal logic and the behavioral subset of Lotos.",
A shape based post processor for Gurmukhi OCR,"A shape based post processing system for an OCR of Gurmukhi script has been developed. Based on the size and shape of a word, the Punjabi corpora has been split into different partitions. The statistical information of Punjabi language syllable combination, corpora look up and holistic recognition of most commonly occurring words have been combined to design the post processor. An improvement of 3% in recognition rate from 94.35% to 97.34% has been reported on machine printed images using the post processing techniques.",
Alborz: a query-based tool for software architecture recovery,"Alborz is a user assisted reverse engineering tool designed for analyzing and recovering the architecture of a software system in the form of cohesive modules and subsystems. The tool's operation is based on techniques from the area of data mining, pattern matching, and clustering.","Software architecture,
Software systems,
Computer architecture,
Pattern matching,
Computer science,
Reverse engineering,
Data mining,
Laboratories,
Councils,
Control systems"
Representation of temporal intervals and relations: information visualization aspects and their evaluation,"A crucial component for turning any temporal reasoning system into a real-world application that can be adopted by a wide base of users is given by its user interface. After analyzing and discussing the state of the art for the visualization of temporal intervals and relations, this paper proposes three new solutions, also evaluating them with a proper user study.","Human computer interaction,
Turning,
User interfaces,
Displays,
Multimedia databases,
Visual databases,
Data visualization,
Mathematics,
Computer science,
Application software"
Distributed token circulation on mobile ad hoc networks,"This paper presents several distributed algorithms that cause a token to continually circulate through all the nodes of a mobile ad hoc network. An important application of such algorithms is to ensure total order of message delivery in a group communication service. Some of the proposed algorithms are aware of, and adapt to changes in, the ad hoc network topology. When using a token circulation algorithm, a round is, said to complete when every node has been visited at least once. Criteria for comparing the algorithms include the average time required to complete a round, number of bytes sent per round, and number of nodes visited per round. Comparison between the proposed algorithms is performed using simulation results obtained from a detailed simulation model (with ns-2 simulator).","Mobile ad hoc networks,
Mobile communication,
Distributed algorithms,
Routing protocols,
Access protocols,
Computer science,
Ad hoc networks,
Network topology,
Wireless communication,
Standards development"
Dual transitions Petri Net based modelling technique for embedded systems specification,"This paper presents a new modelling technique capable of modelling both control and data information using a single unified approach. This is achieved by modifying the classical Petri Net structure, allowing it to have two types of transitions and arcs. As a consequence, loops and conditional operations within complex specifications are easily identified. The system dynamic behaviour is modelled using a new marking scheme of the net consisting of a new element called value for data representation in addition to classical tokens used for control purpose. Structural definitions, behavioural rules and graphical representation of the new modelling technique are given. One potential application of the proposed modelling technique is the internal representation of embedded systems specification. Two examples are included illustrating the applicability and efficiency of the proposed modelling technique.","Embedded system,
Petri nets,
Control systems,
Computer science,
High level synthesis,
Control system synthesis,
Concurrent computing,
Partitioning algorithms,
Scheduling algorithm,
Registers"
Efficient real-time face tracking in wavelet subspace,"We present a new method for visual face tracking that is carried out in wavelet subspace. First, a wavelet representation for the face template is created, which spans a low-dimensional subspace of the image space. The video sequence frames where the face is tracked are then orthogonally projected into this low-dimensional subspace. This can be done efficiently through a small number of applications of the wavelet filters. All further computations are performed in wavelet subspace, which is isomorphic to the image subspace spanned by the sets of wavelets in the representation. Robustness with respect to facial expression and affine deformations, as well as the efficiency of our method, are demonstrated in various experiments.",
A practical comparison of asynchronous design styles,"It is well known that single-rail bundled-delay circuits provide good area efficiency but it can be difficult to match them with appropriate delay models. Conversely delay insensitive circuits such as those employing dual-rail codes are larger but it is easier to ensure timing correctness. In terms of speed bundled-delay circuits need conservative timing but dual-rail circuits can require an appreciable completion detection overhead. This paper compares designs in both of these styles and also a delay-insensitive 1-of-4 coded circuit using the practical example of an ARM Thumb instruction decoder The results show that, through the application of careful optimizations, the 1-of-4 circuits out-performed single-rail circuits and reduced the power compared to dual-rail circuits.","Delay,
Circuits,
Thumb,
Timing,
Decoding,
Very large scale integration,
Wiring,
Computer science,
Rails,
Design optimization"
Reducing delay with dynamic selection of compression formats,"Internet computing is facilitated by a remote execution methodology in which programs transfer to a destination for execution. Since the transfer time can substantially degrade the performance of remotely executed (mobile) programs, file compression is used to reduce the amount of data that is transferred. Compression techniques however, must trade off compression ratio for decompression time, due to the algorithmic complexity of the former, since the latter is performed at run-time in this environment. In this paper, we define the total delay as the time for both the transfer and the decompression of a compressed file. To minimize the total delay, a mobile program should be compressed in the best format for minimizing the delay. Since both the transfer time and the decompression time are dependent upon the current underlying resource performance, selection of the ""best"" format varies and no one compression format minimizes the total delay for all resource performance characteristics. We present a system called Dynamic Compression Format Selection (DCFS) for the automatic and dynamic selection of competitive compression formats based on the predicted values of future resource performance. Our results show that DCFS reduces the total delay imposed by the compressed transfer of Java archives (.jar files) by 52% on average for the networks, compression techniques and benchmarks studied.","Java,
Delay effects,
Encoding,
Computer science,
Degradation,
Runtime environment,
Internet,
Compression algorithms,
Bandwidth,
Costs"
The INFINITY Project: digital signal processing and digital music in high school engineering education,"The importance of mathematics and science education in modern technological society cannot be understated. This keynote paper outlines the structure and goals of The INFINITY Project, a joint effort between educators, administrators, and industry leaders to establish an engineering curriculum at the high school level. The curriculum motivates students to learn about the fundamental concepts and principles of mathematics, science, and engineering through the study of multimedia and information technology. Digital music and audio synthesis are used at the onset of this curriculum to teach students about signals, systems, and modeling. Several examples from this portion of the curriculum are described, including a real-time interactive waveform synthesizer, a real-time guitar synthesizer based on physical modeling, a real-time sinusoidal MIDI player, and an extremely low-cost loudspeaker that each student designs, builds, and tests.","H infinity control,
Digital signal processing,
Multiple signal classification,
Mathematics,
Synthesizers,
Educational technology,
Educational institutions,
Information technology,
Loudspeakers,
Testing"
Application of UML associations and their adornments in design recovery,"Many CASE tools support reverse engineering and UML. However, it can be observed that, usually, only a subset of the UML notation is supported, namely those parts with a more or less direct code representation. Although a lot of research has been done in this field, the more advanced features of UML notations are not commonly supported in reverse engineering. In this paper, we show approaches to discover patterns in program code that can be represented by means of the advanced notational features of UML class diagrams. We obtain the necessary information by reverse-engineering Java programs with different methods. These have been implemented in a prototype implementation.","Unified modeling language,
Reverse engineering,
Java,
Computer aided software engineering,
Prototypes,
Application software,
Computer science,
Handicapped aids,
Marine vehicles,
Software prototyping"
Modeling the real world for data mining: granular computing approach,"In logic, a ""real world"" is modeled by a Cantor set with relational structure. In this paper, the relational structure is confined to the simplest kind, namely, binary relations. From different consideration, in granular computing, such a binary relational structure has been called a crisp/fuzzy binary granulation, or binary neighborhood system (FBNS). Intuitively, the set has been granulated into binary neighborhoods (generalized equivalence classes). Combining the two views, the simplest kind of ""real world"" model is BNS-space. From this view, the classical relational theory is the knowledge representation of the universe whose structure is a finite set of equivalence relations; in a ""real world"" relational theory, a finite set of crisp/fuzzy binary relations. Here knowledge representation is assigning meaningful names to binary neighborhoods (or equivalence classes in relational theory). Depending on the structures, the model can be useful in fuzzy logic or data mining. The focus of this paper is on data mining using granular computing. Experiments show that the computing is extremely fast and the cost of computing extra semantics is very small.","Data mining,
Fuzzy systems,
Fuzzy sets,
Knowledge representation,
Mathematics,
Computer science,
Mathematical model,
Fuzzy set theory,
Fuzzy logic,
Costs"
Timings for associative operations on the MASC model,,"Timing,
Costs,
Phase change random access memory,
Mathematical model,
Computational modeling,
Broadcasting,
Hardware,
Application software,
Mathematics,
Computer science"
Unique sink orientations of cubes,"Suppose we are given (the edge graph of) an n-dimensional hypercube with its edges oriented so that every face has a unique sink. Such an orientation is called a unique sink orientation, and we are interested in finding the unique sink of the whole cube, when the orientation is given implicitly. The basic operation available is the so-called vertex evaluation, where we can access an arbitrary vertex of the cube, for which we obtain the orientations of the incident edges. Unique sink orientations occur when the edges of a deformed geometric n-dimensional cube (i.e., a polytope with the combinatorial structure of a cube) are oriented according to some generic linear function. These orientations are easily seen to be acyclic. The main motivation for studying unique sink orientations are certain linear complementarity problems, which allow this combinatorial abstraction (due to Stickney and Watson, 1978), where orientations with cycles can arise. Similarly, some quadratic optimization problems, like computing the smallest enclosing ball of a finite point set, can be formulated as finding a sink in a unique sink orientation (with cycles possible). For acyclic unique sink orientations, randomized procedures due to Bernd Gartner (1998, 2001) with an expected number of at Most e/sup 2/spl radic/n/ vertex evaluations have been known. For the general case, a simple randomized (3/2)/sup n/ procedure exists (without explicit mention in the literature). We present new algorithms, a deterministic O(1.61/sup n/) procedure and a randomized O((43/20)/sup n/2/)=O(1.47/sup n/) procedure for unique sink orientations. An interesting aspect of these algorithms is that they do not proceed on a path to the sink (in a simplex-like fashion), but they exploit the potential of random access (in the sense of arbitrary access) to any vertex of the cube. We consider this feature the main contribution of the paper. We believe that unique sink orientations have a rich structure, and there is ample space for improvement on the bounds given above.","Artificial intelligence,
Character generation,
Computer science,
Hypercubes,
Cost accounting,
Radio access networks,
Combinatorial mathematics,
Computational geometry,
Lattices,
Labeling"
Investigation of measures for grouping by graph partitioning,"Grouping by graph partitioning is an effective engine for perceptual organization. This graph partitioning process, mainly motivated by computational efficiency considerations, is usually implemented as recursive bi-partitioning, where at each step the graph is broken into two parts based on a partitioning measure. We study four such measures, namely, the minimum cut, average cut, Shi-Malik normalized cut, and a variation of the Shi-Malik normalized cut. Using probabilistic analysis we show that the minimization of the average cut and the normalized cut measure, using recursive bi-partitioning will, on an average, result in the correct segmentation. The minimum cut and the variation of the normalized cut will, on an average, not result in the correct segmentation and we can precisely express the conditions. Based on a rigorous empirical evaluation, we also show that, in practice, the quality of the groups generated using minimum, average or normalized cuts are statistically equivalent for object recognition, i.e. the best, the mean, and the variation of the qualities are statistically equivalent. We also find that for certain image classes, such as aerial and scenes with man-made objects in man-made surroundings, the performance of grouping by partitioning is the worst, irrespective of the cut measure.","Object recognition,
Layout,
Computer science,
Acoustical engineering,
Engines,
Statistics,
Image segmentation,
Clustering methods"
An unavailability analysis of firewall sandwich configurations,"Firewalls form the first line of defense in securing internal networks from the Internet. A Firewall only provides security if all traffic into and out of an internal network passes through the firewall. However, a single firewall through which all network traffic must flow represents a single point of failure. If the firewall is down, all access is lost. A common solution to this problem is to use firewall sandwiches, comprising multiple firewall processors running in parallel. A firewall sandwich system needs load-balancing processes executing on separate processors to manage the flow of packets through the firewall processors. The number of redundant load balancing processors and their redundancy management policies have a major impact on system unavailability. We present a model to analyze the steady-state unavailability of firewall sandwiches and compare the unavailability of various load-balancing configurations. The results show that, using representative non-proprietary values for system parameters, redundancy management policies are at least as important as the number of redundant processing nodes.","IP networks,
Internet,
Load management,
Telecommunication traffic,
Portals,
Protection,
Network servers,
Web server,
Computer science,
Steady-state"
Rethinking classical internal forces for active contour models,"The classical active contour model has two basic internal forces: tension and curvature. These forces are included to provide cohe sion, equal control point spacing, and locally smooth shape. These classical internal forces have undesirable attributes that are in conflict with these original desired characteristics. Tension evenly spaces the control points, but also causes the models to collapse in weak image gradients. Curvature produces locally smooth curvature, but it does so by forcing the model toward a straight line. The paper returns to the original active contour model motivations to reformulate these internal forces. The desired properties are achieved without the introduction of unwanted model behavior A new spacing force and a new constant change in curvature force are introduced and their performance characteristics are discussed. The paper includes experimental results that demonstrate the efficacy and performance of the proposed reformulations.","Active contours,
Shape control,
Computer science,
Force control,
Computer vision,
Force measurement,
Pressure measurement,
Merging,
Deformable models"
Instabilities of two liquid drops in contact,"In previous experimental studies of noncoalescing liquid drops, researchers encountered an instability as the drops were pressed together. The Surface Evolver simulates this instability numerically by assuming that the stability is attributable to a minimization of surface area under a uniform surface tension.","Surface tension,
Bridges,
Extraterrestrial phenomena,
Temperature,
Physics,
Fluid dynamics,
Gravity,
Solid modeling,
Computational modeling,
Numerical simulation"
Stochastic Search for Signal Processing Algorithm Optimization,"This paper presents an evolutionary algorithm for searching for the optimal implementations of signal transforms and compares this approach against other search techniques. A single signal processing algorithm can be represented by a very large number of different but mathematically equivalent formulas. When these formulas are implemented in actual code, unfortunately their running times differ signi.cantly. Signal processing algorithm optimization aims at finding the fastest formula. We present a new approach that successfully solves this problem, using an evolutionary stochastic search algorithm, STEER, to search through the very large space of formulas. We empirically compare STEER against other search methods, showing that it notably can find faster formulas while still only timing a very small portion of the search space.","Stochastic processes,
Signal processing algorithms,
Optimization methods,
Search methods,
Timing,
Computer architecture,
Permission,
Dynamic programming,
Computer science,
Evolutionary computation"
Using the dynamical system approach to navigate in realistic real-world environments,"The dynamical system approach to behaviour based robotics provides a sound mathematical framework to behaviour design and integration. So far this approach has been used in various simulation work and in simple real world settings. This paper is a first step towards application in a more realistic scenario such as a typical office environment. The robot perceives its environment via a set of sonar sensors. Simple geometric representations of the corridor and obstacles are extracted from the perceptual information. Obstacle avoidance and corridor following are implemented as the basic behaviours, which allow the robot to navigate safely through its environment. The robot can reliably distinguish between passages that are either wide enough to pass or too narrow to be traversed. Coordination among the primitive behaviours allows the robot to cope with more complex situations, such as a corridor that is blocked by obstacles, in a flexible manner.","Robot kinematics,
Robot sensing systems,
Sonar navigation,
Numerical analysis,
Computer science,
Computational modeling,
Data mining,
Stability analysis,
Convergence,
Shape"
Specifying OLAP cubes on XML data,"On-Line Analytical Processing (OLAP) enables analysts to gain insight into data through fast and interactive access to a variety of possible views on information, organized in a dimensional model. The demand for data integration is rapidly becoming larger as more and more information sources appear in modern enterprises. In the data warehousing approach, selected information is extracted in advance and stored in a repository. This approach is used because of its high performance. However, in many situations a logical (rather than physical) integration of data is preferable. Previous Web-based data integration efforts have focused almost exclusively on the logical level of data models, creating a need for techniques focused on the conceptual level. Also, previous integration techniques for Web-based data have not addressed the special needs of OLAP tools such as handling dimensions with hierarchies. Extensible Markup Language (XML) is fast becoming the new standard for data representation and exchange on the World Wide Web. The rapid emergence of XML data on the Web, e.g., business-to-business (B2B) e-commerce, is making it necessary for OLAP and other data analysis tools to handle XML data as well as traditional data formats. Based on a real-world case study, the paper presents an approach to the conceptual specification of OLAP DBs based on Web data. Unlike previous work, this approach takes special OLAP issues such as dimension hierarchies and correct aggregation of data into account. Additionally, an integration architecture that allows the logical integration of XML and relational data sources for use by OLAP tools is presented.","XML,
Multidimensional systems,
Information analysis,
Warehousing,
Data analysis,
Data models,
Data warehouses,
Unified modeling language,
Computer science,
Data mining"
FDTD and PSTD simulations for plasma applications,"Three-dimensional finite-difference time domain (FDTD) and pseudospectral time-domain (PSTD) algorithms, with perfectly matched layer absorbing boundary condition, are presented for nonmagnetized plasma as a special case of general inhomogeneous, dispersive, conductive media. The algorithms are tested for three typical frequency bands, and an excellent agreement between the FDTD/PSTD numerical results and analytical solutions is obtained for all cases. Several applications, such as laser-pulse propagation in plasma hollow channels, surface-wave propagation along a plasma column of finite length, and energy deposition of electron cyclotron resonance plasma source, demonstrate the capability and effectiveness of these algorithms. The PSTD algorithm is more efficient and accurate than the FDTD algorithm, and is suitable for large-scale problems, while the FDTD algorithm is more suitable for fine details. The numerical results also show that plasma has complex transient responses, especially in the low-frequency and resonance regimes. Because of their flexibility and generality, the algorithms and computer programs can be used to simulate various electromagnetic waves-plasma interactions with complex geometry and medium properties, both in time and frequency domains.","Finite difference methods,
Time domain analysis,
Plasma simulation,
Plasma applications,
Plasma sources,
Optical propagation,
Resonance,
Perfectly matched layers,
Boundary conditions,
Dispersion"
On sparse signal representations,"An elementary proof of a basic uncertainty principle concerning pairs of representations of /spl Rscr//sup N/ vectors in different orthonormal bases is provided. The result, slightly stronger than stated before, has a direct impact on the uniqueness property of the sparse representation of such vectors using pairs of orthonormal bases as overcomplete dictionaries. The main contribution in this paper is the improvement of an important result due to Donoho and Huo (1999) concerning the replacement of the l/sub 0/ optimization problem by a linear programming minimization when searching for the unique sparse representation.",
Floorplanning with abutment constraints and L-shaped/T-shaped blocks based on corner block list,"The abutment constraint problem is one of the common constraints in practice to favor the transmission of data between blocks. Based on corner block list (CBL), a new algorithm to deal with abutment constraints is developed in this paper. We can obtain the abutment information by scanning the intermediate solutions represented by CBL in linear time during the simulated annealing process and fix the CBL in case the constraints are violated. Based on this algorithm, a new method to deal with L-shaped/T-shaped blocks is proposed. The shape flexibility of the soft blocks and the rotation and reflection of L-shaped/T-shaped blocks are exploited to obtain a tight packing. The experimental results are demonstrated by some benchmark data and the performance shows the effectiveness of the proposed method.","Computer science,
Partitioning algorithms,
Simulated annealing,
Reflection,
Encoding,
Permission,
Shape control,
Data engineering,
Logic design,
Logic circuits"
Dynamic load sharing with unknown memory demands in clusters,"A compute farm is a pool of clustered workstations to provide high performance computing services for CPU-intensive, memory-intensive, and I/O active jobs in a batch mode. Existing load sharing schemes with memory considerations assume jobs' memory demand sizes are known in advance or predictable based on users' hints. This assumption can greatly simplify the designs and implementations of load sharing schemes, but is not desirable in practice. In order to address this concern, we present three new results and contributions in this study. Conducting Linux kernel instrumentation, we have collected different types of workload execution traces to quantitatively characterize job interactions, and modeled page fault behavior as a function of the overloaded memory sizes and the amount of jobs' I/O activities. Based on experimental results and collected dynamic system information, we have built a simulation model which accurately emulates the memory system operations and job migrations with virtual memory considerations. We have proposed a memory-centric load sharing scheme and its variations to effectively process dynamic memory, allocation demands, aiming at minimizing execution time of each individual job by dynamically migrating and remotely submitting jobs to eliminate or reduce page faults and to reduce the queuing time for CPU services. Conducting trace-driven simulations, we have examined these load sharing policies to show their effectiveness.","Application software,
Monitoring,
Electric breakdown,
Computer science,
Educational institutions,
Workstations,
Linux,
Kernel,
Instruments,
Computational modeling"
"Animation can show only the presence of errors, never their absence","A formal specification animator executes and interprets traces on a specification. Similar to software testing, animation can only show the presence of errors, never their absence. However, animation is a powerful means of finding errors, and it is important that we adequately exercise a specification when we animate it. The paper outlines a systematic approach to the animation of formal specifications. We demonstrate the method on a small example, and then discuss its application to a non-trivial, system-level specification. Our aim is to provide a method for planned, documented and maintainable animation of specifications, so that we can achieve a high level of coverage, evaluate the adequacy of the animation, and repeat the process at a later time.","Animation,
Formal specifications,
Specification languages,
Computer errors,
Software testing,
Concrete,
Computer science,
Australia,
Documentation,
Guidelines"
Theory of interorganizational systems: industry structure and processes of change,"The paper concerns the development and adoption of interorganizational systems (IOS): information systems that span organizational boundaries. These business-to-business e-commerce systems have considerable economic importance. The paper outlines a multi-level theory of IOS that explicitly recognizes the importance of the industry as a macro-level unit of analysis in addition to the units (enterprises) at the micro-level. The roles of the external environment and the technology based IOS are also recognized. Theories of intentional agency drawn from the areas of robotics, intelligent software agents and human-computer interaction are used to explain how industry-level activity occurs. Concerted activity is attributed to the reciprocal causal effect of the group upon the individual units, rather than to any form of regular group deliberation about action. It is expected that change at the industry level will tend to be incremental, building on routine, situated actions of different players. Propositions concerning industry structure, processes of change and the development of IOS are illustrated with case studies.","Computer industry,
Information systems,
Intelligent robots,
Information analysis,
Service robots,
Robot kinematics,
Human robot interaction,
Intelligent agent,
Software agents,
Buildings"
An adaptive update lifting scheme with perfect reconstruction,"The lifting scheme provides a general and flexible tool for the construction of wavelet decompositions and perfect reconstruction filter banks. We propose an adaptive version of this scheme which has the intriguing property that it allows perfect reconstruction without any overhead cost. We restrict ourselves to the update lifting step which affects the approximation signal only. The update lifting filter is assumed to depend pointwise on the norm of the associated gradient vector, in such a way that a large gradient induces a weak update filter. Thus, sharp transitions in a signal (eg, edges in an image) will not be smoothed to the same extent as regions which are more homogeneous.","Filter bank,
Image reconstruction,
Wavelet transforms,
Mathematics,
Computer science,
Costs,
Signal processing,
Image processing,
Shape,
Data analysis"
Perturbed Turing machines and hybrid systems,"Investigates the computational power of several models of dynamical systems under infinitesimal perturbations of their dynamics. We consider models for both discrete- and continuous-time dynamical systems: Turing machines, piecewise affine maps, linear hybrid automata and piecewise-constant derivative systems (a simple model of hybrid systems). We associate with each of these models a notion of perturbed dynamics by a small /spl epsi/ (w.r.t. to a suitable metric), and define the perturbed reachability relation as the intersection of all reachability relations obtained by /spl epsi/-perturbations, for all possible values of /spl epsi/. We show that, for the four kinds of models we consider, the perturbed reachability relation is co-recursively enumerable (co-r.e.), and that any co-r.e. relation can be defined as the perturbed reachability relation of such models. A corollary of this result is that systems that are robust (i.e. whose reachability relation is stable under infinitesimal perturbation) are decidable.","Turing machines,
Power system modeling,
Robustness,
Computational modeling,
Automata,
Robust stability,
Chaos,
State-space methods"
A secure image coding scheme using residue number system,A secure image coding scheme using the residue number system (RNS) is presented and tested. The proposed scheme can be also used as the base for a full security multiple access image communication system. Using RNS with multiple look-up tables for different modules increases the security level of the system. This conversion technique can be used to enhance the signal to noise ratio for received images corrupted with AWGN.,"Image coding,
Cryptography,
Digital signal processing,
Cathode ray tubes,
Artificial intelligence,
Systems engineering and theory,
Information security,
Business,
Computer networks,
Arithmetic"
Ad hoc on-demand backup node setup routing protocol,"An ad hoc wireless network provides infrastructureless data networking, where users have network services while they are continually moving. Instructureless, and mobility are properties of the ad hoc wireless networks. Furthermore, each move of the mobile host affects the change of network topology, and affects the change of the transmission route. There are numerous routing protocols developed for ad hoc wireless networks, and they may be generally categorized as table-driven or source-initiated on-demand. However, the properties of the ad hoc wireless networks still affect those routing algorithms. The ad hoc backup node setup routing protocol (ABRP) is proposed to lead us pay move attention on the intrinsic properties of the ad hoc wireless networks. It provides a more complete consideration of the routing quality. According to the proposed ABRP, the destination can receive some more routes in a period of time. Those routes from the source node to the destination node may give us ways to analyze and find some good backup routes to get more help for reconnection when a link failure occurs. The backup route information can be saved in a specific on-the-route node. The enables the backup routes to be rapidly found and traced back to these nodes when there are situations such as disconnection or connection loss. Moreover, the ABRP provides a backup node mechanism to reconnect maintenance quickly, so meeting the property of ad hoc wireless networks.","Routing protocols,
Wireless networks,
Network topology,
Data engineering,
Ad hoc networks,
Mobile communication,
Computer networks,
Information science,
Battery charge measurement,
Switches"
Recent trends in logistics and the need for real-time decision tools in the trucking industry,"For the last ten years or so, the freight transportation industry has been facing new challenges, such as time-sensitive industrial and commercial practices as well as the globalization of markets. In response to these changes, new information-related technologies have developed rapidly: electronic data interchange (EDI) and the Internet, the Global Positioning System (GPS) via satellites, and decision support systems (DSSs). These technologies can greatly enhance the operations planning capability of freight carriers in as much as they make use of this information in order to optimise their operations. GPS, EDI and the Internet can also provide the necessary information required to achieve real-time computer-based decision making using appropriate operations research techniques. Today's decision support tools must therefore be designed to be used in a real-time environment. This paper describes this environment and proposes optimization tools that can be made available to motor carriers.","Logistics,
Global Positioning System,
Internet,
Transportation,
Globalization,
Data handling,
Satellites,
Decision support systems,
Spread spectrum communication,
Technology planning"
Constructing shared-tree for group multicast with QoS constraints,"Group multicast refers to the kind of multicast in which every member of a group may transmit data to the group. Several QoS-aware routing algorithms for group multicast proposed previously take into account bandwidth requirement (which is the most important QoS metric to consider for many applications) and build source-based tree for each individual group member. Per-source tree approach has some advantages over shared-tree approach but suffers the drawbacks of higher control overhead and being less scalable especially with group size. In this paper we present an algorithm which builds shared tree for group multicast and can accommodate multiple QoS requirements including bandwidth and inter-member delay. Besides the advantages of having less control overhead and better scalability, our algorithm can support dynamic membership without recomputing the whole tree. The results from simulation experiments for multicast with bandwidth reservation show that our algorithm has similar performance in terms of tree cost and bandwidth utilization compared with two other per-source tree algorithms.","Multicast algorithms,
Bandwidth,
Multicast protocols,
Costs,
Size control,
Routing protocols,
Computer science,
Delay,
Scalability,
Heuristic algorithms"
High-level automatic pipelining for sequential circuits,"This paper presents a new approach for automatically pipelining sequential circuits. The approach repeatedly extracts a computation from the critical path, moves it into a new stage, then uses speculation to generate a stream of values that keep the pipeline full. The newly generated circuit retains enough state to recover from incorrect speculations by flushing the incorrect values from the pipeline, restoring the correct state, then restarting the computation. We also implement two extensions to this basic approach: stalling, which minimizes circuit area by eliminating speculation; and forwarding, which increases the throughput of the generated circuit by forwarding correct values to preceding pipeline stages. We implemented a prototype synthesizer based on this approach. Our experimental results show that, starting with a non-pipelined or insufficiently pipelined specification, this synthesizer can effectively reduce the clock cycle time and improve the throughput of the generated circuit.","Pipeline processing,
Sequential circuits,
Throughput,
Laboratories,
Computer science,
Prototypes,
Synthesizers,
Clocks,
Logic circuits,
Permission"
Implementing a testbed for mobile multimedia,"In an effort to realize wireless Internet telephony and multimedia streaming in a highly mobile environment a testbed emulating a wireless Internet has been built. This allows the setting up of multimedia calls between IP mobiles and integration between IP and PSTN end-points in a wireless environment. Different functionalities and components involved with the wireless Internet streaming multimedia have been prototyped and experimented in the testbed. These include signaling, registration, dynamic binding, location management as well as supporting the QoS features for the mobile users. This paper describes some of the components of the testbed and highlights the experiences while building this testbed which could be beneficial to some who plan to build a similar testbed to realize several features and capabilities of Mobile Wireless Internet, before actually bringing to the market.","Testing,
Streaming media,
Protocols,
Quality of service,
Authentication,
Computer science,
Internet telephony,
Mobile computing,
Prototypes,
Costs"
A logical reconstruction of SPKI,,"Authorization,
Public key,
Information analysis,
Computer science,
Australia,
Technological innovation,
Information security,
Logic design,
Certification"
CTMS: a novel constrained tree migration scheme for multicast services in generic wireless systems,"This study considers the multicasting problem over mobile wireless systems in the context of generic wireless systems. Specifically, a novel constrained tree migration scheme (CTMS) is created to support multicast services in mobile wireless networks. The salient features of the novel CTMS include: (1) automatically recognizing the inefficiency of the multicast trees, then migrating them to better ones, while maintaining the QoS guarantees specified by mobile users; (2) conserving network resources by maintaining a low-cost multicast tree, thus accommodating more users; (3) operating efficiently in a truly distributed manner through event driven and diffusing computations, thus increasing the degree of scalability; (4) synchronizing data transmission flow for transparency during the tree migration, and thus providing seamless handoff control. Finally, the novel CTMS also handles the concurrent migration problem effectively within the wireless system, thus eliminating the oscillation paradox. Extensive simulation results show that CTMS can significantly reduce the resources used per multicast tree, thus achieving both low handoff-dropping/join-blocking rate and high resource utilization.",
UML-based behavior specification of interactive multimedia applications,"Availability of precise, yet usable modeling languages is essential to the construction of multimedia systems based on software engineering principles and methods. Although several languages have been proposed for the specification of isolated multimedia system aspects, there not yet exists an integrated modeling language that adequately supports multimedia software development in practice. We propose an extension of the Unified Modeling Language (UML) for the integrated specification of multimedia systems based on an object-oriented development method. Since integration of co-existing timed procedural and interactive behavior is at the heart of multimedia systems, we focus on UML-based specification of behavior in this paper. In addition, we outline how these behavioral aspects are to be integrated with media, presentation, and software architecture modeling to achieve a coherent and consistent model.","Object oriented modeling,
Multimedia systems,
Unified modeling language,
Application software,
Software engineering,
Mathematics,
Computer science,
Mathematical model,
Computer architecture,
Information systems"
Self-stabilizing PIF algorithm in arbitrary rooted networks,"We present a deterministic distributed Propagation of Information with Feedback (PIF) protocol in arbitrary rooted networks. The proposed algorithm does not use a preconstructed spanning tree. The protocol is self-stabilizing, meaning that starting from an arbitrary state (in response to an arbitrary perturbation modifying the memory state), it is guaranteed to behave according to its specification. Every PIF wave initiated by the root inherently creates a tree in the graph. So, the tree is dynamically created according to the progress of the PIF wave. This allows our PIF algorithm to take advantage of the relative speed of different components of the network. The proposed algorithm can be easily used to implement any self-stabilizing system which requires a (self-stabilizing) wave protocol running on an arbitrary network.","Intelligent networks,
Feedback,
Protocols,
Tree graphs,
Broadcasting,
Distributed computing,
Algorithm design and analysis,
Fault tolerance,
Computer science,
Fault detection"
Melanoma prediction using data mining system LERS,"One of the important tools for early diagnosis of malignant melanoma is the total dermatoscopy score (TDS), computed using the ABCD (asymmetry, border, color, diameter) formula. Our primary objective was to check whether the ABCD formula is optimal. Using a data set containing 276 cases of melanoma and the LERS (Learning from Examples based on Rough Sets) data mining system, we checked more than 20,000 modified formulas for ABCD, computing the predicted error rate of melanoma diagnosis using 10-fold cross-validation for every modified formula. As a result, we found the optimal ABCD formula for our setup: discretization based on cluster analysis, the LEM2 (Learning from Examples Module, version 2) algorithm (one of the four LERS algorithms for rule induction) and the standard LERS classification scheme. The error rate for the standard ABCD formula was 10.21 %, while for the optimal ABCD formula the error rate was reduced to 6.04%. Some research in melanoma diagnosis shows that the use of the ABCD formula does not improve the error rate. Our research shows that the ABCD formula is useful, since, for our data set, the error rate without the use of the ABCD formula was higher (13.73%).",
Design of a Firewire based data acquisition system for use in animal PET scanners,"The University of Washington is building two different animal PET imaging systems-one optimized for mice and one for larger animals. To support both of these systems, our laboratory is designing and building a data acquisition system based on the IEEE 1394a (Firewire) serial bus standard. This bus standard currently allows up to 63 devices per bus with an overall bandwidth of 400 megabits per second transfer rate. The acquisition system is based on a layered electronics design (analog, digital, and transport) for each detector module. The analog card processes signals from the photomultiplier tube (4 to 64 signals depending on the scanner system). The digital board includes circuitry to digitize the analog signals, latch a clock word used for determining coincidence timing, and logic for a coarse timing window. The digitized data is then latched into the Firewire transport card which adds the detector module number and type to the data block. All of the data is collected event by event (list mode) with a Macintosh computer. Initial analysis indicates that the expected count rates for our animal systems can be accommodated with a Firewire based system. The current design makes no assumptions about the geometry of the system. The different scanner designs can all use the same transport card and only require different analog and digital cards as needed for the number of signals being processed. The advantages of a data acquisition system based on the 1394a standard include considerable flexibility in the design of animal PET imaging systems and taking advantage of inexpensive, but powerful desktop computers with a consumer interface that has been developed for high speed data transfer.","Firewire,
Data acquisition,
Animals,
Signal processing,
Buildings,
Positron emission tomography,
Detectors,
Timing,
Mice,
Laboratories"
Evolutionary learning of Web-document structure for information retrieval,Web documents have a number of tags indicating the structure of documents. The tag information can be utilized to improve the performance of document retrieval systems. The authors propose an approach to retrieve Web documents using HTML tags and then use a genetic algorithm to adapt the tag weights. This method uses a modified similarity measure based on the tag weights. A genetic learning method is used to select the tags for retrieval and get the optimal tag weights. We evaluate our method via experiments on conference pages and TREC document sets. The experimental results show that the tag weights are well trained by the proposed algorithm in accordance with the importance factors for retrieval. The proposed method has achieved about 10% improvement in retrieval accuracy.,"Information retrieval,
HTML,
Artificial intelligence,
Computer science,
Genetic algorithms,
Testing,
Sun,
Learning systems,
Web search,
Search engines"
A trial for data retrieval using conceptual fuzzy sets,"We describe trial applications of fuzzy sets to data retrieval. The objectives are to test their ability to achieve conceptual matching between retrieved objects and the user's intention and to connect real data with symbolic notations. The algorithm proposed retrieves data that conceptually fit the meanings of the entered keyword. An algorithm is described that uses fuzzy sets to handle word ambiguity (the main cause of vagueness in the meaning of a word). It is based on conceptual fuzzy sets (CFSs), which represent the meaning of words by chaining other related words. Two trial applications of this algorithm to data retrieval are described. First, an application to image retrieval shows variation of data retrieval with conceptual matching and transformation of numeric values into symbols. Next, an application to the agent recommending a TV program shows the method that lets CFSs fit to the sense of a user by Hebbian learning.",
Experience-based representation construction: learning from human and robot teachers,"In this paper we address the problem of teaching robots to perform various tasks. We present a behavior-based approach that extends the capabilities of robots, allowing them to learn representations of complex tasks from their own experiences of interacting with a human, and to use the acquired knowledge to teach other robots in turn. A learner robot follows a human or robot teacher and maps its own observations of the environment to its internal behaviors, building at run-time a representation of the experienced task in the form of a behavior network. To enable this, we introduce an architecture that allows the representation and execution of complex and flexible sequences of behaviors and an online algorithm that builds the task representation from observations. We demonstrate our approach in a set of human(teacher)-robot(learner) and robot(teacher)-robot(learner) experiments, in which the robots learn representations for multiple tasks and are able to execute them even in environments with distractor objects that could hinder the learning and the execution process.",
Enhanced component interfaces to support dynamic adaption and extension,"Current component systems offer the possibility to integrate different enterprise systems, e.g., by wrapping legacy components or integrating several object protocols such as RMI, Corba-IIOP, etc. A disadvantage of todays component systems is that the interface descriptions of their component model do not give alone sufficient information to deploy a component correctly and reliably. Therefore the definition of new interface models, which are enhanced by (semantic) applicability information play an important role in enterprise application integration. We describe a new model of software component interfaces, using an extension of finite state machines to describe the protocol to use a component's offered services, and the sequences of calls to external services the component requires to fulfil its offered services. Our model concentrates on protocol issues of interoperability. We present a description of our new interface model and present the algorithms for integration checking, automatic adaption and dynamic extension. These algorithms are implemented in our CoCoNut/J-prototype.","Protocols,
Information systems,
Computer architecture,
Informatics,
Wrapping,
Computer interfaces,
Runtime,
Information security,
Companies,
Software systems"
Building a Web-based federated simulation system with Jini and XML,"In a Web-based federated simulation system, a group of simulation models residing on different machines attached to the Internet, called federates, collaborate with each other to accomplish a common task of simulating a complex real-world system. To reduce the cost of developing and maintaining simulation models and facilitate the process of building complex collaborative simulation systems, reuse of existing simulation models and interoperability between disparate simulation models are of paramount importance. Moreover to make such a system highly extensible, the individual federates, which could reside on the same host or physically distributed hosts, should be able to freely join and leave a federation without full knowledge of its peer federates. Simply put, an ideal simulation system should allow for quick and cheap assembly of a complex simulation out of independently developed simulations and at the same time allow the participating simulations to have maximum independence. Fortunately this is made possible by some emerging Jini technologies, notably Jini and the Extensible Markup Language (XML). We introduce Jini and XML and present the design and prototype implementation of a Web-based federated simulation system using Jini and XML.","XML,
Computational modeling,
Java,
Computer simulation,
Collaboration,
Computer architecture,
Computer science,
Internet,
Costs,
Assembly systems"
An efficient recovery scheme for mobile computing environments,"This paper presents an efficient recovery scheme based on checkpointing and message logging for mobile computing systems. For the efficient management of checkpoints and message logs, a movement-based scheme is proposed. Mobile hosts carrying their recovery information to the nearby mobile support station can recover instantly in case of a failure, however, the cost to transfer the recovery information must be high. On the other hand, the recovery information remaining dispersed over a number of support stations visited by mobile hosts must incur very high recovery cost. To balance the failure-free operation cost and the recovery cost, in the proposed scheme, the recovery information of a mobile host remains at the visited support stations while the host moves within a certain range. Only when the host moves out of the range, the recovery information is transferred to a nearby mobile support station. As a result, the proposed scheme can control the information transfer cost as well as the recovery cost.","Mobile computing,
Checkpointing,
Costs,
Bandwidth,
Fault tolerant systems,
Wireless networks,
Distributed computing,
Batteries,
Computer science,
Energy consumption"
Evolving messy gates for fault tolerance: some preliminary findings,"We investigate a preliminary model of gate-like components with added random noise. We refer to these types of components as messy. The principal idea behind messy gates is that evolving circuits using messy gates may confer some beneficial properties, one being fault-tolerance. The exploitation of the physical characteristics has already been demonstrated in intrinsic evolution of electronic circuits. This provided some of the inspiration for the work reported in this paper. Here we are trying to create a simulateable world in which ""physical characteristics"" can be exploited. We are also trying to study the question: What kind of components are most useful in an evolutionary design scenario?.",
Web search via hub synthesis,"We present a model for web search that captures in a unified manner three critical components of the problem: how the link structure of the web is generated, how the content of a web document is generated, and how a human searcher generates a query. The key to this unification lies in capturing the correlations between these components in terms of proximity in a shared latent semantic space. Given such a combined model, the correct answer to a search query is well defined, and thus it becomes possible to evaluate web search algorithms rigorously. We present a new web search algorithm, based on spectral techniques, and prove that it is guaranteed to produce an approximately correct answer in our model. The algorithm assumes no knowledge of the model, and is well-defined regardless of the model's accuracy.","Web search,
Computer science,
Web pages,
Couplings,
Information retrieval,
Books,
Humans,
Search engines,
Embedded computing"
Agent-based route optimization for mobile IP,"The need for mobility support on existing networks led to the formulation of the mobile IP protocol, which can provide seamless connectivity between fixed and mobile nodes. However, the inherent drawbacks of the protocol led to route optimization proposals of which one was accepted by the IETF. While attempting to solve the triangle routing problem, the adopted route optimization places extra constraints on the network nodes. This work is motivated by the need to find an alternative solution that would effectively solve the triangle routing problem without sacrificing performance, transparency, and simplicity. The proposed solution is an agent-based route optimization, which moves the tasks of maintaining and updating binding caches and encapsulating messages away from individual correspondent nodes to the correspondent agents. We setup a simulation environment to evaluate the proposed methodology. Simulation results show that the proposed protocol outperforms the existing protocols in terms of system message complexity, protocol simplicity, and scalability.","Protocols,
Routing,
Mobile computing,
Home automation,
Computer science,
Proposals,
Internetworking,
Chemical technology,
Constraint optimization,
Scalability"
Automatic seal verification by evaluating positive cost,"Seals instead of signature for person identification are widely used in oriental countries. Several techniques for automatic verification of seal imprints have been proposed, but all of them have dealt with this problem as a general pattern matching problem. In other words, even if it exchanges an input pattern and a reference pattern, evaluated cost does not change. In this paper, it is shown that the seal verification should not be treated as a general pattern matching problem by clarifying a specific characteristic of this problem. Two kinds of costs, called the negative cost and the positive one, may be defined in the correlation with the input and the reference patterns. A genuine imprint may produce only the negative cost to the pattern, and a forgery one may produce both costs to the pattern. Therefore, correlation value should not be calculated by definition of usual correlation. Based on the specific characteristic, a new approach for seal verification is presented. Experiments using both binary reference images and 3D reference ones provide the verification ability for the approach proposed.","Seals,
Costs,
Forgery,
Pattern matching,
Information science,
Eyes,
Information management,
Inspection"
An educational environment for VHDL hardware description language using the WWW and specific workbench,"In this paper, an educational environment for the hardware description language (VHDL) is described. There are three parts that make up this environment: to teach students the fundamental concepts in theory, to design and simulate digital circuits and finally, to check the behavior of the real programmable logic devices (PLD). The theoretical tutorial and the practical manual for the simulations, both evolved using hypertext mark-up language (HTML), allow students to follow the course from any point of the planet through the World Wide Web (WWW). In order to achieve the above mentioned goals, a board has been specifically designed. This device is connected to a shared personal computer. The data transmission and the control of the board has been developed based on a set of Virtual Instruments (VI) designed over LabView (Laboratory Virtual Instrument Engineering Workbench from National Instrument) which is a graphical programming, language that has been widely adopted throughout industry, academia, and research lab as a standard for data acquisition, and instrument control software.",
The globus toolkit for grid computing,,"Grid computing,
Distributed computing,
Computer science,
Instruments,
Resource management,
Mathematics,
Laboratories,
Mathematical programming,
Computer networks,
Data mining"
Wins and losses of algebraic transformations of software architectures,"In order to understand, analyze and modify software, we commonly examine and manipulate its architecture. For example, we may want to examine the architecture at different levels of abstraction. We can view such manipulations as architectural transformations, and more specifically, as graph transformations. We evaluate relational algebra as a way of specifying and automating the architectural transformations. Specifically, we examine Grok, a relational calculator that is part of the PBS toolkit. We show that relational algebra is practical in that we are able to specify many of the transformations commonly occurring during software maintenance and, using a tool like Grok, we are able to manipulate, quite efficiently, large software graphs; this is a ""win"". However, this approach is not well suited to express some types of transforms involving patterns of edges and nodes; this is a ""loss"". By means of a set of examples, the paper makes clear when the approach wins and when it loses.","Computer architecture,
Algebra,
Software maintenance,
Software systems,
Data mining,
Computer science,
Information science,
Information analysis,
Software tools,
Reverse engineering"
Evaluating software project similarity by using linguistic quantifier guided aggregations,"Software projects are often described by linguistic variables such as the experience of programmers and the complexity of modules. Because the existing software project similarity measures take into account only numerical data, we have proposed a set of measures based on fuzzy logic to evaluate the similarity between two software projects when they are described by linguistic values. In this work, we improve the proposed measures by using linguistic quantifiers such as 'most', 'many' and 'few' in the computing process for the various measures.","Software measurement,
Fuzzy logic,
Fuzzy sets,
Open wireless architecture,
Software engineering,
Engineering management,
Laboratories,
Computer science,
Programming profession"
SELFCON: An architecture for self-configuration of networks,"Traditional configuration management involves complex labor-intensive processes performed by experts. The configuration tasks such as installing or reconfiguring a system, provisioning network services and allocating resources typically involve a large number of activities involving multiple network elements. The network elements may be associated with proprietary configuration management instrumentation and may also be spread across heterogeneous network domains thereby increasing the complexity of configuration management. This paper introduces an architecture for the self-configuration of networks (SELFCON). The proposed architecture involves a directory server, which is used to maintain configuration information. The configuration information stored in the directory server is modeled using the standard DEN specification thereby allowing effective exchange of network, system and configuration management data among heterogeneous management domains. SELFCON associates configuration intelligence with the components of the network, rather than limit it to a centralized management station. The network elements are notified about related changes in configuration policies, based upon which, they perform self-configuration. SELFCON is able to provide automation of configuration management and also an effective unifying framework for enterprise management.","Servers,
Protocols,
Standards,
Object oriented modeling,
Complexity theory,
Registers,
Monitoring"
Client-centered load distribution: a mechanism for constructing responsive Web services,"We describe the design, implementation and experimental evaluation of a software mechanism that supports responsive (i.e. highly available and timely) Web services, constructed out of replicated servers. Specifically, this mechanism operates by engaging all the available replicas in supplying a fragment of the Web document that a client requires. The size of the fragment a replica is requested to supply is dynamically evaluated on the basis of the response time that replica can provide its client with. In addition, the proposed mechanism can dynamically adapt to changes in both the network and the replica servers' status, thus tolerating possible replica or communication failures that may occur at run-time. The performance results we have obtained from our experimental evaluation illustrate the adequacy of the mechanism we propose.","Web services,
Web server,
Delay,
Availability,
Runtime,
Workstations,
Web and internet services,
Uniform resource locators,
Redundancy,
Throughput"
A topology preserving deformable model using level sets,"Active contour and surface models, also known as deformable models, constitute a class of powerful segmentation techniques. Geometric deformable models implemented via level-set methods have advantages over parametric ones due to their intrinsic behavior, parameterization independence, and ease of implementation. However, a long claimed advantage of geometric deformable models, the ability to automatically handle topology changes, turns out to be a liability in applications where the objects to be segmented have a known topology that must be preserved. In this paper, we present a geometric deformable model that preserves topology using the simple point concept from digital topology. This algorithm maintains the other advantages of standard geometric deformable models including sub-pixel accuracy and production of nonintersecting curves (or surfaces). Several experiments on simulated and real data are provided to demonstrate the performance of the proposed algorithm.","Topology,
Deformable models,
Active contours,
Image segmentation,
Level set,
Lagrangian functions,
Brain,
Production,
Heart,
Image edge detection"
Converting relational database into XML document,"XML (eXtensible Markup Language) has emerged and is gradually being accepted as the standard for data interchange in the Internet world. XML databases are packaged by the key relational database vendors in the market as the extender or cartridge to the relational database management system. Interoperation of relational database and XML database involves schema and data translations. The paper provides a methodology of translating the conceptual schema of a relational database into XML schema through EER (extended entity relationship) model. Physical data are then translated from relational table to XML document. The semantics of the relational database, captured in EER diagram, are mapped to XML schema using stepwise procedures. The physical data are then mapped to an XML document under the definitions of the XML schema.","Relational databases,
XML,
Object oriented modeling,
Reverse engineering,
Computer science,
Data engineering,
Object oriented databases,
Data mining,
Internet,
Packaging"
"How to select a replication protocol according to scalability, availability and communication overhead","Data replication is playing an increasingly important role in the design of parallel information systems. In particular, the widespread use of cluster architectures in high-performance computing has created many opportunities for applying data replication techniques in new areas. For instance, as part of work related to cluster computing in bioinformatics, we have been confronted with the problem of having to choose an optimal replication strategy in terms of scalability, availability and communication overhead. Thus, we have evaluated several representative replication protocols in order to better understand their behavior in practice. The results obtained are surprising in that they challenge many of the assumptions behind existing protocols. Our evaluation indicates that the conventional read-one/write-all approach is the best choice for a large range of applications requiring data replication. We believe this is an important result for anybody developing code for computing clusters as the read-one/write-all strategy is much simpler to implement and more flexible than quorum-based approaches. In this paper we show that, in addition, it is also the best choice using a number of other selection criteria.","Protocols,
Scalability,
Databases,
Computer science,
Bioinformatics,
Computer architecture,
Concurrent computing,
Availability,
Information systems,
Distributed information systems"
Action Language Verifier,"Action Language is a specification language for reactive software systems. We present the Action Language Verifier which consists of: 1) a compiler that converts Action Language specifications to composite symbolic representations, and 2) an infinite-state symbolic model checker which verifies (or falsifies) CTL properties of Action Language specifications. Our symbolic manipulator (Composite Symbolic Library) combines a BDD manipulator (for boolean and enumerated types) and a Presburger arithmetic manipulator (for integers) to handle multiple variable types. Since we allow unbounded integer variables, model checking queries become undecidable. We present several heuristics used by the Action Language Verifier to achieve convergence.","Object oriented modeling,
Arithmetic,
Binary decision diagrams,
Switches,
Computer science,
Specification languages,
Software systems,
Software libraries,
Formal specifications,
Thyristors"
Distribution expansion problem: formulation and practicality for a multistage globally optimal solution,"Despite numerous research efforts of the past 40 years in the area of distribution expansion, a clear definition of the problem, and a truly multistage formulation that addresses practical concerns is yet to be developed. In this paper, the problem is clearly defined and analyzed from a practical point of view. A directed graph minimum edge cost network flow modeling of the problem for a truly multistage formulation using mathematical programming that guarantees global optimality and addresses the noted deficiencies is proposed. The proposed formulation is implemented on small case studies under varying assumptions. Comparative analysis indicates the importance of improved expansion planning.","Voltage,
Computer science,
USA Councils,
Cost function,
History,
Energy management,
Strategic planning,
Foot,
Sun,
Heuristic algorithms"
Are two pictures better than one?,"A major hurdle in practical content based image retrieval (CBIR) is conveying the user's information need to the system. One common method of query specification is to express the query using one or more example images. The authors consider whether using more examples improves the effectiveness of CBIR in meeting a user's information need. We show that using multiple examples improves retrieval effectiveness by around 9%-20% over single-example queries, but that further improvements in using more than two examples may not justify the added processing required.","Image retrieval,
Image databases,
Information retrieval,
Content based retrieval,
Spatial databases,
Data mining,
Feature extraction,
Computer science,
Multimedia databases,
Manuals"
"New graph bipartizations for double-exposure, bright field alternating phase-shift mask layout","We describe new graph bipartization algorithms for layout modification and phase assignment of bright-field alternating phase-shifting masks (AltPSM). The problem of layout modification for phase-assignability reduces to the problem of making a certain layout-derived graph bipartite (i.e., 2-colorable). Previous work by Berman et al. (2000) solves bipartization optimally for the dark field alternating PSM regime. Only one degree of freedom is allowed (and relevant) for such a bipartization: edge deletion, which corresponds to increasing the spacing between features in order to remove phase conflict. Unfortunately, dark-field PSM is used only for contact layers, due to limitations of negative photoresists. Poly and metal layers are actually created using positive photoresists and bright-field masks. In this paper, we define a new graph bipartization formulation that pertains to the more technologically relevant bright-field regime. The previous work by Berman et al. does not apply to this regime. This formulation allows two degrees of freedom for layout perturbation: (i) increasing the spacing between features, and (ii) increasing the width of critical features. Each of these corresponds to node deletion in a new layout-derived graph that we define, called the feature graph. Graph bipartization by node deletion asks for a minimum weight node set A such that deletion of A makes the graph bipartite. Unlike bipartization by edge deletion, this problem is NP-hard. We investigate several practical heuristics for the node deletion bipartization of planar graphs, including one that has 9/4 approximation ratio. Computational experience with industrial VLSI layout benchmarks shows promising results.","Resists,
Protection,
Phase shifters,
Computer science,
Computer industry,
Very large scale integration,
Interference,
Circuits,
Compaction,
Documentation"
Automated film rhythm extraction for scene analysis,,"Rhythm,
Image analysis,
Motion pictures,
Motion analysis,
Layout,
Feature extraction,
Computer science,
Australia,
Pattern analysis,
Computational modeling"
The dependence list in time warp,"Time Warp is known for its ability to maximize the exploitation of the parallelism inherent in a simulation. However, this potential has been undermined by the cost of processing causality violations. Minimizing this cost has been one of the most challenging issues facing Time Warp. In this paper, we present dependence list cancellation, a direct cancellation technique for Time Warp which is intended for use in a distributed memory environment such as a network of workstations. This approach provides for the swift cancellation of erroneous events, thereby preventing the propagation of their (erroneous) descendants. The dependence list also provides an event filtering function which detects erroneous future events, and also reduces the number of anti-messages used in the simulation. Our experimental work indicates that dependence list cancellation results in a dramatic reduction in the time required to process causality violations in Time Warp.","Discrete event simulation,
Costs,
Time warp simulation,
Parallel processing,
Workstations,
Communication system control,
Very large scale integration,
Computer science,
Filtering,
Event detection"
STRIP - a strip-based neural-network growth algorithm for learning multiple-valued functions,"We consider the problem of synthesizing multiple-valued logic functions by neural networks. A genetic algorithm (GA) which finds the longest strip in V/spl sube/K/sup n/ is described. A strip contains points located between two parallel hyperplanes. Repeated application of GA partitions the space V into certain number of strips, each of them corresponding to a hidden unit. We construct two neural networks based on these hidden units and show that they correctly compute the given but arbitrary multiple-valued function. Preliminary experimental results are presented and discussed.","Logic functions,
Neural networks,
Neurons,
Transfer functions,
Strips,
Network synthesis,
Genetic algorithms,
Multi-layer neural network,
Computer science,
Algebra"
Explicit control of topological transitions in morphing shapes of 3D meshes,"Existing methods of morphing 3D meshes are often limited to cases in which 3D input meshes to be morphed are topologically equivalent. The paper presents a new method for morphing 3D meshes having different surface topological types. The most significant feature of the method is that it allows explicit control of topological transitions that occur during the morph. Transitions of topological types are specified by means of a compact formalism that resulted from a rigorous examination of singularities of 4D hypersurfaces and embeddings of meshes in 3D space. Using the formalism, every plausible path of topological transitions can be classified into a small set of cases. In order to guide a topological transition during the morph, our method employs a key frame that binds two distinct surface topological types. The key frame consists of a pair of ""faces"", each of which is homeomorphic to one of the source (input) 3D meshes. Interpolating the source meshes and the key frame by using a tetrahedral 4D mesh and then intersecting the interpolating mesh with another 4D hypersurface creates a morphed 3D mesh. We demonstrate the power of our methodology by using several examples of topology transcending morphing.","Shape control,
Computer science,
Topology,
Visual effects,
Research and development,
Computer graphics,
Art,
Motion pictures,
TV,
Medical services"
Constructing end-to-end paths for playing media objects,"This paper describes a framework for constructing network services for accessing media objects. The framework, called end-to-end media paths, provides a new approach for building multimedia applications from component pieces. Based on input from the user and resource requirements from the media object, the system first discovers the sequence of nodes (end-to-end path) that both connect the source device to the sink device and possess sufficient resources to play the object. It then configures the individual nodes along this path with the modules (path segment) that implement the service.","Digital audio players,
Personal digital assistants,
Games,
Firewire,
Uniform resource locators,
Access protocols,
Computer architecture,
Computer science,
Buildings,
Application software"
Towards universal software substrate for distributed embedded systems,"The paper proposes a universal software substrate for building various types of distributed embedded systems. The universal software substrate contains an operating system and several middleware components. It offers a universal application programming interface (Universal API) that greatly increases an embedded application's portability. Also, it makes the development speed of distributed embedded applications dramatically fast, since the high level abstraction provided by universal software substrate decreases the amount of software that should be written from scratch. The most important issue of our research is how to control the level of abstraction when designing distributed embedded systems. Our work is looking for methodologies to build portable software for distributed embedded systems in a systematic way. We describe several software components towards realizing universal software substrate, and some research topics for achieving the goals.","Embedded software,
Embedded system,
Home appliances,
Application software,
Computer architecture,
Internet,
Costs,
Computer science,
Software systems,
Microprocessors"
Path selection methods for localized quality of service routing,"Localized quality of service (QoS) routing was recently proposed as an alternative to the QoS routing algorithms that use global network state information to make routing decisions. In localized QoS routing, each router maintains a predetermined set of candidate paths for each of the destinations. A router decides the path for a connection request based on the information maintained locally at the router. Hence, localized QoS routing avoids the problems associated with the maintenance of the global network state information. To achieve good routing performance, localized QoS routing must effectively select the predetermined set of candidate paths. This paper studies path selection methods for localized QoS routing. Five path selection heuristics, namely breadth-first search path selection, per-pair shortest path selection, global path selection, hybrid perpair/global path selection, and per-pair path selection with global tuning, are proposed and their performance is evaluated through simulation. We conclude that path selection methods can greatly affect the performance of localized QoS routing and that an effective path selection algorithm must consider various factors, including path length and load balancing in the whole network.",
Computational challenges in portfolio management,"The authors describe a relatively simple problem that all investors face: managing a portfolio of financial securities over time to optimize a particular objective function. They show how complex such a problem can become when real-world constraints are incorporated into its formulation. More specifically, the authors present the basic dynamic portfolio optimization problem and then consider three aspects of it: taxes, investor preferences, and portfolio constraints. These three issues are by no means exhaustive, they merely illustrate examples of the kinds of challenges financial engineers face today.","Portfolios,
Financial management,
Finance,
Security,
Computer industry,
Bonding,
Scientific computing,
Mathematics,
Professional societies,
Physics computing"
Capturing molecular energy landscapes with probabilistic conformational roadmaps,"Probabilistic roadmaps are an effective tool to compute the connectivity of the collision-free subset of high-dimensional robot configuration spaces. This paper extends them to capture the pertinent features of continuous functions over high-dimensional spaces. We focus here on computing energetically favorable motions of bio-molecules. A molecule is modeled as an articulated structure moving in an energy field. The set of all its 3D placements is the molecule's conformational space, over which the energy field is defined. A probabilistic conformational roadmap (PCR) tries to capture the connectivity of the low-energy subset of a conformational space, in the form of a network of weighted local pathways. The weight of a pathway measures the energetic difficulty for the molecule to move along it. The power of a PCR derives from its ability to compactly encode a large number of energetically favorable molecular pathways, each defined as a sequence of contiguous local pathways. This paper describes general techniques to compute and query PCRs, and presents implementations to study ligand-protein binding and protein folding.","Energy capture,
Proteins,
Drugs,
Computational modeling,
Orbital robotics,
Shape,
Predictive models,
Computer science,
Process design,
Context modeling"
Multiple shared backup cycles for survivable optical mesh networks,"This paper proposes a backup network planning method for survivable WDM mesh networks. The proposed method centers around multiple backup cycles where each network link is assigned m backup cycles and each cycle protects 1/m of the working capacity of a target link. Distributed link restoration is performed using preplanned cycles, in which both the backup paths and the spare capacity can be shared. The preconfiguration of the cycles and the spare capacity placement are derived directly from the network topology off-line, which is independent of the primary traffic status or its dynamic changes over time. The proposed method provides efficiency and simplicity to survivable network design and management, and also to runtime recovery operation. Experimental results show that the proposed method needs on average under 60% of spare capacity redundancy for single link failure while preserving the speed of cycle-based restoration.","Optical fiber networks,
Mesh networks,
Protection,
Runtime,
Computer science,
Wavelength division multiplexing,
WDM networks,
Network topology,
Telecommunication traffic,
High speed optical techniques"
Automatic topic identification using webpage clustering,"Grouping Web pages into distinct topics is one way of organizing the large amount of retrieved information on the Web. In this paper, we report that, based on a similarity metric, which incorporates textual information, hyperlink structure and co-citation relations, an unsupervised clustering method can automatically and effectively identify relevant topics, as shown in experiments on several retrieved sets of Web pages. The clustering method is a state-of-art spectral graph partitioning method based on the normalized cut criterion first developed for image segmentation.","Information retrieval,
Clustering methods,
Clustering algorithms,
Image segmentation,
Computer science,
Laboratories,
Web sites,
Organizing,
Taxonomy,
Search engines"
Minimum sprite plasma density as determined by VLF scattering,"The scattering of VLF sub-ionospheric transmissions by sprite plasma through horizontal angles up to 180/spl deg/ shows that sprite plasma is highly conducting. Following a simple transmission-line (one-dimensional-wave) model in 1997, two-dimensional and three-dimensional models have been produced. Here, we compare the results of the three models, and show that all require a uniform conductivity of at least 30 /spl mu/S/m, corresponding to an electron density at 70 km altitude of /spl sim/10/sup 10/ m/sup -3/ (/spl sim/10/sup 4/ electrons per cc), and so about 10/sup 5//cc at 55 km. The latter ionization density is about that of the daytime E-region, and over six orders of magnitude above the ambient density at 55 km. By contrast, the ""early/fast"" events, as defined by the Stanford group, do not exhibit scatter angles above 15/spl deg/, suggesting that the sprite conductivity rises too slowly (as the plasma cools) to reach adequate backscatter within the time allowed by the ""early/fast"" definition.","Sprites (computer),
Plasma density,
Electromagnetic scattering,
Optical scattering,
Light scattering,
Plasma measurements,
Conductivity,
Nuclear and plasma sciences,
Electrons,
Ionization"
A fuzzy logic based set of measures for software project similarity: validation and possible improvements,"The software project similarity attribute has not yet been the subject of in-depth study, even though it is often used when estimating software development effort by analogy. Among the inadequacies identified (M. Shepperd et al., 1996; 1997) in most of the proposed measures for the similarity attribute, the most critical is that they are used only when the software projects are described by numerical variables (interval, ratio or absolute scale). However, in practice, many factors which describe software projects, such as the experience of programmers and the complexity of modules, are measured in terms of an ordinal (or nominal) scale composed of qualifications such as 'low' and 'high'. To overcome this limitation, we propose a set of new measures based on fuzzy logic for similarity when the software projects are described by categorical data. The proposed measures are validated by means of an axiomatic validation approach. We also present the results of an empirical validation of our similarity measures, based on the COCOMO'81 database.","Fuzzy logic,
Software measurement,
Qualifications,
Programming profession,
Databases,
Software engineering,
Engineering management,
Laboratories,
Computer science,
Software metrics"
Handwritten digit recognition by combining support vector machines using rule-based reasoning,"The idea of combining classifiers in order to compensate their individual weakness and to preserve their individual strength has been widely used in pattern recognition applications. The cooperation of two feature families for handwritten digit recognition using SVM (Support Vector Machine) classifiers is examined. We investigate the advantages and weaknesses of various decision fusion schemes using rule-based reasoning. The obtained results show that it is difficult to exceed the recognition rate of the classifier applied straightforwardly on the feature families as one set. However, the rule-based cooperation schemes enable an easy and efficient implementation of various rejection criteria that leads to high reliability recognition systems.","Handwriting recognition,
Support vector machines,
Support vector machine classification,
Feature extraction,
Information technology,
Pattern recognition,
Character recognition,
Data preprocessing,
Mathematics,
Computer science"
Thinning Arabic characters for feature extraction,"A successful approach to the recognition of Latin characters is to extract features from that character such as the number of strokes, stroke intersections and holes, and to use ad-hoc tests to differentiate between characters which have similar features. The first stage in this process is to produce thinned 1 pixel thick representations of the characters to simplify feature extraction. This approach works well with printed Latin characters which are of high quality. With poor quality characters, however, the thinning process itself is not straightforward and can introduce errors which are manifested in the later stages of the recognition process. The recognition of poor quality Arabic characters is a particular problem since the characters are calligraphic with printed characters having widely varying stroke thicknesses to simulate the drawing of the character with a calligraphy pen or brush. This paper describes the problems encountered when thinning large poor quality Arabic characters prior to the extraction of their features and submission to a syntactic recognition system.","Feature extraction,
Character recognition,
Skeleton,
Optical character recognition software,
Testing,
Optical sensors,
Computer science,
Costs,
Vehicles,
Licenses"
Realization of multiple-output functions by reconfigurable cascades,"A realization of multiple-output logic functions using a RAM and a sequencer is presented. First, a multiple-output function is represented by an encoded characteristic function for non-zeros (ECFN). Then, it is represented by a cascade of look-up tables (LUTs). Finally, the cascade is simulated by a RAM and a sequencer. Multiple-output functions for benchmark functions are realized by cascades of LUTs, and the number of LUTs and levels of cascades are shown. A partition method of outputs for parallel evaluation is also presented. A prototype has been developed by using RAM and FPGA. This realization uses time domain multiplexing, and is useful for the case where the number of output pins is limited.","Binary decision diagrams,
Logic functions,
Programmable logic arrays,
Table lookup,
Read-write memory,
Field programmable gate arrays,
Reconfigurable logic,
Logic programming,
Reconfigurable architectures,
Computer science"
QoS based scheduling for incorporating variable rate coded voice in Bluetooth,"Bluetooth is an emerging standard low-cost indoor pico-cellular wireless systems. It is a master driven time division duplex (TDD) system. Real time services such as voice are given 64 kbps bandwidth in Bluetooth. However most other wireless networks use compressed voice, which requires much lesser bandwidth, leading to a substantial increase in system capacity. Bandwidth can be further conserved by using voice activity detection (VAD) techniques and variable rate voice codecs. We propose and analyse modifications to be made to Bluetooth for incorporating variable rate coded voice. Current mechanisms in Bluetooth use synchronous channels with fixed slot allocation for voice and a best effort service for data. We propose and study two scheduling strategies which optimise bandwidth consumption by using variable rate coded voice. In the first scheme, adaptive T/sub SCO/ scheduling, we modify the conventional scheduling policy to change the time period of scheduling a voice channel depending upon its activity. In the voice over ACL scheduling, we schedule voice asynchronously like data using a QoS based scheduling scheme with maximum scheduling delay tolerable by packets as the QoS parameter. This scheme can also be used to schedule other multimedia applications with varying QoS requirements. We observe from simulations that the voice over ACL scheme gives more than 115% increase in bandwidth over the currently used scheduling in the presence of two voice connections.","Bluetooth,
Bandwidth,
Speech coding,
Quality of service,
Processor scheduling,
Speech codecs,
Computer science,
Code standards,
Wireless networks,
Speech analysis"
Model checking with multi-valued temporal logics,"Multi-valued logics support the explicit modeling of uncertainty and disagreement by allowing additional truth values in the logic. Such logics can be used for verification of dynamic properties of systems where complete, agreed upon models of the system are not available. This paper presents a symbolic model checker for multi-valued temporal logics. The model checker works for any multi-valued logic whose truth values form a quasi-boolean lattice. Our models are generalized Kripke structures, where both atomic propositions and transitions between states may take any of the truth values of a given multi-valued logic. Properties to be model checked are expressed in CTL, generalized with a multi-valued semantics. The design of the model checker is based on the use of MDDs, a multi-valued extension of binary decision diagrams.","Multivalued logic,
Hardware,
Uncertainty,
Logic design,
Logic circuits,
Computer science,
Educational institutions,
Software design,
Occupational safety,
Printers"
Robust detection of stylized text events in digital video,"Automatic content-based video indexing is an important research problem. One approach is to extract text appearing in video as an indication of a scene's semantic content. Most work so far has focused only on detecting the spatial extent of text instances in individual video frames. But text occurring in video usually persists for several seconds. This constitutes a text event that should be entered only once in the video index. Therefore it is necessary to determine the temporal extent of text events by combining the results of text detection on individual frames, over time. This is a nontrivial problem because a text event may move, rotate, grow, shrink, or otherwise change throughout its lifetime. Such text effects are common in television programs and commercials to attract viewer attention, but have so far been ignored in the literature. We present a method for detecting and tracking moving, changing caption text events in MPEG-1 compressed video.","Robustness,
Event detection,
Video compression,
TV,
Data mining,
Transform coding,
Computer science,
Navigation,
Machine assisted indexing,
Motion pictures"
A preliminary topological debugger for MPI programs,"Most parallel programs use regular topologies to support their computation. Since they define the relationship between processes, process topologies present an excellent opportunity for debugging. The primary benefit is that patterns of expected behaviour can be abstracted and identified, and unexpected behaviour reported. However, topology support is inadequate in may environments, including the popular Message Passing Interface (MPI). Programmers typically implement topology support themselves, increasing the possibility of introducing errors. Moreover, debugger support that exploit topological information is lacking. We have undertaken to develop a debugger that exploits topological information. This paper presents DEPICT (DEbugger of Parallel but Inconsistent Communication Traces), a (preliminary) topology-based debugger for MPI. Currently, DEPICT presents high-level visualisations of parallel program communication behaviour, where logically similar processes are clearly indicated in a manner that allows the programmer insight into overall program behaviour. To assist in understanding unexpected behaviour, DEPICT allows programmers to investigate the observed semantic differences between processes. In addition to its current facilities, DEPICT's implementation details and underlying algorithms are also described.","Topology,
Debugging,
Supercomputers,
Costs,
Programming profession,
Computer science,
Software engineering,
Parallel programming,
Power engineering and energy,
Concurrent computing"
Adaptive block rearrangement algorithms for video-on-demand server,"Video-on-demand (VOD) is increasingly becoming one of the most important and successful services due to the recent advances in storage subsystems, compression technology and, networking. Therefore, the investigation of various alternatives to improve the performance of VOD servers has become a major research focus. The reduction of disk access time through intelligent data placement strategies is one such avenue and is the theme of this paper: Movie rental patterns indicate that accesses to movies are highly localized with only a small number of movies receiving most of the accesses. In this paper we exploit the access patterns and propose an adaptive rearrangement of the blocks on each disk within the server. With this approach, the blocks of the movies with comparable access frequencies are kept closer to each other We analyze two rearrangement schemes, called centered and sequential. In the centered layout, blocks are placed according to their access patterns starting with the most popular movie at the center. The sequential layout places movies in the order of their popularity starting at the edge of the disk. We compare and evaluate, through an intensive simulation study, the effectiveness of these layouts with respect to arbitrary layouts. The simulation results indicate that significant disk improvements could be attained by adopting the proposed schemes, and that the centered layout is the best performer.","Motion pictures,
Network servers,
Frequency,
Bandwidth,
Quality of service,
Computer science,
Pattern analysis,
Measurement,
Streaming media"
GPS query optimization in mobile and wireless networks,"We propose an efficient routing algorithm, for mobile ad hoc networks, which we refer to as GZRP, a hybrid protocol that makes use of the zone routing protocol (ZRP) scheme and the Global Positioning System (GPS). As opposed to the ZRP, our GZRP scheme consists of propagating the routing (query) messages only to the nodes that are further away from the query source. We discuss the algorithm, its implementation, and report on the performance of the GZRP scheme. Our results indicate clearly that GZRP outperforms ZRP by reducing significantly the number of route query messages, and thereby increases the efficiency of the network load. Furthermore, we show that a careful GPS screening angle is an important factor in the success of the GZRP ad hoc routing protocol.","Global Positioning System,
Query processing,
Intelligent networks,
Wireless networks,
Routing protocols,
Laboratories,
Computer science,
Mobile ad hoc networks,
Network topology,
Cellular networks"
Dynamic coupled component analysis,"We present a method for simultaneously learning linear models of multiple high dimensional data sets and the dependencies between them. For example, we learn asymmetrically coupled linear models for the faces of two different people and show how these models can be used to animate one face given a video sequence of the other. We pose the problem as a form of Asymmetric Coupled Component Analysis (ACCA) in which we simultaneously learn the subspaces for reducing the dimensionality of each dataset while coupling the parameters of the low dimensional representations. Additionally, a dynamic form of ACCA is proposed, that extends this work to model temporal dependencies in the data sets. To account for outliers and missing data, we formulate the problem in a statistically robust estimation framework. We review connections with previous work and illustrate the method with examples of synthesized dancing and the animation of facial avatars.","Principal component analysis,
Facial animation,
Training data,
Computer vision,
Computer science,
Video sequences,
Robustness,
Avatars,
Face recognition,
Linear approximation"
KaBaGe-RL: Kanerva-based generalisation and reinforcement learning for possession football,"The complexity of most modem systems prohibits a hand-coded approach to decision making. In addition, many problems have continuous or large discrete state spaces; some have large or continuous action spaces. The problem of learning in large spaces is tackled through generalisation techniques, which allow compact representation of learned information and transfer of knowledge between similar states and actions. In this paper Kanerva. coding and reinforcement learning are combined to produce the KaBaGe-RL decision-making module. The purpose of KaBaGe-RL is twofold. Firstly, Kanerva coding is used as a generalisation method to produce a feature vector from the raw sensory input. Secondly, the reinforcement learning uses this feature vector in order to learn an optimal policy. The efficiency of KaBaGe-RL is tested using the ""3 versus 2 possession football"" challenge, a subproblem of the RoboCup domain. The results demonstrate that the learning approach outperforms a number of benchmark policies including a hand-coded one.","Decision making,
State-space methods,
Function approximation,
Supervised learning,
Computer science,
Modems,
Benchmark testing,
State estimation,
Neural networks,
Fuzzy logic"
An in-system routing strategy for evolvable hardware programmable platforms,"On of the major limiting factors for the development of hardware platforms able to support evolvable hardware principles is the lack of simple and compact in-system dynamic routing strategies. In this paper we shall present a programmable hardware architecture whose internal organization permits to perform dynamic routing processes. The architecture is based on a regular bi-dimensional array of functional cells. A hierarchical layered organization has been provided for these cells. Specific routing resources have been included in one of these layers, so that they permit to construct in an incremental way routing paths among the functional cells. The dynamic routing strategy is based on a replication process that is able to connect a source cell with various target cells. One of the major advantages of the proposed routing strategy lies in the fact that its complexity grows only linearly with the array size. Furthermore it is scalable, accommodating without performance degradation to any array size. Behavioral hardware descriptions haven been created for the functional cells that constitute the array. As the simulation and synthesis results will show, the proposed routing strategy will permit the implementation of actual evolvable hardware principles.","Routing,
Hardware,
Field programmable gate arrays,
System-on-a-chip,
Computer architecture,
Computer science,
Logic,
Laboratories,
Degradation,
Velocity measurement"
Polynomial time synthesis of Byzantine agreement,"We present a polynomial time algorithm for automatic synthesis of fault-tolerant distributed programs, starting from fault-intolerant versions of those programs. Since this synthesis problem is known to be NP-hard, our algorithm relies on heuristics to reduce the complexity. We demonstrate that our algorithm is able to synthesize an agreement program that tolerates a Byzantine fault.",
How to compare the performance of two SMT microarchitectures,,"Surface-mount technology,
Microarchitecture,
Multithreading,
Yarn,
Computational modeling,
Measurement,
Throughput,
Space exploration,
Computer simulation,
Computer science"
Haptic sculpting of volumetric implicit functions,"Implicit functions characterized by the zero-set of polynomial-based algebraic equations and other commonly-used analytic equations are extremely powerful in graphics, geometric design, and visualization. But the potential of implicit functions is yet to be fully realized due to the lack of flexible and interactive design techniques. The paper presents a haptic sculpting system founded upon scalar trivariate B-spline functions. All the solids sculpted in our environment are semi-algebraic sets of volumetric implicit functions. We develop a large variety of sculpting toolkits equipped with an intuitive haptic interface to facilitate the direct manipulation of implicit functions in real-time. To facilitate multiresolution editing and different levels of details, we employ three techniques: hierarchical B-splines, CSG-based functional composition, and knot insertion. Our experiments demonstrate that our algorithms and haptics-based techniques can greatly overcome the modeling difficulties associated with implicit functions. The novel modeling techniques and their haptics-based design principle are extensible to the design of arbitrary implicit functions.","Haptic interfaces,
Equations,
Spline,
Solid modeling,
Polynomials,
Shape control,
Virtual environment,
Computer science,
Computer graphics,
Visualization"
Bandwidth brokers of instantaneous and book-ahead requests for differentiated services networks,"The quality of service (QoS) reservations in differentiated service (DiffServ) networks can be classified into two sets: book-ahead (BA) requests and instantaneous requests (IRs). When an admitted BA request becomes active, some ongoing IRs are dropped when the bandwidth is insufficient for supporting both IRs and BA requests. The admission control should predict the lifetime, i.e. look-ahead time, of the IRs to prevent the admitted IRs from being dropped. The control should then check whether the available bandwidth during the look-ahead time is sufficient for the incoming IRs. We propose an application-aware look-ahead admission control for IRs, which determines the look-ahead time for specific types of IR applications. An admitted BA request might block subsequent ones that could bring more effective revenue. Thus, we propose the deferrable model of the admission control for BA requests. Simulation results indicate that the application-aware look-ahead admission control successfully reduces the dropping probability and wasted revenue of IRs by up to 10 times and 30%, respectively. Besides, the deferrable model indeed results in more BA effective revenue.","Bandwidth,
Diffserv networks,
Admission control,
Quality of service,
Computer networks,
Information science,
Books,
Wire,
Communication system traffic control,
Upper bound"
An experiential approach to incorporating software testing into the computer science curriculum,"Testing accounts for about half of the cost of software, but testing receives little treatment in most curricula. This paper presents an approach to giving students experiences in software testing throughout the curriculum, rather than the usual approach of offering a separate course in testing. The centerpiece of the authors' approach is the Software TestLab, which exists to train selected students in the art and science of testing, and to transfer testing practice into core courses. TestLab students become agents of technology transfer. This paper describes the conceptual framework used to define the set of essential test experiences, and presents lessons learned, to date.","Software testing,
Computer science,
Software quality,
Costs,
Educational institutions,
Engineering profession,
Art,
Technology transfer,
Certification,
Software systems"
A general-purpose CMOS vision chip with a processor-per-pixel SIMD array,"The paper discusses the architecture and implementation of a new SIMD focal-plane processor array integrated circuit. The chip employs switched-current ""analogue microprocessors"" as processing nodes in a digital-like massively parallel computer architecture. Using analogue processing elements allows the achievement of real-time image processing speeds with high efficiency in terms of silicon area and power dissipation. The prototype 2121 SCAMP vision chip is fabricated in a 0.6m CMOS technology and achieves a cell size of 98.6m98.6m. The approach is compared with state-of-the-art vision chips build using digital SIMD arrays and CNN-based processors. Experimental results are presented.","CMOS process,
Image processing,
Optical arrays,
CMOS technology,
Computer vision,
Power dissipation,
Circuits,
Pixel,
Sensor arrays,
Hardware"
Just how accurate are performance counters?,"The wide use of performance counters by application developers and benchmarking teams gives evidence that performance counters are well worth the silicon and design time required to include them on modern microprocessors. These counters provide rudimentary performance measurements that may or may not be accurate. This paper presents our methodology for determining the accuracy of these counters as well as preliminary results of a study that, using this methodology, evaluates the accuracy of the R12000 performance counters with respect to eight of 30 measurable events. The results indicate that care must be taken when using data generated by performance counters because, in some cases, this data may lead to erroneous conclusions. This can occur when the granularity of the measured code is not sufficient to ensure that the overhead introduced by counter interfaces does not dominate the event counts.","Counting circuits,
Application software,
Hardware,
Computer science,
Silicon,
Operating systems,
Performance analysis,
Microprocessors,
Measurement,
Sun"
An improved generalization of mesh-connected computers with multiple buses,"Mesh-connected computers (MCCs) are a class of important parallel architectures due to their simple and regular interconnections. However, their performances are restricted by their large diameters. Various augmenting mechanisms have been proposed to enhance the communication efficiency of MCCs. One major approach is to add nonconfigurable buses for improved broadcasting. A typical example is the mesh-connected computer with multiple buses (MMB). We propose a new class of generalized MMBs, the improved generalized MMBs (IMMBs). We compare IMMBs with MMBs and a class of previously proposed generalized MMBs (GMMBs). We show the power of IMMBs by considering semigroup and prefix computations. Specifically, as our main result we show that for any constant 0","Concurrent computing,
Distributed computing,
Broadcasting,
Parallel architectures,
Parallel processing,
Computer science,
Parallel algorithms,
Nearest neighbor searches,
Hardware,
Image processing"
Automatic generation of database instances for white-box testing,"Testing is a critical activity for database application programs as faults if undetected could lead to unrecoverable data loss. Database application programs typically contain statements written in an imperative programming language with embedded data manipulation commands, such as SQL. However relatively little study has been made in the testing of database application programs. In particular, few testing techniques explicitly consider the inclusion of database instances in the selection of test cases and the generation of test data input. In this paper, we study the generation of database instances that respect the semantics of SQL statements embedded in a database application program. The paper also describes a supporting tool which generates a set of constraints. These constraints collectively represent a property against which the program is tested. Database instances for program testing can be derived by solving the set of constraints using existing constraint solvers.","Automatic testing,
Application software,
Software testing,
Transaction databases,
System testing,
Database systems,
Calculus,
Computer science,
Logic,
Relational databases"
From verification to control: dynamic programs for omega-regular objectives,"Dynamic programs, or fixpoint iteration schemes, are useful for solving many problems on state spaces. For Kripke structures, a rich fixpoint theory is available in the form of the /spl mu/-calculus, yet few connections have been made between different interpretations of fixpoint algorithms. We study the question of when a particular fixpoint iteration scheme /spl phi/ for verifying an /spl omega/-regular property /spl Psi/ on a Kripke structure can be used also for solving a two-player game on a game graph with winning objective /spl Psi/. We provide a sufficient and necessary criterion for the answer to be affirmative in the form of an extremal-model theorem for games: under a game interpretation, the dynamic program /spl phi/ solves the game with objective /spl Psi/ iff both (1) under an existential interpretation on Kripke structures, /spl phi/ is equivalent to /spl exist//spl Psi/, and (2) under a universal interpretation on Kripke structures, /spl phi/ is equivalent to /spl forall//spl Psi/. In other words, /spl phi/ is correct on all two-player game graphs iff it is correct on all extremal game graphs, where one or the other player has no choice of moves. The theorem generalizes to quantitative interpretations, where it connects two-player games with costs to weighted graphs. While the standard translations from /spl omega/-regular properties to the /spl mu/-calculus violate (1) or (2), we give a translation that satisfies both conditions. Our construction, therefore, yields fixpoint iteration schemes that can be uniformly applied on Kripke structures, weighted graphs, game graphs, and game graphs with costs, in order to meet or optimize a given /spl omega/-regular objective.","Game theory,
Equations,
State-space methods,
Cost function,
Calculus"
Kinetic collision detection: algorithms and experiments,"Efficient collision detection is important in many robotic tasks, from high-level motion planning in a static environment to low-level reactive behavior in dynamic situations. Specially challenging are problems in which multiple robots are moving among multiple moving obstacles. In this paper we present a number of collision detection algorithms formulated under the kinetic data structures (KDS) framework, a framework for design and analyzing algorithms for objects in motion. The KDS framework leads to event-based algorithms that sample the state of different parts of the system only as often as necessary for the task at hand. Earlier work has demonstrated the theoretical efficiency of KDS algorithms. In this paper we present new algorithms and demonstrate their practical efficiency as well as by an implementable and direct comparison with classical broad and narrow phase collision detection techniques.","Kinetic theory,
Motion detection,
Robots,
Object detection,
Phase detection,
Computer science,
Detection algorithms,
Algorithm design and analysis,
Computational modeling,
Computer aided manufacturing"
Asynchronous collaboration around multimedia and its application to on-demand training,"Multimedia content is a central component of on-demand training and education delivered over the World Wide Web. Supporting asynchronous collaboration around educational multimedia is a key requirement for making the online educational environment effective. A multimedia annotation system tightly integrated with email provides a powerful platform on which to base such functionality. Building on our earlier work with multimedia annotations, we present user interface and system extensions to support asynchronous collaboration for on-demand training. We report results from a study of the effectiveness of the system, including student experience, instructor experience, and user interface appropriateness. Overall, the student experience was positive: they appreciated the flexibility of on-demand delivery and benefited from the collaborative features.","Collaboration,
Multimedia systems,
Collaborative work,
Application software,
User interfaces,
TV broadcasting,
Streaming media,
Internet,
Watches,
Computer science"
VIBe: a micro-benchmark suite for evaluating virtual interface architecture (VIA) implementations,"The Virtual Interface Architecture (VIA) has been recently proposed to standardize different existing user-level networking protocols for System Area Networks (SANs). Since the introduction of VIA, software and hardware implementations of VIA have become available. VIA has different components (such as doorbells completion queues, and virtual-to-physical address translation) and attributes (such as maximum transfer unit and reliability modes). Different implementations of VIA lead to different design strategies for efficiently implementing higher level communication layers/libraries (such as Message Passing Interface (MPI)). It also has implication on the performance of applications: Currently, there is no framework for evaluating different design choices and for obtaining insight about the design choices made in a particular implementation of VIA and their impact on the performance. In this paper, we address these issues by proposing a new microbenchmark suite called Virtual Interface Architecture Benchmark (VIBe) This suite consists of several microbenchmarks which are divided into three major categories: non-data transfer related micro-benchmarks, data transfer related micro-benchmarks, and programming model related micro-benchmarks. By using the new benchmark suite, the performance of VIA implementations can be evaluated under different communication scenarios and with respect to the implementation of different components and attributes of VIA. We demonstrate the use of VIBe to evaluate three implementations of VIA (M-VIA on Gigabit Ethernet, Berkeley VIA on Myrinet and cLAN VIA on Giganet). We show how the VIBe suite can provide insights to the implementation details of VIA and help software developers of programming model layers on top of VIA.","Computer architecture,
Access protocols,
Hardware,
Storage area networks,
Telecommunication network reliability,
Network interfaces,
Information science,
Software libraries,
Message passing,
Ethernet networks"
Integration of domain-specific elements into visual language based collaborative environments,"This paper presents an approach for the integration of domain related elements and operational semantics into collaborative environments based on visual languages. This integration allows for supporting domain specific collaborative tasks, e.g. in the area of ""collaborative discovery learning"" in science education, by integrating data modelling with generic discussion support. A special focus is set on flexibility and parameterisation of the system which is achieved through providing the syntax definition of the visual language as a separate resource file.","Collaboration,
Computer science education,
Collaborative work,
Shape,
Petri nets,
Computer languages,
Laboratories,
Buildings,
Visualization,
Process design"
Omni-rig: linear self-recalibration of a rig with varying internal and external parameters,"We describe the principles of building a moving vision platform (a Rig) that once calibrated can thereon self-adjust to changes in its internal configuration and maintain an Euclidean representation of the 3D world using only projective measurements. We term this calibration paradigm ""Omni-Rig"". We assume that after calibration the cameras may change critical elements of their configuration, including internal parameters and centers of projection. Theoretically we show that knowing only the rotations between a set of cameras is sufficient for Euclidean calibration even with varying internal parameters and unknown translations. No other information of the world is required.","Cameras,
Calibration,
Computer science,
Maintenance engineering,
Image recognition,
Computer graphics,
Machinery,
Nonlinear equations"
The preplanned weighted restoration scheme,"One of the expected benefits of wavelength division multiplexing (WDM) is the possibility to provide an optical layer (OL) with built-in capabilities to survive network component faults, e.g., fiber cut. Several schemes exist to design a reliable OL. These schemes are generally divided into protection and restoration techniques. Protection schemes reserve in advance a dedicated backup path and a wavelength that are readily available upon disruption of the working path. Restoration schemes, on the contrary, dynamically look for backup paths of spare wavelengths upon failure occurrence. The authors propose a fast and efficient path restoration scheme called preplanned weighted restoration (PWR). In the PWR scheme distinct restoration paths are precomputed at the source node during the connection setup. Upon failure, one of the preplanned paths is randomly selected depending on specific weights precalculated by the source node. The proposed scheme requires limited signaling upon failure occurrence as coordination among the source nodes involved in the restoration process is not required. Therefore, the scheme is fast and scalable in terms of number of network nodes, link or fiber capacity, and number of connections. Yet, the presented results show that the PWR scheme may considerably decrease the blocking probability of the restoration attempts when compared to other schemes such as alternate routing.",
Mediating diverse visualisations for comprehension,"For ease of use, comprehension, and general acceptability it is important that tools that purport to aid comprehension are able to provide a variety of customisable views. The most important underlying factor is the wide differences between users in terms of their culture, experience, visual together, working preferences, and general abilities. Comprehension studies have also shown that different views, and the ability to selectively filter data are important. Combining all of these ideals into one, automated infrastructure is the focus of this paper. Component technologies provide a way of making this a reality, and also for providing visualisation creators with a general application into which they can leverage their work. Advances in this arena have allowed for component brokerage and automatic dynamic data connections to become an actuality.","Data visualization,
Software systems,
Software tools,
Usability,
Software engineering,
Computer science,
Filters,
Displays,
Lead"
An estimator for functional data with application to MRI,"The authors propose a method for restoring the underlying true signal in noisy functional images. The Nadaraya-Watson (NW) estimator described in, e.g., G. S. Watson, ""Smooth regression analysis,"" Sankhya Series A, vol. 26, p. 101-16 (1964) is a classical nonparametric estimator for this problem. Since the true scene in many applications contains abrupt changes between pixels of different types, a modification of the NW estimator is needed. In the data the authors study, the characteristics of each pixel are given as a function of time. This means that a curve of data points is observed at each pixel. Utilizing this time information, the NW weights can be modified to obtain a weighted average over pixels with the same true value. Theoretical results showing the estimator's properties are developed. Several parameters play an important role for the restoration result. Practical guidelines are given for how these parameters can be selected. Finally, the authors demonstrate how the method can be successfully applied both to artificial data and Magnetic Resonance Images.","Magnetic resonance imaging,
Image restoration,
Blood,
Mathematics,
Biomedical imaging,
Data analysis,
Smoothing methods,
Speech recognition,
Signal restoration,
Layout"
Detecting conflicts in a role-based delegation model,"The RBAC96 access control model has been the basis for extensive work on role-based constraint specification and role-based delegation. However these practical extensions can also lead to conflicts at compile and run-time. We demonstrate, following a role-based, declarative approach, how conflicts between specified separation of duty constraints and delegation activities can be detected. This approach also demonstrates the general suitability of Prolog as an executable specification language for the simulation and analysis of role-based systems. Using an extended definition of a role we show how at least one of the conflicts can be resolved and discuss the impacts of this extension on the specified constraints.","Access control,
Runtime,
Computer science,
Specification languages,
Analytical models,
Role transfer,
NIST,
Permission"
Generating daily changes in market variables using a multivariate mixture of normal distributions,The mixture of normal distributions provides a useful extension of the normal distribution for modeling of daily changes in market variables with fatter-than-normal tails and skewness. An efficient analytical Monte Carlo method is proposed for generating daily changes using a multivariate mixture of normal distributions with arbitrary covariance matrix. The main purpose of this method is to transform (linearly) a multivariate normal with an input covariance matrix into the desired multivariate mixture of normal distributions. This input covariance matrix can be derived analytically. Any linear combination of mixtures of normal distributions can be shown to be a mixture of normal distributions.,"Gaussian distribution,
Covariance matrix,
Probability distribution,
Portfolios,
Mathematics,
Computer science,
Finance,
Transforms,
Analysis of variance,
Nonlinear equations"
An automated tool for analyzing Petri nets using Spin,"The Spin model checker is a system that has been used to model and analyze a large number of applications in several domains including the aerospace industry. One of the novelties of Spin is its relatively simple specification language, Promela, as well as the powerful abilities of the model checker. The Petri net notation is a mathematical tool for modeling various classes of systems, especially those that involve concurrency and parallelism. The Honeywell Domain Modeling Environment (DOME) is a tool that supports system design using a wide variety of modeling notations, including UML diagrams and Petri nets. We describe a tool that supports the use of the Spin model checker to analyze and verify Petri net specifications that have been constructed using the DOME tool. In addition to discussing the translation of Petri nets into Promela, we present several example Petri net specifications as well as their analysis using Spin.","Petri nets,
Power system modeling,
Application software,
Specification languages,
Unified modeling language,
Computer science,
Electronic mail,
Aerospace industry,
Mathematical model,
Concurrent computing"
Fast k-space sample selection in MRSI with a limited region of support,"One of the primary drawbacks in the application of magnetic resonance spectroscopic imaging is the long acquisition times required to obtained the desired resolution. When region of support information is available, the number of phase-encoding steps and thus time can be reduced without loss of information if the k-space locations are chosen well. The authors propose to select locations using a rectangular sampling array that is shifted to various positions in k-space to obtain the necessary sampling density. This method allows multiple samples to be selected simultaneously and reduces the computation required to evaluate the selection criterion. The authors present an efficient forward selection algorithm for optimizing the shift pattern so that the image can be reconstructed as reliably as possible from a periodic nonuniform set of samples. The proposed algorithm has important practical potential in that it can finish the selection in less than half a minute for typical image sizes and can reconstruct the image with fewer samples than regular sampling. With appropriate imaging hardware, this new algorithm makes selective sampling possible in a real-time image acquisition setting.","Spectroscopy,
Magnetic resonance imaging,
Image sampling,
Spatial resolution,
Image resolution,
Image reconstruction,
Hardware,
Heart,
Myocardium,
Lipidomics"
A framework for segmentation of talk and game shows,"In this paper, we present a method to remove commercials from talk and game show videos and to segment these videos into host and guest shots. In our approach, we mainly rely on information contained in shot transitions, rather than analyzing the scene content of individual frames. We utilize the inherent differences in scene structure of commercials and talk shows to differentiate between them. Similarly, we make use of the well-defined structure of talk shows, which can be exploited to classify shots as host or guest shots. The entire show is first segmented into camera shots based on color histogram. Then, we construct a data-structure (shot connectivity graph) which links similar shots over time. Analysis of the shot connectivity graph helps us to automatically separate commercials from program segments. This is done by first detecting stories, and then assigning a weight to each story based on its likelihood of being a commercial. Further analysis on stories is done to distinguish shots of the hosts from shots of the guests. We have tested our approach on several full-length shows (including commercials) and have achieved video segmentation with high accuracy. The whole scheme is fast and works even on low quality video (160/spl times/120 pixel images at 5 Hz).","Games,
Testing,
Image segmentation,
Software libraries,
Digital video broadcasting,
Computer vision,
Computer science,
Information analysis,
Layout,
Cameras"
Substation monitoring by acoustic emission techniques,"The deterioration of oil-paper insulation in high voltage equipment is a matter of continuous concern. The degradation typically occurs under normal operating conditions although insulation failure may be accelerated under certain in-service conditions, such as switching and lightning impulses, overvoltages and moisture ingress. For this reason, only continuous monitoring can provide the necessary field data to better understand in-service insulation behaviour, particularly insulation degradation, thus preventing catastrophic damage to property and risk to human life. Based on the promising results obtained previously from an acoustic emission (AE) portable detection system an AE substation monitoring system has been developed. This has been operational in a 33 kV primary substation for eighteen months and detailed data have been collected and downloaded to a computer from a remote site on a daily basis. The analysed results have indicated that a specific cable box in the substation is a source of heavy discharge activity and a recommendation has been made to the Regional Electricity Company that a high-voltage test be carried out on the suspected switchgear for further investigation.","high-voltage techniques,
acoustic emission testing,
acoustic applications,
substation insulation,
insulation testing,
fault diagnosis,
overvoltage protection,
partial discharge measurement,
computerised monitoring,
moisture"
Integrating symbolic and numeric techniques in atomic physics,"The article describes ISNAP, a program for calculating atomic properties that uses an integrated symbolic and numerical approach for arbitrary excitations from closed-shell atoms. This program generates transition matrix elements and energy formulas up to third-order perturbation via the symbolic programming language Mathematica.","Computer languages,
Computer errors,
Physics computing,
Power engineering and energy,
Supercomputers,
Performance evaluation,
Software testing,
Lithium,
Writing,
Bridges"
Flycasting: using collaborative filtering to generate a playlist for online radio,"In recent years, the popularity of online radio has exploded. This new entertainment medium affords an opportunity not available to conventional broadcast radio: the instantaneous listening audience can be known, or what is more important, the musical tastes of the current listening audience can be known. Thus, it is possible in the new medium to tailor the playlist in real-time to the musical tastes of the listening audience. We discuss a method, termed flycasting, for using collaborative filtering techniques to generate a playlist in real-time based on the request histories of the current listening audience. We also describe a concrete implementation of the technique.","Online Communities/Technical Collaboration,
Filtering,
Radio broadcasting,
History,
Auditory system,
Web pages,
Computer science,
Concrete,
Broadcast technology,
Contracts"
A constructive algorithm for memory-aware task assignment and scheduling,"This paper presents a constructive algorithm for memory-aware task assignment and scheduling, which is a part of the prototype system MATAS. The algorithm is well suited for image and video processing applications which have hard memory constraints as well as constraints on cost, execution time, and resource usage. Our algorithm takes into account code and data memory constraints together with the other constraints. It can create pipelined implementations. The algorithm finds a task assignment, a schedule, and data and code memory placement in memory. Infeasible solutions caused by memory fragmentation are avoided. The experiments show that our memory-aware algorithm reduces memory utilization compared to greedy scheduling algorithm which has time minimization objective. Moreover, memory-aware algorithm is able to find task assignment and schedule when time minimization algorithm fails. MATAS can create pipelined implementations, therefore the design throughput is increased.","Scheduling algorithm,
Memory management,
Processor scheduling,
Costs,
Computer science,
Permission,
Application software,
Minimization methods,
Throughput,
Signal processing"
Overtraining in fuzzy ARTMAP: Myth or reality?,"We examine the issue of overtraining in fuzzy ARTMAP. Over-training in fuzzy ARTMAP manifests itself in two different ways: 1) it degrades the generalization performance of fuzzy ARTMAP as training progresses; and 2) it creates unnecessarily large fuzzy ARTMAP neural network architectures. In this work we demonstrate that overtraining happens in fuzzy ARTMAP and propose an old remedy for its cure: cross-validation. In our experiments we compare the performance of fuzzy ARTMAP that is trained: 1) until the completion of training, 2) for one epoch, and 3) until its performance on a validation set is maximized. The experiments were performed on artificial and real databases. The conclusion derived from these experiments is that cross-validation is a useful procedure in fuzzy ARTMAP, because it produces smaller fuzzy ARTMAP architectures with improved generalization performance. The trade-off is that cross-validation introduces additional computational complexity in the training phase of fuzzy ARTMAP.","Neural networks,
Fuzzy neural networks,
Fuzzy sets,
Databases,
Computer architecture,
Computer science,
Degradation,
Computational complexity,
Testing,
Supervised learning"
Cognitive effects of animated visualization in exploratory visual data analysis,"The goal of this research is to study the role and effects of the use of animated information visualization in the early stages of exploratory data analysis tasks. Despite the existence of a large body of research on information visualization, there is little known regarding how and when one should use and how to interact with animated visualization to help explore data. By animated visualization, we mean a type of information visualization technique that produces autonomous motions of representations. This research explored the issue from two aspects: what cognitive effects animated information visualization has, and what interactions people have with animated visualization when exploring data. We conducted two user studies to investigate each aspect, and identified research challenges for designing an interactive animated information visualization environment that supports the early stages of exploratory data analysis. These findings help us further study how to extend the notions developed in spatial visualization to temporal visualization - e.g. what focus+context means when applied to the time dimension in animated visualization.","Animation,
Data visualization,
Data analysis,
Space exploration,
Motion analysis,
Object oriented programming,
Laboratories,
Information science,
Computer graphics,
Libraries"
Investigating the relationship between usability and conceptual gaps for human-centric CASE tools,"Several interviews that we conducted highlight that many of the ease-of-use (usability) problems of CASE tools are instances of ""conceptual gaps"". A conceptual gap arises because of some difference between the software developer's mental model of the integrated development environment (IDE) and the way it can be used. Filling these gaps is the first step towards human-centric IDE. In this article, we begin by motivating our investigations with a survey highlighting common usability problems in the most popular Java IDEs. We then discuss how the developer's experiences with the complicity of cognitive studies can minimize these conceptual gaps while making the IDE more human-centered. We close our discussion with recommendations for establishing a rigorous scientific investigation for filling these conceptual gaps, as well as for developing and evaluating the ease of use of IDEs.","Usability,
Computer aided software engineering,
Programming,
Cognitive science,
Productivity,
Conducting materials,
Computer science,
Electronic mail,
Filling,
Java"
Efficient data dissemination to mobile clients in e-commerce applications,"Mobile commerce is the next growing area in electronic commerce and mobile computing. These are sophisticated, data-intensive mobile applications whose success depends strongly on the efficiency by which data are disseminated to a large number of mobile users. Different techniques have been put forward, of which the most promising are the push-based techniques that explore the asymmetry in wireless communications and the reduced energy consumption of the receiving mode on mobile devices. This paper proposes a new broadcast indexing scheme, termed ""constant-size I-node distributed indexing"" (CI), that offers more energy savings in practical applications. Our detailed simulation results indicate that CI, which is a variant of the currently best-performing distributed indexing scheme, outperforms the latter for broadcast sizes of 12,000 or fewer data items, reducing the access time by up to 25% and the tuning time by up to 15%.","Broadcasting,
Indexing,
Energy consumption,
Application software,
Mobile computing,
Electronic commerce,
Mobile communication,
Bandwidth,
Switches,
Computer science"
A conceptual framework for demographic groups resistant to online community interaction,"Demographic groups normally do not constitute communities. However they have the potential for becoming online communities when individuals share common interests, needs and goals for problem solving and support and when they can easily find and communicate with each other and establish relationships. People in these demographic groups may be highly resistant to interacting online despite their regular use of the Internet for information gathering and email. The causes for this resistance, suggestions for mitigating it and approaches for bringing such demographic groups into online community environments and sustaining their online interaction are presented. The article describes a study of a demographic group of mid-life career changers. Suggested improvements in online community sociability and usability are recommended for these demographic groups to successfully interact online. In particular, the sociability and usability requirements of these demographic groups should direct the selection and implementation of technology.","Demography,
Engineering profession,
Usability,
Internet,
Discussion forums,
Computer networks,
Problem-solving,
Social network services,
Privacy,
Subscriptions"
Experiences from teaching PSP for freshmen,"The Personal Software Process (PSP) is launched as a means for improving software development capabilities for the individual engineer. It is proposed that it should be used in software engineering curricula; some authors propose it to be used already during the first student year. The PSP course is successfully given for graduate students at Lund University since 1996. During the spring semester of 1999, it was given to undergraduate students at the software engineering program in their first year of university studies, directly after their first programming course. This paper reports results and experiences from the course given to these freshmen students. A quantitative analysis is conducted to compare the freshmen student data to data from graduate student courses, and a qualitative evaluation is conducted concerning the differences between the courses. It can be concluded that mostly the same improvement trends can be identified with freshmen students as with graduate students. However, the qualitative analysis shows that the freshmen students are more concerned with programming than with the software process issues. Based on the results, it is decided to move the PSP course to the second year in order to enable the programming skills to be better established before the PSP is launched.","Programming profession,
Application software,
Software engineering,
Computer science,
Costs,
Engineering management,
Computer science education,
Educational programs,
Springs,
Software measurement"
Multi-resolution stereo matching using genetic algorithm,"In this paper, a new genetic-based stereo matching algorithm is presented. Our motivation is to improve the accuracy of the disparity map generated by removing the mismatches caused by both occlusions and false targets. In our approach, the stereo matching problem is considered as an optimization problem. The algorithm first takes advantage of multi-view stereo images to detect occlusions, therefore, removes mismatches caused by visibility problems. A genetic algorithm is then used to optimize both the compatibility between corresponding points and the continuity of the disparity map, which removes mismatches caused false targets. In addition, the quadtree structure is used to implement a multiresolution framework. Since nodes at different level of the quadtree cover different number of pixels, selecting nodes at different levels gives similar effect as adjusting the window size at different locations of the image. The experimental results show that our approach can generate more accurate disparity maps than two existing approaches.","Genetic algorithms,
Stereo vision,
Multiresolution analysis,
Pixel,
Cameras,
Computer vision,
Image edge detection,
Iterative algorithms,
Computer science,
Markov random fields"
An agent-based personalized distance learning system,"The Internet has becoming very popular and many education learning systems which use the Internet are proposed. There now exist many stand alone education systems. But, the use of computer networks make possible distance learning which will be a more effective learning process. To build a distance learning systems agent-based systems can be an effective way. An agent can examine the learner's information from the learning history. This information is used to provide the right teaching materials. In this paper, we propose an agent-based personalized distance learning system which can deliver the learning materials and test the learner's understanding.","Computer aided instruction,
Computer science education,
Computer networks,
Internet,
Educational programs,
Network servers,
Learning systems,
Application software,
TV broadcasting,
World Wide Web"
Symbolic vs. connectionist learning: an experimental comparison in a structured domain,"During the last two decades, the attempts to find effective solutions to the problem of learning any kind of structured information have been splitting the scientific community. A ""holy war"" has been fought between the advocates of a symbolic approach to learning and the advocates of a connectionist approach. One of the most repeated claims of the symbolic party has been that symbolic methods are able to cope with structured information while connectionist ones are not. However, in the last few years, the possibility of employing connectionist methods for structured data has been widely investigated and several approaches have been proposed. A novel algorithm for learning structured descriptions, ascribable to the category of symbolic techniques, is proposed. It faces the problem directly in the space of graphs by defining the proper inference operators, as graph generalization and graph specialization, and obtains general and consistent prototypes with a low computational cost with respect to other symbolic learning systems. The proposed algorithm is compared with a recent connectionist method for learning structured data (P. Frasconi et al., 1998), with reference to a problem of handwritten character recognition from a standard database on the Web. The orthogonality of the two approaches strongly suggests their combination in a multiclassifier system so as to retain the strengths of both of them, while overcoming their weaknesses. The results on an experimental case study demonstrated that the adoption of a parallel combination scheme of the two algorithms could improve the recognition performance by about 10 percent. A truce or an alliance between the symbolic and the connectionist worlds?.","Prototypes,
Machine learning,
Inference algorithms,
Computational efficiency,
Learning systems,
Character recognition,
Databases,
Machine learning algorithms,
Computer science,
Software engineering"
Transport layer support for highly-available network services,"We advocate a transport layer protocol for highly available network services by means of transparent migration of the server endpoint of a live connection between cooperating servers that provide the same service. We propose a transport protocol that: (i) offers a better alternative than the simple retransmission to the same server, which may be suffering from overload or a DoS attack, may be down, or may not be easily reachable due to congestion; and (ii) decouples a given service from the unique/fixed identity of its provider. Our protocol can be viewed as an extension to the existing TCP, and compatible with it.","Network servers,
Web server,
Transport protocols,
Degradation,
Computer crime,
Resumes,
Computer science,
Delay,
Throughput,
Web and internet services"
Experimentation in the IA program,"Over the past two years, DARPA's Information Assurance program has employed an approach which is unique for a computer science-based effort: a paradigm of experimentation has been used to identify, pursue, and investigate key issues in the development of an integrated approach to assurance for information systems. This paper describes the motivation for the experimental approach, how that approach was applied, strengths and weaknesses of the approach, and provides some example results.","Information systems,
ISO,
Testing,
Computer science,
Springs,
Command and control systems,
Ice,
Particle measurements,
Systems engineering and theory,
Organizing"
Maintenance support tools for Java programs: CCFinder and JAAT,"This paper describes two software maintenance support tools, CCFinder (Code Clone Finder) and JAAT (Java Alias Analysis Tool), for Java programs. CCFinder identifies code clones in Java programs, while JAAT executes alias analysis for Java programs.","Java,
Cloning,
Software maintenance,
Maintenance engineering,
Software systems,
Information science,
Algorithm design and analysis,
Large-scale systems,
Detection algorithms,
Computer industry"
Transparent distributed Web caching,"Layer 5 switching-based transparent Web caching intercepts HTTP requests and redirects requests according to their contents. This technique makes the deployment and configuration of a caching system easier and improves its performance by ensuring that non-cacheable HTTP requests bypass the cache servers. We propose a Load Balancing Layer 5 switching-based (LB-L5) Web caching scheme that uses the Layer 5 switching-based technique to support distributed Web caching. We present simulation results that show that LB-L5 outperforms existing Web caching schemes, namely ICP, Cache Digest, and basic L5 transparent Web caching, in terms of cache server workload balancing and response time. LB-L5 is also shown to be more adaptable to high HTTP request intensity than the other schemes.",
Testing Java monitors through deterministic execution,"Java is a popular, modern programming language that supports monitors. However, monitor implementations, like other concurrent programs, are hard to test due to the inherent non-determinism. The paper presents a method for testing Java monitors, which extends the work of P. Brinch Hansen (1978) on testing Concurrent Pascal monitors. A monitor is tested by executing a concurrent program in which the processes are synchronised by a clock to make the sequence of interactions deterministic and reproducible. The method is extended to account for the differences between Concurrent Pascal monitors and Java monitors, and to provide additional coverage of the implementation under test. Tool support and documentation in the form of a test plan are also provided. The method is illustrated in detail on an asymmetric producer-consumer monitor, which is the same example that was used to illustrate the original method. The application of the method to the readers and writers problem is also discussed.","Testing,
Java,
Computer displays,
Clocks,
Australia,
Synchronization,
Documentation,
Computer science,
Modems,
Computer languages"
Enhancing distance learning with panoramic video,"This paper describes a new system for panoramic two-way video communication. Digitally combining images from an array of inexpensive video cameras results in a wide-field panoramic camera, from inexpensive off-the-shelf hardware. This system can aid distance learning in several ways, both by presenting a better view of the instructor and teaching materials to the students, and by enabling better audience feedback to the instructor. Because the camera is fixed with respect to the background, simple motion analysis can be used to track objects and people of interest. Electronically selecting a region of this results in a rapidly steerable ""virtual camera."" We present system details and a prototype distance learning scenario using multiple panoramic cameras.","Computer aided instruction,
Cameras,
Videoconference,
Electrical capacitance tomography,
Video sharing,
Lenses,
Laboratories,
Fiber reinforced plastics,
Humans,
Automatic control"
Quantum search algorithms in science and engineering,"The kinds of search algorithms that can run on quantum computers are qualitatively different from those that run on classical computers. As this article shows, quantum searches are thus vastly more efficient on certain kinds of problems.","Quantum computing,
Telephony,
Search problems,
Concurrent computing,
Polynomials,
Cryptography,
Parallel processing,
Software algorithms,
Algebra,
Computational modeling"
Sensory evaluation of expressive actions of InterRobot for human interaction and communication support,"InterRobot is the speech driven embodied interaction robot which is developed for supporting human interaction and communication by generating the expressive actions and motions coherently related to human speech input. Talkers can share the context of communication by mutual embodied entrainment via InterRobot. This paper evaluates the proposed InterRobot interaction model. Then, the effectiveness of InterRobot expressive actions in interaction is demonstrated by the sensory evaluation of paired comparison under three conditions of simple nodding, multiple nodding and the whole body motion. Actual applications of InterRobot to human interface are also demonstrated for developing a new embodied communication industry.","Speech analysis,
Human robot interaction,
Biological system modeling,
Speech synthesis,
Context,
Face,
Computer science,
Systems engineering and theory,
Arm,
Communication industry"
On-line recognition of UML diagrams,"Unified Modeling Language (UML) diagrams are widely used by software engineers to describe the structure of software systems. Early in the software design cycle, software engineers informally sketch initial UML diagrams on paper or whiteboards. The information provided by these UML diagrams needs to be made available to computer assisted software engineering (CASE) tools. In order to smooth this transition from paper to electronic form, we have developed an online recognition system for UML diagrams. The system accepts input from an electronic whiteboard, a data tablet or a mouse. Efforts have been made to separate the domain-independent and domain-specific parts of the recognition system. The kernel of the system is retargetable, providing a general front end for online recognition of any glyph-based diagram notation. The kernel is extended with UML-specific routines for segmentation, recognition of glyphs, and recognition of glyph relationships.","Unified modeling language,
Kernel,
Object oriented modeling,
Computer aided software engineering,
Mice,
Software tools,
Engineering drawings,
Shape,
Keyboards,
Information science"
MACS: music audio characteristic sequence indexing for similarity retrieval,"We present a prototype method of indexing raw-audio music files in a way that facilitates content-based similarity retrieval. The algorithm tries to capture the intuitive notion of similarity perceived by humans: two pieces are similar if they are fully or partially based on the same score, even if they are performed by different people or at different speed. Local peaks in signal power are identified in each audio file, and a spectral vector is extracted near each peak. Nearby peaks are selectively grouped together to form ""characteristic sequences"" which are used as the basis for indexing. A hashing scheme known as ""locality-sensitive hashing"" is employed to index the high-dimensional vectors. Retrieval results are ranked based on the number of final matches filtered by some linearity criteria.","Multiple signal classification,
Indexing,
Music information retrieval,
Content based retrieval,
Audio databases,
Humans,
Internet,
Computer science,
Prototypes,
Signal processing"
Supporting coordinated adaptation in networked systems,"Summary form only given. Our position is that the true potential of adaptation can only be realized if support is provided for more general solutions, including adaptations that span multiple hosts and multiple system components, and algorithmic adaptations that involve changing the underlying algorithms used by the system at runtime. Such a general solution must, however, address the difficult issues related to these types of adaptations. Adaptation by multiple related components, for example, must be coordinated so that these adaptations work together to implement consistent adaptation policies. Likewise, large-scale algorithmic adaptations need to be coordinated using graceful adaptation strategies in which as much normal processing as possible continues during the changeover. Here, we summarize our approach to addressing these problems in Cactus, a system for constructing highly-configurable distributed services and protocols.","Intelligent networks,
Large-scale systems,
Prototypes,
Transport protocols,
Bridges,
Computer science,
Programmable control,
Adaptive control,
Fuzzy logic,
Fuzzy sets"
SPAM-based recipes for continuum simulations,"Smooth Particle Applied Mechanics (SPAM) is a technique which provides a versatile approach to simulating many difficult problems in continuum mechanics, such as the breakup of a cavitating fluid and the penetration of one solid by another. SPAM also provides a simple evaluation method for all the continuum variables, as well as the spatial gradients required by the evolution equations. This global evaluation simplifies interpolation, reasoning, and Fourier transformation. Because SPAM is so flexible and easy to program, it should be included in the toolkit of anyone doing simulations.","Interpolation,
Unsolicited electronic mail,
Partial differential equations,
Tensile stress,
Differential equations,
Grid computing,
Computational modeling,
Energy measurement,
Extraterrestrial measurements,
Velocity measurement"
A dual-frequency compact microstrip patch antenna,"In this paper, a new dual-frequency, compact antenna, which uses an H-shaped microstrip patch with a shorting pin, is described. Compared with the conventional rectangular patch antenna, this antenna can achieve both a significant reduction of antenna size and a dual-frequency operation with a single feed. More freedoms for tuning the resonant frequencies, the frequency ratio, and the input impedance are available because of more design parameters. A detailed parameter study is performed, and the theoretical analysis is based on the finite difference time domain (FDTD) method. The FDTD programs are developed and validated by measurement results. The effects of several antenna parameters on two resonant frequencies, frequency ratio, and radiation pattern characteristics of the antenna are analyzed and compared. It is shown that various frequency ratios (1.914.23) can be obtained by varying the design parameters of this antenna. Several design curves are presented.",
Software-assisted cache replacement mechanisms for embedded systems,"We address the problem of improving cache predictability and performance in embedded systems through the use of software-assisted replacement mechanisms. These mechanisms require additional software controlled state information that affects the cache replacement decision. Software instructions allow a program to kill a particular cache element, i.e. effectively make the element the least recently used element, or keep that cache element, i.e. the element will never be evicted. We prove basic theorems that provide conditions under which kill and keep instructions can be inserted into program code, such that the resulting performance is guaranteed to be as good as or better than the original program run using the standard LRU policy. We developed a compiler algorithm based on the theoretical results that, given an arbitrary program, determines when to perform software-assisted replacement, i.e., when to insert either a kill or keep instruction. Empirical evidence is provided that shows that performance and predictability (worst-case performance) can be improved for many programs.",
Compiler-assisted heterogeneous checkpointing,We consider the problem of heterogeneous checkpointing in distributed systems. We propose a new solution to the problem that is truly heterogeneous in that it can support new architectures without any information about the architecture. The ability to support new architectures without additional knowledge or custom configuration is an important contribution of this work. This ability is particularly useful in mobile settings in which there is no a priori knowledge of the potential machines on which the application might execute. Our solution supports execution in unknown settings as long as there is compiler support for the high-level language in which the application is written. We precisely define what it means for a particular solution to be heterogeneous and discuss the heterogeneity of our solution and other solutions. We use code instrumentation at the source code level to provide heterogeneous checkpointing and recovery.,"Checkpointing,
Instruments,
High level languages,
Operating systems,
Computer science,
System testing,
Sun,
Linux,
Random access memory,
Yarn"
"Houghing the Hough: peak collection for detection of corners, junctions and line intersections","We exploit the Accumulator Array of the Hough Transform by finding collections of (2 or more) peaks through which a given sinusoid will pass. Such sinusoids identify points in the original image where lines intersect. Peak collection (or line aggregation) is performed by making a second pass through the edge map, but instead of laying points down in the accumulator array (as with the original Hough Transform), we compute the line integral over each sinusoid that corresponds to the current edge point. If a sinusoid passes through /spl ges/2 peaks, we deposit that sum/integral into a new accumulator array - an array that has a direct one-to-one correspondence with the original image. Thus, ""Houghing the Hough"" identifies points that correspond to corners, junctions or line intersections in image space. During initial peak collection, we include in the line integral only the most (locally) significant peaks while sifting out other (comparatively) weaker peaks from the current as well as subsequent peak collections. This ""contextual peak sifting"" greatly reduces computation, the effect of noise and the occurrence of false positives. Virtual line intersections (vanishing points, occluded corners, etc.) are detected as peaks without proximate edge support. Results in real-world images show the technique performs well in identifying corners, junctions and intersecting lines in a variety of scenes containing manmade objects such as buildings, documents, etc.","Image edge detection,
Image segmentation,
Voting,
Layout,
Shape,
Computer science,
Noise reduction,
Object detection,
Object recognition,
Termination of employment"
"Focus: a light-weight, incremental approach to software architecture recovery and evolution","During the past decade (1991-2001), object-orientation (OO) has become the dominant software development methodology, accompanied by a number of modeling notations, programming languages, and development environments. OO applications of today are increasingly complex and user driven. They are also developed more rapidly and evolved more frequently than was the case with software systems of the past. All of these factors contribute to a plethora of potential problems when maintaining and evolving an OO application. These problems are caused by architectural erosion, where the initial architecture of an application is (arbitrarily) modified to the point where its key properties no longer hold. We propose an approach, called Focus, whose goal is to enable effective evolution of such an application with minimal effort, by recovering its architecture and using it as the basis of evolution. Focus allows engineers to direct their primary attention to the part of the system that is directly impacted by the desired change; subsequent changes will incrementally uncover additional parts of the system's architecture. We have applied Focus to four off-the-shelf applications to date. We discuss its key strengths and point out several open issues that will frame our future work.","Software architecture,
Application software,
Computer science,
Programming,
Software systems,
Computer architecture,
Graphical user interfaces,
Computer languages,
Software quality,
Frequency"
A bound on attacks on payment protocols,"Electronic payment protocols are designed to work correctly in the presence of an adversary that can prompt honest principals to engage in an unbounded number of concurrent instances of the protocol. This paper establishes an upper bound on the number of protocol instances needed to attack a large class of protocols, which contains versions of some well-known electronic payment protocols, including SET and 1KP. Such bounds clarify the nature of attacks on and provide a rigorous basis for automated verification of payment protocols.","Protocols,
Upper bound,
History,
Computer science,
Space exploration,
Security,
State-space methods,
Electronic mail,
Yarn"
Collaborative exploration for the construction of visual maps,"We examine the problem of learning a visual map of the environment while maintaining an accurate pose estimate. Our approach is based on using two robots in a simple collaborative scheme. Without outside information, as a robot collects training images, its position estimate accumulates errors, thus corrupting its knowledge of the positions from which observations are taken. We address this problem by deploying a second robot to observe the first one as it explores, thereby establishing a virtual tether, and enabling an accurate estimate of the robot's position while it constructs the map. We refer to this process as cooperative localization. The images collected during this process are assembled into a representation that allows vision-based position estimation from a single image at a later date. In addition to developing a formalism and concept, we validate our results experimentally and present quantitative results demonstrating the performance of the method in over 90 trials.","Collaboration,
Robots,
Robotic assembly,
Robustness,
Computer science,
Machine learning,
Path planning,
Debugging,
Sonar measurements,
Magnetic sensors"
Optimal control of discrete event systems under partial observation,"We are interested in a class of optimal control problems for discrete event systems (DES). We adopt the formalism of supervisory control theory and model the system as a finite state machine (FSM). Our control problem is characterized by the presence of uncontrollable as well as unobservable events, the notion of occurrence and control costs for events and a worst-case objective function. We first derive an observer for the partially unobservable FSM, which allows us to construct an approximation of the unobservable trajectory costs. We define the performance measure on this observer rather than on the original FSM itself. Further, we use the algorithm of Sengupta and Lafortune (1998) to synthesize an optimal submachine of the observer. This submachine leads to the desired supervisor for the system.","Optimal control,
Discrete event systems,
Control system synthesis,
Cost function,
Automata,
Computer science,
Supervisory control"
Generalized communicators in the message passing interface,"We propose extensions to the message passing interface (MPI) that generalize the MPI communicator concept to allow multiple communication endpoints per process, dynamic creation of endpoints, and the transfer of endpoints between processes. The generalized communicator construct can be used to express a wide range of interesting communication structures, including collective communication operations involving multiple threads per process, communications between dynamically created threads or processes, and object-oriented applications in which communications are directed to specific objects. Furthermore, this enriched functionality can be provided in a manner that preserves backward compatibility with MPI. We describe the proposed extensions, illustrate their use with examples, and describe a prototype implementation in the popular MPI implementation MPICH.","Message passing,
Programming profession,
Computer science,
Computer Society,
Prototypes,
Multithreading,
Interference,
Communication channels"
A study of two dimensional linear discriminants for ASR,"We study the information in the joint time-frequency domain using 1515 dimensional-15 spectral energies and temporal span of 1s-block of spectrogram as features. In this feature space, we first derive 20 joint linear discriminants (JLDs) using linear discriminant analysis (LDA). Using principal component analysis (PCA), we conclude that information in this block of the spectrogram can be analyzed independently across the time and frequency domains. Under this assumption, we propose a sequential design of two dimensional discriminants (CLDs), i.e., spectral discriminants followed by temporal discriminants. We show that these CLDs are similar to first few JLDs and the discriminant features derived from the CLDs outperform those obtained from JLDs in the continuous-digit recognition task.","Automatic speech recognition,
Linear discriminant analysis,
Principal component analysis,
Time frequency analysis,
Spectrogram,
Independent component analysis,
Information analysis,
Frequency domain analysis,
Finite impulse response filter,
Computer science"
Analysis and synthesis of human faces with pose variations by a parametric piecewise linear subspace method,A framework for learning an accurate and general parametric facial model from 2D images is proposed and its application for analyzing and synthesizing facial images with pose variation is demonstrated. Our parametric piecewise linear subspace method covers a wide range of pose variation in a continuous manner through a weighted linear combination of local linear models distributed in a pose parameter space. The linear design helps to avoid typical nonlinear pitfalls such as overfitting and time-consuming learning. Experimental results show sub-degree and sub-pixel accuracy within /spl plusmn/55 degree full 3D rotation and good generalization capability over unknown head poses when learned and tested for specific persons.,"Humans,
Face,
Piecewise linear techniques,
Image analysis,
Head,
Information analysis,
Computer science,
Application software,
Ear,
Testing"
High quality behavioral verification using statistical stopping criteria,"In order to improve the efficiency of behavioral model verification, it is important to determine the points of diminishing return for a given verification strategy. This paper compares the existing stopping rules and presents a new stopping rule based on static Bayesian technique. The new stopping rule was applied to verifying 14 complex VHDL models. We used the figure of merit to compare the efficiency of the stopping rules. The results in terms of coverage and verification time were shown to consistently outperform existing stopping rules.","Software testing,
Computer science,
Bayesian methods,
Time to market,
Test pattern generators,
Time measurement,
Time factors,
Reliability engineering,
Software measurement,
Software systems"
Knowledge by user demand and self-reflection: new models for teaching and assessment in edutainment software design,"This paper reports on experiences from applying knowledge by user demand (KBUD) to teaching at the Edutainment Software Design programme at Halmstad University, Sweden. 'Edutainment' denotes educational and recreational systems for homes, schools and work. Upon graduation, students may find careers in game design, project management, systems development etc. KBUD emphasizes participation in selection and creation of course contents, to foster a continuous review process of how to attain competencies needed to accomplish a task. Continuous self-reflection, on how and why competencies develop, and how personal learning can be refined, is highlighted in discussions and through using projects from previous courses as case studies. KBUD classes offer a core of approximately 50% of course content. Students select another 25% from a range of teacher prepared themes and develop themselves the remaining themes under teacher supervision. KBUD modules combine subject adherence with encouraging student interest and participation, whilst also ensuring content vitality.","Education,
Educational institutions,
Software design,
Application software,
Home computing,
Hardware,
Engineering profession,
Project management,
Information systems,
Software tools"
Robust congestion signaling,"We present an improved explicit congestion notification (ECN) mechanism that enables a router to signal congestion to the sender without trusting the receiver or other network devices along the signaling path. Without our mechanism, ECN-based transports can be manipulated to undermine congestion control. Web clients seeking faster downloads, for example, can trivially conceal congestion signals from Web servers. A misbehaving connection would exceed its fair bandwidth share at the expense of competing traffic by as much as an order of magnitude in our simulations. Our improved mechanism is robust because it does not depend on correct implementation at locations other than the sender and marking router, and it is practical because it admits an efficient implementation that is backwards-compatible with prior ECN and TCP/IP mechanisms.","Robustness,
Bandwidth,
Channel allocation,
Computer science,
TCPIP,
Web server,
Internet,
Performance loss,
Transport protocols,
Computer network reliability"
"Computer supported learning. A large-scale, web-based learning and assessment system to support flexible education","With the increase in cost and resources constraints experienced by many tertiary institutions globally, the pressure to find alternative methods to deliver teaching and assessment increases. Whilst mindful of the learning experience of the individual, the need to manage the numbers, resources and assessment provide an administrative overhead and headache to every professor who has ever taught. With this predicament in mind the computer supported learning system (CSL) is a web-based teaching and learning resource and administration system, developed by the Business Education On-Line Unit of the Auckland Business School, at the University of Auckland (New Zealand). We call it Cecil. The paper describes the Cecil structure and discusses the potential benefits that a university wide resource management system may have in terms of the educational flexibility and resource sharing.","Large-scale systems,
Computer science education,
Resource management,
Databases,
Data models,
Learning systems,
Computer architecture,
Information management,
Management information systems,
Costs"
Combined unsigned and two's complement hybrid squarers,"Designs for high-speed combined squarers, capable of operating on either unsigned or two's complement numbers, are presented. High speed is achieved in part by using a modestly sized ROM table to generate the less significant bits of the square, and combinational logic to generate the more significant bits. These squarers have a shorter carry propagate chain in the final adder and a smaller amount of combinational logic than previous hybrid designs. Area and delay estimates indicate that the combined hybrid squarers presented in this paper have between 28% and 64% percent less area and between 9% and 15% percent less delay than previous unsigned hybrid squarers for 32-bit operands.","Read only memory,
Symmetric matrices,
Logic design,
Delay estimation,
Computer architecture,
Digital arithmetic,
Laboratories,
Computer science,
Design engineering,
Digital signal processing"
A framework for managing QoS and improving performance of dynamic Web content,"The proportion of dynamic objects has been growing at a fast rate in the World Wide Web. However, because of additional resource requirements and the changing nature of these objects, the performance of accessing dynamic Web content has been observed to be poor in the current generation Web services. We propose a framework called WebGraph that helps in improving the response time for accessing dynamic objects. The WebGraph framework manages a graph for each of the Web pages. Both the nodes and the edges have attributes that are used in managing the Web pages. Instead of recomputing and recreating the entire page, the node and edge attributes are used to update a subset of the Weblets are then integrated to form the entire page. In addition to the performance benefits in terms of lower response time, the WebGraph framework facilitates Web caching, QoS support, load balancing, overload control, personalized services, and security for both dynamic as well as static Web pages.","Content management,
Quality of service,
Web pages,
Web server,
Web services,
Web sites,
Load management,
Delay estimation,
Engineering management,
Computer science"
The Astrophysics Simulation Collaboratory: a science portal enabling community software development,"Grid Portals, based on standard Web technologies, are emerging as important and useful user interfaces to computational and data grids. Grid portals enable virtual organizations, comprised of distributed researchers to collaborate and access resources more efficiently and seamlessly. The Astrophysics Simulation Collaboratory (ASC) Grid Portal provides a framework to enable researchers in the field of numerical relativity to study astrophysical phenomenon by making use of the Cactus computational toolkit. We examine user requirements and describe the design and implementation of the ASC Grid Portal.","Astrophysics,
Collaborative software,
Portals,
Programming,
Grid computing,
Computational modeling,
Computer interfaces,
Collaboration,
Problem-solving,
Distributed computing"
Accelerating Boolean satisfiability through application specific processing,"This paper presents an application specific multiprocessor system for SAT, utilizing the most recent results such as the development of highly efficient sequential SAT algorithms, the emergence of commercial configurable processor cores and the rapid progress in IC manufacturing techniques. Based on an analysis of the basic SAT search algorithm, we propose a new parallel SAT algorithm that utilizes fine grain parallelism. This is then used to design a multiprocessor architecture in which each processing node consists of a processor and a communication assist node that deals with message processing. Each processor is an application specific processor built from a commercial configurable processor core. All the system configurations are determined based on the characteristics of SAT algorithms, and are supported by simulation results. While this hardware accelerator system does not change the inherent intractability of the SAT problems, it achieves a 30-60x speedup over and above the fastest known SAT solver - Chaff. We believe that this system can be used to expand the practical applicability of SAT in all its application areas.",
Analysis of attacks on SDMI audio watermarks,"This paper explains and analyzes the successful attacks submitted by the authors on four audio watermark proposals during a 3-week SDMI public challenge. Our analysis points out some weaknesses in the watermark techniques currently under SDMI consideration and suggests directions for further improvement. The paper also discusses the framework and strategies for analyzing the robustness and security of watermarking systems as well as the difficulty, uniqueness, and unrealistic expectations of the attack setup.",
Nonconventional computing paradigms in the new millennium: a roundtable,"Computational models based on natural phenomena have gained popularity. Other techniques are more esoteric, such as DNA-based and quantum computation. Many perceive them to hold the promise for building more powerful massively parallel computers that can provide considerably more computing power than we currently have.",Parallel processing
SIMLAB-a simulation environment for storage area networks,"In this paper we present a simulation environment for storage area networks called SIMLAB. SIMLAB is a part of the PRESTO project, which is a joint project of the Electrical Engineering Department and the Computer Science Department of the Paderborn University. The aim of the PRESTO project is to construct a scalable and resource-efficient storage network that can support the real-time delivery of data. SIMLAB has been implemented to aid the development and verification of distributed algorithms for this storage network. However, it has been designed in such a way that it can also be used for the simulation of many other types of networking problems. SIMLAB is based on C++ and common libraries and input/output formats, which ensures that SIMLAB can be used on many different platforms. We therefore expect SIMLAB to be useful also for other people working on similar problems.","Storage area networks,
Computer science,
Network servers,
Computational modeling,
Distributed algorithms,
Scalability,
Availability,
Computer simulation,
Libraries,
Enterprise resource planning"
Image properties of list-mode likelihood reconstruction for a rectangular positron emission mammograph with DOI measurements,"A positron emission mammography scanner is under development at the authors' laboratory. The tomograph has a rectangular geometry consisting of four banks of detector modules. For each detector, the system can measure the depth of interaction information inside the crystal. The rectangular geometry leads to irregular radial and angular sampling and spatially variant sensitivity that are different from conventional positron emission tomography (PET) systems. The authors adapted the theoretical analysis that they had developed for conventional PET systems to the list-mode likelihood reconstruction for this tomograph. The local impulse response and covariance of the reconstruction can be easily computed using the fast Fourier transform. These theoretical results are also used with computer observer models to compute the signal-to-noise ratio for lesion detection. The analysis reveals the spatially variant resolution and noise properties of the list-mode likelihood reconstruction. The theoretical predictions are in good agreement with Monte Carlo results.","Image reconstruction,
Positron emission tomography,
Geometry,
Detectors,
Radioactive decay,
Mammography,
Sampling methods,
Fast Fourier transforms,
Signal to noise ratio,
Lesions"
Including practical software evolution in software engineering education,"Software engineering typically requires more effort in maintenance than in development time. As such, software engineering education needs to actively include software evolution. Teaching software evolution to undergraduate students usually includes the theoretical aspects, but does not normally include the actual implementation. This paper describes the practice of teaching software evolution to undergraduate computing students at Monash University. It demonstrates how the four maintenance activities: corrective, adaptive, perfective and preventative, can be included into the practical component of a software engineering course, providing students with a much more realistic view of software engineering.","Software engineering,
Software maintenance,
Error correction,
Computer science education,
Software systems,
Computer science,
Software tools,
Engineering education,
Educational programs,
Hardware"
The telecomputing laboratory: a multipurpose laboratory,"The authors' telecomputing laboratory is a multiuse state-of-the-art undergraduate teaching facility designed to be both flexible and powerful. The controlled workstation and computer communications facilities allow undergraduate students an opportunity to apply and integrate classroom knowledge in communications, signal processing, computer architecture and algorithm and software development via a set of practical experiments. Students are exposed to state-of-the-art computer communications systems, on which they can implement everything from modern digital communication and signal processing algorithms to parallel algorithms and real-time databases. State-of-the-art computer software provides the undergraduate students modern models of quality software, as well as the tools necessary to design and build sophisticated software/hardware systems. This laboratory significantly impacts many undergraduate students who will take positions with organizations in the telecommunications/computer industry, where the skills gained from laboratory experiences will be greatly valued.",Computer science education
Single-port electronically steerable passive array radiator antenna based space-time adaptive filtering,"In wireless communications, there are two problems that the systems suffer from, one is the inter-symbol interference (ISI), the other is the cochannel interference (CCI). Space-time adaptive processing (STAP) is considered to provide superior performance in suppressing both the ISI and the CCI. To reduce the complexity and the cost, using switched parasitic elements or the parasitic elements with variable reactances to change the mutual coupling and so as to provide spatial diversity attracts much attention. The electronically steerable passive array radiator (ESPAR) antenna is one of the parasitic elements based single port antennas with several variable reactances. We propose a single port ESPAR antenna-based space-time adaptive filtering. Unlike the nulls formed by the conventional adaptive antenna, here we develop the potential of jointly using the variable pattern and the temporal equalizer to realize space-time adaptive filtering.","Antenna arrays,
Intersymbol interference,
Radiofrequency interference,
Adaptive arrays,
Adaptive filters,
Wireless communication,
Interchannel interference,
Costs,
Communication switching,
Mutual coupling"
Efficient dense depth estimation from dense multiperspective panoramas,"In this paper we study how to compute a dense depth map with panoramic field of view (e.g., 360 degrees) from multi-perspective panoramas. A dense sequence of multiperspective panoramas is used for better accuracy and reduced ambiguity by taking advantage of significant data redundancy. To speed up the reconstruction, we derive an approximate epipolar plane image that is associated with the planar sweeping camera setup, and use one-dimensional window for efficient matching. To address the aperture problem introduced by one-dimensional window matching, we keep a set of possible depth candidates from matching scores. These candidates are then passed to a novel two-pass tensor voting scheme to select the optimal depth. By propagating the continuity and uniqueness constraints non-iteratively in the voting process, our method produces high-quality reconstruction results even when significant occlusion is present. Experiments on challenging synthetic and real scenes demonstrate the effectiveness and efficacy of our method.","Image reconstruction,
Iterative algorithms,
Voting,
Layout,
Stereo image processing,
Councils,
Geometry,
Computer science,
Tensile stress,
Navigation"
E-government: aspects of security on different layers,"The global IT revolution is growing rapidly. Governments and businesses have to be ready to meet the increased demand of effective and secure online services. Providing secure online services in e-government requires, however, careful deliberation on different levels and for the distinct domains of e-government. The first issues on security coming to mind regard of course technical aspects. Here, many concepts and tools have been developed to provide secure transactions, to protect against non-allowed access to information and data, to protect against hacker attacks etc. Yet, security aspects concern not only technical issues. Instead, these need careful investigation from a non-technical viewpoint as well. We suggest a holistic concept that integrates security aspects from the strategic level down to the data and information level in order to address different security aspects in a comprehensive way.",
Visualization-specific compression of large volume data,"When interactive real-time applications are developed with very large volume data, the use of lossy compression is often inevitable. Lossy compression schemes generally encode data without consideration of the purpose of visualization that is actually performed, which often results in inefficient compression. In this paper, we present a new method for classifying voxels according to their importance in visualization, and assigning appropriate weights to them. The associated weight information can be combined with lossy compression schemes to reduce the visual degradation of reconstructed images, resulting in higher compression rates and visual fidelity. Test results demonstrate that the proposed technique improves both the amount of compression and the quality of visualization significantly.","Data visualization,
Image coding,
Application software,
Data mining,
Vector quantization,
Isosurfaces,
Computer science,
Degradation,
Testing,
Filters"
Design and analysis of a spatial 3-DOF micromanipulator for tele-operation,"In this paper, we propose and develop a spatial 3-DOF tele-micromanipulator for precise position control of micro-objects. Typical feature of this device is a one-module flexure hinge that consists of a revolute joint and a spherical joint. Based on preliminary kinematic analysis and stiffness modeling of the system, optimal design and actuator sizing for the device are performed. Furthermore, FEM analysis and resonant frequency analysis are executed to validate the results of analytic design process. The designed device was successfully implemented to tele-micromanipulation system.",
Integrating linguistic primitives in learning context-dependent representation,"The paper presents an explicit connectionist-inspired, language learning model in which the process of settling on a particular interpretation for a sentence emerges from the interaction of a set of ""soft"" lexical, semantic, and syntactic primitives. We address how these distinct linguistic primitives can be encoded from different modular knowledge sources but strongly involved in an interactive processing in such a way as to make implicit linguistic information explicit. The learning of a quasi-logical form called context-dependent representation, is inherently incremental and dynamical in such a way that every semantic interpretation will be related to what has already been presented in the context created by prior utterances. With the aid of the context-dependent representation, the capability of the language learning model in text understanding is strengthened. This approach also shows how the recursive and compositional role of a sentence as conveyed in the syntactic structure can be modeled in a neurobiologically motivated linguistics based on dynamical systems rather on combinatorial symbolic architecture. Experiments with more than 2000 sentences in different languages illustrating the influences of the context-dependent representation on semantic interpretation, among other issues, are included.","Cognitive science,
Context modeling,
Natural languages,
Computer architecture,
Computational linguistics,
Casting,
Humans,
Strips,
Context awareness,
Couplings"
Implementing associative processing: rethinking earlier architectural decisions,,"Associative processing,
Associative memory,
Costs,
Broadcasting,
Switches,
Mathematics,
Computer science,
Logic devices,
Reduced instruction set computing,
Assembly"
Linear-time matrix transpose algorithms using vector register file with diagonal registers,"Matrix transpose operation (MT) is used frequently in many multimedia and high performance applications. Therefore, using a faster MT operation results in a shorter execution time of these applications. In this paper we propose two new MT algorithms. The algorithms exploit diagonal register properties to achieve a linear-time execution of MT operation using vector processor that supports diagonal registers. We demonstrate the algorithms as well as proofs, examples, and various enhancements to the proposed algorithms A performance evaluation shows that the proposed algorithms are at least twice as fast as one of the leading MT algorithms such as an algorithm that is implemented using Motorola's AltiVec architecture (n/spl ges/16). We believe that our work opens new doors to improve the execution time of many two-dimensional operations such as DCT, DFT, and Shearsort.","Registers,
Vector processors,
Acceleration,
Application software,
Computer architecture,
Computer science,
Discrete cosine transforms,
Multimedia computing,
Scientific computing,
Sections"
Accumulation and summarization of human daily action data in one-room-type sensing system,"Since human behavior patterns vary from person to person, computers have to understand someone's personal behavior in order to supply him with a robotic support system for his daily life. It is necessary to know his behavior over the long term. The ideal intelligent support system can measure the data about human actions in the daily life, accumulate them, understand the behavior and the patterns thereof, and adapt thereto. An accumulation and summarization system of human daily action data based on sensing in 1 room, ""Robotic Room II"", is designed and the accumulation system integrating with the measurement parts is implemented. It is confirmed that accumulation over the long term is possible by experiments for accumulation of human action data in ordinary life. Moreover, as the first step of an analysis of the accumulated data, a summarization algorithm based on the changes of the room states is proposed. It is confirmed that the summarized data enables us to catch the most of the changes of the room states. Finally, a possibility of application for life monitoring by means of summarization is also illustrated.","Humans,
Diseases,
Intelligent sensors,
Data engineering,
Algorithm design and analysis,
Application software,
Computerized monitoring,
Intelligent systems,
Intelligent robots,
Pediatrics"
Visualization of state transition graphs,,"Visualization,
Automata,
Application software,
Computer science,
Mathematics,
Tree data structures,
Read only memory,
Tree graphs,
Biology,
Biological system modeling"
Graph sketches,,"Navigation,
Data visualization,
Large screen displays,
Laboratories,
Computer science,
Computer displays,
Filters,
Geographic Information Systems,
Telecommunication traffic,
Internet"
Empirical validation of class diagram complexity metrics,"One of the principal objectives of software engineering is to improve the quality of software products. It is widely recognised that the quality assurance of software products must be guaranteed from the early phases of development. As a key artifact produced in the early development of object-oriented (OO) information systems (OOISs), class diagram quality has a great impact on the quality of the software product which is finally delivered. Hence, class diagram quality is a crucial issue that must be evaluated (and improved if necessary) in order to obtain quality OOISs, which is the main concern of present-day software development organisations. After a thorough review of the existing OO measures that are applicable to class diagrams at a high-level design stage, M. Genero et al. (2000) presented in a set of metrics for the structural complexity of class diagrams built using the Unified Modelling Language (UML). We focus on class diagram structural complexity, an internal quality attribute which we believe could be closely correlated with one of the most critical external quality attributes, such as class diagram maintainability. Since the main goal of this paper is the empirical validation of those metrics, we present two controlled experiments carried out to corroborate if those metrics are closed to class diagram maintainability and thus could be used as early maintainability indicators. Based on data collected in the experiments, we build a prediction model for class diagram maintainability using a method for the induction of fuzzy rules.","Software quality,
Unified modeling language,
Object oriented modeling,
Predictive models,
Software measurement,
Computer science,
Software engineering,
Quality assurance,
Information systems,
Programming"
"Passive microwave radiometry, fractals, and ocean dynamics","An idea to explore fractal structures in the field of ocean microwave radiance is suggested. To solve this problem we must be able to relate natural ocean surface features to their thermal microwave emission signatures. This task has two problems: (1) the study of hydrodynamic fractal structures in the ocean, and (2) the creation of an adequate microwave imaging model. One possibility is to consider a fractal model of the ocean surface with scaling (power law) wave number spectra. Microwave emission effects from a rough water surface are calculated quite using a small-perturbation approach well known in the ocean passive microwave radiometry. Combined computer simulations allow us to generate microwave fractal images and local patterns. Some related experimental data have been found previously during polarimetric scanning radiometer measurements.","Microwave radiometry,
Fractals,
Sea surface,
Ocean temperature,
Rough surfaces,
Surface roughness,
Photothermal effects,
Hydrodynamics,
Microwave imaging,
Surface waves"
Generating linguistic spatial descriptions from sonar readings using the histogram of forces,"We show how linguistic expressions can be generated to describe the spatial relations between a mobile robot and its environment, using readings from a ring of sonar sensors. Our work is motivated by the study of human-robot communication for non-expert users. The eventual goal is to use these linguistic expressions for navigation of the mobile robot in an unknown environment, where the expressions represent the qualitative state of the robot with respect to its environment, in terms that are easily understood by human users. In the paper, we describe the histogram of forces and its application to sonar sensors on a mobile robot. Several environment examples are also included with the generated linguistic descriptions.","Histograms,
Mobile robots,
Sonar navigation,
Robot sensing systems,
Sensor phenomena and characterization,
Human robot interaction,
Mobile communication,
Intelligent sensors,
Computer science,
Sonar applications"
Backchannel: whispering in digital conversation,"Backchannel communication in digital conversations permits private communication which is visible only to the sender and receiver. Backchannel is multithreaded, substantial, and governed by many social conventions; it persists only if captured in users' private logs. To better understand backchannel's function - and to predict ways in which it may be affected by application design and by attempts to capture it on a server-wide scale for research and analysis - we analyzed private transcripts of meetings and class sessions held in MUDs. We identified five backchannel categories: process-oriented, content-oriented, participation-enabling, tangential and independent backchannel. Software designers can use these results to understand how backchannel should function in digital conversation applications. Making backchannel overtly available for study would require making its presence and content visible and its content persist, affecting the nature of the backchannel and raising social and ethical issues.","Clouds,
Application software,
Software design,
Multiuser detection,
Taxonomy,
Dictionaries,
Cybernetics,
Government,
Testing,
Mirrors"
Quick and easy interactive molecular dynamics using Java3D,"A new branch of interactive molecular dynamics has emerged. Unlike typical molecular dynamics simulations, it lets the user see the studied system and its behavior while interacting with it. This requires a graphical output device such as a Unix workstation. Desktop computers have reached a level of graphical power that allows rendering virtual universes in real time. This power enables the use of common desktop PCs for interactive molecular dynamics. High-end visualizations are usually based on native 3D graphics libraries. Using these libraries to create an interactive simulation application requires substantial effort for upgrading existing simulation code; many developers shrink from this. I present Java Interactive Molecular Dynamics, interactive molecular dynamics software for common desktop computers and workstations. JIMD is not a stand-alone interactive molecular dynamics application; it is a software package that can be quickly and easily adapted to existing molecular dynamics simulation software. For rendering the molecular system, JIMD uses Java3D, the application programming interface (API) for writing 3D graphics applications in Java.","Java,
Computational modeling,
Application software,
Workstations,
Rendering (computer graphics),
Computer graphics,
Software libraries,
Personal communication networks,
Visualization,
Software packages"
Power-aware partitioned cache architectures,"Focuses on partitioning the cache resources architecturally for energy and energy-delay optimizations. Specifically, we investigate ways of splitting the cache into several smaller units, each of which is a cache by itself (called subcache). Subcache architectures not only reduce the per-access energy costs but can potentially improve the locality behavior as well. We present a unified framework for designing, implementing and evaluating different subcache architectures. Different techniques for data placement, subcache prediction, and selective probing are proposed and evaluated using a diverse set of applications. The results show that intelligent subcache mechanisms proposed in this paper are effective.","Capacitance,
Computer architecture,
Hardware,
Energy consumption,
Wiring,
Circuits,
Permission,
Space exploration,
Computer science,
Power engineering and energy"
Radicals of presentation in persistent conversation,"Going forward from N. Frye's (1969) ""Anatomy of Criticism"", we derive from genre literature the idea that radicals, i.e. root characteristics, of persistent conversation exist and can help define important aspects of such conversations. We identify from longitudinal interviews with members of a distributed, computer-supported learning environment, three dimensions of interactivity that revolve around speaker-audience relations. We propose three ""radicals of presentation"" in persistent conversation: (1) visibility (the means, methods and opportunities for presentation, addressing primarily speakers' concerns with the presentation of self); (2) relation (the tie between speaker and audience, and among audience co-participants, addressing the speaker's concerns with the range and identity of the audience, and audience members' concerns about relations with each other); and (3) co-presence (the temporal, virtual and/or physical co-presence of speaking and listening participants, addressing concerns about being with others at the same time and place, and giving and receiving immediate feedback). We conclude with implications for social and technical design.","Libraries,
Distributed computing,
Information science,
Feedback,
Educational institutions,
Visualization,
Speech,
Writing,
Computer mediated communication,
Educational programs"
Adaptive minimum-BER linear multiuser detection,"An adaptive minimum bit error rate (MBER) linear multiuser detector (MUD) is proposed for DS-CDMA systems. Based on the approach of kernel density estimation for approximating the bit error rate (BER) from training data, a least mean squares (LMS) style adaptive algorithm is developed for training linear MUDs. Computer simulation results show that this adaptive MBER linear MUD outperforms two existing LMS-style adaptive MBER algorithms.","Multiuser detection,
Bit error rate,
Detectors,
Least squares approximation,
Multiaccess communication,
Niobium,
Error probability,
Equalizers,
Computer science,
Kernel"
Computational grids,"The authors discuss computational and information grids. Peer-to-peer networks are built on the analogy of providing services in a community of peers. Grids, on the other hand, are built on the analogy of users tapping into ubiquitous computing and information resources, similar to plugging into an electrical grid. Together, these two powerful analogies cover much of the research and indeed commercial deployment of Web based distributed systems. Underlying both concepts are two inevitable trends: using Internet hardware and software infrastructure to build the communication and control (middleware) of distributed systems, and using a Web browser as the user interface or portal to an application.","Grid computing,
Pervasive computing,
Peer to peer computing,
Ubiquitous computing,
Information resources,
Internet,
Hardware,
Communication system software,
Distributed control,
Communication system control"
Designing networks incrementally,"We consider the problem of incrementally designing a network to route demand to a single sink on an underlying metric space. We are given cables whose costs per unit length scale in a concave fashion with capacity. Under certain natural restrictions on the costs (called the Access Network Design constraints), we present a simple and efficient randomized algorithm that is competitive to the minimum cost solution when the demand points arrive online. In particular, if the order of arrival is a random permutation, we can prove a O(1) competitive ratio. For the fully adversarial case, the algorithm is O(K) -competitive, where K is the number of different pipe types. Since the value of K is typically small, this improves the previous O(log n log log n)-competitive algorithm which was based on probabilistically approximating the underlying metric by a tree metric. Our algorithm also improves the best known approximation ratio and running time for the offline version of this problem.","Approximation algorithms,
Cables,
Algorithm design and analysis,
Computer science,
Extraterrestrial measurements,
Economies of scale,
Cost function,
Application software,
Telephony,
Nearest neighbor searches"
A case study in testing distributed systems,"This paper describes a case study in the testing of distributed systems. The software under test is a middleware system developed in Java. The full test life cycle is examined including unit testing, integration testing, and system testing. Where possible, traditional tools and techniques are used to carry out the testing. One aspect where this is not possible is the testing of the low-level concurrency, which is often overlooked when testing commercial distributed systems, since the middleware or application server is already developed by a third-party and is assumed to operate correctly. This paper examines testing the middleware system itself, and therefore, a method for testing the concurrency properties of the system is used. The testing revealed a number of faults and design weaknesses, and showed that, with some adaptation, traditional tools and techniques go a long way in the testing of distributed applications.","Computer aided software engineering,
System testing,
Middleware,
Life testing,
Concurrent computing,
Delay,
Software testing,
Physics computing,
Computer science,
Australia"
Semi-direct product in groups and zig-zag product in graphs: connections and applications,"We consider the standard semi-direct product A/spl times/B of finite groups A, B. We show that with certain choices of generators for these three groups, the Cayley graph of A/spl times/B is (essentially) the zigzag product of the Cayley graphs of A and B. Thus, using the results of O. Reingold et al. (2000), the new Cayley graph is an expander if and only if its two components are. We develop some general ways of using this construction to obtain large constant-degree expanding Cayley graphs from small ones. A. Lubotzky and B. Weiss (1993) asked whether expansion is a group property; namely, is being an expander for (a Cayley graph of) a group G depend solely on G and not on the choice of generators. We use the above construction to answer the question in the negative, by showing an infinite family of groups A/sub i//spl times/B/sub i/ which are expanders with one choice of a (constant-size) set of generators and are not with another such choice. It is interesting to note that this problem is still open, though for ""natural"" families of groups like the symmetric groups S/sub n/ or the simple groups PSL(2, p).","Computer science,
Graph theory,
Mathematics,
Application software,
Geometry,
Bridges,
Algebra,
Power generation,
Labeling,
Error correction"
"BUC, a simple yet efficient concurrency control technique for mobile data broadcast environment","Due to the limited bandwidth of the wireless channel, data should be organized and presented to the user based on need. Moreover when a number of users have similar data interests, data broadcast can be used to manage sending the needed data to the listening users. This eases the impact of limited bandwidth and improves the performance of the mobile system. We propose a concurrency control algorithm that solves the inconsistency problem that may be observed by the client during the broadcast. We use broadcasted and updated cycles (BUC) for the purpose of conflict checking. Our technique is simple, yet efficient in both space and overhead.","Concurrency control,
Bandwidth,
Satellite broadcasting,
Network servers,
Mobile computing,
Computer science,
Concurrent computing,
Radio broadcasting,
Land mobile radio cellular systems,
Wireless LAN"
A Web-based laboratory on control of a two-degree-of-freedom helicopter,"We discuss the design and implementation of a series of linear or nonlinear controllers for a two-degree-of-freedom (2DOF) helicopter through a Web-based laboratory. The physical structure of the 2DOF helicopter makes it an ideal platform for implementing and evaluating control strategies such as proportional-integral-derivative control (PID), fuzzy control and general state space feedback control. Thus, it is appropriate for all levels of university education and research. The Web-based laboratory on helicopters is currently being utilized in teaching both undergraduate and postgraduate courses in the Department of Electrical and Computer Engineering, National University of Singapore. The system is particularly beneficial to part-time students, who are unable to access the University laboratory facilities during normal operating hours. The Web-based laboratory on helicopters, together some other laboratories implemented earlier, can be accessed at the Web site http://vlab.ee.nus.edu.sg/vlab.","Laboratories,
Helicopters,
Pi control,
Proportional control,
Three-term control,
Fuzzy control,
State-space methods,
Feedback control,
Computer science education,
Control engineering education"
Text extraction from color documents-clustering approaches in three and four dimensions,"Colored paper documents often contain important text information. For automating the retrieval process, identification of text elements is essential. In order to reduce the number of colors in a scanned document, color clustering is usually done first. In this article two histogram-based color clustering algorithms are investigated. The first is based on the RGB color space exclusively, while the second takes spatial information into account, in addition to the colors. Experimental results have shown that the use of spatial information in the clustering algorithm has a positive impact. Thus the automatic retrieval of text information can be improved. The proposed methods for clustering are not restricted to document images. They can also be used for processing Web or video images, for example.","Clustering algorithms,
Computer science,
Information retrieval,
Books,
Mathematics,
Machine assisted indexing,
Data mining,
Color,
Histograms,
Marine vehicles"
Multi-resolution resource behavior queries using wavelets,"Different adaptive applications are interested in the dynamic behavior of a resource over different fine- to coarse-grain time-scales. The resource's sensor runs at some fine-grain resource-appropriate sampling rate, producing a discrete-time resource signal. It can be very inefficient to to answer a coarse-grain application query by directly using the fine-grain resource signal. We address this gap between the sensor and its different client applications with a novel query model that explicitly incorporates time-scale as a parameter. The query model is implemented on top of an inherently multi-scale wavelet-based representation of the signal (which could be communicated over a set of multicast channels). A query uses only the wavelet coefficients necessary for its time-scale (and thus could listen to a subset of the channels), greatly reducing the data that need to be communicated. We present very promising initial results on host load signals, showing the tradeoff between compactness and query error. Finally, we describe some of the other operations that the wavelet representation enables.","Application software,
Sampling methods,
Computer science,
Distributed computing,
Processor scheduling,
Lenses,
Weather forecasting,
Time measurement,
Frequency diversity,
Bandwidth"
Dynamic policy model for large evolving enterprises,"The nature of an open distributed environment provides a resoundingly diverse yet potentially chaotic environment for users. A great deal of research has focused on the management of resources in such an environment and policy-based management has emerged as one such promising solution. In order to support large evolving enterprises, we are currently developing a policy model that will be scalable and able to cope with the extraordinarily high rate of change inherent in such environments. To do this, we have developed a number of novel concepts including: enterprise domain, policy space and policy authority which are unique and central to our approach. The overall objective of producing this model is to support dynamic roles, dynamic policies and the subsequent dynamic, conflict analysis. Prevailing policy-based management models largely promote the static definition and analysis of policy. We argue that although these models are suitable for homogenous, largely static environments, they are too rigid to adequately represent large, heterogeneous, evolving enterprise as the fluidity and complexity of interactions occurring in such environments are genuinely difficult and often impossible to pre-empt at the time of policy specification and role assignment.","Environmental management,
Resource management,
Runtime,
Computer science,
Cyclic redundancy check,
Australia,
Chaos,
Permission,
Qualifications,
Knowledge management"
Computer simulations of diffusion-limited reactions,"In many situations, it is possible to model a system by entities that diffuse and occasionally react upon encounter. Examples include chemical reactions, electron-hole recombination, annealing of defects in crystals, interacting excitons (solitons, photoexcited states, and so on), predator-prey models, and the spreading of ideas through social intercourse. For concreteness and without loss of generality, we refer to the interacting entities as particles. We discuss various ways to simulate diffusion-reaction systems. For simplicity, we restrict our discussion to simple reaction schemes, such as coalescence and annihilation, and to the case where the particles occupy a 1D lattice. It turns out that diffusion-limited kinetics deviates the most from the classical reaction-limited regime in d=1, so the focus on one dimension is reasonable. The generalization of the techniques discussed to more realistic models (off-lattice, higher dimensions, and so on), is straightforward.","Computer simulation,
Chemicals,
Spontaneous emission,
Annealing,
Crystals,
Excitons,
Solitons,
Predator prey systems,
Lattices,
Kinetic theory"
Watcher: the missing piece of the security puzzle,"Modern intrusion detection systems are comprised of three basically different approaches, host based, network based, and a third relatively recent addition called procedural based detection. The first two have been extremely popular in the commercial market for a number of years now because they are relatively simple to use, understand and maintain. However, they fall prey to a number of shortcomings such as scaling with increased traffic requirements, use of complex and false positive prone signature databases, and their inability to detect novel intrusive attempts. The procedural based intrusion detection, systems represent a great leap forward over current security technologies by addressing these and other concerns. This paper presents an overview of our work in creating a true procedural Disallowed Operational Anomaly (DOA) system.","Computer security,
Intrusion detection,
Radio access networks,
Operating systems,
Hardware,
Information security,
Internet,
Monitoring,
Computer science,
Software systems"
The 21st Century Engineer,,"Cognition,
Proteins,
Nanobioscience,
Joining processes,
Chaos,
Dictionaries,
Computer architecture,
Internet,
Computational biology,
Supercomputers"
Guaranteeing recoverability in electronic commerce,"Electronic commerce systems (retail, auction, etc.) are good examples of data-based systems that operate under correctness and resilience requirements of a transactional nature but go beyond conventional databases, as they are formed by the aggregation of heterogeneous, autonomous components. We introduce a framework to specify, analyze and reason about the behavior of such systems, focusing on how they are designed to make consistent progress in spite of failures. The contributions are: (a) the introduction of the Guarantee abstraction to deal with transactional applications; (b) a framework based on guarantees and protocols to specify the behaviors of systems and their components and reason about the properties of systems and their components; and (c) application of the framework to a common e-commerce scenario. The framework allows the hierarchical composition of transactional systems and their properties, as well as the proofs of these properties: we specify a system's behavior at its most abstract level, and proceed to decompose the specification mirroring the structure of the system's components, considering the role of guarantee-preserving component systems and recovery in each case. In particular we show how the lower-level properties are supported by the component systems, which we also characterize within the same framework.","Electronic commerce,
Resilience,
Transaction databases,
Protocols,
Computer science,
Failure analysis,
History"
A network management framework for multi-layered network survivability: an overview,"We consider an interconnected network environment where one network can act as a 'provider' or 'service' network to the 'user' network or a collection of 'user' networks. In such an environment, a major failure in a provider network can affect the user networks. Very often, failure management is addressed in each individual network domain independently. We present an overview of a loosely-coupled network management framework through the development of a multi-layered network manager of managers concept for correlated management, for failure cases that can not be addressed by each component network individually.","Resource management,
Technology management,
IP networks,
Routing,
Computer network management,
Computer science,
Cities and towns,
Web and internet services,
Internet telephony,
Switches"
Shapely hierarchical graph transformation,"Diagrams can be represented by graphs, and the animation and transformation of diagrams can be modeled by graph transformation. This paper studies extensions of graphs and graph transformation that are important for programming with graphs: /spl middot/ We extend graphs by a notion of hierarchy that supports value composition, and define hierarchical graph transformation in an intuitive way that resembles term rewriting. /spl middot/ We require that admissable shapes for hierarchical graphs are specified by context free graph grammars, in order to set up a type discipline for shapely hierarchical graph transformation. The resulting computational model shall be the basis of the visual language DIAPLAN for programming with graphs that represent diagrams.",
A continuum of theories of lambda calculus without semantics,"In this paper, we give a topological proof of the following result: there exist 2(/sub 0/) lambda theories of the untyped lambda calculus without a model in any semantics based on D.S. Scott's (1972, 1981) view of models as partially ordered sets and of functions as monotonic functions. As a consequence of this result, we positively solve the conjecture, stated by O. Bastonero and X. Gouy (1999) and by C. Berline (2000), that the strongly stable semantics is incomplete.","Calculus,
Equations,
Algebra,
Lattices,
Mathematical model,
Context modeling,
Topology"
A general approach for the stability analysis of time-domain finite element method,Considerable attention has been devoted to time-domain numerical methods for simulating electromagnetic transient phenomena. A general approach based on discrete system analysis is developed for investigating the stability behavior of time-domain finite element methods (TDFEM). This approach is applied to a variety of recently developed TDFEM schemes. These include: (1) time-domain finite element modeling of dispersive media; (2) time-domain finite element-boundary integral (FE-BI) method; (3) higher-order time-domain finite element schemes; and (4) orthogonal time-domain finite element method. Numerical results demonstrate the validity of the proposed approach for stability analysis.,"Stability analysis,
Time domain analysis,
Finite element methods,
Finite difference methods,
Dispersion,
Computational electromagnetics,
Computational modeling,
Computer simulation,
Integral equations,
Partial differential equations"
Query efficient PCPs with perfect completeness,"For every integer k>1, we present a PCP characterization of NP where the verifier uses logarithmic randomness, queries 4k+k/sup 2/ bits in the proof, accepts a correct proof with probability 1 (i.e. it is has perfect completeness) and accepts any supposed proof of a false statement with a certain maximum probability. In particular, the verifier achieves optimal amortized query complexity of 1+/spl delta/ for arbitrarily small constant /spl delta/>0. Such a characterization was already proved by A. Samorodnitsky and L. Trevisan (2000), but their verifier loses perfect completeness and their proof makes an essential use of this feature. By using an adaptive verifier, we can decrease the number of query bits to 2k+k/sup 2/, the same number obtained by Samorodnitsky and Trevisan. Finally, we extend some of the results to larger domains.",
Adaptive quasiconformal kernel metric for image retrieval,"The paper presents a novel approach to ranking relevant images for retrieval. Distance in the feature space associated with a kernel is used to rank relevant images. An adaptive quasiconformal mapping based on relevance feedback is used to generate successive new kernels. The effect of the quasiconformal mapping is a change in the spatial resolution of the feature space. The spatial resolution around irrelevant samples is dilated, whereas the spatial resolution around relevant samples is contracted. This new space created by the quasiconformal kernel is used to measure the distance between the query and the images in the database. An interesting interpretation of the metric is found by looking at the Taylor series approximation to the original kernel. Then the squared distance in the feature space can be seen as a combination of a parzen window estimate of the squared Chi-squared distance and a weighted squared Euclidean distance. Experimental results using real-world data validate the efficacy of our method.","Kernel,
Image retrieval,
Feedback,
Mars,
Information retrieval,
Computer science,
Spatial resolution,
Image databases,
Content based retrieval,
Extraterrestrial measurements"
Hybrid ensembles and coincident-failure diversity,"This paper presents an approach for constructing hybrid ensembles with two categories of members: trained neural networks and decision trees in the hope of increasing diversity and reducing coincident failures. The diversity among the members of an ensemble has been generally recognised as a key factor for improving the overall performance of the ensemble. Of the two heterogeneous members evaluated with a number of the diversity measures, we found that there is a statistically low level coincident-failure diversity among homogeneous members but a relatively high level diversity between heterogeneous members. This provides theoretical basis for constructing hybrid ensembles with members developed by different methodologies. The hybrid ensembles have been built for a real-world problem: predicting training injury of military recruits. Their performances are evaluated in terms of diversity, reliability and prediction accuracy, and also compared with the homogeneous ensembles of neural nets or decision trees. The results indicate that the hybrid ensembles have a considerably high level diversity and thus are able to produce a better performance.","Neural networks,
Diversity reception,
Decision trees,
Injuries,
Recruitment,
Bagging,
Boosting,
Computer networks,
Computer science,
Biomedical engineering"
Assessment of maintainability in object-oriented software,"In software industrial practice, the high cost of the maintenance process for large-scale software has placed emphasis on the need to manage the maintainability in earlier phases of the software life-cycle. This paper discusses the applicability of architectural complexity measures to managing the complexity in the software maintenance phase and the reliability of the maintenance process. It also discusses the theoretical validity of the architectural complexity measures, as has been established using PVS, a formal verification system.","Software maintenance,
Software systems,
Software measurement,
Laboratories,
Phase measurement,
Formal verification,
Information theory,
Computer science,
Computer industry,
Costs"
Recognizing geometric patterns for beautification of reconstructed solid models,"Boundary representation models reconstructed from 3D range data suffer from various inaccuracies caused by noise in the data and the model building software. The quality of such models can be improved in a beautification step, which finds regular geometric patterns approximately present in the model and imposes a maximal consistent subset of constraints deduced from these patterns on the model. This paper presents analysis methods seeking geometric patterns defined by similarities. Their specific types are derived from a part survey estimating the frequencies of the patterns in simple mechanical components. The methods seek clusters of similar objects which describe properties of faces, loops, edges and vertices, try to find special values representing the clusters, and seek approximate symmetries of the model. Experiments show that the patterns detected appear to be suitable for the subsequent beautification steps.","Pattern recognition,
Solid modeling,
Surface fitting,
Pattern analysis,
Reverse engineering,
Frequency estimation,
Milling machines,
Computer science,
Face detection,
Data mining"
"Developing software components with the UML, Enterprise Java Beans and aspects","Component-based systems have become increasingly popular approaches to developing complex systems, offering well formed abstractions, strong potential for reuse, dynamic plug-and-play and sometimes end-user application enhancement. Unfortunately the design, implementation and deployment of components is very challenging, particularly achieving appropriate division of responsibility among components, designing components and implementing components. We have developed the aspect-oriented component engineering method to help improve component development by the use of aspects during component specification, design, implementation and deployment. We describe our work extending the UML to facilitate aspect-oriented component design and the use of Enterprise Java Beans to implement these designs.","Unified modeling language,
Java,
Object oriented modeling,
Application software,
XML,
Runtime,
Software standards,
Standards development,
User interfaces,
Computer science"
Adaptive cluster computing using JavaSpaces,,"Java,
Parallel processing,
Concurrent computing,
Distributed computing,
Application software,
Software systems,
Resource management,
Laboratories,
Engineering profession,
Computerized monitoring"
Mining generalized fuzzy quantitative association rules with fuzzy generalization hierarchies,"Association rule mining is an exploratory learning task to discover some hidden dependency relationships among items in transaction data. Quantitative association rules denote association rules with both categorical and quantitative attributes. There have been several works on quantitative association rule mining such as the application of fuzzy techniques to quantitative association rule mining, the generalized association rule mining for quantitative association rules, and importance weight incorporation into association rule mining for taking into account the user's interest. This paper introduces a new method for generalized fuzzy quantitative association rule mining with importance weights. The method uses fuzzy concept hierarchies for categorical attributes and generalization hierarchies of fuzzy linguistic terms for quantitative attributes. It enables the users to flexibly perform the association rule mining by controlling the generalization levels for attributes and the importance weights for attributes.","Data mining,
Association rules,
Transaction databases,
Computer science,
Information technology,
Data engineering"
Practical software process improvement - the IMPACT project,"For many years now software process improvement (SPI) has been recognised as an effective way for companies to improve the quality of the software they produce and the productivity with which they work. Much work has gone into developing and selling improvement paradigms, assessment methods, modelling languages, tools and technologies. The challenge for -small-to-medium software development companies (SMEs) now is to find a way to apply these SPI technologies to realise their company's improvement goals. For SMEs the most pressing requirements for improvement paradigms are that they are not only effective but that they realise tangible results quickly, can be implemented incrementally and utilise the many existing process improvement technologies. The paper presents a framework for SPI that realises these needs. The framework is designed to utilise a range of improvement technologies and supports continuous and highly focused improvement over many projects, thus producing timely, cost-effective and tangible improvements for SMEs. The effectiveness of the framework is illustrated with its application in a small, Sydney-based, Web development company.","Australia,
Application software,
Programming,
Large-scale systems,
Investments,
Computer science,
Software quality,
Productivity,
Pressing,
Profitability"
Template-matching approach to edge detection of volume data,"This paper proposes a template-matching approach to the edge detection of volume data. Twenty-six templates of an ideal step-like edge in the 3/spl times/3/spl times/3 neighborhood of volume data are given, and the step-like edge of volume data is detected by matching such patterns in various orientations. The approach is a simple and straightforward one for edge detection of volume data. It generalizes the well-known Kirsch operator for 2D images. It can detect change of intensity in every direction, and has the property of rotation invariance in 18-neighborhood. Implementation of proposed approach is given for biological and medical volume data, including MRI and CT volume data.","Image edge detection,
Detectors,
Biomedical imaging,
Sea surface,
Magnetic resonance imaging,
Computed tomography,
Lifting equipment,
Computer science,
Data engineering,
Orthopedic surgery"
Modeling realistic virtual hairstyles,"The author presents an effective method for modeling realistic curly hairstyles, taking into account both artificial hairstyling processes and natural curliness. The result is a detailed geometric model of hairs that can be rendered and animated via existing methods. Our technique exploits the analogy between hairs and a vector field; it interactively and efficiently models global and local hair flows by superimposing procedurally defined vector field primitives that have local influence. Usually only a very small number of vector field primitives are needed to model a complicated hairstyle. An initial model of hair strands is extracted from the superimposed vector fields by tracing their field lines. Random natural or artificial curliness can be added to the initial model through a parametric hair offset function with a randomized distribution of parameters over the scalp. Techniques for shearing and clustering are also designed to improve the overall appearance of the hair model. Our technique has been successfully applied to generate a variety of realistic hairstyles with different curliness and length distributions.","Hair,
Scalp,
Shape,
Animation,
Shearing,
Deformable models,
Humans,
Computer graphics,
Gravity,
Computer science"
The social affordances of computer-supported collaborative learning environments,Field observations report that contemporary computer-supported collaborative learning (CSCL) environments do not completely fulfill the expectations of both educators and learners regarding their potential for supporting interactive learning and collaborative knowledge construction. This raises questions amongst educators whether CSCL environments do indeed provide genuine opportunities for learning processes that rely on social interaction. The authors believe that one of the factors that may explain this is the generally accepted assumption that social interaction can be taken for granted. Social interaction in CSCL environments can no more be taken for granted than it can in face-to-face settings. A theoretical framework is presented proposing the support of social interaction by embedding certain properties in the CSCL environment that act as social contextual facilitators relevant for the learner's social interaction. They refer to these properties as social affordances. A group awareness widget (GAW) is a software tool providing the learner with group awareness on the others in different contexts while at the same time enabling the learner to communicate with them. The authors hypothesize that GAWs provoke social affordances and formulate guidelines for designing GAWs.,"Collaborative work,
Collaboration,
Distributed computing,
Computer mediated communication,
Software tools,
Context awareness,
Guidelines,
Communications technology,
Embedded computing,
Education"
The airport gate assignment problem: mathematical model and a tabu search algorithm,"Considers an airport gate assignment problem that dynamically assigns airport gates to scheduled flights based on passengers' daily origin and destination flow data. The objective of the problem is to minimize the overall connection times during which passengers walk to catch their connection flights. We formulate this problem as a mixed 0-1 quadratic integer programming problem and then reformulate it as a mixed 0-1 integer problem with a linear objective function and constraints. We design a simple tabu search meta-heuristic to solve the problem. The algorithm exploits the special properties of different types of neighborhood moves, and create highly effective candidate list strategies. We also address issues of tabu short-term memory, dynamic tabu tenure, aspiration rules, and various intensification and diversification strategies. Preliminary computational experiments are conducted, and the results are presented and analyzed.","Airports,
Mathematical model,
Legged locomotion,
Job shop scheduling,
Dynamic scheduling,
Scheduling algorithm,
Linear programming,
Computer industry,
Processor scheduling,
Customer service"
DNS-based Internet client clustering and characterization,"This paper proposes a novel protocol which uses the Internet domain name system (DNS) to partition Web clients into disjoint sets, each of which is associated with a single DNS server. We define an L-DNS cluster to be a grouping of Web clients that use the same Local DNS server to resolve Internet host names. We identify such clusters in real-time using data obtained from a Web Server in conjunction with that server's authoritative DNS-both instrumented with an implementation of our clustering algorithm. Using these clusters, we perform measurements from four distinct Internet locations. Our results show that L-DNS clustering enables a better estimation of proximity of a Web client to a Web server than previously proposed techniques. Thus, in a content distribution network, a DNS-based scheme that redirects a request from a web client to one of many servers based on the client's name server coordinates (e.g., hops/latency/loss-rates) would perform better with our algorithm.","Internet,
Web server,
Network servers,
Instruments,
Clustering algorithms,
Computer science,
Protocols,
Domain Name System,
Availability,
Performance evaluation"
Using a classifier system to improve dynamic load balancing,"Dynamic load balancing is a very important problem in distributed processing. This problem aims to redistribute running processes to achieve better results according some optimization criterion. Since it is an NP-complete problem in its general formulation, it is worth using heuristics to seek better results in a reasonable time. One of the heuristics that has been successfully applied in various static scheduling problems is genetic algorithms (GAs). We propose to use a classifier system that is an adaptive system that applies a GA over a population of decision rules to achieve better decisions about when to carry out preemptive migrations in a distributed environment. The results have been impressive and the classifier system was able to surpass, without previous knowledge of the workload, the performance of a well designed analytic criterion.","Load management,
Processor scheduling,
Genetic algorithms,
Distributed processing,
Scheduling algorithm,
Dynamic scheduling,
Computer science,
Adaptive systems,
Performance analysis,
Distributed computing"
A schema theory analysis of mutation size biases in genetic programming with linear representations,"Understanding operator bias in evolutionary computation is important because it is possible for the operator's biases to work against the intended biases induced by the fitness function. Developments in genetic programming (GP) schema theory can be used to better understand the biases induced by the standard subtree crossover when GP is applied to variable-length linear structures. In this paper, we use the schema theory to better understand the biases induced on linear structures by two common GP subtree mutation operators: FULL and GROW mutation. In both cases, we find that the operators do have quite specific biases and typically strongly oversample shorter strings.","Genetic mutations,
Genetic programming,
Equations,
Computer science,
Standards development,
Ear,
Mathematics,
Evolutionary computation,
Shape"
Efficient comparison-based fault diagnosis of multiprocessor systems using an evolutionary approach,"In comparison models for system-level fault diagnosis pairs of units are given the same job and results are compared. The result of such a comparison test can be 0 (match) or 1 (mismatch) and diagnosis is based on the collection of test results. Two such models have been studied, among others: the symmetric model of Chwa and Hakimi and the asymmetric model of Malek. In this paper a novel approach is proposed for identifying faulty units, based on a well-known optimization procedure, as genetic algorithms, which have proven to be useful in various kinds of problems. Furthermore, a new problem-specific genetic mutation is presented and shown to be better than the standard one. A series of simulations was conducted to show the efficiency of the genetic-based approach.",
Hybrid population-based metaheuristic approaches for the space allocation problem,"A hybrid population-based metaheuristic for the space allocation problem in academic institutions is presented that is based upon previous experiments using a range of techniques including hill-climbing, simulated annealing, tabu search and genetic algorithms. The proposed approach incorporates the best characteristics of each technique, makes an automatic selection of the parameters according to the problem characteristics and surpasses the performance of these standard techniques in terms of the solution quality evaluated with a penalty function. This approach incorporates local search heuristics, adaptive cooling schedules and population-based techniques. Our experiments show that this technique produces competitive solutions for the space allocation problem. In this problem, it is often desirable to obtain a set of candidate solutions so that the decision maker can select the best among them. By controlling a common cooling schedule for the whole population in the simulated annealing component, it is possible to find one excellent solution or to produce a population of good solutions.","Simulated annealing,
Space cooling,
Processor scheduling,
Computational modeling,
Genetic algorithms,
Adaptive scheduling,
Temperature control,
Testing,
Computer science,
World Wide Web"
Provably good global buffering by multiterminal multicommodity flow approximation,"To implement high-performance global interconnect without impacting the placement and performance of existing blocks, the use of buffer blocks is becoming increasingly popular in structured-custom and block-based ASIC methodologies. Recent works by Cong, Kong and Pan (1999) and Tang and Wong (2000) give algorithms to solve the buffer block planning problem. In this paper, we address the problem of how to perform buffering of global multiterminal nets given an existing buffer block plan. We give a provably good algorithm based on a recent approach of Garg and Konemann (1998) and Fleischer (1999) [see also Albrecht (2000) and Dragan et al. (2000)]. Our method routes connections using available buffer blocks, such that required upper and lower bounds on buffer intervals-as well as wirelength upper bounds per connection-are satisfied. In addition, our algorithm allows more than one buffer to be inserted into any given connection and observes buffer parity constraints. Most importantly, and unlike previous works on the problem, we take into account multiterminal nets. Our algorithm outperforms existing algorithms for the problem, which are based on 2-pin decompositions of the nets. The algorithm has been validated on top-level layouts extracted from a recent high-end microprocessor design.","Repeaters,
Inverters,
Routing,
Computer science,
Application specific integrated circuits,
Upper bound,
Algorithm design and analysis,
Mathematics,
Educational institutions,
High performance computing"
Teaching the nonscience major: EE101-The digital information age,"EE 10l-The Digital Information Age, a course taught for the past six years to nonscience majors and freshmen considering electrical engineering as a major, is one the largest courses at Yale with a cumulative enrollment of approximately 2700 students. The goal is to describe how common-place digital information systems work and why they work that way by illustrating clever engineering solutions to technological problems. The course considers the following topics: information sources; logic gates; computer hardware; and software, measuring information using entropy, error detection and correction coding, compression, encryption, data transmission and data manipulation by computer. Earlier versions of EE101 included both hardware and software projects. The hardware project was to implement a bean counter using digital logic modules. The software project involved writing a personal World Wide Web page and developing a Web page for a Yale-affiliated organization. Recent versions replated the hardware project with additional Internet projects that receive data from a Web page viewer and that measure transmission times and the number of nodes between a source and destination. Having completed the course, students feel that they have an appreciation for the digital information systems they encounter on a daily basis.",Electrical engineering education
Shape representation using space filled sub-voxel distance fields,"Voxelisation is the process of converting a source object of any data type into a three-dimensional grid of voxel values. This voxel grid should represent the original object as closely as possible, although some inaccuracies will occur due to the discrete nature of the voxel grid representation. In this paper we report our ongoing research into methods for representing objects as voxelised distance fields, in particular we report fast methods for accurate distance field production. A review of current alternative voxelisation methods is also given.","Shape,
Computer graphics,
Rendering (computer graphics),
Production,
Animation,
Interpolation,
Computer science,
Image segmentation,
World Wide Web,
Three dimensional displays"
Alleviating the Adverse Effects of Residual Stress in RF MEMS Switches,"This paper presents two methods for counteracting the unwanted deflection due to warping or buckling effects, which are serious potential problems in many fabrication processes of microelectromechanical (MEMS) structures due to thin film phenomena. This study is primarily suited for electrostatically actuated RF MEMS switches whose RF and DC performance can be significantly deteriorated by out of plane warping. It can also be applied to MEMS accelerometers, resonators and other similar systems. The first technique focuses on modifying the support structure and the spring constant of the switch, while the second involves a more complicated fabrication process, which selectively increases the switch thickness. Both these techniques yield switches with two to ten times less warping under the same fabrication conditions. The second method, however, presents the additional advantage of maintaining the actuation voltage almost unaffected.","Residual stresses,
Radiofrequency microelectromechanical systems,
Switches,
Fabrication,
Micromechanical devices,
Transistors,
Radio frequency,
Accelerometers,
Springs,
Voltage"
"Interestingness, peculiarity, and multi-database mining","In order to discover new, surprising, interesting patterns hidden in data, peculiarity oriented mining and multidatabase mining are required. In the paper, we introduce peculiarity rules as a new class of rules, which can be discovered from a relatively low number of peculiar data by searching the relevance among the peculiar data. We give a formal interpretation and comparison of three classes of rules: association rules, exception rules, and peculiarity rules, as well as describe how to mine more interesting peculiarity rules in multiple databases.","Data mining,
Relational databases,
Association rules,
Image databases,
Computer science,
Transaction databases,
Electronic mail,
Filtration,
Distributed processing,
Acoustic testing"
Comparing type-based and proof-directed decompilation,"In the past couple of years interest in decompilation has widened from its initial concentration on reconstruction of control flow into well-founded-in-theory methods to reconstruct type information. A. Mycroft (1999) described Type-Based Decompilation and S. Katsumata and A. Ohori (2001) described Proof-Directed Decompilation. The article summarises the two approaches and identifies their commonality, strengths and weaknesses; it concludes by suggesting how they may be integrated.","Laboratories,
Internet telephony,
Information science,
Computer science,
Assembly,
Data mining,
Intellectual property,
Web server,
Logic,
Calculus"
Supporting problem based learning by a collaborative virtual environment: a cooperative hypermedia approach,"Problem based learning (PBL) promotes engagement in meaningful learning and cooperation among students. When applying PBL in distributed groups distance has to be bridged by means of technology. Collaborative virtual environments (CVE) can help to overcome two crucial problems of PBL, if used in a distributed learning situation: Firstly, learners have problems to understand, interact with, and tailor shared learning environments, so that they match their needs. Secondly, groups of learners have problems to construct shared knowledge in a shared learning environment. Our approach is to provide a CVE designed to support PBL. We use the metaphor of a virtual institute to organize the learning environment and to facilitate orientation in and tailoring of the CVE. In addition, we provide a graphical cooperative knowledge representation tool to help groups to construct shared knowledge in a PBL process. We use cooperative hypermedia technology to represent both shared learning spaces and shared information spaces as shared hyperdocuments.","Collaboration,
Virtual environment,
Space technology,
Psychology,
Humans,
Information technology,
Information systems,
Knowledge representation,
Problem-solving,
Learning systems"
Inferring link characteristics from end-to-end path measurements,"In the Internet, because of huge scale and distributed administration, it is of practical importance to infer network-internal characteristics that cannot be measured directly. We propose a general method of determining characteristics of links from given characteristics of end-to-end paths. Our method can be applied to an arbitrary path-topology. Furthermore we show the general conditions that the link characteristics to be inferred must satisfy. Packet loss and queuing delay time are shown to satisfy them. Case studies which our method can treat are also provided.","Quality of service,
Delay effects,
Internet,
Network topology,
Mathematics,
Physics,
Informatics,
Computer science,
IP networks,
Current measurement"
How to half wire lengths in the layout of cyclic shifters,Cyclic shifters are required in many central parts of microprocessors and floating-point units. The main difficulty in conventional cyclic shifter designs are the long internal wire connections. For this reason we propose cyclic shifter layouts that improve the accumulated wire length on the critical path by rearranging the placement of the logical gates. We can show that in this way the wire length complexity on the critical path can be reduced from /spl Omega/(n log(n)) in conventional designs to O(n) in our optimized designs where n is the width of the shifted operand. For the practical case of n=64 we shorten the accumulated wire length on the critical path by a factor of 2.20. In the same design the maximal size of a net that has to be driven by a single gate is cut down by a factor of 1.86. This leads to faster cyclic shifter designs with less power consumption.,"Wire,
Microprocessors,
Design optimization,
Energy consumption,
Circuit synthesis,
Routing,
Computer science,
Electronic mail"
Aggregated hierarchical multicast for active networks,"Active networking is the basis for a range of new and innovative applications that make use of computational resources inside network routers. One such application is aggregated hierarchical multicast, which aims at implementing efficient many-to-many communication. In certain scenarios, it is possible to transmit less accurate, aggregated data and thus achieve better scalability. Using active networks, the aggregation computation can be done transparently by network routers without end system support. We show how aggregated data streams can be structured in a hierarchical fashion to allow easy access of data at the desired aggregation level. We introduce two application examples to illustrate the system design, analyze the performance of the aggregation mechanism and evaluate it using a prototype implementation.","Information systems,
Application software,
Computer networks,
Scalability,
Prototypes,
Bandwidth,
Multiprocessor interconnection networks,
Computer science,
Performance analysis,
Telecommunication traffic"
Generating EDI message translations from visual specifications,"Electronic data interchange (EDI) systems are used in many domains to support inter-organisational information exchange. To get systems using different EDI message formats to communicate, complex message translations (where data must be transformed from one EDI message format into another), are required. We describe a visual language and support environment which greatly simplify the task of the systems integrator by using a domain-specific visual language to express data formats and format translations. Complex message translations are automated by an underlying transformation engine. We describe the motivation for this system, its key visual language and transformation engine features, a prototype environment, and experience translating it into a commercial product.","XML,
Protocols,
Message-oriented middleware,
Engines,
Prototypes,
Medical treatment,
Data handling,
Biomedical informatics,
Costs,
Computer science"
Efficient systematic error-correcting codes for semi-delay-insensitive data transmission,"A lot of papers have been written on error correcting/detecting codes for data transmission, but none of the codes are designed to address error correction for delay-insensitive or semi-delay-insensitive (SDI) data transmission. In this paper we address the problem of SDI communication where errors may occur during transmission or encoding. Three error models for SDI data transmission are defined and the general solution schemes for these models are proposed. An efficient systematic SDI error-correcting code for the four-phase handshaking protocol is designed to solve one asymmetric error model. Our code is efficient in terms of encoding, decoding and completion checking. The novelty of our SDI error-correcting code is that our code can correct errors and detect completion at the same time in SDI data transmission. The proposed schemes are particularly suitable for deep-submicron and high-speed board design.","Error correction codes,
Data communication,
Delay,
Encoding,
Circuits,
Wires,
Decoding,
Computer science,
Data engineering,
Computer errors"
Interoperability for accessing DBs by e-commerce applications,"E-commerce applications cannot dictate the content of databases (DBs) or how the data services are provided. Thus the problem of interoperability arises when accessing existing data stores. A generalized version of this problem is called an impedance mismatch problem that arises when object oriented programs store objects in relational DBs. We describe a framework that hides the complexities associated with creating and accessing distributed objects that access DBs through SQL queries. An application programmer (developer) needs only to specify the set query and select the DB and its driver. Access to the data is provided through an automatically generated application program interface (API) in the selected language. Access to a DB server is through a distributed object implemented using either CORBA or DCOM, as specified by the developer.","Satellite broadcasting,
Application software,
Object oriented modeling,
Logic programming,
Computer languages,
Information retrieval,
Computer science,
Relational databases,
Programming profession,
Switches"
A compact dual-band microstrip-fed monopole antenna,"The rapid developments in the wireless communications industry demand novel antenna designs that can be used in more than one frequency band. Applications include: dual-band cellular phones; laptop computers for wireless printer and modem connections; laptops in wireless local area network (LAN) applications; Bluetooth devices. Ali, M. et al., (see 1998 IEEE Antennas Propagat. Soc. Int. Symp. Dig., p.794-7, 1998) describe a dual-band strip-sleeve monopole antenna for use on a laptop computer. We extend the antenna design to be used for dual-band ISM applications. In this application there is a particular interest in obtaining an increased operational bandwidth for the antenna. The effects of design parameters of the microstrip-fed uniplanar monopole antenna on its operational frequency and impedance bandwidth are presented and discussed. The antenna resonates at 1800 MHz and 2450 MHz.","Microstrip antennas,
Dual band,
Antennas and propagation,
Application software,
Portable computers,
Frequency,
Wireless LAN,
Bandwidth,
Wireless communication,
Communication industry"
Summary of dynamically discovering likely program invariants,"The dissertation dynamically discovering likely program invariants introduces dynamic detection of program invariants, presents techniques for detecting such invariants from traces, assesses the techniques' efficacy, and points the way for future research. Invariants are valuable in many aspects of program development, including design, coding, verification, testing, optimization, and maintenance. They also enhance programmers' understanding of data structures, algorithms, and program operation. Unfortunately, explicit invariants are usually absent from programs, depriving programmers and automated tools of their benefits. The dissertation shows how invariants can be dynamically detected from program traces that capture variable values at program points of interest. The user runs the target program over a test suite to create the traces, and an invariant detector determines which properties and relationships hold over both explicit variables and other expressions. Properties that hold over the traces and also satisfy other tests, such as being statistically justified, not being over unrelated variables, and not being implied by other reported invariants, are reported as likely invariants. Like other dynamic techniques such as testing, the quality of the output depends in part on the comprehensiveness of the test suite. If the test suite is inadequate, then the output indicates how, permitting its improvement. Dynamic analysis complements static techniques, which can be made sound but for which certain program constructs remain beyond the state of the art. Experiments demonstrate a number of positive qualities of dynamic invariant detection and of a prototype implementation, Daikon. Invariant detection is accurate-it rediscovers formal specifications-and useful-it assists programmers in programming tasks. It runs quickly and produces output of modest size. Test suites found in practice tend to be adequate for dynamic invariant detection.","Testing,
Instruments,
Programming profession,
Detectors,
Data structures,
Formal specifications,
Laboratories,
Computer science,
Design optimization,
Prototypes"
An anonymous fair exchange e-commerce protocol,,"Protocols,
Information science,
Internet,
Protection,
Computer crime"
Virtualization considered harmful: OS design directions for well-conditioned services,"We argue that existing OS designs are ill-suited for the needs of Internet service applications. These applications demand massive concurrency (supporting a large number of requests per second) and must be well-conditioned to load (avoiding degradation of performance and predictability when demand exceeds capacity). The transparency and virtualization provided by existing operating systems leads to limited concurrency and lack of control over resource usage. We claim that Internet services would be far better supported by operating systems by reconsidering the role of resource virtualization. We propose a new design for server applications, the staged event-driven architecture (SEDA). In SEDA, applications are constructed as a set of event driven stages separated by queues. We present the SEDA architecture and its consequences for operating system design.","Operating systems,
Web and internet services,
Resource virtualization,
Concurrent computing,
Resource management,
Application software,
Control systems,
Web server,
Application virtualization,
Computer science"
CodeWeb: data mining library reuse patterns,"Developers learn to use a software library not just from its documentation but also from toy examples and existing real-life application code (e.g. by using grep). The CodeWeb tool takes this simple idea further by a deeper analysis of a large collection of applications to see what characteristic usage of the library is like. We demonstrate the tool by showing how the KDE core libraries are used in real-life KDE applications (KDE is a graphical desktop environment for UNIX). Moreover, we look at a recently-developed feature that helps software developers port an application from an old version of a library to a new one.","Data mining,
Application software,
Software libraries,
Documentation,
Computer science,
Australia,
Open source software,
Association rules"
Strong normalisation in the /spl pi/-calculus,"Introduces a typed /spl pi/-calculus where strong normalisation is ensured by typability. Strong normalisation is a useful property in many computational contexts, including distributed systems. In spite of its simplicity, our type discipline captures a wide class of converging name-passing interactive behaviours. The proof of strong normalisability combines methods from typed /spl lambda/-calculi and linear logic with process-theoretic reasoning. It is adaptable to systems involving state and other extensions. Strong normalisation is shown to have significant consequences, including finite axiomatisation of weak bisimilarity, a fully abstract embedding of the simply-typed /spl lambda/-calculus with products and sums and basic liveness in interaction. Strong normalisability has been extensively studied as a fundamental property in functional calculi, term rewriting and logical systems. This work is one of the first steps to extend theories and proof methods for strong normalisability to the context of name-passing processes.","Tin,
Distributed computing,
Logic,
Computer languages,
Computer science,
Context,
Calculus,
Mathematics,
Functional programming,
Embedded computing"
A neural model combining attentional orienting to object recognition: preliminary explorations on the interplay between where and what,"We propose a model of primate vision that integrates both an attentional orienting (""where"") pathway and an object recognition (""what"") pathway. The fast visual attention front-end rapidly selects the few most conspicuous image locations, and the slower object recognition back-end identifies objects at the selected locations. The model is applied to classical visual search tasks, consisting of finding a specific target among an array of distracting visual patterns (e.g., a circle among many squares). The encouraging results obtained, in which substantial speedup is achieved by the combined attention recognition model while maintaining good recognition performance compared to an exhaustive search, suggest that the biologically-inspired architecture proposed represents an efficient solution to the difficult problem of rapid scene analysis.","Object recognition,
Biological system modeling,
Image analysis,
Focusing,
Streaming media,
Computer science,
Content addressable storage,
Neuromorphics,
Brain modeling,
Biological control systems"
Statistical analysis of final year project marks in the computer engineering undergraduate program,"This study is to find the existence of discrepancy between the supervisor and assessor in project assessment. A systematic approach is suggested using statistical analysis. Project marks of the computer engineering program obtained from the past two years are recorded using Excel and analyzed using SPSS. Using one-way analysis of variance, it is found that the reason for the discrepancy is due to the excessively low marks given by the assessors. The outcome of this study helps to identify those projects that need to be reassessed. Standardization of the project assessment guarantees that the marking is fair and truly reflects the student performance.",Computer science education
The intelligent assessment system in Web-based distance learning education,"Based on the modern student-oriented education mode, the paper investigates the importance of the assessment system in the Web-based distance learning education environment and establishes an intelligent assessment system model which is characterized for its positive performance and adaptability. Besides, an implementation of the assessment system is given, based on this model. In an integrated education environment, this kind of assessment system cooperates with other subsystems during teaching and learning processes, and plays an important role in improving the performance of the whole education system.","Intelligent systems,
Computer aided instruction,
Education,
System testing,
Data mining,
Web mining,
Distance learning,
Feedback,
Adaptive systems,
Neural networks"
Adaptive runtime partitioning of AMR applications on heterogeneous clusters,,"Runtime,
Adaptive mesh refinement,
Adaptive systems,
Load management,
Resource management,
Application software,
Grid computing,
Engines,
Software systems,
Cost accounting"
Preventing traffic analysis in packet radio networks,"Traffic analysis poses a serious threat to communication security, especially in wireless networks. Strong encryption and traffic padding are often used to hide message contents as well as traffic patterns. In a dynamic environment, when real traffic patterns change, i.e. when the network switches its operation mode, determining an optimal cover mode such that the real traffic service is not degraded and the real traffic patterns are not revealed becomes a challenge. In this paper, we discuss different methods of constructing a traffic cover mode, we formulate an optimality problem, and we present a solution to it.","Telecommunication traffic,
Intelligent networks,
Packet radio networks,
Communication system security,
Protection,
Public key cryptography,
Data security,
Information security,
Payloads,
Computer science"
Understanding object-oriented programming concepts,"Most power engineering students these days have had at least a casual introduction to computer programming concepts. Specifically, they have become functionally fluent in one or more programming languages and can use their knowledge of basic syntax to write simple programs that perform some desired task. Some students may also have had the opportunity to learn basic software design concepts, such as the benefits of top-down design and the practice and virtues of programming in a modular fashion using a structured programming approach. However, fewer students have been exposed to the theory and practice of object-oriented software development. Understanding object-oriented programming concepts requires that the student undergo a paradigm shift. The student must move from thinking about modeling systems and problems in terms of the actions that must be performed to thinking about the objects that must interact with each other to perform those actions. Successfully navigating this change in thought is not a trivial undertaking, but the rewards of doing so can be tremendous. This presentation provides a brief introduction to the concepts and benefits of the object-oriented approach and explains why power engineering students may benefit greatly from a more formal introduction to the topic.","Object oriented programming,
Programming profession,
Power system modeling,
Object oriented modeling,
Power engineering,
Power system analysis computing,
Power engineering and energy,
Systems engineering and theory,
Power system management,
Encapsulation"
Distributed security management using LDAP directories,"Presently, many companies share business information by interconnecting their networks through the Internet. However, this advanced degree of connectivity also increases the network security management complexity. Most of this complexity results form the need of controlling the connectivity of each network with respect to the others and the Internet. Also, it is necessary to take into account changes on users, shared resources and services, not only in the local network, but also in the interconnected networks. Because of these changes, network administrators are systematically confronted with firewall and other network elements reconfiguration. This paper proposes the use of a LDAP global directory service to simplify the task of managing the security in large-scale networks. By taking advantage of the distributed features of directory services, the paper defines a strategy for managing a group of interconnected networks as a single entity, without removing the administration autonomy of each independent network.","IP networks,
Access control,
Access protocols,
Protection,
Operating systems,
Web pages,
Electronic mail,
Web and internet services,
Companies,
Information security"
Allocation of multicast nodes in wavelength-routed networks,"We investigate the allocation of multicast nodes and formalize it as splitter placement in wavelength-routed networks (SP-WRN) problem. The SP-WRN problem entails the placement of multicast nodes so that the overall network blocking probability is minimized. To gain a deeper insight into the computational complexity of the SP-WRN problem, we define a graph-theoretic version of the splitter placement problem (SPG), and show that even SPG is NP-complete. We develop three heuristics for the SP-WRN problem with different degrees of trade-off between computation time an quality of solution. The first heuristic uses CPLEX, the second heuristic is based on a greedy approach, and the third heuristic employs simulated annealing. Numerical examples demonstrate that: (i) no more than 50% of the cross-connects need to be multi-case capable, and (ii) the iterative simulated annealing heuristic provides fast near-optimal solutions.","Intelligent networks,
All-optical networks,
Wavelength routing,
Computational modeling,
Simulated annealing,
Unicast,
Optical fiber communication,
Wavelength assignment,
Computer science,
Computational complexity"
Measuring behavioral correspondence to a timed concurrent model,"Research in formal methods has produced fruitful techniques that can verify global properties of a design of a real-time system, or exact behavioral correspondence to the design. Exactness is often not achieved, however, and yet understanding how close the design and system correspond still would be very valuable to direct further efforts in achieving exactness or to modify the design where the system simply cannot achieve the requirements. The paper describes a method and tool that fills this niche, by quantitatively measuring how closely the behavior of a real-time system corresponds to its specification, given in a timed, concurrent model.","Real time systems,
Helium,
Computer science,
Maintenance,
Design engineering,
Reverse engineering,
Government,
Software testing,
Humans,
Automata"
A trace transformation technique for communication refinement,"Models of computation like Kahn and dataflow process networks provide convenient means for modeling signal processing applications. This is partly due to the abstract primitives that these models offer for communication between concurrent processes. However, when mapping an application model onto an architecture, these primitives need to be mapped onto architecture level communication primitives. We present a trace transformation technique that supports a system architect in performing this communication refinement. We discuss the implementation of this technique in a tool for architecture exploration named SPADE and present examples.","Signal processing,
Computational modeling,
Computer architecture,
Signal design,
Signal mapping,
Computer networks,
Computer science,
Mobile communication,
TV,
Embedded software"
Reengineering relational databases to object-oriented: constructing the class hierarchy and migrating the data,"The object-oriented data model is predicted to be the heart of the next generation of database systems. Users want to move from old legacy databases into applying this new technology that provides extensibility and flexibility in maintenance. However, a major limitation on the wide acceptance of object-oriented databases is the amount of time and money invested on existing database applications, which are based on conventional legacy systems. Users do not want to loose the huge amounts of data present in conventional databases. This paper presents a novel approach to transform a given conventional database into an object-oriented database. It is assumed that the necessary characteristics of the conventional database to be re-engineered are known and available. The source of these characteristics might be the data dictionary and/or an expert in the given conventional database. We implemented a system that builds an understanding of a given conventional database by taking these characteristics as input and produces the corresponding object-oriented database as output. The system derives a graph that summarizes the conceptual model. Links in the graph are classified into inheritance links and aggregation links. This classification leads to the class hierarchy. Finally, we handle the migration of data from the conventional database to the constructed object-oriented database.","Relational databases,
Object oriented databases,
Database systems,
Data models,
Heart,
Information systems,
Computer science,
Data engineering,
Dictionaries,
Object oriented modeling"
Parallel IP packet forwarding for tomorrow's IP routers,"The invention and evolution of the dense wavelength division multiplexing (DWDM) technology has brought a breakthrough to high-speed networks, and it has put a lot of pressure on research in the area of IP router to catch up. Besides, with up-coming quality of service (QoS) requirements raised by a wide range of communication-intensive, real-time multimedia applications, the next-generation IP routers should be QoS-capable. Limited by Moore's law, one possible solution is to introduce parallelism as well as the differentiated service (DiffServ) scheme [5, 11] into the router architecture to provide QoS provision at a high speed and a low cost. We propose a novel architecture called the high-performance QoS-capable IP router (HPQR). We address one key design issue in our architecture-the distribution of IP packets to multiple independent routing agents so that the workload at routing agents is balanced and the packet ordering is preserved. We introduce the enhanced hash-based distributing scheme (EHDS) as the solution. Simulations are carried out to study the effectiveness of EHDS. The results show that EHDS does meet our design goals very well.","Routing,
Switches,
Quality of service,
Parallel processing,
Wavelength division multiplexing,
Internet,
Fabrics,
Bandwidth,
Computer science,
High-speed networks"
Integrated design of fault diagnosis and accommodation schemes for a class of nonlinear systems,"The paper presents an integrated methodology for detecting, isolating and accommodating faults in a class of nonlinear dynamical systems. A fault diagnosis module is used for fault detection and isolation. Based on the fault information obtained during the fault diagnosis procedure, a fault-tolerant control module is designed to compensate for the effects of faults. In the presence of a fault, a nominal controller guarantees the boundedness of all the system signals until the fault is detected. Then the controller is reconfigured after fault detection and after fault isolation, respectively, to improve the control performance using the fault information generated by the diagnosis module. Under certain assumptions, the stability of the closed-loop system is rigorously investigated.","Fault diagnosis,
Nonlinear systems,
Fault detection,
Fault tolerance,
Control systems,
Algorithm design and analysis,
Computer science,
Stability,
Analytical models,
Redundancy"
New trends in evolutionary computation,"In the last five years, the field of evolutionary computation (EC) has seen a resurgence of new ideas, many stemming from new biological inspirations. The paper outlines four of these new branches of research: creative evolutionary systems, computational embryology, evolvable hardware and artificial immune systems, showing how they aim to extend the capabilities of EC. Recent, unpublished results by researchers in each area at the Department of Computer Science, UCL are provided.","Evolutionary computation,
Evolution (biology),
Computer science,
Educational institutions,
Embryo,
Hardware,
Blades,
Biology computing,
Artificial immune systems,
Technological innovation"
Leveraging program analysis for Web site reverse engineering,"Web sites are complex and heterogeneous systems, characterized by a large number of employed technologies. Evolving these systems requires the skills of a ""renaissance reverse engineer"". In order to assist reverse engineers in their efforts, new program analyses need to be developed that are specifically tailored to the unique task of Web site reverse engineering. To illustrate the design space for program analyses, we introduce a classification based on dichotomies and discuss each of them in the light of Web site reverse engineering. The main contribution of the paper is a better understanding of the program analyses features for Web site reverse engineering.","Reverse engineering,
Software maintenance,
Web pages,
Reliability engineering,
Standards publication,
Computer science,
Time to market,
Software engineering,
Software systems,
Information analysis"
Model-checking for validation of a fault protection system,"The Fault Protection (FP) system of a spacecraft is a critical component for its operation. The system diagnoses problems with the health of the spacecraft, and directs actions to resolve those problems. It therefore warrants a high degree of assurance as to its correctness. In this paper, we describe the use of model checking to help validate key requirements of such a FP system. The particular system we deal with is that of a generic FP engine ""networked"" to the rest of the spacecraft. Its design is specified with a high degree of rigor, using state machine diagrams to define both the FP engine, and the spacecraft-specific responses that the engine directs. We describe the way we have modeled the FP engine and its operating environment so as to validate key requirements of its operation, and the influence of the above design characteristics on this effort.",
Adaptive smoothing tangential direction fields on polygonal surfaces,"The paper develops a simple and effective method for adaptive smoothing tangential direction fields defined on piecewise smooth surfaces approximated by triangle meshes. The method consists of simultaneous and iterative updating each vertex direction by a weighted sum of the directions at the neighboring vertices. The weights themselves depend on directions: if the direction at a given vertex differs strongly from the direction at a neighboring vertex, then the weight associated with that neighboring direction is chosen small. Choosing such nonlinear weights allows us to preserve important direction field discontinuities during the smoothing process. Applications of the developed smoothing technique to curvature extrema detection and non-photorealistic rendering with curvature lines are given.","Smoothing methods,
Cities and towns,
Image processing,
Iterative methods,
Visualization,
Computer science,
Application software,
Computer graphics,
Fingerprint recognition,
Image matching"
Automatic knot determination of NURBS for interactive geometric design,"This paper presents a novel modeling technique and develops an interactive algorithm that facilitates the automatic determination of non-uniform knot vectors as well as other control variables for NURBS curves and surfaces through the unified methodology of energy minimization, variational principle, and numerical techniques. Many geometric algorithms have been developed for NURBS during the past three decades. Recently, the optimization principle has been widely studied, which affords designers to interactively manipulate NURBS via energy functionals, simulated forces, qualitative and quantitative constraints, etc. The existing techniques primarily concentrate on NURBS control points. In this paper we further augment our NURBS modeling capabilities by incorporating NURBS' non-uniform knot sequence into our shape parameter set. The automatic determination of NURBS knots will facilitate the realization of the full geometric potential of NURBS. We also have developed a modeling framework which supports a large variety of functionals ranging from simple quadratic energy forms to non-linear curvature-based (or area-based) objective functionals.","Spline,
Surface topography,
Surface reconstruction,
Shape control,
Algorithm design and analysis,
Deformable models,
Solid modeling,
Design optimization,
Weight control,
Computer science"
Multiresolution time domain modeling for large scale wireless communication problems,"The time domain modeling of large scale cosite interference problems, arising in wireless communication channel analysis and design studies, is discussed. It is pointed out that the use of the conventional finite difference time domain method for the treatment of such problems typically results in long, computationally burdensome simulations that limit the ability of this technique to provide an efficient CAD-oriented tool for commercial and military applications. For the purpose of accelerating these simulations, the use of wavelet based time domain solvers along with parallelization techniques is proposed.",
Use of context-awareness in mobile peer-to-peer networks,"Mobile ad-hoc network are an emerging research field due to the potential range of applications that they support and for the problems they present due to their dynamic nature. Peer-to-peer is an example of a class of applications that have recently been deployed on top of ad-hoc networks. In this paper we propose an approach based on context-awareness to allow peer-to-peer applications to exploit information on the underlying network context to achieve better performance and better group organization. Information such as availability of resources, battery power, services in reach and relative distances can be used to improve the routing structures of the peer-to-peer network, thus reducing the routing overhead.","Intelligent networks,
Peer to peer computing,
Ad hoc networks,
Routing,
Broadcasting,
Mobile computing,
Availability,
Gold,
Computer science,
Educational institutions"
Focus games for satisfiability and completeness of temporal logic,"Introduce a simple game-theoretic approach to satisfiability checking of temporal logic, for LTL (linear time logic) and CTL (computation tree logic), which has the same complexity as using automata. The mechanisms involved are both explicit and transparent, and underpin a novel approach to developing complete axiom systems for temporal logic. The axiom systems are naturally factored into what happens locally and what happens in the limit. The completeness proofs utilise the game-theoretic construction for satisfiability: if a finite set of formulas is consistent then there is a winning strategy (and therefore construction of an explicit model is avoided).","Logic,
Automata,
Game theory,
Informatics,
Costs,
Books,
Impedance,
Plugs,
Calculus,
Testing"
Recursive LMS L-filters for noise removal in images,"The problem of designing the weights for recursive L-filters optimized by the least mean square (LMS) algorithm is addressed. The coefficients derived for nonrecursive filtering are not optimal for recursive implementation, where the estimate of current pixel depends on the past outputs of the filter. To combat this, analogous to the design of adaptive IIR filters, the optimization scheme referred to as equation-error formulation is employed. The recursive filter performs better in suppressing noise than its nonrecursive counterpart.","Least squares approximation,
Filters,
Filtering,
Recursive estimation,
Pixel,
Convergence,
Attenuation,
Computer science,
Software engineering,
Australia"
Fuzzy hypersphere neural network classifier,"In this paper fuzzy hypersphere neural network (FHSNN) is proposed with its learning algorithm. The FHSNN utilizes fuzzy sets as pattern classes in which each fuzzy set is an union of fuzzy set hyperspheres. Its performance is compared with other two fuzzy neural networks and found to be superior with respect to the training time, recall time per pattern and the generalization.","Fuzzy neural networks,
Neural networks,
Fuzzy sets,
Clustering algorithms,
Character recognition,
Handwriting recognition,
Neurons,
Computer science,
Educational institutions,
Pattern classification"
Multiple fault diagnosis of analog circuits by locating ambiguity groups of test equation,"This paper proposes a method to diagnose the multiple faults in linear analog circuits. The test equation establishes the relationship between the measured responses and faulty excitations due to faulty elements. The QR factorization is applied to identify ambiguity groups in the test verification matrix. The suspicious faulty excitations of the minimum size are determined. Faulty parameters are evaluated using the structural incident signal matrix. Finally, this method is illustrated with an example circuit.","Circuit testing,
Fault diagnosis,
Analog circuits,
Equations,
Circuit faults,
Electrical fault detection,
Fault detection,
Voltage measurement,
Computer science,
Automation"
Development of the Signals and Systems Concept Inventory (SSCI) assessment instrument,"Linear systems theory is a core component of the undergraduate curriculum in electrical and computer engineering. This work-in-progress describes the Signals and Systems Concept Inventory (SSCI), an assessment tool designed to measure students' understanding of fundamental concepts in linear systems. The objective of this research is to develop a means of evaluating pedagogical techniques and curricular reform. The SSCI is patterned after the Force Concept Inventory, which is used to assess students' conceptual understanding of Newtonian physics. This paper discusses the development of the SSCI and its initial testing at the University of Massachusetts Dartmouth (UMassD) and George Mason University (GMU).","Instruments,
Linear systems,
Physics,
Remuneration,
Electrical engineering computing,
Materials testing,
Signal processing,
Signal design,
Electrical engineering,
Processor scheduling"
Event identification based on the information map-INFOMAP,"We present a knowledge representation scheme, INFOMAP, together with a mechanism that matches the event of a natural language sentence with part of the domain ontology in the INFOMAP The design of this scheme is to facilitate both human browsing and computer processing of the domain ontology. INFOMAP is also a knowledge framework designed to facilitate knowledge sharing by different application systems. We constructed a question answering, system to demonstrate the power of INFOMAP. When the QA-system receives a user's query, it will extract the corresponding events or scripts based on the ontology in INFOMAP The understanding of a question involves extracting such information as the question type, the question subject, the question condition and the question context A dialogue on the question is triggered at the same time to guide the user to retrieve more relevant information.","Ontologies,
Natural languages,
Data mining,
Knowledge representation,
Humans,
Application software,
Information retrieval,
Information science,
Large-scale systems,
Speech recognition"
Annotating reusable software architectures with specialization patterns,"An application framework is a collection of classes implementing the shared architecture of a family of applications. It is shown how the specialization interface (""hot spots"") of a framework can be annotated with specialization patterns to provide task-based guidance for the framework specialization process. The specialization patterns define various structural, semantic, and coding constraints over the applications derived from the framework. We also present a tool that supports both the framework development process and the framework specialization process, based on the notion of specialization patterns. We outline the basic concepts of the tool and discuss techniques to identify and specify specialization patterns as required by the tool. These techniques have been applied in realistic case studies for creating programming environments for application frameworks.",
Fast extraction of adaptive multiresolution meshes with guaranteed properties from volumetric data,"We present a new algorithm for extracting adaptive multiresolution triangle meshes from volume datasets. The algorithm guarantees that the topological genus of the generated mesh is the same as the genus of the surface embedded in the volume dataset at all levels of detail. In addition to this ""hard constraint"" on the genus of the mesh, the user can choose to specify some number of soft geometric constraints, such as triangle aspect ratio, minimum or maximum total number of vertices, minimum and/or maximum triangle edge lengths, maximum magnitude of various error metrics per triangle or vertex, including maximum curvature (area) error, maximum distance to the surface, and others. The mesh extraction process is fully automatic and does not require manual adjusting of parameters to produce the desired results as long as the user does not specify incompatible constraints. The algorithm robustly handles special topological cases, such as trimmed surfaces (intersections of the surface with the volume boundary), and manifolds with multiple disconnected components (several closed surfaces embedded in the same volume dataset). The meshes may self-intersect at coarse resolutions. However, the self-intersections are corrected automatically as the resolution of the meshes increase. We show several examples of meshes extracted from complex volume datasets.","Data mining,
Mesh generation,
Biomedical imaging,
Data visualization,
Computer science,
Robustness,
Application software,
Fluid dynamics,
Computational modeling,
Medical simulation"
A framework for business rule presentation,"A business rule is a statement that defines or constrains some aspects of a business. There has been a growing interest in developing techniques to support the extraction of business rules buried in legacy systems. However, little has been done so far to help understand the semantics of extracted business rules. We propose a framework to support the comprehension of business rules extracted from legacy systems. The framework consists of two levels: a representation level and a presentation level. At the representation level, we proposed a language, BRL, to express business rules. We also perform logical inferences over the set of business rules at this level. This helps to recover some properties that may not be explicitly available from the extracted business rules, but are essential to their understanding by users. The presentation level, on the other hand, is concerned with how to convey the semantics of business rules to different users. We believe that the expressiveness and reasoning power of our proposed approach significantly improve previous techniques in helping users to comprehend extracted business rules.","Remuneration,
Computer science,
Information systems,
Investments"
A measurement theory perspective for MCDM,"In this paper, we investigate the meaningfulness of conclusions drawn using the Sugeno (1974) and Choquet integrals as aggregation operators in multicriteria decision making. After introducing measurement theory and fuzzy integration in the MCDM setting, we point out that there is a limit to the comparison of alternatives that one can make, and we indicate what are the meaningful conclusions available with fuzzy integrals.","Decision making,
Fuzzy set theory,
Computer science,
Gold,
Postal services,
Australia,
Decision theory,
Fuzzy systems"
The home model and competitive algorithms for load balancing in a computing cluster,"Most implementations of a computing cluster (CC) use greedy-based heuristics to perform load balancing. In some cases, this is in contrast to theoretical results about the performance of online load balancing algorithms. We define the home model in order to better reflect the architecture of a CC. In this new theoretical model, we assume a realistic cluster structure in which every job has a ""home"" machine which it prefers to be executed on, e.g. due to I/O considerations or because it was created there. We develop several online algorithms for load balancing in this model. We first provide a theoretical worst-case analysis, showing that our algorithms achieve better competitive ratios and perform less reassignments than algorithms for the unrelated machines model, which is the best existing theoretical model to describe such clusters. We then present an empirical average-case performance analysis by means of simulations. We show that the performance of our algorithms is consistently better than that of several existing load balancing methods, e.g. the greedy and the opportunity cost methods, especially in a dynamic and changing CC environment.","Clustering algorithms,
Load management,
Home computing,
Costs,
Computer science,
Computer architecture,
Computational modeling,
Communication networks,
Computer networks,
Bridges"
Wearable security services,"Active spaces and smart rooms are quickly gaining popularity in both the research arena, and in commercial projects. Active spaces are physical spaces populated with massive numbers of specialized embedded computing devices that increase human productivity by enabling users to interact seamlessly with the surrounding environment. Security for active spaces is necessary. In such settings, it is essential to have lightweight, mobile and convenient security services for users that enable them to prove their identities and interact securely with the smart devices that populate the surrounding environment. We propose embedding the security services in a basic wearable device that is already being worn and used by many people on daily basis, the wristwatch. We describe our implementation of these wearable security services, and show how the system is used in our active space test-bed.","Information security,
Computer science,
Embedded computing,
Humans,
Productivity,
TV,
System testing,
Temperature,
Telephony,
Writing"
Type-2 fuzzy sets for modelling nursing intuition,"Expert nurses concurrently assess the patient need for nursing care in several domains of concern to nursing. It is suggested that the process takes into account the overall context of patient need in relation to at least five domains. The degree of need within each domain is prioritised, reflecting the need for nursing intervention. This is an intuitive process in that the expert nurse uses inexact or imprecise information to make judgements based on nursing knowledge and practice wisdom. This paper presents the argument that type-2 fuzzy logic is able to model the perceptions present in the patient assessment. The problem of nursing assessment is discussed and the role of type-2 fuzzy sets in modelling nursing perceptions described. Some results are presented that indicate type-2 fuzzy logic has much to offer for modelling nursing intuition.",
Dual-binarization and anisotropic diffusion of Chinese characters in calligraphy documents,"In this paper, we propose a method for calligraphy document binarization. First, the properties of calligraphy documents are analyzed to classify calligraphy documents into two classes: tablet documents and writing documents. Because calligraphy documents have serious noises and large variations in gray scales of the characters, the foreground will be broken after thresholding. The anisotropic diffusion technique is used to smooth input calligraphy documents, which can preserve character boundaries well. Last, noises are removed from binarized images based on the information of stroke widths. In our experiments, 90 tablet documents and 51 writing documents are binarized and well binarized results are obtained.","Anisotropic magnetoresistance,
Writing,
Image sampling,
Histograms,
Gray-scale,
Sampling methods,
Computer science,
Humans,
Application software,
Image converters"
A perturbation-free replay platform for cross-optimized multithreaded applications,"Development of multithreaded applications is particularly tricky because of their non-deterministic execution behaviors. Tools that support the debugging and performance timing of such applications are needed. Key to the construction of such tools is the ability to repeat the nondeterministic execution behavior of a multithreaded application. A clean separation between the application and the system that runs it facilitates supporting that ability. This paper presents a platform for constructing such tools in a context in which any separation between the application and the underlying system (and between both and the platform's own instrumentation code) has been obscured. DejaVu supports deterministic replay of nondeterministic executions of multithreaded Java programs on the Jalapeno virtual machine (running on a uniprocessor). Jalapeno is written in Java and its optimizing compiler regularly integrates application, virtual machine, and DejaVu instrumentation code into unified machine-code sequences. DejaVu ensures deterministic replay through symmetric instrumentation-side-effect identical instrumentation in both record and replay modes-and remote reflection which exposes the state of an application without perturbing it.","Instruments,
Java,
Application software,
Debugging,
Runtime environment,
Virtual machining,
Optimizing compilers,
Reflection,
Computer science,
Buildings"
Geometric distributions for catadioptric sensor design,"Catadioptric sensors are visual sensors that employ lenses (dioptrics) and mirrors (catoptrics). We present a general method of catadioptric sensor design for realizing prescribed projections. Our method makes use of geometric distributions in. 3-dimensional space, which are generalizations of vector fields. The main idea is this: if one desires a reflective surface that will image the world in a certain way, then this condition determines the orientation of the tangent planes to the surface. Analytically, this means that the surface will then be determined by a pair of partial differential equations, which may or may not have a common solution. We show how to check if a common solution exists. If no common solution exists, we describe a method for obtaining optimal approximate solutions in a least-squares sense. As an example application, we construct a mirror that will give a panoramic view of a scene without any digital unwarping.","Mirrors,
Layout,
Image sensors,
Sensor phenomena and characterization,
Partial differential equations,
Lenses,
Cameras,
Yagi-Uda antennas,
Mathematics,
Computer science"
Learning programming using program visualization techniques,"This paper describes the programming knowledge and skills that learners need to develop, and concludes that this is an area of computer science education where those involved in the teaching of programming need to further explore the nature, structure and function of domain specific knowledge. It has been argued that conceptual models can serve to enhance learners' conceptual understanding of programming. The methods used to enhance the development of accurate mental models include: designing the interface so that users can interact actively with it; using metaphors and analogies to explain concepts; and using spatial relationships so that users can develop capabilities for mental simulations. With the insights afforded by looking more closely at conceptual understanding, we describe how visualization techniques may effectively be used in the learning and reaching of programming.","Visualization,
Programming profession,
Computer science education,
Pervasive computing,
Functional programming,
Internet,
Educational programs,
Information systems,
Cognitive science,
Computational modeling"
Three theorems regarding testing graph properties,"Property testing is a relaxation of decision problems in which it is required to distinguish YES-instances (i.e., objects having a predetermined property) from instances that are far from any YES-instance. We present three theorems regarding testing graph properties in the adjacency matrix representation. More specifically, these theorems relate to the project of characterizing graph properties according to the complexity of testing them (in the adjacency matrix representation). The first theorem is that there exist monotone graph properties in /spl Nscr//spl Pscr/ for which testing is very hard (i.e., requires one to examine a constant fraction of the entries in the matrix). The second theorem is that every graph property that can be tested making a number of queries that is independent of the size of the graph, can be so tested by uniformly selecting a set of vertices and accepting iff the induced subgraph has some fixed graph property (which is not necessarily the same as the one being tested). The third theorem refers to the framework of graph partition problems, and is a characterization of the subclass of properties that can be tested using a one-sided error tester, making a number of queries that is independent of the size of the graph.","Testing,
Computer science"
Data collection and restoration for heterogeneous process migration,"This study presents a practical solution for data collection and restoration to migrate a process written in high level stack-based languages such as C and Fortran over a network of heterogeneous computers. We study a logical data model which recognizes complex data structures in process address space. Then, novel methods are developed to incorporate the model into a process and to collect and restore data efficiently. We have implemented a prototype software and performed experiments on different programs. Experimental and analytical results show that (I) a user-level process can be migrated across different computing platforms, (2) semantic information of data structures in the process's memory space can be correctly collected and restored, (3) the costs of data collection and restoration depend on the complexity of the logical model representing the process's data structures and the amount of data involved and (4) the implantation of the data collection and restoration mechanisms into the process is not a decisive factor of incurring execution overheads; with appropriate program analysis, we can achieve practically low overhead.","Data structures,
Computer networks,
Java,
Information analysis,
Sun,
Computer science,
Data models,
Software prototyping,
Software performance,
Costs"
Tradeoff between system profit and user delay/loss in providing near video-on-demand service,"In a near video-on-demand (near-VOD) system, requests for a movie arriving in a period of time are grouped (or ""batched"") together and served with a single multicast stream. We consider providing near-VOD services when there is a cost associated with using a network multicast channel. We address the tradeoff between system profit, given by the total pay-per-view collected minus the total channel cost, and user delay or user loss (due to reneging). We first analyze and compare the tradeoff of two traditional ""basic"" schemes, namely, the window-based schemes in which a maximum user delay can be guaranteed, and the batch-size based scheme in which system profit can be guaranteed. By combining these basic schemes, we present a scheme which can adaptively balance system profit and user delay when the underlying request rate fluctuates. We then consider the case in which delayed users may renege and determine how system profit can be maximized by sizing the batching period given user's reneging behavior. We show that maximizing profit can lead to excessively high user loss rate, especially when the channel cost is high and users are not very patient. Therefore, a shorter suboptimal batching period should be used for this case in reality. We finally introduce schemes which are able to offer high profit or low user loss when the underlying arrival rate fluctuates.",
I-CoPES: fast instruction code placement for embedded systems to improve performance and energy efficiency,"The ratio of cache hits to cache misses in a computer system is, to a large extent, responsible for its characteristics such as energy consumption and performance. In recent years energy efficiency has become one of the dominating design constraints, due to the rapid growth in market share for mobile computing/communication/internet devices. We present a novel fast constructive technique that relocates the instruction code in such a manner into the main memory that the cache is utilized more efficiently. The technique is applied as a re-processing step, i.e. before the code is executed. it is applicable for embedded systems where the number and characteristics of tasks running on the system are known a priori. The technique does not impose any computational overhead to the system. As a result of applying our technique to a variety of real-world applications we measured (through simulation) that the number of cache misses drops significantly. Further, this reduces the energy consumption of a whole system (CPU, caches, buses, main memory) by up to 65% at an only slightly increased memory size of 13% on average.","Embedded system,
Costs,
Energy efficiency,
National electric code,
Computer science,
Power engineering and energy,
Mobile computing,
Energy consumption,
Modems,
Heuristic algorithms"
Mining user access behavior on the WWW,"In this paper, an affinity-based approach that provides good similarity measures for Web document clustering to discover user access behavior on the World Wide Web (WWW) is proposed. The proposed approach generates the similarity measures for groups of Web documents by considering the user access patterns. Any clustering algorithm using better similarity measures should yield better clusters for discovering user access behavior. By utilizing the discovered user access behavior, for example, the companies can precisely target their potential customers and convince them to purchase their products or services in electronic commerce. An experiment on a real data set is conducted and the experimental result shows that the proposed approach yields a better performance than the cosine coefficient and the Euclidean distance method under the partitioning around medoid (PAM) method.","World Wide Web,
Uniform resource locators,
Electronic commerce,
Web sites,
Clustering algorithms,
Multimedia systems,
Laboratories,
Computer science,
Electric variables measurement,
Euclidean distance"
Illumination insensitive eigenspaces,Variations in illumination can have a dramatic effect on the appearance of an object in an image. In this paper we propose how to deal with illumination variations in eigenspace methods. We demonstrate that the eigenimages obtained by a training set under a single illumination condition (ambient light) can be used for recognition of objects taken under different illumination conditions. The major idea is to incorporate a set of gradient based filter banks into the eigenspace recognition framework. This can be achieved since the eigenimage coefficients are invariant for linearly filtered images (input and eigenimages). To achieve further illumination insensitivity we devised a robust procedure for coefficient recovery. The proposed approach has been extensively evaluated on a set of 2160 images and the results were compared to other approaches.,"Lighting,
Nonlinear filters,
Principal component analysis,
Image coding,
Object recognition,
Filtering,
Filter bank,
Image recognition,
Information science,
Image sampling"
Enabling active flow manipulation in silicon-based network forwarding engines,"A significant challenge in today's Internet is the ability to efficiently incorporate customizable network intelligence in commercial high performance network devices. This paper tackles the challenge by introducing the Active Flow Manipulation (AFM) mechanism, a key enabling technology of the programmable networking platform Openet. AFM enhances the control intelligence of network devices through programmability. With AFM, customer network services can exercise active network control by identifying specific flows and applying particular actions thereby altering network behavior in real-time. These services are offered through Openet dynamically deployed in the CPU-based control plane of the network node and are closely coupled with the silicon-based forwarding plane, without negatively impacting forwarding performance. The effectiveness of our approach is demonstrated by four experimental applications on commercial network nodes.","Switches,
Hardware,
Real-time systems,
Routing,
Servers,
IP networks"
On a Class of Support Vector Kernels Based on Frames in Function Hilbert Spaces,"There has been an increasing interest in kernel-based techniques, such as support vector techniques, regularization networks, and gaussian processes. There are inner relationships among those techniques, with the kernel function playing a central role. This article discusses a new class of kernel functions derived from the so-called frames in a function Hilbert space.",
Coordination of collaborative activities: a framework for the definition of tasks interdependencies,"The coordination of interdependencies between tasks in collaborative environments is a very important and difficult endeavour. The separation between tasks and interdependencies allows for the use of different coordination policies in the same collaborative environment by changing only the coordination mechanisms that control the interdependencies. This paper presents a framework for the definition of interdependencies that frequently occur in collaborative activities. By means of a clear characterization of interdependencies, it is possible to identify coordination mechanisms to manage them, opening the way toward a powerful coordination tool capable of encompassing a wide range of collaborative applications. An implementation of the coordination model of a collaborative virtual environment based on the proposed framework is given as example.","Collaboration,
Collaborative work,
Virtual environment,
Production,
Computer industry,
Automation,
Software engineering,
Laboratories,
Computer science,
Energy management"
An interoperable data architecture for data exchange in a biomedical research network,"Knowledge discovery and data correlation require a unified approach to basic data management. However, achieving such an approach is nearly impossible with hundreds of disparate data sources, legacy systems and data formats. This problem is pervasive in the biomedical research community, where data models, taxonomies and data management systems are locally implemented. These local implementations create an environment where interoperability and collaboration between researchers and research institutions are limited. The authors demonstrate how technology developed by NASA's Jet Propulsion Laboratory (JPL) for space science can be used to build an interoperable data architecture for bioinformatics. JPL has taken a novel approach towards solving this problem by exploiting Web technologies usually dedicated to e-commerce, combined with a rich, metadata-based environment. This paper discusses the approach taken to develop a prototype data architecture for the discovery and validation of disease biomarkers within a biomedical research network. Biomarkers are measured parameters of normal biological processes, pathogenic processes or pharmacological responses to a therapeutic intervention. Biomarkers are of growing importance in biomedical research for therapeutic discovery, disease prevention and detection. A bioinformatics infrastructure is crucial to support the integration and analysis of large, complex biological and epidemiological data sets.","Space technology,
Biomarkers,
Bioinformatics,
Diseases,
Knowledge management,
Data models,
Taxonomy,
Collaboration,
Propulsion,
Prototypes"
Nonlinear multiresolution techniques with applications to scientific visualization in a haptic environment,"This paper develops nonlinear multiresolution techniques for scientific visualization utilizing haptic methods. The visualization of data is critical to many areas of scientific pursuit. Scientific visualization is generally accomplished through computer graphic techniques. Recent advances in haptic technologies allow visual techniques to be augmented with haptic methods. The kinesthetic feedback provided through haptic techniques provides a second modality for visualization and allows for active exploration. Moreover, haptic methods can be utilized by individuals with visual impairments. Haptic representations of large data sets, however, can be confusing to a user, especially if a visual representation is not available or cannot be used. This paper develops a multiresolution data decomposition method based on the affine median filter. This results in a hybrid structure that can be tuned to yield a decomposition that varies from a linear wavelet decomposition to that produced by the median filter. The performance of this hybrid structure is analyzed utilizing deterministic signals and statistically in the frequency domain. This analysis and qualitative and quantitative implementation results show that the affine median decomposition has advantages over previously proposed methods. In addition to multiresolution decomposition development, analysis, and results, haptic implementation methods are presented.",
Accuracy assessment of layer decomposition using simulated angiographic image sequences,"Layer decomposition is a promising method for obtaining accurate densitometric profiles of diseased coronary artery segments. This method decomposes coronary angiographic image sequences into moving densitometric layers undergoing translation, rotation, and scaling. In order to evaluate the accuracy of this technique, the authors have developed a technique for embedding realistic simulated moving stenotic arteries in real clinical coronary angiograms. They evaluate the accuracy of layer decomposition in two ways. First, they compute tracking errors as the distance between the true and estimated motion of a reference point in the arterial lesion. The authors find that noise-weighted phase correlation and layered background subtraction are superior to cross correlation and fixed mask subtraction, respectively. Second, they compute the correlation coefficient between the true vessel profile and the raw and processed images in the region of the stenosis. They find that layer decomposition significantly improves the correlation coefficient.","Image sequences,
Background noise,
Biomedical imaging,
Medical diagnostic imaging,
Arteries,
Biomedical measurements,
Computational modeling,
Clinical trials,
Angiography,
Quantum cellular automata"
A logic for modeling the dynamics of beliefs in cryptographic protocols,"We present a logic of modeling the dynamics of beliefs in cryptographic protocols. Differently from previous proposals, our logic is situation based, in which a protocol is viewed as a finite sequence of actions performed by various principals at different situations, and each action is a primitive term in the language. Therefore, it becomes possible to model the dynamic change of each principal's beliefs at each step of the protocol within the logic system. Our logic has a precise semantics and is sound with respect to the underlying automatic system.","Logic,
Cryptographic protocols,
Authentication,
Body sensor networks,
Information technology,
Australia,
Electronic mail,
Information security,
Cryptography,
Information analysis"
Beyond stereotyping: metamodeling approaches for the UML,"UML is being used as the universal technique for modeling object-oriented applications across a wide range of domains. Developing a truly adequate uniform modeling technique in the face of these diverse domains seems an unsolvable quest and contrasts domain specific software engineering activities. Recently, many adaptations to UML have been made to reflect a domain's world view. These adaptations often exceed the UML's own extension mechanisms and result in yet another urban UML slang. However, domain-specifically adapting the UML metamodel becomes increasingly important in the context of model checking and code generation mechanisms. Therefore solutions should be found to fully support metamodeling within the UML and UML CASE tools. The paper discusses and evaluates the UML's inherent as well as proprietary metamodeling approaches and provides domain driven ideas for a meta-modeling approach for a diversely used Unified Modeling Language.","Metamodeling,
Unified modeling language,
Object oriented modeling,
Software engineering,
Application software,
Computer science,
Context modeling,
Computer aided software engineering,
Power system modeling,
Engineering management"
Parsing strategies for BWT compression,"Block-sorting is an innovative compression mechanism introduced by Burrows and Wheeler (1994), and has been the subject of considerable scrutiny in the years since it first became public. Block-sorting compression is usually described as involving three steps: permuting the input one block at a time through the use of the Burrows-Wheeler transform (BWT); applying a move-to-front (MTF) transform to each of the permuted blocks; and then entropy coding the output with a Huffman or arithmetic coder. In this paper we prepend a fourth transformation to this sequence: parsing. In the BWT implementations that have been considered to date the unit of transmission has been taken to be the ASCII character. But there is no particular reason why this should be so, and a range of other strategies can be used to construct the sequence of symbols that is fed into the BWT process. We consider some of the issues associated with making this change, and show that in some situations the introduction of a simple parsing stage allows improved compression to be obtained compared to an otherwise equivalent character-based BWT implementation. We also describe an MTF-like ranking transformation that caters better to large-alphabet situations than does the strict MTF rule used in conventional BWT implementations.","Computer science,
Software engineering,
World Wide Web,
Data compression,
Sorting,
Entropy coding,
Arithmetic,
Decoding,
Australia Council,
Dictionaries"
Data analysis and mining in ordered information tables,"Many real-world problems deal with ordering objects instead of classifying objects, although the majority of the research in machine learning and data mining has been focused on the latter. For the modeling of ordering problems, we generalize the notion of information tables to ordered information tables by adding order relations on attribute values. The problem of mining ordering rules is formulated as finding associations between the orderings of attribute values and the overall ordering of objects. An ordering rule may state, for example, that ""if the value of an object x on an attribute a is ordered ahead of the value of another object y on the same attribute, then x is ordered ahead of y"". For mining ordering rules, we first transform an ordered information table into binary information, and then apply any standard machine learning and data mining algorithms. As an illustration, we analyze in detail the Maclean's university ranking for the year 2000.","Data analysis,
Data mining,
Machine learning,
Machine learning algorithms,
Consumer products,
Manufacturing,
Computer science,
Electronic mail,
Warranties,
Rough sets"
On-line scheduling of real-time distributed computers with complex communication constraints,"We consider the scheduling of periodic tasks running on distributed computers. Every execution of a task must meet its deadline. Response time analysis of the tasks is used to prove the schedulability of hard real-time distributed systems according to the on-line priority rules that schedule the processors and the network. Its main advantage is to take into account the precedence dependencies of the schedules of the tasks on the processors and the messages sent on the network(s). Past works have addressed the issue of tasks related by asynchronous communication constraints with the senders and the receivers working at the same rate. We study more general relations among tasks when the rates of dependent tasks are not equal. We call such relations generalized communication constraints. Usually distributed systems are scheduled using a synchronization protocol and an on-line scheduling algorithm by processor. We present a graph theoretical approach to this schedulability analysis. Our algorithm transforms complex communication relations into classical ones, so that the classical scheduling analysis can be fully applied. That transformation is independent of the architecture of the distributed systems and no assumption is made on the synchronization protocol considered.",
Binary decision diagram with minimum expected path length,We present methods to generate a Binary Decision Diagram (BDD) with minimum expected path length. A BDD is a generic data structure which is widely used in several fields. One important application is the representation of Boolean functions. A BDD representation enables us to evaluate a Boolean function: Simply traverse the BDD from the root node to the terminal node and retrieve the value in the terminal node. For a BDD with minimum expected path length will be also minimized the evaluation time for the corresponding Boolean function. Three efficient algorithms for constructing BDDs with minimum expected path length are proposed.,
An efficient recovery mechanism for MPLS-based protection LSP,"Network reliability and survivability have been very important issues to provide for application services that may require a real-time service or high priority QoS (quality of service) in the Internet. IETF has proposed largely two recovery mechanisms for MPLS-based protection label switching path (LSP), which are protection switching and rerouting models. However, from the viewpoint of TE (traffic engineering), IETF's recovery mechanisms have not considered the optimal backup path for the recovery of an LSP in the occurrence of a network failure. This paper suggests an efficient pre-qualified recovery mechanism, which optimizes the network performance by considering link usage. Since an existing recovery mechanism, pre-qualified rerouting, selects a backup path only once at the LSP setup time, it may not reflect the exact status of network resources at the time of a fault. In contrast, our approach exchanges network status information among LSRs so that the backup path selection engine can use up-to-date information and decide an optimal backup path for a possible failure. The performance of the proposed recovery method has been demonstrated by simulation using MNS (MPLS network simulator). The new proposed recovery mechanism can always maintain an optimized network state regardless of the fault occurrence.","Telecommunication traffic,
Multiprotocol label switching,
Protection switching,
IP networks,
Traffic control,
Web and internet services,
Switches,
Computer science,
Electronic mail,
Computer network reliability"
Lock-free scheduling of logical processes in parallel simulation,"With fixed lookahead information in a simulation model, the overhead of asynchronous conservative parallel simulation lies in the mechanism used for propagating time updates in order for logical processes to safely advance their local simulation clocks. Studies have shown that a good scheduling algorithm should preferentially schedule processes containing events on the critical path. This paper introduces a lock-free algorithm for scheduling logical processes in conservative parallel discrete-event simulation on shared-memory multiprocessor machines. The algorithm uses fetch and add operations that help avoid inefficiencies associated with using locks. The lock-free algorithm is robust. Experiments show that, compared with the scheduling algorithm using locks, the lock-free algorithm exhibits better performance when the number of logical processes assigned to each processor is small or when the workload becomes significant. In models with large number of logical processes, our algorithm shows only modest increase in execution time due to the overhead in the algorithm for extra bookkeeping.","Discrete event simulation,
Scheduling algorithm,
Computational modeling,
Processor scheduling,
Computer simulation,
Clocks,
Contracts,
Out of order,
Error correction,
Computer science"
Supporting scientific analysis within collaborative problem solving environments,"This paper first lists some features of scientific analysis tools that are important for effective analysis in CPSEs (collaborative problem solving environments). Next, design criteria for achieving these features are presented. Then requirements for a CPSE architecture to support these design criteria are listed. Some proposed architectures for CPSEs are reviewed and their capabilities to support these design criteria are discussed. The most popular architecture for remote application sharing, the ITU (International Telecommunication Union) T.120 architecture, does not support highly interactive, dynamic, high resolution graphics. A popular scientific analysis tool that conforms to the design criteria has been integrated into a collaborative environment and tested for effectiveness. The tests showed that the tool was highly effective for both synchronous and asynchronous collaborative analyses.","Collaborative work,
Computer graphics,
Performance analysis,
Testing,
Computer architecture,
Collaborative tools,
NASA,
Tellurium,
Video sharing,
Computer displays"
Various ways academics teach simulation: are they all appropriate?,,
An analysis of the gap between the knowledge and skills learned in academic software engineering course projects and those required in real: projects,"This paper describes how the Software Engineering Body of Knowledge, (SWEBOK) can be used as a guide to assess and improve software engineering courses. A case study is presented in which the guide is applied to a typical undergraduate software engineering course. The lessons learned are presented which the authors believe are generalizable to comparable courses taught at many academic institutions. A novel approach involving largescale software project simulation is also presented a way to overcome some of the course deficiencies identified by the guide.","Software engineering,
Knowledge engineering,
Collaborative software,
Taxonomy,
Computational modeling,
Software quality,
Computer science,
Computer science education,
Collaborative work,
Professional societies"
"Why co-evolution beats temporal difference learning at Backgammon for a linear architecture, but not a non-linear architecture","No Free Lunch theorems show that the algorithm must suit the problem. This does not answer the novice's question: for a given problem, which algorithm to use? This paper compares co-evolutionary learning and temporal difference learning on the game of Backgammon, which (like many real-world tasks) has an element of random uncertainty. Unfortunately, to fully evaluate a single strategy using undirected sampling of board positions, using only random dice rolls, requires a great deal of computation. Evolution's all-or-nothing replacement of entire solutions needs accurate evaluation, but relatively rare board positions are needed to train above a certain level. Temporal difference learning, with its incremental changes, does not use such an all-or-nothing approach. These results have relevance to a variety of real-world tasks with uncertainty, such as schedule optimization.","Uncertainty,
Sampling methods,
Computer architecture,
Optimal scheduling,
Law,
Legal factors,
Neural networks,
Cognitive science,
Computer science,
Scheduling algorithm"
Performance of adaptive routing strategies in wavelength-routed networks,"As the trend of increased reconfigurability emerges in wavelength division multiplexing (WDM) networks, lightpath establishment would be moved from static toward dynamic regimes. Therefore our interest is in real-time traffic where connection requests arrive dynamically and last for a random holding time if granted. We are concerned with the performance of adaptive routing algorithms in wavelength-routed networks with wavelength conversion capability. We first investigate several adaptive routing strategies, including the shortest path strategy, the least-loaded path strategy, and the proposed weighted-shortest path strategy . We then develop an analytical model to estimate blocking performance and compare the analyses with simulations. The numerical results show that the weighted-shortest path strategy can enhance blocking performance and fairly distribute load among the links.","Wavelength routing,
Intelligent networks,
Optical wavelength conversion,
Analytical models,
Computer science,
Wavelength division multiplexing,
WDM networks,
Spine,
Centralized control,
Computational modeling"
An Overview of the COVEN Platform,"A central aim of the COVEN project was to prototype large-scale applications of collaborative virtual environments (CVEs) that went beyond the existing state of the art. These applications were used in a series of real-scale networked trials that allowed us to gather many interesting human and technological results. To fulfill the technological and experimental goals of the project, we have modified an existing CVE platform: the DIVE (distributed interactive virtual environment) toolkit. In this paper, we present the different services and extensions that have been implemented within the platform during the four years of the project. Such a presentation will exemplify the different features that will have to be offered by nextgeneration CVE platforms. Implementation of the COVEN services has had implications at all levels of the platform: from a new networking layer through to mechanisms for high-level semantic modeling of applications.",
A technique for mutation of Java objects,"Mutation analysis inserts faults into a program to create test sets that distinguish the mutant from the original program. Inserted faults must represent plausible errors. Standard transformations can mutate scalar values such as integers, floats, and character data. Mutating objects is an open problem, because object semantics are defined by the programmer and can vary widely. We develop mutation operators and support tools that can mutate Java library items that are heavily used in commercial software. Our mutation engine can support reusable libraries of mutation components to inject faults into objects that instantiate items from these common Java libraries. Our technique should be effective for evaluating real-world software testing suites.","Genetic mutations,
Java,
Testing,
Programming profession,
Software libraries,
Runtime,
Object oriented modeling,
Computer science,
Computer errors,
Software tools"
Fitting nature's basic functions. II. Estimating uncertainties and testing hypotheses,"For pt.I see ibid., previous issue. In the last issue we considered a linear statistical model. The development was motivated by global annual average temperature data which were plotted as discrete circles. The two curves are the best-fitting first- and fifth-degree polynomials. The fifth-degree polynomial tracks the data better, but we need more statistical analysis to determine whether the improvement obtained justifies the addition of four new free parameters. This is one of the questions that we address in this installment. We discuss simple diagnostics for the fit, uncertainties in the estimates, estimate correlations, assigning confidence levels, testing hypotheses, and a time series diagnostic.","Uncertainty,
Testing,
Equations,
Polynomials,
Algorithms,
Temperature,
Statistical analysis,
Mathematical model,
Matrices"
A reflective component-based and architecture aware framework to manage architecture composition,"Large scale distributed systems are typically evolving environments that have to deal with interoperability, scalability, mobility and QoS adaptability requirements. Generically, these systems need adaptation mechanisms to cope with short-term (cf. programmed reconfiguration) and long-term requirements (cf. evolutionary reconfiguration). We propose a reflective component-based framework with architecture style awareness for managing architecture composition and constraining adaptation. Specifically, this framework provides the necessary tools to generate and manipulate the programming model abstractions (i.e. components, connectors and respective properties and interfaces). The framework offers a principled way to deal with both introspection and adaptation of basic and composite components. It provides the developers with the ability to choose, extend and modify architecture style managers. These managers are responsible to represent and check architecture constraints both at development and deployment time, i.e. before any architectural reconfiguration may be committed.","Computer architecture,
Connectors,
Computer science,
Software reusability,
Software architecture,
Software systems,
Distributed computing,
Electromagnetic compatibility,
Large-scale systems,
Scalability"
"A unified memory based approach to cut, dissolve, key frame and scene analysis","We review a memory-based buffer model of visual perception, that combines the lower and middle stages in the analysis of video. This model was originally developed for the detection of breaks between physical scene changes (""story units""). We show how this method can also be applied for shot detection. Moreover, it gives rise to a unified approach for detecting cuts and gradual changes. Additionally, as a straightforward corollary to its definition, it leads to a more natural definition of key frames. We derive the theoretic performance of the model, given arbitrary measures of frame-to-frame dissimilarity. We show several examples of its response, using standard color histogram differencing (with the L/sub 1/ norm as measure). We evaluate the model's performance in detecting cuts and dissolves against a complete hand-segmented situation comedy.","Image analysis,
Gunshot detection systems,
Layout,
Histograms,
Computer science,
Electronic mail,
Measurement standards,
Psychology,
Aggregates,
Time measurement"
Peak-to-average power ratio reduction of an OFDM signal using data permutation with embedded side information,The use of a set of fixed permutations has recently been proposed for peak-to-average power ratio (PAR) reduction of an OFDM signal. Sending the identity of the used permutation to the receiver reliably is critical in this method. This paper presents the use of data permutation with embedded side information (SI) to reduce the PAR of an OFDM signal. SI is coded using a simple forward-error-correction code and inserted into the information sequence. The PARs of the permuted sequences and the original information sequence are then computed using IDFTs. The sequence with the lowest PAR is chosen for the transmission. The PAR statistics do not degrade due to the inclusion of SI. This paper also presents theoretical expressions for the complementary cumulative density function (CCDF) of the PAR of an interleaved OFDM (IOFDM) signal and for the average number of interleavers in the adaptive interleaving approach.,"Peak to average power ratio,
OFDM,
Partial transmit sequences,
Interleaved codes,
Degradation,
Wireless LAN,
Discrete Fourier transforms,
Interference,
Computer science,
Software engineering"
Towards electronic contract performance,"An increasing volume of research in e-commerce is concerned with the development of tools and environments to support various aspects of business-to-business electronic contract formation and performance. This paper is mainly concerned with the latter and takes up the suggestion that automated execution of an agreement between (at least) two parties can be effected through a central control mechanism (a so-called e-marketplace). We revisit modal action logic to model an agreement as a state-based system and specify acceptable and unacceptable states of a business transaction. Unacceptable states result from violations of contractual obligations or prohibitions and call for appropriate recovery mechanisms to be specified, so that they can be enforced by the central control mechanism. We comment on the relations between contract violations and the concepts of fault tolerance and recovery arising in the broader distributed systems context, on the one hand, and contrary-to-duty structures from the (theoretical) deontic logic perspective, on the other.","Contracts,
Logic,
Consumer electronics,
Law,
Legal factors,
Centralized control,
Fault tolerant systems,
Monitoring,
Computer science,
Educational institutions"
Effect of speculative prefetching on network load in distributed systems,"Previous studies in speculative prefetching focus on building and evaluating access models for the purpose of access prediction. This paper on the other hand investigates the performance of speculative prefetching. When prefetching is performed speculatively, there is bound to be an increase in the network load. Furthermore, the prefetched items must compete for space with existing cache occupants. These two factors-increased load and eviction of potentially useful cache entries-are considered in the analysis. We obtain the following conclusion: to maximise the improvement in access time, prefetch exclusively all items with access probabilities exceeding a certain threshold.","Prefetching,
Intelligent networks,
Predictive models,
Bandwidth,
Performance analysis,
Computer science,
Australia,
Time measurement,
Delay,
Degradation"
SMART: a scalable middleware solution for ubiquitous multimedia service delivery,,"Middleware,
Ubiquitous computing,
Multimedia computing,
Application software,
Pervasive computing,
Computer science,
Internet,
Computer networks,
Physics computing,
Home appliances"
Constructions for perfect 2-burst-correcting codes,"In this correspondence, we present two constructions. In the first construction, we show how to generate perfect linear codes of length 2/sup r-1/, r/spl ges/5, and redundancy r, which correct a single burst of length 2. In the second construction, we show how to generate perfect linear codes organized in bytes of length 2/sup r-1/, r/spl ges/5, with redundancy divisible by r, which correct a single burst of length 2 within the bytes.",Linear codes
"Linear codes with covering radius R=2, 3 and codimension tR","Let [n,n-r]/sub q/R denote a linear code over F/sub q/ with length n, codimension r, and covering radius R. We use a modification of constructions of [2q+1, 2q-3]/sub q/2 and [3q+1, 3q-5]/sub q/3 codes (q/spl ges/5) to produce infinite families of good codes with covering radius 2 and 3 and codimension tR.",
On area-efficient low power array multipliers,"Multiplication is one of the most critical operations in many computational systems. In this paper, we present an improved architecture for a multiplexer-based multiplication algorithm. Also through intensive HSPICE simulation, it has been shown in this paper that due to smaller internal capacitance, the multiplexer-based array multiplier outperforms the modified Booth multiplier in both speed and power dissipation by 13% to 26%. In addition, we demonstrate that using area-efficient full adder circuits (SERF and 10T) can help reduce the overall routing capacitance, resulting in less power consumption for multipliers built upon those adder circuits. Therefore, a multiplexer-based multiplier following the suggested architecture, along with area-efficient full adder circuits, can be used for low power high performance parallel multiplier designs.","Adders,
Signal processing algorithms,
Energy consumption,
Digital signal processing,
Circuit simulation,
Capacitance,
Digital signal processors,
Throughput,
Algorithm design and analysis,
Computer science"
New geometric concepts in fuzzy-ART and fuzzy-ARTMAP: category regions,"We introduce new geometric concepts regarding categories in fuzzy ART (FA) and fuzzy ARTMAP (FAM), which add a geometric facet to the process of node selection in the F/sub 2/ layer by patterns. Apart from providing the means to better understand the training and performance phase of these two architectures, the new concepts, namely the category regions, lead us to interesting theoretical results, when training either architecture. First, we define the commitment test as a novelty detection mechanism similar to the vigilance test. Next, we define various category regions. Through those definitions and 3 derived lemmas we identify areas in the vigilance-choice parameter space, for which 4 results are stated that are applicable to both FA and the FAM classifier training phase.","Testing,
Computer science,
Subspace constraints,
Neural networks,
Resonance,
Unsupervised learning,
Supervised learning,
Organizing,
Computer vision,
Computer architecture"
JMangler - a framework for load-time transformation of Java class files,"Current proposals for load-time transformation of Java classes are either dependent on the use of a specific class loader or dependent on a specific JVM implementation. This is not due to an inadequacy of the Java platform but to the wrong choice of the level at which to hook into the Java Class Loader Architecture. JMangler follows a novel approach that ensures both class loader and JVM independence by hooking into the base class of all class loaders. Furthermore, existing proposals do not allow transformers to be treated as components because implicit dependencies must be resolved manually. The paper shows that automatic composition is possible for the well-defined class of interface transformations that still include powerful transformations, like addition of fields, methods and classes, and changes to the class hierarchy. Consequently, interface transformers can be deployed jointly even if developed independently.","Java,
Proposals,
Computer science,
Legislation,
Programming,
Information analysis,
Runtime,
Reflection,
Joining processes"
Comparison of algorithms for combining X-ray angiography images,"Using a one-dimensional convective-dispersive model of contrast agent flow in a blood vessel, the authors optimized and compared algorithms for combining a temporal sequence of X-ray angiography images, each with incomplete arterial filling, into a single-output image with fully opacified arteries. The four algorithms were: maximum opacity (MO) with a maximum over time at each spatial location; matched filtering (MAT); recursive filtering (REC) with a maximum opacity; and an approximate matched filter (AMF) consisting of a correlation with a kernel that approximates the matched filter kernel followed by a maximum opacity operation. Based on the contrast-to-noise ratio (CNR), MAT is theoretically the best algorithm. However, with spatially varying clinical images, a poorly matched MAT kernel greatly degraded CNR to the point of even inverting artery contrast. The practical AMF method maintained uniform CNR values over the entire field of view and gave >90% of the theoretical limit set by MAT. REC and MO created fully opacified arteries, but provided little CNR enhancement. By holding CNR at a nominal reference value, simulations predicted that AMF could be used with a contrast agent volume reduced by as much as 66%. Alternatively, X-ray exposure rate could be lowered. Although MO and REC are more easily implemented, the contrast enhancement with AMF makes it attractive for processing diagnostic angiography images acquired with a reduced contrast agent dose.","X-ray imaging,
Angiography,
Matched filters,
Arteries,
Kernel,
Blood vessels,
Biomedical imaging,
Filling,
Filtering algorithms,
Degradation"
Spectral techniques in binary and multiple-valued switching theory. A review of results in the decade 1991-2000,This paper presents a tutorial review of spectral methods in switching and multiple-valued logic theory and the design of digital system in the last decade. The paper continues review work in this area done by the authors in 1981 and 1991.,"Logic design,
Computer science,
Design engineering,
Application software,
Signal processing,
Digital systems,
Harmonic analysis,
Electrical engineering computing,
History,
Mathematics"
Efficient computation of adaptive threshold surfaces for image binarization,"The problem of binarization of gray level images acquired under nonuniform illumination is reconsidered. Yanowitz and Bruckstein (1989) proposed to use an adaptive threshold surface, determined by interpolation of the image gray levels at points where the image gradient is high. The rationale is that a high image gradient indicates probable object edges, and there the image values are between the object and background gray levels. The threshold surface was determined by successive overrelaxation as the solution of the Laplace equation. This work proposes a different method to determine an adaptive threshold surface. In this new method, inspired by multiresolution approximation, the threshold surface is constructed with considerably lower computational complexity and is smooth, yielding faster image binarizations and better visual performance.","Laplace equations,
Lighting,
Computational complexity,
Pixel,
Computer science,
Interpolation,
Relaxation methods,
Degradation,
Surface waves"
Real-time on-line network simulation,,"Computational modeling,
Telecommunication traffic,
Traffic control,
Computer science,
Economic indicators,
Computer simulation,
Delay,
Ad hoc networks,
Protocols,
Computer networks"
A hierarchical recurrent neuro-fuzzy system,"Fuzzy systems, neural networks and their combination in neuro-fuzzy systems are already well established in data analysis and system control. Especially, neuro-fuzzy systems are well suited for the development of interactive data analysis tools, which enable the creation of rule-based knowledge from data and the introduction of a-priori knowledge into the process of data analysis. However, its recurrent variants-especially recurrent neuro-fuzzy models-are still rarely used. In this article a (hybrid) recurrent neuro-fuzzy model is presented which is designed for application in time series prediction and identification of dynamic systems. It has been implemented in a tool for the interactive design of hierarchical recurrent fuzzy systems.","Fuzzy neural networks,
Fuzzy sets,
Fuzzy systems,
Gaussian processes,
Data analysis,
Neural networks,
Feedforward systems,
Logistics,
Learning systems,
Computer science"
Using clustering to discover the preferences of computer criminals,The ability to predict computer crimes has become increasingly important. The paper describes a method for discovering the preferences of computer criminals. This method involves sequential clustering based on the variance of clusters discovered in higher order clustering. These discovered preferences can be used for the direct protection of computer systems against ongoing attacks or for the construction of simulations of future attacks.,"Computer crime,
Predictive models,
Protection,
Law enforcement,
Business,
Internet,
Utility theory,
Computational modeling,
Computer simulation,
Data mining"
An educational genetic algorithms learning tool,"During the last thirty years, there has been a rapidly growing interest in a field called genetic algorithms (GAs). The field is at a stage of tremendous growth as evidenced by the increasing number of conferences, workshops and papers concerning it, as well as the emergence of a central journal for the field. With their great robustness, genetic algorithms have proven to be a promising technique for many optimization, design, control, and machine learning applications. Students who take a GAs course study and implement a wide range of difference techniques of GAs. And practical implementation experience plays a very important role in learning computer relative courses. Herein, an educational genetic algorithm learning tool (EGALT) has been developed to help students facilitate GAs course. With the readily available tool students can reduce the mechanical programming aspect of learning and concentrate on principles alone. A friendly graphic user interface was established to help students operate and control not only the structural identification but also the parametric identification of GAs. It outlines how to implemented genetic algorithms, how to set parameters of different kinds of problems, and recommends a set of genetic algorithms, which were suggested in previous studies.",Computer science education
Towards a formalization of constraint diagrams,"Geared to complement UML and to the specification of large software systems by non-mathematicians, constraint diagrams are a visual language that generalizes the popular and intuitive Venn diagrams and Euler circles, and adds facilities for quantifying over elements and navigating relations. The language design emphasizes scalability and expressiveness while retaining intuitiveness. Spider diagrams form a subset of the notation, leaving out universal quantification and the ability to navigate relations. Spider diagrams have been given a formal definition. This paper extends that definition to encompass the constraint diagram notation. The formalization of constraint diagrams is nontrivial: it exposes subtleties concerned with the implicit ordering of symbols in the visual language, which were not evident before a formal definition of the language was attempted. This has led to an improved design of the language.","Unified modeling language,
Navigation,
Data visualization,
Gas insulated transmission lines,
Computer science,
Mathematics,
Laboratories,
Software systems,
Scalability,
Acceleration"
3D rectilinear motion planning with minimum bend paths,"Computing rectilinear shortest paths in two dimensions has been solved optimally using a number of different techniques. A variety of related problems have been solved, including minimizing the number of bends in the path, the total rectilinear distance, or some combination of both. However, solutions to the 3D versions of these problems are less common. We propose a solution to the 3D minimum-bend path problem, which has theoretical as well as practical interest. Applications include motion planning problems where straight line motion is preferred over taking arbitrary turns. We employ our results in motion planning for self-repair in self-reconfigurable robots.","Path planning,
Motion planning,
Robots,
Shortest path problem,
Automatic control,
Motion control,
Computer science,
Educational institutions,
Robotics and automation,
Machining"
Two case studies in predictable application scheduling using Rialto/NT,"This paper analyzes the results of two case studies in applying the Rialto/NT scheduler to real Windows 2000 applications. The first study is of a soft modem-a modem whose signal processing work is performed on the host CPU, rather than on a dedicated signal processing chip. The second is of an audio player application. Both of these are frequently used real-time applications-ones running on systems that were not designed to support predictable real-time execution. To function correctly, both applications require that ongoing computations be performed in a timely manner. In both cases, we first measured an original version designed to run on Windows 2000, and then modified the application to take advantage of ongoing CPU reservations provided by the Rialto/NT scheduler. We report on the benefits and problems observed when using reservations in these real-world scenarios. In both cases, we found that a real-time scheduler can provide the needed predictability for the application in the presence of competing applications, while also providing other benefits, such as minimizing the soft modem's impact on the scheduling predictability of other computations in the system. We also describe the methodologies we used to analyze the real-time behavior of the operating system and applications during these studies, including the use of instrumented kernels to produce execution traces.","Computer aided software engineering,
Real time systems,
Processor scheduling,
Modems,
Operating systems,
Yarn,
Computer science,
Signal processing,
Application software,
Semiconductor device measurement"
Support for speculative update propagation and mobility in Deno,"This paper presents the replication framework of Deno, an object replication system specifically designed for mobile and weakly-connected environments. Deno uses weighted voting for availability and pair-wise, epidemic information flow for flexibility. This combination allows the protocols to operate with less than full connectivity, to easily adapt to changes in group membership, and to make few assumptions about the underlying network topology. Deno has been implemented and runs on top of Linux and Win32 platforms. We use the Deno prototype to characterize the performance of two versions of Deno's protocol. The first version enables globally serializable execution of update transactions. The second supports a weaker consistency level that still guarantees transactionally-consistent access to replicated data. We demonstrate that the incremental cost of providing global serializability is low, and that speculative dissemination of updates can significantly improve commit performance.",
Towards dynamic meta modeling of UML extensions: an extensible semantics for UML sequence diagrams,"The unified modeling language (UML) still lacks a formal and commonly agreed specification of its semantics that also accounts for UML's built-in semantic variation points and extension mechanisms. The semantic specification of such extensions must be formally integrated and consistent with the standard UML semantics without changing the latter. Feasible semantic approaches must thus allow advanced UML modelers to define domain-specific language extensions in a precise, yet usable manner. We proposed dynamic meta modeling for specifying operational semantics of UML behavioral diagrams based on UML collaboration diagrams that are interpreted as graph transformation rules. Herein we show how this approach can be advanced to specify the semantics of UML extensions. As a case study we specify the operational semantics of UML sequence diagrams and extend this specification to include features for modeling multimedia applications.","Unified modeling language,
Object oriented modeling,
Mathematics,
Computer science,
Application software,
Software standards,
Software systems,
Real time systems,
Stress,
Collaboration"
Image interpolation using across-scale pixel correlation,"A novel method is proposed for image interpolation. It is assumed that the pixel correlation between local regions across scales would remain similar. In addition, this a priori similarity could be extracted from a set of available image data that have the same content but different resolutions. A simple architecture is devised to estimate the correlation efficiently, which is then used to predict the unknown pixel values in a high-resolution image. Evaluation shows a promising performance of the proposed algorithm.","Interpolation,
Pixel,
Image resolution,
Data mining,
Image storage,
Layout,
Kernel,
Computer science,
Software engineering,
Australia"
Modeling crossover-induced linkage in genetic algorithms,"The dynamics of a genetic algorithm undergoing ranking selection, mutation, and two-point crossover for the ones-counting problem is studied using a statistical mechanics approach. This approach has been used previously to study this problem, but with uniform crossover. Two-point crossover induces additional linkage between nearby loci, which changes the dynamics significantly. To account for this linkage, the evolution of the autocorrelation function is incorporated into a model of the dynamics. This complicates the analysis and requires several additional approximations to be made. However, the model we derive is shown to capture the main features of the dynamics and is in good agreement with simulations.","Couplings,
Genetic algorithms,
Genetic mutations,
Autocorrelation,
Evolutionary computation,
Biological cells,
Computer science"
Pairwise coupling for machine recognition of hand-printed Japanese characters,"Machine recognition of hand-printed Japanese characters has been an area of great interest for many years. A major problem of this classification task is the huge number of different characters. Applying standard ""state-of-the-art"" techniques, such as SVM, to multi-class problems of this kind imposes severe problems of both a conceptual and technical nature: (i) separating one class from all others may be an unnecessarily hard problem; and (ii) solving these subproblems can impose unacceptably high computational costs. In this paper, a new approach to Japanese character recognition is presented that successfully overcomes these shortcomings. It is based on a pairwise coupling procedure for probabilistic two-class kernel classifiers. Experimental results for Hiragana recognition effectively demonstrate that our method attains an excellent level of prediction accuracy while imposing very low computational costs.","Character recognition,
Kernel,
Support vector machines,
Computational efficiency,
Costs,
Biological system modeling,
Large-scale systems,
Computer science,
Accuracy,
Computational biology"
TRAM: a tool for requirements and architecture management,"Management of system requirements and system architectures is part of any software engineering project. But it is usually very tedious and error prone. In particular, managing the traceability between system requirements and system architectures is critical but difficult. The article introduces a tool, TRAM, for managing system requirements, system architectures and more importantly the traceability between them. Its primary design objective is ""being practical"" and ready for practitioners to use without much overhead. The issues discussed in the paper include an information model that underlies the capture of requirements, architectures and their traceability, a set of document templates implementing the information model, and the support tool.","Computer architecture,
Project management,
Engineering management,
Computer networks,
Australia,
Computer network management,
Software engineering,
Machine vision,
System testing,
System analysis and design"
Simulation tools for digital design and computer organization and architecture,"An ever-increasing number of software simulation tools are available for many topics covered in undergraduate engineering programs. Most are strong in terms of computation, allowing students to design and simulate system behavior. However, many of these tools do not help students visualize system behavior and understand why things are designed the way they are. This paper presents the methodology used to design several web-based simulation tools for undergraduate topics in digital design and computer organization and architecture. It discusses the design criteria common to all projects, as well as such logistical items as development tools and student support. This paper also discusses simulation tools for a computer system, a microprocessor, and a programmable logic device, which were developed using this methodology. The executable files and source code for these three simulators are available without cost under the GNU Public License.","Computational modeling,
Computer simulation,
Software tools,
Visualization,
Design methodology,
Computer architecture,
Microprocessors,
Programmable logic devices,
Costs,
Licenses"
Zone content classification and its performance evaluation,"We present an improved zone content classification method and its performance evaluation. We added two new features to the feature vector from one previously published method (Sivaramakrishnan et al., 1995). We assumed different independence relationships in two zone sets. We used an optimized binary decision tree to estimate the maximum zone content class probability in one set while using the Viterbi algorithm to find the optimal solution for a zone sequence in the other set. The training, pruning and testing data set for the algorithm include 1,600 images drawn from the UWCDROM III document image database. The classifier is able to classify each given scientific and technical document zone into one of the nine classes, 2 text classes (of font size 4 - 18pt and font size 19 - 32 pt), math, table, halftone, map/drawing, ruling, logo, and others. Compared with our previous work (Wang et al., 2000), it raised the accuracy rate to 98.52% from 97.53% and reduced the mean false alarm rate to 0.53% from 1.26%.","Decision trees,
Image databases,
Hidden Markov models,
Computer science,
Educational institutions,
Viterbi algorithm,
Testing,
Equations,
Classification tree analysis,
Optical character recognition software"
The inside story on shared libraries and dynamic loading,"Traditionally, developers have built software as stand-alone applications written in a single language such as Fortran, C, or C++. However, many scientists are starting to build their applications as extensions to scripting language interpreters or component frameworks. This often involves shared libraries and dynamically loadable modules. However, the inner workings of shared libraries and dynamic loading are some of the least understood and most mysterious areas of software development. We tour the inner workings of linkers, shared libraries, and dynamically loadable extension modules. Rather than simply providing a tutorial on creating shared libraries on different platforms, we want to provide an overview of how shared libraries work and how to use them to build extensible systems. For illustration, we use a few examples in C/C++ using the gcc compiler on GNU-Linux-i386. However, the concepts generally apply to other programming languages and operating systems.","Software libraries,
Joining processes,
Concatenated codes,
Application software,
Dynamic programming,
Functional programming,
Software maintenance,
Resource management"
Satisfying timing constraints of preemptive real-time tasks through task layout technique,"In multi-tasking preemptive real-time systems, a tighter estimate of the Worst Case Response Time (WCRT)s of the tasks can be obtained if the layout of the tasks is taken into account in the analysis. This is because the Cache Related Preemption Delay (CRPD) depends on the inter-task interference in the cache. We present an ILP formulation and an algorithm for generating a layout of the tasks such that the timing constraints of all the tasks are met. An attempt is made to generate a layout such that the CRPD is reduced for all the tasks. The performance of the proposed formulation is demonstrated.","Timing,
Delay,
Costs,
Interference,
Real time systems,
Computer science,
Embedded computing,
Performance analysis,
Equations,
Linear approximation"
Computation and performance trade-offs in motion estimation algorithms,"Real-time video/visual communication applications require trade-offs in terms of processing speed, visual image quality and power consumption. Motion estimation is one of the tasks in video coding that requires significant amount of computation. Block matching motion estimation algorithms such as the three-step search and the diamond search algorithms are being used in video coding schemes as alternatives to full search algorithms. Fast motion estimation algorithms reduce the computational complexity, at the expense of reduced performance. Special purpose fast processors can be employed as an alternative to meet the computational demand. However, the processing speed comes at the expense of higher power consumption. This paper investigates motion estimation algorithms and presents the computational, and performance trade-offs involved in choosing a motion estimation algorithm for video coding applications. Fast motion estimation algorithms often assume monotonic error surface in order to speed up the algorithm. The argument against this assumption is that the search might be trapped in local minima and may result in a noisy motion field. Prediction methods have been suggested in the literature as a solution to avoid these local minima and noisy motion field. The paper also investigates the effects of the monotonic error surface assumption as well as the appropriate choice of initial motion vectors that results in better performance of the motion estimation algorithms.","Motion estimation,
Video coding,
Image coding,
Computational complexity,
Video compression,
MPEG 4 Standard,
Algorithm design and analysis,
Computer science,
Real time systems,
Visual communication"
Automatic cell planning of broadband fixed wireless networks,"This paper presents an optimization model for the design of broadband fixed wireless access networks. The model considers the siting and configuration of the infrastructure necessary to provide a required service to a set of users, ie, selects the base station locations and the configures the antennas located at the base stations , eg, antenna type used, azimuth, tilt, channel assignment, and power selection.","Wireless networks,
Base stations,
Broadband antennas,
Azimuth,
Telecommunication traffic,
Directive antennas,
Computer science,
Buildings,
Design optimization,
Broadband communication"
A hardware/software codesign senior capstone design project in computer engineering,"This paper describes a senior capstone design project in computer engineering that incorporates the concept of hardware/ software codesign. Details of the project, required infrastructure and tools, and results of the first implementation of this project are described.","Hardware,
Design engineering,
Robot sensing systems,
Educational robots,
Leg,
Control systems,
Embedded system,
Microprocessors,
Embedded software,
Software performance"
Direct training of subspace distribution clustering hidden Markov model,"It generally takes a long time and requires a large amount of speech data to train hidden Markov models for a speech recognition task of a reasonably large vocabulary. Previously, we proposed a compact acoustic model called ""subspace distribution clustering hidden Markov model"" (SDCHMM) with an aim to save some of the training effort. SDCHMMs are derived from tying continuous density hidden Markov models (CDHMMs) at a finer subphonetic level, namely the subspace distributions. Experiments on the Airline Travel Information System (ATIS) task show that SDCHMMs with significantly fewer model parameters-by one to two orders of magnitude-can be converted from CDHMMs with no loss in word accuracy. With such compact acoustic models, one should be able to train SDCHMM directly from significantly less speech data (without intermediate CDHMMs). We devise a direct SDCHMM training algorithm, assuming an a priori knowledge of the subspace distribution tying structure. On the ATIS task, it is found that both a context-independent and a context-dependent speaker-independent 20-stream SDCHMM system trained with 8 min of speech perform as well as their corresponding CDHMM system trained with 105 min and 36 h of speech, respectively.","Hidden Markov models,
Vocabulary,
Automatic speech recognition,
Training data,
Parameter estimation,
Associate members,
Speech recognition,
Information systems,
Laboratories,
Computer science"
A framework for energy estimation of VLIW architecture,"VLIW architectures are being increasingly used in mobile environments where energy-efficiency is an important consideration. The energy efficiency of the VLIW architecture is determined by the underlying hardware and compiler technologies. In order to support efficient exploration of the energy tradeoffs of different architectural configurations and compiler optimizations, this work presents a new energy-estimation framework built over the Trimaran VLIW toolset. We investigate the influence of both architectural and compiler optimizations on energy-efficiency using the proposed framework.","VLIW,
Optimizing compilers,
Hardware,
Computer architecture,
Energy consumption,
Design optimization,
Digital signal processing,
Registers,
Statistics,
Computer science"
A discussion on the history of research in arithmetic and Reed-Muller expressions,This paper discusses early work by Komamiya in Reed-Muller and arithmetic expressions for switching functions.,"History,
Arithmetic,
Polynomials,
Discrete transforms,
Computer science"
Evolutionary algorithms with adaptive Levy mutations,"An evolutionary programming algorithm with adaptive mutation operators based on Levy probability distribution is studied. Levy stable distribution has an infinite second moment. Because of this, Levy mutation is more likely to generate an offspring that is farther away from its parent than Gaussian mutation, which is often used in evolutionary algorithms. Such likelihood depends on a parameter /spl alpha/ in the distribution. Based on this, we propose an adaptive Levy mutation in which four different candidate offspring are generated by each parent, according to /spl alpha/=1.0, 1.3, 1.7, and 2.0, and the best one is chosen as the offspring for the next generation. The proposed algorithm was applied to several multivariate function optimization problems. We show empirically that the performance of the proposed algorithm was better than that of classical evolutionary algorithms using Gaussian mutation.","Evolutionary computation,
Genetic mutations,
Probability distribution,
Genetic programming,
Shape,
Electronic mail,
Artificial intelligence,
Functional programming,
Gaussian distribution,
Computer science"
Analyzing fault effects in fault insertion experiments,Addresses the problem of evaluating system sensitivity to hardware faults. For this purpose the authors use a software implemented fault injector with extended statistical capabilities (FITS). The main contribution of the paper is the formulation of basic factors influencing system dependability and checking them in experiments. These experiments have been performed on an IBM PC platform.,"Circuit faults,
Fault detection,
Hardware,
Application software,
Fault tolerance,
Fault tolerant systems,
Registers,
Computer errors,
Computer science,
System testing"
Innovative interactive media for electrical engineering education,"The techniques used to educate future electrical engineers need to integrate innovative teaching approaches with today's technology in order to reach and motivate the diverse groups (students, faculty, professionals) that will be involved. This paper provides an overview of the work in progress to create, demonstrate, and implement web-based interactive learning modules (ILMs) and educational multimedia technologies that are being produced under a NSF-CCLI grant at Rensselaer Polytechnic Institute. These technologies operate within a browser environment and include: a means to capture, review and process a student's actions while using a interactive learning module (Scribe); a technology that allows for interactive, collaborative exploration, design and testing across the Internet (WebTeam); and the ability to provide Internet access to instrumented experimentation and data gathering using an inexpensive microcontroller board (LongLab).","Electrical engineering education,
Educational technology,
Consumer electronics,
Multimedia systems,
Power engineering computing,
Internet,
Instruments,
Design engineering,
Chemical technology,
Educational institutions"
Teaching engineering ethics: a new approach,"Engineering programs across the USA are experimenting with new ways to integrate engineering ethics into the curriculum. In a large part, this is motivated by criterion three of ABET's Engineering Criteria 2000 that requires that engineering programs demonstrate that their graduates have an understanding of professional and ethical responsibility. Faculty are challenged to find an appropriate place to integrate this material into their curricula. The Electrical and Computer Engineering Department at California State University, Northridge has been experimenting with the use of ""The Ethics Challenge"", a board game that has been developed and used by the Lockheed Martin Corporation for its in-house annual ethics awareness training. This approach allows ethics to be integrated into the curriculum in many places and at levels from freshman to senior. This paper describes ""The Ethics Challenge"" and how it can be used to teach engineering ethics in any engineering program.","Education,
Ethics,
Design engineering,
Aerospace engineering,
Biological materials,
Educational institutions,
Calculus,
Thermodynamics,
Mechanical engineering,
Design methodology"
Mediators over ontology-based information sources,"We propose a model for providing integrated and unified access to multiple information sources of the kind of Web catalogs. We assume that each source comprises two parts: (a) an ontology i.e. a set of terms structured by a subsumption relation, and (b) a database that stores objects of interest under the terms of the ontology. We assume that these objects belong to an underlying domain that is common to all sources (e.g. a set of Web pages of interest), and that different sources may use different ontologies with terms that correspond to different natural languages or to different levels of granularity. Information integration is obtained through a mediator comprising two parts: (a) an ontology, and (b) a set of articulations to the sources. Here, by articulation to a source we mean a set of relationships between terms of the mediator and terms of that source. Information requests (queries) are addressed to the mediator whose task is to analyze each query into sub-queries, translate them into queries to the appropriate sources, then merge the results to answer the original query. We study the querying and answering process in such a model and present algorithms for handling the main tasks of the mediator, namely, query translation between the mediator and the sources, source selection and result merging to produce the final answer.","Ontologies,
Databases,
Web pages,
Computer science,
Catalogs,
Natural languages,
Merging,
Physics"
An FPGA implementation of Walsh-Hadamard transforms for signal processing,"This paper describes two approaches suitable for an FPGA implementation of Walsh-Hadamard transforms. These transforms are important in many signal processing applications including speech compression, filtering and coding. Two novel architectures for the fast Hadamard transforms using both systolic architecture and distributed arithmetic techniques are presented. The first approach uses the Baugh-Wooley multiplication algorithm for a systolic architecture implementation. The second approach is based on both distributed arithmetic ROM and accumulator structure, and a sparse matrix factorisation technique. Implementations of the algorithms on a Xilinx FPGA board are described. Distributed arithmetic approach exhibits better performances when compared with the systolic architecture approach.","Field programmable gate arrays,
Signal processing,
Discrete transforms,
Computer architecture,
Signal processing algorithms,
Arithmetic,
Discrete Fourier transforms,
Mathematical model,
Image coding,
Read only memory"
Randomly colouring graphs with lower bounds on girth and maximum degree,"We consider the problem of generating a random q-colouring of a graph G=(V, E). We consider the simple Glauber Dynamics chain. We show that if the maximum degree /spl Delta/>c/sub l/ ln n and the girth g>c/sub 2/ ln ln n (n=|V|), then this chain mixes rapidly provided C/sub 1/, C/sub 2/ are sufficiently large, q/A>/spl beta/, where /spl beta//spl ap/1.763 is the root of /spl beta/=e/sup 1//spl beta//. For this class of graphs, this beats the 11/spl Delta//6 bound of E. Vigoda (1999) for general graphs. We extend the result to random graphs.","Chromium,
Character generation,
Computer science"
Built-in self-testable data path synthesis,"In this paper, we describe a high-level data path allocation algorithm to facilitate built-in self test. It generates self-testable data path design while maximizing the sharing of modules and test registers. The sharing of modules and test registers enables only a small number of registers is modified for BIST, thereby decreasing the hardware area which is one of the major overheads for BIST technique. In our approach, both module allocation and register allocation are performed incrementally. In each iteration, module allocation is guided by a testability balance technique while register allocation aims at increasing the sharing degrees of registers. With a variety of benchmarks, we demonstrate the advantage of our approach compared with other conventional approaches.","Built-in self-test,
Circuit testing,
Registers,
Hardware,
Automatic testing,
Test pattern generators,
Logic testing,
Digital circuits,
Computer science,
Manufacturing"
Designing user interfaces using activity theory,"The mainstream framework of computer-interaction research of cognitive psychology has come under increasing criticism lately because of the gap between research results and practical design. According to K. Kuutti (1996), the main criticism is that traditional cognitive psychology of design is not able to penetrate the human side of the interface. To overcome these limitations, an alternative approach to interface design is necessary. Activity Theory, which originated within Soviet psychology, appears to have much to offer. Activity Theory incorporates notions of intentionality, history, mediation, motivation, understanding, culture and community and it is these aspects that have proved attractive to interface design. We believe that Activity Theory offers several benefits to interface design, compared to the traditional cognitive psychology approach. This paper describes a case study involving the use of Activity Theory for the design and evaluation of a tourist information kiosk.","User interfaces,
Psychology,
Human computer interaction,
Computer interfaces,
History,
Mediation,
Autonomous agents,
Product design,
Laboratories,
Instruments"
Using a model framework in developing and delivering a family of software engineering project courses,"The University of Southern California (USC) teaches a two-semester real-client project course as a core course in USC's MSCS-SE (Master of Science in Computer Science and Software Engineering) degree program. The course has evolved rapidly, each year introducing many changes in order to satisfy the course stakeholder's win conditions. The course has also been our primary experimental testbed for evolving our MBASE (Model-Based Architecting and Software Engineering) model integration framework. In turn, this framework, along with the CRESST (Center for Research on Evaluation, Standards and Student Testing) cognitive demands analysis has served as an effective means of managing the course's rapid evolution. A further test of the framework has been its application to undergraduate software engineering project courses at other institutions. This paper provides a description and examples of USC's experience in constructing and evolving a family of software engineering project courses based on the MBASE software engineering model integration framework. A discussion of course needs, use of software engineering models, a description of MBASE, use of the CRESST model of learning objectives, course development and experiences are presented. The approach has been used successfully at Columbia University and to a lesser degree at other institutions. Much of the current and historical materials and resources described in this paper are freely available for educational use.","Software engineering,
Object oriented modeling,
Educational programs,
Software testing,
Education,
Application software,
Project management,
Engineering management,
System testing,
Documentation"
Experimental data about knowledge evaluation in a distance learning system,"Distance education has become an especially popular topic in recent years, mostly due to the powerful Web-based educational services offered by universities as well as the comfort of learning at home. In this paper, we present an algorithm for the testing and knowledge evaluation of the users in a Web-based distance education system. The algorithm employs fuzzy logic techniques, thus offering a linguistic expression of the estimated level of the user's knowledge. A prototype system is built in the Java language. The experimental results expose compatibility in the evaluated expert levels given by the system and the tutor, but they also emphasize the benefits of the system from the pedagogical point of view.","Computer aided instruction,
System testing,
Automatic testing,
Distance learning,
Fuzzy logic,
Iterative algorithms,
Computer science,
Prototypes,
Java,
Usability"
Robust invisible watermarking of volume data using the 3D DCT,"Proposes a novel watermarking algorithm for 3D volume data based on the spread-spectrum communication technique, which is invisible and robust. ""Invisible"" means that the 2D rendered image of this watermarked volume is perceptually indistinguishable from that of the original volume. ""Robust"" watermarking implies that the watermark is resistant to most intentional or unintentional attacks. We have implemented the algorithm in a software package and have conducted experiments showing that the watermark is invisible in the volume-rendered 2D images. This is further confirmed by computing the signal-to-noise ratio (SNR) and the peak signal-to-noise ratio (PSNR) of the 3D watermarked volume data. We addressed different attacks, putting more emphasis on those attacks that are most commonly performed by attackers. The experiments show that the watermarking scheme is robust.","Robustness,
Watermarking,
Discrete cosine transforms,
Rendering (computer graphics),
Nonlinear filters,
Filtering,
Spread spectrum communication,
Nonlinear distortion,
Computer science,
Information retrieval"
Availability study of dynamic voting algorithms,"Fault-tolerant distributed systems often select a primary component to allow a subset of the processes to function when failures occur. The dynamic voting paradigm defines rules for selecting the primary component adaptively: when a partition occurs, if a majority of the previous primary component is connected, a new and possibly smaller primary component is chosen. Several studies have shown that dynamic voting leads to more available solutions than other paradigms for maintaining a primary component. However, these studies have assumed that every attempt made by the algorithm to form a new primary component terminates successfully. Unfortunately, in real systems, this is not always the case: a change in connectivity can interrupt the algorithm while it is still attempting to form a new primary component; in such cases, algorithms may block until the processes can resolve the outcome of the interrupted attempt. This paper uses simulations to evaluate the effect of interruptions on the availability of dynamic voting algorithm. We study four dynamic voting algorithms and identify two important characteristics that impact an algorithm's availability in runs with frequent connectivity changes. First, we show that the number of processes that need to be present in order to resolve past attempts impacts the availability, especially during long runs with numerous connectivity changes. Second, we show that the number of communication rounds exchanged in an algorithm plays a significant role in the availability achieved, especially in the degradation of availability as connectivity changes become more frequent.","Voting,
Heuristic algorithms,
Aerodynamics,
Partitioning algorithms,
Fault tolerant systems,
Contracts,
Computer science,
Oils,
Degradation,
Telegraphy"
Evaluating register file size in ASIP design,Interest in synthesis of Application Specific Instruction Set Processors or ASIPs has increased considerably and a number of methodologies have been proposed for ASIP design. A key step in ASIP synthesis involves deciding architectural features based on application requirements and constraints. In this paper we observe the effect of changing register file size on the performance as well as power and energy consumption. Detailed data is generated and analyzed for a number of application programs. Results indicate that choice of an appropriate number of registers has a significant impact on performance.,"Application specific processors,
Registers,
Aerospace electronics,
Space exploration,
Data flow computing,
Computer science,
Costs,
Automatic control,
Control system synthesis,
Permission"
Multi-level watermarking with independent decoding,"A drawback of most watermarking techniques is the need for some additional information in order to retrieve the watermark. Additionally, the robustness of the watermark decreases as the number of information bits stored in the image increases. We present a watermarking technique which requires no information for decoding in addition to the watermarked image. The watermark is multi-level with few bits embedded robustly at low levels and longer watermark sequences embedded less robustly at higher levels. This allows detection of ""tampered"" and ""attacked"" images by detection of existence of the watermark at low levels and deterioration of the watermark at high levels. In order to prevent interference of the watermarks at different levels various image representation spaces are used. The watermark is shown to be non-visible and robust.","Watermarking,
Decoding,
Robustness,
Image representation,
Computer science,
Information retrieval,
Frequency,
Random number generation,
Interference,
Digital images"
Chemical-shift imaging utilizing the positional shifts along the readout gradient direction,"Describes a method that uses the linear phase acquired during the readout period due to chemical shift to generate individual magnetic resonance (MR) images of chemically shifted species. The method utilizes sets of Fourier (or /spl kappa/-space) data acquired with different directions of the readout gradient and a postprocessing algorithm to generate chemical shift images. The methodology is developed for both Cartesian data acquisition and for radial data acquisition. The method is presented here for two chemically shifted species but it can be extended to more species. Here, the authors present the theory, show the results in phantoms and in human images, and discuss the artifacts and signal-to-noise ratio of the images obtained with the technique.","Chemicals,
Magnetic resonance imaging,
Optical imaging,
Radiology,
High-resolution imaging,
Data acquisition,
Lipidomics,
Resonant frequency,
Optical saturation,
Magnetic resonance"
A reconfigurable extension to the network interface of beowulf clusters,,"Network interfaces,
Costs,
Computer interfaces,
Computer networks,
Prototypes,
Field programmable gate arrays,
Supercomputers,
Application software,
Sorting,
Hardware"
Disconnection proofs for motion planning,"Probabilistic road-map (PRM) planners have shown great promise in attacking previously infeasible motion planning problems with many degrees of freedom. Yet when such a planner fails to find a path, it is not clear that no path exists, or that the planner simply did not sample adequately or intelligently the free part of the configuration space. We propose to attack the motion planning problem from the other end, focusing on disconnection proofs, or proofs showing that there exists no solution to the posed motion planning problem. Just as PRM planners avoid generating a complete description of the configuration space, our disconnection provers search for certain special classes of proofs that are compact and easy to find when the motion planning problem is 'obviously impossible,"" avoiding complex geometric and combinatorial calculations. We demonstrate such a prover in action for a simple, yet still realistic, motion planning problem. When it fails, the prover suggests key milestones, or configurations of the robot that can then be passed on and used by a PRM planner. Thus by hitting the motion planning problem from both ends, we hope to resolve the existence of a path, except in truly delicate border-line situations.","Orbital robotics,
Sampling methods,
Motion planning,
Robots,
Space exploration,
Computer science,
Path planning,
Road accidents"
Toward a realization of the value of benefit in real-time systems,,"Real time systems,
Application software,
Video compression,
Multimedia systems,
Computer science,
Taxonomy,
Displays,
Lifting equipment,
Robustness,
Time factors"
Enhancing technical communication skills of engineering students: an experiment in multidisciplinary design,"A multidisciplinary team of chemical engineering and computer science students collaborated to design a plant capable of producing commercial quantities of citric acid. This project required the students to produce a bench-scale chemical engineering facility and a computer system to monitor production in accordance with Food and Drug Administration (FDA) regulations. A previous attempt at student collaboration on a similar project produced less than stellar results. An evaluation of that experience revealed the most significant challenge to project success was establishing effective teamwork and appropriate technical communication across the two disciplines. This paper describes the results of the most recent multidisciplinary team experiment, in which emphasis was placed on developing communication between student teams. A description of the synchronization of project development methodologies between the participating disciplines is discussed as well as how this contributed to enhancing technical communication between the teams and enabled the latest project to progress to a successful conclusion.","Professional communication,
Engineering students,
Computer science,
Chemical engineering,
Teamwork,
Computer science education,
Writing,
Collaboration,
Chapters,
Pharmaceuticals"
Quality events: a flexible mechanism for quality of service management,"The paper describes a software mechanism, called quality events, that utilizes application- and/or system-level service extensions to provide quality of service (QoS) guarantees to end users. Such extensions offer flexibility in: (1) how application- and/or system-level services are dynamically managed to maintain required quality, (2) when such management occurs, and (3) where this service management is performed. Several adaptive QoS management strategies are implemented with quality events and compared with respect to their ability to meet application-specific QoS requirements. These management strategies have different service adaptation latencies, and different degrees of coordination between services. Significant performance variations observed for these alternative strategies demonstrate the importance of a flexible QoS management mechanism like quality events. Finally, we show that adaptive QoS, and hence resource management strategies can lead to more efficient use of resources, and better qualities of service for certain applications than non-adaptive resource management methods.","Quality of service,
Quality management,
Runtime,
Availability,
Resource management,
Monitoring,
Computer science,
Software quality,
Delay,
Educational institutions"
Exploiting constraints during prioritized path planning for teams of mobile robots,"Coordinating the motion of multiple mobile robots is one of the fundamental problems in robotics. The predominant algorithms for coordinating teams of robots are decoupled and prioritized, thereby avoiding combinatorially hard planning problems typically faced by centralized approaches. We present a method for finding solvable priority schemes for such prioritized and decoupled planning techniques. Existing approaches apply a single priority scheme which makes them overly prone to failure in cases where valid solutions exists. By searching in the space of priorization schemes, our approach overcomes this limitation. To focus the search, our algorithm is guided by constraints generated from the task specification. To illustrate the appropriateness of this approach, the paper discusses experimental results obtained with real robots and through systematic robot simulation. The experimental results demonstrate that our approach successfully solves many more coordination problems than previous decoupled and prioritized techniques.","Path planning,
Mobile robots,
Robot kinematics,
Orbital robotics,
Motion planning,
Computer science,
Multirobot systems,
State-space methods,
System recovery,
Interconnected systems"
Incremental computation and maintenance of temporal aggregates,"Considers the problems of computing aggregation queries in temporal databases and of maintaining materialized temporal aggregate views efficiently. The latter problem is particularly challenging, since a single data update can cause aggregate results to change over the entire time-line. We introduce a new index structure called the SB-tree, which incorporates features from both segment trees (S-trees) and B-trees. SB-trees support the fast lookup of aggregate results based on time, and can be maintained efficiently when the data changes. We also extend the basic SB-tree index to handle cumulative (also called moving-window) aggregates. For materialized aggregate views in a temporal database or data warehouse, we propose building and maintaining SB-tree indices instead of the views themselves.","Aggregates,
Spatial databases,
Data warehouses,
Computer science,
Database languages,
Information analysis,
Drugs"
Usage testing of military simulation systems,Scalability and input domain explosion make it impossible to exhaustively test simulation systems. Improved methods such as statistical usage testing are needed to provide quantitative support for test planning and test management. This paper describes the challenges and the state! of the practice of testing simulation systems. A brief introduction to statistical usage testing is provided. An approach to developing an abstract usage model structure appropriate for testing military simulation systems is suggested and illustrated. This approach supports the creation and analysis of test scenarios that are flexible enough to handle a wide range of uses in military simulations.,"System testing,
Scalability,
Automatic testing,
Software testing,
Computational modeling,
Explosions,
Analytical models,
Character generation,
Military computing,
Computer science"
Student use of personal digital assistants in a computer engineering course,"Penn State Abington has integrated the student use of personal digital assistant (PDA) technology to foster active and collaborative learning experiences in the classroom and laboratory. Palm/sup TM/ PDA technology was introduced into a sophomore-level digital systems course in the fall of 2000. The students have investigated handheld software tools for enhancing learning and instruction in both the lecture and laboratory components of the course. Handheld databases, simple CAD tools, C programming, image capture, web-based tools, and robotics applications have been explored.",
A visual approach to XML document design and transformation,"This paper presents a visual approach to the representation and validation of multimedia document structures specified in XML and transformation of one structure to another. The underlying theory of our approach is a context-sensitive graph grammar formalism. The paper demonstrates the conciseness and expressiveness of the graph grammar formalism. An example XML structure is provided and its graph grammar representation, validation and transformation to a multimedia representation are presented.",
TAR: temporal association rules on evolving numerical attributes,"Data mining has been an area of increasing interest. The association rule discovery problem in particular has been widely studied. However, there are still some unresolved problems. For example, research on mining patterns in the evolution of numerical attributes is still lacking. This is both a challenging problem and one with significant practical applications in business, science, and medicine. In this paper we present a temporal association rule model for evolving numerical attributes. Metrics for qualifying a temporal association rule include the familiar measures of support and strength used in traditional association rule mining and a new metric called density. The density metric not only gives us a way to extract the rules that best represent the data, but also provides an effective mechanism to prune the search space. An efficient algorithm is devised for mining temporal association rules, which utilizes all three thresholds (especially the strength) to prune the search space drastically. Moreover, the resulting rules are represented in a concise manner via rule sets to reduce the output size. Experimental results on real and synthetic data sets demonstrate the efficiency of our algorithm.","Association rules,
Data mining,
Databases,
Remuneration,
Numerical models,
Density measurement,
Promotion - marketing,
Computer science,
Marketing and sales,
Frequency"
A set theory within fuzzy logic,"This paper proposes a possibility of developing an axiomatic set theory, as first-order theory within the framework of fuzzy logic in the style of Hajek's Basic fuzzy logic BL. In classical Zermelo-Fraenkel set theory, we use an analogy of the construction of a Boolean-valued universe-over a particular algebra of truth values-we show the nontriviality of our theory. We present a list of problems and research tasks.","Set theory,
Fuzzy logic,
Algebra,
Fuzzy set theory,
Logic functions,
Computer science,
Multivalued logic,
Inspection"
Duration normalization for improved recognition of spontaneous and read speech via missing feature methods,"Hidden Markov models (HMMs) are known to model the duration of sound units poorly. We present a technique to normalize the duration of each phone to overcome this weakness, with the conjecture that speech with normalized phone durations may be better modeled and discriminated using standard HMM acoustic models. Duration normalization is accomplished by dropping frames if a phone is longer than the desired duration and by adding ""missing"" frames and reconstructing them if a phone is shorter than the desired duration. If phone segmentations are known a priori, we achieve a 15.8% reduction in relative word error rate (WER) on spontaneous speech and a 10.3% reduction in relative WER on read speech. Preliminary work with automatic phone segmentations derived from the data is also presented.",
Group task analysis for groupware usability evaluations,"Techniques for inspecting the usability of groupware applications have recently been proposed. These techniques focus on the mechanics of collaboration rather than the work context in which a system is used and offer time and cost savings by not requiring actual users or fully functional prototypes. Although these techniques are valuable, adding information about task and work context could improve the quality of inspection results. We introduce a method for analysing group tasks that can be used to add context to discount groupware evaluation techniques. Our method allows for the specification of collaborative scenarios and tasks by considering the mechanics of collaboration, levels of coupling during task performance, and variability in task execution. We describe how this type of task analysis could be used in a new inspection technique based on cognitive walkthrough.",
A cache cooperation management for wireless,"Proxy cache located at a base station is helpful for improving the performance of wireless multimedia streaming. In this paper, a cooperated proxy cache architecture, MobileCache, is proposed for the cell-based wireless environment. The proxies cooperate to provide seamless streaming for a wireless client. In order to improve the efficiency for MobileCache, a cache cooperation management scheme including cache replacement, cache migration and cache replication is presented to improve the revenue for MobileCache. Simulation proves that MobileCache achieves higher performance by employing the cache cooperation management than other schemes.","Streaming media,
Internet,
Wireless communication,
Quality of service,
Base stations,
Prefetching,
Relays,
Local area networks,
Computer science,
Computer architecture"
The right algorithm at the right time: comparing data flow analysis algorithms for finite state verification,"Finite-state verification is emerging as an important technology for proving properties about software. In our experience, we have found that analysts have different expectations at different times. When an analyst is in an exploratory mode, initially formulating and verifying properties, analyses usually find inconsistencies because of flaws in the properties or in the software artifacts being analyzed. Once an inconsistency is found, the analyst begins to operate in a fault-finding mode, during which meaningful counter-example traces are needed to help determine the cause of the inconsistency. Eventually, systems become relatively stable, but still require re-verification as evolution occurs. During such periods, the analyst is operating in a maintenance mode and would expect re-verification to usually report consistent results. Although it could be that one algorithm suits all three of these modes of use, the hypothesis explored in this paper is that each would be best served by an algorithm optimized for the expectations of the analyst.","Data analysis,
Algorithm design and analysis,
Engines,
Software systems,
Counting circuits,
Software algorithms,
Laboratories,
Software engineering,
Computer science,
Debugging"
Real-time flat-panel pixel imaging system and control for X-ray and neutron detection,"We present in this paper industrial nondestructive X-ray and neutron testing applications with a real-time digital imaging device and control system X-View based on active matrix flat-panel imager technology. X-View consists of X-ray or neutron converters, arrays of amorphous silicon (a-Si:H) thin-film transistors and photodiodes, a fast real-time electronic system for readout and digitization of images and appropriate computer tools for control, real-time image treatment data representation, and off-line analysis. Some basic image-quality parameters and different objects were assessed for quantitative and qualitative analysis. Results show a wide dynamic range (16 bits ADC resolution) and lack of blooming, a high frame rate (up to 25 fps), and rapid image capture. Images are directly displayed, on-line, on a PC monitor and archived in a digital form for radiography and radioscopy procedures and limitless industrial applications in X-ray and neutron inspections.","Real time systems,
Pixel,
Optical imaging,
Control systems,
X-ray imaging,
Neutrons,
Image analysis,
Electrical equipment industry,
Industrial control,
Nondestructive testing"
Uniform leader election protocols for radio networks,"A radio network is a distributed system with no central arbiter, consisting of n radio transceivers, henceforth referred to as stations. We assume that the stations are identical and cannot be distinguished by serial or manufacturing number. The leader election problem asks to designate one of the stations as leader. A leader election protocol is said to be uniform if in each time slot every station transmits with the same probability. In a seminal paper Willard (1986) presented a uniform leader election protocol for single-channel single-hop radio stations terminating in log log n+o(log log n) expected time slots. It was open whether Willard's protocol featured the same time performance with ""high probability"". We propose a uniform leader election protocol that terminates, with probability exceeding 1-1/f for every f/spl ges/1, in log log n+o(log log n)+O(log f) time slots. We also prove that for every f/spl isin/e/sup O(n)/, in order to ensure termination with probability exceeding 1-1/f, Willard's protocol must take log log n+/spl Omega/(/spl radic/f) time slots. Finally, we provide simulation results that show that our leader election outperforms Willard's leader election protocol in practice.","Nominations and elections,
Protocols,
Radio networks,
Radio network,
Radio transceivers,
Upper bound,
Information science,
Computer science,
Manufacturing,
Radio frequency"
Evolving Control Metabolisms for a Robot,"This article demonstrates a new method of programming artificial chemistries. It uses the emerging capabilities of the system's dynamics for information-processing purposes. By evolution of metabolisms that act as control programs for a small robot one achieves the adaptation of the internal metabolic pathways as well as the selection of the most relevant available exteroceptors. The underlying artificial chemistry evolves efficient information-processing pathways with most benefit for the desired task, robot navigation. The results show certain relations to such biological systems as motile bacteria.","genetic programming,
artificial chemistry,
chemical information processing,
autonomous robots"
Increasing the throughput of multihop packet radio networks with power adjustment,"The packet radio network (PRN) is an attractive architecture to support mobile and wireless communication. Although the code assignment problem has been studied extensively on PRN, we observe that the power control problem has been ignored by most works, but may have significant impact on performance. By power control, we mean that the transmission ranges of stations are tunable. We show, given a PRN in which each host already received a code, how to adjust the powers of stations to control/improve the topology of the PRN without violating the original code assignment. Several schemes are proposed. Through simulations, we demonstrate that although the code assignment problem is NP-complete and thus computationally very expensive, using our power adjustment schemes can easily improve the network performance by about 20% with polynomial costs.","Throughput,
Spread spectrum communication,
Packet radio networks,
Protocols,
Power control,
Network topology,
Computer networks,
Computer science,
Wireless communication,
Computational modeling"
A high assurance on-line recovery technology for a space on-board computer,"A high-assurance online recovery technology for a space on-board computer that can be realized using commercial devices is proposed whereby a faulty processor node confirms its normality and then recovers without affecting the other processor nodes in operation. Also, the result of an evaluation test using the breadboard model (BBM) implementing this technology is reported. Because this technology enables simple and assured recovery of a faulty processor node regardless of its degree of redundancy, it can be applied to various applications, such as a launch vehicle, a satellite or a reusable space vehicle. As a result, decreasing the cost of an on-board computer is possible while maintaining its high reliability.","Space technology,
Satellites,
Space vehicles,
Application software,
Costs,
Maintenance,
Vehicle safety,
Space shuttles,
Computer science,
Testing"
Virtual community knowledge evolution,"Puts forth a vision and a possible architecture for a community knowledge evolution system. We propose augmenting a multimedia document repository (digital library) with innovative knowledge evolution support, including computer-mediated communications, community process support, decision support, advanced hypermedia features and conceptual knowledge structures. These tools and the techniques developed around them would enable members of a virtual community to learn from, contribute to, and collectively build upon the community's knowledge and improve many member tasks. The resulting collaborative knowledge evolution support system (CKESS) would provide an enhanced digital library infrastructure serving as an ever-evolving repository of the community's knowledge, which members would actively use in everyday tasks and regularly update.","Professional societies,
Software libraries,
Computer mediated communication,
Collaborative work,
Computer architecture,
Art,
Computer vision,
Technological innovation"
A process and technology-tolerant I/sub DDQ/ method for IC diagnosis,"The use of I/sub DDQ/ test as a defect reliability screen has been widely used to improve device quality. However, the increase in subthreshold leakage currents in deep submicron technologies has made it difficult to set an absolute pass/fail threshold. Recent work has focused on strategies that calibrate for process and/or technology-related variation effects. In this paper, a new I/sub DDQ/ technique is proposed that is based on an extension of a V/sub DDT/-based method called Transient Signal Analysis (TSA). The method, called Quiescent Signal Analysis or QSA, uses the I/sub DDQ/s measured at multiple supply pins as a means of localizing defects. Increases in I/sub DDQ/ due to a defect are regionalized by the resistive element of the supply grid. Therefore, each supply pin sources a unique fraction of the total I/sub DDQ/ drawn by the defect. The method analyzes the regional I/sub DDQ/s and ""triangulates"" the position of the defect to an (x,y) location in the layout. This information can be used in combination with fault dictionary-based techniques as a means of further resolving the defect's location.","Signal analysis,
Pins,
Testing,
Subthreshold current,
Transient analysis,
Regression analysis,
Signal processing,
Computer science,
Circuit faults,
Integrated circuit reliability"
Erroneous requirements: a linguistic basis for their occurrence and an approach to their reduction,"Poor communication of domain knowledge is implicated as a major threat to the validity of a requirements specification. It is hypothesized that careful examination of the mechanisms used for this communication can yield insight to motivate a response. We recognize that natural language is amenable to rigorous inspection and look to research results in linguistics to illuminate the nature of communicative breakdowns. From a better understanding of these breakdowns, we develop an approach, called the domain map, that through its structure and the process of its creation shows potential to aid in the systematic reduction of the incidence of communicative breakdowns during requirements. Essential domain semantics are better preserved in the conceptual model of the developers and thus the threat to validity of the model is reduced. Tool support enabling the creation and analysis of domain maps is also described.",
Ice crystals and raindrop canting angle affecting the performance of a satellite system suffering from differential rain attenuation and cross-polarization,"For frequencies above 10 GHz, which are of high importance in current satellite systems, interference is mainly aggravated because of the following reasons: potentially existing differential rain attenuation along the wanted and interfering paths as well as depolarization induced by the rainfall medium. The latter source concerns, of course, satellite systems using the frequency-sharing technique. In the present paper an already proposed method to predict the degradation of the carrier-to-interference (C/I) ratio due to the above sources is properly modified by taking into account ice crystals and raindrop canting angle effects. The novel assumptions reflect upon the more accurate estimation of cross-polarization discrimination and thus contribute to the reliable design of the system being interfered with. The present results are compared with the so far existing ones. The sensitivity of various parameters affecting the interference performance of the system is investigated. As a general conclusion, the inclusion of the ice crystals along with the raindrop canting angle effects may be of importance in some cases, such as the operation in the K band combined with the circular incident polarization, for the accurate estimation of the degradation of the total (C/I) ratio.","Rain,
Ice,
Attenuation,
Crystals,
Satellites,
Satellite broadcasting,
Interference"
On the size and shape of multi-level context templates for compression of map images,"We present a method for estimating optimal context templates that are used for conditioning the pixel probabilities in context-based image compression. The algorithm optimizes the location of the context pixels within a limited neighborhood area, and produces an ordered template as a result. The ordering can be used to determine the shape of the context template for a given template size. The optimal template size depends on the size of the image, when the template shape depends on the image type. We apply the method to the compression of multi-component map images consisting of several semantic layers represented as binary images. We estimate the shape of the context-template for each layer separately, and compress the layers as generic regions using the Joint Bi-level Image Group standard JBIG2 compression technique.",
Internet search engine freshness by Web server help,"We study how to keep the Internet search engines up-to-date with the changes occurring at the various Web servers in the Internet. Currently, Web search engines poll the Web servers on a per-URL basis for obtaining update information. We advocate an approach in which Web servers themselves track the changes happening to their content files for propagating updates to search engines. We propose an algorithm which uses both freshness and popularity of data at the Web servers for deciding the discrepancy between a Web site and a search engine. This algorithm batches the push of updates from the Web server to the search engine. We prove that this algorithm is competitive with an optimal algorithm.","Internet,
Search engines,
Web server,
Web search,
Delay,
Computer science,
Indexing,
Information resources,
Databases,
Web pages"
Modeling high-energy heavy-ion damage in silicon,"We identify a discrepancy between experimental data and model predictions by a widely used radiation transport code, SRIM. We describe a method for determining the implant and damage profiles of energetic heavy ions that better agrees with experimental data than SRIM Monte Carlo predictions. Results of our method are given and compared to experimental data for a range of ion types and energies. Finally, the reason for discrepancy is discussed.","Silicon,
Monte Carlo methods,
Implants,
Predictive models,
Ion implantation,
Helium,
Ion beams,
Solid state circuits,
Radiation detectors,
Particle accelerators"
Simulation of induction-logging response using conjugate gradient method with nonuniform fast Fourier and fast Hankel transforms,"Previously, the conjugate gradient method combined with fast Fourier and fast Hankel transform (CG-FFHT) was developed to solve an integral equation for borehole induction measurements in axisymmetric media. In the CG-FFHT method, the regular fast Fourier transform algorithm uses equal-spaced sample points, while the fast Hankel transform algorithm requires the sample points to distribute uniformly on a logarithmic scale. The uniform grid limits the utility of the CG-FFHT method since it is not the most efficient way to discretize a problem, especially when there are fine structures in the geometry. These limitations are removed in this work by the use of the newly developed nonuniform fast Fourier transform (NUFFT) and nonuniform fast Hankel transform (NUFHT) algorithms. The combination of the CG procedures and NUFFT and NUFHT leads to the CG-NUFFHT method. It retains the computational efficiency of the CG-FFHT method but has the flexibility of a nonuniform grid. Excellent agreement is shown between the CG-NUFFHT results and those from the numerical mode-matching method.","Transforms,
Antenna measurements,
Finite element analysis,
Convolution,
Geophysical measurements,
Algorithm design and analysis,
Magnetic field measurement"
MAGE: a distributed programming model,"Writing distributed programs is difficult. To ease this task, we introduce a new programming abstraction which we call a mobility attribute. Mobility attributes provide a syntax that describes the mobility semantics of program components. Programmers attach mobility attributes to program components to dynamically control the placement of these components within the network. Mobility attributes intercept component invocations and decide whether and where to move a component before the component executes. This allows the programmer to improve her program's runtime efficiency by colocating components and resources. We present MAGE, an object oriented distributed system, that supports mobility attributes and illustrates their utility.","Programming profession,
Logic programming,
Distributed computing,
Object oriented modeling,
Laboratories,
Computer science,
Writing,
Computer architecture,
Large-scale systems,
Costs"
Comparing effort prediction models for Web design and authoring using boxplots,"Software practitioners recognise the importance of realistic estimates of effort to the successful management of software projects, the Web being no exception. Having realistic estimates at an early stage in a project's life-cycle allow project managers and development organisations to manage resources effectively. Prediction is a necessary part of an effective process, be it authoring, design, testing, or Web development as a whole. The first part of this paper describes a case study evaluation (CSE) where we measured the characteristics of Web applications and the effort involved in designing and authoring those applications. The second half presents two prediction models generated using statistical techniques, namely linear regression and stepwise multiple regression. The prediction power of the models employed is then compared using boxplots of the residuals. The results suggest that stepwise regression gives better predictions than linear regression.","Predictive models,
Web design,
Project management,
Resource management,
Costs,
Linear regression,
Computer science,
Life estimation,
Testing,
Particle measurements"
PatchODMRP: an ad-hoc multicast routing protocol,"We propose an ad-hoc multicast routing protocol, referred to as PatchODMRP. PatchODMRP extends the ODMRP (on-demand multicast routing protocol), which is a mesh-based multicast routing protocol proposed for ad-hoc networks. In ODMRP, the nodes that are on the shortest paths between the multicast group members are selected as forwarding group (FG) nodes, and form a forwarding mesh for the multicast group. The ODMRP reconfigures the forwarding mesh periodically to adapt it to the node movements. When the number of sources in the multicast group is small, usually the forwarding mesh is formed sparsely and it can be very vulnerable to mobility. In this case, very frequent mesh reconfigurations are required in ODMRP, resulting in a large control overhead. To deal with this problem in a more efficient way, PatchODMRP deploys a local patching scheme instead of having very frequent mesh reconfigurations. In PatchODMRP, each FG node keeps checking if there is a symptom of mesh separation around itself. When an FG node finds such a symptom, it tries to patch itself to the mesh with local flooding of control messages. Through a course of simulation experiments, the performance of PatchODMRP is compared to the performance of ODMRP. The simulation results show that PatchODMRP improves the data delivery ratio, and reduces the control overheads. It has also been shown that the performance gain is larger when the degree of node mobility is bigger.","Multicast protocols,
Routing protocols,
Ad hoc networks,
Floods,
Computer science,
Performance gain,
Spread spectrum communication,
Wireless networks,
Base stations,
Scalability"
Web access to supercomputing,"Theoretically, computational and data grids are the computing paradigm of choice, but they will not gain wide acceptance until users have seamless access to them. The authors describe how to provide comfortable, intuitive, yet powerful Web access to supercomputing, A Web-based, grid-enabled application that processes, analyzes, and delivers remote-sensing images provides an example of the technology at work.","Grid computing,
Distributed computing,
Testing,
Power engineering computing,
Concurrent computing,
Power grids,
Computer networks,
High-speed networks,
Supercomputers,
NASA"
Gulfstream - a system for dynamic topology management in multi-domain server farms,,"Network servers,
Network topology,
Prototypes,
Electronic switching systems,
Software prototyping,
Hardware,
Local area networks,
Computer science,
Computer network management,
Ethernet networks"
XML-based method and tool for handling variant requirements in domain models,"A domain model describes common and variant requirements for a system family. UML notations used in requirements analysis and software modeling can be extended with variation points to cater for variant requirements. However, UML models for a large single system are already complicated enough. With variants UML domain models soon become too complicated to be useful. The main reasons are the explosion of possible variant combinations, complex dependencies among variants and inability to trace variants from a domain model down to the requirements for a specific system, member of a family. We believe that the above mentioned problems cannot be solved at the domain model description level alone. We propose a novel solution based on a tool that interprets and manipulates domain models to provide analysts with customized, simple domain views. We describe a variant configuration language that allows us to instrument domain models with variation points and record variant dependencies. An interpreter of this language produces customized views of a domain model, helping analysts understand and reuse software models. We describe the concept of our approach and its simple implementation based on XML and XMI technologies.","Unified modeling language,
Computer architecture,
Systems engineering and theory,
Design engineering,
Computer science,
Explosions,
Instruments,
XML,
Runtime,
Programming"
An implementation of and experiment with semantic differencing,"Software maintainers face a wide range of difficult tasks including impact analysis and regression testing. Understanding semantic relationships, such as the semantic cohesiveness in a program or the semantic differences between two programs, can help a maintainer address these problems. However, semantic analysis is a difficult problem. For example, few semantic differencing algorithms and even fewer implementations exist. The first semantic differencing implementation for the C language is presented and studied. A large collection of semantic differences of 10 programs are computed. The average size reduction was 37.70%. The study presented illustrates the practicality of semantics differencing. Finally, the application of semantic differencing in the area of program testing and impact analysis is considered.","Software maintenance,
Software debugging,
Performance analysis,
Computer science,
Educational institutions,
Electronic switching systems,
Application software,
Software testing,
Computer languages,
Runtime"
CBI/Tomash fellowship: sponsoring a generation of scholars in the history of information processing,"Dissertation research in the history of computing received a major boost with the establishment of the Adelle and Erwin Tomash Fellowship. This fellowship, as the author explains, was part of the Charles Babbage Institute's underlying philosophy. One of the Charles Babbage Institute's foundational concepts to advance scholarship on the history of information processing was providing fellowships for dissertation research. As a result, a CBI/Tomash Fellowship has been granted every year from 1978 to the present, with the exception of two years in which the fellowship had corecipients (1979-1980 and 1986-1987). In 1989, the Charles Babbage Institute Fellowship was renamed the Adelle and Erwin Tomash Fellowship to recognize Erwin and Adelle's generous support of the Institute and its programs. The CBI/Tomash,Fellows have consistently produced important dissertations about the history of information processing. A number of them have been revised and published as books, or they have been the basis for significant articles. The majority of the fellows came from programs in the history of technology and science, but doctoral students from other disciplines conducting historically based dissertation projects have also received the awards. Many fellowship recipients have become leading scholars of information processing history, while others have furthered understanding of contemporary and historical issues in computing through research and teaching in economics, management, communications, and other disciplines.","History,
Information processing,
Military computing,
Computer industry,
Internet,
Relays,
Reliability engineering,
Fellows,
Cultural differences,
Computer science"
Certifying domain-specific policies,"Proof-checking code for compliance to safety policies potentially enables a product-oriented approach to certain aspects of software certification. To date, previous research has focused on generic, low-level programming-language properties such as memory type safety. In this paper we consider proof-checking higher-level domain-specific properties for compliance to safety policies. The paper first describes a framework related to abstract interpretation in which compliance to a class of certification policies can be efficiently calculated. Membership equational logic is shown to provide a rich logic for carrying out such calculations, including partiality, for certification. The architecture for a domain-specific certifier is described, followed by an implemented case study. The case study considers consistency of abstract variable attributes in code that performs geometric calculations in Aerospace systems.","Logic programming,
Equations,
Software safety,
Computer languages,
Aerospace safety,
Product safety,
Certification,
Software engineering,
Computer science,
NASA"
Online text-independent speaker verification system using autoassociative neural network models,"In this paper, we present an approach based on autoassociative neural network (AANN) model for online implementation of text-independent speaker verification system. The distribution capturing ability of an AANN model is exploited to build the speaker model. A rank-based approach using individual background models is used to verify the claim of the speaker.","Neural networks,
Computer science,
Speech,
Biometrics,
Authentication,
Spatial databases,
Transaction databases,
Feature extraction,
Testing,
Feedforward neural networks"
Robust recognition and pose determination of 3-D objects using range images in eigenspace approach,"In this paper we propose a robust method for recognition and pose determination of 3-D objects using range images in the eigenspace approach. Instead of computing the coefficients by a projection of the data onto the eigenimages, we determine the coefficients by solving a set of linear equations in a robust manner. The method efficiently overcomes the problem of missing pixels, noise and occlusions in range images. The results show that the proposed method outperforms the standard one in recognition and pose determination.",
On-line realignment of clients in networked databases,"Upper bounds on the scalability of client-server databases (CSDs) persist due to long delays experienced at the servers' queues. To this end, we have proposed a three-tier architecture that exploits similarities in object access behavior demonstrated by clients. Clients are statically grouped into logical clusters and object requests can be then served in an ""internal to cluster"" fashion. This is achieved with the introduction of an intermediate directory tier. Good client clustering yields more scalable CSD configurations as it minimizes the number of inter-cluster data accesses. We introduce the problem of client realignment in light of changing client localities and propose an online reclustering framework to address it. Online reclustering facilitates adaptive reconfiguration and redistribution of sites. The core of our proposal is a change detection approach that uses meta-data extracted from observed clients' access patterns. We evaluate the impact of employing a multi-featured change detection scheme in the three-tier CSD architecture and experimentally investigate its performance and trade-offs involved.","Intelligent networks,
Databases,
Upper bound,
Scalability,
Delay,
Information science,
Computer architecture,
Proposals,
Data mining,
Network servers"
Maximizing functional cohesion of comprehension environments by integrating user and task knowledge,"Program comprehension tools should facilitate the comprehension strategies used by programmers to achieve specific tasks. Many reverse engineering tools have been developed to derive abstract representations from existing source code and to apply a variety of analysis techniques. Yet, most of these software programs fail to provide users with the necessary guidance in choosing the appropriate methods, tools, abstraction levels and analysis techniques, and they frequently expose the user to unrelated information. The author presents a task and user-centered comprehension environment that maximizes the functional cohesion among the tools and comprehension techniques by focusing on a particular user task and its appropriate comprehension strategy. At the same time, we try to minimize the data coupling for the selected task by providing only the necessary task specific information, therefore reducing the data overload. This environment integrates user specific information with reverse engineered information to select the most appropriate comprehension strategy for a particular task.","Software maintenance,
Reverse engineering,
Software systems,
Computer science,
Programming profession,
Failure analysis,
Information analysis,
Bridges,
Design methodology,
Pattern matching"
Prefetch scheduling for composite hypermedia,Composite hypermedia documents are fast becoming ubiquitous in the Web. More Websites are effectively serving hypermedia however with hypertext technology. We explore a novel segment-based background prefetch technique that utilizes the rendering properties of various individual media elements and their interdependencies. The technique can accelerate surfing in composite hypermedia.,"Prefetching,
Delay,
Rendering (computer graphics),
Acceleration,
Web pages,
HTML,
Hardware,
Processor scheduling,
Internetworking,
Computer science"
Parallel FP-LAPW for distributed-memory machines,"To meet the high computing-power and memory requirements of applying the full-potential linearized augmented plane wave method to complex problems, the authors have parallelized the FP-LAPW code WIEN97 for distributed-memory machines. They then implemented the parallel code on a Cray T3E.","Electrons,
Wave functions,
Magnetic cores,
Computer architecture,
Tin,
Atomic measurements,
Concurrent computing,
Distributed computing,
Silicon,
Graphics"
Lower bounds for quantum communication complexity,"We prove new lower bounds for bounded error quantum communication complexity. Our methods are based on the Fourier transform of the considered functions. First we generalize a method for proving classical communication complexity lower bounds developed by R. Raz (1995) to the quantum case. Applying this method we give an exponential separation between bounded error quantum communication complexity and nondeterministic quantum communication complexity. We develop several other Fourier based lower bound methods, notably showing that /spl radic/(s~(f)/log n) n, for the average sensitivity s~(f) of a function f, yields a lower bound on the bounded error quantum communication complexity of f (x/spl and/y/spl oplus/yz), where x is a Boolean word held by Alice and y, z are Boolean words held by Bob. We then prove the first large lower bounds on the bounded error quantum communication complexity of functions, for which a polynomial quantum speedup is possible. For all the functions we investigate, only the previously applied general lower bound method based on discrepancy yields bounds that are O(log n).","Complexity theory,
Quantum computing,
Protocols,
Quantum mechanics,
Polynomials,
Mechanical factors,
Physics,
Quantum entanglement,
Application software,
Computer science"
Design of a framework for multi-user/application oriented WebGIS services,"In this paper, a framework is presented to provide a new mode for WebGIS services. This framework consists of three layers: User layer, Application layer and WebGIS service layer. User Layer supports users to access WebGIS services via different network environment; application layer represents WebGIS applications constructed by different organizations or companies, which integrate GIS services to solve some domain-oriented problems; WebGIS service layer is the core layer in the framework, which provides different GIS services. Professionals maintain WebGIS service layer unitedly. WebGIS services can be dynamically modified and added, but WebGIS application needn't adjust. If there are no appropriate GIS services in WebGIS service layer, the WebGIS application constructors can order them from WebGIS service layer. WebGIS service layer is a multi-agent system, where WebGIS services are implemented as agents, which can match the use of different users/applications.","Geographic Information Systems,
Application software,
Multiagent systems,
Data visualization,
Buildings,
Artificial intelligence,
Computer science,
Information analysis,
Data analysis,
Database systems"
A motion planning approach to folding: from paper craft to protein folding,"We present a framework for studying folding problems from a motion planning perspective. Modeling foldable objects as tree-like multi-link objects allows one to apply motion planning techniques to folding problems. An important feature of this approach is that it not only allows one to study foldability questions, such as, can an object be folded (or unfolded) into another object, but also provides one with another tool for investigating the dynamic folding process itself. The framework proposed here has application to traditional motion planning areas such as automation and animation, and presents a novel approach for studying protein folding pathways. Preliminary experimental results with traditional paper crafts (e.g., box folding) and small proteins (approximately 60 residues) are quite encouraging.","Proteins,
Packaging,
Computational geometry,
Computer science,
Path planning,
Automation,
Wrapping,
Microscopy,
Process planning,
Assembly"
Dynamic queries and brushing on choropleth maps,"Users who must combine demographic, economic or other data in a geographic context are often hampered by the integration of tabular and map representations. Static, paper-based solutions limit the amount of data that can be placed on a single map or table. By providing an effective user interface, we believe that researchers, journalists, teachers, and students can explore complex data sets more rapidly and effectively. This paper presents Dynamaps, a generalized map-based information visualization tool for dynamic queries and brushing on choropleth maps. Users can use color coding to show a variable on each geographic region, and then filter out areas that do not meet the desired criteria. In addition, a scatterplot view and a details-on-demand window support overviews and specific fact-finding.","User interfaces,
Computer science,
Educational institutions,
Business,
Computational Intelligence Society,
World Wide Web,
Scattering,
Joining processes,
Data visualization,
Demography"
Robust object tracking using an adaptive color model,"In this paper we present a new robust face tracking method based on the condensation algorithm that uses a sampling based density representation. A two-dimensional color model is used to approximate the face color. We modified the condensation algorithm to provide color adaptability to the abrupt change of illumination and to the tracking of differently colored people. According to the face size and location uncertainty, the searching range is automatically determined and it makes the algorithm extremely robust and efficient. The tracker operates at real-time and actively controls a camera pan-tilt in order to locate a person's face in the center of the image. Experimental results show the algorithm's robustness to the agile motion of face and to the dramatic change of illumination in the presence of complex background.","Robustness,
Lighting,
Cameras,
Uncertainty,
Application software,
Human computer interaction,
Computational efficiency,
Computer science,
Sampling methods,
Tracking"
Predictive process simulation and stress-mediated diffusion in silicon,"The silicon-based metal oxide semiconductor field effect transistor (MOSFET) is at the heart of today's semiconductor industry. Because the switching speed of a MOSFET increases linearly with shrinking dimensions, the semiconductor industry has constantly improved computer performance by scaling a more or less unchanged device geometry. Despite the successful history of device miniaturization, scaling is reaching the physical limits of traditional device materials. With the reduction of gate lengths and the use of more exotic materials such as metal gates, the influence of stress on diffusion becomes a more prevalent component in determining the final dopant profile and subsequent device performance. We present the development of a complete predictive simulation capability for the effects of general anisotropic nonuniform stress on dopant diffusion in silicon as an example for modern physical process modeling. We also discuss how to effectively integrate predictive modeling tools such as this into the development of state-of-the-art semiconductor devices.","Predictive models,
MOSFET circuits,
Electronics industry,
Semiconductor materials,
Stress,
Semiconductor process modeling,
Computational modeling,
FETs,
Heart,
Computer performance"
Design and development of a Web-based academic advising system,"Academic advising is an important and time-consuming task and different tools and techniques can be used to make it an effective and efficient process. This paper describes the design and development of a Web-based advising system that supplements the conventional advising process. The system's goals include: to minimize repetitive tasks performed by advisors, to encourage students to adopt a proactive attitude towards advising, to make advising-related information available to remote students in a single place, in electronic format, and to minimize inconsistencies in the advising process. The system supports three different types of users (students, advisors, and secretaries), each of which has different privileges and allowed operations. Student users may use the system to find relevant advising-related information, such as course descriptions and advising FAQs. They can also ask the system which course(s) to take next, based on the classes they have already taken.","Computer science,
Employee welfare,
Processor scheduling,
IEEE news,
HTML"
Affine-invariant sketch-based retrieval of images,"The advent of the digital library and multimedia database require robust techniques for multimedia content searches. Content-based retrieval techniques have been developed to overcome some of the limitations associated with conventional keyword-based browsing or searching of visual data. We present an affine invariant shape-based retrieval technique for image retrieval which is efficient and robust. More importantly, the technique supports a query presented in the form of hand-drawn sketches and has the potential of supporting affine invariant partial shape retrieval.","Image retrieval,
Multimedia databases,
Information retrieval,
Content based retrieval,
Multimedia computing,
Robustness,
Shape measurement,
Data mining,
Computer science,
Application software"
Maintenance support for web sites: a case study,"This paper reports on results gathered during a case study using a Web maintenance tool called Perlbot. The main objective of the study is to observe the history of modifications made on various Web documents. Based on selected metrics, we collect data with respect to how such sites are maintained and evolve. The data gathered support our hypothesis that there is a worldwide tendency to accumulate large numbers of stale documents over long periods of time. Moreover, our quantitative results reveal inconsistent Web maintenance activities spanning over various international sites.","Computer aided software engineering,
Robots,
Web sites,
Internet,
World Wide Web,
Computer science,
Software engineering,
History,
Statistics,
Computer graphics"
Approximating functions for embedded and ASIC applications,"Often embedded programmers and application specific integrated circuit (ASIC) designers are frustrated by the inability to realize near floating-point accuracy. in a fixed-point application. The problem is not limited to function approximation but also impacts FIR filter design. In this paper we examine the problem of approximating a known function on a closed interval and show that a genetic algorithm (GA) may be used to obtain results superior to those obtained by implementing floating-point algorithms, such as Taylor series and Chebyshev polynomials, in fixed-point directly.","Application specific integrated circuits,
Chebyshev approximation,
Polynomials,
Least squares approximation,
Taylor series,
Function approximation,
Application software,
Computer science,
Finite impulse response filter,
Genetic algorithms"
Communication state transfer for the mobility of concurrent heterogeneous computing,"In a dynamic environment, where a process can be migrated from one host to another host, communication state transfer is a key issue of process coordination. This paper presents algorithms for data communication and migration protocols to support communication state transfer in a dynamic, distributed parallel environment. These algorithms collectively presence the semantics of the communication and are practical for large-scale distributed systems. The assumptions and validity of our solution are discussed. Based on our early results in process migration, we implement a prototype system for process state transfer. Experimental results confirm our design is valid and has a true potential in practice.","Concurrent computing,
Data communication,
Protocols,
Sun,
Large-scale systems,
Distributed computing,
Snow,
Computer science,
Prototypes,
Computer network management"
Scalable secure group communication over IP multicast,"We introduce and analyze a scalable re-keying scheme for implementing secure group communications over IP multicast. We show that our scheme incurs constant processing, message, and storage overhead for a re-key operation when a single member joins or leaves the group, and logarithmic overhead for bulk simultaneous changes to the group membership. These bounds hold even when group dynamics are not known a priori. Our re-keying algorithm requires a particular clustering of the members of the secure multicast group. We describe a protocol to achieve such clustering and show that it is feasible to efficiently cluster members over realistic Internet-like topologies. We evaluate the overhead of our own re-keying scheme and also of previously published schemes via simulation over an Internet topology map containing over 280,000 routers. Through analysis and detailed simulations, we show that this re-keying scheme performs better than previous schemes for a single change to group membership. Further, for bulk changes, our algorithm outperforms all previously known schemes by several orders of magnitude in terms of actual bandwidth usage, processing costs and storage requirements.","Multicast algorithms,
Internet,
Topology,
Costs,
Cryptography,
Computer science,
Educational institutions,
Clustering algorithms,
Performance analysis,
Analytical models"
On-line adaptive canonical prefix coding with bounded compression loss,"Semistatic minimum-redundancy prefix (MRP) coding is fast compared with rival coding methods, but requires two passes during encoding. Its adaptive counterpart, dynamic Huffman coding, requires only one pass over the input message for encoding and decoding, and is asymptotically efficient. Dynamic Huffman (1952) coding is, however, notoriously slow in practice. By removing the restriction that the code used for each message symbol must have minimum-redundancy and thereby admitting some compression loss, it is possible to improve the speed of adaptive MRP coding. This paper presents a controlled method for trading compression loss for coding speed by approximating symbol frequencies with a geometric distribution. The result is an adaptive MRP coder that is asymptotically efficient and also fast in practice.",Adaptive coding
Design and implementation of bitmap indices for scientific data,"Bitmap indices are efficient multi-dimensional index data structures for handling complex adhoc queries in read-mostly environments. They have been implemented in several commercial database systems but are only well suited for discrete attribute values which are very common in typical business applications. However, many scientific applications usually operate on floating point numbers and cannot take advantage of the optimisation techniques offered by current database solutions. We thus present a novel algorithm called GenericRangeEval for processing one-sided range queries over floating point values. In addition, we present a cost model for predicting the performance of bitmap indices for high-dimensional search spaces. We verify our analytical results by a detailed experimental study, and show that the presented bitmap evaluation algorithm scales well also for high-dimensional search spaces requiring only a fairly small index. Because of its simple arithmetic structure, the cost model could easily be integrated into a query optimiser for deciding whether the current multi-dimensional query shall be answered by means of a bitmap index or better by sequentially scanning the data values, without using an index at all.","Business,
Data structures,
Predictive models,
Database systems,
Cost function,
Indexing,
Computer science,
Informatics,
Algorithm design and analysis,
Data warehouses"
KAIST interactive bicycle simulator,"This paper presents key technologies and system integration issues of the KAIST interactive bicycle simulator. The rider on the bicycle feels the motion and has the visual experience as if he/she is riding in the campus of the Korea Advanced Institute of Science and Technology. The simulator consists of a bicycle, a Stewart platform, a magnetorheological handle, a pedal resistance system to generate motion feelings, a real-time visual simulator and projection system, sub-controllers and an integrating control network.","Bicycles,
Computational modeling,
Vehicle dynamics,
Immune system,
Motion control,
Aerospace simulation,
Computer simulation,
Mechanical engineering,
Computer science,
Real time systems"
Information granules in spatial reasoning,"The authors consider schemes of approximate reasoning approaches to transforming of information granules from different information sources. By tuning the parameters of schemes, the information granules representing patterns relevant to given tasks can be generated. More specifically, the authors discuss foundations of a calculus on information granules developed on the basis of rough set and rough mereological approaches.","Calculus,
Mathematics,
Computer science,
Information processing,
Intelligent systems,
Object recognition,
Path planning,
Uncertainty,
Rough sets"
Comparing and reconciling usability-centered and use case-driven requirements engineering processes,"During the two last decades, the human-computer interaction community has developed a large variety of techniques and tools for gathering, specifying and validating usability requirements including user characteristics, tasks, work environment as well as usability goals such as effectiveness, efficiency and user satisfaction. Unfortunately, even if their importance are accepted by software developers, they are not yet cost-effectively integrated into software engineering methodologies. This paper presents the rationale for our ACUDUC approach by identifying the different issues for enhancing the use case-driven software requirements approach with RESPECT, one of the most advanced frameworks for user-centered requirements. Beyond this specific example (use cases and RESPECT), our investigations aim to reconcile user-centered and use case-driven requirements engineering and to cross-pollinate software engineering and usability engineering.","Usability,
Software engineering,
Programming,
Interactive systems,
IEC standards,
ISO standards,
Process design,
Object oriented modeling,
Application software,
Computer science"
What should a classifier system learn?,"We consider the issue of how a classifier system should learn to represent a Boolean function. We identify four properties which may be desirable of a representation; that it be complete, accurate, minimal and nonoverlapping, and distinguish variations on two of these properties for the XCS system. We question whether the bias against overlapping rules evident in some systems is appropriate, and find that XCS's bias against overlapping rules is very strong.","Boolean functions,
Computer science,
Testing,
Machine learning,
Monitoring,
Statistics"
Qualitative fuzzy sets: a comparison of three approaches,"A real world fuzzy set should be able to tolerate ""small amounts"" of perturbations. There are three approaches. Thiele's (1999; 2000) approach, observing some weak points of Lin's earlier versions, proposed a refined approach based on Kripke style semantics; his results, but independently, are essentially applying Lin's (1996) view using Lin's (1998) new notion of neighborhood systems. A type II fuzzy set realizes the idea by representing each grade by a fuzzy number. Lin's approach represents a fuzzy set by a ""fuzzy set"" of membership functions. Lin did this in two ways; one by granulation and another by ""physical"" perturbation. In summary, Thiele's approach has a good Kripke style semantics. Type II fuzzifies the grade; its advantage is simplicity. Lin's approach fuzzifies the membership function; its advantage is to formalize the intuitive ""physical"" perturbation. The three approaches are not equivalent, but do have common intersection. Three results are discussed in different universes (categories).","Fuzzy sets,
Mathematics,
Computer science,
Topology,
Numerical analysis"
Joint likelihood methods for mitigating visual tracking disturbances,"We describe a framework that explicitly reasons about data association and combines estimates to improve the tracking performance in many difficult visual environments. This work extends two previously reported algorithms: the probabilistic data association filter (PDAF), which handles single-target tracking tasks involving agile motions and clutter; and the joint probabilistic data association filter (JPDAF), which shares information between multiple same-modality trackers (such as homogeneous regions, textured regions, or snakes). The capabilities of these methods are improved in two steps: first, by a joint likelihood filter that allows mixed tracker modalities when tracking several objects and accommodates overlaps robustly. A second technique, the constrained joint likelihood filter, tracks complex objects as conjunctions of cues that are diverse both geometrically (e.g., parts) and qualitatively (e.g., attributes). Rigid and hinge constraints between part trackers and multiple descriptive attributes for individual parts render the whole object more distinctive, reducing the susceptibility to mistracking. The generality of our approach allows for easy application to different target types, and it is flexibly defined for straightforward incorporation of other modalities.",
TGA: a new integrated approach to evolutionary algorithms,"The genetic algorithm (GA) is a well-known heuristic optimization algorithm. However, it suffers from the serious problem of premature convergence, which is caused mainly by the population diversity decreasing in evolution. The authors propose a novel algorithm, called TGA, which integrates the memory structure and search strategy of Tabu Search (TS) with GA. As such, the selection efficiency is improved and the population diversity is maintained by incorporating the regeneration operator. The traveling salesman problem is used as a benchmark to evaluate TGA and compare it with GA and TS. Experimental results show that TGA achieves better performance than GA and TS in terms of both convergence speed and solution quality.","Evolutionary computation,
Thermodynamics,
Genetic algorithms,
Heuristic algorithms,
Convergence,
Computer science,
Research and development management,
Genetic engineering,
Sun,
Traveling salesman problems"
Hand gesture recognition for man-machine interaction,"Addresses some basic problems of hand gesture recognition. Three steps important in building gestural vision-based interfaces are discussed. The first step is the hand location through detection of skin colored regions. Representative methods based on 2D color histograms are described and compared. The next step is the hand shape (posture) recognition. An approach that uses the morphological hit-miss operation is presented. Finally, application of hidden Markov models in recognition of dynamic gestures is explained. An experimental system that uses the discussed methods in real time is described and recognition results are presented.","Skin,
Shape,
Histograms,
Hidden Markov models,
Human robot interaction,
Image sequences,
Robustness,
Paper technology,
Computer science,
Application software"
Charging for QoS in internetworks,"Pricing is an effective regulatory tool to provide proper incentives so that users' self-interest will lead them to modify their usage according to their needs. This leads to better overall network utilization and enhanced users' satisfaction. In this work, a scalable pricing framework for QoS capable networks supporting real time, adjustable real time, and non-real time traffic is studied. The scheme, which belongs to usage-based methods, is independent of the underlying network and the mechanisms for QoS provisioning. The framework is credit-based ensuring the fairness, comprehensibility, and predictability of usage cost. On the other hand, it provides a means for the network providers to ensure, with high probability, cost recovery and profit, competitiveness of prices, and encouragement of client behavior that will enhance the network's efficiency. This is achieved by appropriate charging mechanisms and suitable incentives. Simulation results suggest that users have better overall satisfaction; providers are able to recover costs; better network utilization is achieved while reduced call blocking probability is observed. The implementation and usage costs of the framework are low.",
Expiration of historical databases,"We present a technique for automatic expiration of data in a historical data warehouse that preserves answers to a known and fixed set of first-order queries. In addition, we show that for queries with output size bounded by a function of the active data domain size (the number of values that have ever appeared in the warehouse), the size of the portion of the data warehouse history needed to answer the queries is also bounded by a function of the active data domain size and therefore does not depend on the age of the warehouse (the length of the history).","Databases,
History,
Data warehouses,
Warehousing,
Computer science,
Data analysis,
Encoding,
Engines,
Algebra,
Logic"
Simulation based HPC workload analysis,"Before implementing scheduling policies (i.e. job prioritization) on a system, it is imperative that their effects on performance be understood. Changing policies without this knowledge may result in issues such as job starvation, increased queue time, and decreased system utilization. This paper proposes a means of reproducibly and accurately determining the true impact of changes in scheduling policy, resource configuration, and workload distribution. The proposed solution, the Maui Scheduler possesses an advanced, easy-to-use, integrated simulator capable of simulating and producing statistics to analyze the impact of an immense array of real world system configurations, policy sets, and workloads. This paper describes the capabilities and use of Maui's internal simulator and demonstrates these capabilities by way of a number of real world examples.",
Continuous global evidence-based Bayesian modality fusion for simultaneous tracking of multiple objects,"Robust, real-time tracking of objects from visual data requires probabilistic fusion of multiple visual cues. Previous approaches have either been ad hoc or relied on a Bayesian network with discrete spatial variables which suffers from discretisation and computational complexity problems. We present a new Bayesian modality fusion network that uses continuous domain variables. The network architecture distinguishes between cues that are necessary or unnecessary for the object's presence. Computationally expensive and inexpensive modalities are also handled differently to minimise cost. The method provides a formal, tractable and robust probabilistic method for simultaneously tracking multiple objects. While instantaneous inference is exact, approximation is required for propagation over time.",
Fault tolerant control strategy for OmniKity-III,"In this paper a novel wheeled omnidirectional mobile robot, the OmniKity-III (OK-III), is developed and its kinematic model and control laws are derived. A special gear train with omnidirectional mechanism was developed using conventional tire wheels. Several control laws are provided by considering the robot as a redundantly actuated wheeled mobile robot on the plane. Since the robot has three degrees of freedom in motion, failure in any one of the motor can be compensated by the other two motors in posture control although the omnidirectional mobility is lost. Simulations and experiments demonstrate the effectiveness of the proposed omnidirectional mechanism.","Fault tolerance,
DC motors,
Wheels,
Mobile robots,
Gears,
Kinematics,
Tires,
Teeth,
Computer science,
Motion control"
Convergence of the simultaneous algebraic reconstruction technique (SART),"In 1984, the simultaneous algebraic reconstruction technique (SART) was developed as a major refinement of the algebraic construction technique (ART). Since then, the SART approach remains a powerful tool for iterative image reconstruction. However, the convergence of the SART has never been established. This long-standing conjecture is proven under the condition that coefficients of the linear imaging system are non-negative. It is shown that from any initial value the sequence generated by the SART converges to a weighted least square solution. The importance of the SART and several relevant issues are also discussed.",
Password-capabilities: their evolution from the password-capability system into Walnut and beyond,"Since we first devised and defined password capabilities as a new technique for building capability-based operating systems, a number of research systems around the world have used them as the bases for a variety of operating systems. Our original Password-Capability System was implemented on custom built hardware with a novel address translation and protection scheme specifically designed to support password-capabilities. The password-capability concept later formed the basis of Opal developed at the University of Washington, and Mungi from the University of New South Wales, both of which used commercially available hardware. A second generation password-capability based system, Walnut, was developed at Monash University in the 1990s. Walnut was designed to run on commercially available hardware. It addressed some shortcomings of the original Password-Capability System but had to sacrifice some features that depended on hardware support. A third generation system that will extend Walnut to support mandatory security policies and other advanced features is currently being considered. This paper analyses the evolution of the Password-Capability System into Walnut, examines the shortcomings of the systems, and identifies issues to be addressed in the new system.","Hardware,
Protection,
Operating systems,
Permission,
Security,
Tagging,
Forgery,
Cryptography,
Computer science,
Software engineering"
Variational calculus approach to multiresolution image mosaic,"Image mosaic combines two or more images. It has found many applications in computer vision, image processing, and computer graphics. A common goal of the problem is to join two or more images such that there is an invisible boundary around the seam line and the mosaic image is as little distorted from the original images as possible. We propose a new image mosaic method by wavelet multiresolution analysis and variational calculus. We first project the images into wavelet spaces. The projected images at each wavelet space are then blended. Variational calculus techniques are applied to balance the quality between the smoothness around the seam line and the fidelity of the combined image relative to the original images in image blending. A mosaic image is finally obtained by summing the blended images at the wavelet spaces. Experimental results based on our method are demonstrated.","Calculus,
Image resolution,
Wavelet analysis,
Multiresolution analysis,
Information science,
Image processing,
Computer graphics,
Buildings,
Laplace equations,
Discrete wavelet transforms"
Extended performance graphs for cluster retrieval,"Performance evaluations in probabilistic information retrieval are often presented as precision-recall or precision-scope graphs avoiding the otherwise dominating effect of the embedding irrelevant fraction. However, precision and recall values as such offer an incomplete overview of the information retrieval system under study: information about system parameters like generality (the embedding of the relevant fraction), random performance, and the effect of varying the scope is missed In this paper two cluster performance graphs are presented In those cases where complete ground truth is available (both cluster size and database size) the cluster precision-recall (Cluster PR) graph and the generality-precision=recall graph are proposed.","Information retrieval,
Image retrieval,
Content based retrieval,
Computer science,
Image databases,
Audio databases,
Books,
NIST,
Area measurement,
Performance analysis"
Applications of discrete event simulation modeling to military problems,"The military is a big user of discrete event simulation models. The use of these models range from training and wargaming their constructive use in important military analyses. In this paper we discuss the uses of military simulation, the issues associated with military simulation to include categorizations of various types of military simulation. We then discuss three particular simulation studies undertaken with the Air Force Institute of Technology's Department of Operational Science focused on important Air Force and Army issues.","Discrete event simulation,
Computational modeling,
Resource management,
Computer simulation,
Analytical models,
Uncertainty,
Weapons,
Aerospace simulation,
Aggregates,
Mathematical model"
"A secure, publisher-centric Web caching infrastructure","The current Web caching infrastructure, though it has a number of performance benefits for clients and network providers, does not meet publishers' requirements. We argue that to satisfy these requirements, caches should be enhanced in both the data and control planes. In the data plane, caches will dynamically generate content for clients by running code provided by publishers. In the control plane, caches will return logs of client accesses to publishers. In this paper, we introduce Gemini, a system which has both of these capabilities, and discuss two of its key components: security and incremental deployment. Since Gemini caches are deeply involved in content preparation and logging, ensuring that they perform correctly is vital. Traditional end-to-end security mechanisms are not sufficient to protect clients and publishers, so we introduce a new security model which consists of two pieces: an authorization mechanism and a verification mechanism. The former allows a publisher to authorize a set of caches to run its code and serve its content, while the latter allows clients and publishers to probabilistically verify that authorized caches are operating correctly. Because it is unrealistic to assume that Gemini caches will be deployed everywhere simultaneously, we have designed the system to be incrementally deployable and to coexist with legacy clients, caches, and servers. Finally, we describe our implementation of Gemini and present preliminary performance results.",
Division with limited precision primitive operations,"We present a digit-recurrence division algorithm which uses limited-precision primitive operators consisting of multipliers, adders/subtractors, and table-lookups. The scheme is suitable for modular implementation. We describe the algorithm and develop a design. Comparisons is made for radix-512 with a division scheme with prescaling. The proposed scheme is estimated to have 28% longer cycle time at a 40% area reduction with a corresponding effect on power dissipation making the scheme interesting for low-power designs.","Convolution,
Computer science,
Algorithm design and analysis,
Error compensation,
Delay"
Minimal CDMA recoding strategies in power-controlled ad-hoc wireless networks,,"Multiaccess communication,
Intelligent networks,
Wireless networks,
Ad hoc networks,
Spread spectrum communication,
Transmitters,
Computer science,
Packet radio networks,
Heuristic algorithms,
Base stations"
Routing with a clue,"We suggest a new simple forwarding technique to speed up IP destination address lookup. The technique is a natural extension of IP, requires 5 bits in the IP header (IPv4, 7 in IPv6), and performs IP lookup nearly as fast as IP/Tag switching but with a smaller memory requirement and a much simpler protocol. The basic idea is that each router adds a ""clue"" to each packet, telling its downstream router where it ended the IP lookup. Since the forwarding tables of neighboring routers are similar, the clue either directly determines the best prefix match for the downstream router, or provides the downstream router with a good point to start its IP lookup. The new scheme thus prevents repeated computations and distributes the lookup process across the routers along the packet path. Each router starts the lookup computation at the point its upstream neighbor has finished. Furthermore, the new scheme is easily assimilated into heterogeneous IP networks, does not require routers coordination, and requires no setup time. Even a flow of one packet enjoys the benefits of the scheme without any additional overhead. The speedup we achieve is about 10 times faster than current standard techniques. In a sense, this paper shows that the current routers employed in the Internet are clue-less; namely, it is possible to speed up the IP lookup by an order of magnitude without any major changes to the existing protocols.","Routing,
Multiprotocol label switching,
Protocols,
Computer science,
Distributed computing,
IP networks,
Internet,
Packet switching,
Bandwidth"
Learning of structural descriptions of graphic symbols using deformable template matching,"Accurate symbol recognition in graphic documents needs an accurate representation of the symbols to be recognized. If structural approaches are used for recognition, symbols have to be described in terms of their shape, using structural relationships among extracted features. Unlike statistical pattern recognition, in structural methods, symbols are usually, manually defined from expertise knowledge, and not automatically, inferred from sample images. In this work we explain one approach to learn from examples a representative structural description of a symbol, thus providing better information about shape variability. The description of a symbol is based on a probabilistic model. It consists of a set of lines described by, the mean and the variance of line parameters, respectively, providing information about the model of the symbol, and its shape variability. The representation of each image in the sample set as a set of lines is achieved using deformable template matching.","Shape,
Pattern recognition,
Cost function,
Computer graphics,
Pattern matching,
Computer vision,
Computer science,
Feature extraction,
Multi-stage noise shaping,
Uncertainty"
Reliability of systems of independently developable end-user assessable logical (IDEAL) programs,"Computers are being used to automate critical services, including manufacturing systems, transportation, etc. For these critical applications, it is necessary to be able not only to achieve high quality but also to rigorously demonstrate that high quality has in fact been achieved. One approach that is used to facilitate prevention as well as detection of software faults is to decompose the requirements specification into more manageable portions. However, this does not necessarily enable the demonstration of high quality. This paper discusses a method of decomposing software into aspects that allows the system reliability to be inferred from the aspect reliabilities. Each aspect is independently developable, i.e., it can be designed and implemented independently of the other aspects in the system. In addition, each aspect is end-user assessable, i.e., it can be tested or verified by the end-user independently of any other aspect. We identify five classes of IDEAL (Independently Developable End-user Assessable Logical) aspects and, for each class, we present the conditions that must be satisfied in order to compute the system reliability from the aspect reliabilities.","Manufacturing systems,
Application software,
Fault detection,
Reliability,
Instruments,
Artificial satellites,
Control systems,
Software quality,
Computer science,
Computer aided manufacturing"
The architecture of a framework for building distributed learning environment,"Distributed learning environments (DLEs) are being cited as solutions to the ambitious political goals of better education, wider access and lower costs. The paper outlines the architecture of the TAGS framework (C. Allison et al., 2000) for building DLEs. The main features that the architecture facilitates are strong support for group work, anytime/anywhere access, the input of real world data, event monitoring facilities, authentication, authorisation and protection, and the controlling of delay between user actions and system responses. These features have required the development of an expanded view of quality of service (QoS) parameters. The architecture has been refined over a number of years as the result of collaborative development between users and programmers. In particular, we have found that the bringing together of systems programmers, computer science researchers and subject-specific educationalists has allowed needs to be identified and addressed effectively. We believe that TAGS makes it possible for educationalists to easily build DLEs that meet their needs.","Buildings,
Technical Activities Guide -TAG,
Quality of service,
Programming profession,
Costs,
Monitoring,
Authentication,
Authorization,
Protection,
Control systems"
Stationary background generation in mpeg compressed video sequences,,"Transform coding,
Video sequences,
MPEG 4 Standard,
Video compression,
Streaming media,
Discrete cosine transforms,
Video coding,
Image segmentation,
Image edge detection,
Computer science"
A randomized error recovery algorithm for reliable multicast,"An efficient error recovery algorithm is essential for a liable multicast in large groups. Tree-based protocols (RMTP, TMTP, LBRRM) group receivers into local regions and select a repair server for performing error recovery in each region. Hence a single server bears the entire responsibility of error recovery for a region. In addition, the deployment of repair servers requires topological information of the underlying multicast tree, which is generally not available at the transport layer. This paper presents RRMP, a randomized reliable multicast protocol which improves the robustness of tree-based protocols by diffusing the responsibility of error recovery among all members in a group. The protocol works well within the existing IP multicast framework and does not require additional support from routers. Both analysis and simulation results show that the performance penalty due to randomization is low and can be tuned according to application requirements.","Multicast algorithms,
Multicast protocols,
Computer errors,
Application software,
Collaborative software,
Algorithm design and analysis,
Error correction,
Transport protocols,
Computer science,
Robustness"
Behavioural modelling of operational amplifier faults using analogue hardware description languages,"The use of behavioural modelling for operational amplifiers has been well known for many years and previous work has included modelling of specific fault conditions using a macro-model. In this paper, the models are implemented in a more abstract form using analogue hardware description languages (HDL), including MAST, taking advantage of the ability to control the behaviour of the model using high-level fault condition states. The implementation method allows a range of fault conditions to be integrated without switching to a completely new model. The various transistor faults are categorised, and used to characterise the behaviour of the HDL models. Simulations compare the accuracy and speed of the transistor and behavioural level models under a set of representative fault conditions.","Operational amplifiers,
Hardware design languages,
Circuit faults,
Circuit simulation,
Circuit testing,
Fault diagnosis,
Integrated circuit modeling,
Computer science,
Time measurement,
Bandwidth"
Two low cost algorithms for improved diagonal edge detection in JPEG-LS,JPEG-LS is the latest lossless and near lossless image compression standard introduced by the Joint Photographic Experts Group (JPEG) in 1999. In this standard simple localized edge detection techniques are used in order to determine the predictive value of each pixel. These edge detection techniques only detect horizontal and vertical edges and the corresponding predictors have only been optimized for the accurate prediction of pixels in the locality of horizontal and/or vertical edges. As a result JPEG-LS produces large prediction errors in the locality of diagonal edges. We present two low cost algorithms for the detection and prediction of diagonal edge pixels in JPEG-LS. Experimental results show that the proposed schemes aid in the reduction of predictive mean squared error of up to 2-3 percent as compared to the standard.,
Poisson distributions,"Scientists and engineers often use Poisson's probability distribution to characterize the statistics of rare events whose average number is small. Using it correctly is crucial if we are to validate claims of discovery of new phenomena, such as a new fundamental particle (few candidate collision events among millions), a remote galaxy (few photons in the telescope among the billions emitted), or brain damage from using cell phones (few tumors among millions of users). In risk assessment, such as estimating the chance of dying from a horse kick if you're in the Prussian army or from suicide (two of its early uses), it plays a crucial role, which should interest actuaries as well as morticians. The author has noticed that the Poisson distribution is often misunderstood and misapplied, so he describes some of its interesting and relevant properties. He emphasizes visualizing what's happening in the mathematics by using Mathematica.",
Improved table lookup algorithms for postscaled division,"Postscaled division is a non-iterative algorithm delivering a quotient of single precision accuracy by the three term product (xy/spl circ/)c and of double precision accuracy by the formula [(xy/spl circ/)c][2-(yy/spl circ/)c]. Here x is the dividend, y/spl circ/ is a low order part complemented form of the divisor y, and c is a table lookup value approximating a ""reciprocal function"" 1/(yy/spl circ/) to a precision of over 27 bits. Table lookup latency is hidden by performing the lookup in parallel with the first multiplication (xy/spl circ/), with the second multiplication the ""postscaling"" by the lookup function value. Our contribution is the description of two new lookup algorithms for approximating the reciprocal function 1/yy/spl circ/ to high accuracy in fewer cycles than a typical floating point multiply latency. Our indirect bipartite lookup procedure has a latency of two successive lookups followed by a small integer addition. This first algorithm generates a 27 bit approximation of 1/yy/spl circ/ with total table size about 5 Kbytes. Our second lookup algorithm generates a 34 bit approximation with latency determined by 11 and 12 bit table lookups and a 4-1 addition. This second approximation employs some 20 Kbytes of tables to allow for a double extended precision division result in the same number of cycles as a double precision result.","Table lookup,
Delay,
Approximation algorithms,
Computer science,
Arithmetic,
Acceleration,
Indium tin oxide,
Interpolation,
Concurrent computing"
Data-driven process decomposition for circuit synthesis,"This paper presents a new method for decomposing a high-level program description of a circuit into a collection of small modules in the same program notation. The modules are simple enough to be immediately implemented as transistor networks. The method consists of two main steps: the description is first put into Dynamic Single Assignment form in which each variable is assigned at most once in each execution; secondly, the DSA program is projected on different sets of variables and channels to produce each module. The decomposed system is semantically equivalent to the original one under the assumption of slack elasticity: the communicating channels between the modules can have an arbitrary slack (buffer length). In some cases, the decomposition produces modules that are too small and have to be grouped together. Unlike all other methods for circuit synthesis from high-level programs, this method is not syntax-directed. Rather it is based on data dependency among the different variables and channels of the system. The method is general: it is not restricted to asynchronous implementation.","Circuit synthesis,
Network synthesis,
Microprocessors,
Computer science,
Elasticity,
Very large scale integration,
Frequency,
Prototypes,
Throughput,
Large-scale systems"
A GUI environment to manipulate FSMs tor testing GUI-based applications in Java,"The development of GUI-based applications has raised a lot of new issues, one of them being how to effectively test complicated graphical user interactions. We present a visual environment for manipulating test specifications of GUI-based applications in Java. In our approach, the internal representation of a test specification, which contains the contexts of GUI input and output, is generated interactively by running the application under test (AUT). In this way, existing testing tools, such as tools for test case generation, can possibly be applied on it. We provide a graphical interface to obtain such kind of internal test specifications so that testers do not need to know the details of the internal representation, and the test specification can be easily modified. We present our running prototype which lets users graphically manipulate the test specification given in the form of a finite state machine, and the implementation of AUT is a GUI-based Java application.","Graphical user interfaces,
Java,
System testing,
Application software,
Software tools,
Automatic testing,
Computer science,
Prototypes,
Automata,
Software design"
Integrating color and spatial features for content-based video retrieval,"We present a novel scheme for content-based video retrieval by exploring the spatio-temporal information. A shot with significant content changes can be segmented into several subshots that are of coherent content, and a shot similarity measure for video retrieval can be computed from the similarity between corresponding subshots. To characterize the temporal content variations in one shot, we developed two descriptors: dominant color histograms (DCH) and spatial structure histograms (SSH). By fusing temporal information into color content, DCHs for a ""group of frames"" (GoF) are trying to capture the dominant colors with long durations, which would be the colors of the focused objects or background. SSH is a set of features extracted from color-blob maps to describe spatial information for one individual frame. Experimental results on real-world sports videos prove that our proposed approach achieves the best performance on the average recall (AR) and average normalized modified retrieval rank (ANMRR) for video shot retrievals.","Content based retrieval,
Histograms,
Data mining,
Information retrieval,
Image retrieval,
Image color analysis,
Laboratories,
Computer science,
Focusing,
Feature extraction"
Automated design of finite state machine predictors for customized processors,"Customized processors use compiler analysis and design automation techniques to take a generalized architectural model and create a specific instance of it which is optimized to a given application or set of applications. These processors offer the promise of satisfying the high performance needs of the embedded community while simultaneously shrinking design times. Finite State Machines (FSM) are a fundamental building block in computer architecture, and are used to control and optimize all types of prediction and speculation, now even in the embedded space. They are used for branch prediction, cache replacement policies, and confidence estimation and accuracy counters for a variety of optimizations. In this paper we present a framework for automated design of small FSM predictors for customized processors. Our approach can be used to automatically generate small FSM predictors to perform well over a suite of applications, tailored to a specific application, or even a specific instruction. We evaluate the use of these customized FSM predictors for branch prediction over a set of benchmarks.",
Information granulation via neural network-based learning,"This paper concerns with an information granulation approach that is based on neural network learning. The approach involves three key phases. First, information granules are induced in the space of numerical data via a soft competitive learning algorithm with the ability to automatically determine the granularity level needed to properly model the data. Then, information granules are fuzzified, i.e. quantified in terms of fuzzy sets and used as building blocks of a fuzzy rule-based model. Finally, a supervised learning phase is applied to adjust the shape and the distribution of fuzzy granules. The approach is illustrated with the aid of a numerical example that provides insight into the validity of the induced granules and their effect on the results of computing.",
Robust pixel unmixing,"Pixel unmixing is commonly performed by employing a least squared (LS) error criterion, making it sensitive to outliers. As an alternative, the least median of squares (LMedS) method is proposed. Not only is it extremely robust, but it is efficient and straightforward both to implement and use.",
Path-planning algorithms for public transportation systems,"Computing travel plans for desired trips in public transportation systems is not exactly the same as finding a shortest driving path in a given area. Path planning in the context of public transportation systems must consider the route constraint that public vehicles serve on particular paths and that passengers cannot order the drivers to change the bus routes. Explicit representation of the route constraint helps us to design efficient algorithms that focus on viable routes for computing travel plans of interest. This paper presents two strategies for capturing the route constraint. The first strategy employs connectivity matrices, and applies special properties of matrices for quickly identifying feasible travel plans for the desired trips. The second strategy uses hubs where many service routes concentrate for computing travel plans. Our algorithms perform very well in field tests.",
Finding connected components in digital images,An efficient algorithm is presented to label the connected components in an array representation of 2D images. A new data structure is suggested to maintain the equivalence table that makes use of a modified version of union-find algorithms. The operations supported by the equivalence table enable aggressive reuse of labels and (lower) bound the size of the table to [N/2]+1 for an N/spl times/N image. It is also shown that the maintenance of the table has an overall linear amortized cost.,"Digital images,
Pixel,
Computer science,
Data structures,
Maintenance engineering,
Costs,
Computer vision,
Image processing,
Application software"
Charge-sharing alleviation and detection for CMOS domino circuits,"Charge sharing, which occurs in any complementary metal-oxide-semiconductor (CMOS) domino gate, may degrade the output voltage level or may even cause an erroneous output value. In this paper, this problem is thoroughly investigated by considering circuit topology and circuit function. We describe a method to measure the sensitivity [called charge-sharing (CS) vulnerability] of the CS problem for each domino gate. A method to derive the CS vulnerability and the test vector for each domino gate is suggested. We also propose a transistor reordering method to dramatically reduce the CS vulnerabilities for all domino gates so that the CS problem can be alleviated. We also prove theoretically that a set of test vectors generated for single charge-sharing faults (SCSFs) can also detect all multiple charge-sharing faults (MCSFs). This good property significantly guarantees the test quality for the CS faults of domino circuits.",
Incorporating PSP into a traditional software engineering course: an experience report,This paper presents an approach to incorporate PSP (Personal Software Process) into a traditional software engineering course that is typically contained within a computer science curriculum. Advantages and disadvantages of similar approaches are discussed. The approach has been implemented twice in an undergraduate course at the University of Memphis. This successful experience is described and gives support that the proposed approach is beneficial to both students and educators.,"Software engineering,
Computer science,
Education,
Laboratories,
Programming,
Educational institutions,
Investments,
Object oriented modeling,
Conducting materials"
Approximating directed multicuts,"The seminal paper of F.T. Leighton and S. Rao (1988) and subsequent papers presented approximate min-max theorems relating multicommodity flow values and cut capacities in undirected networks, developed the divide-and-conquer method for designing approximation algorithms, and generated novel tools for utilizing linear programming relaxations. Yet, despite persistent research efforts, these achievements could not be extended to directed networks, excluding a few cases that are ""symmetric"" and therefore similar to undirected networks. The paper is an attempt to remedy the situation. We consider the problem of finding a minimum multicut in a directed multicommodity flow network, and give the first nontrivial upper bounds on the maxflow-to-min multicut ratio. Our results are algorithmic, demonstrating nontrivial approximation guarantees.",
Test generation for time critical systems: Tool and case study,"Generating timed test sequences by hand is error-prone and time consuming, and it is easy to overlook important scenarios. The paper presents a tool based on formal methods that automatically computes a test suite for conformance testing of time critical systems. The generated tests are selected on the basis of a coverage criterion of the specification. The tool guarantees production of sound test cases only, and is able to produce a complete covering test suite. We demonstrate the tool by generating test cases for the Philips Audio Protocol.","System testing,
Computer aided software engineering,
Acoustic testing,
Automatic testing,
Protocols,
Humans,
Timing,
Automata,
Sequential analysis,
Computer science"
Low-cost high-performance scientific visualization,"The authors discuss the development of a low-cost stereoscopic visualization system using commonly available components. The system is used to improve understanding about the field-line structure and associated dynamics, confinement, and geometry of spheromak plasma. Such a system might interest research groups doing remote large-scale computing.","Hardware,
Graphics,
Rendering (computer graphics),
Linux,
Polarization,
Open source software,
Data visualization,
Geometry,
Costs,
Filters"
Range estimation from a pair of omnidirectional images,We address the problem of recovering range information from a pair of images obtained from camera viewpoints whose relative poses are known (stereo vision). The images we deal with are cylindrical re-projections of images captured by an omnidirectional vision sensor. Images obtained by such a device have a relatively low spatial resolution in comparison to standard camera images. We analyze the sensitivity of range estimation from image correspondences with respect to errors caused by discretization. The analysis reveals the significance of obtaining sub-pixel accuracy for range estimation. We present our method and experimental results. The results indicate that high accuracy range estimates can be obtained using our method.,"Image sensors,
Cameras,
Mirrors,
Stereo vision,
Solid modeling,
Robot sensing systems,
Spatial resolution,
Machine vision,
Geometry,
Computer science"
Grounded auditory development by a developmental robot,"A developmental robot is one that learns and practices autonomously in the real physical world by interacting with the environment through sensors and effecters, probably under human supervision. The study of developmental robots is motivated by the autonomous developmental process of higher animals and humans from infancy to adulthood. Our goal is to enable a robot to learn autonomously from real-world experiences. The paper presents a case study of a developmental robot developing its auditory related behaviors to follow human trainers' voice commands. A learning architecture is proposed to resolve automatic representation generation and selective attention issues. Both simulation results and experiments on a real robot are reported to show the effectiveness.","Robot sensing systems,
Human robot interaction,
Robotics and automation,
Computer science,
Animals,
Computational modeling,
Analytical models,
Computer simulation,
Design engineering,
Robot control"
Blendeforming: ray traceable localized foldover-free space deformation,"Since the introduction of Free Form Deformation (FFD) by Sederberg and Parry (1986), a great deal of research has been performed in the area of space deformation for 3D shape modeling. However, many techniques do not provide local control, and almost all are non-unique mappings that are capable of producing foldover, making them unsuitable for use with implicit surfaces, or any object that should not self-intersect, and causing some problems for ray tracing. In this paper a new approach to deformation is presented. Blendeforming (Blended Deforming) provides foldover-free deformation with local control, which preserves implicit functions. Blendeformers are reversible deformations that are suitable for use in interactive modeling and ray tracing.","Ray tracing,
Lattices,
Deformable models,
Ellipsoids,
Shape,
Computer science,
Control systems,
Weight control,
Spline,
Surface topography"
GroupProcess: using process knowledge from the participative design and practical operation of ad hoc processes for the design of structured workflows,"Many of today's companies already have integrated workflow management systems (WFMS) within their IT-infrastructure which are mainly used for the core processes of the company. Furthermore these predefined processes are designed and implemented by specialists, whilst the process knowledge of the involved employees remains mostly unused. The daily business life, especially in office environments, often additionally requires flexible, rather short lived processes (ad hoc workflows), that can only be predetermined in advance to some degree. These two types of workflows have many interdependencies which are inadequately or not supported by currently available WFMS. For example some ad hoc processes that are used more than once and therefore get well tried by practical experience, could become established and important processes for the company. Thus these should be transitioned into predefined workflows in a traditional WFMS. The GroupProcess project examines the broad possibilities to support ad hoc processes in companies and creates connections to existing systems like WFMS, knowledge- and office-management systems.","Electrical capacitance tomography,
Process design,
Companies,
Prototypes,
Application software,
Collaborative software,
Collaborative work,
Computer architecture,
Workflow management software,
Environmental management"
Exact GP schema theory for headless chicken crossover and subtree mutation,"A new general genetic programming (GP) schema theory for headless chicken crossover and subtree mutation is presented. The theory gives an exact formulation for the expected number of instances of a schema at the next generation, either in terms of microscopic quantities or in terms of macroscopic ones. This paper gives examples which show how the theory can be specialised to specific operators.","Genetic mutations,
Genetic programming,
Computer science,
Microscopy,
Mathematics,
Shape"
Prototype of Cyber Teaching Assistant,"To improve computer-based education courses, we constructed support facilities, which enable interactive communication between teacher and students. When a student falls into a difficult situation, he can send a help request to the teacher's cockpit. When the teacher detects a problem situation, he can answer the question by voice communication or can try to resolve it by remote control operation. However, if many questions rush to the teacher's cockpit, he has to contact each student in order. Therefore, students have to wait for a long time. To improve this problem, we developed a prototype of the Cyber Teaching Assistant (CTA). CTA is a Cyber Person software based on 3D computer graphics installed in the student's PC. CTA reads Japanese text aloud by voice synthesizer with facial expression and plays specified body actions according to the scenario saved in the network file server. The teacher can start any CTA with an appropriate scenario by remote control operation. When the teacher detects the student's question or help request and if the student's difficult situation could be resolved by a typical or easy answer, he can start the CTA with an appropriate scenario. As a result, CTA provides a reduction in student waiting time.","Prototypes,
Computer aided instruction,
Communication system control,
Synthesizers,
Computer displays,
Computer science education,
Computer graphics,
File servers,
Seminars,
Microphones"
Continuous estimation of distribution algorithms with probabilistic principal component analysis,"Many evolutionary algorithms have been studied to build and use a probability distribution model of the population for optimization problems. Most of these methods tried to represent explicitly the relationship between variables in the problem with factorization techniques or a graphical model such as Bayesian or Gaussian networks. Thus enormous computational cost is required for constructing those models when the problem size is large. We propose a new estimation of distribution algorithm by using probabilistic principal component analysis (PPCA) which can explain the high order interactions with the latent variables. Since there are no explicit search procedures for the probability density structure, it is possible to rapidly estimate the distribution and readily sample the new individuals from it. Our experimental results support that the presented estimation of distribution algorithms with PPCA can find good solutions more efficiently than other EDAs for the continuous spaces.","Principal component analysis,
Bayesian methods,
Evolutionary computation,
Probability distribution,
Electronic design automation and methodology,
Clustering algorithms,
Algorithm design and analysis,
Artificial intelligence,
Computer science,
Marine vehicles"
Quantifying rollback propagation in distributed checkpointing,"Proposes a new classification of executions with checkpoints that is based on the notion of k-rollback, indicating the maximal number of checkpoints that may need to be rolled back during recovery. The relation between known execution classes is explored, and it is shown that coordinated checkpointing, SZPF (strictly Z-path free) and ZPF (Z-path free) are 1-rollback mechanisms, while ZCF (Z-cycle free) is (n-1)-rollback, where n is the number of participants in an execution. A new class of executions, called d-BC (d-bounded cycles), is introduced, and is shown to be an [(n-1)/spl middot/d]-rollback mechanism (ZCF is a special case of d-BC for d=1). Finally, a d-BC protocol is presented. This protocol has the nice property that it does not impose any control information overhead on an application's messages, yet it only sends a few control messages of its own. Moreover, the protocol maintains information about recovery lines, which enables very efficient discovery of the most recent recovery line that existed a short time before the failure.","Checkpointing,
Application software,
Protocols,
Computer science,
Electronic mail,
Fault tolerant systems,
Software debugging,
Information retrieval,
Distributed computing"
"Path selection in the structural testing: proposition, implementation and application of strategies","Structural testing criteria help the tester in the generation and evaluation of a test case set T. They are predicates to be satisfied to consider the testing activity ended and generally require the execution of a set P of paths, capable of exercising certain elements in the program under testing. Determining P is an important and hard task, and its automation is strongly desirable for easing the criteria application. This task can influence on the efficacy and on the testing effort and costs. This work explores the use of diverse programs characteristics to propose strategies for selection of testing paths. The work also describes a module that implements a framework for representation and automation of those strategies. Using this module, a testing procedure is presented and a strategy, that uses the number of predicates to select paths, is evaluated. The obtained results give some information about the main advantage of this strategy: to easy the automatic test data generation by reducing the number of selected infeasible paths.","Automatic testing,
Software testing,
Flow graphs,
Automation,
Costs,
Software metrics,
Software engineering,
Software quality,
Input variables"
A statistical Lempel-Ziv compression algorithm for personal digital assistant (PDA),"We present a lossless data compression technique called the statistical Lempel-Ziv (1997, 1978) compression algorithm for personal digital assistants (PDAs). This compression algorithm may be viewed as a variant of the LZ77 and the contribution of this algorithm is to include the statistical properties of the source information while most of the LZ-based compression methods, such as LZ78 and LZW do not take this property into consideration. In addition, a prefix entropy-coding scheme is designed to improve the lookup table time for decoding. These prefix codes are especially suitable for PDAs in certain situations. The decoding of prefix codes works very fast by using simple logical and arithmetic operations.","Personal digital assistants,
Compression algorithms,
Data compression,
Books,
Decoding,
Electronic publishing,
Clocks,
Frequency,
Computer science,
Arithmetic"
Algebras for hazard detection,"Hazards pulses are undesirable short pulses caused by stray delays in digital circuits. Such pulses not only may cause errors in the circuit operation, but also consume energy, and add to the computation time. It is therefore very important to detect hazards in circuit designs. Two-valued Boolean algebra, which is commonly used for the analysis and synthesis of digital circuits, cannot detect hazard conditions directly. To overcome this limitation several multi-valued algebras have been proposed for hazard detection. This paper surveys these algebras, and studies their mathematical properties. Also, some recent results unifying most of the multi-valued algebras presented in the literature are described. Our attention in this paper is restricted to the study of static and dynamic hazards in gate circuits.","Hazards,
Computer science,
Pulse circuits,
Boolean algebra,
Delay,
Digital circuits,
Computer errors,
Circuit synthesis,
Circuit analysis,
Books"
An observational theory of integration testing for component-based software development,"Integration testing plays a crucial role in component-based software development. Complementary to the existing works on the selection of test cases and measurement of test adequacy in integration testing, the paper focuses on questions about how to observe the behaviours of a large and complicated system during dynamic testing. We first analyse the structure of white-box integration testing and propose a family of integration testing methods. We then discuss and formalise the requirements of proper uses of test drivers and component stubs in incremental integration. Finally, we propose a set of axioms for integration testing of concurrent systems.","Software testing,
Programming,
System testing,
Software systems,
Design methodology,
Unified modeling language,
Software architecture,
Genetic mutations,
Helium,
Computer science"
A practical model for the development of Web based interactive courses,"Web based courses are very important for distance education, the core business of the authors' university. Hence, they developed a model for the design of attractive and motivating web courses, which is easy to use and learn, and independent of browser and composing tool. The model presents learning atoms in consecutive steps on the screen, using interaction, images, audio, exercises and feedback, but little text per step. The learning material itself is entered in separate documents, following strict guidelines on grouping and naming. After uploading to the web, accompanying software shows the documents in the mainframe of a three frames structure. Another frame contains an automatically generated tree menu for navigation. Problem solutions and a glossary appear in the third frame. The model supports seamless integration of Web and CD-ROM, personal student notes, feedback forms, external links and multimedia. The model is currently used for the development of new courses and the experiences are encouraging.","Books,
Internet,
Distance learning,
Feedback,
Navigation,
CD-ROMs,
Education,
Technological innovation,
On the job training,
Guidelines"
Minimum register instruction sequence problem: revisiting optimal code generation for DAGs,"We revisit the optimal code generation or evaluation order determination problem-the problem of generating an instruction sequence from a data dependence graph (DDG). In particular, we are interested in generating an instruction sequence S that is optimal in terms of the number of registers used by the sequence S. We call this MRIS (Minimum Register Instruction Sequence) problem. We developed an efficient heuristic solution for the MRIS problem based on the notion of instruction lineage. This solution facilitates the sharing of registers among instructions within a lineage and across lineages by exploiting the structure of a DDG. We implemented our solution on a production compiler and measured the reduction in the number of (spill) loads and (Spill) stores and the wall-clock execution time for the SPEC95 floating point benchmark suite. On average our method reduced the number of loads and stores by 11.5% and 15.9%, respectively, and decreased the total execution time by 2.5%.","Registers,
Magnetic resonance imaging,
Processor scheduling,
Runtime,
Yarn,
Switches,
Supercomputers,
Computer science education,
Computer science,
Automation"
A hybrid program slicing framework,"Program slicing is a decomposition technique that transforms a large program into a smaller one that contains only statements relevant to the computation of a selected function. Applications of program slicing can be found in software testing, debugging, and maintenance by reducing the amount of data that has to be analyzed in order to comprehend a program or parts of its functionality. In this paper, we present a general dynamic and static slicing algorithm. Both algorithms are based on the notion of removable blocks and compute executable slices for object-oriented programs. In the second part of the paper we present our hybrid-slicing framework that was designed to take advantage of static and dynamic slicing algorithms that share the common notion of removable blocks, to enhance traditional slicing techniques. The hybrid-slicing framework is an integrated part of our existing MOOSE software comprehension framework that is used to demonstrate the applications and usability of these algorithms for the comprehension of software systems.","Computer science,
Reverse engineering,
Testing,
Debugging,
Application software,
Software maintenance,
Programming profession,
Heuristic algorithms"
Query expansion using conceptual fuzzy sets for search engine,"We propose a search engine which conceptually matches input keywords and web pages. The conceptual matching is realized by context-dependent keyword expansion using conceptual fuzzy sets. First, we show the necessity and the problems of applying fuzzy sets to information retrieval. Next, we introduce the usefulness of conceptual fuzzy sets in overcoming those problems, and propose the realization of conceptual fuzzy sets using Hopfield networks. We also propose the architecture of the search engine which can execute conceptual matching dealing with context-dependent word ambiguity. Finally, we evaluate our proposed method through two simulations of retrieving actual web pages, and compare the proposed method with the ordinary TF-IDF method. We show that our method can correlate seemingly unrelated input keywords and produce matching Web pages, whereas the TF-IDF method cannot.",
On axiomatic characterization of fuzzy approximation operators. III. The fuzzy diamond and fuzzy box based cases,"For pt.II, 31st IEEE International Symposium on Multiple-Valued Logic (2001). In three previous papers we have developed axiomatic characterizations of approximation operators which: firstly, are defined by classical diamond and box operator of the modal logic (Thiele, 2000); secondly, are defined by the ""fuzzified"" diamond and box operator applied to crisp sets, i.e. by using the concept of fuzzy rough set (Thiele, 2000); thirdly, are defined by the classical diamond and box operator applied to fuzzy sets, i.e. by using the concepts of rough fuzzy sets (Thiele, 2001). The paper presented is the last one concerning these ideas, i.e. we take the ""fuzzified"" diamond and box operator and apply these to fuzzy sets. So we define a further type of fuzzy approximation operators and develop an axiomatic characterization of such operators.","Fuzzy sets,
Fuzzy logic,
Set theory,
Computer science,
World Wide Web,
Fuzzy set theory,
Collaboration,
Fuzzy systems"
Incorporating project engineering and professional practice into the major design experience,"The culminating major design experience is a critical element in students' preparation for professional practice. Large, diverse programs face significant challenges in providing sufficient and appropriate design opportunities that incorporate a wide range of issues and constraints, as required by Engineering Criteria 2000. With the switch from quarters to semesters in 1999, the School of Electrical and Computer Engineering at Georgia Tech introduced a new, major design experience that addresses these limitations. All electrical engineering and computer engineering seniors complete a new course, Project Engineering and Professional Practice, followed by a specialized team-based design project. This new course, using a combination of lectures and interactive recitations, includes topics related to design methods, project management, teaming, engineering economics, ethics, risks, and professional issues. A formal paper and multiple presentations develop communication skills and provide a lead-in to the team project in the follow-on course. This paper describes the new course, emphasizing the unique elements, and summarizes initial assessment results.",
Affine 3-D reconstruction from two projective images of independently translating planes,"Consider two views of a multi-body scene consisting of k planar bodies moving in pure translation one relative to the other. We show that the fundamental matrices, one per body, live in a 3-dimensional subspace, which when represented as a step-3 extensor is the common transversal on the collection of extensors defined by the homograph matrices H/sub 1/,...,H/sub k/ of the moving planes. We show that as much as five bodies are necessary for recovering the common transversal from the homograph matrices, from which we show how to recover the fundamental matrices and the affine calibration between the two cameras.",
Extracting Web table information in cooperative learning activities based on abstract semantic model,"A great deal of Web table information exists in cooperative learning activities. The paper presents a new method that extracts information from tables of Web documents. Using a tabled abstract semantic model to describe complicated tables and understand tables from the point of view of semantics, the method reduces the dependence for the design difference of table constructions in the extraction process. At the same time, it utilizes the characteristics of HTML and the techniques of natural language processing to design some heuristic rules, and thus aids the identification of table items. On the above basis, we design a prototype, ""EXTable"", and then gain a better result according to experimentation.",
A new method for image normalization,"Image normalization is very useful in image understanding systems. In general, there are four basic forms of distortion of planar patterns: translation, rotation, scaling and skew. The authors propose a novel method which basically solves all problems in image normalization. First, we compute the covariance matrix of a given pattern. Then, we rotate the pattern according to the eigenvectors of the covariance matrix, and scale the pattern along the two eigenvectors according to the eigenvalues to bring the pattern to its most compact form. The processed pattern is invariant to translation, scaling and skew. Only the rotation problem remains unsolved. The final step is to rotate the image with respect to the image ellipse tilt angle, which is determined by the three second-order central moments, to make the pattern invariant to rotation. Thus, the resulting pattern is invariant to translation, rotation, scaling and skew. The correctness and effectiveness of the proposed method are approved by numerical experiments on aircraft images.","Transmission line matrix methods,
Eigenvalues and eigenfunctions,
Shape,
Covariance matrix,
Computer science,
Aircraft manufacture,
Pattern recognition,
Neural networks,
Data preprocessing,
Cameras"
"Scientific rigour, an answer to a pragmatic question: a linguistic framework for software engineering","Discussions of the role of mathematics in software engineering are common and have probably not changed much over the last few decades. There is now much discussion about the ""intuitive"" nature of software construction and analogies are drawn (falsely) with graphic design, (conventional) architecture, etc. The conclusion is that mathematics is an unnecessary luxury and that, like these other disciplines, it is not needed in everyday practice. We attempt to refute these arguments by recourse to ideas from the philosophy of science developed over the past century. We demonstrate why these ideas are applicable, why they establish a framework (in the sense of Carnap) in which many central ideas in software engineering can be formalised and organised, why they refute the simplistic recourse to ""intuition"", and why they provide a scientific/engineering framework in which contributions to the theory and practice of software engineering can be judged.","Mathematics,
Graphics,
Software engineering,
Design engineering,
Software systems,
Computer science,
Mathematical programming,
Buildings,
Informatics,
Computer architecture"
Generic model abstraction from examples,"The recognition community has long avoided bridging the representational gap between traditional, low-level image features and generic models. Instead, the gap has been artificially eliminated by either bringing the image closer to the models, using simple scenes containing idealized, textureless objects,,or by bringing the models closer to the images, using 3-D CAD model templates or 2-D appearance model templates. In this paper, we attempt to bridge the representational gap for the domain of model acquisition. Specifically, we address the problem of automatically acquiring a generic 2-D view-based class model from a set of images, each containing an exemplar object belonging to that class. We introduce a novel graph-theoretical formulation of the problem, and demonstrate the approach on real imagery.",
A software evaluation model using component association views,"Introduces a view-based architectural design evaluation model that allows one to quantitatively evaluate and categorize the design of a software system. The model is based on the notion of component association, which is a generalization of coupling and cohesion metrics. The component association is defined as a measure of the overall dependency among high-level system components, such as files, modules or subsystems, with regard to a collection of criteria. The associations are discovered by applying data mining techniques to a database of data-flow and control-flow dependencies extracted from the software system. The proposed association-view and modularity metrics allow the user to evaluate the design quality of a software system.","Software systems,
Software measurement,
Proposals,
Data mining,
Databases,
Control systems,
Computer science,
Reverse engineering,
Computer aided software engineering,
Laboratories"
Soft decision differential phase detection with Viterbi decoding in satellite mobile systems,"Noncoherent sequence detection is considered with soft decision differential phase detection and Viterbi decoding. The proposed receiver is analyzed with convolutionally encoded binary FSK and DPSK signals over narrowband fading channels. Numerical results presented for a Gaussian, satellite mobile and land mobile channels indicate that the proposed soft decision sequence detection receiver performs significantly better than uncoded signals and hard decision sequence detection. The division of subregions in the soft decision differential phase detector is also analyzed.","Measurement,
Error probability,
Decoding,
Merging,
Convolutional codes,
Viterbi algorithm,
Receivers"
"Design and Implementation of FMPL, a Fast Message-Passing Library for Remote Memory Operations","A fast message-passing library FMPL has been designed and developed to maximize communication performance by utilizing general architectural communication support such as remote memory operations, as well as to maximize total performance by eliminating dynamic communication overhead and overlapping communication and computation. FMPL provides a low-cost general-purpose point-to-point communication and collective communication such as broadcast, barrier synchronization and reduction. On a Hitachi SR8000, FMPL achieves an 8-byte latency of 12.8sec., while MPI achieves 20sec. FMPL is designed for building more highly functional message-passing libraries like BLACS as well as applications that need maximum performance.","Hardware,
Broadcasting,
Software libraries,
Linear algebra,
Government,
Runtime library,
Communication industry,
Computer industry,
Business communication,
Programming"
A comparative study of classical and fuzzy filters for noise reduction,"In this paper we present the results of a comparative study of classical and fuzzy filters for image noise reduction. The discussed fuzzy filters are classified, and their performance is compared with classical filters and evaluated by numerical and visual experiments.",
Fuzzy Markov chains,"We first review some of the basic results of finite Markov chains based on probability theory, then we present fuzzy finite Markov chains based on possibility theory, and compare the results of the two theories. Then we introduce finite horizon Markovian decision processes based on fuzzy Markov chains and study an example in detail showing our solution procedure.",
SVMs using geometric algebra for 3D computer vision,"This paper shows the analysis of support multivector machines using the coordinate-free system of Clifford or geometric algebra. Real-, complex- and quaternion-valued neural networks are simple particular cases of the geometric algebra multidimensional neural networks and that they can be generated using support multivector machines. Particularly, the generation of RBF for neurocomputing in geometric algebra is easier using the SMVM, which allows to find the optimal parameters automatically. The use of SVM in the geometric algebra framework expands its sphere of applicability for multidimensional learning. As illustration we present the estimation of 3D rigid motion and 3D pose of rigid objects using visual information captured by a trinocular head.","Support vector machines,
Algebra,
Computer vision,
Biological neural networks,
Tensile stress,
Physics,
Motion estimation,
Magnetic heads,
Matrices,
Computer science"
"Petri Net Script: a visual language for describing action, behaviour and plot","Current techniques for behavioural specification provide the ability to describe individual methods of action, interaction and behaviour, but do not allow scripting of a complex behavioural sequence as is required for a virtual actor within the plot of a stage-play. These existing specification techniques are also designed primarily for use by computing experts, usually as a text based language, and are therefore not easily accessible by laymen. The paper presents Petri Net Script (PNS), a new graphical language for specification of virtual actor behaviour. PNS provides a graphical interface to behavioural scripting that enables specification of actions and interactions for virtual actors that can then interact with human actors in real time. A quantitative justification as to the effectiveness of the new language is evaluated through comparison of required complexity to achieve a simple behaviour against that of a more traditional specification technique using an adaptation of T.J. McCabe's (1976) Cyclomatic Complexity.",
IR and AI: using co-occurrence theory to generate lightweight ontologies,"This paper illustrates the application of cooccurrence theory to generate lightweight ontologies semi-automatically. First, the relationship of Information Retrieval (IR) and Artificial Intelligence (AI) is discussed in a general way. Then two case studies have been conducted to generate lightweight ontologies in specific domains (Information Retrieval domain and European part of CIA FactBook). Further discussion is articulated and future work is proposed, especially the possible future research direction on ontology learning.",
A new architecture and a new metric for lightwave networks,"The notion of a logically routed network was developed to overcome the bottlenecks encountered during the design of a large purely optical network. In the last few years, researchers have proposed the use of torus. Perfect shuffle, hypercube, de Bruijn graph, Kautz graph, and Cayley graph as an overlay structure on top of a purely optical network. All these networks have regular structures. Although regular structures have many virtues, it is often difficult in a realistic setting to meet these stringent structural requirements. In this paper, we propose generalized multimesh (GM), a semiregular structure, as an alternate to the proposed architectures. In terms of simplicity of interconnection and routing, this architecture is comparable to the torus network. However, the new architecture exhibits significantly superior topological properties to the torus. For example, whereas a two-dimensional (2-D) torus with N nodes has a diameter of /spl Theta/(N/sup 0.5/), a generalized multimesh network with the same number of nodes and links has a diameter of /spl Theta/(N/sup 0.25/). In this paper, we also introduce a new metric, flow number, that can be used to evaluate topologies for optical networks. For optical networks, a topology with a smaller flow number is preferable, as it is an indicator of the number of wavelengths necessary for full connectivity. We show that the flow numbers of a 2-D torus, a multimesh, and a de Bruijn network, are /spl Theta/(N/sup 1.5/), /spl Theta/(N/sup 1.25/), and /spl Theta/(N log N), respectively, where N is the number of nodes in the network. The advantage of the generalized multimesh over the de Bruijn network lies in the bet that, unlike the de Bruijn network, this network can be constructed for any number of nodes and is incrementally expandable.","Optical fiber networks,
Network topology,
Spread spectrum communication,
Optical design,
Optical fiber communication,
Hypercubes,
Routing,
Image motion analysis,
Wavelength division multiplexing,
Computer science"
Selecting objects with freehand sketches,"We present in this paper the design of an interactive tool for selecting objects using simple freehand sketches. The objective is to extract object boundaries precisely while requiring little skill and time from the user. The tool proposed achieves this objective by integrating user input and image computation in a two-phase algorithm. In the first phase, the input sketch is used along with a coarse global segmentation of the image to derive an initial selection and a triangulation of the region around the boundary. The triangles are used to formulate subproblems of local finer-grained segmentation and selection. Each of the subproblems is processed independently in the second phase, where a linear approximation of the local boundary as well as a local, finer-grained segmentation are computed. The approximate boundary is then used with the local segmentation to compute a final selection, represented with an alpha channel to fully capture diffused object boundaries. Experimental results show that the tool allows very simple sketches to be used to select objects with complex boundaries. Therefore, the tool has immediate applications in graphics systems for image editing, manipulation, synthesis, retrieval and processing.","Image segmentation,
Graphics,
Handheld computers,
Cranes,
Computer science,
Linear approximation,
Image retrieval,
Content based retrieval,
Digital images,
TV broadcasting"
An analytic performance model of parallel systems that perform N tasks using P processors that can fail,We present a Markov model for analyzing the performance of parallel/distributed processors that execute a job consisting of N independent tasks in parallel using P processors. The model is a Markov chain with states representing service and failure rates with k (0,"Performance analysis,
Failure analysis,
Computer applications,
Costs,
Computer science,
Time measurement,
Finishing,
Virtual prototyping,
Prototypes,
Workstations"
Object-oriented concepts for modular robotics systems,"Describes how object-oriented concepts can be exploited in the design of modular robot systems. The approach is motivated in particular by the observation that, when sensors or end-effectors are mounted on mobile bases or at the end of manipulator arms, they can be said to acquire, or 'inherit', motion functionality. Other aspects of modular robotics systems similarly afford parallels with other object-oriented concepts, including polymorphism, aggregation and encapsulation or abstraction. The key contribution of the model we describe is the way in which it exploits qualitative descriptions of modules to reason about modules and their relationships in modular robot configurations. The model proposed is still in its infancy but offers benefits directly to modular robot system design (reducing software development efforts) and to modular object systems in general.","Object oriented modeling,
Robot sensing systems,
Mars,
Encapsulation,
Robot control,
Computer architecture,
Application software,
Computer science,
Cybernetics,
Propulsion"
"Implementing TreadMarks over Virtual Interface Architecture on Myrinet and gigabit Ethernet: Challenges, design experience, and performance evaluation","In recent years, several user-level communication systems have been developed to eliminate the gap between the performance of networking technologies used in Network Based Computing platforms and that experienced by the applications. The Virtual Interface Architecture (VIA) specification has been recently developed to standardize these communication systems and to make their features available in commercial systems. In this paper, we take on a challenge of developing a communication substrate over VIA such that applications using the popular TreadMarks DSM package can take advantage of the enhanced communication performance of VIA. We discuss various design alternatives, derive the best set of these alternatives and implement them on two enhanced implementations of VIA (M-VIA and Berkeley VIA) on two different networking technologies, Gigabit Ethernet and Myrinet, respectively. We evaluate the performance of our implementation by using several microbenchmarks and applications. We show that the communication and wait times, and therefore the total execution times of different applications can be significantly reduced by using VIA. A reduction in the overall execution time up to 2.05 on an eight node system is demonstrated in comparison with the original UDP implementation. The new implementation also demonstrates better parallel speedup as the system size increases.","Ethernet networks,
Computer architecture,
Middleware,
Computer networks,
Packaging,
Programming environments,
Information science,
Application software,
Hardware,
Communication industry"
The healthcare information technology context: a framework for viewing legal aspects of telemedicine and teleradiology,"Explores the healthcare information technology context as a backdrop for viewing the legal issues that accompany telemedicine and teleradiology. In this age of managed care, healthcare informatics has become a burgeoning field. Computer-based technologies help automate processes such as patient data collection. As patient records become increasingly digitized, they are more easily transmitted between various healthcare sites and personnel. The security of electronic medical data transfer, however, is sometimes inadequate. Digitized medical records give rise to a number of legal issues. A well-known example is the security of electronic medical data. Data security and other legal issues pose enormous challenges to the adoption of healthcare technologies; these barriers can potentially inhibit their diffusion. In the case of telemedicine, many of the current laws are underdeveloped and unstable, and pending bills are often obscure. In addition to confidentiality, other legal issues I discuss include malpractice, reimbursement and licensure.","Medical services,
Information technology,
Law,
Telemedicine,
Legal factors,
Data security,
Information security,
Personnel,
Protection,
Computer hacking"
Object oriented metrics useful in the prediction of class testing complexity,"Adequate metrics of object-oriented software enable one to determine the complexity of a system and estimate the effort needed for testing already in the early stage of system development. The metrics values enable to locate parts of the design that could be error prone. Changes in these parts could significantly, improve the quality of the final product and decrease testing complexity. Unfortunately only few of the existing Computer Aided Software Engineering tools (CASE) calculate object metrics. In this paper methods allowing proper calculation of class metrics for some commercial CASE tool have been developed. New metric, calculable on the basis of information kept in CASE repository and useful in the estimation of testing effort have also been proposed. The evaluation of all discussed metrics does not depend on object design method and on the implementation language.","Computer aided software engineering,
Design methodology,
Programming,
Software measurement,
Costs,
Software quality,
Computer science,
System testing,
Computer errors,
Lab-on-a-chip"
An automated change-detection algorithm for HTML documents based on semantic hierarchies,"The data at many Web sites is changing rapidly, and a significant amount of this data is presented in HTML documents that consist of markups and data contents. Although XML is becoming more popular for data exchange, the presentation of data contained in XML documents is given, by and large, in the HTML format using XSL(T). Since HTML was designed to ""display"" data from the human perspective, it is not trivial for a machine to detect (hierarchical) changes of data in an HTML document. In this paper, we propose a heuristic algorithm, called SCD (Semantic Change Detection), to detect semantic changes to the hierarchical data contents in any two HTML documents automatically. Semantic changes differ from syntactic changes since the latter refer to changes of data contents with respect to markup structures according to the HTML grammar. SCD does not require pre-processing, nor any knowledge of the internal structure of the source documents beforehand. The time complexity of SCD is O[(|X|/spl times/|Y|)log(|X|/spl times/|Y|)], where |X| and |Y| are the number of unique branches in the syntactic hierarchies of any two given HTML documents, respectively.","Change detection algorithms,
HTML,
XML,
Eyes,
Computer science,
Humans,
Heuristic algorithms,
Displays,
Testing"
An efficient depalletizing system based on 2D range imagery,"We propose an efficient approach towards the solution of the de-palletizing problem, based on active vision. We describe a system comprising an industrial robot and a time of flight laser sensor, which performs the depalletizing task in real time, and independently of lighting conditions. In our case, the target objects are solid boxes of known identical dimensions, neatly layered but with arbitrary orientation within a layer, which are all placed on a platform. The layered structure of the target platform allows for 2D imagery. The system locates the position of the boxes by tracking one of the corners they expose to the laser source. The system locates the desired corners by applying the scan line approximation technique, adapted to fit the needs of our application, to the 2D input data. The advantages of our system over existing applications are its simplicity, robustness, speed and ease of installation.","Robustness,
Service robots,
Humans,
Pattern recognition,
Image processing,
Computer science,
Robot sensing systems,
Sensor phenomena and characterization,
Solids,
Target tracking"
A system for automatic recording and prediction of design quality metrics,"We present recent extensions to the METRICS infrastructure that allow optimization of design processes at the flow level, rather than only at the individual tool level. As previously reported, METRICS infrastructure allows automatic recording of design and process information. Our extensions include (i) the collection of design flow information for use in flow optimization, and (ii) integration with data mining tools to allow automatic generation of design and flow QOR predictors. Our flow optimization experiments try to optimize incremental multilevel FM partitioner runs in an incremental (ECO-oriented) design flow. We also demonstrate QOR predictors that are generated automatically from the METRICS data warehouse by the Cubist data mining tool for industry placement, clock tree generation, and routing tools.","Process design,
Design optimization,
Transmitters,
Data warehouses,
XML,
Computer science,
Clocks,
Routing,
Time to market,
Electronics industry"
Faster template matching without FFT,"We consider the template matching problem, in which one wants to find all good enough occurrences of a pattern template P from a larger image I. The current standard of template matching is based on cross correlation computed in Fourier domain, using the fast Fourier transform (FFT). This can be extended to rotations of the template by a suitable rotational sampling of the template. An alternative approach, pursued here, is to solve the problem in the spatial domain using so-called filtering algorithms. Such algorithms scan the image quickly and find all promising areas for a more accurate (but slower) examination such that no good enough occurrences of the template are lost. The paper shows that the filtering approach can be orders of magnitude faster than the FFT based method. Especially in the 3D case the FFT is intolerably slow.","Pattern matching,
Image sampling,
Filtering algorithms,
Filters,
Computer science,
Fast Fourier transforms,
Pixel,
Hamming distance,
Correlation,
Euclidean distance"
Web-based distribution of GIS metropolitan maps,"Much research on spatial data retrieval has been accumulated in the past two decades. However, the research of Internet distribution of GIS contents is still in its infancy. This paper describes a systematic approach to optimize Internet distribution of GIS vector maps. The goal is to reduce user-perceived response time and improve viewers' navigation efficiency. At the center of this approach is a hierarchical content management model. Each multiple-layer vector map is divided into partitions, or blocks, of a predetermined size to build a hierarchy. This size is chosen so that most compressed blocks can be downloaded reasonably quickly at typical connection speeds. A front-end cache and a novel application-level locality-based pre-fetching method are devised. Experiments with a Java prototype have shown smooth panning and fast zooming when large map data sets are accessed.",
Assessment strategies: feedback is too late!,"The newly started Electrical and Computer Engineering program at Rowan University, USA, was visited by ABET's EAC for the first time in October 2000. As the authors prepared for their program evaluation under EC 2000, they explored novel mechanisms for assessing their curricular outcomes and ""closing the loop"" in an effort to develop ongoing methods for continuous program improvement. They were motivated by a strong desire to make the assessment process minimally intrusive, yet maximally effective. They have developed an assessment instrument called an X-File and a unique course called ""Clinic Consultant""-both these mechanisms are closely related to each other. This paper describes their implementation efforts.","Feedback,
Laboratories,
Instruments,
Knowledge engineering,
Accreditation,
Innovation management,
Design engineering,
Mathematics,
Electrical engineering computing,
Electric variables measurement"
On correlations of a family of generalized geometric sequences,"In this correspondence, we study families of generalized geometric sequences formed bp applying a feedforward function to certain sums of decimated m-sequences with elements in a finite field. We compute their correlation functions, which for certain families turn out to be close to the square root of the period. The size of these families equals their period. We also show that in the binary case, the linear complexities of these sequences are much larger than those of cascaded geometric sequences, although in these cases the maximum correlations are larger.",Feedforward systems
"Cluster computing in the classroom: topics, guidelines, and experiences","With the progress of research on cluster computing, more and more universities have begun to offer various courses covering cluster computing. A wide variety of content can be taught in these courses. Because of this, a difficulty that arises is the selection of appropriate course material. The selection is complicated by the fact that some content in cluster computing is also covered by other courses such as operating systems, networking, or computer architecture. In addition, the background of students enrolled in cluster computing courses varies. These aspects of cluster computing make the development of good course material difficult. Combining our experiences in teaching cluster computing in several universities in the USA and Australia and conducting tutorials at many international conferences all over the world, we present prospective topics in cluster computing along with a wide variety of information sources (books, software, and materials on the Web) from which instructors can choose. The course material described includes system architecture, parallel programming, algorithms, and applications. We share our experiences in teaching cluster computing and the topics we have chosen depending on course objectives.","Guidelines,
Conducting materials,
Educational institutions,
Computer architecture,
Education,
Computer networks,
Operating systems,
Australia,
Books,
Parallel programming"
A visual language for design pattern modelling and instantiation,"We describe the Design Pattern Modelling Language, a notation supporting the specification of design pattern solutions and their instantiation into UML (Unified Modeling Language) design models.","Unified modeling language,
Production facilities,
Concrete,
Graphical user interfaces,
Computer science,
Humans,
Software design,
Software systems,
Collaborative work,
Software prototyping"
The optimization of traffic signal light using artificial intelligence,"In the past, when there were few vehicles on the road, the TOD (time of day) traffic signal worked very well. The TOD signal operates on a preset signal cycling which cycles on the basis of the average number of average passenger cars in the memory device of an electric signal unit. Nowadays, with increasing vehicles on restricted roads, the conventional traffic light creates prove startup-delay time and end-lag-time. The conventional traffic light loses the function of optimal cycle, so 30-45% of the conventional traffic cycle is not matched to the present traffic cycle. We propose an electrosensitive traffic light using a fuzzy look up table method which will reduce the average vehicle waiting time and improve average vehicle speed. Computer simulation results prove that reducing the average vehicle waiting time is better than than fixed signal method which doesn't consider vehicle length.","Artificial intelligence,
Traffic control,
Road vehicles,
Fuzzy logic,
Telecommunication traffic,
Delay effects,
Fuzzy set theory,
Educational institutions,
Computer science,
Computer simulation"
Byzantine fault tolerance can be fast,"Byzantine fault tolerance is important because it can be used to implement highly-available systems that tolerate arbitrary behavior from faulty components. We present a detailed performance evaluation of BFT, a state-machine replication algorithm that tolerates Byzantine faults in asynchronous systems. Our results contradict the common belief that Byzantine fault tolerance is too slow to be used in practice, BFT performs well so that it can be used to implement real systems. We implemented a replicated NFS file system using BFT that performs 2% faster to 24% slower than production implementations of the NFS protocol that are not fault-tolerant.","Fault tolerance,
Fault tolerant systems,
Libraries,
File systems,
Computer science,
Protocols,
Optimization,
Laboratories,
Production systems,
Availability"
Evolution on a chip: evolvable hardware aims to optimize circuit design,"A group at the Swiss Federal Institute of Technology are studying nature's ability to create living systems that can produce energy, process information, self-replicate, self-repair, function with defects, adapt to their surroundings and improve the species over time through evolution. The group is looking at these natural processes in order to translate their underlying principles into the language of information technology - specifically, those aspects that bear on circuit design. Configurable hardware will be the catalyst that ultimately enables the future of information processing systems.","Hardware,
Design optimization,
Circuit synthesis,
Reconfigurable logic,
Computer aided manufacturing,
Field programmable gate arrays,
Logic devices,
Computational modeling,
Circuit simulation,
Genetic algorithms"
Precise exceptions in asynchronous processors,The presence of precise exceptions in a processor leads to complications in its design. Some recent processor architectures have sacrificed this requirement for performance reasons at the cost of software complexity. We present an implementation strategy for precise exceptions in asynchronous processors that does not block the instruction fetch when exceptions do not occur; the cost of the exception handling mechanism is only encountered when an exception occurs during execution - an infrequent event.,"Hardware,
Costs,
Computer science,
Modems,
Computer architecture,
Software performance,
Operating systems,
Algorithms,
Floating-point arithmetic,
Laboratories"
Stopping criteria comparison: towards high quality behavioral verification,Verification of complex behavioral models has become a critical and time-consuming process. Determine when to switch to different testing strategy phase is the key to improving efficiency. This paper presents an overview of the existing statistical stopping rules that can be used for behavioral model verification. We examined the stopping rules using two VHDL models for five consecutive test phases. The results of the coverage gained and the number of testing patterns applied are then compared for each stopping rule. We conclude that the confidence-based stopping criterion outperforms others in terms of efficiency.,"Testing,
Hardware design languages,
Switches,
Computer science,
Power generation,
Process design,
Monitoring,
Time to market,
Feedback,
Productivity"
A formal approach to component-based software engineering: education and evaluation,"Summarizes an approach for introducing component-based software engineering (CBSE) early in the undergraduate computer science curriculum, and an evaluation of the impact of the approach at two institutions. Principles taught include a modular style of software development, an emphasis on human understanding of component behavior even while using formal specifications, and the importance of maintainability, as well as classical issues such as efficiency analysis and reasoning. Qualitative and quantitative evaluations of student outcomes and end-to-end changes in student attitudes show mostly positive results that are statistically significant, confirming that: (1) it is possible to teach CBSE principles without displacing ""classical"" principles usually taught in introductory courses, (2) students can understand and reuse formally specified components without knowing their implementations, and (3) student attitudes towards software engineering can be altered in directions heretofore often assumed to be difficult to achieve.","Software engineering,
Formal specifications,
Computer science education,
Programming,
Computer science,
Software systems,
Assembly systems,
Software reusability,
Statistics,
Humans"
Enhancing Jini with group communication,"Reliable group communication has proven to be an important technology for building fault-tolerant applications, yet many frameworks for distributed application development (e.g. DCOM, Jini and Enterprise JavaBeans) do not support it. The only notable exception to this situation is CORBA, which has been recently extended to include a replication service. We claim that lack of group communication support in other development frameworks constitutes a major obstacle for their wider diffusion in the industry. In this paper, we discuss issues related to integrating reliable group communication and Jini technologies.","Fault tolerance,
Java,
Communication industry,
Application software,
Computer science,
Communications technology,
Sun,
Collaborative work,
Standardization,
Insulation"
Multilevel modelling and design of visual interactive systems,"Designers of interactive computer-based artefacts need to specify their products with respect to both the artefact's external behaviour and to its internal one. In order to manage the complexity of the design and the implementation process, we introduce a 3D interaction modelling space that takes into account the different levels at which the computer internal activities, the messages on the screen, and the user activities are specified. Multilevel modelling helps to consistently design and implement interactive systems.","Interactive systems,
Process design,
Computer languages,
Mice,
Keyboards,
Computer science,
Human computer interaction"
A 1.25 GHz 32-bit tree-structured carry lookahead adder,"In this paper, a 32-bit tree-structured carry lookahead adder (CLA) is proposed by using the modified all-N-transistor (ANT) design. The 32-bit CLA not only possesses few transistor count, but also occupies small area size. Moreover, the post-layout simulation results given by TimeMill show that the clock used in this 32-bit CLA can run up to 1.25 GHz. The proposed architecture is also easily expanded for long data additions.","CMOS logic circuits,
Clocks,
Signal generators,
CMOS technology,
Logic design,
Robustness,
CMOS process,
Feedback,
Councils,
Computer science"
Assisting the comprehension of legacy transactions,"One of the principal aims of data reverse engineering is to facilitate comprehension of the internal structure and behaviour of data-intensive systems by programmers. However the majority of the research in this area has so far concentrated on the development of techniques to assist comprehension of the data structures used by such systems, and much less attention has been paid to the equally pressing problem of understanding the ways in which those structures are manipulated by the systems to solve business problems. In particular, there are few tools which can help a programmer to comprehend the functionality provided by a collection of database transactions. While some relevant techniques have been developed by the software reverse engineering community, in general their applicability to transactions is limited by the fact that they focus on program state alone. In-depth analysis of source code that manipulates data stored in a database (or other persistent store) must take into account the effects of that source code on both the program state and the database state. In this paper we present an extension of the technique of symbolic execution that takes into account the specific semantics of database access commands. This extended symbolic execution engine can extract declarative representations of the data manipulations carried out by transactions, in the form of logical conditions over the database state rather procedural manipulation of the program state. The engine is then used to annotate transactions with automatically generated comments, to assist the transaction programmer in comprehending its behaviour.","Transaction databases,
Data mining,
Spatial databases,
Reverse engineering,
Programming profession,
Computer science,
Data structures,
Engines,
Pressing,
Object oriented databases"
Finite-state analysis of the CAN bus protocol,"We formally specify the data link layer of the Controller Area Network (CAN), a high-speed serial bus system with real-time capabilities, widely used in embedded systems. CAN's primary application domain is automotive, and the physical and data link layers of the CAN architecture were the subject of the ISO 11898 international standard. We checked our specification against 12 important properties of CAN, eight of which are gleaned from the ISO standard; the other four are desirable properties not directly mentioned in the standard. Our results indicate that not all properties can be expected to hold of a CAN implementation and we discuss the implications of these findings. Moreover, we have conducted a number of experiments aimed at determining how the size of the protocol's state space is affected by the introduction of various features of the data link layer, the number of nodes in the network the number of distinct message types, and other parameters.","Protocols,
ISO standards,
Control systems,
Embedded system,
Automotive engineering,
Manufacturing,
Error correction,
Computer science,
Application software,
State-space methods"
A novel strategy to test core based designs,"This paper proposes a novel technique for testing core based system-on-a-chip (SOC), targeting to reduce the test application time as well as the test hardware. The proposed work is to be done in two parts: (i) Core Level and (ii) Interconnect Level. To date, many authors have studied the problem of testing core-based systems, but not much work exists on testing the cores and the interconnects together. Also proposed is an efficient test access design to reduce test cost by minimising test application time. Test access is a major challenge for testing of core-based system-on-a-chip designs. Several issues related to the Test Access Mechanism (TAM) design such as assignment of cores to test buses, optimal number of buses required, distribution of test data bandwidth between several buses have been handled in this paper. In doing so, the testing time has been found to be drastically reduced at the cost of some extra test hardware.","System testing,
Hardware,
Rails,
System-on-a-chip,
Bandwidth,
Costs,
Computer science,
Computer architecture,
System buses,
Optimization methods"
Effect of event orderings on memory requirement in parallel simulation,"A new formal approach based on partial order set (poset) theory is proposed to analyze the space requirement of discrete-event parallel simulation. We divide the memory required by a simulation problem into memory to model the states of the real-world system, memory to maintain a list of future event occurrences, and memory required to implement the event synchronization protocol. We establish the relationship between poset theory and event orderings in simulation. Based on our framework, we analyze the space requirement using an open and a closed system as examples. Our analysis shows that apart from problem size and traffic intensity that affects the memory requirement, event ordering is an important factor that can be analyzed before implementation. In an open system, a weaker event ordered simulation requires more memory than strong ordering. However, the memory requirement is constant and independent of event ordering in closed systems.","Discrete event simulation,
Computational modeling,
Protocols,
Memory management,
Computer simulation,
Analytical models,
Open systems,
Parallel processing,
Constraint optimization,
Computer science"
A case for systems thinking and system dynamics,"Systems thinking is a way of thinking that focuses on the relationships between the parts forming a purposeful whole. System dynamics is concerned with building computer models of complex problem situations and then experimenting with and studying the behaviour of these models over time. This paper is a review of systems thinking that considers its unique history and influences, paradigms and methodologies, and presents a case for the system dynamics methodology as the best tool for the most diverse range of problem situations.","Computer aided software engineering,
Humans,
Performance analysis,
Computer science,
History,
Sociology,
Feedback,
Systems engineering and theory,
Operations research,
Cybernetics"
Fault-tolerance scheme for an RNS MAC: performance and cost analysis,"Residue number systems (RNS) are especially useful in applications in which fault-tolerance is a requirement. Accordingly, this paper discusses a novel and elegant fault-tolerance scheme incorporated into a multiply-accumulate (MAC) unit based on RNS. The fault-tolerance is achieved by using inexpensive forward conversion procedures. The cost and performance are analyzed with respect to other designs, and the analysis indicates superior cost:performance measure for our design.",
A deformable model for image segmentation in noisy medical images,"Deformable-model-based segmentation techniques can overcome some limitations of the traditional image processing techniques. Currently developed deformable models can cope with gaps and another irregularities in object boundaries. However, they present problems in noisy images. Our approach is able to segment objects in noisy images by defining a new energy function associated with image noise and avoiding the tendency of contour points to bunch up. The model is validated for vessel segmentation on mammograms.",
On-line error detectable carry-free adder design,"A technique for designing carry-free adders with on-line error checking capability is presented. The adders use signed binary digits (SBDs) internally. An adder consists of sign-magnitude binary to SBD converters, an intermediate adder block that generates partial sum and carry digits, a second adder block that produces a sum digit computed from a partial sum and a partial carry digit, and an error checker that indicates whether the code word corresponding to a final sum digit is error-free or not.","Adders,
Computer errors,
Design engineering,
Very large scale integration,
Digital systems,
Logic circuits,
Electrical fault detection,
Fault detection,
Computer vision,
Computer science"
Dynamic load-balancing via a genetic algorithm,"We produce a genetic algorithm (GA) scheduling routine, which with often relatively low cost finds well-balanced schedules. Incoming tasks (of varying durations) accumulate, then are periodically scheduled, in small batches, to the available processors. Two important priorities for our scheduling work are that loads on the processors are well balanced, and that scheduling per se remains cheap in comparison to the actual productive work of the processors. We also include experimental results, exploring a variety of distributions of task durations, which show that our scheduler consistently produces well-balanced schedules, and quite often does so at relatively low cost.","Genetic algorithms,
Processor scheduling,
Costs,
Computer science,
Delay effects"
Comparing and contrasting micro-payment models for e-commerce systems,"The current macro-payment systems used by most e-commerce sites are not suitable for high-volume, low-cost produce or service purposes, such as charging per page for Web site browsing. These payment technologies suffer from the use of heavyweight encryption technologies and reliance on always-online authorisation servers. Micro-payment systems offer an alternative strategy of pay-as-you-go charging, even for very low-cost, very high-volume charging. However, several different micro-payment schemes exist, not all of which are suitable for all e-commerce uses. We compare and contrast several micro-payment models and outline a new micro-payment technology which we have been developing.",
Decomposition of multi-valued functions into min- and max-gates,"This paper presents algorithms that allow the realization of multi-valued functions as a multi-level network consisting of min- and max-gates. The algorithms are based on bi-decomposition of function intervals, a generalization of incompletely specified functions. Multi-valued derivation operators are applied to compute decomposition structures. For validation the algorithms have been implemented in the YADE system. Results of the decomposition of functions from machine learning applications are listed and compared to the results of another decomposer.","Machine learning algorithms,
Machine learning,
Calculus,
Boolean functions,
Microelectronics,
Mechatronics,
Computer science,
Data mining,
Multivalued logic,
Humans"
Anticipated route maintenance (ARM) in location-aided mobile ad hoc networks,"Mobile ad hoc networks are composed of moving wireless hosts which, when within range of each other, form wireless networks. For communication to occur between hosts that are not within each other's range, routes involving intermediate nodes must be established; however, since the hosts may be in motion, a host that was part of a route may move away from its upstream and downstream partners, thus breaking the route. Our work shows that by utilizing only local geographic information (now a part of some route finding algorithms), a host can anticipate its neighbor's departure and, if other hosts are available, choose a host to bridge the gap, keeping the path connected. We present a distributed algorithm that anticipates route failure and performs preventative route maintenance using location information to increase a route lifespan. The benefits are that this reduces the need to find new routes (which is very expensive) and prevents interruptions in service. Simulation shows that as the density of nodes increase the chance to successfully utilize our route maintenance approach increases, and so does the savings.","Intelligent networks,
Mobile ad hoc networks,
Routing,
Ad hoc networks,
Computer science,
Laboratories,
Drives,
Wireless networks,
Mobile communication,
Bridges"
Feedback guided dynamic loop scheduling; A theoretical approach,In this paper we review existing loop scheduling algorithms and also describe the feedback-guided dynamic loop scheduling (FGDLS) algorithm that was proposed in Bull et al. (1996) and Bull (1998). The FGDLS algorithm uses a feedback mechanism to schedule a parallel loop within a sequential outer loop. It has been shown to perform well for scheduling problems for which the load associated with the parallel loop changes relatively slowly as the outer sequential loop executes. However the question of convergence of the FGDLS algorithm has remained an open question. In this paper we are able to establish sufficient conditions (essentially requiring that the workload does not change too rapidly with loop iteration count) for the (global) convergence of a continuous analogue of the feedback-guided algorithm.,"Feedback loop,
Dynamic scheduling,
Scheduling algorithm,
Processor scheduling,
Computer science,
Performance loss,
Educational institutions,
Algorithm design and analysis,
User-generated content,
Convergence"
Adaptive multicast routing for satellite-terrestrial network,"Multicast provides an efficient way of distributing multimedia information to a set of destinations simultaneously with the highest possible data rate. To enhance the multicasting performance, an adaptive multicast routing (AMR) approach for the satellite-terrestrial network (a hybrid network interconnected by VSAT systems) is proposed. It can dynamically adjust the routing path to obtain a minimal routing cost through a re-routing operation and support dynamic membership, joining and leaving. The simulation results demonstrate that the AMR has better performance and lower routing costs than any other Internet multicast algorithm.","Routing,
Satellite broadcasting,
Costs,
Multicast algorithms,
Telecommunication traffic,
Internet,
Chaos,
Computer science,
Data engineering,
Bandwidth"
Model-based control for reconfigurable manufacturing systems,"The manufacturing industry cannot stay competitive and survive in today's market without agile adaptation to rapidly changing customers' demands. This in turn requires manufacturing systems to be reconfigurable for timely introduction of new products in the market. Unfortunately, at present, the system designers cannot systematically and completely manage their design data, because manufacturing systems have gradually become too large and too complicated to manage. In order to reconfigure and reuse H/W and S/W components in manufacturing systems, and improve the engineering environment of system control design, we propose a model-based control design using state transition diagrams and a general graph description, while taking reconfiguration and reuse of design data into account. We demonstrate the utility of the proposed approach using a real application.","Manufacturing systems,
Control design,
Laboratories,
Industrial electronics,
Computer aided manufacturing,
Computer science,
Manufacturing industries,
Agile manufacturing,
Data engineering,
Design engineering"
Steganographic watermarking for documents,"Defines digital seals for documents, their scope, application environment and limitations. These seals can be used to insert information into documents, as with watermarking, or to use documents as a communication channel for sending concealed messages, as is the goal of steganography. Users can decide to employ one or another functionality, or a combination of them, depending on their needs or preferences. As a step towards these objectives, a system was developed which constitutes a kit with several sealing options. The system writes either visible or invisible marks in digital documents, following the different methods designed and created in this project. These marks or seals, in turn, can be viewed through a seal recognizer. The implementation is done in RTF (rich text format), which is a commercial, massively accessible format.","Steganography,
Watermarking,
Seals,
Cryptography,
Protection,
Security,
Art,
Pulp manufacturing,
Computer science,
Application software"
Using abstraction to improve fault tolerance,"Software errors are a major cause of outages and they are increasingly exploited in malicious attacks. Byzantine fault tolerance allows replicated systems to mask some software errors but it is expensive to deploy. The paper describes a replication technique, BFTA, which uses abstraction to reduce the cost of Byzantine fault tolerance and to improve its ability to mask software errors. BFTA reduces cost because it enables reuse of off-the-shelf service implementations. It improves availability because each replica can be repaired periodically using an abstract view of the state stored by correct replicas, and because each replica can run distinct or non-deterministic service implementations, which reduces the probability of common mode failures. We built an NFS service that allows each replica to run a different operating system. This example suggests that BFTA can be used in practice; the replicated file system required only a modest amount of new code, and preliminary performance results indicate that it performs comparably to the off-the-shelf implementations that it wraps.","Fault tolerance,
Software libraries,
Laboratories,
Computer errors,
Costs,
Computer science,
Fault tolerant systems,
Availability,
Operating systems,
Software systems"
Dynamic superimposition of synthetic objects on rigid and simple-deformable real objects,"A current challenge in augmented reality applications is the accurate superimposition of synthetic objects on real objects within the environment. This challenge is heightened when the real objects are in motion and/or are nonrigid. In this article, we present a robust method for realtime, optical superimposition of synthetic objects on dynamic rigid and simple-deformable real objects. Moreover, we illustrate this general method with the VRDA Tool, a medical education application related to the visualization of internal human knee joint anatomy on a real human knee.","Tracking,
Biomedical optical imaging,
Visualization,
Humans,
Knee,
Joints,
Anatomy,
Augmented reality,
Computer science,
Application software"
Tracking body parts of multiple people: a new approach,"Tracking body parts of multiple people in a video sequence is very useful for face/gesture recognition systems as well as human computer interaction (HCI) interfaces. This paper describes a framework for tracking multiple objects (e.g., hands and faces of multiple people) in a video stream. We use a probabilistic model to fuse the color and motion information to localize the body parts and employ a multiple hypothesis tracking (MHT) algorithm to track these features simultaneously. The MHT algorithm is capable of tracking multiple objects with limited occlusions and is suitable for resolving any data association uncertainty. We incorporated a path coherence function along with MHT to reduce the negative effects of spurious measurements that produce unconvincing tracks and needless computations. The performance of the framework has been validated using experiments on synthetic and real sequence of images.","Target tracking,
Layout,
Human computer interaction,
Face recognition,
Fuses,
Computer vision,
Data mining,
Time measurement,
Velocity measurement,
Computer science"
Asynchronous MPI messaging on Myrinet,"MPI-NP II is a network processor based message manager for MPI. The objectives of MPI-NP II were to reduce host processing and to make use of the processor on the network interface board to do message scheduling and message matching. The design of MPI-NP II is based on microchannels and message rendezvous. MPI-NP II implements MPI nonblocking communication as asynchronous communication and is able to overlap computation with communication. We introduce the concept of k.safe programs to guarantee message delivery based on the available envelope resources independent of the message size. We achieve these benefits without unduly burdening the NIC processor. We obtain a latency of 22 microseconds and a bandwidth of 92 MB/s, which is comparable to other Myrinet MPI implementations that perform MPI message management on the host.",
Object recognition using boosted discriminants,"We approach the task of object discrimination as that of learning efficient ""codes"" for each object class in terms of responses to a set of chosen discriminants. We formulate this approach in an energy minimization framework. The ""code"" is built incrementally by successively constructing discriminants that focus on pairs of training images of objects that are currently hard to classify. The particular discriminants that we use partition the set of objects of interest into two well-separated groups. We find the optimal discriminant as well as partition by formulating an objective criteria that measures the well-separateness of the partition. We derive an iterative solution that alternates between the solutions for two generalized eigenproblems, one for the discriminant parameters and the other for the indicator variables denoting the partition. We show how the optimization can easily be biased to focus on hard to classify pairs, which enables us to choose new discriminants one by one in a sequential manner We validate our approach on a challenging face discrimination task using parts as features and show that it compares favorably with the performance of an eigenspace method.","Object recognition,
Face detection,
Training data,
Computer science,
Object detection,
Eyes,
Extraterrestrial measurements,
Hamming distance"
Cooperative learning in computer architecture: an educational project and its network support,"This paper describes the design, setup and evaluation of an educational project in computer architecture studies. Its design through the educational-telematic framework DELFOS, proposed by the authors' research group, aims to employ computer supported collaborative learning in computer engineering studies. Its main objective is to integrate lectures, simple assignments and laboratory work through projects and case studies in computer architecture design and evaluation. Experimental studies were carried out during two academic years with very encouraging results. At least three student groups in each session dealt with one out of 5 distinct case studies. Students assumed the roles of consulting firm and computer manufacturer, who try to give an answer to a specific customer/market sector. Cooperative learning was carried out through synchronous guided discussions within the class in key dates, asynchronous document sharing and discussions, as well as collaborative edition of the final project report. Analysis of the experimental work was performed on both ethnographic data and computer transcriptions, while current studies deal with student interactions with social network analysis methods.","Intelligent networks,
Computer architecture,
Collaborative work,
Telematics,
Computer aided manufacturing,
Performance analysis,
Computer networks,
Computer science education,
Teamwork,
Telecommunication computing"
Genetic tuning of fuzzy rule-based systems integrating linguistic hedges,"Tuning fuzzy rule-based systems for linguistic modeling is an interesting and widely developed task. It involves adjusting the membership functions composing the knowledge base. To do that, changing the parameters defining each membership function as using linguistic hedges to slightly modify them may be considered. This paper introduces a genetic tuning process for jointly making these two tuning approaches. The experimental results show that our method obtains accurate linguistic models in both approximation and generalization aspects.",
Placement of read-write Web proxies on the Internet,"This paper investigates the optimal placement of proxies of a Web server on the Internet. With the consideration of both read and write operations to the data on the Web server. First, we study the problem of optimal placement of k proxies in a system to minimize the total access cost to the Web server. Then, for unknown number of proxies, we find the optimal number of proxies required in the system. The problems are formulated using a dynamic programming method and optimal solutions are obtained. Intensive simulations have been conducted to evaluate the performance of the proposed algorithms, and to demonstrate the relationship between the number of proxies required in the system and the read-write ratio. This work can significantly alleviate the Web access traffic on the Internet and improve the performance of the Web server.",
Livelock avoidance for Meta-schedulers,"Meta-scheduling, a process which allows a user to schedule a job across multiple sites, has a potential for livelock. Current systems avoid livelock by locking down resources at multiple sites and allowing a meta-scheduler to control the resources during the lock down period or by limiting job size to that which will fit on one site. The former approach leads to poor utilization; the later poses limitations on job size. This research uses BYU's Meta-scheduler (YMS) which allows jobs to be scheduled across multiple sites without the need for locking down the nodes. YMS avoids livelock through exponential back-off. This research quantifies the potential for livelock, determines a suitable back-off period, and provides a structure upon which to test theoretical local schedulers. The results show that livelock exists and that a suitable exponential back-off not only avoids livelock but reduces the scheduling time for each job.","Processor scheduling,
System recovery,
Ethernet networks,
Computer science,
Size control,
Control systems,
Testing,
Supercomputers,
Intellectual property,
USA Councils"
Animating to improve learning: a model for studying multimedia effectiveness,"This paper presents a model for designing experiments to test the effectiveness of multimedia in education. It is based upon a discussion of an experimental study that examined the impact of one specific multimedia enhancement-animation-on a very specific aspect of learning-the ability to apply knowledge. The prospect of multimedia as a tool that can enrich a learning environment is quite alluring. The potential to actively engage learners through multiple communication channels is, intuitively, very compelling as a means of promoting learning for a larger percentage of students. This enthusiasm must, however, be tempered by two sobering facts. The cost of multimedia production is so very great that, should it not prove effective, our educational system could be seriously impacted. Perhaps more importantly, there is little scientific evidence in support of the value of multimedia as an enhancement to a learning environment. The paucity of substantial evidence supporting the value of multimedia as an educational enhancement can at least partially be attributed to definitional problems within experimental designs. What specifically does ""learning"", and, for that matter, ""multimedia"", mean? If these terms are not explicitly defined and intimately incorporated into the experimental controls, there is little hope for creditable results. This study indicates that careful attention to defining these two terms can produce meaningful results.","Animation,
Testing,
Design for experiments,
Communication channels,
Costs,
Multimedia systems,
Production systems,
Technological innovation,
Teamwork,
Position measurement"
Relief mosaics by joint view triangulation,"Relief mosaics are collections of registered images that extend traditional mosaics by supporting motion parallax. A simple parallax interpolation algorithm based on computed correspondence information allows high quality blur-free and ghost-free mosaics to be created using images from moving hand-held cameras that would not be suitable for traditional mosaicing. The renderer can also display local parallax changes, giving a local but visually convincing illusion of depth. Moreover, relief mosaics can be used for approximate plenoptic modeling from hand-held cameras at lower spatial sampling rates than existing light-field methods. We present a fully automatic correspondence based construction system for relief mosaics, and show how they can be used in applications.","Cameras,
Interpolation,
Rendering (computer graphics),
Layout,
Image sampling,
Computer science,
Displays,
Calibration,
Costs,
Video sequences"
Mining categories of learners by a competitive neural network,"Addresses the problem of user modeling, which is a crucial step in the development of adaptive hypermedia systems. In particular, we focus on adaptive educational hypermedia systems, where the users are learners. Learners are modeled in the form of categories that are extracted from empirical data, represented by responses to questionnaires, via a competitive neural network. The key feature of the proposed network is that it is able to adapt its structure during learning so that the appropriate number of categories is automatically revealed. The effectiveness of the proposed approach is shown on two questionnaires of different type.",
A UML-based design environment for interactive applications,"The Unified Modeling Language (UML) can be used for modelling both the structure and behaviour of software applications. However, although UML supports many different modelling notations, minimal support is provided for user interface (UI) design. The Unified Modeling Language for Interactive Applications (UMLi) is an extension of UML that provides support for UI design. UMLi has a user interface diagram for modelling abstract UI presentations and an extended activity diagram that provides constructors for modelling common UI behaviours. The paper presents the support provided for UI design by the UMLi design environment. Designers can use the environment to model applications and their UIs using UML and its extensions in UMLi. The tool provides facilities for modelling interaction objects, and the collaboration of these interaction objects with domain objects.",
A non-repudiation metering scheme,"A metering scheme enables a Web server to measure the number of visits from clients. In addition, a proof needs to be presented by the server as evidence corresponding to this measured number. In this letter, we propose an efficient metering scheme that incorporates the hash-chaining technique and the digital signature algorithm to provide a nonrepudiation proof of this measured number.","Digital signatures,
File servers,
Web server,
Cryptography,
Authentication,
Protocols,
Personal communication networks,
Registers,
Computer science"
Optimized recovery of DOCSIS networks using reserved persistent ranging,"Power outage events can severely disrupt digital services over community antenna television (CATV) networks. This paper proposes two schemes for reserved persistent initial ranging of the data over cable service interface specification (DOCSIS) protocol, namely priority and cluster ranging. Both schemes restrain the number of CMs that can compete simultaneously for the acquisition of the upstream channel and minimize the number of idle opportunities, leading to faster recovery of the network. Their employment involves the introduction of a new field in the MAP message and a new cable modem (CM) parameter in the configuration file downloaded from the cable modem termination system (CMTS). Our results show an absolute reduction in the network's recovery time of up to 42% with an average of 38%.","Cable TV,
Collision mitigation,
Communication cables,
Modems,
Media Access Protocol,
Digital video broadcasting,
Helium,
Computer science,
Termination of employment,
Hybrid fiber coaxial cables"
A direct spatial-domain representation for the fields excited by an arbitrary incident field at a planar dielectric interface,"A new expression is constructed for scattering from a planar dielectric interface. This expression provides the exact boundary fields excited by all arbitrary incident field on a flat surface without requiring operator inversion or spectral decomposition of the incident field. While analytically exact, the simulation results can be affected by numerical and discretization errors when the operators are not handled properly. Particular attention must be paid to operator products and discretization. Extension of this result to smooth closed bodies is addressed in a separate presentation.","Integral equations,
Scattering,
Polarization,
Surface waves,
Surface treatment,
Dielectric losses,
Green function,
Computer interfaces,
Iterative methods,
Geometry"
Adaptation of traditional usability testing methods for remote testing,"Traditional usability testing methods are difficult to use in producing Web sites and Web applications, mainly because of the decreased development times that companies demand for this type of software. Users of Web sites have diverse platforms, computer expertise and expectations. Companies want to use Web sites to sell merchandise and provide services to customers. Therefore, it is essential to make usability a high priority in the development of Web-based software. How can we resolve this seemingly contradictory situation? We believe that usability testing tools that are remote, rapid and automated would be helpful in providing more usability information in a shorter time and in a form that can be immediately useful to usability professionals. In this paper, we discuss the approach we have taken to designing such tools. We currently have several tools available for public use that are also discussed in this paper. Our next steps will be to conduct methodological studies to validate the use of these tools.","Usability,
Software testing,
Application software,
Programming,
NIST,
Merchandise,
Automatic testing,
Web sites,
Books,
Investments"
Criteria importances in median-like aggregation,"An axiomatization of criteria importances appearing in a given aggregation operator is proposed. Some distinguished examples are recalled. For the class of k-medians, an integral based approach for inclusion of criteria importances is introduced. Several examples are given. The case of ordinal scales is also discussed.","Mathematics,
Computer science"
Lossless and near-lossless compression of ECG signals,"We present a linear transformation algorithm (LTA), which is based on a new transformation, linear order transformation (LOT). Experimental results show that the LTA yields comparable results to the Burrows-Wheeler algorithm (BWA) and outperforms the Gzip, and shorten waveform coder for near-lossless ECG compression; for lossless ECG compression it yields better compression than all the other techniques.","Electrocardiography,
Sorting,
Image coding,
Mathematics,
Compressors,
Computer science,
Memory,
Data communication,
Law,
Legal factors"
Optimal grasping based on non-dimensionalized performance indices,"For a multi-fingered hand to grasp an object, there are numerous ways to grasp it stably, and thus an optimal grasp planning is necessary to find the optimal grasp point for achieving the objective of the given task. First, we define several grasp indices to evaluate the quality of each feasible grasp. Since the physical meanings of the defined grasp induces are different from each other, it is not easy to combine those indices to identify the optimal grasping. In this paper, we propose a new generalized grasping performance index to represent all of the grasp indices as one measure based on a non-dimensional technique. Through simulations, we show that the proposed optimal grasp planning is resemblant to the physical sense of human grasping.","Grasping,
Uncertainty,
Fingers,
Computer science,
Intelligent systems,
Intelligent control,
Control systems,
Humans,
Performance analysis,
Ellipsoids"
Medisoft 4: a software procedure for the control of the Medipix2 readout chip,"We present here Medisoft 4, a software procedure for the control and the readout of an imaging system in the form of a hybrid pixel detector (256/spl times/256 pixels, 55 /spl mu/m pitch) developed in the framework of the Medipix2 collaboration. This system, originally designed for biomedical applications, is based on the single-photon counting Medipix2 read-out chip, the MUROS-2 interface board and a commercially available digital I/O board. Following the Medipix2 system evolution from its first version (Medipix1, 64/spl times/64 pixels, 170 /spl mu/m pitch), the Medisoft 4 software system allows the user to access the new implemented features, such as the higher spatial resolution, the faster chip-to-PC communication, the daisy-chain multichip read-out mode, the energy-windowed acquisition and the continuous acquisition mode. We describe here the software architecture, the complete file system, the software functions and features.",
Improving Leung's bidirectional learning rule for associative memories,"Leung (1994) introduced a perceptron-like learning rule to enhance the recall performance of bidirectional associative memories (BAMs). He proved that his so-called bidirectional learning scheme always yields a solution within a finite number of learning iterations in case that a solution exists. Unfortunately, in the setting of Leung a solution only exists in case that the training set is strongly linear separable by hyperplanes through the origin. We extend Leung's approach by considering conditionally strong linear separable sets allowing separating hyperplanes not containing the origin. Moreover, we deal with BAMs, which are generalized by defining so-called dilation and translation parameters enlarging their capacity, while leaving their complexity almost unaffected. The whole approach leads to a generalized bidirectional learning rule which generates BAMs with dilation and translation that perform perfectly on the training set in a case that the latter satisfies the conditionally strong linear separability assumption. Therefore, in the sense of Leung, we conclude with an optimal learning strategy which contains Leung's initial idea as a special case.","Magnesium compounds,
Associative memory,
Code standards,
Computer science,
Intelligent networks,
Neurons"
Prophesy: automating the modeling process,"Performance models provide significant insight into the performance relationships between an application and the system, either parallel or distributed, used for execution. The development of models often requires significant time, sometimes in the range of months, to develop; this is especially the case for detailed models. This paper presents our approach to reducing the time required for model development. We present the concept of an automated model builder within the Prophesy infrastructure, which also includes automated instrumentation and extensive databases for archiving the performance data. In particular, we focus on the automation of the development of analytical performance models. The concepts include the automation of some well-established techniques, such as curve fitting, and a new technique that develops models as a composition of other models of core components or kernels in the application.","Analytical models,
Instruments,
Databases,
Automation,
Performance analysis,
Data analysis,
Curve fitting,
Kernel,
Power system modeling,
NASA"
Digital filter design using multiple Pareto fronts,"Evolutionary approaches have been used in a large variety of design domains, from aircraft engineering to the designs of analog filters. Many of these approaches use measures to improve the variety of solutions in the population. In this paper, clustering and Pareto optimisation are combined into a single evolutionary design algorithm. The objective of this is to prevent the system from converging prematurely to a local minimum and to encourage a number of different designs that fulfil the design criteria. Our approach is demonstrated in the domain of digital filter design. Using a polar coordinate based pole-zero representation, two different lowpass filter design problems are explored. The results are compared to designs created by a human expert. The results demonstrate that the evolutionary process is able to create designs that are competitive with those created using a conventional design process by a human expert. They also demonstrate that each evolutionary run can produce a number of different designs with similar fitness values, but very different characteristics.","Digital filters,
Hardware,
Polynomials,
Equations,
Design engineering,
Pareto optimization,
Humans,
Computer science,
Programmable control,
Aircraft"
Minimal trap design,"This paper addresses the issue of trap design for sensorless automated assembly. First, we present a simple algorithm that determines in O(nm /spl alpha/(nm) log(nm)) time whether an n-sided polygonal part will fall through an m-sided polygonal trap. We then introduce the notion of a minimal trap for a polygonal part, and develop an algorithm to design a family of minimal feeders built from these traps. The algorithm runs in O(kn/sup 3+/spl epsiv//) time, where k is the number of stable orientations of P. Moreover, it is complete in the sense that we can always find a feeder, provided that one exists that rejects and supports the appropriate poses of the part.",
Multiple view geometry of non-planar algebraic curves,"We introduce a number of new results in the context of multi-view geometry from general algebraic curves. We start with the derivation of the extended Kruppa's equations which are responsible for describing the epipolar constraint of two projections of a general (non-planar) algebraic curve. As part of the derivation of those constraints we address the issue of dimension analysis and as a result establish the minimal number of algebraic curves required for a solution of the epipolar geometry as a function of their degree and genus. We then establish new results on the reconstruction of general algebraic curves from multiple views. We address three different representations of curves: (i) the regular point representation for which we show that the reconstruction from two views of a curve of degree d admits two solutions, one of degree d and the other of degree d(d-1), (ii) the dual space representation (tangents) for which we derive a lower bound for the number of views necessary for reconstruction as a function of the curve degree and genus, and (iii) a new representation (to computer vision) based on the set of lines meeting the curve which does not require any curve fitting in image space, for which we also derive lower bounds for the number of views necessary for reconstruction as a function of the curve degree alone.","Geometry,
Transmission line matrix methods,
Image reconstruction,
Computer science,
Layout,
Sparse matrices,
Mathematics,
Computer vision,
Surface reconstruction,
Machinery"
Deconstructing Shostak,"Decision procedures for equality in a combination of theories are at the core of a number of verification systems. R.E. Shostak's (J. of the ACM, vol. 31, no. 1, pp. 1-12, 1984) decision procedure for equality in the combination of solvable and canonizable theories has been around for nearly two decades. Variations of this decision procedure have been implemented in a number of specification and verification systems, including STP, EHDM, PVS, STeP and SVC. The algorithm is quite subtle and a correctness argument for it has remained elusive. Shostak's algorithm and all previously published variants of it yield incomplete decision procedures. We describe a variant of Shostak's algorithm, along with proofs of termination, soundness and completeness.",
A signal/noise analysis of quasi-static MR elastography,"In quasi-static magnetic resonance elastography, strain images of a tissue or material undergoing deformation are produced. In this paper, the signal/noise (S/N) ratio [SNR] of elastographic strain images, as measured by a phase-contrast technique, is analyzed. Experiments are conducted to illustrate how diffusion-mediated signal attenuation limits maximum strain SNR in small displacement cases, while the imaging point-spread function limits large displacement cases. A simple theoretical treatment agrees well with experiments and shows how an optimal displacement encoding moment can be predicted for a given experimental set of parameters to achieve a maximum strain SNR. A further experiment demonstrates how the limitation on strain SNR posed by the imaging point-spread function may potentially be overcome.",
Pipelining considerations for an FPGA case,"This paper presents a semi-synchronous pipeline scheme, here referred as single-pulse pipeline, to the problem of mapping pipelined circuits to a Field Programmable Gate Array (FPGA). Area and timing considerations are given for a general case and later applied to a systolic circuit as illustration. The single-pulse pipeline can manage asynchronous worst-case data completion and it is evaluated against two chosen asynchronous pipelining: a four-phase bundle-data pipeline and a doubly-latched asynchronous pipeline. The semi-synchronous pipeline proposal takes less FPGA area and operates faster than the two selected fully-asynchronous schemes for an FPGA case.",
Fast residue arithmetic multipliers based on signed-digit number system,"Fast residue arithmetic multipliers based on a radix-2 signed-digit (SD) number are presented. For a given modulus m, 2/sup p/-1/spl les/m/spl les/ 2/sup p/+2/sup p-1/-1, in a residue number system (RNS), the modulo m addition is performed by using one or two p-digit SD adders, and the modulo m addition time is independent of the word length of operands. We propose two kinds of modulo m multipliers which are constructed using a modulo m SD adder and a binary tree of the adders and the modulo m multiplication are performed in a time proportional to p and log/sub 2/p, respectively. 16-digit residue arithmetic multiplier circuits have been designed using VHDL, and the results show that high speed residue arithmetic circuits can be implemented.","Adders,
Circuits,
Equations,
Computer science,
Digital arithmetic,
Binary trees"
Better reasoning about software engineering activities,"Software management oracles often contain numerous subjective features. At each subjective point, a range of behaviors is possible. Stochastic simulation samples a subset of the possible behaviors. After many such stochastic simulations, the TAR2 treatment learner can find control actions that have (usually) the same impact despite the subjectivity of the oracle.","Software engineering,
Project management,
Stochastic processes,
Resource management,
Computer science,
Histograms,
Programming profession,
Costs"
A graphical representation of the state spaces of hierarchical level-of-detail scene descriptions,"We present a new graphical representation of the level-of-detail state spaces generated by hierarchical geometric scene descriptions with multiple levels of detail. These level-of-detail graphs permit the analytical investigation of the hierarchical level-of-detail optimization problem that arises for such descriptions. As an example of their use, we prove the equivalence of two hierarchical level-of-detail algorithms.","Layout,
State-space methods,
Algorithm design and analysis,
Computer Society,
Robustness,
Cost function,
Computer science,
Cities and towns,
Africa"
Multi-branch and two-pass HMM modeling approaches for off-line cursive handwriting recognition,"Because of large shape variations in human handwriting, cursive handwriting recognition remains a challenging task. Usually, the recognition performance depends crucially upon the pre-processing steps, e.g. the word baseline detection and segmentation process. Hidden Markov models (HMMs) have the ability to model similarities and variations among samples of a class. In this paper, we present a multi-branch HMM modeling method and an HMM-based two-pass modeling approach. Whereas the multi-branch HMM method makes the resulting system more robust with word baseline detection, the two-pass recognition approach exploits the segmentation ability of the Viterbi algorithm and creates another HMM set and carries out a second recognition pass. The total performance is enhanced by the combination of the two recognition passes. Experiments recognizing cursive handwritten words with a 30,000-word lexicon have been carried out. The results demonstrate that our novel approaches achieve better recognition performance and reduce the relative error rate significantly.","Hidden Markov models,
Handwriting recognition,
Character recognition,
Humans,
Image segmentation,
Shape,
Text recognition,
Feature extraction,
Computer science,
Robustness"
"Concepts, observations, and simulation of refractive index turbulence in the lower atmosphere","Advances in computers and in computational techniques now allow the calculation of electromagnetic (EM) wave propagation through simulated refractive index turbulence in the lower atmosphere. Such applications call for instantaneous turbulence fields, not turbulence statistics, the traditional focus of the turbulence community. We clarify their important differences and review what is known about key statistics of refractive index turbulence. We discuss the calculation of EM propagation with a parabolic equation model that uses composite refractive index fields, the larger scales being calculated with a dynamical mesoscale model and the smaller scales being calculated through large-eddy simulation. The locally, instantaneously sharp top of the atmospheric boundary layer can have a profound effect on forward scatter of EM waves. This top appears to be even sharper than is revealed by conventional measurements, particularly in the convective boundary layer.","Atmospheric modeling,
Refractive index,
Atmospheric waves,
Predictive models,
Ocean temperature,
Sea surface,
Mathematical model"
Selectivity estimation for spatial joins,"Spatial joins are important and time consuming operations in spatial database management systems. It is crucial to be able to accurately estimate the performance of these operations so that one can derive efficient query execution plans, and even develop/refine data structures to improve their performance. While estimation techniques for analyzing the performance of other operations, such as range queries, on spatial data has come under scrutiny, the problem of estimating selectivity for spatial joins has been little explored. The limited forays into this area have used parametric techniques, which are largely restrictive on the datasets that they can be used for since they tend to make simplifying assumptions about the nature of the datasets to be joined. Sampling and histogram based techniques, on the other hand, are much less restrictive. However, there has been no prior attempt at understanding the accuracy of sampling techniques, or developing histogram based techniques to estimate the selectivity of spatial joins. Apart from extensively evaluating the accuracy of sampling techniques for the very first time, this paper presents two novel histogram based solutions for spatial join estimation. Using a wide spectrum of both real and synthetic datasets, it is shown that one of our proposed schemes, called Geometric Histograms (GH), can accurately quantify the selectivity of spatial joins.",
Using the Cockroft-Walton voltage multiplier design in handheld devices,"A variation of the basic Cockroft-Walton (C-W) Voltage Multiplier circuit design may be used to generate multiple voltages at sufficient currents to drive the dynodes of a photomultiplier tube. In a battery-operated handheld device, the current draw on the batteries must be kept to a minimum. Several other parameters must be considered carefully during the design as well. Components must be chosen based on size restrictions, expected load current, expected output voltage range, and the maximum allowable ripple in the output voltage. A prototype surface mount C-W board was designed and tested to power two photomultipliers. The whole system, including the detectors, draws less than 15 mA of supply current with the outputs at 1000 VDC.","Voltage,
Handheld computers,
Power supplies,
Batteries,
Testing,
Circuits,
Photomultipliers,
Electric shock,
Hazards,
Design engineering"
The network performance of multi-rate FDD-mode UMTS,The performance of a multi-rate mobile cellular network using the Frequency Division Duplex (FDD) mode of the Code Division Multiple Access (CDMA) based Universal Mobile Telecommunication System (UMTS) is investigated. The new call blocking and call dropping probabilities and the tolerable network load are studied in the context of a multi-rate FDD-mode UMTS network incorporating dynamic threshold assisted soft handovers and shadow fading.,"3G mobile communication,
Multiaccess communication,
Resource management,
Power control,
Mobile computing,
Mobile communication,
Downlink,
Transmitters,
Computer science,
Frequency conversion"
Estimates on the packet loss ratio via queue tail probabilities,"We consider the connection between the packet loss ratio (PLR) in a switch with a finite buffer of size L and the tail distribution of the corresponding infinite buffer queue Q. In the literature the PLR is often approximated with the tail probability P(Q > L), and in practice the latter is often a good conservative estimate on the PLR. Therefore, efforts have mainly focused on finding bounds and asymptotic expressions concerning the tail probabilities of the infinite queue. However, our first result shows that the ratio PLR/P(Q > L) can be arbitrary, in particular the PLR can be larger than the tail probability. We also determine an upper bound on this ratio yielding an upper bound on the PLR using the tail distribution of the infinite queue. The bound is fairly tight for certain traffic patterns. In many situations it clearly improves the estimation with the tail probability, and it is rarely significantly larger than the estimate P(Q > L), while it is an upper bound. On the other hand, if the PLR is much smaller than P(Q > L), then our bound is usually loose. For this case a practically good approximation on their ratio is proposed.","Tail,
Traffic control,
Upper bound,
Quality of service,
Probability distribution,
Telecommunication traffic,
Switches,
Computer science,
Information theory,
Performance analysis"
A bipartition-codec architecture to reduce power in pipelined circuits,"This paper proposes a new approach to synthesize pipelined circuits with low-power consideration. We treat each output value of a combinational circuit as one state of a finite-state machine (FSM). If the output of a combinational circuit transits mainly among some few states, we could extract those states (output) and the corresponding input to build a subcircuit. After bipartitioning the circuit, we apply the encoding technique to the highly active subcircuit for further power reduction. In this paper, we formulate the bipartition problem and present a probabilistic-driven algorithm to bipartition a circuit so as to minimize the power dissipation. Our experimental results show that an average power reduction on several Microelectronic Center of North Carolina (MCNC) benchmarks of 31.6% is achievable.","Power dissipation,
Combinational circuits,
Clocks,
Logic,
Circuit synthesis,
Input variables,
Computer science,
Encoding,
Microelectronics,
Coupling circuits"
On the effectiveness of a counter-based cache invalidation scheme and its resiliency to failures in mobile environments,"Caching frequently accessed data items on the client side is an effective technique to improve the performance of data dissemination in mobile environments. Classical cache invalidation strategies are not suitable for mobile environments due to the disconnection and mobility of the mobile clients. One attractive cache invalidation technique is based on invalidation reports (IRs). However, IR-based approach suffers from long query latency and it cannot efficiently utilize the broadcast bandwidth. In this paper, we propose techniques to address these problems. We first extend the UIR-based approach to reduce the query latency. Then, we propose techniques to efficiently utilize the broadcast bandwidth based on counters associated with each data item. Novel techniques are designed to maintain the accuracy of the counter in case of server failures, client failures, and disconnections. Extensive simulations are provided and used to evaluate the proposed methodology. Compared to previous IR-based algorithms, the proposed solution can significantly reduce the query latency, improve the bandwidth utilization, and effectively deal with disconnections and failures.","Delay,
Bandwidth,
Broadcasting,
Batteries,
Mobile computing,
Counting circuits,
Computer science,
Data engineering,
Explosives,
Portable computers"
PenCalc: a novel application of on-line mathematical expression recognition technology,"Most of the calculator programs found in existing pen-based mobile computing devices, such as personal digital assistants (PDA) and other handheld devices, do not take full advantages of the pen technology offered by these devices. Instead, input of expressions is still done through a virtual keypad shown on the screen, and the stylus (i.e., electronic pen) is simply used as a pointing device. In this paper we propose an intelligent handwriting-based calculator program with which the user can enter expressions simply by writing them on the screen using a stylus. In addition, variables can be defined to store intermediate results for subsequent calculations, as in ordinary algebraic calculations. The proposed software is the result of a novel application of on-line mathematical expression recognition technology which has mostly been used by others only for some mathematical expression editor programs.","Personal digital assistants,
Handheld computers,
Application software,
Mobile computing,
Handwriting recognition,
Keyboards,
Computer science,
Writing,
Linux,
Pattern matching"
Hardware implementation for fast convolution with a PN code using field programmable gate array,"In a code division multiple access (CDMA) system, receivers spend a long time acquiring the signals. This is mostly due to the use of expensive FFT-based convolvers in the acquisition process. The paper shows a substitute algorithm for calculating the convolution that requires less computation time. The algorithm uses a Walsh transform instead of FFTs. The FFT-based algorithm requires 2 FFTs and one IFFT in addition to complex multiplications and additions. On the other hand, in the Walsh-based method the Walsh transform is required once and there is no multiplication. Therefore, using the Walsh-based algorithm can cut the processing time to about 5 percent of the required time. The additional steps in this algorithm are the permutation of the input samples and the output results. The design uses a field programmable gate array (FPGA) to apply a parallel processing concept. The paper discusses the algorithm and the implementation issues. A case study of a large code was carried out. The whole system has been implemented and showed high performance that speeds up the process to 2500 times the speed of a microprocessor based design.",
An overview of classical and fuzzy-classical filters for noise reduction,"In this paper we give an overview of classical and fuzzy-classical filters for image noise reduction. Together with our overview (2001) of fuzzy filters, this paper can be seen as a preparation to our comparative study of classical and fuzzy filters for image noise reduction.",
GMSOCKS - a direct sockets implementation on myrinet,,"Sockets,
TCPIP,
Storage area networks,
Application software,
Ethernet networks,
Delay,
Protocols,
Registers,
Kernel,
Computer science"
Measures of ruggedness using fuzzy-rough sets and fractals: applications in medical time series,"This paper attempts to characterize the medical time series by quantifying the ruggedness of the time series. The presence of two close data points on the time axis implies that these points are similar along the time axis. It creates the fuzzy similarity. Following the principle ""similar causes create similar effects"", we expect that the magnitudes corresponding to those two data points should also be similar. However, if other features are considered along with the time information, then those two apparently similar data points might look different. The closeness creates the fuzziness, the one-to-many relationship creates the roughness, and together they form fuzzy-roughness. If the ruggedness is expressed as the fuzzy-roughness, then in some time series it is observed that the fuzzy-roughness of a part of the time series is similar to that of the whole time series. Experiments on ICU data sets show that the ruggedness measure using the fuzzy-rough set based fractal dimension is more robust than the Hurst exponent which is used frequently to measure the ruggedness of a fractal time series.","Time measurement,
Fractals,
Statistics,
Application software,
Dispersion,
Fourier transforms,
Biomedical computing,
Computer science,
Robustness,
Electrocardiography"
A new rate allocation scheme for progressive fine granular scalable coding,"We examine the enhancement layer rate allocation problem in progressive fine granular scalable (PFGS) coding. The problem arises from the fact that different frames in the enhancement layer have different rates in PFGS coding. A joint operational rate-distortion (R-D) function for a multi-frame group is first established in enhancement layer PFGS coding, followed by experiments to verify its validity using real test sequences. The optimal rate allocation among frames in the group for the enhancement layer PFGS coding is then given based on the R-D function. Coding results show that, compared to uniform bit allocation, the new bit allocation scheme not only makes the quality variation in decoded video frames smoother, but also improves the coding efficiency of PFGS coding by up to 1.0 dxxB in PSNR. Furthermore, the new scheme has very low complexity, making it suitable for video multicast applications.",
An architecture-based approach substantiating interagent connections in platforms,"The subject of this paper is the functional description and refinement of a connector service to manage dynamic interactions in multi-service agent platforms. First, the concept of architectural frames is introduced as a common development framework for application systems and interpreted for the class of agent systems. For the program-technical realization and execution of agent application systems, services are needed meeting the concepts of the architecture-based approach. Since platform services are based on application-independent building blocks and concepts, they are derived and developed by examining, analyzing and abstracting the requirements of application-technical building blocks on platforms. This is discussed for the aspect of inter-agent connections: a common process model of inter-agent connection is next introduced. Analyzing its partial processes, process types are identified and assigned to components of the connector service. The connector service is modeled object-orientedly and mapped onto a multi-service agent platform named ADE (Architecture type-based Development Environment), corresponding to concepts of the architecture-based approach.","Connectors,
Object oriented modeling,
Computer science,
Petroleum,
Minerals,
Refining,
Computer architecture,
Software architecture,
Application software,
Software agents"
Formal treatment of certificate revocation under communal access control,"The conventional approach to distributed access control (AC) tends to be server-centric. Under this approach, each server establishes its own policy regarding the use of its resources and services by its clients. The choice of this policy, and its implementation, are generally considered the prerogative of each individual server. This approach to access control may be appropriate for many current client-server applications, where the server is an autonomous agent, in complete charge of its resources. It is not suitable for the growing class of applications where a group of servers, and sometimes their clients, belong to a single enterprise, and are subject to the enterprise-wide policy governing them all. One may not be able to entrust such an enterprise-wide policy to the individual servers, for two reasons: first, it is hard to ensure that an heterogeneous set of servers implement exactly the same policy. Second, as demonstrate, an AC policy can have aspects that cannot, in principle, be implemented by servers alone. As argued in a previous paper (Minsky, 2000), what is needed in this situation is a concept of communal policy that governs the interaction between the members of a distributed community of agents involved in some common activity along with a mechanism that provides for the explicit formulation of such policies, and for their scalable enforcement. We focus on the communal treatment of expiration and revocation of the digital certificates used for the authentication of the identity and roles of members of the community.",
Distributed sequential computing using mobile code: Moving computation to data,"Sequential computations can benefit from a distributed environment consisting of a network of workstations through the use of a mobile agent system. We found significant performance improvement when sequential algorithms for solving large industrial problems are implemented using mobile agents. This is because raw data is distributed so that the cost of disk paging is completely eliminated, and a principle of ""code moving to data"" is followed to achieve efficient communication through the network. We argue that mobile agent systems provide a new level of abstraction in which application programming is made easier because the sequential algorithms remain essentially unchanged in mobile agent code.","Distributed computing,
Mobile computing,
Workstations,
Mobile agents,
Computer networks,
Concurrent computing,
Computer science,
Costs,
Mobile communication,
Linear systems"
Determining dramatic intensification via flashing lights in movies,,
Framework for third party testing of component software,"To ensure that a component-based software system can run properly and effectively, the qualities of constituent components have to be assured. Third-party certification is a safe approach that buyers should trust when dealing with component software. However, current third-party certification methods do not support proper functional testing, which is the most important factor when selecting the component. In this paper, we will suggest a framework of third party component testing of functionality, satisfying the following given constraint. evaluation of a large number of components within short period of time in a cost-effective way. In this framework, we identify, the metadata that a component developer should provide to the third party tester and define the process for third party testing using these metadata. To evaluate our framework, we conducted several experiment with Component Test Manager (CTM), prototype tool for EJB component testing. The result shows that our metadata are positively necessary and the participation of a component developer is the key to third party testing.","Software testing,
Software systems,
Software safety,
Software prototyping,
Prototypes,
Computer science,
Software engineering,
System testing,
Costs,
Assembly systems"
Program execution based module cohesion measurement,"Module cohesion describes the degree to which different actions performed by a module contribute towards a unified function. High module cohesion is a desirable property of a program. The program modifications during successive maintenance interventions can have negative effect on the structure of the program resulting in less cohesive modules. Therefore, metrics that measure module cohesion are important for software restructuring during maintenance. The existing static slice based module cohesion metrics significantly overestimate cohesion due to the limitations of static slicing. In this paper, we present a novel program execution based approach to measure module cohesion of legacy software. We define cohesion metrics based on definition-use pairs in the dynamic slices of the outputs. Our approach significantly improves the accuracy of cohesion measurement. We implemented our technique and measured module cohesion for several programs. Cohesion measurements using our technique were found to be more insightful than static slice based measurements.",
High-order window functions and fast algorithms for calculating dyadic electromagnetic Green's functions in multilayered media,"A type of bell-shaped compact high-order window function (x, y) is studied, and closed-form and approximation formulas are derived for its Hankel transforms. Applications of these results are presented as fast algorithms for calculating dyadic Green's functions for electromagnetic scattering in multilayered media. Numerical results in electromagnetics are provided to demonstrate the accuracy and efficiency of the algorithms.","Algorithm design and analysis,
Green's function methods,
Fourier transforms,
Approximation algorithms,
Media,
Electromagnetics"
On multiagent co-ordination architectures: a traffic management case study,"Reports our experiences with agent-based architectures for intelligent traffic management. We describe and compare TRYS and TRYSA/sub 2/, two multi-agent systems that perform decision support for real-time traffic management in the urban motorway network around Barcelona. Both systems draw upon similar traffic management knowledge, but the former is based on a centralised architecture, while in the latter, co-ordination emerges upon the lateral interaction of autonomous traffic management agents. We conclude that the centralised approach applied by TRYS promotes efficiency for real-time operation, whereas the decentralised approach used in TRYSA/sub 2/ promotes scalability.",
"Enabling effective learning, curriculum delivery reform at the University of Denver",The Engineering Department at the University of Denver (USA) has received a Sturm Program Development Award for reform of its curricula. Specifically they have: (1) changed the learning environment from predominately teacher centered to student centered; (2) implemented a quality based grading method that allows students to know what is expected of them and to actually take part in the procedure; (3) created more studio type classrooms with current technology to facilitate learning and demonstrate state of the art engineering methods and procedures; and (4) blended engineering topics with English topics in an integrated freshman sequence.,
Timed Petri net models of multi-robot cluster tools,"A systematic approach to modeling the stationary behavior of cluster tools by timed Petri nets is presented. The performance of the derived models is evaluated by structural methods (place invariants), without the exhaustive generation of the state space. Multiple robots are introduced to reduce the limitations of ""transport bound"" tools. Simple examples are provided to illustrate the proposed approach.",
Recognition of face profiles from the mugshot database using a hybrid connectionist/HMM approach,"Biometrical systems have been the focus of concentrated research efforts in recent years. These systems can be used to identify a person or to grant a person access to something, e.g., a room. Face recognition technology has reached a level of performance at which frontal-view recognition of faces with slightly different facial expressions, view angles or head poses can be considered nearly solved. We present a novel hybrid ANN/HMM approach to recognize a person from that person's profile view (90) although the recognition system is trained with only one single frontal view of the person. Such a system can be useful for mugshot identification where a victim or witness has seen the criminal from the side only. Our approach uses neural methods in order to synthesize a profile out of the frontal view using no additional knowledge about the 3D shape and structure of a human head. The classification of the generated images is accomplished using a statistical HMM-approach.","Face recognition,
Hidden Markov models,
Head,
Image databases,
Computer science,
Focusing,
Shape,
Humans,
Image generation,
Pattern recognition"
The Visual Acts model for automated camera placement during teleoperation,"This paper describes the Visual Acts theory and architecture for automated camera placement during teleoperation. The theory is motivated by the goal to provide operators with task relevant information in a timely manner. The Visual Acts architecture combines top-down deliberative task modelling with bottom-up reactive observation of an operator to select camera views which provide the operator with task-relevant information. The paper describes the reactive component of the architecture, its implementation in an agent-based architecture and experimental results that confirm the validity of the approach.",
IJCNN 2001 challenge: generalization ability and text decoding,In IJCNN 2001 a new event is a competition with three challenging problems. In this paper we describe our approaches on solving the first two problems: generalization ability challenge and teat decoding challenge. The main learning technique used is the support vector machine.,"Decoding,
Support vector machines,
Support vector machine classification,
Training data,
Machine learning,
Computer science,
Length measurement,
Risk management,
Error correction"
A complex mechatronic system: from design to application,"Progress in mobile robotics requires the researchers to access and improve all modules that compose the robot, from low-level mechanical components to high-level reasoning systems. The paper presents the development process of the robots built at the Autonomous Systems Lab, EPFL Lausanne, Switzerland. Starting from the mechanical and electrical design up to the application, we show the challenges that needed to be faced as well as the solutions that have been devised. The description covers aspects like the operating system and framework, because of its role in the overall safety and dependability of the whole software system, the research as a precondition for innovative products, and the man-machine interface, which is indispensable for conveying information to the user as well as allowing the user to interact with the robot. The issues that have been faced stem from the hierarchical, layered construction of a complex mechatronic product, where the operation of the machine depends on the smooth cooperation of each layer. In the same way, the overall safety is undermined by the least reliable piece building the system.","Mechatronics,
Mobile robots,
Computer science,
Operating systems,
Product safety,
Application software,
Software safety,
Software systems,
User interfaces,
Buildings"
Process oriented knowledge management,"Knowledge management and process orientation meanwhile are well established techniques. Nevertheless, their practical use still shows some severe deficiencies. We show that these shortcomings can optimally be overcome when leveraging both techniques on each other. This idea consequently leads to the idea of process oriented knowledge management. The paper presents the basic idea of process oriented knowledge management as well as a first prototype implementation.","Knowledge management,
Aggregates,
Computer science,
Database systems,
Prototypes,
Productivity,
Time to market,
Pulp manufacturing,
Costs,
Appropriate technology"
A medium access control protocol with token passing and retransmission by the hub station in the asynchronous transfer mode of Wireless 1394,"We propose a medium access protocol with token passing and retransmission by the hub station in the asynchronous transfer mode of Wireless 1394 in high data rate home network. In the proposed protocol, to improve the waiting time performance, the token passing is adopted. Moreover, when the data packets are received incorrectly because of the shadowing, the hub station transmits the corresponding data packets instead of transmitting from the same station to improve the recovery rate performance. By the performance evaluation using computer simulations, we show that the proposed protocol can improve the waiting time performance and the recovery rate performance comparing to the conventional Wireless 1394.",
Web accessibility,"Summary form only given, as follows. Web accessibility encompasses a variety of concerns ranging from societal, political, and economic to individual, physical, and intellectual through to the purely technical. Thus, there are many perspectives from which web accessibility can be understood. In order to discuss these concerns and to gain a better understanding of web accessibility, an accessibility framework is proposed, using as its base a layered evaluation framework from Computer Supported Cooperative Work research and the ISO standard, ISO/IEC 9126 on software quality. The former recognises the collaborative nature of the web and its importance in facilitating communication. The latter is employed to refine and extend the technical issues and to highlight the need for considering accessibility from the viewpoint of the web developer and maintainer as well as the web user. A technically inaccessible web is unlikely to be evolved over time. A final goal of the accessibility framework is to provide web developers and maintainers with a practical basis for considering web accessibility through the development of a set of accessibility factors associated with each identified layer.","Collaborative work,
ISO standards,
Computer science,
Laboratories,
IEC standards,
Software quality,
Conferences"
Data dissemination approaches for performance discovery in grid computing systems,,
Fuzzy modeling based signature verification system,"Presents an approach for the verification of a signature using fuzzy modeling. For feature extraction, a signature is enclosed in a box. Taking the left side bottom corner of the box as the origin, angles of all pixels are calculated and then distributions of angles are generated using fixed class intervals. By considering these distributions as fuzzy sets, a Takagi-Sugeno model is constructed by defining the output of the signature to be a fixed number. This model is then used for the twin-purpose of verification and forgery detection. The results are demonstrated on several signature samples.","Fuzzy systems,
Handwriting recognition,
Feature extraction,
Forgery,
Data mining,
Takagi-Sugeno model,
Computer science,
Educational institutions,
Banking,
Law enforcement"
Generalized regression neural networks for biomedical image interpolation,"A neural-statistical approach to biomedical image interpolation using generalized regression neural networks is presented. These networks are basis function architectures that approximate any arbitrary function between input and output vectors directly from training samples, and with any desired degree of smoothness, and thus can be used for multidimensional interpolation. Experimental results compare favorably with other interpolation techniques. Because of their flexibility and ease of training, generalized regression networks can be used to complement existing approaches, and can be especially useful for post-registration image fusion and visualization.","Neural networks,
Biomedical imaging,
Interpolation,
Kernel,
Biomedical engineering,
Spline,
Computer science,
Biomedical computing,
Computer networks,
Visualization"
Meta-learning with backpropagation,"Introduces gradient descent methods applied to meta-learning (learning how to learn) in neural networks. Meta-learning has been of interest in the machine learning field for decades because of its appealing applications to intelligent agents, non-stationary time series, autonomous robots, and improved learning algorithms. Many previous neural network-based approaches toward meta-learning have been based on evolutionary methods. We show how to use gradient descent for meta-learning in recurrent neural networks. Based on previous work on fixed-weight learning neural networks, we hypothesize that any recurrent network topology and its corresponding learning algorithm(s) is a potential meta-learning system. We tested several recurrent neural network topologies and their corresponding forms of backpropagation for their ability to meta-learn. One of our systems, based on the long short-term memory neural network developed a learning algorithm that could learn any two-dimensional quadratic function (from a set of such functions) after only 30 training examples.",
In search of an easy witness: exponential time vs. probabilistic polynomial time,"Restricting the search space {0, 1}/sup n/ to the set of truth tables of ""easy"" Boolean functions on log n variables, as well as using some known hardness-randomness tradeoffs, we establish a number of results relating the complexity of exponential-time and probabilistic polynomial-time complexity classes. In particular, we show that NEXP/spl sub/P/poly/spl hArr/NEXP=MA; this can be interpreted to say that no derandomization of MA (and, hence, of promise-BPP) is possible unless NEXP contains a hard Boolean function. We also prove several downward closure results for ZPP, RP, BPP, and MA; e.g., we show EXP=BPP/spl hArr/EE=BPE, where EE is the double-exponential time class and BPE is the exponential-time analogue of BPP.","Polynomials,
Boolean functions,
Computer science,
Encoding,
Complexity theory,
Concrete,
Circuit simulation,
Algorithm design and analysis"
Deterministic generators and games for LTL fragments,"Deciding infinite two-player games on finite graphs with the winning condition specified by a linear temporal logic (LTL) formula is known to be 2EXPTIME-complete. In this paper, we identify LTL fragments of lower complexity. Solving LTL games typically involves a doubly-exponential translation from LTL formulas to deterministic /spl omega/-automata. First, we show that the longest distance (length of the longest simple path) of the generator is also an important parameter, by giving an O(d log n)-space procedure to solve a Buchi game on a graph with n vertices and longest distance d. Then, for the LTL fragment with only eventualities and conjunctions, we provide a translation to deterministic generators of exponential size and linear longest distance, show both of these bounds to be optimal and prove the corresponding games to be PSPACE-complete. Introducing ""next"" modalities in this fragment, we provide a translation to deterministic generators that is still of exponential size but also with exponential longest distance, show both bounds to be optimal and prove the corresponding games to be EXPTIME-complete. For the fragment resulting by further adding disjunctions, we provide a translation to deterministic generators of doubly-exponential size and exponential longest distance, show both bounds to be optimal and prove the corresponding games to be EXPSPACE. Finally, we show tightness of the double-exponential bound on the size as well as the longest distance for deterministic generators for LTL, even in the absence of ""next"" and ""until"" modalities.","Open systems,
Logic,
Engineering profession,
Computational modeling,
Automata,
NIST"
Hybrid face recognition systems for profile views using the MUGSHOT database,"Face recognition has established itself as an important sub-branch of pattern recognition within the field of computer science. Many state-of-the-art systems have focused on the task of recognizing frontal views or images with just slight variations in head pose and facial expression of people. We concentrate on two approaches to recognize profile views (90 degrees) with previous knowledge of only the frontal view, which is a challenging task even for human beings. The first presented system makes use of synthesized profile views and the second one uses a joint parameter estimation technique. The systems we present combine artificial neural networks (NN) and a modeling technique based on hidden Markov models (HMM). One of the main ideas of these systems is to perform the recognition task without the use of any 3D-information of heads and faces such as a physical 3D-models, for instance. Instead, we represent the rotation process by a NN, which has been trained with prior knowledge derived from image pairs showing the same person's frontal and profile view. Another important restriction to this task is that we use exactly one example frontal view to train the system to recognize the corresponding profile view for a previously unseen individual. The presented systems are tested with a sub-set of the MUGSHOT database.","Face recognition,
Hidden Markov models,
Head,
Neural networks,
Pattern recognition,
Computer science,
Image recognition,
Humans,
Network synthesis,
Parameter estimation"
Design of Haar wavelet transforms and Haar spectral transform decision diagrams for multiple-valued functions,"In spectral interpretation, decision diagrams (DDs) are defined in terms of some spectral transforms. For a given DD, the related transform is determined by an analysis of expansion rules used in the nodes and the related labels of edges. The converse task, design of a DD in terms of a given spectral transform often requires decomposition of basic functions in spectral transform to determine the corresponding expansion rules and labels at the edges. We point out that this problem relates to the assignment of nodes in Pseudo-Kronecker DDs(PKDDs). Due to that, we generalize the definition of Haar spectral transform DDs (HSTDDs) to multiple-valued (MV) functions. Conversely, from such defined HSTDDs, we derive various Haar transforms for MV functions.","Wavelet transforms,
Data structures,
Discrete transforms,
Computer science,
Boolean functions,
Signal resolution,
Matrix decomposition,
Logic design"
Tool support for production use of formal techniques,"The relatively scant use of formal techniques in software development is the result, in part, of a lack of suitable support tools. Many tools have been developed that provide novel analysis capabilities but lack basic yet commonplace facilities which are essential in production software development. More importantly, many existing tools for the development of formal specifications fail to provide mechanisms for the manipulation of natural language despite the fact that natural language is essential to give meaning to the terms in the formal specification. In this paper, we describe a toolset that has been developed with the specific intent of providing comprehensive facilities for creating formal specifications in production software development. The toolset supports a powerful formal notation, Z but also provides comprehensive and fully integrated support for natural language. As well as describing the toolset we present a preliminary evaluation of its use on a commercial specification.","Production,
Formal specifications,
Natural languages,
Programming,
Industrial training,
Computer science,
Application software,
Computer industry,
Personnel,
Software packages"
Joint UEP and layered source coding with application to transmission of JPEG-2000 coded images,"This paper presents a joint source-channel coding framework based on layered source coding and Reed-Solomon channel coding for unequal error protection. An iterative procedure is described to search for the best source coding rate and the optimal UEP of layered bitstreams. We apply our JSCC technique to transmission of JPEG-2000 coded images over binary symmetric channels. Compared to results reported in the literature, our UEP based approach gives better results while having lower complexity.","Source coding,
Reed-Solomon codes,
Error correction codes,
Decoding,
Image reconstruction,
Application software,
Computer science,
Image coding,
Channel coding,
Computer errors"
Becoming a virtual professor: pedagogical roles and ALN,"This paper presents a qualitative study of role changes that occur when faculty becomes virtual professors. In 20 semi-structured interviews of faculty, coded with pattern analysis software, the authors captured role changes enacted by instructors in asynchronous learning network (ALN) settings-cognitive roles, affective roles, and managerial roles. The cognitive role, which relates to mental processes of learning, learning, information storage, and thinking, shifts to one of deeper cognitive complexity. The affective role, which relates to influencing the relationships between students, the instructor, and the classroom atmosphere, required faculty to find new tools to express emotion, yet they found the relationship with students more intimate. The managerial role, which deals with class and course management, requires greater attention to detail, more structure, and additional student monitoring. Overall, facility reported a change in their teaching persona, towards more precision in their presentation of materials and instructions, combined with a shift to a more Socratic pedagogy, emphasizing multilogues with students.","Education,
Information systems,
Pattern analysis,
Atmosphere,
Monitoring,
Web sites,
IP networks,
Online Communities/Technical Collaboration,
Information technology,
Computer science"
Image detection under varying illumination and pose,"This paper focuses on the detection of objects with Lambertian surface under both varying illumination and pose. We offer to apply a novel detection method that proceeds by modeling the different illuminations from a small number of images in the training set; this automatically voids the illumination effects, allowing fast illumination invariant detection, without having to create a large training set. It is demonstrated that the method ""fits in"" nicely with previous work about the modeling of the set of object appearances under varying illumination. In the experiments, an object was correctly detected under image plane rotations in a 15-degrees range, and a wide variety of different illuminations.","Lighting,
Light sources,
Object detection,
H infinity control,
Pixel,
Shadow mapping,
Computer science,
Reflectivity,
Singular value decomposition"
Safety grounding approach for the National Ignition Facility power conditioning system,"This paper describes a set of analyses and tests performed to evaluate approaches to provide a safe and robust grounding approach for the main Power Conditioning System (PCS) in the National Ignition Facility (NIF) facility presently under construction at the Lawrence Livermore National Laboratory (LLNL). The Power Conditioning System consists of up to 192 capacitor bank modules, each storing 2.2 MJ and capable of producing a peak current over 500 kA. The grounding system must minimize touch potentials associated with operation of the Power Conditioning System. In the event of severe faults, the system must assure that the energy delivered to a person through contact with ""grounded"" structures is very low. Based on computer modeling and low-voltage, low current tests, we have concluded that the most effective approach is a set of metal enclosures around the output cables (effectively heavy-wall closed cable trays) extending from the capacitor bank modules to their flashlamp loads. This paper will discuss the safety standards identified for this application, the approach to meeting the standards, and the predicted performance of the safety system.","Safety,
Grounding,
Ignition,
Power conditioning,
Capacitors,
Cables,
Performance analysis,
System testing,
Performance evaluation,
Robustness"
How does TCP generate pseudo-self-similarity?,"Long-range dependence has been observed in many recent Internet traffic measurements. In addition, some recent studies have shown that under certain network conditions, TCP itself can produce traffic that exhibits dependence over limited timescales, even in the absence of higher-level variability. In this paper, we use a simple Markovian model to argue that when the loss rate is relatively high, TCP's adaptive congestion control mechanism indeed generates traffic with OFF periods exhibiting power-law shape over several timescales and thus introduces pseudo-long-range dependence into the overall traffic. Moreover, we observe that more variable initial retransmission timeout values for different packets introduces more variable packet inter-arrival times, which increases the burstiness of the overall traffic. We can thus explain why a single TCP connection can produce a time-series that can be misidentified as self-similar using standard tests.",
A scalable approach to visualization of large virtual cities,Visualization of large urban complexes on the Web is a highly demanding task both from the networking and computational point of view. The whole three-dimensional model of a city is mostly too big to be downloaded in a reasonable time and to be stored in a memory of commonly used computers. This paper presents a scalable approach for subdividing an urban model into smaller parts. Dynamic loading and unloading data allows theoretically unlimited extension of a model with constant network and computation load. Further requirements specific to presentation of virtual cities are also discussed. Theoretical considerations are followed by a practical implementation utilizing the VRML language for modeling the historical city of Prague.,"Visualization,
Cities and towns,
Computer networks,
Graphics,
Rendering (computer graphics),
Computer science,
Mathematics,
Physics computing,
Central Processing Unit,
Acceleration"
A methodology for creating e-business strategy,"In the ""now"" economy, knowledge, trust, technology, and the relationships among stakeholders are the keys to success. Although for almost eighty years, strategy literature stated that these concepts are important, we were not in a position to effectively leverage and/or effectively execute knowledge and relationship management in real time until the turn of the twenty-first century. Many companies have not yet adjusted the way they work to the capabilities of the present-day knowledge management and technology enablers. Also widespread is the fact that companies have not yet developed methodologies or models to create e-business strategy to support corporate strategies, or to monitor the success of an e-business strategy. The paper develops a methodology to create e-business strategy based on a simple e-business model for e-business. The primary stakeholders are the customer, operational partner, strategic partner, governance, and community. The value propositions for each stakeholder is outlined, and questions that business should ask are provided to guide in developing the e-business strategy. A sample mini-case study is developed to illustrate application of the methodology. This methodology supports the eBizReadiness!/sup TM/ framework which details infrastructure requirements for e-business.","Companies,
Knowledge management,
Business,
Costs,
Content management,
Computer science,
Monitoring,
Conference management,
Technology management,
Internet"
Equivalence checking of integer multipliers,"In this paper, we address on equivalence checking of integer multipliers, especially for the multipliers without structure similarity. Our approach is based on Hamaguchi's backward substitution method with the following improvements: (1) automatic identification of components to form proper cut points and thus dramatically improve the backward substitution process; (2) a layered-backward substitution algorithm to reduce the number of substitutions; and (3) Multiplicative Power Hybrid Decision Diagrams (*PHDDs) as our word-level representation rather than *BMD in Hamaguchi's approach. Experimental results show that our approach can efficiently check the equivalence of two integer multipliers. To verify the equivalence of a 32/spl times/32 array multiplier versus a 32/spl times/32 Wallace tree multiplier, our approach takes about 57 CPU seconds using 11 Mbytes, while Stanion's approach took 21027 seconds using 130 MBytes. We also show that the complexity of our approach is upper bounded by O(n/sup 4/), where n is the word size, but our experimental results show that the complexity of our approach grows cubically O(n/sup 3/).",
KelpIO: a telescope-ready domain-specific I/O library for irregular block-structured applications,"To ameliorate the need to spend significant programmer time modifying parallel programs to achieve high-performance, while maintaining compact, comprehensible source codes, the paper advocates the use of telescoping language technology to automatically apply, during the normal compilation process, high-level performance enhancing transformations to applications using a high-level domain-specific I/O library. We believe that this approach will be more acceptable to application developers than new language extensions, but will be just as amenable to optimization by advanced compilers, effectively making it a domain-specific language extension for I/O. The paper describes a domain-specific I/O library for irregular block-structured applications based on the KeLP library, describes high-level transformations of the library primitives for improving performance, and describes how a high-level domain-specific optimizer for applying these transformations could be constructed rising the telescoping languages framework.","Libraries,
Application software,
Programming profession,
Optimizing compilers,
High performance computing,
Costs,
Computer science,
Space technology,
Aggregates,
Data structures"
Intelligent decision support of Support and Stability Operations (SASO) through symbolic visualization,"The Advanced Tactical Architecture for Combat Knowledge System (ATACKS, formerly known as ABATIS) has been designed as a visualization tool to support commander's decision-making in a complex battlespace environment. ATACKS expands standard battlefield symbology by providing symbols for Stability and Support Operation (SASO) on three dimensional (3D) abstract battlespace terrains. Furthermore, it extends normal spatial visualization through process centered displays that seek to enhance the commander's understanding of the situation by presenting qualitative data in novel formats. In addition, external decision support tools communicate with ATACKS, through an application program interface (API) that can allow communication over a network as well as between systems on differing operating systems. With this capability, ATACKS can serve as an integration point for intelligent aiding systems'.","Stability,
Visualization,
Cognitive science,
Displays,
Intelligent systems,
Humans,
Military computing,
Computer architecture,
Knowledge based systems,
Decision making"
Design and evaluation of a parallel-polled virtual output queued switch,"Input-buffered switches with virtual output queueing require crossbar switch matrix scheduling algorithms. Existing scheduling algorithms are non-deterministic and are based on parallel and iterative request-grant-accept arbitration schemes. This presents challenges to flow-level scheduling for guaranteed throughput and bounded delay services and also to scalability. In this paper, the parallel-polled virtual output queued (PP-VOQ) switch is presented. Using parallel token passing, the PP-VOQ switch has deterministic and bounded scheduling delay and is implementable for 16 or 32 ports of 10-Gigabit Ethernet. The PP-VOQ switch is shown, via simulation, to perform very similar to an iSLIP switch. The PP-VOQ switch is extended to a ""cube switch"" design that decouples input port VOQ selection from output port selection to reduce the scheduling delay and improve scalability. This decoupled parallel polling is very scalable and is shown to result in better performance than an iSLIP switch.","Switches,
Scheduling algorithm,
Delay,
Packet switching,
Throughput,
Quality of service,
Stability,
Scalability,
Ethernet networks,
Computer science"
OLAP databases and aggregation functions,Aggregation functions are a class of generic functions which must be usable in any database application. We characterize the case where the aggregation functions can be correctly applied on macrodata (data cube) which are computed on the microdata.,"Aggregates,
Computer science,
Algebra,
Marketing and sales,
Production,
Operations research,
Application software,
Relational databases,
H infinity control,
Filtration"
Combining the animation and testing of abstract data types,"A formal specification animator interprets and executes specifications to give them the appearance of liveliness. This can be used to identify errors in a specification because it provides concrete examples of the behaviour of the specification. By providing a suitable user interface, it allows users unfamiliar with specification languages and notations to interact with the specification. We exploit the similarities between animation and testing, and present a method that combines specification animation and software testing of abstract data types (ADTs). Tool support is provided by Peach, which supports the animation of a specification and the testing of an implementation of that specification. We demonstrate the use of our method and tool on a small example, and discuss its application to a larger example.",
Computers play the Beer Game: can artifical agents manage supply chains,"In this study, we model an electronic supply chain that is managed by artificial agents. We investigate whether artificial agents do better than humans when playing the MIT Beer Game. Can the artificial agents mitigate the Bullwhip effect or discover good and effective business strategies? In particular, we study the following questions: can agents learn reasonably good policies in the face of deterministic demand with fixed lead-time? Can agents cope reasonably well in the face of stochastic demand with stochastic lead-time? Can agents learn and adapt in various contexts to play the game? Can agents cooperate across the supply chain?.","Supply chain management,
Supply chains,
Costs,
Games,
Humans,
Stochastic processes,
Stochastic systems,
Delay effects,
Educational institutions,
Virtual enterprises"
Teaching to identify problems in a creative way,"Identification and specification belong to the initial stages of problem management. Traditionally, creative methods in these phases have not been considered a part of a computer scientist's expertise or skills. For innovative systems, however, it is crucial that computer experts have a stronger contribution in formulating and even recognizing problems. Managing a problem up to its solution as a software product is a long process that needs creativity and flexibility. Thus, in addition to the basic elements of computer science curriculum, namely knowledge and skills, also training at attitudinal level is required. A dedicated course in creative problem management as a combination of these elements. Preliminary feedback of a pilot course indicates that the course increased students' motivation and supported their self-image as creative agents.",
Bayesian data mining on the Web with B-Course,"B-Course is a free Web based Bayesian data mining service. This service allows the users to analyze their own data for multivariate probabilistic dependencies represented as Bayesian network models. In addition to this, B-Course also offers facilities for inferring certain types of causal dependencies from the data. The software is especially suitable for educational purposes as the tutorial style user friendly interface intertwines steps in the data analysis with support material that gives an informal introduction to the Bayesian approach adopted. Nevertheless, although the analysis methods, modeling assumptions and restrictions are totally transparent to the user, this transparency is not achieved at the expense of analysis power: with the restrictions stated in the support material, B-Course is a powerful analysis tool exploiting several theoretically elaborate results developed recently in the fields of Bayesian and causal modeling.","Bayesian methods,
Data mining,
Data analysis,
Context modeling,
Computer science,
Uniform resource locators,
Software tools,
Standards development,
Application software,
Application specific processors"
Rules for a cellular automaton to model quantum-dot cellular automata,"Quantum-dot cellular automata are one of several new device architectures whose operation is based on local interactions, much like cellular automata. We have implemented several rule sets for a cellular automaton that could be used to model the behavior of quantum-dot cellular automata and used them to test most of the wire and gate configurations proposed for these devices. Arrangements of cells for which any particular cell has neighbors which are not adjacent to each other generally behave as expected. Unfavorable arrangements of cells such as those with bends and crosses tend to either have incorrect outputs or be unstable for some of the possible inputs. These results suggest that quantum-dot cellular automata need more than strictly local interactions in order to operate correctly.","Quantum dots,
Quantum cellular automata,
Electrons,
Computer science,
Computer architecture,
Automatic testing,
Wire,
Large scale integration,
Temperature,
Degradation"
A component-based architecture for the development and deployment of WAP-compliant transactional services,"The Wireless Application Protocol (WAP) is an open, global specification that empowers mobile users with wireless devices (such as mobile phones) to easily access and interact with information and services on the Internet. Although some components of the WAP suite have been developed, it lacks a complete general architecture integrating software components of both the Internet and wireless contexts in a transparent way. This paper presents a general architectural framework to develop and deploy portable applications and services accessible by WAP-compliant mobile terminals, extending end-to-end services between terminal and business applications. Different implementation strategies for the application logic are presented, allowing either fast prototyping or the realisation of robust, portable, complex applications and transactional services. Moreover, a technique to handle terminal disconnection is presented.","Wireless application protocol,
Application software,
Prototypes,
Robustness,
Computer architecture,
Mobile communication,
Wireless networks,
Mobile handsets,
Web and internet services,
Logic"
A proxy caching scheme for continuous media streams on the Internet,"We propose a proxy caching scheme for continuous media data that stores a portion of continuous media stream or entire stream on the Internet. By caching the initial fraction of the stream data, service startup latency can be reduced, and by varying the size of the fraction of stream to be cached according to variation of stream popularity, we can utilize the cache space efficiently and maximize the amount of data served directly from the cache. We use the caching utility of each stream for cache replacement, which represents the correlation between popularity of a stream and the size of storage space that is allocated for a stream. We also propose the method of measuring popularity of continuous media stream using the amount of data played back by clients. Finally, we have performed simulations to evaluate our caching policy.","Streaming media,
Internet,
Delay,
File servers,
Network servers,
Web server,
Cache storage,
Bandwidth,
Computer science,
Performance evaluation"
Implicit restart scheme for large scale Krylov subspace model reduction method,"An implicitly restarted Krylov subspace model reduction method is presented. By this technique a stable, linear transfer function f(s) of order n can be approximated by an order m, where n/spl Gt/m. The oblique projection of stable systems onto a Krylov subspace may generate unstable partial realizations along with undesirable modes. To ensure that the Krylov projected model for a stable system is also stable and contains useful modes only, further operations like stable projection and balanced truncation are then performed to compute a reduced order model f/sub r/(s) of order r (where r","Large-scale systems,
Reduced order systems,
Transfer functions,
Stability,
Equations,
Control system synthesis,
Time domain analysis,
Algorithm design and analysis,
Computer science,
Frequency"
Approximate query evaluation using linear constraint databases,"This paper shows that constraint databases can be used for the approximation of several types of discretely recorded continuous data, for example time series data and some spatio-temporal geographic data. We show that time series data can be approximated by a piecewise linear approximation that runs in linear time in the number of data points, and the piecewise linear approximation can be represented in a linear constraint database. Similarly, the spatio-temporal geographic data that is composed of a set of spatial locations, where each location is associated with a time series, can be also approximated and represented in a linear constraint database. The approximations provide data compression, faster query evaluation-that preserve high precision and recall-and interpolation enabling the evaluation of queries that could not be evaluated before.",
Decentralized supervisory control with single-bit communications,"We present a preliminary examination of constrained communication between controllers of a decentralized discrete-event system. Previously, controllers have communicated state estimates and/or partially observed sequences of events. The communication protocol presented here restricts controllers to the communication of a single symbol. The effect of this ""singlebit"" communication on the generation of a useful communication policy to solve the control problem is examined. The issue of the decidability of decentralized discrete-event control problems with communication is also briefly discussed.","Supervisory control,
Communication system control,
Control systems,
Physics,
Laboratories,
Discrete event systems,
State estimation,
Mathematics,
Computer science,
Niobium"
Preventing denial of service attacks on quality of service,"Capabilities are being added to IP networks to support quality of service (QoS) guarantees. These guarantees are needed for many applications, such as voice and video transmission, real-time control, etc. Little attention has been paid to making these capabilities secure; in their present form, they are vulnerable to attack. The ARQoS project is examining these vulnerabilities, and ways to prevent denial-of-service attacks on QoS capabilities. In this paper, we describe two important parts of the project. The first part is the application of a pricing paradigm to resource allocation. User acquisition of network resources must be authorized, and the relative amount of resources that can be requested is carefully controlled. We present a distributed method of pricing which is highly flexible and responsive to changing conditions. Experimental results illustrate its effectiveness. The second part is the detection of TCP dropping attacks by compromised routers. The detection occurs at the end system and does not require any cooperation from the network. We have enhanced a method of statistically analyzing traffic patterns to detect dropping attacks. The method has been implemented and tested over the Internet, and results are presented.","Computer crime,
Quality of service,
IP networks,
Computer science,
Bandwidth,
Access protocols,
Computer networks,
Pricing,
Resource management,
Upper bound"
Evaluation of path length made in sensor-based path-planning with the alternative following,"As the sensor-based path-planning algorithm whose upper bound of path length is the smallest, Alg1 and Alg2 have been well known. In these algorithms, after encountering a visited point (that is, enters into a loop), a mobile robot follows an uncertain obstacle by an opposite direction until it can leave the obstacle. We call this as the ""reverse procedure"". Independently, if a robot always changes a direction following an uncertain obstacle alternatively, the robot arrives at a destination earlier on average (a probability for the robot to join a loop around a destination decreases). We call this as the ""alternative following"". In this paper, by mixing the reverse procedure and alternative following, we design new sensor-based path-planning algorithms Rev1 and Rev2. The upper bound 2D+2/spl Sigma//sub i/P/sub i/ in Rev1 and Rev2 is slightly longer than the upper bound D+2/spl Sigma//sub i/P/sub i/ in Alg1 and Alg2 (P/sub i/: the perimeter of an i-th encountered obstacle, D: the Euclidean distance between start and goal points). However, the average bound of path lengths in Rev1 and Rev2 will be shorter than that in Alg1 and Alg2. The former is fortunately evaluated by mathematical proofs, but the latter is unfortunately ascertained by several simulation results.","Path planning,
Upper bound,
Robot sensing systems,
Algorithm design and analysis,
Euclidean distance,
Computer science,
Mobile robots,
Convergence,
Clocks"
RoMR: a robust multicast routing protocol for ad-hoc networks,"Support for multicast services is crucial for ad-hoc networks to become a viable alternative to infrastructured wired and wireless networks. We propose RoMR, a robust multicast routing algorithm for ad-hoc networks. The basic tenet of the algorithm is to build multiple reliable multicast trees that adapt to topology changes in a dynamic fashion. The main characteristics of RoMR are its robustness and mobility awareness in efficiently supporting multicast communication. The robustness of our multicast strategy stems from two techniques in creating the multicast trees. First, RoMR constructs the multicast trees with links that are less prone to failure than other links. Second, multiple trees are constructed, possibly interconnected, so that if a link fails in one tree, reliable paths established from an alternate tree are immediately available. Links that have a high probability of existing in the next time interval are chosen as the links that the trees have in common.",
An adaptive segmentation scheme for the Bluetooth-based wireless channel,"Bluetooth has been regarded as a promising solution for wireless connection between hand-held devices. In Bluetooth, a message is segmented into short packets to be transmitted over different frequency bands to randomize error occurrences. There are various kinds of packets with different sizes and error-handling schemes. Since each packet type has its own performance characteristics, care must be taken in selecting a packet type. This paper proposes an adaptive method that selects the best packet type depending on the condition of the channel. One of challenging issues in the proposed scheme is how to predict the packet error rate of one packet type from another. We interpolate packet error rates of different packets based on a uniform bit error model. Based on the packet error rates obtained from the interpolation, the proposed scheme selects a packet type that leads to the best performance. We also propose a revised scheme that works even when bit errors occur in a non-uniform pattern. The proposed scheme is especially useful in a harsh environment where the bit error rate is worse than 10/sup -4/.","Bluetooth,
Error analysis,
Bit error rate,
Frequency,
Computer errors,
Protocols,
Quality of service,
Computer science,
Mechanical engineering,
Interpolation"
Collaborative evolutionary multi-project resource scheduling,"Large, real-world resource constrained multiple-project scheduling problems involve complex quality criteria. Clearly, issues such as meeting resource constraints, and minimising slippage, flow-time and tardiness are all appropriate and will be applied in such cases. However, often crucial criteria exist which are difficult or impossible to formalise. Since such large projects span multiple resources and often long time-spans, these extra criteria may include seasonal issues, deep knowledge of the scheduler concerning resource issues at different times, or perhaps knowledge and experience concerning the individual project managers involved in the projects. We justify and describe a collaborative evolutionary scheduling system, which enables the search to be guided by both the standard schedule quality criteria and also the master scheduler's non-formalised knowledge and experience. Preliminary experiments are described which indicate great promise for the approach.","Collaboration,
Processor scheduling,
Large-scale systems,
Project management,
Logistics,
Computer science,
Cybernetics,
Knowledge management,
Resource management,
Programming"
A hybrid approach to the recovery of deformable superquadric models from 3D data,"The problem of recovering the shape of objects from three-dimensional data is important to many areas of computer graphics and vision. We present here a method for the recovery of single-part objects from unstructured 3D points sets, based on the fitting of deformable superquadric models. The limitations of least-squares minimisation as a technique for fitting superquadric models are discussed. After investigating the possibility of using a genetic algorithm as an alternative, we propose a hybrid approach to the recovery of deformable superquadrics based on a two-stage fitting process that combines a genetic algorithm and nonlinear least-squares minimization.","Deformable models,
Genetic algorithms,
Minimization methods,
Computer graphics,
Parametric statistics,
Solids,
Shape control,
Computer science,
Computer vision,
Medical robotics"
Orienting parts by inside-out pulling,"A common task in automated manufacturing processes is that of orienting (or feeding) parts prior to assembly. We propose a new type of feeder. We consider sensorless orientation of polygonal parts with elevated edges by pull actions with an overhead finger. We show that any asymmetric convex polygonal part can be oriented by a sequence of pull operations. We give an O(n/sup 3/) algorithm to compute the shortest sequence of pull operations to orient a convex polygonal part with n vertices, if such a sequence exists. We also show that there exist non-convex parts that cannot be fed by a sequence of pull operations.",
Topic Forest: a plan-based dialog management structure,"There are many task-oriented dialog systems, but few of them can cope with the issues such as the multiple-topic issue, the topic changing issue, information sharing among different topics, and the difference in importance for different information items. To provide efficient solutions, a plan-based dialog management structure named Topic Forest is proposed, which makes the mixed-initiative dialog control easier. The Topic Forest based reasoning engine with a certain strategy for both remembering and forgetting is also described. The reasoning strategy is designed to be domain-independent; therefore it makes the dialog management model easy to be ported to other different domains.",
Searching scientific databases for guides to experiment and theory,"Based on information derived from bibliographic titles in the INSPEC database, this report identifies trends in experimental and theoretical materials research. It considers three specific questions: which chemical elements in the given literature are most widely studied by experiment and simulation; how did the experiment and theory of these materials change from 1989 to 1999; and can we observe any trends from such work?.","Databases,
Data mining,
Citation analysis,
Mining industry,
Libraries,
Explosions,
Data engineering,
Acceleration,
Computer networks,
Content based retrieval"
Facilitating the legislation process using a shared conceptual model,The Power (Program for an Ontology-based Working Environment for Rules and regulations) research program combines a knowledge-capitalization/knowledge-codification approach with an organization-dynamics approach. Initial results indicate that this method will help to improve the quality of law enforcement and decrease the time needed for implementing legislation and regulation changes.,
Composing concerns with a framework approach,"As concurrent software systems become larger, the interaction of their components is becoming more complex. This interaction may limit reuse, making it difficult to validate design and correctness and perhaps forcing reengineering of these systems in order to meet future requirements. In order to reduce this complexity and to build stable and adaptable concurrent software systems, we present an approach that emphasizes the separation of interaction components from the functional components.","Software systems,
Concurrent computing,
Mathematical programming,
Maintenance engineering,
Design engineering,
Software maintenance,
Software reusability,
Computer science,
Computer errors,
Software quality"
Proving sequential consistency by model checking,"Sequential consistency is a multiprocessor memory model of both practical and theoretical importance. Unfortunately, the general problem of verifying that a finite-state protocol implements sequential consistency is undecidable, and in practice, validating that a real-world, finite-state protocol implements sequential consistency is very time-consuming and costly. In this work, we show that for memory protocols that occur in practice, a small amount of manual effort can reduce the problem of verifying sequential consistency into a verification task that can be discharged automatically via model checking. Furthermore, we present experimental results on a substantial, directory-based cache coherence protocol, which demonstrate the practicality of our approach.","Computer science,
Application software,
Coherence,
US Department of Transportation,
Access protocols,
Councils,
Interleaved codes,
Hardware,
Error correction"
Analyzing the impact of changing requirements,"Determining the impact of requirement changes on software development is critical to project management. We present an impact analysis method to evaluate requirement changes for software development projects that is based on requirements traceability. By using attributes of the work products and traces, we create classes of requirement changes prioritized according to the potential impact. We present a case study that shows a favorable comparison between the actual impact and the predicted impact. Finally, we discuss the expansion of the method.","Project management,
Product development,
Programming,
Computer science,
Electrical capacitance tomography,
Software development management,
Production,
Laboratories"
A new query processing technique for XML based on signature,"XML is represented as a tree and the query as a regular path expression. The query is evaluated by traversing each node of the tree. Several indexes are proposed for regular path expressions. In same cases these indexes may not cover all possible paths because of storage requirements. We propose a signature-based query optimization technique to minimize the number of nodes retrieved from the database when the indexes cannot be used. The signature is a hint attached to each node, and is used to prune unnecessary sub-trees as early as possible when traversing nodes. For this goal, we propose a signature-based DOM(s-DOM) as a storage model and a signature-based query executor(s-NFA). Our experimental results show that the signature method outperforms the original.","Query processing,
XML,
Computer science,
Database systems,
Information retrieval,
Indexes,
Data models,
Object oriented modeling,
Telephony,
Finance"
Gnu/Maverik: A Microkernel for Large-Scale Virtual Environments,"This paper describes a publicly available virtual reality (VR) system, GNU/MAVERIK, which forms one component of a complete VR operating system. We give an overview of the architecture of MAVERIK, and show how it is designed to use application data in an intelligent way, via a simple, yet powerful, callback mechanism that supports an object-oriented framework of classes, objects, and methods. Examples are given to illustrate different uses of the system and typical performance levels.",
Understanding organizational memory,"Many organizational memory (OM) models and definitions can be found in the literature. Most models are complex or too general to directly build a computer system to manage them, i.e., to capture significant information, organize it and make it available to people who needs it. This paper presents a review of some OM models as well as some systems intended to manage part of the information stored in it. A few observations about the human memory from a cognitive science point of view are also included, giving design ideas for new OM systems. Finally, a new OM model is presented. This model is based on a previous collaborative application. The model emphasizes information privacy aspects.","Humans,
Computer science,
Privacy,
Brain modeling,
Biological system modeling,
Cognitive science,
Collaboration,
Proposals,
Power system management,
Cybernetics"
Learning hierarchical observable Markov decision process models for robot navigation,"We propose and investigate a general framework for hierarchical modeling of partially observable environments, such as office buildings, using hierarchical hidden Markov models (HHMMs). Our main goal is to explore hierarchical modeling as a basis for designing more efficient methods for model construction and usage. As a case study we focus on indoor robot navigation and show how this framework can be used to learn a hierarchy of models of the environment at different levels of spatial abstraction. We introduce the idea of model reuse that can be used to combine already learned models into a larger model. We describe an extension of the HHMM model to includes actions, which we call hierarchical POMDPs, and describe a modified hierarchical Baum-Welch algorithm to learn these models. We train different families of hierarchical models for a simulated and a real world corridor environment and compare them with the standard ""flat"" representation of the same environment. We show that the hierarchical POMDP approach, combined with model reuse, allows learning hierarchical models that fit the data better and train faster than flat models.","Robots,
Navigation,
Hidden Markov models,
Artificial intelligence,
Tree data structures,
Computer science,
Buildings,
Design methodology,
Learning,
Decision making"
"Re-store: a system for compressing, browsing, and searching large documents",,"Computer science,
Software engineering,
Software systems,
Engineering management,
Pattern matching,
Vocabulary,
Decoding"
Interactive multimedia presentation management in distributed multimedia systems,Multimedia presentations are difficult to handle in the existence of user interactions and complex synchronization requirements. We present a synchronization model which can handle both time-based and event-based actions. The model can cope with the VCR-type user interactions. We also give a simple rule-based synchronization specification language which does not complicate as user interactions are allowed.,
An effective feature-preserving mesh simplification scheme based on face constriction,"A novel mesh simplification scheme that uses the face constriction process is presented. By introducing a statistical measure that can distinguish triangles having vertices of high local roughness from triangles in flat regions into our weight-ordering equation, along with other heuristics, our scheme can better preserve visually important features in the original mesh. To improve the shape quality of triangles, we adopt nonlinear face area sensitivity in the weight ordering. A learning and feedback mechanism is also utilized to enhance user controllability. The computations are simple, making our scheme time-effective and easy to implement. In addition to comparing our scheme with other mesh simplification algorithms empirically, we compare their performances by establishing a unifying ground among three basic simplification processes: decimate vertex, collapse edge, and constrict face. This unification allows us to analyze the intrinsic merits and demerits of simplification algorithms to help users make better selections.","Graphics,
Computer science,
Feedback,
Controllability,
Algorithm design and analysis,
Nonlinear equations,
Shape,
Workstations,
Laser modes,
Rendering (computer graphics)"
XML rule based source code generator for UML CASE tool,"Generating program source code based on a design model using a CASE tool is an important area in forward engineering. The generation of code from a design model is valuable in making developers maintain consistency between a model and its implementation and abating the routine work of writing skeleton source codes. However, implementing code generation with a CASE tool is not simple due to the metadata format, language, and policies of adopting a modeler's option. Because of the continuous introduction of development environments like EJB and COM, the extensibility of CASE tools becomes the principal factor for comparison. We believe that its feasible to generate source code in various languages based on a generation rule. In this paper, we propose an XML based code generation rule and code generator. The proposed rule provides higher level constructs to the developer for describing code generation, and by making the code generator independent of repository format, the increased applicability of the code generator is shown.","XML,
Unified modeling language,
Computer aided software engineering,
Maintenance engineering,
Design engineering,
Writing,
Skeleton,
Component architectures,
Computer science,
Electronic mail"
A streaming architecture for next generation Internet,"With the influx of multi-media streaming content over the Internet, building a streaming infrastructure that would support flexible next generation Internet application in a scalable may on heterogenous access technology is most desirable. This paper presents a streaming architecture which provides some innovative methods and technique that would help build an IP based radio/TV network. This would provide tools for E-commerce and would benefit the broadcasting stations, local affiliates, ISPs, and the end users. Additionally this paper highlights the mechanism behind some of the components associated with this architecture such as local content, global content, program management with local control, payment model, security, advertisement insertion, seamless mobility within a domain.",
Volumes of expression: artistic modelling and rendering of volume datasets,"This paper presents the design and implementation of artistic effects in modelling and rendering of volume datasets. Following different stages of a volume-based graphics pipeline, we examine various properties of volume data, and illustrate how expressive and non-photorealistic effects can be implemented. We demonstrate that the true 3D nature of volume data makes it particularly applicable for this use, allowing the addition of complex effects at the modelling stage as well as during rendering.",
Endpoint admission control: network based approach,"Proposes a network-based endpoint admission control system for scalable QoS-guaranteed real-time communication services. This system is based on a sink tree-based resource management strategy and is particularly well-suited for differentiated services-based architectures. By performing the admission decision at the endpoints, the flow setup latency and the signaling overhead are kept to a minimum. In addition, the proposed system integrates routing and resource reservation along the routes, and therefore displays higher admission probability and better link resource utilization. This approach achieves a low overall admission control overhead because much of the delay computation is done during system configuration, and so resources can effectively be pre-allocated before run time. We investigate a number of resource-sharing approaches that allow resources to be efficiently re-allocated at run time with minimized additional overhead. We provide simulation experiments that illustrate the benefits of using sink tree-based resource management for resource pre-allocation and for routing, both with and without resource sharing.","Admission control,
Resource management,
Intserv networks,
Bandwidth,
Routing,
Scalability,
Computer science,
Real time systems,
Delay effects,
Computational modeling"
An efficient heuristic approach to solve the unate covering problem,"The paper presents a new approach to solve the unate covering problem based on exploitation of information provided by Lagrangean relaxation. In particular, main advantages of the proposed heuristic algorithm are the effective choice of elements to be included in the solution, cost-related reductions of the problem, and a good lower bound on the optimum. The results support the effectiveness of this approach: on a wide set of benchmark problems, the algorithm nearly always hits the optimum and in most cases proves it to be such. On the problems whose optimum is actually unknown, the best known result is strongly improved.",
Attractive Periodic Sets in Discrete-Time Recurrent Networks (with Emphasis on Fixed-Point Stability and Bifurcations in Two-Neuron Networks),"We perform a detailed fixed-point analysis of two-unit recurrent neural networks with sigmoid-shaped transfer functions. Using geometrical arguments in the space of transfer function derivatives, we partition the network state-space into distinct regions corresponding to stability types of the fixed points. Unlike in the previous studies, we do not assume any special form of connectivity pattern between the neurons, and all free parameters are allowed to vary. We also prove that when both neurons have excitatory self-connections and the mutual interaction pattern is the same (i.e., the neurons mutually inhibit or excite themselves), new attractive fixed points are created through the saddle-node bifurcation. Finally, for an N-neuron recurrent network, we give lower bounds on the rate of convergence of attractive periodic points toward the saturation values of neuron activations, as the absolute values of connection weights grow.",
Mentoring undergraduates in computer vision research,"The future of society will be shaped by the young and talented minds going through colleges and universities today. During the last 14 years roughly 130 undergraduate students from several institutions have participated in the research experiences for undergraduates (REU) program funded by the National Science Foundation (NSF). A large fraction of our students have been able to prepare a paper for submission to a conference, have the paper accepted, and then attend the conference to present the paper. Several participants have even accomplished enough substantial research to result in journal publications. Many past participants are now pursuing graduate studies at various institutions. In this paper, the REU model is described in detail; some examples of student success are discussed; and some observations are summarized.",Machine vision
CACAO a collimation means well suited for pixellated /spl gamma/-camera,Pixellated detectors are a fast growing area of research. However performances of such detectors will be spoiled by the poor figures of conventional collimators. The computer aided collimation gamma camera (CACAO) is a new collimation possibility for emission tomography. The CACAO system uses large hole collimator aimed at increasing the sensitivity. This article studies the possible advantages of coupling the CACAO project with pixellated detectors. Simulations of images reconstruction seem to confirm the synergistic advantages of both techniques.,
Composing a new ECE program: the first five years,"The authors have developed a new Electrical and Computer Engineering (ECE) program at Rowan University, NJ, USA. The first class graduated in May 2000. Features include: a continuous engineering clinic sequence; a mixture of two-, three- and four-credit courses; and technology focus electives. Project-based instruction is employed as a tool for motivating students and to demonstrate the relevancy of material. Multidisciplinary courses provide the opportunity for students in different disciplines to work together. Some of the approaches-and lessons learned-of interest to other start-ups and programs considering transformation.","Engineering education,
Educational institutions,
Electrical engineering computing,
Chemical engineering,
Feedback,
Modems,
Guidelines,
Mathematics"
Theorems and extensions of single wire replacement,"In this paper, we discuss the theorems and extensions of single alternative wire that attempts to replace one wire by another wire without changing the logic functionality. The wire replacement technique has been successfully applied to achieve logic optimization and routability improvement. However, there still exist several fundamental problems that have not been addressed such as whether the algorithm can find all single alternative wires. First, we present some cases of alternative wires, which the previous work (Chang et al., 1997) cannot obtain. Then, several theorems of tight necessary conditions and dominating conditions for a wire to be an alternative wire are proposed. With these theorems, we are able to derive an efficient procedure to find all possible alternative wires. The experimental results are very encouraging.","Wire,
Automatic test pattern generation,
Circuit testing,
Automatic logic units,
Logic testing,
Minimization,
Redundancy,
Circuit topology,
Computer science,
Terminology"
Analysis of privacy and non-repudiation on pay-TV systems,"Lee-Chang-Lin-Hwang (see IEEE Trans. On Consumer Electronics, vol.46, no.1, p.20-26, 2000) in 2000 proposed a set of protocols for pay-TV systems in order to secure subscriber's privacy and build a fair pay-TV system. However, we have found that an attacker can easily get other subscriber's privacy in watching TV-programs. We analyze the reason and discuss the possible amendments. Moreover, we expose a weakness on non-repudiation and suggest an improvement to support non-repudiation.","Privacy,
Protocols,
Content addressable storage,
Subscriptions,
Computer science,
Electronic mail,
Cable TV,
TV broadcasting,
Watches,
Information analysis"
A universal characterization of the closed Euclidean interval,"We propose a notion of interval object in a category with finite products, providing a universal property for closed and bounded real line segments. The universal property gives rise to an analogue of primitive recursion for defining computable functions on the interval. We use this to define basic arithmetic operations and to verify equations between them. We test the notion in categories of interest. In the category of sets, any closed and bounded interval of real numbers is an interval object. In the category of topological spaces, the interval objects are closed and bounded intervals with the Euclidean topology. We also prove that an interval object exists in and elementary topos with natural numbers object.","Mechanical factors,
Computer science,
Informatics,
Arithmetic,
Equations,
Testing,
Topology,
Set theory,
Logic,
Convergence"
WCET analysis of reusable portable code,"Traditional worst-case execution-time analysis (WCET analysis) computes upper bounds for the execution times of code. This analysis uses knowledge about the execution contest of the code and about the target architecture. In contrast, the WCET analysis for reusable and portable code has to abstract from parameters that are unknown until the code is finally used. The analysis is done in two steps. The first step computes abstract WCET information to support the reuse and portability of the WCET information. The second step uses the abstract WCET information to compute concrete WCET bounds when the application context and the timing parameters of the target system are known. The paper describes each of the two analysis steps. It demonstrates how WCET information can be made portable and reusable.","Timing,
Java,
Concrete,
Computer aided instruction,
Computer science,
Upper bound,
Computer architecture,
Contracts,
Hardware,
Performance analysis"
Trademark retrieval by relaxation matching on fluency function approximated image contours,"We propose an automatic retrieval method that addresses the problem of finding similar trademark images from a database when compared with an input. Our method is based on evaluating the compatibility of relative relations among extracted image contour segments between the input and the registered images using relaxation matching. The overall compatibility, after finding a best matching configuration between contour segments of the input and each registered image, provides the basis for a distance metric used in similarity ranking. Experiments were carried out on 300 randomly selected trademark images obtained from the Japan Patent Office's Website for performance evaluation. The applicability of our method, however, is not limited to trademark retrieval but can be extended to other applications having the need for image similarity matching.","Trademarks,
Image retrieval,
Image segmentation,
Information retrieval,
Humans,
Polynomials,
Information science,
Data mining,
Government,
Web page design"
Analysis of formation flying control of a pair of nano-satellites,"Gives an analysis and provides implementable solutions for constrained control problems of stabilization and maneuver control of two nano-satellites. The methodology provides solutions under various constraints of propulsion and differential drag control: magnitude constraints, disturbances and measurement errors are taken into account. The methods are illustrated in simulation of a realistic nano-satellite pair.","Satellites,
Space vehicles,
Motion control,
Extraterrestrial measurements,
Space stations,
Position measurement,
Master-slave,
Computer science,
Educational institutions,
Propulsion"
Improved person tracking using a combined pseudo-2D-HMM and Kalman filter approach with automatic background state adaptation,"This paper presents the continuation of our work on object tracking in presence of non-stationary background using a combination of a pseudo-2D hidden Markov model (P2DHMM) and a Kalman filter. It presents a major improvement by introducing a novel method that allows an automatic adaptation of our system to the changing background. Other improvements of the system's tracking capabilities are achieved by refined person models and normalization procedures. One of the major goals of our approach to tracking is to achieve high quality tracking results despite non-stationary background that can be caused e.g. by moving objects in the background or by camera operations such as panning or zooming. In previous publications we demonstrated that our combined P2DHMM/Kalman filter approach is an interesting solution to this problem, because it enables us to perform person tracking without the use of motion information. In this paper, we show that this approach can be further improved by adapting the system to the constantly changing background. We further demonstrate that such a background adaptation is very difficult to achieve in standard tracking approaches but can be effectively realized in our combined P2DHMM/Kalman filter approach. The effectiveness of this new procedure is demonstrated in experiments, where the tracking results and the quality of the person segmentation of our original system is compared to the results obtained with the improved approach.","Hidden Markov models,
Cameras,
Image segmentation,
Computer science,
Tracking,
Filters,
Video sequences,
Surveillance,
Robustness,
Neural networks"
On the Laws-Parsons distribution of raindrop sizes,A new analytical function is given for the distribution density of raindrop diameters implied by the measured Laws-Parsons distribution of the volume fraction of water in rain. It yields results that differ importantly from those obtained via the Marshall-Palmer exponential function of drop diameter in the millimeter range of radar wavelengths.,"Rain,
Attenuation,
Solids,
Volume measurement,
Radar,
Reflectivity"
A group-based load balance scheme for software distributed shared memory systems,"Load balance is an important issue for the performance of software distributed shared memory (DSM) systems. One solution for addressing this issue is to exploit dynamic thread migration at runtime. In order to reduce the data consistency communication increased by thread migration, an effective load balance scheme must carefully choose the threads and the destination nodes for workload migration. A group-based load balance scheme is proposed to resolve this problem. The main characteristic of this scheme is to classify the overloaded nodes and the lightly loaded nodes into a sender group and a receiver group, and then consider all the threads of the sender group and all the nodes of the receiver group for each thread migration decision. The experimental results show that the group-based load balance scheme reduces more communication than previous methods. Besides, the paper also resolves the problem of the high overhead caused by group-based schemes. Therefore, the performance of the test programs is effectively enhanced after minimizing the communication increased by thread migration.","Yarn,
Load management,
Application software,
Computer science,
Software performance,
Runtime,
Testing,
User interfaces,
Computer interfaces,
Computer networks"
How to successfully use software project simulation for educating software project managers,"A crucial factor for the success or failure of software development projects is the qualification of project managers. But how can future project managers be prepared for their role? To successfully manage a software project, theoretical knowledge is not sufficient. Practical experience is necessary, too. This paper presents a new approach for teaching software project management based on the interactive simulation of software projects. This approach teaches theoretical knowledge on software engineering and project management and also offers students the chance to gather reality-like management experience. One important feature of this approach is to analyse the course and outcomes of the simulated projects to give students feedback on their management strengths and weaknesses.",
Studies of ALN: an empirical assessment,"This analysis of research literature seeks to gain insight into the study of the effectiveness of asynchronous learning networks (ALN). The terminology of ALN was coined by the Sloan Foundation to distinguish anytime/anyplace computer mediated communications (CMC), such as bulletin boards and email, from same time versions of distance learning. A database of material gathered from the papers reporting the studies is described. The current picture of that data is highlighted and discussed. A framework of three sets of factors related to ALN effectiveness is described. Within this framework an analysis of the current collection of papers is offered. From the 15 papers in which the effectiveness of ALN was compared to that for face-to-face classes, 2/3 reported it was more effective, and the remainder reported ""no significant difference."" Additional papers are expected to be supportive of a need for better research techniques in the ALN community.","Computer aided instruction,
Databases,
Collaborative software,
Terminology,
Communications technology,
Natural languages,
Conference proceedings,
Computer networks,
Education,
Writing"
Design of a real-time interactive tele-exercise classroom for computer exercises over a Gigabit Network,"This paper proposes a tele-exercise classroom for computer exercises over the Japan Gigabit Network. The requirements for constructing the tele-exercise classroom be discussed. The main features of the classroom are (1) a teacher can monitor a strident's operations, (2) give instructions to the student, (3) show a demonstration to the student. Besides, exercise materials and/or problems happening during the exercise class can be presented by using a synchronous/asynchronous white board, and discussions between the teacher and a student (students) can be carried out through interactive real-time video/audio. We present a systems model which shows the configuration of the system, user computer terminal types, system modes, and action modes. In addition, the extendibility of the system is considered in the model.","Computer science,
Cities and towns,
Computer aided instruction,
Real time systems,
Application software,
Computer networks,
Internet,
Computerized monitoring,
Education,
Multimedia systems"
On the average-case hardness of CVP,"We prove a connection of the worst-case complexity to the average-case complexity based on the Closest Vector Problem (CVP) for lattices. We assume that there is an efficient algorithm which can approximately solve a random instance of CVP, with a non-trivial success probability. For lattices under a certain natural distribution, we show that one can approximately solve several lattice problems (including a version of CVP) efficiently for every lattice with high probability.","Lattices,
Polynomials,
Computer science,
Linear programming,
Councils,
Tellurium,
Distributed computing,
Probability distribution"
Modeling and verification of iterated systems and protocols,"The high complexity of modern hardware systems necessitates the use of formal methods for checking the satisfaction of desired properties and the absence of design flaws. Some powerful methods such as model checking and the to-automata approach have found wide acceptance, but suffer from the ""state explosion"" problem. To avoid this issue we recently proposed a new formal verification method based on series parallel posets. The technique is applicable to verifying the proper sequencing of events occurring in non-iterated, as well as globally-iterated/locally-non-iterated systems. In this paper we extend the series parallel poset verification to handle the much broader class of general iterated systems. This allows us to model and verify the behavior of systems involving feedback on multiple levels, as well as the behavior of communication, interconnect, and cache coherence protocols. The verification algorithms retain a low-order polynomial space- and time complexity.","Protocols,
Hardware,
Formal verification,
Power system modeling,
Polynomials,
Computer science,
Educational institutions,
Feedback,
Coherence,
System testing"
Strengths and weaknesses of genetic list scheduling for heterogeneous systems,"List scheduling combined with genetic algorithms has already been shown to be a powerful approach (Grajcar, 1999,Yu-Kwong Kwok and Ahmad, 1996). We investigate the problems associated with using a list scheduler for a heterogeneous system. Furthermore, we present cases, for which most list schedulers fail to find the optimum. We propose and experimentally evaluate novel ideas avoiding it.","Processor scheduling,
Genetic algorithms,
Computer architecture,
Computer science,
Application software,
Embedded system,
Timing,
Costs,
Process design,
Force measurement"
A probabilistic scheme for hierarchical QoS routing,"Quality of service (QoS) routing consists of two parts: (a) collecting network QoS resource availability information using topology aggregation schemes and (b) computing-feasible paths using this information. This paper firstly proposes a probabilistic scheme for topology aggregation in large networks which are QoS sensitive. Our goal is to account for the inherent dynamic nature of the QoS resources by using probabilistic measures as opposed to the existing deterministic ones. Additionally, this paper also proposes a modification to an existing probabilistic QoS routing algorithm for concave QoS parameters. The proposed heuristic algorithm increases the global network resource utilization.","Routing,
Quality of service,
Delay,
Availability,
Network topology,
Heuristic algorithms,
Bandwidth,
Computer science,
Computer networks,
Resource management"
Robot box-pushing with environment-embedded sensors,We address the problem of detecting and pushing stationary objects in a planar environment by using an environment-embedded sensor network and a simple mobile robot. The stationary sensors are used to detect pushable objects. The robot pushes a detected object based on pose information obtained from the sensors over a wireless network. We show that the accuracy of the push operation is correlated to the number of sensors used to determine object pose.,
Evaluation of the reliability in kinetic analysis for dual tracer injection of FDG and flumazenil PET study,"The kinetic analysis for dual tracer injection with 2-input compartment model is challenging in order to assess the two different functions In the same time and same situation. In this study, we investigated the possibility of kinetic analysis with two tracers, /sup 18/F-FDG and /sup 11/C-flumazenil (FMZ), by means of the computer simulation. The reliability of estimated parameters was Investigated for various injection protocols and noise levels. Simulated decaying tissue time activity curves were generated for various injection protocols with input function of FDG and FMZ and true k-values by using the 2-input 3-tissue compartment 5-parameter model, i.e. 2-tissue compartment 3-parameter for FDG and 1-tissue compartment 2-parameter for FMZ. The injection Interval of two tracers was changed from 0 to 20 minutes. The noise was generated depending on the total collected count and added each decaying tissue time activity curve. The rate constants for FDG and FMZ were estimated by nonlinear least square method. The reliability of parameter estimates was evaluated by mean absolute difference between true and estimated value of one thousand runs for each injection protocol and noise level. As a result, it was found that parameters were estimated most reliably when FDG was injected 15 minutes later than FMZ injection. In 5% last frame noise, the mean absolute difference between true and estimated value of Ki, reflecting the uptake of FDG, was about 8%, that of DV, distribution volume of FMZ, was 7%. The reliability was independent on the ratio of administration dose of FDG to that of FMZ. In the simulation study, the possibility of kinetic analysis for dual tracer injection was shown.","Kinetic theory,
Parameter estimation,
Protocols,
Noise level,
Computer simulation,
Analytical models,
Performance analysis,
Positron emission tomography,
Time measurement,
Plasma simulation"
Kinematic modeling of mobile robots by transfer method of augmented generalized coordinates,"A kinematic modeling method is proposed which uses augmenting variables to match the dimensions of the input vector and output vector of each serial subchain of the mobile robots. Firstly, kinematic models of various type of wheels are derived including skidding and sliding velocities. Then the method of augmented generalized coordinates is applied to obtain inverse and forward kinematic models. The kinematic models derived by the direct inverse of the augmented matrices provide an accurate solution, which was not achievable by the modeling method based on the pseudo-inverse method which provides an approximate solution. Lastly, two kinematic models for typical, differential-driven mobile robots are presented to show the effectiveness of the proposed modeling method.",
Specifications and FPGA implementation of a systolic Hopfield-type associative memory,"Neural networks are non-linear static or dynamical systems that learn to solve problems from examples. Most of the learning algorithms require a lot of computing power and, therefore, could benefit from fast dedicated hardware. One of the most common architectures used for this special-purpose hardware is the systolic array. The design and implementation of different neural network architectures in systolic arrays can be complex, however. The paper shows the manner in which the Hopfield neural network can be mapped into a 2-D systolic array and presents an FPGA implementation of the proposed 2-D systolic array.",
An empirical study of maintenance issues within process improvement programmes in the software industry,"Anecdotal evidence from our work with software developers suggests that maintenance is a significant problem for software development companies. A problem that is absorbing increasing amounts of precious development effort. In parallel, software companies are increasingly applying process improvement principles to development problems. In this paper we discuss how maintenance is addressed in process improvement programmes. We look at how well maintenance is addressed by formal process models like CMM. We also present empirical evidence from our study of process improvement in UK software companies. Our main findings are that although developers report that maintenance is indeed a problem, it is not always their most important problem. Furthermore, our findings also suggest that companies are often not well prepared for the maintenance phase of developments and that formal process improvement models do not pay enough attention to maintenance.",
Constraints in CASE tools: results from curiosity driven research,"Curious about a report of disagreement between ratings of restrictiveness of methodological constraints in computer-aided software engineering (CASE) tools, two inspection-style studies were carried out to determine the extent of variation in end-users' beliefs about methodological constraint environments in CASE tools. The extent of variation was found to be significant. Major variations in belief were caused by varying use of the methodology checker, inability to create proper tests for the presence of constraints, and ambiguity in natural language expressions of the methodological constraints under inspection. Our findings have important implications for CASE tool design and construction regarding the methodology checker, support for alternative approaches to design work, and the human-computer interface. Researchers must also take cognizance of individual variation when searching for the 'ideal' methodological constraint environment.","Computer aided software engineering,
Constraint theory,
Computer science,
Design for disassembly,
Testing,
Natural languages,
Inspection,
Cultural differences,
Australia,
Environmental management"
Issues in accessing Web sites from mobile devices,"Mobile devices such as PDAs and cellular phones that offer Internet access are proliferating. Due to the many unique characteristics of these devices, such as their small screen size and limited network connectivity, existing Web sites may have to undergo significant evolution to support this type of mobile client. The paper outlines some of the issues in accessing Web sites from mobile devices, such as the Apple Newton, the Compaq iPAQ, and WAP-enabled cell phones. These issues are based on our experiences as both users of mobile devices and as content providers.","Personal digital assistants,
Cellular phones,
Wireless application protocol,
Mobile computing,
North America,
Computer science,
Internet,
Books,
Handheld computers,
Operating systems"
The effects of context switching on branch predictor performance,,
A novel channel estimation method for OFDM in high-speed mobile system,"In order to meet the requirements which are considered in an OFDM system under a high-speed mobile environment, lots of techniques have been studied. In mobile communication systems, fading distortion affects the overall performance. Furthermore, the higher the velocity of any mobile system, the more severe the effect of fading is. To solve this problem, in an OFDM system, exact channel estimation can be an important method. We propose a channel estimation method which outperforms conventional methods. The analytical results demonstrate that the proposed system has better performance than the conventional methods such as PSAM and ESAE.",
Engineering a distributed computational collaboratory,"This paper presents the design, implementation, and deployment of the DISCOVER Web-based computational collaboratory. Its primary goal is to bring large distributed simulations to the scientists'/engineers' desktop by providing collaborative Web-based portals for monitoring, interaction and control. DISCOVER supports a 3-tier architecture composed of detachable thin-clients at the front-end, a network of interaction servers in the middle, and a control network of sensors, actuators, and interaction agents at the back-end. The interaction servers enable clients to connect and collaboratively interact with registered applications using a conventional browser. The application control network enables sensors and actuators to be encapsulated within and directly deployed with the computational objects. The application interaction gateway manages overall interaction. It uses the Java Native Interface to create Java proxies that mirror computational objects and allow them to be directly accessed at the interaction server. Security and authentication are provided using customizable access control lists and SSL-based secure servers.","Distributed computing,
Collaboration,
Network servers,
Actuators,
Java,
Computational modeling,
Portals,
Monitoring,
Computer architecture,
Computer networks"
Camera self-calibration from ellipse correspondences,"We introduce a new technique for camera self-calibration using ellipse correspondences. Based on an analysis of ellipse matches between the images obtained from the same viewpoint but with different and unknown view directions, our approach estimates the intrinsic camera parameters. We present both linear and nonlinear solutions to recovering intrinsic camera parameters. The algorithm's performance is validated extensively using both synthetic and real image data. Compared with similar techniques but using points, we observe a comparable performance. The use of ellipses, however, greatly simplifies feature matching between images, improves matching accuracy, and avoids mismatch.","Cameras,
Calibration,
Robustness,
Robot vision systems,
Layout,
Shape,
Computer science,
Automatic testing,
Image reconstruction,
Stereo image processing"
Emulating petaflops machines and blue gene,,"CMOS technology,
Programming environments,
Application software,
Concurrent computing,
Buildings,
Debugging,
Microprocessors,
Testing,
Computer science,
Semiconductor device modeling"
Computer aided design system for Japanese kimono,"A yukata is a type of traditional Japanese clothing. An alignment of its texture pattern is an important factor of yukata design. The calculation of the size, texture alignment and the creation of the cutting pattern are manually performed. Especially, the texture alignment depends on the experience and intuition of the skilled person. In this paper, we describe a CAD system for the yukata. First, we developed a measurement system for the wearer's body shape. Second, we developed an algorithm for performing garment simulation of Japanese yukata. The designer can understand the condition of the texture alignment exactly because the yukata is displayed three dimensionally on the wearer's body shape. As a result, designers can easily tailor the yukata.","Design automation,
Clothing,
Shape measurement,
Biological system modeling,
Computer science education,
Databases,
Displays,
Image processing,
MONOS devices,
Humans"
Synchronization in Relaxation Oscillator Networks with Conduction Delays,"We study locally coupled networks of relaxation oscillators with excitatory connections and conduction delays and propose a mechanism for achieving zero phase-lag synchrony. Our mechanism is based on the observation that different rates of motion along different nullclines of the system can lead to synchrony in the presence of conduction delays. We analyze the system of two coupled oscillators and derive phase compression rates. This analysis indicates how to choose nullclines for individual relaxation oscillators in order to induce rapid synchrony. The numerical simulations demonstrate that our analytical results extend to locally coupled networks with conduction delays and that these networks can attain rapid synchrony with appropriately chosen nullclines and initial conditions. The robustness of the proposed mechanism is verified with respect to different nullclines, variations in parameter values, and initial conditions.",
The approach of data mining methods for medical database,"With the advancement of computer science; the database concept has been widely used in medical information system for processing large volumes of data. Symbolic and numeric data will define the need for new data analysis techniques and tools for knowledge discovery. Three popular algorithms for data mining which includes Bayesian Network (BN), C4.5 in Decision Tree (DT), and Back Propagation Neural Network (BPN) were evaluated in this paper. Two classes of dataset are used as testing data. The first dataset, Fine Needle Aspiration Cytology, is used to check whether the breast tumor is malignant. The second dataset, Tongue Diagnosis Image, is used to check whether upper GI is disorder. The result shows that BN had a good presentation in diagnosis ability. By using BN showed the accuracy about 94.6% in diagnosing breast tumor and 85.5% in upper GI disorder. The C4.5 learning algorithms in DT was able to explain diagnosis knowledge and rules. Its accuracy was 94.4% to diagnose a breast tumor, but it was just only 63.9% in upper GI disorder. The best performance among these three algorithms is BPN and its accuracy is 96.0% in diagnosing a breast tumor and 91.6% in diagnosing upper GI disorder, respectively.","Data mining,
Databases,
Breast tumors,
Biomedical imaging,
Medical diagnostic imaging,
Computer science,
Medical information systems,
Data analysis,
Bayesian methods,
Decision trees"
Case study: visualization of particle track data,"The Relativistic Heavy Ion Collider (RHIC) experiment at the Brookhaven National Lab is designed to study how the universe came into being. It is believed that after the Big Bang, the universe expanded and cooled, consisting of a soup of quarks, gluons, electrons and neutrinos. As the temperature lowered, electrons combined with protons and formed neutral atoms. Later, clouds of atoms contracted into stars. In this paper, we describe how techniques of volume rendering and information visualization are used to visualize the large particle track data set generated from this high energy physics experiment. The system, called TrackVis, is based on our earlier work of VolVis - Volume Visualization software. Example images of real particle collision data are shown, which are helpful to physicists in investigating the behavior of strongly interacting matter at high energy density.",
Aspects of the InfiniBand architecture,,"Switches,
Routing,
Copper,
Computer architecture,
Optical attenuators,
Connectors,
Communication switching,
Computer networks,
USA Councils,
Packet switching"
Permutation rewriting and algorithmic verification,"Proposes a natural subclass of regular languages, called alphabetic pattern constraints (APC), which is effectively closed under permutation rewriting, i.e. under iterative application of rules of the form ab/spl rarr/ba. It is well-known that regular languages do not have this closure property in general. Our result can be applied for example to regular model checking, for verifying properties of parametrized linear networks of regular processes and for modeling and verifying properties of asynchronous distributed systems. We also consider the complexity of testing membership in APC, and show that the question is complete for PSPACE when the input is an NFA (nondeterministic finite automaton) and complete for NLOGSPACE when it is a DFA (deterministic finite automaton). Moreover, we show that both the inclusion problem and the question of closure under permutation rewriting are PSPACE-complete when we restrict ourselves to the APC class.","Logic testing,
Radio access networks,
System testing,
Automata,
Transducers,
Interleaved codes,
Context modeling"
Fault management using passive testing for mobile IPv6 networks,"In this paper, we employ the communicating finite state machine (CFSM) model for networks to investigate fault management using passive testing. First, we introduce the concept of passive testing. Then, we introduce the CFSM model, the observer model and the fault model with necessary assumptions. We introduce the fault detection algorithm using passive testing. Then, we briefly present our new passive testing approach for fault location, fault identification, and fault coverage based on the CFSM model. We illustrate the effectiveness of our new technique through simulation of a practical protocol example, a 4-node mobile IPv6 network. Finally, conclusions and potential extensions are discussed.","Testing,
Fault detection,
Fault location,
Fault diagnosis,
Automata,
Protocols,
Computer network management,
Open systems,
Computer science,
Educational institutions"
Fuzzy Bayes predictor in electric load forecasting,"We present the fuzzy Bayes predictor (FBP), a hybrid system for the task of monthly electric load forecasting. The FBP is a modification we introduce in the naive Bayes classifier in order to enable it to predict numerical values. We consider three versions of the FBP, each one with a different dependence among the input data: independence, first-order and second-order dependence. For verifying the efficiency of the FBP's prediction, we compare it with two fuzzy systems and two traditional forecasting methods, Box-Jenkins and Winters exponential smoothing.","Load forecasting,
Fuzzy systems,
Power engineering and energy,
Smoothing methods,
Testing,
Niobium compounds,
Systems engineering and theory,
Computer science,
Predictive models,
Electric variables measurement"
An adaptive framework for 'single shot' motion planning: a self-tuning system for rigid and articulated robots,"Describes an enhanced version of an adaptive framework for single shot motion planning (Vallejo et al., 2000). This framework is versatile, and particularly suitable for crowded environments. Our iterative strategy analyzes the characteristics of the query and adaptively selects planners whose strengths match the current situation. Contributions in the paper include an automatic method for setting and adaptively tuning planner characterizations, and reducing the reliance on programmer expertise present in the original framework. The adaptive refinement enables the system to evolve parameters specifically suited for particular classes of applications. The system now supports articulated robots, which were not supported previously. Our experimental results in complex 3D CAD environments show that our strategy solves queries that none of the planners could solve on their own.","Motion planning,
Robot kinematics,
Robotics and automation,
Design automation,
Strategic planning,
Tree graphs,
Systems engineering and theory,
Computer science,
Tuning,
Programming profession"
Adequate reverse engineering,"Reverse engineering a program constructs a high-level representation suitable for various software development purposes such as documentation or reengineering. Unfortunately however, there are no established guidelines to assess the adequacy of such a representation. We propose two such criteria, completeness and accuracy, and show how they can be determined during the course of reversing the representation. A representation is successfully reversed when it is given as input to a suitable code generator, and a program equivalent to the original is produced. To explore this idea, we reverse engineer a small but complex numerical application, represent our understanding using algebraic specifications, and then use a code generator to produce code from the specification. We discuss the strengths and weaknesses of the approach as well as alternative approaches to reverse engineering adequacy.","Reverse engineering,
Educational institutions,
Documentation,
Power system management,
Engineering management,
Polynomials,
Testing,
Computer science,
Guidelines,
Software systems"
Urdu computing standards: Urdu Zabta Takhti (UZT) 1.01,"Software development in Urdu has been going on for more than three decades. However, until recently there were no industry standards for coding in Urdu, similar to ASCII standard for English. Keeping in view the necessity and urgency of standardization needs, three years ago a national effort was initiated to get relevant people together and formulate a common standard for everybody to follow. This paper discusses the contents and related specifications of Urdu Zabta Takhti 1.01 in detail. The discussion is divided into subsections on logical groups of characters that form this takhti, Urdu text file format and how sorting may be done in Urdu with UZT.","Hardware,
Software standards,
Arithmetic,
Standards development,
Standardization,
Government,
Code standards,
Sorting,
Cities and towns,
Computer science"
A data preparation framework based on a multidatabase language,"Integration and analysis of data from different sources have to deal with several problems resulting from potential heterogeneities. The activities addressing these problems are called data preparation and are supported by various available tools. However, these tools process mostly in a batch-like manner, not supporting the iterative and explorative nature of the integration and analysis process. The authors present a framework for important data preparation tasks based on a multidatabase language. This language offers features for solving common integration and cleaning problems as part of query processing. Combining data preparation mechanisms and multidatabase query facilities permits applying and evaluating different integration and cleaning strategies without explicit loading and materialization of data. The paper introduces the language concepts and discusses their application for individual tasks of data preparation.","Cleaning,
Data analysis,
Database languages,
Computer science,
Query processing,
Warehousing,
Data mining,
Read-write memory,
Database systems,
Memory management"
Petri nets in cryptographic protocols,,"Petri nets,
Cryptographic protocols,
Cryptography,
Public key,
Data security,
Authentication,
Character generation,
Laboratories,
Computer security,
Computer science"
Effective continued fractions,"Only the leading seven terms of a continued fraction are needed to perform on-line arithmetic, provided the continued fractions are of the correct form. This forms the basis of a proof that there is an effective representation of the computable reals as continued fractions; we also demonstrate that the basic arithmetic operations are computable using this representation.","Computer science,
Digital arithmetic,
Convergence,
Floors"
Factoring and recognition of read-once functions using cographs and normality,"An approach for factoring general Boolean functions was described [Golumbic, M et al., 1999] which is based on graph partitioning algorithms. In this paper, we present a very fast algorithm for recognizing and factoring read-once functions which is needed as a dedicated factoring subroutine to handle the lower levels of that factoring process. The algorithm is based on algorithms for cograph recognition and on checking normality. Our method has been implemented in the SIS environment, and an empirical evaluation is given.","Partitioning algorithms,
Boolean functions,
Computer science,
Mathematics,
Polynomials,
Gallium nitride,
Computer applications,
Permission,
Educational institutions,
Circuits"
Code-disjoint carry-dependent sum adder with partial look-ahead,"In this paper a self-checking code-disjoint carry-dependent sum adder is proposed. To reduce the hardware overhead, only every 4-th carry is generated by a look-ahead unit. To check the input parity, internal nodes of the adder cells are utilized.","Computer science,
Hardware,
Computer errors,
Gas detectors,
Parity check codes,
Equations"
Cortex segmentation - a fast variational geometric approach,An automatic cortical gray matter segmentation from three-dimensional brain images (MR or CT) is a well known problem in medical image processing. We formulate it as a geometric variational problem for propagation of two coupled bounding surfaces. An efficient numerical scheme is used to implement the geodesic active surface model. Experimental results of cortex segmentation on real three-dimensional MR data are provided.,"Image segmentation,
Brain modeling,
Deformable models,
Solid modeling,
Computer science,
Cities and towns,
World Wide Web,
Computed tomography,
Biomedical image processing,
Cerebral cortex"
E-Government meets E-business: a portal site for startup companies in Switzerland,"Networked computer technology for business purposes is no longer a domain of companies alone. The Internet has invaded private homes and is becoming a communication interface between private parties (e.g. personal E-Mail), companies (E-Commerce) and the government (E-Government). The pervasion of the ""virtual world"" triggers government agencies to think about their role in setting a proper social and legal framework and to offer electronic interfaces to citizens and companies alike. The paper gives a description of the relations between E-Business and E-Government with definitions of the new terms in the E-Business arena. We describe a portal site for the organization of intellectual property rights for startup companies as a representative E-Government scenario. We conclude by summarizing our main findings and stressing the importance of E-Government for a healthy economic and socio-political environment.","Electronic government,
Portals,
Companies,
Computer networks,
Internet,
Electronic mail,
Law,
Legal factors,
Intellectual property,
Environmental economics"
'Making conversation': sequential integrity and the local management of interaction on Internet newsgroups,"Argues for a detailed empirical investigation of newsgroup interaction. It presents a framework for analysis that emphasizes the machinic (machine-related) and human-related characteristics of newsgroup activity with the concept of ""(human) orientation to the (machinic) default"". By problematizing the notion of newsgroup ""conversation"", this paper reveals the ""sequential integrity"" of newsgroup practices through the detailed investigation of participants' ""local management of interaction"". Newsgroup interaction is asynchronous: participation does not occur in ""real time"" and participants are geographically dispersed. Potentially, therefore, participation could be chaotic and disordered, yet observation reveals it to be a highly ordered activity. A fundamental question, then, is how this interactional order is achieved - and achieved as conversational. Newsgroup activity is characterized by sequential integrity. Messages are constructed in such a way as to exhibit both relational features (between messages) and internal features (in the text of messages) that mimic and respect sequential ordering.","Internet,
Humans,
Chaotic communication,
Concurrent computing,
Delay,
Computer mediated communication,
Amorphous materials"
Approximate maximum likelihood source separation using the natural gradient,This paper addresses a maximum likelihood approach to source separation in the case of overdetermined mixtures corrupted by additive white Gaussian noise. We present an objective function that is an approximate likelihood function based on the Laplace approximation. Then we derive a natural gradient adaptation algorithm which maximizes the corresponding approximate likelihood function. Useful behavior of the proposed method is verified by numerical experiments.,
Profiling turns in interaction: discourse structure and function,"Turn-taking provides a basis for comparing interactions in different communication environments, and this paper demonstrates that readily observable features of turns can be linked to principles that organize and manage the interaction. Results are based on 150 decision-making interactions elicited in a face-to-face environment, an asynchronous e-mail environment and several types of synchronous computer-mediated environments. We show that three features of turns can be linked to discourse structure and function. First, as turns increase in size, participants switch from serial to parallel strategies to organize their decision-making. Second, pivot turns, which are turns that are much shorter than the turns that precede or follow them, can reflect the discourse functions of the relevant turns. Finally, turns can be used for measures of dominance based on turn size. We conclude that designers of communication systems can take advantage of peoples' ability to develop effective strategies for packaging messages in different environments.",
Simulation of peeling using 3D-surface cellular automata,"The paper describes a method for modeling the natural peeling phenomenon over any 3D surface. Using crack input data, precomputed with a semi physical solution, this method allows simulation of peeling on any type of triangulated 3D object. The model's main advantage is that it proposes a method that is intuitive enough to make it easy to understand and convenient to apply, user controllable, and easily extensible, especially to simultaneous natural phenomena. We first introduce the general development of peeling over various materials. Then, we present how to define groups of peeling, how to determine a realistic and natural peeling order, and how to simulate the rendering of two types of special peelings. Finally, a set of graphical results concludes the paper, presenting a series of direct peeling and curling patterns using classical paintings or plastic-paintings simulated materials.","Surface cracks,
Computational modeling,
Computer graphics,
Automata,
Computer science,
Rendering (computer graphics),
Painting,
Biological materials,
Corrosion,
Stress"
Moving Sound Source Synthesis for Binaural Electroacoustic Music Using Interpolated Head-Related Transfer Functions (HRTFs),,
"""Planar"" tautologies hard for resolution","We prove exponential lower bounds on the resolution proofs of some tautologies, based on rectangular grid graphs. More specifically, we show a 2/sup /spl Omega/(n)/ lower bound for any resolution proof of the mutilated chessboard problem on a 2n/spl times/2n chessboard as well as for the Tseitin tautology (G. Tseitin, 1968) based on the n/spl times/n rectangular grid graph. The former result answers a 35 year old conjecture by J. McCarthy (1964).","Computer science,
Bipartite graph,
Graph theory"
X-compiler: yet another integrated novice programming environment,"The paper presents a simple programming language, called X, and an educational programming environment, called X-Compiler, designed to introduce students to programming. X-Compiler can be used to edit, compile, debug and run programs written in X, a subset of Pascal. X-Compiler could be didactically interesting because of the following features: (a) users can watch the intermediate steps of the execution of a program: source code compilation, correspondence of source and pseudo-assembly code during execution, register content, and intermediate values of user and temporary system variables; also, they can edit the produced pseudo-assembly code and re-execute it, (b) there are many detailed and explanatory messages that can guide novice programmers when debugging their programs and, in general, help them write better programs.",
Why we need to offer a modeling and simulation engineering curriculum,"This paper describes some identifiable trends in the manufacturing industry regarding the increased use of simulation tools, especially by small- to medium-sized companies. These trends have resulted in the need for a new type of engineer, namely simulation engineer. This need prompted the University of Skovde to develop a B.Sc. simulation engineering study program. The contents and layout of the program, which started in Autumn 2000, are described. After receiving a firm foundation in manufacturing, logistics and mathematics in the first year, the main focus of the second year is on simulation. In the third year, which includes a substantial examination project, a specialization in manufacturing or in logistics is possible. Although simulation-related examination projects are already now carried out in other study programs, the simulation engineer will be able to cover a larger part of simulation projects and will have a broader overview of available simulation tools.",
Timing driven gate duplication in technology independent phase,"We propose a timing driven gate duplication algorithm for the technology independent phase. Our algorithm is a generalization of a gate duplication strategy suggested previously (C. Chen and C. Tsui, 1999). Our technique gets a more global view by duplicating multiple gates at a time. We compare the minimum circuit delay obtained by SIS with the delay obtained by using our gate duplication. Results show that up to 11% improvement in delay can be obtained. Our algorithm does not have an adverse effect on the overall synthesis time, indicating that gate duplication is an efficient strategy for timing optimization.","Timing,
Logic,
Computer science,
Circuit synthesis,
Delay effects,
Circuit topology,
Minimization,
Partitioning algorithms,
Tail"
Ranging schemes for fast dynamic recovery of DOCSIS networks,"Power outages can seriously disrupt the digital services over community antenna television (CATV) networks. This paper proposes two novel schemes for the ranging process of the Data Over Cable Service Interface Specification (DOCSIS) protocol, within the dynamic persistent ranging framework, which offer an absolute reduction of up to 34% in a cable network's recovery time with an average of 28%. Based on the number of previously, active cable modems (CM) the CMTS allocates the best backoff start/end and updates them once (single backoff shift, SBS) or continuously (continuous backoff shift, CBS) as soon as a percentage of a non-ranged CMs is reached.","Cable TV,
Collision mitigation,
Media Access Protocol,
Modems,
Communication cables,
Bandwidth,
Digital video broadcasting,
Helium,
Computer science,
Hybrid fiber coaxial cables"
Speech transcript analysis for automatic search,"We address the problem of finding collateral information pertinent to a live television broadcast in real time. The solution starts with a text transcript of the broadcast generated by an automatic speech recognition system. Speaker independent speech recognition technology, even when tailored for a broadcast scenario, generally produces transcripts with relatively low accuracy. Given this limitation, we have developed algorithms that can determine the essence of the broadcast from these transcripts. Specifically, we extract named entities, topics, and sentence types from the transcript and use them to automatically generate both structured and unstructured search queries. A novel distance-ranking algorithm is used to select relevant information from the search results. The whole process is performed online and the query results (i.e., the collateral information) are added to the broadcast stream.","Speech analysis,
TV broadcasting,
Multimedia communication,
Streaming media,
Speech recognition,
Data mining,
Couplings,
Computer displays,
Auditory displays,
Information analysis"
A comparison between single-agent and multi-agent classification of documents,,"Thesauri,
Information filtering,
Information filters,
Collaboration,
Information retrieval,
Computer science,
Multiagent systems,
Vocabulary,
Libraries,
Information science"
Spreading knowledge about Gnutella: a case study in understanding net-centric applications,"The paper describes our experiences in attempting to understand the functional nature, high-level design, and implementation details of Gnut, a program that implements the Gnutella peer-to-peer Internet file system protocol. Gnutella is representative of a new breed of net-centric applications that is both qualitatively and quantitatively different than the typical legacy systems that are usually the focus of program understanding exercises. The primary motivation for this analysis is our interest in evaluating the applicability of traditional reverse engineering tools to aid in the understanding of this type of contemporary software system. A secondary motivation for the case study is an assessment of an existing framework for categorizing the capabilities of software reverse engineering environments, to further develop its capabilities. We found that the representative tool we used to examine Gnut, Source Navigator, does not adequately address the requirements of understanding net-centric applications. In particular, the emphasis on static source code analysis is no longer appropriate for this type of application. Dynamic analysis and network monitors are also needed to gain a more complete understanding of the subject system. We also found that the framework requires some alteration to be used in this new context, although the description of the tool's support for the canonical activities of reverse engineering and its quality attributes remains pertinent.","Computer aided software engineering,
Application software,
Reverse engineering,
Peer to peer computing,
Internet,
Software systems,
Computer networks,
Computer science,
File systems,
Protocols"
Why we need a different view of software architecture,"The definition and understanding of software architectures and architecture views still shows considerable disagreement in the software engineering community. The paper argues that the problems we face exist because our understanding is based on specious analogies with traditionally engineered artefacts. A review of the history of ideas shows the evolution of this understanding. A detailed examination is then presented of the differences that exist between the nature of the systems, the content of their large-scale representations, and how they are used in practice in the respective disciplines. These differences seriously undermine the analogies used to develop our understanding and this is discussed in terms of software engineering as a whole.",
Software quality management and software process improvement in Denmark,"The software business is a fast growing industry sector and lack of quality has a significant consequence for society and economy. However, reports about unfinished development projects, project overrruns and system and software failures are still the rule. Software quality management (SQM) standards and software process improvement (SPI) methodologies are all promoted to solve these problems. However, little is known about how prevalent these standards and methodologies actually are. We have, therefore performed a questionnaire-based survey in the Danish software producing industry. The standards and methodologies are not known by 40% of the responding organizations. Only 36% of the enterprises use one or several of the approaches to some degree, while two thirds of the organizations that had not adopted any of the approaches had never heard about them. These and additional results about the use of standards and the problems with introducing standards are presented and discussed in detail in the paper.","Software quality,
Quality management,
Software standards,
Business,
Computer industry,
Standards organizations,
Programming,
Coordinate measuring machines,
Informatics,
Software systems"
Differentiated link based QoS routing algorithms for multimedia traffic in MPLS networks,"MPLS networks require a QoS routing algorithm providing the characteristics of MPLS networks that establish a bandwidth guaranteed tunnel from source to destination. Our proposed QoS routing scheme gives a priority to multimedia traffic prone to block and differentiates network links into four classes based on link state information. Links have the different weights according to their classes and type of traffic transferred. We select the path with the minimum weight sum of links on the path. We also consider the observed blocking probability to reduce the frequency of the QoS state exchanges and supplement the out-of-date QoS information since the QoS state update results in the network overhead. Through the comparison between other existing schemes and ours, we show that our scheme with its low overhead and comparable performance is a better alternative to QoS routing schemes.",
Extending role-based access control model with states,The role-based access control (RBAC) concept is a powerful way of describing access control policy. The goal of this paper is to include the notion of states and state transitions into the RBAC model and to view changes of components of RBAC model as transitions between states of one access control policy. This allows us to better review properties of the access control policy as well as compare the policies.,"Access control,
Permission,
Security,
Computer science,
Power engineering and energy,
Information technology,
Electronic mail"
New cepstral zero-pole vocal tract models for TTS synthesis,Speech is an analog sound signal produced by exciting the human vocal tract. The magnitude response of the vocal tract exhibits both peaks (formants) and valleys (antiformants). Vocal tract models are differentiated according to whether they model the formants alone (LPC models) or also antiformants (ARMA and cepstral models). New structures are proposed for an effective realization of cepstral vocal tract models that model both formants and antiformants.,"Cepstral analysis,
Speech synthesis,
Transfer functions,
Frequency,
Difference equations,
Sampling methods,
Cepstrum,
Digital filters,
Computer science,
Human voice"
Multimodal control of reaching-simulating the role of tactile feedback,"By the onset of reaching, young infants are already able to keep track of the position of their hand by using visual feedback from the target and proprioceptive feedback from the arm. How is this multimodal coordination achieved? We propose that infants learn to coordinate vision and proprioception by using tactile feedback from the target. In order to evaluate this hypothesis, we employ an evolutionary-based learning algorithm as a proxy for trial-and-error sensorimotor development in young infants. A series of simulation studies illustrate how touch: 1) helps coordinate vision and proprioception; 2) facilitates an efficient reaching strategy; and 3) promotes intermodal recalibration when the coordination is perturbed. We present two developmental predictions generated by the model and discuss the relative importance of visual and tactile feedback while learning to reach.","Feedback,
Pediatrics,
Target tracking,
Psychology,
Predictive models,
Computer science,
Councils,
Information resources,
Computer vision,
Monitoring"
Using an engineering approach to understanding and predicting Web authoring and design,"Software practitioners recognise the importance of realistic estimates of effort to the successful management of software projects, the Web being no exception. Having realistic estimates at an early stage in a project's life-cycle allows project managers and development organisations to manage resources effectively. Prediction is a necessary part of an effective process, whether it be authoring, design, testing or Web development as a whole. The first part of this paper describes a case study evaluation (CSE) where we measured characteristics of Web applications and the effort involved in designing and authoring those applications. The second half explores how we can use the measurements obtained to predict the effort involved in designing and authoring future Web applications. The prediction models employed were two algorithmic models - linear regression and stepwise multiple regression. Although the models generated did not have good prediction power, we believe that by using techniques that are more flexible, we will have better results.","Design engineering,
Predictive models,
Project management,
Resource management,
Costs,
Computer science,
Software maintenance,
Life estimation,
Testing,
Linear regression"
Teaching introductory programming for engineers in an interactive classroom,"This paper reports the authors' experiences in teaching introductory programming for engineers in an interactive classroom.. The authors describe how the course has evolved from the traditional course, the structure of the classroom, the choice of software, and the elements involving interactive, active, and collaborative learning. They discuss their strategy for assessment. They describe the assessment results including a retrospective assessment of the previous course. They suggest how the course relates to the nontraditional student. They conclude with some suggestions for future modifications.","Education,
Computer aided instruction,
Educational institutions,
Computer applications,
Packaging,
Programming profession,
Stress,
Design engineering,
Collaborative software,
Collaborative work"
A new architecture of providing end-to-end quality-of-service for Differentiated Services network,"This paper focuses on providing end-to-end multiple quality of service (QoS) in the Differentiated Services (DiffServ) network, including guaranteed services, assured services and best-effort services. We propose a new architecture that can not only provide coarse grain QoS, but also guarantee end-to-end QoS for premium service without per-flow state management at core routers under the DiffServ architecture. In order to accomplish it, we develop a signaling protocol to help to select the route satisfying the end-to-end QoS requirements and reserve resources for each the flow when a session is initiated. A new mechanism is proposed to make packets of each flow keep the same route. Meanwhile packets belonging to the other classes of service are still in normal DiffServ. Bandwidth broker (BB) is introduced to allocate the resources in an autonomous domain and between them. We also give a simple discussion of our architecture and some consideration of the decentralized implementation of BB.","Quality of service,
Diffserv networks,
Telecommunication traffic,
Aggregates,
Scalability,
Computer architecture,
Internet,
Computer science,
Protocols,
Bandwidth"
Dimensions of the knowledge management process,"One of the concerns of knowledge management should be the management of the knowledge management process itself. Our purpose is to start discussion on this aspect by subjecting knowledge management to a preliminary ontological study. The aim is for this discussion is to initiate work on a capability maturity model for knowledge management. We consider knowledge to take the form of domain models, and the latter to be composed of data, process descriptions, rules, and capabilities. Knowledge management is seen as relating to the acquisition, representation, and use of knowledge. Throughout, we consider the uncertainties inherent in knowledge management.","Knowledge management,
Capability maturity model,
Ontologies,
Uncertainty,
Remuneration,
Computer science,
Costs,
Investments,
Contracts,
Marketing and sales"
Learning to Solve Planning Problems Efficiently by Means of Genetic Programming,"Declarative problem solving, such as planning, poses interesting challenges for Genetic Programming (GP). There have been recent attempts to apply GP to planning that fit two approaches: (a) using GP to search in plan space or (b) to evolve a planner. In this article, we propose to evolve only the heuristics to make a particular planner more efficient. This approach is more feasible than (b) because it does not have to build a planner from scratch but can take advantage of already existing planning systems. It is also more efficient than (a) because once the heuristics have been evolved, they can be used to solve a whole class of different planning problems in a planning domain, instead of running GP for every new planning problem. Empirical results show that our approach (EVOCK) is able to evolve heuristics in two planning domains (the blocks world and the logistics domain) that improve PRODIGY4.0 performance. Additionally, we experiment with a new genetic operatorInstance-Based Crossoverthat is able to use traces of the base planner as raw genetic material to be injected into the evolving population.","search,
Genetic planning,
genetic programming,
evolving heuristics,
planning"
Radio wave propagation measurements in tunnel entrance environment for intelligent transportation systems applications,"Underground environments are of great interest to intelligent transportation system (ITS) applications, since they occur frequently in both urban and rural situations, and have peculiar propagation characteristics. This investigation includes two of the major fields in ITS applications: road-to-vehicle, and vehicle-to-vehicle communications. Besides that, the propagation measurements presented also focus on the transition effects when moving between the regions outside and inside a tunnel, which represents its entrance environment. The experimental results of path loss and delay spread profiles are presented and analyzed.","Intelligent transportation systems,
Pricing,
Intelligent Transportation Systems Society,
Road vehicles,
Urban areas,
Area measurement,
Pollution measurement,
Propagation losses,
Propagation delay,
Computer science"
Academic software engineering: what is and what could be? Results of the first annual survey for international SE programs,"According to data received from an international survey, almost 6800 students are enrolled in software engineering degree programs in 11 countries, as of January, 2001. A total of 94 academic programs in software engineering are in place at 60 universities with 350 full-time faculty and nearly 200 part-time faculty teaching hundreds of undergraduate and graduate courses in the discipline. Over 5500 people have obtained degrees in software engineering since 1979. The authors are conducting the first of an ongoing annual survey of international academic software engineering programs, as a joint ACM/IEEE-CS project. This status report covers: history, audience, initial survey, initial partial results available on the WWW, request for evaluation of WWW-site, request for additional questions for next version of survey, time-line for next version of the survey, ""lessons learned,"" and some future directions. The annual report and survey results will be posted on a wide variety of Web pages. A more current report, based on the sabbatical of the first author, will be presented at the conference. The sabbatical involves the initial development of an ""International Software Engineering University Consortium-ISEUC"". A sample scenario for an employee in industry who becomes a student in ISEUC is given.","Software engineering,
Educational institutions,
Databases,
Computer science,
World Wide Web,
Information science,
Postal services,
Education,
History,
Web pages"
Backtracking in independent and-parallel implementations of logic programming languages,"In this paper, we present an implementation model which efficiently supports backtracking in an independent and-parallel nondeterministic system. The problem is tackled in the context of logic programming, although the solution proposed is sufficiently general to be easily extended to different nondeterministic systems, such as constraint programming systems. The complexity of the problem is demonstrated by the fact that most existing and-parallel systems either do not support backtracking over and-parallel calls or simply avoid analyzing the performance of their systems in the presence of nondeterministic benchmarks. The implementation model we present is an extension of the backtracking scheme developed by Hermenegildo and Nasr (1986) and relies on a novel memory organization scheme and on the use of various optimizations to reduce communication and overhead. The solution developed has been implemented in the ACE Parallel Prolog system. The performance of the system is analyzed on a variety of benchmarks. The results obtained are remarkable: speedups achieved during forward execution are not lost in heavy backtracking activities and, frequently, super-linear speedups are obtained thanks to a semi-intelligent backtracking scheme.","Logic programming,
Parallel processing,
Performance analysis,
Computer science,
Constraint optimization,
Computer languages,
Artificial intelligence,
Search problems"
M3C: managing and monitoring multiple clusters,"PC clusters running Linux, provide the computational power of supercomputers of just a few years ago at a fraction of the purchase price. The lack of good administration and user level application management tools makes the operation of clusters more difficult than it need be and results in an increased operation cost. This paper describes an ongoing effort at Oak Ridge National Laboratory to develop tools to simplify the administration and use of computation clusters. The two tools integrated in this work include the M3C (Managing and Monitoring Multiple Clusters) and C3 (Cluster Command and Control) tool suite. M3C provides a Web-based graphical user interface for cluster administration. It is designed as an extendable framework that can work with different underlying back-end tools. C3 is an independent project that provides the underlying commands to effect an operation on a cluster. In this paper it serves as an example how a back-end tool can be integrated into the M3C framework.","Linux,
Command and control systems,
Web server,
Computerized monitoring,
Computer science,
Mathematics,
Supercomputers,
Costs,
High performance computing,
Sliding mode control"
Automatic memory hierarchy characterization,,"Documentation,
Timing,
Optimizing compilers,
Performance analysis,
Size measurement,
Delay,
Lifting equipment,
Computer science,
Electronic mail,
Design optimization"
Specification of channel filters for an interferometric radiometer,"Differences among the channels that form an interferometric radiometer result in amplitude and phase errors in the cross correlations measured from the signals collected by each pair of antennas (visibility samples). Since point source calibration of a satellite-embarked instrument operating in a radio astronomy protected band is, in principle, not possible, and although separable phase errors can be corrected, the nonseparable ones produce an irrecoverable loss of information and have to be kept below specified limits. Preliminary work performed by the authors established basic requirements on the channel transfer function so as to meet these strict specifications imposed by proper calibration and inversion procedures. These requirements were set, from a simple filter model, in terms of three basic parameters: center frequency, bandwidth, and overall group delay. The purpose of this work is, for any kind of filter, to translate these requirements into a given insertion loss mask that must be satisfied by all filters so as to keep nonseparable error terms under certain specified bounds. The numerical results and computer simulations presented are based on the specifications of MIRAS, a two-dimensional aperture synthesis radiometer currently under study by the European Space Agency, and its proposed in-orbit calibration procedure.","Radiometry,
Voltage measurement,
Decorrelation,
Pain,
Satellite antennas,
Current measurement,
Brightness"
Discrimination measures for target classification,"Target detection does not necessarily yield target classification since the detected targets may belong to different classes and cannot be differentiated one from another by target detection. This often occurs when no prior target knowledge is available. In order to resolve this dilemma, four measures are proposed for target discrimination in this paper, two of which are designed based on the Bhattacharyya distance and the other two are derived from the concept of the matched filter. They will be used to cluster the detected anomalies into different types of targets in an unsupervised manner. These four target discrimination measures take advantage of the second-order statistics of the image data to account for sample spectral correlation. Consequently, they all outperform the commonly used single-pixel based similarity measures, such as spectral angle mapper (SAM), Euclidean distance. An experiment-based quantitative study is conducted for their performance evaluation. Interestingly, all these four measures perform very similarly.","Covariance matrix,
Matched filters,
Signal processing,
Object detection,
Remote sensing,
Image processing,
Laboratories,
Computer science,
Statistics,
Euclidean distance"
An integrated high-level test synthesis algorithm for built-in self-testable designs,"Describes a high-level test synthesis algorithm for operation scheduling and data path allocation. It generates highly self-testable data path design while maximizing the sharing of test registers, which means only a small number of registers is modified for BIST. The algorithm also produces design with high test concurrency, thereby decreasing test time. In the approach, module allocation is guided by a testability balance technique. Register allocation is achieved by an incremental sharing measurement which chooses allocation steps that result in large increases in the sharing degrees of registers. Scheduling, on other hand, is carried out by rescheduling transformations which change the default scheduling to improve testability. With a variety of benchmarks, we demonstrate the advantage of our approach compared with other conventional approaches.","Automatic testing,
Built-in self-test,
Algorithm design and analysis,
Circuit testing,
Registers,
Concurrent computing,
Costs,
Production,
Scheduling algorithm,
Computer science"
A new design for a SPECT small-animal imager,"We demonstrate, using computer models, the feasibility of a new SPECT system for imaging small animals such as mice. This system consists of four modular scintillation cameras, four multiple-pinhole apertures, electronics, and tomographic reconstruction software. All of these constituents have been designed in our laboratory. The cameras are 120 mm/spl times/120 mm with a resolution of approximately 2 mm, the apertures can have either single or multiple pinholes, and reconstruction is performed using the OS-EM algorithm. One major advantage of this system is the design flexibility it offers, as the cameras are easy to move and the apertures are simple to modify. We explored a number of possible configurations. One promising configuration had the four camera faces forming four sides of a cube with multiple-pinhole apertures employed to focus the incoming high-energy photons. This system is rotated three times, so that data are collected from a total of sixteen camera angles. It is shown that this hybrid system has some superior properties to single-aperture-type systems. We conclude that this proposed system offers advantages over current imaging systems in terms of flexibility, simplicity, and performance.",
Improving cache performance of network intensive workloads,"The performance of servers for network-intensive workloads such as web services and online transaction processing applications depends on the effective utilization of the processor caches. A detailed analysis of the cache space utilization of web workloads shows us that several memory addresses are referenced only once during their lifetime in the cache. These references frequently reside in the cache for a long time contributing to the pollution of cache. The most commonly adopted least-recently-used (LRU) replacement scheme does not exploit this characteristic. In this paper, we propose an alternative block replacement policy called Single-Touch Aware Replacement (STAR) algorithm. This algorithm predicts blocks that will potentially be referenced only once and replaces them early enough to improve cache efficiency. The STAR scheme was implemented in a trace-driven cache simulator and the performance with several commercial workloads was analyzed. The use of the STAR algorithm results in up to 20% improvement in cache performance for web workloads (SPECweb96, SPECweb99) and up to 5% improvement in online transaction processing (TPC-C) workloads.","Pollution,
Web services,
Performance analysis,
Internet,
Hardware,
Computer science,
Computer architecture,
Service oriented architecture,
Laboratories,
Network servers"
"The Exu approach to safe, transparent and lightweight interoperability","Exu is a new approach to automated support for safe, transparent and lightweight interoperability in multilanguage software systems. The approach is safe because it enforces appropriate type compatibility across language boundaries. It is transparent since it shields software developers from the details inherent in low-level language-based interoperability mechanisms. It is lightweight for developers because it eliminates tedious and error-prone coding (e.g., JNI) and lightweight at run-time since it does not unnecessarily incur the performance overhead of distributed, IDL-based approaches. The Exu approach exploits and extends the object-oriented concept of meta-object, encapsulating interoperability implementation in meta-classes so that developers can produce interoperating code by simply using meta-inheritance. An example application of Exu to the development of Java/C++ (i.e., multilanguage) programs illustrates the safety and transparency advantages of the approach. Comparing the performance of the Java/C++ programs produced by Exu to the same set of programs developed using IDL-based approaches provides preliminary evidence of the performance advantages of Exu.","Computer science,
Runtime,
Application software,
Lifting equipment,
Software systems,
Safety,
Software libraries,
Computer languages,
Object oriented modeling,
Software architecture"
Application of road-to-vehicle communication and ranging system using spread spectrum technique to ADS,"As a solution to traffic congestion, ADS (Advanced Demand Signals scheme) which controls a traffic light by passing vehicles has been proposed. However it is assumed that this system correctly obtains the relative distance between each vehicle and the traffic signal through the information from vehicles. This paper discusses the application of road-to-vehicle communication and ranging system using spread spectrum technique to ADS.","Spread spectrum communication,
Communication system traffic control,
Lighting control,
Road vehicles,
Automotive engineering,
Control systems,
Intelligent systems,
Computer science,
Directive antennas,
Receiving antennas"
Testing subgraphs in large graphs,"Let H be a fixed graph with h vertices, let G be a graph on n vertices and suppose that at least /spl epsi/n/sup 2/ edges have to be deleted from it to make it H-free. It is known that in this case G contains at least f (/spl epsi/, H)n/sup h/ copies of H. We show that the largest possible function f (/spl epsi/, H) is polynomial in /spl epsi/ if and only if H is bipartite. This implies that there is a one-sided error property tester for checking H-freeness, whose query complexity is polynomial in 1//spl epsi/, if and only if H is bipartite.",
SIPTool: the 'Signal and Image Processing Tool'-an engaging learning environment,"The 'Signal and Image Processing Tool' is a multimedia software environment for demonstrating and developing signal and image processing techniques. It has been used at CalPoly for three years. A key feature is extensibility via C/C++ programming. The tool has a minimal learning curve, making it amenable for weekly student projects. The software distribution includes multimedia demonstrations ready for classroom or laboratory use. SIPTool programming assignments strengthen the skills needed for life-long learning by requiring students to translate mathematical expressions into a standard programming language, to create an integrated processing system (as opposed to simply using canned processing routines).","Signal processing,
Image processing,
Displays,
Filtering,
Programming profession,
Mathematical programming,
Microphones,
Signal processing algorithms,
Image resolution,
Real time systems"
Experimental evaluation of CPU performance features,The paper addresses the problem of evaluating CPU performance in real system environment. We present an efficient methodology of CPU performance analysis at the architecture (coarse grained) and microarchitecture (fine grained) levels. It is based on time and internal event monitoring technique. This methodology is referred to Intel processors operating in IBM PC environment. The usefulness of the presented approach was proved in many experiments described in the paper.,"Monitoring,
Performance analysis,
Computer architecture,
Hardware,
Circuit simulation,
Microprocessors,
Software performance,
System testing,
Central Processing Unit,
Computer science"
Core-stateless fair bandwidth allocation for TCP flows,"The standard end-to-end flow control implemented by the TCP protocol is ill-suited when it comes to achieving fair bandwidth allocation among competing TCP flows. Indeed, the lack of feedback from intermediate nodes does not allow a TCP source to regulate its throughput so that it is not sending more than its fair share, thus penalizing other, less aggressive flows. We propose a novel scheme that, building on existing work on network-layer stateless fair queueing, extends the approach to the TCP layer. Also, we discuss a possible implementation of both the network-layer and the transport-layer architecture. We test our solution under different traffic scenarios and show that not only is a fair bandwidth allocation achieved, but the overall network utilization for TCP flows is also increased.","Channel allocation,
Bandwidth,
Testing,
Computer science,
Protocols,
Feedback,
Throughput,
Buildings,
Telecommunication traffic,
Traffic control"
Optimistic transaction processing algorithms in pure-push and adaptive broadcast environments,"Considering the properties of mobile computing environments, push-based data dissemination systems have lately attracted considerable interest. However, the skewed access pattern of mobile clients makes the average wait time worse and they may want to request the data object to the server explicitly through the backchannel. We call the broadcast model supporting backchannels adaptive broadcast. We devise new algorithms for adaptive broadcast based on our previous works; that is, we divide data objects which the server maintains into push-data and pull-data. Clients have to explicitly request data objects in pull-data. Maintaining transactional consistency in both pure-push and adaptive broadcast environment is our main concern. We also evaluate the performance behavior through a simulation study.","Broadcasting,
Mobile computing,
Information retrieval,
Costs,
Delay,
Environmental management,
Computer science,
Data engineering,
Computer science education,
Systems engineering education"
Publish/subscribe vs. shared dataspace coordination infrastructures. Is it just a matter of taste?,"Two train architectural styles emerged in the panorama of inter-agent coordination infrastructures: the publish/subscribe model, according to which agents communicate via the raising and catch of events, and the shared-dataspace - Linda-like - approach in which communication is achieved by introducing and consuming objects from a common repository. In this paper we perform a rigorous comparison of the two approaches with a particular emphasis on their interchangeability. We obtain the following results: (1) the shared dataspace model can be reduced to the publish/subscribe architecture, while (2) the vice versa holds only if a global coordination operation is provided among the shared dataspace operations.","Computer science,
Context,
Electronic mail,
Indium phosphide,
Testing,
Mars,
Data analysis,
Performance analysis"
Multi-choice versus descriptive examinations,"Examinations using multi-choice questions can be automatically graded in a fraction of the time and cost of manually grading descriptive questions. There is much controversy concerning the efficacy of multi-choice questions in determining student ability, particularly in synthesis. Students also find such examinations confronting and generally prefer the chance to ""express themselves"". Random guessing can result in an undeserved grade. Negative points assigned to each question according to the number of choices, fixes this problem but students are scared of earning ""negative points"". Multiple-choice questions are very much more difficult to write than descriptive questions. However, considerable experience has been built up in the nature and use of multiple-choice examinations in all the areas mentioned. Written properly, multiple-choice examinations correlate strongly with assessments by descriptive tests. In this paper, a sampling of rules and techniques of writing multichoice-questions is given together with the experience gained using multi-choice tests.","Writing,
Humans,
Costs,
Sampling methods,
Embedded computing,
Feedback,
Automatic testing,
Computer graphics,
Animation"
Image segmentation based on statistically principled clustering,"A statistically principled approach to 1-dimensional clustering was introduced by Pauwels abd Frederix (2000). In this approach clustering is achieved by finding the smoothest density that is statistically compatible with the observed data. In the current contribution we propose two solutions for the optimisation problem that is at the heart of this algorithm. The first solution is based on spline-functions, while the second hinges on an expansion of the density in terms of Gaussians. The latter is reminiscent of mixture-models but fundamentally different in its interpretation. Finally, we argue that 1-dimensional histogram segmentation yields a powerful local nonparametric cluster-validity criterion that can be used to check the quality of proposed clusterings in higher dimensions.","Image segmentation,
Histograms,
Distribution functions,
Heart,
Clustering algorithms,
Mathematics,
Computer science,
Spline,
Fasteners,
Gaussian processes"
New approaches to service restoration in MPLS-based networks,"Multiprotocol label switching (MPLS) has emerged as the technology of choice for future IP networks. As the demands put on networks increase, MPLS can serve as the basis for providing better reliability, manageability, and overall quality of service. This paper proposes a new approach to service restoration in MPLS networks based on the concept of domain protection, where protection of paths leading from all ingress routers to a common egress router is calculated in one step. The scheme allows a flexible protection path placement independent of the working path placement. Protection paths are calculated using an algorithm with low polynomial complexity. We discuss a mechanism for switching over to the protection paths after a failure is detected. It ensures faster switching time, minimal loss and packet re-ordering. A simulation study is used to evaluate the benefits of the proposed scheme and to compare it with two recently proposed schemes for MPLS restoration: RSVP (resource reservation protocol) backup tunnels and fast reroute. The results show that the proposed scheme provides a better solution than the fast reroute scheme and comparable protection to the RSVP backup tunnels while exhibiting lower algorithmic complexity. Unlike the proposed scheme, both fast reroute and RSVP backup tunnels require simultaneous calculation of working and protection paths in order to guarantee a link disjoint placement.","Intelligent networks,
Multiprotocol label switching,
Protection,
Signal restoration,
Telecommunication traffic,
Packet switching,
Protocols,
Availability,
Computer science,
Electronic mail"
Analysis of clustering techniques to detect hand signs,"The term multimedia has a different meaning to different communities. The computer industry uses this term to refer to a system that can display audio and video clips. Generally speaking, a multimedia system supports multiple presentation modes to convey information. Humans have five senses: sight, hearing, touch, smell and taste. In theory, a system based on this generalized definition must be able to convey information in support of all senses. This would be a step towards virtual environments that facilitate total recall of an experience. This study builds on our previous work with audio and video servers and explores haptic data in support of touch and motor skills. It investigates the use of clustering techniques to recognize hand signs using haptic data. An application of these results is communication devices for the hearing impaired.","Handicapped aids,
Haptic interfaces,
Auditory system,
Computer science,
Computer industry,
Computer displays,
Auditory displays,
Humans,
Virtual environment,
Natural languages"
Spherical-Lens Antennas For Millimeter Wave Radars,"A low-cost 77 GHz spherical lens antenna has been developed for mobile applications. The antenna system consists of a 5 cm spherical Teflon lens which is fed by an endfire tapered slot antenna on a 127 m-thick Duroid substrate. The measured patterns result in symmetrical diffraction limited E and H-plane patterns with a 3-dB beamwidth of 5.5, a sidelobe level below -20 dB and a crosspolarization level of -19 dB. The calculated efficiency of this antenna system is 54%. The efficiency includes spillover loss, amplitude taper, non-uniform phase, and reflection loss. This very low cost antenna can be easily extended to provide 170 coverage using a circular feed array around the spherical Teflon lens.","Millimeter wave radar,
Radar antennas,
Lenses,
Slot antennas,
Millimeter wave measurements,
Mobile antennas,
Antenna measurements,
Diffraction,
Optical reflection,
Costs"
The SawMill framework for virtual memory diversity,"We present a framework that allows applications to build and customize VM services on the LA microkernel. While the LA microkernel's abstractions are quite powerful, using these abstractions effectively requires higher-level paradigms. We propose the dataspace paradigm which provides a modular VM framework. The modularity introduced by the dataspace paradigm facilitates implementation and permits dynamic configurability. Initial performance results from a prototype are promising.","Virtual manufacturing,
Yarn,
Computer science,
Costs,
Prototypes,
Computer architecture,
Power engineering and energy,
Application software,
Virtual prototyping,
Programming profession"
A study on using network flows in hierarchical QoS routing,"QoS routing is the process of routing a connection based on the connection's resource requirements. The overhead involved in QoS routing increases with the network size. State aggregation is an important technique that helps to reduce the overhead. We propose a new state aggregation technique based on ""network-flow"". Our approach allows a domain to update the aggregate sent by other domains and keep track of resource availability in other domains. We study the efficacy of our approach with respect to various network and traffic parameters. Preliminary simulation results show that our approach gives a better bandwidth admission ratio when compared with existing techniques.","Intelligent networks,
Routing,
Bandwidth,
Aggregates,
Delay,
Network topology,
Availability,
Joining processes,
Quality of service,
Computer science"
Visual QoS programming environment for ubiquitous multimedia services,,
Using Multiattribute Prediction Suffix Graphs to Predict and Generate Music,,
A predictive measurement-based fuzzy logic connection admission control,"This paper presents a novel measurement-based connection admission control (CAC) which uses fuzzy set and fuzzy logic theory. Unlike conventional CAC, the proposed CAC does not use complicated analytical models or a priori traffic descriptors. Instead, traffic parameters are predicted by an on-line fuzzy logic predictor (Qiu et al. 1999). QoS requirements are targeted indirectly by an adaptive weight factor. This weight factor is generated by a fuzzy logic inference system which is based on arrival traffic, queue occupancy and link load. Admission decisions are then based on real-time measurement of aggregate traffic statistics with the fuzzy logic adaptive weight factor as well as the predicted traffic parameters. Both homogeneous and heterogeneous traffic were used in the simulation. Fuzzy logic prediction improves the efficiency of both conventional and measurement-based CAC. In addition, the measurement-based approach incorporating fuzzy logic inference and using fuzzy logic prediction is shown to achieve higher network utilization while maintaining QoS.",
Numerical computation of Hankel functions of integer order for complex-valued arguments,"The accurate computation of a series of integer- order Bessel functions is often required in applications in engineering and physics. Numerous authors have shown how a recurrence relationship can be used in the backward and forward directions to accurately compute integer-order Bessel functions of the first and second kinds, respectively, for real- valued arguments. However, significant round-off errors can result when these standard recurrence algorithms are employed and the argument for the Bessel function is complex valued. C. F. du Toit recently developed an algorithm that overcomes these numerical instabilities for Bessel functions of the first and second kinds. However, if one needs to compute integer-order Hankel functions with complex arguments, then we have found that numerical round-off errors can lead to inaccurate results if Bessel functions of the first and second kinds are superimposed to obtain the desired Hankel functions. To address this problem, a technique is presented in this paper that uses a combination of forward and backward recurrence for the computation of integer-order Hankel functions with complex-valued arguments.","Electrical engineering,
Computers,
Physics"
"Performance analysis of ""content-aware"" load balancing strategy FLEX: two case studies","FLEX is a new cost effective, locality aware load balancing solution for a shared Web hosting service implemented on a cluster of machines (Cherkasova, 2000). FLEX allocates hosted Web sites to different machines in the cluster based on the sites' processing and memory requirements which are estimated using the site logs. Appropriate routing of requests can be achieved by using the DNS infrastructure, since each hosted Web site has a unique domain name. Using a simulation based on real traces, we evaluate the potential benefits of the new solution. We compare the performance of FLEX against Round-Robin and Optimal strategy. FLEX significantly outperforms Round-Robin (up to 110-250% in average server throughput), getting within 8%-30% of optimal performance achievable for those traces. FLEX solution shows superliner speedup when the number of nodes is increased from four to eight because it takes advantage of both the doubled processing power and memory.",
CSCL: an Internet based education model,"The paper provides a novel educational model on the Internet called CSCL, Computer Supported Cooperative Learning. It introduces the current models of education, overviews the whole system, elaborates the process of realization, discusses its implementation using Web technology, and finally, it provides some idea of the future of the CSCL model.","Internet,
Computer science education,
Radio access networks,
Computer science,
Educational technology,
Collaborative work,
Computer architecture,
Books"
On the signal bounding problem in timing analysis,"In this paper, we study the propagation of slew dependent bounding signals and the corresponding slew problem in static timing analysis. The selection of slew from the latest arriving signal, a commonly used strategy, may violate the rule of monotonic delay. Several methods for generating bounding signals to overcome this difficulty are described. The accuracy and monotonicity of each method is analyzed. These methods can be easily implemented in a static timer to improve the accuracy.","Timing,
Signal analysis,
Signal generators,
Circuit optimization,
Switches,
Computer science,
Chip scale packaging,
Circuit simulation,
Propagation delay,
Delay lines"
Glauber dynamics on trees and hyperbolic graphs,"We study discrete time Glauber dynamics for random configurations with local constraints (e.g. proper coloring, Ising and Potts models) on finite graphs with n vertices and of bounded degree. We show that the relaxation time (defined as the reciprocal of the spectral gap 1-/spl lambda//sub 2/) for the dynamics on trees and on certain hyperbolic graphs, is polynomial in n. For these hyperbolic graphs, this yields a general polynomial sampling algorithm for random configurations. We then show that if the relaxation time /spl tau//sub 2/ satisfies /spl tau//sub 2/=O(n), then the correlation coefficient, and the mutual information, between any local function (which depends only on the configuration in a fixed window) and the boundary conditions, decays exponentially in the distance between the window and the boundary. For the Ising model on a regular tree, this condition is sharp.","Tree graphs,
Sampling methods,
Polynomials,
Computer science,
Mutual information,
Boundary conditions,
Convergence,
Physics,
Terminology"
Document distribution algorithm for load balancing on an extensible Web server architecture,"Access latency and load balancing are the two main issues in the design of clustered Web server architecture for achieving high performance. We propose a novel document distribution algorithm for load balancing on a cluster of distributed Web servers. We group Web pages that are likely to be accessed during a request session into a migrating unit, which is used as the basic unit of document placement. A modified binning algorithm is developed to distribute the migrating units among the Web servers to fulfil the load balancing. We also present a redirection mechanism, which makes use of a migrating unit's property, to reduce the cost of request redirections. The distribution of Web documents would be recomputed periodically to adapt to the changes in client request patterns and system configuration. Simulation results show that our solution can reduce the amount of request redirection and document migration, and it can distribute workload properly among Web servers.","Load management,
Web server,
Service oriented architecture,
Delay,
Computer architecture,
Web pages,
File servers,
Clustering algorithms,
Computer science,
Information systems"
PLAN: a framework and specification language with an event-condition-action (ECA) mechanism for clinical test request protocols,"It has been generally recognised that the need to improve the quality of health care has lead to a strong demand for clinical protocols/guidelines supported by computer systems in their creation and execution. How to efficiently and effectively manage, query and manipulate the computerised clinical test protocols has posed a major challenge but received little research attention sofar. This paper addresses this issue from a unified approach based on an active rule mechanism. This paper first presents PLAN-a generic modelling framework and specification language with an Event-Condition-Action (ECA) mechanism for clinical test request protocols. It then discusses in detail components of the test protocol and patient test plan expressed in PLAN. The implementation of PLAN is also briefly discussed. This research is unique in that it introduces an active ECA mechanism into test protocol modelling and specification, and emphasises the importance of the efficient and effective management of computerised rest request protocols.","Specification languages,
Protocols,
Diabetes,
Medical services,
Electrical capacitance tomography,
Best practices,
Utility programs,
System testing,
Computer science,
Optical computing"
Exact matching in image databases,,
A test management and software visualization framework for heterogeneous distributed applications,"Large scale distributed applications are widely used by internet service providers for customers and by companies for their own use. There is a need to provide test management facilities for such systems to validate, evaluate performance and ensure conformance to service level agreements. These facilities are also needed for the certification process required as part of maintaining such systems. This paper describes an extensible framework called RiOT that can be used to manage testing, and allow dynamic visualization of heterogeneous distributed component-based applications.","Software testing,
Visualization,
Application software,
Certification,
System testing,
Software maintenance,
Java,
Quality assurance,
Computer science,
Large-scale systems"
Scheduling value-based transactions in distributed real-time database systems,,"Real time systems,
Database systems,
Access protocols,
Delay,
Processor scheduling,
Computer science,
Concurrency control,
Transaction databases,
Time factors,
System recovery"
A fast and updatable IP address lookup scheme,"The Internet is growing very rapidly in both the size and the amount of traffic. This growth has placed excessive strain on the Internet infrastructure, especially on routers. The IP address lookup is the operation that searches the longest matching prefix for the destination address of an incoming packet in order to determine the next hop of the packet. This operation is complex and is a major bottleneck in high-performance routers. In this paper, we propose a fast and updatable lookup scheme that can be easily implemented in hardware. We also present the memory allocation policy that supports for the incremental update of the forwarding table. Since our lookup scheme can be implemented with the small-bit logic and SRAM, the average delay per one lookup is about 18 ns. That is, our scheme can achieve 55.56 /spl times/ 10/sup 6/ routing lookups/s in average.","Internet,
Routing,
Computer science,
Capacitive sensors,
Hardware,
Logic,
Delay,
Random access memory,
Web sites,
Bandwidth"
Design of a generalized phase-controlled class E inverter,"The design of a generalized phase-controlled class E inverter is presented. This design takes into account most of the important parameters which affect the inverter performance. Moreover, we calculate the design values which achieve zero voltage switching continuously when phase-shift of two inverters varies.","Inverters,
Zero voltage switching,
Switches,
Frequency,
Equations,
Switching circuits,
Computer science,
Voltage control,
Magnetic variables control,
Power generation"
Broadcasting consistent data in mobile computing environments,"Recent studies on transaction processing in broadcast environments have an implicit assumption that the server is able to broadcast consistent data to the mobile clients. However, this assumption may not be valid unless there is a special algorithm to handle broadcasting data in a consistent and timely manner. To the best of our knowledge, there is no related work on this problem yet. The authors devise an efficient algorithm that causes least interference with the update transactions to read the entire database for broadcasting at the server. Our future work is to develop a complete framework for scheduling the broadcast transaction to meet the following objectives: read the entire database consistently with minimum impact to the system; broadcast fresh data; and meet the time constraint imposed by the periodicity of broadcast cycle.","Broadcasting,
Mobile computing,
Transaction databases,
Concurrency control,
Protocols,
Testing,
Computer science,
Time factors,
Scheduling algorithm,
Mobile communication"
A configuration management system supporting component-based software development,"Component-based software development has been viewed as an emerging paradigm of software development. This paper analyzes the requirements of configuration management in component-based development process. Based on the analysis, a prototype configuration management system is proposed to meet the requirements. An example of using the system is also given.","Software development management,
Programming,
Connectors,
Software architecture,
Computer science,
Prototypes,
Software prototyping,
Software maintenance,
Process control,
Software engineering"
Model based voice decomposition method with time constraint,"This paper proposes a new voice decomposition method with time constraint. Speech recognition of mixture of two and more voices and sounds is still very difficult. The model-based voice decomposition method proposed in our previous study solves the above problem; however, the solution is of a local optimal problem and the given spectral sequence sometimes varies rapidly and is non-realistic behavior. A new decomposition method solves a global optimal problem and the given spectral sequence changes are milder due to the time continuity constraint. This paper formulates the decomposition problem as an optimal path searching in the time-frequency domain. As the result of evaluation experiments, the average decomposition distortion is 4.16 dB and about 0.92 dB improvement is achieved.","Time factors,
Speech recognition,
Humans,
Hidden Markov models,
Microphone arrays,
Linear predictive coding,
Autocorrelation,
Spectrogram,
Computer science,
Acoustical engineering"
Cliques and fuzzy cliques in fuzzy graphs,"The authors define the concept of a fuzzy clique. In the case of a fuzzy graph, the concept of a cycle and a fuzzy cycle has been studied. The aim of the paper is to present the concept of a clique and a fuzzy clique in fuzzy graphs, consistent with the definition of cycles and fuzzy cycles in fuzzy graphs. Various interesting properties of fuzzy cliques are presented. A complete characterization of the structure of the fuzzy clique is also presented.",
The Dildis-project-using applets for more demonstrative lectures in digital systems design and test,"This paper presents a new teaching concept supporting the learning process by several features. It supports the possibility of distance learning as well as a Web-based computer-aided teaching, offers a set of tools (""interactive modules"") to inspect the teaching topics and carries out laboratory research. A big reservoir of examples and the possibility to generate others makes the learning process more interesting and allows learning at an individual depth and duration. The interactive modules are focused on correct solutions, easy action and reaction, multilingual descriptions, learning by doing, a game-like use, and fostering students in critical thinking, problem solving skills and creativity.","Digital systems,
System testing,
Education,
Electronic equipment testing,
Very large scale integration,
Circuit testing,
Integrated circuit technology,
Built-in self-test,
Internet,
Design automation"
Using models in virtual world design,"One possibility that the Internet gives us is to create graphical virtual worlds that give the participants an experience of being present at a virtual location and interact with other people there. We argue that new methods for design and development are needed for creating virtual worlds. The paper reports on a design project where the goal was to create a virtual world for academic interaction. In the project we explored an alternative design approach where physical models in Lego were utilized. Our conclusions are that virtual world design can benefit from concepts and methods from other design principles but also needs to adhere to the unique characteristics of the medium, and that the use of physical models has both advantages and drawbacks but can be beneficial to the design process. We also make an observation regarding the possibilities for virtual worlds as platforms for learning activities.",
Traffic shaping in end systems attached to QoS-supporting networks,"Quality of service (QoS) supporting network architectures, like the differentiated services architecture, require a certain agreement regarding service levels. Traffic characteristics like data rates will be part of such agreements and, thus, senders must take care to stay within the agreed limits. Traffic shaping is one important mechanism to avoid penalties in networks (dropped or delayed packets) due to violations of the agreement. This paper presents a new class-based QoS traffic shaper for Linux that aims at shaping aggregate traffic as well as individual flows within an aggregate. In comparison to other approaches (e.g., class-based queuing), our flow based queuing mechanism causes significantly less jitter. Implemented in end-systems, this approach even benefits from direct interaction with applications to create traffic in accordance to application requirements.","Telecommunication traffic,
Intelligent networks,
Traffic control,
Quality of service,
Aggregates,
Pricing,
Web and internet services,
Telematics,
Computer science,
Computer architecture"
Lifetime of a depression in the plasma density over Jicamarca produced by space shuttle exhaust in the ionosphere,"When the space shuttle orbiting maneuver subsystem (OMS) engines burn in the ionosphere, a plasma density depression, or hole, is produced. Charge exchange between the exhaust molecules and the ambient O+ ions yields molecular ion beams that eventually recombine with electrons. The resulting plasma hole in the ionosphere can be studied with ground-based, incoherent scatter radars (ISRs). This type of ionospheric modification is being studied during the Shuttle Ionospheric Modification with Pulsed Localized Exhaust (SIMPLEX) series of experiments over ISR systems located around the globe. The SIMPLEX 1 experiment occurred over Jicamarca, Peru, in the afternoon on October 4, 1997, during shuttle mission STS 86. An electron density depression was produced at 359 km altitude at the midpoint of a magnetic field line. The experiment was scheduled when there were no zonal drifts of the plasma so the modified field line remained fixed over the 50 MHz Jicamarca radar. The density depression was filled in by plasma flowing along the magnetic field line with a time constant of 4.5 min. The density perturbation had completely vanished 20 min after the engine burn. The experimental measurements were compared with two models: (1) SAMI2, a fully numerical model of the F region, and (2) an analytic representation of field-aligned transport by ambipolar diffusion. The computed recovery time from each model is much longer than the observed recovery time. The theory of ambipolar diffusion currently used in ionospheric models seems to be inadequate to describe the SIMPLEX 1 observations. Several possible sources for this discrepancy are discussed. The SIMPLEX 1 active experiment is shown to have the potential for testing selected processes in ionospheric models.","Radar,
Particle beams,
Atmospheric modeling,
Plasma density,
Magnetic field measurement,
Plasma measurements,
Mathematical model"
"Information security: science, pseudoscience, and flying pigs","The state of the science of information security is astonishingly rich with solutions and tools to incrementally and selectively solve hard problems. In contrast, the state of the actual application of science, and the general knowledge and understanding of existing science, is lamentably poor. Still we face a dramatically growing dependence on information technology, e.g., the Internet, that attracts a steadily emerging threat of well-planned, coordinated hostile attacks. A series of hard-won scientific advances gives us the ability to field systems having verifiable protection, and an understanding of how to powerfully leverage verifiable protection to meet pressing system security needs. Yet, we as a community lack the discipline, tenacity and will to do the hard work to effectively deploy such systems. Instead, we pursue pseudoscience and flying pigs. In summary, the state of science in computer and network security is strong, but it suffers unconscionable neglect.","Information security,
Computer security,
Computer networks,
Power system protection,
Power system security,
Information technology,
Internet,
Pressing,
Information theory,
Information systems"
2D TSA-tree: a wavelet-based approach to improve the efficiency of multi-level spatial data mining,"Due to the large amount of the collected scientific data, it is becoming increasingly difficult for scientists to comprehend and interpret the available data. Moreover typical queries on these data sets are in the nature of identifying (or visualizing) trends and surprises at a selected sub-region in multiple levels of abstraction rather than identifying information about a specific data point. The authors propose a versatile wavelet-based data structure, 2D TSA-tree (Trend and Surprise Abstractions Tree), to enable efficient multi-level trend detection on spatial data at different levels. We show how 2D TSA-tree can be utilized efficiently for sub-region selections. Moreover, 2D TSA-tree can be utilized to precompute the reconstruction error and retrieval time of a data subset in advance in order to allow the user to trade off accuracy for response time (or vice versa) at query time. Finally, when the storage space is limited, our 2D Optimal TSA-tree saves on storage by storing only a specific optimal subset of the tree. To demonstrate the effectiveness of our proposed methods, we evaluated our 2D TSA-tree using real and synthetic data. Our results show that our method outperformed other methods (DFT and SVD) in terms of accuracy, complexity and scalability.","Data mining,
Delay,
Propulsion,
Data visualization,
Information retrieval,
Earth,
Global Positioning System,
Ocean temperature,
Computer science,
Laboratories"
Differentiated services with statistical real-time guarantees in static-priority scheduling networks,"We propose and analyze a methodology for providing absolute differentiated services with statistical performance guarantees for real-time applications in networks that use class-based (as opposed to flow-aware) static priority schedulers. We develop a method that can be used to derive statistical delay guarantees in a flow-unaware fashion. Traditionally, both deterministic and statistical delay analysis methods either depend on schedulers that keep per-flow state information, or require detailed information about flow population at delay analysis time. The fact that no such information is needed for delay analysis allows us to perform deadline tests during system (re-)configuration time. We are so able to reduce the runtime admission control to a simple utilization test. No explicit delay computation is necessary at admission time, making this approach scalable to large systems.","Intelligent networks,
Admission control,
Delay effects,
Processor scheduling,
Information analysis,
Intserv networks,
Scalability,
Computer science,
Runtime,
Scheduling algorithm"
Almost tight upper bounds for vertical decompositions in four dimensions,"We show that the complexity of the vertical decomposition of an arrangement of n fixed-degree algebraic surfaces or surface patches in four dimensions is O(n/sup 4+/spl epsi//) for any /spl epsi/ > 0. This improves the best previously known upper bound for this problem by a near-linear factor, and settles a major problem in the theory of arrangements of surfaces, open since 1989. The new bound can be extended to higher dimensions, yielding the bound O (n/sup 2d-4+/spl epsi//), for any /spl epsi/ > 0, on the complexity of vertical decompositions in dimensions d /spl ges/ 4. We also describe the immediate algorithmic applications of these results, which include improved algorithms for point location, range searching, ray shooting, robot motion planning, and some geometric optimization problems.",
"Nanoscience, nanotechnology, and modeling",,
Year,,
Orienting university students to the language and culture of their disciplines,"International concerns about the inability of university English programs in the United States and elsewhere to adequately prepare students for the English of their professions prompted the creation of two important English language education movements: Writing Across the Curriculum (WAC) and English for Specific Purposes (ESP). Both of these movements have considerably improved English language education for specific disciplines, but neither has adequately addressed discipline-specific English language needs in a comprehensive fashion, neither theoretically nor in practice. The paper reports on progress toward this goal for Japanese university students in the field of computing and then suggests how this work might be carried out for specific populations of English language-learners in other disciplines.","Natural languages,
Electrostatic precipitators,
Writing,
Computer science,
Computer science education,
Context,
Educational programs,
Medical services,
Australia,
Engineering students"
A model-theoretic approach to regular string relations,"We study algebras of definable string relations, classes of regular n-ary relations that arise as the definable sets within a model whose carrier is the set of all strings. We show that the largest such algebra-the collection of regular relations-has some quite undesirable computational and model-theoretic properties. In contrast, we exhibit several definable relation algebras that have much tamer behavior: for example, they admit quantifier elimination, and have finite VC dimension. We show that the properties of a definable relation algebra are not at all determined by the one-dimensional definable sets. We give models whose definable sets are all star-free, but whose binary relations are quite complex, as well as models whose definable sets include all regular sets, but which are much more restricted and tractable than the full algebra of regular relations.","Algebra,
Formal languages,
Automata,
Computational modeling,
Virtual colonoscopy,
Logic functions,
Computer science,
Postal services,
Ores"
"Reduced complexity analysis of microstrip patch arrays, conformally mounted to a cylindrical conducting surface","Standard integral equation techniques, such as the moment method (MoM) face difficulties for very large antenna sizes, since the memory and CPU time they require may increase beyond computational limits. The method of auxiliary sources (MAS) demands substantially lower CPU time than MoM, retaining, nevertheless, the MoM accuracy. Unfortunately, its capabilities are depleted when applied to thin or open structures, due to the mandatory proximity of source and collocation points occurring in such geometries. The modified method of auxiliary sources (MMAS) has been developed to circumvent this difficulty, and has been successfully applied to the analysis of microstrip patch antennas, even when the dielectric substrate is thin enough to render conventional MAS inapplicable. In this paper, the MMAS is applied to the analysis of microstrip patch arrays, conformal to cylindrical surfaces, a model that simulates a communication antenna mounted to an aircraft fuselage. Several unpublished features of MMAS are discussed, whereas extensive, recently produced numerical results are presented and compared to reference solutions, validating the reliability and versatility of the method.","Microstrip antenna arrays,
Patch antennas,
Computer aided manufacturing,
Message-oriented middleware,
Microstrip antennas,
Geometry,
Testing,
Aerospace engineering,
Aircraft propulsion,
Aircraft manufacture"
Simple EM based channel estimation and data detection using estimation in the preceding frame in space-time coded signals,New EM-based channel estimation and data detection technique is proposed to reduce the number of iteration and make the estimation of space-time coding (STC) signals simple.,
Evaluation of logical story unit segmentation in video sequences,,"Video sequences,
Layout,
Image segmentation,
Time factors,
Intelligent systems,
Intelligent sensors,
Information systems,
Computer science,
Visualization,
Navigation"
Counting axioms do not polynomially simulate counting gates,"We give a family of tautologies whose algebraic translations have constant-degree, polynomial size polynomial calculus refutations over Z/sub 2/, but which require superpolynomial size bounded-depth Frege proofs from Count/sub 2/ axioms. This gives a superpolynomial size separation of bounded-depth Frege plus mod 2 counting axioms from bounded-depth Frege plus parity gates. Combined with another result of the authors, it gives the first size (as opposed to degree) separation between the polynomial calculus and Nullstellensatz systems.","Polynomials,
Circuits,
Calculus,
Computer science,
Computational modeling,
Computer simulation,
Complexity theory,
Approximation methods,
Decision trees"
Improving technical writing via web-based peer review of final reports,This paper describes the use of web-based software for peer evaluation of final environmental impact assessment documents written by senior environmental resources engineering students. The use of peer evaluation of documents was implemented in order to respond to the need to improve the technical writing skills of our graduates as part of our ABET 2000 outcomes assessment program. This paper discusses how students demonstrate their understanding of environmental impact assessment and their writing skills through the peer evaluation of their final documents. Students report that this activity assists them in the development of their writing skills. These evaluations are a student reflection activity and provide the instructor information for assessing the acquisition of course knowledge as well as the improvement of students' technical writing skills.,"Writing,
Professional communication,
Education,
Engineering students,
Reflection,
Displays,
HTML,
Software tools,
Teamwork,
Feedback"
Design of the data-retrieving engine for distributed multimedia presentations,"To provide the smooth progress of a Synchronization Multimedia Integration Language (SMIL) based distributed multimedia presentation, the data-retrieving engine for the player must pre-fetch each object before its playback time. In this paper, a smart data-retrieving engine is proposed, which adopts a retrieval policy named the just-in-time policy. The policy requires the retrieval process of an object to be finished right before the playback time of the object. By converting the synchronization relationship of objects in the SMIL document to the real-time synchronization model, which simplifies the handling of the synchronization relationship, and considering the network condition, the data-retrieving engine could determine the request time for each object. The engine then makes the request to fetch each object for the ongoing presentation according to the pre-computed request time, and provides the player with proper media objects for smooth playback of the presentation.","Engines,
Information retrieval,
Delay effects,
Multimedia communication,
Laboratories,
Computer science,
Data engineering,
Document handling,
Timing,
Web sites"
Supporting mobile agent applications using wrappers,"The TACOMA project has investigated software support for mobile agents. Several prototypes have been developed, with experiences in distributed applications directing the effort. The paper presents a mechanism that supports implementing agent applications by creating troops of agents using wrappers. This solution requires little extra support from the agent system, and may be used to construct applications with a wide variety of functionality requirements. We show how this mechanism can be used to dynamically install (3rd party) components at agent servers. We demonstrate this concept with a dynamically installed service locator which uses the Gnutella peer-to-peer protocol.","Mobile agents,
Application software,
Protocols,
Intelligent agent,
Security,
Fault tolerance,
Computer science,
Peer to peer computing,
Robots,
Humans"
Deterministic computation of the Frobenius form,"A deterministic algorithm for computing the Frobenius canonical-form of a matrix over a field is described. A similarity transformation-matrix is recovered in the same time. The algorithm is nearly optimal, requiring about the same number of field operations as required for matrix multiplication. Previously-known reductions to matrix multiplication are probabilistic.","Bismuth,
Costs,
Polynomials,
Algorithms,
Algebra,
Shape,
Testing,
Application software,
Computer science"
Empowerment to success: the class structure in an honors engineering course,"Structuring a quality honors course is a challenge in most disciplines. The authors have offered an honors section in their introductory computer science class for several years. During this time, they have restructured their honors section several times, including shifting both the focus and format. They started with a concentration in Web design, shifted to robotics, and then advanced onto student presentations. The layout began as a traditionally structured course, evolved with the introduction of student mentors, and most recently has advanced to student-directed learning. It is evidenced from both their evaluation of the student work and the student evaluations of the course that self-directed learning (i.e., empowerment) has resulted in a satisfactory and successful honors class.","Computer science,
Web design,
Education,
Robots,
Engineering students,
Capacitive sensors,
History"
An integrated mobile robot path (re)planner and localizer for personal robots,"We describe a method for navigation in a known indoor environment, such as a home or office, that requires only inexpensive range sensors. Our framework includes a high-level planner which integrates and coordinates path planning and localization modules with the aid of a module for computing regions which are expected, with high probability, to contain the robot at any given time. The localization method is based on simple geometric properties of the environment which are computed during a preprocessing stage. The roadmap-based path planner enables one to select routes, and subgoals along those routes, that will facilitate localization and other optimization criteria. In addition, our framework enables one to quickly plan new routes, dynamically, based on the current position as computed by intermediate localization operations. We present simulation and hardware experimental results that illustrate the practicality and potential of our approach.","Mobile robots,
Robot kinematics,
Navigation,
Indoor environments,
Path planning,
Robustness,
Road safety,
Error correction,
Computer science,
Application software"
Real-time satellite data transfer system for Siberian NOAA image database,"Tohoku University (TU) in Sendai, Japan, and the Siberian Branch of the Russian Academy of Science (SBRAS) in Novosibirsk, Russia, are cooperating in an academic exchange program. One of the main themes is to construct the infrastructure because there is about 5,000 km distance between them. The main systems are a communication satellite network and a NOAA station built in Siberia. The received NOAA data are transferred to TU by this system in real-time. We report the current condition of the data transfer for NOAA AVHRR data which are received at Novosibirsk, Russia.","Real time systems,
Image databases,
Artificial satellites,
Computer networks,
Lakes,
Satellite antennas,
Automatic control,
Hard disks,
Earth,
Computerized monitoring"
RT-level fault simulation based on symbolic propagation,The rapid rise in size and complexity of VLSI circuits has stimulated a need to handle fault simulation at higher levels of abstraction. We outline an RT-level fault simulation technique that utilizes symbolic data to group fault effects. Experimental results show that the proposed methodology provides superior speed-ups and accurate fault coverages.,
A stream tapping protocol with partial preloading,"Stream tapping-also known as patching-can reduce the bandwidth requirements of video-on-demand services by allowing new customer requests to ""tap"" the data streams of other requests for the same video. Previous studies have shown that stream tapping works best when the request arrival rate does not exceed ten to twenty requests per hour for a two-hour video. At higher arrival rates, it performs much worse than broadcasting protocols. To overcome this limitation, we propose a stream tapping protocol that preloads in the customer set-top box the first few minutes of all popular videos. To offset the cost of the additional buffer space, our protocol never requires the set-top box to receive data from the video server at more than twice video consumption rate. Our simulations indicate that preloading the first eight minutes of a two-hour video was enough to achieve lower bandwidth requirements than the best broadcasting protocols at any request arrival rate.","Protocols,
Multimedia communication,
Broadcasting,
Streaming media,
Bandwidth,
Costs,
Proposals,
US Department of Transportation,
Computer science,
Aggregates"
"Embedding among HCN(n,n), HFN(n,n) and hypercube","We analyze the embedding among HCN(n,n), HFN(n,n) and 2n-dimensional hypercube Q/sub 2n/. Further we prove that it is possible to embed HCN(n,n) to HFN(n,n) with dilation 3 and the cost of embedding HFN(n,n) to HCN(n,n) is O(n). We prove that it is possible to embed 2n-dimensional hypercube Q/sub 2n/ to HCN(n,n) and HFN(n,n) with dilation 3 and the cost of embedding HCN(n,n) and HFN(n,n) to 2n-dimensional hypercube Q/sub 2n/ is O(n).","Hypercubes,
Costs,
Multiprocessor interconnection networks,
Computer science,
Embedded computing,
Network topology,
Broadcasting,
Helium"
A web-based intelligent tutoring system teaching nursing students fundamental aspects of biomedical technology,"In this paper, we present the architecture of a Web-based Intelligent Tutoring System (ITS) for distant education of nursing students in fundamental aspects of the most common medical equipment. It offers course units covering the needs of users with different knowledge levels and characteristics. It tailors the presentation of the educational material to the users' diverse needs by using AI techniques to specify each user's model as well as to make pedagogical decisions. This is achieved via an expert system that uses a hybrid knowledge representation formalism integrating symbolic rules with neurocomputing.","Intelligent systems,
Medical services,
Computer science education,
Biomedical equipment,
Artificial intelligence,
Educational programs,
Expert systems,
Biomedical computing,
Biomedical engineering,
Computer architecture"
Empirical Modelling of Genetic Algorithms,"This paper addresses the problem of reliably setting genetic algorithm parameters for consistent labelling problems. Genetic algorithm parameters are notoriously difficult to determine. This paper proposes a robust empirical framework, based on the analysis of factorial experiments. The use of a graeco-latin square permits an initial study of a wide range of parameter settings. This is followed by fully crossed factorial experiments with narrower ranges, which allow detailed analysis by logistic regression. The empirical models derived can be used to determine optimal algorithm parameters and to shed light on interactions between the parameters and their relative importance. Re-fined models are produced, which are shown to be robust under extrapolation to up to triple the problem size.","line labelling,
Genetic algorithms,
empirical models,
factorial experiments,
constraint satisfaction"
Study on the pumping wavelength dependency of S/sup +/-band fluoride based thulium doped fiber amplifiers,We present the pump wavelength dependency of a S/sup +/-band thulium doped fluoride fiber amplifier by using a newly developed numerical model for the 1.47 /spl mu/m amplification band. This is for the first time to authors' knowledge. The optimum pump wavelength varies from 1045 nm to 1070 nm depending on the operating conditions. Insights on the amplifier operation dependence on the pump wavelength and pumping power will be discussed.,"Doped fiber amplifiers,
Optical fiber communication,
Optical fiber amplifiers,
Equations,
Computer science,
Numerical models,
Power amplifiers,
Absorption,
Electronic mail,
Semiconductor optical amplifiers"
Dual auto-regressive modelling approach to Gaussian process identification,,"Independent component analysis,
Delay effects,
Symmetric matrices,
Computer science,
Partial response channels,
White noise,
Wireless communication,
Speech recognition,
Data analysis,
Signal processing"
Differential logging: a commutative and associative logging scheme for highly parallel main memory database,"With a GByte of memory priced at less than $2000, main-memory DBMSs (MMDBMSs) are emerging as an economically viable alternative to disk-resident DBMSs (DRDBMSs) in many problem domains. The MMDBMS can show significantly higher performance than the DRDBMS by reducing disk accesses to the sequential form of log writing and occasional checkpointing. Upon a system crash, the recovery process begins by accessing the disk-resident log and checkpoint data to restore a consistent state. With increasing CPU speed, however, such disk access is still the dominant bottleneck in MMDBMSs. To overcome this bottleneck, this paper explores alternatives of parallel logging and recovery. The major contribution of this paper is the so-called differential logging scheme that permits unrestricted parallelism in logging and recovery. Using the bit-wise XOR operation both to compute the differential log between the before and after images and to recover the consistent database state, this scheme offers the room for significant performance improvement in the MMDBMS. First, with logging done on the difference, the log volume is reduced to almost half compared with the conventional physical logging. Second, the commutativity and associativity of XOR enables processing of log records in an arbitrary order. This means that we can freely distribute log records to multiple disks to improve the logging performance. During the recovery time, we can do a parallel restart independently for each log disk. This paper shows the superior performance of the differential logging compared to the physical logging in a shared-memory multiprocessor environment.","Checkpointing,
Computer crashes,
Image restoration,
Computer science,
Environmental economics,
Data structures,
Writing,
Image databases"
Can we do without ranks in Burrows Wheeler transform compression?,"Compressors based on the Burrows Wheeler transform (1994) convert the transformed text into a string of (move-to-front) ranks. These ranks are then encoded with an Order-0 model, or a hierarchy of such models. Although these rank-based methods perform very well, we believe the transformation to MTF numbers blurs the distinction between individual symbols and is a possible cause of inefficiency. Instead of relying on symbol ranking, we examine the problem of directly encoding the symbols in the BWT text.","Computer science,
Compressors,
Encoding,
Software engineering,
Arithmetic,
Access protocols,
Throughput,
Australia Council,
USA Councils,
World Wide Web"
Low-cost fault-tolerance for mobile nodes in mobile IP based systems,"We first identify some problems occurring in the case where a traditional causal message logging approach for distributed systems is used as a fault-tolerance technique for mobile nodes in Mobile IP based systems. Then, we present a new causal message logging protocol with independent checkpointing for efficiently handling several constraints of mobile nodes such as: mobility and disconnection, limited life of battery power, small amount of storage and low bandwidth on wireless link. Moreover, the protocol can considerably reduce the number of additional messages and forced checkpoints needed when garbage collecting the log information of mobile nodes compared with previous causal message logging protocols.","Fault tolerant systems,
Batteries,
Access protocols,
Wireless application protocol,
Bandwidth,
Computer science,
Mobile computing,
Checkpointing,
Routing,
Operating systems"
Decision analysis of network-based intrusion detection systems for denial-of-service attacks,"Two of practical issues in designing a network-based intrusion detection system for denial-of-service attacks are; how to represent the distributions of detection probability, false alarm probability and miss probability; how to achieve a high detection probability, a low false alarm probability and a low miss probability for decision making. This paper gives the representations to describe three probability distributions. Based on them, the authors derive a detection region within which one may achieve a high detection probability, a low false alarm probability and a low miss probability by selecting a suitable threshold value. A case study is demonstrated.","Intrusion detection,
Computer crime,
Telecommunication traffic,
Traffic control,
Computer science,
Decision making,
Probability distribution,
Data security,
Information security,
Pattern matching"
Providing quality of service guarantees using only edge routers,"Providing strict bandwidth guarantees to packet flows in the Internet is an inherently challenging task. It requires signaling mechanisms, policing mechanisms, accurate and rapid accounting of network resources and call admission control. We present a novel protocol that provides bandwidth guarantees to Internet packet flows. This protocol, called the edge-assisted quality of service (EQOS) protocol, requires modifications to only a subset of an administrative domain's routers, namely the edge routers on the domain's periphery. Legacy routers within the core of the domain require no modifications. Our protocol maintains a high network utilization by dynamic sharing of bandwidth between the reserved and best effort flows. We present the results of several simulation experiments showing that EQOS is able to provide bandwidth guarantees to competing flows.","Quality of service,
Bandwidth,
Internet,
Protocols,
Intserv networks,
Computer science,
Call admission control,
Tail,
Contracts,
Microwave integrated circuits"
Image-based path-planning algorithm on the joint space,"To design an image-based path-planning algorithm, we should decrease the sum of moving distances of degrees-of-freedom of a manipulator. The algorithm A* selects the shortest path in the joint space by hard search. The combination search requires long distance moving. In a computer (virtual) world, the moving is not always time consuming. However in a real world, the moving is extremely time consuming. To overcome this drawback, we are seeking for a one-way search. For this reason, the sensor-based path-planning algorithms are preferred to an image-based path-planning algorithm. On the observation, we adopt a three or more dimensional sensor-based path-planning algorithm as the image-based path-planning algorithm. Its heuristics is defined by a difference between present and objective images. If the heuristics amounts to zero, the image-based algorithm stops. Therefore, convergence of a camera on a manipulator tip to an objective state is kept, and it is not influenced at all by uncertainties of robot and camera parameters.","Path planning,
Orbital robotics,
Costs,
Convergence,
Robot vision systems,
Cameras,
Robot sensing systems,
Algorithm design and analysis,
Computer science,
Uncertainty"
DiffServer: application level differentiated services for Web servers,"Web content hosting, in which a Web server stores and provides Web access to documents for different customers, is becoming increasingly common. For example, a Web server can host Web pages for several different companies and individuals. Traditionally, Web service providers (WSPs) provide all customers with the same level of performance (best-effort service). Most service differentiation has been in the pricing structure (individual vs. business rates) or the connectivity type (dial-up access vs. leased line, etc.). This report presents DiffServer, a program that implements two simple, server-side, application-level mechanisms (server-centric and client-centric) to provide different levels of Web service. The results of the experiments show that there is not much overhead due to the addition of this additional layer of abstraction between the client and the Apache Web server under light load conditions. Also, the average waiting time for high priority requests decreases significantly after they are assigned priorities as compared to a FIFO approach.","Web server,
Web services,
Quality of service,
Web and internet services,
Pricing,
Bandwidth,
Telecommunication traffic,
Costs,
Application software,
Computer science"
The challenge of accurate software project status reporting: a two stage model incorporating status errors and reporting bias,"Software project managers perceive and report about the project's status. Recognizing that their status perceptions might be wrong and that they may not faithfully report what they believe leads to a natural question - how different is the true software project status from the reported status? In this paper, we construct a two-stage model which accounts for project manager errors in perception and bias that might be applied before reporting the project's status to executives. We call the combined effect of errors in perception and bias ""project statics distortion"". The probabilistic model has its roots in information theory and uses the discrete project status from traffic-light reporting. The true states of projects of varying risk were elicited from a panel of five experts, and these formed the model input. Key findings suggest that executives should be skeptical of favorable status reports, and that, for higher-risk projects, executives should concentrate on reducing bias if they are to improve the accuracy of project reporting.","Project management,
Computer errors,
Management information systems,
Snow,
Information theory,
Traffic control,
Information technology,
Ethics,
Forward contracts,
Conference management"
A timed automata semantics for real-time UML specifications,"We introduce extensions of the UML class, object and statechart diagrams and define the semantics of the UML extensions by means of extended timed graphs (XTG), a timed automata variant. This approach opens the possibility to specify properties of the UML specifications using the timed computation tree logic. The transformation of the UML-specification into XTG allows the verification the system by model checking using the LPMC model checker tool, which uses XTG as its input language.","Automata,
Unified modeling language,
Real time systems,
Clocks,
Object oriented modeling,
Application software,
Formal verification,
Mathematics,
Computer science,
Tree graphs"
Intelligent systems in biology: why the excitement?,"Biology has rapidly become a data-rich, information-hungry science because of recent massive data generation technologies. Our biological colleagues are designing more clever and informative experiments because of recent advances in molecular science. These experiments and data hold the key to the deepest secrets of biology and medicine, but we cannot fully analyze this data due to the wealth and complexity of the information available. The result is a great need for intelligent systems in biology. There are many opportunities for intelligent systems to help produce knowledge in biology and medicine. Intelligent systems probably helped design the last drug your doctor prescribed, and they were probably involved in some aspect of the last medical care you received. Intelligent computational analysis of the human genome will drive medicine for at least the next half-century. Intelligent systems are working on gene expression data to help understand genetic regulation and ultimately the regulated control of all life processes including cancer, regeneration, and aging. Knowledge bases of metabolic pathways and other biological networks make inferences in systems biology that, for example, let a pharmaceutical program target a pathogen pathway that does not exist in humans, resulting in fewer side effects to patients. Modern intelligent analysis of biological sequences produces the most accurate picture of evolution ever achieved. Knowledge-based empirical approaches currently are the most successful method known for general protein structure prediction. Intelligent literature-access systems exploit a knowledge flow exceeding half a million biomedical articles per year. Machine learning systems exploit heterogenous online databases whose exponential growth mimics Moore's law.","Intelligent systems,
Systems biology,
Computational intelligence,
Learning systems,
Humans,
Data analysis,
Information analysis,
Drugs,
Biology computing,
Genomics"
A neuromorphic paradigm for extrinsically evolved hybrid analog/digital device controllers: initial explorations,"This paper argues that the continuous time recurrent neural network (CTRNN) provides a particularly attractive paradigm for the extrinsic evolution of analog device controllers. The paper begins with a discussion of motivations and difficulties faced in evolving electrical circuits and then illustrates how some of these difficulties have been successfully addressed in the context of evolved CTRNNs. This paper provides a presentation of a new, hardware friendly, CTRNN formulation as well as some preliminary experimental results demonstrating that practical devices can be evolved under the new model. Finally, the paper will conclude with a discussion of open issues and a summary of current plans to close those gaps.","Neuromorphics,
Digital control,
Neurons,
Recurrent neural networks,
Neural networks,
Circuit analysis,
Neural network hardware,
Computer science,
Evolutionary computation,
Genomics"
An improved search algorithm for motion estimation using adaptive search order,"An adaptive search order (ASO) algorithm is presented to speed up the block motion estimation in digital video coding. According to the motion trend, a table of the adaptive search order is defined. For each searching iteration, a better search order is derived and then the best matched block can be found in the early search stage. Some experimental results demonstrate the computational advantage of the proposed improved algorithm when compared to previous algorithms, such as the full search algorithm, the successive elimination algorithm, the block sum pyramid algorithm, and the multilevel successive elimination algorithm.","Motion estimation,
Signal processing algorithms,
Video coding,
Data structures,
Computational complexity,
Councils,
Information management,
Computer science,
Maximum likelihood estimation,
Image segmentation"
A performance comparison of fullband and different subband adaptive equalisers,"We present two different fractionally spaced (FS) equalisers based on subband methods, with the aim of reducing the computational complexity and increasing the convergence rate of a standard fullband FS equaliser. This is achieved by operating in decimated subbands; at a considerably lower update rate and by exploiting the prewhitening effect that a filter bank has on the considerable spectral dynamics of a signal received through a severely distorting channel. The two presented subband structures differ in their level of realising the feedforward and feedback part of the equaliser in the subband domain, with distinct impacts on the updating. Simulation results pinpoint the faster convergence at lower cost for the proposed subband equalisers.","Adaptive equalizers,
Feedback,
Switches,
Convergence,
Filters,
Computational complexity,
Delay,
Least squares approximation,
Eigenvalues and eigenfunctions,
Computer science"
Use of categorization and structuring of messages in order to organize the discussion and reduce information overload in asynchronous textual communication tools,"This paper shows how the use of categorization and structuring of messages in asynchronous textual communication tools could be useful in course delivery via the Internet in order to facilitate the argumentation and to guide the participants to reflect about their messages. Although the use of categorization caused an increase in the total number of messages, there was a reduction of the information overload and an increase in the quality of the discussion. We also show how the message categorization was used in a distance course delivered through the AulaNet environment, showing how we defined and improved the set of categories, and how it helped the students.","Internet,
Software engineering,
Laboratories,
Computer science,
Scattering,
Instruments,
Data mining,
Context,
Organizing,
Writing"
Design and implementation of metadata management system for WWW coursewares,"The number of coursewares available on the Internet has increased greatly. The amount of information we can obtain from the networked environment is enormous. However, the importance of getting right information draws much attention from researchers in the community. Currently, searching techniques are developed to help the learners as well as teachers find information in the World Wide Web. Nevertheless, the users can be confused with search results due to irrelevant information, misinformation, insufficient information, etc. Searching can be performed accurately and efficiently by utilizing metadata for the Web courseware. This paper proposes a metadata management system trying to help search appropriate coursewares and shows that utilizing metadata for a search can facilitate obtaining right information on the Web. Elements of the metadata are selected and their relationships are identified in order to organize them in a relational database. Procedures to obtain the metadata for Web courseware are designed and implemented in the Microsoft ASP (active server pages). Issues on managing the metadata are addressed and implemented in the system so that addition, deletion, and update operations can be done easily via the Web. The overall metadata management system provides user-friendly interfaces to give essential information on the courseware and to handle user enquiry correctly and efficiently.",
Adaptive wavelets based multiresolution modeling of irregular meshes via harmonic maps,"We propose an adaptive wavelets based multiresolution scheme by using harmonic maps for 3D irregular meshes. This approach extends the previous works by M. Eck et al. (see SIGGRAPH '95, p.173-82, 1995) and M. Lounsbery (see ""Multiresolution Analysis for Surfaces of Arbitrary Topological Type"", PhD thesis, Department of Computer Science and Engineering, University of Washington, p.129, 1994) which have been developed for regular triangular mesh subdivision. First, we construct parameterizations of the original mesh that results in a remesh having a subdivision connectivity for the wavelets decomposition. Next, the local subdivision based multiresolution scheme is presented. Our algorithm represents effectively a region of interest or a region having complex and high curvature geometry by using bi-orthogonal wavelets. Through the computer simulation tested on some example meshes, we show that the proposed method is more effective than the previous regular subdivision methods.","Surface waves,
Geometry,
Wavelet analysis,
Mesh generation,
Computer simulation,
Testing,
Computer graphics,
Solid modeling,
Rendering (computer graphics),
Topology"
Characterization of the surface impedance of a superconducting thin film with application to propagation characteristics of surface acoustic waves,"In this paper, an algorithm suitable for the computer aided design (CAD) has been developed to estimate and model the main characteristic parameters of superconducting thin films. The detailed parameters of the superconducting thin film such as the surface impedance are described with the aid of the two-fluid model. An impedance function that varies with the temperature and frequency has been obtained. The attenuation constant has been also computed and presented. The obtained results show that, within a certain range of temperature, the surface impedance exhibits a negative real part. This negative resistance phenomenon is indicative of amplification which may occur if the superconducting thin film is embedded in a proper microwave circuit. It can be seen that, the attenuation strongly depends on the frequency and the dispersion-less behavior of the superconducting thin film can be observed. As an application, the obtained surface impedance of a high Tc superconducting thin film can be used to study the propagation characteristics of a surface acoustic wave (SAW). As a result of low loss and hence negative part, the implementation of high gain surface acoustic wave amplifier becomes possible. The computer simulation results are verified by comparison with results using the surface impedance formula of Mattis-Bardeen theory and show a good agreement.","Surface impedance,
Superconducting thin films,
Surface acoustic waves,
Frequency,
Attenuation,
Surface resistance,
Acoustic waves,
Superconducting microwave devices,
Algorithm design and analysis,
Design automation"
Generating efficient tests for continuous scan,"Conventional scan-based designs spend a lot of testing time in shifting test patterns and output responses, which greatly increases the testing cost. In this paper, we propose a modified approach for scan-based design in which a test is conducted in every clock cycle. This approach may significantly reduce the test application time when appropriate test vectors are applied. We develop algorithms to generate efficient test input for the test environment, and experimental results show that we can achieve high fault coverage with only about 10%-30% of the clock cycles required in conventional scan-based design.","Clocks,
Circuit testing,
Flip-flops,
Design for testability,
Registers,
Permission,
Computer science,
Costs,
Circuit faults,
Algorithm design and analysis"
Fault isolation in a class of nonlinear uncertain input-output systems,"Presents a robust fault isolation scheme for a class of nonlinear systems with unstructured modeling uncertainty and partial state measurement. The proposed fault diagnosis architecture consists of a fault detection and approximation estimator and a bank of isolation estimators. The isolation estimators are activated only if a fault is detected.. Based on the class of nonlinear systems under consideration, a fault isolability condition is rigorously investigated, characterizing the class of nonlinear faults which are isolable by the proposed scheme. Moreover, the non-conservativeness of the fault isolability condition is illustrated by deriving a subclass of nonlinear systems and faults for which this condition is also necessary for fault isolability.","Fault detection,
Nonlinear systems,
Fault diagnosis,
Robustness,
Uncertainty,
Nonlinear dynamical systems,
Estimation error,
Computer science,
Electric variables measurement,
Maintenance"
Probabilistic reasoning about uncertain relations between temporal points,"A wide range of AI applications should manage time varying information. Many published research articles in the area of temporal representation and reasoning assume that temporal data is precise and certain, even though in reality this assumption is often false. However, in many real applications temporal information is imperfect and there is a need to find some way of handling it. An uncertain relation between two temporal points is represented as a vector with three probability values denoting the probabilities of the three basic relations: ""<"" (before), ""="" (at the same time), and "">"" (after). The reasoning mechanism includes inversion, composition, addition, and negation operations. We propose formulas to calculate the probability values within the uncertainty vectors representing the resulting relations of the reasoning operations. We also consider an example of using the proposed representation and reasoning mechanism.","Uncertainty,
Computer science,
Artificial intelligence,
Ontologies,
Management information systems,
Application software,
Information management,
Time varying systems,
Probability,
Natural languages"
Automatic knowledge acquisition from subject matter experts,"This paper presents current results in developing a practical approach, methodology and tool, for the development of knowledge bases and agents by subject matter experts, with limited assistance from knowledge engineers. This approach is based on mixed-initiative reasoning that integrates the complementary knowledge and reasoning styles of a subject matter expert and a learning agent, and on a division of responsibilities for those elements of knowledge engineering for which they have the most aptitude. The approach was evaluated at the US Army War College, demonstrating very good results and a high potential for overcoming the knowledge acquisition bottleneck.","Knowledge acquisition,
Knowledge engineering,
Knowledge based systems,
Problem-solving,
Expert systems,
Engines,
Ontologies,
Automotive engineering,
Laboratories,
Computer science"
Cluster and grid superservers: the dawning experiences in China,,"Application software,
Computer networks,
Grid computing,
Costs,
Internet,
Computer architecture,
High performance computing,
Databases,
Sun,
Cities and towns"
A predicate-based approach to defining visual language syntax,"This paper presents an approach to the specification of visual language syntax. Based on attributed graphs as the notion of abstract syntax, syntactical correctness is specified by a set of predicates over that structure. The proposed technique facilitates the natural embedding of other visual and textual notations, the definition of complex syntactic and static-semantic properties, as well as a precise error diagnosis and localization. An editing environment supporting this technique is briefly discussed.","Computer science,
Australia,
Legged locomotion"
Affine transformations of 3D objects represented with neural networks,"An experiment is conducted to prove that multilayer feed-forward neural networks are capable of representing most classes of 3D objects, used in computer graphics. Furthermore, simple affine transformations were applied on those objects showing that modeling is possible using this type of representation. One network is used per one volumetric description of a 3D object. The neural network employed, is a function that takes as inputs 3D coordinates in object space and produces as output a value that indicates if the point belongs to the object or not. The representation method is tested by repeated evaluations of the network for points inside the object space. Objects that have a simple analytical form, e.g. a sphere or a cube, are represented by specifying the networks' parameters manually. For objects with more complicated shapes we generate training examples. These training examples consist of points on the objects' surface and points lying on inclosed and enclosing surfaces. The algorithm for generating the training data, is a simple heuristic that uses the surface normal to determine whether a point in the vicinity of the surface belongs to the inside or the outside of the object. The network is finally trained on the generated examples, using the back propagation technique. The experimental results prove that this representation method is accurate and compact. Feedforward neural networks being hardware implementable offer the ability for a faster representation. This paper is the second step, on a series of ideas, towards creating a real time 3D renderer based entirely on neural networks.","Neural networks,
Feedforward neural networks,
Computer graphics,
Shape,
Multi-layer neural network,
Testing,
Computer science,
Training data,
Neural network hardware,
Solids"
Cross-checking and good scores go together: students shrug,"Students can acquire greater confidence in their work by cross-checking their answers, but it seems that they seldom do so. Based on the premise that engineering instructors can and should be training students to acquire this skill as a habit, this paper reports on a one-semester experiment designed to provide substantial motivation for cross-checking and to measure how frequently students did so. In addition, evidence was gathered that establishes a positive correlation between good scores and crosschecking. Alas, despite continued emphasis throughout the term and updates on the correlation between cross-checking and good scores, the number of students who attempted to check their work remained remarkably low throughout the semester. The authors' results establish that time pressure is not a dominant reason that students do not check their answers. Implications of this puzzling resistance to (or apathy towards) cross-checking, possible reasons for it, and suggestions for combating it are discussed.","Circuit analysis,
Engineering students,
Design engineering,
Calculators,
Frequency,
Performance evaluation,
Current measurement,
Electrical engineering computing,
Computer science"
Preference for procedural ordering in distributed groups: how do media and repeated interaction affect perceptions and procedural structuring?,"Groups use procedural structures to organize their efforts in meetings. These structures are affected by group members' preferences for the degree of procedural order they want in a meeting, as well as by the communication media available in the meeting environment. Analysis of thirty partially distributed experimental groups that met over a series of four sessions was conducted by using two methods. Questionnaires were administered to ascertain perceptions of satisfaction and procedural practices. Content analysis was used to determine actual procedural behavioral patterns. Results indicate that members' preferences for procedural order do appear to affect the patterns of their actual structuring behaviors, but do not affect their satisfaction with the group process.","Decision making,
Pattern analysis,
Guidelines,
Collaborative software,
Distributed computing,
Bandwidth,
Feedback,
Context,
Computer mediated communication,
Information processing"
"Inexpensive throughput enhancement in small-scale embedded microprocessors with block multithreading: extensions, characterization, and tradeoffs","This paper examines differential multithreading (DMT) as an attractive organization for coping with pipeline stalls in small-scale processors like those used in embedded environments. The paper proposes extensions to block multithreading to cope with data- and instruction-cache misses, and then explores some of the design tradeoffs that this enables. Results show that DMT boosts throughput substantially and can in fact replace dynamic branch prediction or data forwarding, or can be used to reduce the sizes of the instruction and data caches. Block multithreading, described by Farrens and Pleszkun (1991), is a technique to achieve high throughput from a single-issue microarchitecture by switching among multiple instruction streams in response to pipeline stalls. Although single-issue organizations are no longer used in high-performance processors, they remain common even in newly-designed processors for small-scale, embedded devices. Like the original description of block multithreading, DMT uses auxiliary pipeline registers to save the state of in-flight instructions. By coping with data- and instruction-cache misses, however, our implementation can attack all the major sources of pipeline stalls. Overall, we find that DMT can substantially lower the cost and complexity of microprocessors for embedded environments, especially environments for which throughput rather than speed is the primary concern. In addition, DMT is an attractive prospect for use in chip-multiprocessing environments.","Throughput,
Microprocessors,
Multithreading,
Pipelines,
OFDM modulation,
Switches,
Yarn,
Hardware,
Games,
Computer science"
Requirements for maintaining Web access for hearing-impaired individuals,"Current textual and graphical interfaces to computing, including the Web, are a dream come true for the hearing impaired. However, improved technology for voice and audio interfaces threaten to end this dream. Requirements are identified for continued access to computing for the hearing impaired. Consideration is also given to improving access to the sight impaired.","Telephony,
Auditory system,
Computer interfaces,
Electronic mail,
Lips,
Computer science,
Motion pictures,
Magnetic heads,
Teleprinting,
TV"
Tight coupling of timing-driven placement and retiming,"Retiming is a widely investigated technique for performance optimization. In general, it performs extensive modifications on a circuit netlist, leaving it unclear whether the achieved performance improvement will still be valid after placement has been performed. This paper presents an approach for integrating retiming into a timing-driven placement environment. The experimental results show the benefit of the proposed approach on circuit performance in comparison with design flows using retiming only as a pre- or post-placement optimization method.","Registers,
Delay,
CMOS technology,
Wires,
Minimization methods,
Circuit testing,
Timing,
Semiconductor device modeling,
Coupling circuits,
Computer science"
Pivot selection techniques for proximity searching in metric spaces,"With a few exceptions, proximity search algorithms in metric spaces based on the use of pivots select them at random among the elements of the metric space. However, it is well-known that the way in which the pivots are selected can affect the performance of the algorithm. Between two sets of pivots of the same size, better-chosen pivots can reduce the search time. Alternatively, a better-chosen small set of pivots (requiring less space) can yield the same efficiency as a larger, randomly chosen set. We propose an efficiency measure to compare two pivot sets, combined with an optimization technique that allows selecting good sets of pivots. We obtain abundant empirical evidence showing that our technique is effective. We also show that good pivots are outliers, but that selecting outliers does not ensure that good pivots are selected.","Extraterrestrial measurements,
Information retrieval,
Euclidean distance,
Indexes,
Biology computing,
Computer applications,
Multimedia databases,
Machine learning,
Quantization,
Image coding"
A prototype for an agent-based secure electronic marketplace including reputation tracking mechanisms,"The future of electronic commerce will be shaped by open, heterogeneous and complex structures, consisting of networked marketplaces. Software agents will interact and negotiate on behalf of their human (or organizational) principals. Principals will be able to implement fraudulent strategies in their agents, which cannot be countered by technical security alone. In the absence of a single correctional institution, agents will have to rely on social mechanisms for assessing reliability and reputation of other, unknown agents. The multi-agent system AVALANCHE is a prototype for an agent based secure electronic commerce marketplace environment. The reputation tracking mechanism, which is implemented in AVALANCHE's software agents, evaluates transaction behavior and influences partner selection and negotiation strategy in future transactions, while protecting the privacy of the participants. The successful result is an increasing expulsion of fraudulent agents from the market.","Prototypes,
Consumer electronics,
Software agents,
Humans,
Tellurium,
Business,
Computer science,
Protection,
Read only memory,
Electronic commerce"
Robust real-time 3D trajectory tracking algorithms for visual tracking using weak perspective projection,"In this paper, motion estimation algorithms for the most general tracking situation are developed. The proposed motion estimation algorithms are used to predict the location of target and then to generate a feasible control input so as to keep the target stationary in the center of the image. The work differs from the previous algorithm of motion estimation in that it is capable of decoupling the estimation of motion from the estimation of structure. The weak perspective projection is used to solve this problem. The modified optical flow is first calculated and then fed to motion estimation algorithms so as to generate an appropriate camera motion that achieves tracking. The important contribution of this work is that simple, numerically stable, none computation intensive, and correspondence-free 3D motion estimation algorithms are derived. A visual tracking system can be easily implemented and run in real-time due to the simplicity of the proposed algorithms and thus increases their efficiency. The robustness and feasibility of the proposed algorithms has been validated by a number of experiments.","Robustness,
Target tracking,
Motion estimation,
Cameras,
Trajectory,
Adaptive control,
Computer science,
Motion control,
Image motion analysis,
Real time systems"
"Advanced training for crisis decision making: simulation, critiquing, and immersive interfaces","Crises demand swift and effective decision making. Yet crises entail unique characteristics that hinder training of personnel with the process knowledge necessary to achieve these two goals. First, crises are, by definition, rare; thus, it is usually not possible for humans to acquire decision-making expertise directly through experience in natural settings. Second, managing crises often involves dealing with massive uncertainty and complexity under conditions of acute stress. Each of these features poses a unique challenge to training. We present an example of a trainer for ship damage control that addresses these challenges. It consists of a first-principles simulator that generates large numbers of realistic scenarios, an immersive multimedia interface that helps replicate decision-making information overload, and a critiquing expert system that provides real-time and post-session feedback on human decision-making performance. Experimental results are presented that indicate that the described computer-based trainer has psychological realism from the standpoint of allowing a trainee to practice decision making processes while under a high level of stress.","Decision making,
Management training,
Humans,
Personnel,
Crisis management,
Uncertainty,
Stress,
Marine vehicles,
Computational modeling,
Multimedia systems"
Proving secrecy is easy enough,,"Cryptographic protocols,
Authentication,
Safety,
Logic,
Computer science,
Laboratories,
Benchmark testing,
Debugging,
Automation,
Jacobian matrices"
Enhancing BIST quality of sequential machines through degree-of-freedom analysis,"Designing a BIST structure for sequential circuits is rather a complex problem as some states remain unreachable and some act as the sink under any input sequence. This paper reports an efficient scheme to provide uniform mobility, referred to as degree of freedom, in a sequential machine by enhancing the reachability as well as the emittability of the states. The uniform mobility of states ensures higher fault efficiency in a BIST structure of the circuit. Moreover, as a non-scan scheme, the technique provides lower test application time and at-speed testing.","Built-in self-test,
Circuit testing,
Sequential circuits,
Circuit faults,
Strontium,
Computer science,
Automatic testing,
Flip-flops,
Test pattern generators,
Sequential analysis"
An Adaptive Multiresolution Method for Progressive Model Transmission,"Although there are many adaptive (or view-dependent) multiresolution methods, support for progressive transmission and reconstruction has not been addressed. A major reason for this is that most of these methods require a large portion of the hierarchical data structure to be available at the client before rendering starts. This is due to the dependency constraints among neighboring vertices. In this paper, we present an efficient, adaptive, multiresolution method that allows progressive and selective model transmission. It is achieved by reducing the neighboring dependency to a minimum. The new method allows visually important parts of an object to be transmitted to the client at higher priority than the less important parts, and progressively reconstructed there for display. It is even possible to transmit only the visible parts of a model and reconstruct these visible parts at the client. The ability to selectively transmit allows the visualization of very large models across the network with minimal delay. We will present how our method works in a client-server environment. We will also show the data structure of the transmission record and some performance results of the method.",
Extending the product family approach to support n-dimensional and hierarchical product lines,"The software product-line approach (for software product families) is one of the success stories of software reuse. When applied, it can result in cost savings and increases in productivity. In addition, in safety-critical systems the approach has the potential for reuse of analysis and testing results, which can lead to safer systems. Nevertheless, there are times when it seems like a product family approach should work when, in fact, there are difficulties in properly defining the boundaries of the product family. The authors draw on their experiences in applying the software product-line approach to a family of mobile robots as well as case studies done by others to: (1) illustrate how domain structure can currently limit applicability of product-line approaches to certain domains, and (2) demonstrate our initial progress towards a solution using a set-theoretic approach to reason about domains of what we call n-dimensional and hierarchical product families.","Costs,
Productivity,
Mobile robots,
Computer science,
System testing,
Programming profession,
Tree data structures"
Providing QoS guarantees for unicast/multicast traffic with fixed/variable-length packets in multiple input-queued switches,"With a deep understanding on the properties of stable matching in the context of multiple input-queued switches, we propose the efficient schemes to guarantee the QoS of any unicast and multicast traffic with fixed- or variable-length packets in an unified way. Using these schemes, the QoS of fixed- or variable-length unicast and multicast packets can be guaranteed by independently employing suitable service disciplines at the packets' destined outputs like what is being done in an output queueing switch. One of our proposed schemes uses totally 4N input/output buffers, each with an internal speed-up of 2 independent of N, for an N/spl times/N switch to support any fixed- or variable-length multicast and unicast packets with QoS guarantees.","Unicast,
Switches,
Packet switching,
Traffic control,
Scheduling,
Poles and towers,
Computer science,
Water,
Scalability,
Delay"
The design of pedagogical agent for distance virtual experiment,"Experimental learning about electric-machinery is high risk and time-consuming, students do not always enjoy the laboratory, the teacher also needs to pay full attention to all students and equipment, besides it is hard to provide enough machine sets and space for learning. So it is our motivation to design a virtual laboratory for the teaching of electric-machinery experiment by Internet under the more real environment without concern about danger or limitations. It is taught by an experienced teacher using a pedagogical agent which is the most important key factor. Taking the above features into consideration, an interactive virtual laboratory based on an expert system has been designed and implemented to improve the learning, operation and control in electric-machinery experiments. The system set up is a highly intelligent and interactive mechanism. Through the system, students could have a more complete environment for distance learning.","Virtual reality,
Expert systems,
Computer aided instruction,
Education,
Laboratories,
Graphical user interfaces,
Machinery,
Internet,
Machine learning,
Control systems"
Word-based block-sorting text compression,"Block sorting is an innovative compression mechanism introduced in by M. Burrows and D.J. Wheeler (1994). It involves three steps: permuting the input one block at a time through the use of the Burrows-Wheeler transform (BWT); applying a move-to-front (MTF) transform to each of the permuted blocks; and then entropy coding the output with a Huffman or arithmetic coder. Until now, block-sorting implementations have assumed that the input message is a sequence of characters. In this paper, we extend the block-sorting mechanism to word-based models. We also consider other transformations as an alternative to MTF, and are able to show improved compression results compared to MTF. For large text files, the combination of word-based modelling, BWT and MTF-like transformations allows excellent compression effectiveness to be attained within reasonable resource costs.","Dictionaries,
Entropy coding,
Arithmetic,
Costs,
Computer science,
Software engineering,
Decoding,
Frequency,
Tree data structures"
The effect of timeout prediction and selection on wide area collective operations,"Failure identification is a fundamental operation concerning exceptional conditions that network programs must be able to perform. In this paper, we explore the use of timeouts to perform failure identification at the application level. We evaluate the use of static timeouts and of dynamic timeouts based on forecasts using the Network Weather Service. For this evaluation, we perform experiments on a wide-area collection of 31 machines distributed in eight institutions. Though the conclusions are limited to the collection of machines used, we observe that a single static timeout is not reasonable, even for a collection of similar machines over time. Dynamic timeouts perform roughly as well as the best static timeouts and, more importantly, they provide a single methodology for timeout determination that should be effective for wide-area applications.","Sockets,
Weather forecasting,
Computer science,
High performance computing,
Computer networks,
Performance evaluation,
Grid computing,
Fault diagnosis,
Software packages,
Software libraries"
Watermarking graph partitioning solutions,"Trends in the semiconductor industry towards extensive design and code reuse motivate a need for adequate intellectual property protection (IPP) schemes. We offer a new general IPP scheme called constraint-based watermarking and analyze it in the context of the graph partitioning problem. Graph partitioning is a critical optimization problem that has many applications, particularly in the semiconductor design process. Our IPP technique for graph partitioning watermarks solutions to graph partitioning problems so that they carry an author's signature. Our technique is transparent to the actual CAD tool which does the partitioning. Our technique produces solutions that have very low quality degradation levels, yet carry signatures that are convincingly unambiguous, extremely unlikely to be present by coincidence, and difficult to detect or remove without completely resolving the partitioning problem.","Watermarking,
Design optimization,
Process design,
Permission,
Computer science,
Electronics industry,
Intellectual property,
Protection,
Design automation,
Degradation"
Providing heterogeneous multicast services with AMnet,"AMnet is a framework for flexible and rapid service creation and provides primarily heterogeneous user-tailored group communication services. AMnet is based on Active and Programmable Networking technologies and uses active nodes (AMnodes) located within the network to execute so-called service modules for the provision of individual group communication services. It is, however, not limited to group communication since it provides a rather generic framework. The AMnode design is shaped for the efficient execution of flexible loadable service modules, including dedicated hardware support.","Receivers,
Protocols,
Quality of service,
Error correction,
Java,
Measurement,
IP networks"
Optimal Smoothing in Visual Motion Perception,"When a flash is aligned with a moving object, subjects perceive the flash to lag behind the moving object. Two different models have been proposed to explain this flash-lag effect. In the motion extrapolation model, the visual system extrapolates the location of the moving object to counteract neural propagation delays, whereas in the latency difference model, it is hypothesized that moving objects are processed and perceived more quickly than flashed objects. However, recent psychophysical experiments suggest that neither of these interpretations is feasible (Eagleman & Sejnowski, 2000a, 2000b, 2000c), hypothesizing instead that the visual system uses data from the future of an event before committing to an interpretation. We formalize this idea in terms of the statistical framework of optimal smoothing and show that a model based on smoothing accounts for the shape of psychometric curves from a flash-lag experiment involving random reversals of motion direction. The smoothing model demonstrates how the visual system may enhance perceptual accuracy by relying not only on data from the past but also on data collected from the immediate future of an event.",
Dynamic location management with variable size location areas,"This paper introduces a dynamic location management algorithm with variable size location areas (LAs), which helps in reducing location update (LU) cost and hence total location management cost. Basically there are two types of location management strategies - Static and Dynamic. In the static strategy (used in GSM), LAs consist of static and arbitrarily defined collections of cells, which do not take into account individual subscriber's mobility pattern and hence these LAs remain same for all the subscribers. Since mobility pattern of an individual subscriber may differ significantly, this approach is far from optimal. Proposed location management algorithm uses the mobility history of individual subscribers to dynamically form individualized LAs based on his previous movements from cell to cell. It defines the size of LAs based on subscriber's speed and call arrival probability. An activity-based mobility model is developed to test the proposed algorithm and the performance of this algorithm is compared with other algorithms, like fixed location strategy, grid-based location management strategy and dynamic location management algorithm. Overall, the proposed algorithm incurred significantly lower location management cost, in terms of signaling messages generated, as compared to all other algorithms.","Paging strategies,
Costs,
Heuristic algorithms,
Cellular networks,
Base stations,
Resource management,
Land mobile radio cellular systems,
Mobile radio mobility management,
History,
Computer science"
Engineering design for software: on defining the software engineering profession,"Since the mid-1980s, software engineering has been accepted as a formal field of study in academia. Software engineering education is maturing from specialized courses in computer science, to numerous Master's programs, and more recently to the advent of undergraduate as well as PhD programs. What is new today is the widespread impetus from many fronts to consider software development as engineering profession. The notion of whether software development is engineering can be answered in a number of ways. In this paper, the authors look at generally accepted definitions of engineering and show their correspondence or applicability to software development. They demonstrate through a detailed analysis how prominent features that cut across all engineering disciplines are found in software engineering as well. They conclude with a discussion of the educational implications.","Design engineering,
Software design,
Software engineering,
Programming,
Computer science education,
Educational programs,
Computer science,
USA Councils,
Certification,
Engineering profession"
Dynamic Bayesian framework for extracting temporal structure in video,"In this paper, we develop the concept of descriptors based on perceptual-level motion features such as time-to-collision, shot transition and temporal motion and it is shown that by including them the representational level of the video classes is significantly enhanced, e.g. violence could be detected. The temporal context cues, which had been largely neglected by present content-based retrieval (CBR) systems, are integrated into the framework. A dynamic Bayesian framework for the CBR systems which can learn the temporal structure through the fusion of all the features is designed The experimental results for more than 4 hours of videos are presented for a number of key applications like sequence identifier, highlight extraction for sports, and detecting climax or violence.",
Localist Attractor Networks,"Attractor networks, which map an input space to a discrete output space, are useful for pattern completioncleaning up noisy or missing input features. However, designing a net to have a given set of attractors is notoriously tricky; training procedures are CPU intensive and often produce spurious attractors and ill-conditioned attractor basins. These difficulties occur because each connection in the network participates in the encoding of multiple attractors. We describe an alternative formulation of attractor networks in which the encoding of knowledge is local, not distributed. Although localist attractor networks have similar dynamics to their distributed counterparts, they are much easier to work with and interpret. We propose a statistical formulation of localist attractor net dynamics, which yields a convergence proof and a mathematical interpretation of model parameters. We present simulation experiments that explore the behavior of localist attractor networks, showing that they yield few spurious attractors, and they readily exhibit two desirable properties of psychological and neurobiological models: priming (faster convergence to an attractor if the attractor has been recently visited) and gang effects (in which the presence of an attractor enhances the attractor basins of neighboring attractors).",
Knowledge management in mobile CSCW: evaluation results of a mobile physical/virtual meeting support system,"We have developed and evaluated RoamWare, a mobile physical/virtual meeting support system that is intended to support knowledge management (KM) in mobile CSCW (computer-supported cooperative work). The design of RoamWare was based on a previous empirical study of knowledge management and mobility at Telia Na/spl uml/ra (R. Lindgren et al., 2000). The evaluation of RoamWare was done in two ways: (1) by exercises and focus groups, and (2) by demonstrations and interviews. The main objective of this research was to explore two issues: (1) the use of the RoamWare system to support knowledge management, and (2) the way in which RoamWare could be integrated in everyday work. The paper ends by highlighting important aspects of knowledge management of particular concern when designing mobile CSCW systems, as well as summing up the impact of activity checklists as an analytical tool and the strengths of using the Umea approach to guide the design of knowledge management systems to support mobile CSCW.","Knowledge management,
Mobile computing,
Collaborative work,
Mobile communication,
Informatics,
Physics computing,
Computer mediated communication,
Libraries,
Collaborative software,
Meetings"
Design of security system based on immune system,"The authors design a network security system using an analogy of natural world immunology. We apply not only the immune mechanism that recognizes self or non-self, but also cooperation among immune cells of the system. This system defines an immune cell as one agent based on our multiagent language, which is an extension of concurrent logic programming languages. These agents can detect and reject intrusion by cooperating with each other.","Immune system,
Intrusion detection,
Information security,
Computer networks,
Protection,
Communication system security,
Logic programming,
Computer security,
IP networks,
Information analysis"
Performance analysis of dynamic soft real-time systems,"Soft real-time processing is real-time processing in which some or all applications are allowed to miss deadlines, particularly in situations of system overload. Resource allocation decisions in such systems are often based in part on specifications of application utility or benefit and directly affect which applications will miss deadlines and by how much. A feedback mechanism is often provided to inform the applications of their current resource allocations, allowing them to modify their processing to provide the best possible performance. These characteristics result in a situation in which traditional measures of performance, both general-purpose and real-time, can provide incomplete or inaccurate measures of system and application performance. We examine these issues and shortcomings, identify performance factors that need to be measured to address them, and propose specific metrics to quantify these performance factors. We conclude with results from the application of these metrics to our QoS level soft real-time system.","Performance analysis,
Real time systems,
Resource management,
Application software,
Time measurement,
Computer science,
Feedback,
Delay,
Jitter,
Degradation"
The boundaries of scientific culture in virtual heritage,"It has become commonplace to say that science now contributes to global culture by applying its discipline to projects involving computer reconstructions of elements of humanity's cultural heritage. These words imply that science can and will effectively catapult the treasures of the past, along with the ideas they express and the ideals on which they were founded, accurately into the future in whole form. This paper explores the effect that science, as a system of belief has on the conveyance of ideas and information relating to cultural heritage. First, it identifies boundaries created by scientific methods to the delivery of traditional ideas and beliefs. Then follows an historical review that demonstrates the evolution of these boundaries. Then follows a number of examples that show the limits imposed, on virtual heritage projects that adhere to scientific methods. The rest of the essay focuses on approaches to using science effectively in the service of virtual heritage project development without being bound by scientific culture. This also includes case studies of a number of actual virtual heritage projects that have taken such approaches which subordinate science to a practical level in order to accomplish specific cultural goals which would have been unachievable by adhering rigorously to scientific methods. It further argues that virtual heritage studies must evolve, for the sake of human culture, in ways which will ensure that scientific biases and prejudices will not become a bottleneck beyond which traditional cultures may become unable to reach the future in living form. The paper concludes with a visualization of virtual heritage studies as field which is developing, changing, and becoming more meaningful in itself as a culture informed by many systems of belief.","Cultural differences,
Humans,
Art,
Visualization,
Writing,
Multimedia systems,
Laboratories,
Immune system"
The real options approach to standardization,"In this paper we propose a new model of technology standardization under market uncertainty and show how its value is quantifiable using the theory of real options. Our options-based approach to standardization shows that a rational way to standardize some IT technology in uncertain markets is with correct structure and proper staging of the standard. First, highly modularized standards provide a higher option value because of the ability to pick and choose the best modules to change at a fine granularity. Secondly, a modular structure that promotes easy and non-disruptive parallel experimentation (such as end-2-end applications) enhances the option value by providing a larger field of options from which to select. Lastly, allowing the standard to evolve along with the customers' expectations of the technology is a good strategy to match standards with uncertain user markets.",
Time-parallel algorithms for simulation of multiple access protocols,"We present time-parallel algorithms for parallel simulation of multiple access protocols for medium access-in particular slotted Aloha and slotted p-persistent CSMA. Two mechanisms are presented-regeneration point-based and fix up-based. Aloha is simulated using both mechanisms and CSMA is simulated using only the first mechanism. An analytical technique is developed to predict speedup for the regeneration point-based scheme for slotted Aloha. Speedup values obtained from the analytical technique are found to be in good agreement with those obtained from simulations. In general, it is observed that any mechanism that reduces the number of backlogged packets has good parallel performance regardless of the protocol simulated or the mechanism used to parallelize the simulation.","Access protocols,
Discrete event simulation,
Multiaccess communication,
Checkpointing,
Computational modeling,
Computer simulation,
Computer science,
Analytical models,
Large-scale systems,
Wireless application protocol"
Combination of vector quantization and hidden Markov models for Arabic speech recognition,"We present experiments performed to recognize isolated Arabic words. Our recognition system is based on a combination of the vector quantization technique at the acoustic level and Markovian modelling. Hidden Markov models (HMMs) are widely used in a number of practical applications and are especially suitable in speech recognition because of their ability to handle variability of the speech signal. In our system, a word is analysed and represented as a set of acoustic vectors, then transformed into a symbolic sequence using the vector quantizer. This observation sequence is compared to reference Markov models. The word associated with the model obtaining the highest score is declared to be the recognized word.","Vector quantization,
Hidden Markov models,
Speech recognition,
Feature extraction,
Testing,
Signal processing,
Computer science,
Artificial intelligence,
Cepstral analysis,
Signal analysis"
VLIW scheduling for energy and performance,"We present and evaluate several instruction scheduling algorithms that reorder a given sequence of instructions taking into account the energy considerations. We first compare a performance oriented scheduling technique with three energy-oriented instruction scheduling algorithms from both performance (execution cycles of the resulting schedules) and energy consumption points of view. Then, we propose scheduling algorithms that consider energy and performance at the same time. The results obtained using randomly generated directed acyclic graphs show that these techniques are quite successful in reducing energy consumption and their performance (in terms of execution cycles) is comparable to that of a pure performance-oriented scheduling.","VLIW,
Energy consumption,
Processor scheduling,
Scheduling algorithm,
Optimizing compilers,
Computer science,
Power engineering and energy,
Software performance,
Computer architecture,
Operating systems"
Learning achievement evaluation strategy using fuzzy membership function,"In this paper, the authors suggest a new learning achievement evaluation strategy in student's learning procedure. They call this fuzzy evaluation. They may assign fuzzy lingual variables to each question pertaining to its importance, complexity and difficulty by using fuzzy membership functions. Then one can evaluate a score depending on the membership degree of uncertainty factors in each question. In addition, they consider the time consuming element for solving a question. They adapt an inverse sigmoid function to consider time consuming elements, fuzzy concentration and dilation function for importance, a sigmoid function for complexity, and fuzzy square method for difficulty.",
Quantum algorithmic entropy,"Extends algorithmic information theory to quantum mechanics, taking a universal semi-computable density matrix (""universal probability"") as a starting point, and defines complexity (an operator) as its negative logarithm. A number of properties of Kolmogorov complexity extend naturally to the new domain. Approximately, a quantum state is simple if it is within a small distance from a low-dimensional subspace of low Kolmogorov complexity. The von-Neumann entropy of a computable density matrix is within an additive constant from the average complexity. Some of the theory of randomness translates to the new domain. We explore the relations of the new quantity to the quantum Kolmogorov complexity defined by P.M.B. Vita/spl acute/nyi (1999) (we show that the latter is sometimes as large as 2n-2 log n) and the qubit complexity defined by A. Berthiaume et al. (2000). The ""cloning"" properties of our complexity measure are similar to those of qubit complexity.","Entropy,
Quantum computing,
Quantum mechanics,
Information theory,
Hilbert space,
Computer science,
Additives,
Cryptography,
Convergence,
Density measurement"
Reliable multicast protocol applied local FEC,"One of the most important technical problems in reliable multicast protocols is reducing redundant NAKs (negative acknowledgements) to avoid NAK implosion. A number of NAK suppression mechanisms have been proposed to deal with this problem. In MBONE, which is a virtual multicast network and makes multi-point communication across the Internet feasible, the source link, the links directly connected to or very close to the source, contributes 5% packet loss. In the NAK suppression mechanism, in the case of such a loss, the source link loss, all receivers suffer the same packet loss and the NAK suppression mechanism does not work effectively. In this paper we propose a reliable multicast protocol applied local FEC, called the local FEC, where the source link loss is recovered with the FEC applied locally only to the source link. To investigate the performance of the local FEC, it is compared with the reliable multicast protocol with a conventional NAK suppression mechanism. Our simulation results show that the local FEC outperforms the NAK suppression protocol from the view point of scalability and wasted network bandwidth.","Multicast protocols,
Forward error correction,
Telecommunication network reliability,
Scalability,
Multicast communication,
Communication system control,
Reliability engineering,
Information science,
Feedback,
Information systems"
Group consistency for read-only transactions in mobile environments,,
RSVP-SQOS : a secure RSVP protocol,,"Quality of service,
Multicast protocols,
Protection,
Authentication,
Computer science,
Unicast,
Bandwidth,
Scalability,
Application software,
Delay"
Using CAD drawings for robot navigation,"Exploration and navigation of an environment by a robot usually involves the steps of mapping, localization and path planning. Here we look at the problem of navigation in an environment about which the robot has some a priori information available, namely in the form of an architectural CAD drawing. The CAD drawing is utilized to obtain: (i) a topological map of the environment which is used for large scale path planning between regions in the environment and (ii) a skeleton of each region in the environment for path planning within regions. We then propose that a hierarchy of representations consisting of the topological map, skeleton and a reactive hazard-avoidance control system can be used effectively for navigation and exploration by a robot.",
A virtual laboratory for introductory electrical engineering courses to increase the student performance,"The increasing power and availability of software and hardware on personal computers has already begun to revolutionize the way engineering subjects are taught and learned. This paper describes a computer based laboratory, called Virtual Laboratory, to assist and advance learning endeavors in electrical engineering programs at Monterrey Tech, USA. The authors focus their initial efforts on their introductory circuit analysis and electronic courses, which are taken by undergraduate engineering students of several programs.","Laboratories,
Instruments,
Power engineering and energy,
Data acquisition,
Hardware,
Power engineering computing,
Circuit analysis,
Virtual environment,
Electrical engineering,
Engineering students"
A priori algorithm for sub-category classification analysis of handwriting,"The sub-category classification problem is that of discriminating a pattern to all sub-categories. Not surprisingly, sub-category classification performance estimates are useful information to mine as many researchers are interested in any trend of pattern in specific sub-category. This paper presents a datamining technique to mine a database consisting of experimental and observational unit variables. Experimental unit variables are those attributes which make sub-categories of the entity, e.g., demographic data and observational unit variables are features observed to classify the entity, e.g., test results or handwriting styles, etc. Since there are an enormously large number of subcategories based on the experimental unit variables, we apply the a priori algorithm to select only sub-categories that have enough support among all possible ones in a given database. Those selected sub-categories are then discriminated using observational unit variables as input features to the Artificial Neural Network (ANN) classifier. The importance of this paper is twofold. First, we propose an algorithm that quickly selects all sub-categories that have enough both support and classification rate. Second, we successfully applied the proposed algorithm to the field of handwriting analysis. The task is to determine similarity of handwriting style of a specific group of people. Document examiners are interested in trends in the handwriting of specific groups, e.g., (i) does a male write differently from a female? (ii) can we tell the difference in handwriting of age group between 25 and 45 from others?, etc. Subgroups of white males in the age group 15-24 and white females in the age group 45-64 show 87 % correct classification performance.","Classification algorithms,
Algorithm design and analysis,
Spatial databases,
Demography,
Artificial neural networks,
Computer science,
Information systems,
Testing,
Data mining,
Information analysis"
Cache characterization surfaces and predicting workload miss rates,"In this paper, we use locality surfaces to predict cache miss rates. To do this, we introduce two new surfaces. The miss surface characterizes how a trace is filtered by a particular cache in terms of locality. A cache characterization surface helps us examine caches in terms of what stride/delay relationships are likely to cause misses in the cache. The cache characterization surface is independent of any workload. We use these surfaces to quantitatively predict cache miss rates with some degree of accuracy.","Computer science,
Displays,
Delay effects,
Information filtering,
Information filters"
Time-space tradeoffs in the counting hierarchy,"Extends the lower-bound techniques of L. Fortnow (2000) to the unbounded-error probabilistic model. A key step in the argument is a generalization of V.A. Nepomnjas/spl caron/c/spl caron/ii/spl breve/'s (1970) theorem from the Boolean setting to the arithmetic setting. This generalization is made possible due to the recent discovery of logspace-uniform TC/sup 0/ circuits for iterated multiplication (A. Chiu et al., 2000). As an example of the sort of lower bounds that we obtain, we show that MAJ-MAJSAT is not contained in PrTiSp(n/sup 1+o(1)/, n/sup /spl epsiv//) for any /spl epsiv/<1. We also extend one of Fortnow's lower bounds, from showing that S~A~T~ does not have uniform NC/sup 1/ circuits of size n/sup 1+o(1)/, to a similar result for SAC/sup 1/ circuits.","Circuits,
Computer science,
Arithmetic,
Automation,
Binary decision diagrams,
Polynomials,
Complexity theory"
Using XML to semi-automatically derive user interfaces,"Today, more and more data is being stored electronically, requiring systems to be able to save and retrieve large amounts of data efficiently. At the same time, user interfaces have to satisfy specific quality-of-service requirements. For example, in order to have a multimedia digital library supporting three different kinds of output devices (17"" screens, PDAs, and mobile phones), each user interface has to be programmed separately according to its representation capabilities. This means that if the output devices are becoming more advanced, e.g., are colored or have higher resolutions, the interface parameters will change and, in many cases, the quality-of-service requirements too. This results in the user interfaces having to be amended or sometimes completely rewritten. The same applies when additional devices need to be supported, and they subsequently require new user interfaces to be built from scratch. Obviously, this is time-consuming and costly. The authors present an approach using XML as the basic technology to semiautomatically derive any kind of user interface for data intensive systems.",
Year,,
The canonical functional design based on the domination-relationship among data,"We study the problem of creating a functional design from a dataflow diagram D. We use the domination-relationship on data-items in D to obtain a canonical function calling-scheme S(D) which is optimal in that it uses the minimum number of global variables for the interface among functions, while keeping the function parameters to a minimum. The difficulty of determining a function calling-scheme that is both valid and optimal is because the number of valid calling-schemes is exponentially large in the size of D. We also use S(D) to obtain a decomposition of D into larger single-output function-blocks. In previous work we give an algorithm to generate the basic pseudocode for each function, including its interface, for the calling-scheme S(D).","Algorithm design and analysis,
Partitioning algorithms,
Computer science,
Software design,
Software algorithms,
Process design,
Design methodology"
Fast and scalable parallel algorithms for matrix chain product and matrix powers on distributed memory systems,"Given N matrices A/sub 1/, A/sub 2/,..., A/sub N/ of size N/spl times/N, the matrix chain product problem is to compute A/sub 1//spl times/A/sub 2//spl times//spl middot//spl middot//spl middot/A/sub N/. Given an N/spl times/N matrix A, the matrix powers problem is to calculate the first N powers of A, i.e., A, A/sup 2/A/sup 3/,..., A/sup N/. We consider distributed memory systems (DMS) with p processors that can support one-to-one communications in O(T(p)) time. Assume that the time complexity of the best known sequential algorithm for matrix multiplication is O(N/sup /spl alpha//), where /spl alpha/<2.3755. Let p be arbitrarily chosen in the range 1/spl les/p/spl les/N/sup /spl alpha/+1//log N. We show that the two problems can be solved on a p-processor DMS in T/sub chain/(N,p)=O(N/sup /spl alpha/+1//p+T(p)(N/sup 2(1+1//spl alpha/)//p/sup 2// /sup /spl alpha//(log p/N)/sup 1-2//spl alpha//+log(p log N/N/sup /spl alpha//) log N)) and T/sub power/(N,p)=0(N/sup /spl alpha/+1//p+T(p)(N/sup 2(1+1//spl alpha/)//p/sup 2// /sup /spl alpha//(log p/log N)/sup 1-2//spl alpha//+(log N)/sup 2/)) times, respectively. We also give instantiation of the above results in distributed memory parallel computers and DMS with hypercubic networks, and show that our parallel algorithms are either fully scalable or highly scalable.","Parallel algorithms,
Concurrent computing,
Distributed computing,
Computer networks,
Computer science,
Polynomials,
Graph theory,
Phase change random access memory,
High performance computing,
Equations"
Nonlinear population dynamics in the chemostat,"The author describes the chemostat, a device used to study bacterial populations under nutrient-limited conditions. The article shows that a simple criterion can help predict which species survive in the long run, an example of the principle of mutual exclusion.",
Technopreneurial inclinations and career management strategy among information technology professionals,"This paper examines the career goals and career management strategy of information technology (IT) professionals in Singapore. The results of this research found that IT professionals seek to advance their careers in three ways. The first two are standard career paths, which involve specializing in either a technical or a managerial career. The third career path, termed technopreneurship, involves IT professionals setting up their own IT business. The results of this research also showed that: (1) career planning has a positive impact on career strategy, (2) career strategy has a positive impact on professional enhancement, and (3) professional enhancement has a positive impact on career satisfaction. One important implication of these results is that it is importance for IT professionals to plan and strategize their career.","Engineering profession,
Information management,
Technology management,
Information technology,
Strategic planning,
Path planning,
Computer industry,
Shape,
Testing,
Conference management"
On the relationships of faults for Boolean specification based testing,"Various methods of generating test cases based on Boolean specifications have previously been proposed. These methods are fault-based in the sense that test cases are aimed at detecting particular types of faults. Empirical results suggest that these methods are good at detecting particular types of faults. However, there is no information on the ability of these test cases in detecting other types of faults. The paper summarizes the relationships of faults in a Boolean expression in the form of a hierarchy. A test case that detects the faults at the lower level of the hierarchy will always detect the faults at the upper level of the hierarchy. The hierarchy helps us to better understand the relationships of faults in a Boolean expression, and hence to select fault-detecting test cases in a more systematic and efficient manner.","Fault detection,
Software testing,
System testing,
Humans,
Hardware,
Information technology,
Australia,
Programming,
Manufacturing,
Computer science"
Tracing lineage of array data,"Arrays are a common and important class of data. They can model digital images, digital video, scientific and experimentation data, matrices, finite element grids, and many other types of data. Although array manipulations are diverse and domain-specific, they often exhibit structural regularities. The paper presents an algorithm called SUN-pushdown to compute data lineage in such array computations. The array manipulations are expressed in the Array Manipulation Language (AML) that was introduced previously (A.P. Marathe and K. Salem, 1997). SUB-pushdown has several useful features. First, the lineage computation is expressed as an AML query. Second, it is not necessary to evaluate the AML lineage query to compute the array data lineage. Third, SUB-pushdown never gives false-negative answers. SUB-pushdown has been implemented as part of the ArrayDB prototype array database system that we built (A.P. Marathe, 2001).","Signal sampling,
Frequency,
Computer science,
Digital images,
Shape,
Application software,
Finite element methods,
Prototypes,
Database systems,
Multidimensional systems"
Comparing and combining read miss clustering and software prefetching,"A recent latency tolerance technique, read-miss clustering, restructures code to send demand-miss references in parallel to the underlying memory system. An alternative, widely-used latency tolerance technique is software prefetching, which initiates data fetches ahead of expected demand-miss references by a certain distance. Since both techniques seem to target the same types of latencies and use the same system resources, it is unclear which technique is superior or if both can be combined. This paper shows that these two techniques are actually mutually beneficial, each helping to overcome limitations of the other: We perform our study for uniprocessor and multiprocessor configurations, in simulation and on a real machine (the Convex Exemplar). Compared to prefetching alone (the state-of-the-art implemented in systems today), the combination of the two techniques reduces the execution time by an average of 21% across all cases studied in simulation, and by an average of 16% for 5 out of 10 cases on the Exemplar. The combination sees execution time reductions relative to clustering alone averaging 15% for 6 out of 11 cases in simulation and 20% for 6 out of 10 cases on the Exemplar.","Prefetching,
Delay,
Parallel processing,
Out of order,
Pipeline processing,
Hardware,
Concurrent computing,
Computer science,
Software algorithms,
Steady-state"
Approximate shape fitting via linearization,"Shape fitting is a fundamental optimization problem in computer science. The authors present a general and unified technique for solving a certain family of such problems. Given a point set P in R/sup d/, this technique can be used to /spl epsi/-approximate: (i) the min-width annulus and shell that contains P, (ii) minimum width cylindrical shell containing P, (iii) diameter, width, minimum volume bounding box of P, and (iv) all the previous measures for the case the points are moving. The running time of the resulting algorithms is O(n + 1//spl epsi//sup c/), where c is a constant that depends on the problem at hand. Our new general technique enables us to solve those problems without resorting to a careful and painful case by case analysis, as was previously done for those problems. Furthermore, for several of those problems our results are considerably simpler and faster than what was previously known. In particular, for the minimum width cylindrical shell problem, our solution is the first algorithm whose running time is subquadratic in n. (In fact we get running time linear in n.).","Shape,
Computer science,
Cities and towns,
Volume measurement,
Chromium,
Graphics,
Spatial databases,
Metrology,
Kinetic theory,
Object detection"
4-dimensional computed tomography (4D CT) - its concepts and preliminary development,"4D CT is a dynamic volume imaging system of moving organs with an image quality comparable to conventional CT. Dynamic cone-beam CT can realize it with several breakthroughs. They are: 1) large-area 2-dimensional (2D) detector, 2) high-speed data transfer system, 3) reconstruction algorithm, 4) ultra-high-speed reconstruction computer and 5) high-speed and continuous rotating gantry. Among them the development of the 2D detector is one of the biggest tasks because it should have as wide dynamic range and high data acquisition speed (view rate) as present CT detectors. We are now developing a 4D CT-scanner together with the key-components. It will take one volume in 0.5 sec with a 3D matrix of 512 /spl times/ 512 /spl times/ 512. This paper describes its concepts and design, as well as preliminary development of the 2D detector.","Computed tomography,
Image reconstruction,
Dynamic range,
Data acquisition,
Reconstruction algorithms,
X-ray detection,
X-ray detectors,
Image quality,
Humans,
Medical treatment"
A method of non-data-aided carrier recovery with modulation identification,"A non-data aided carrier recovery technique using modulation format identification is proposed. This technique can also be interpreted as a modulation identification method that is robust against static phase and frequency offsets. The performance of the proposed technique is studied and analytical expressions derived for the mean acquisition time to detect lock in the cases of M-PSK, M=2,4,8, and 16-QAM modulation, with respect to frequency offset and signal-to-noise ratio. The results are verified with Monte Carlo simulations. The main advantage of the proposed method lies in its simpler implementation and faster lock detection, when compared to conventional methods.","Frequency,
Phase locked loops,
Phase modulation,
Digital modulation,
Switches,
Wireless communication,
Phase detection,
Detectors,
Robustness,
Computer science"
Server switching: yesterday and tomorrow,"Server switches distribute incoming request traffic across the nodes of Internet server clusters and Web proxy cache arrays. These switches are a standard building block for large-scale Internet services, with many commercial products on the market. As Internet applications and service architectures continue to evolve, the role of server switches and the demands on their request routing policies will also change. This paper explores inter-related factors shaping the role of server switching. These factors include the emergence of content delivery networks, the increasing prevalence of dynamic content, persistent connections in the HTTP 1.1 standard, the changing nature of Web server clusters and the opportunities to apply server switching techniques to service protocols other than HTTP. We outline the potential role of server switches in virtualizing IP-based storage protocols and as a foundation for adaptive resource provisioning in server clusters.","Web server,
Network servers,
Switches,
Web and internet services,
Large-scale systems,
Routing,
Telecommunication traffic,
Protocols,
Content management,
Computer science"
The implementation of the STAR data-acquisition system using a Myrinet Network,"We present results from the first year of operation of the STAR data-acquisition (DAQ) system using a Myrinet Network. STAR is one of four experiments to have been commissioned at the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National Laboratory during 1999 and 2000. The DAQ system is fully integrated with a Level 3 Trigger. The combined system currently consists of 33 Myrinet Nodes, which run in a mixed environment of MVME processors running VxWorksj DEC Alpha workstations running Linux, and SUN Solaris machines. The network will eventually contain up to 150 nodes for the expected final size of the L3 processor farm. Myrinet is a switched high-speed low-latency network produced by Myricom and available for PCI and PMC on a wide variety of platforms. The STAR DAQ system uses the Myrinet Network for messaging, L3 processing, and event building. After the events are built, they are sent via gigabit ethernet to the RHIC computing facility and stored to tape using HPSS. The combined DAQ/L3 system processes 160-MB events at 100 Hz, compresses each event to 20 MB, and performs tracking on the events to implement a physics-based filter to reduce the data storage rate to 20 MB/s.",
Modeling a fault-tolerant distributed system,A C-based simulation model of the time-triggered protocol (TTP/C) has been designed and implemented as a tool for verifying the properties of a system designed on the basis of it. The model has been provided with a user-friendly interface to allow easy visualization and evaluation of the results. The functionality of this general-purpose model is demonstrated on a simple TTP/C cluster application running under the influence of fault injection. The first round of experiments shows that the system is tolerant toward some typical transient faults like memory data distortion.,"Fault tolerant systems,
Object oriented modeling,
Real time systems,
Time division multiple access,
Access protocols,
Communication system control,
Computer science,
Computational modeling,
Libraries,
Application software"
Collaborative examinations for asynchronous learning networks: evaluation results,"This paper presents the evaluation results of two student surveys on a collaborative examination process using asynchronous learning network (ALN) for a graduate-level course at NJIT. The exam process includes students making up questions, picking out questions, answering, grading and appealing the grades. The process was conducted on the Virtual Classroom/sup R/ and Webboard/sup R/ during the fall 1999 and spring 2000 semesters, with some revision of the process in the second semester. The surveys following each exam elicited feedback from 138 students. Results show the majority of students felt they learned throughout the process, the exams were successful in demonstrating what they learned, and it was an enjoyable process. Students' concerns and our experiences are presented as well as suggestions for future research on this topic.","Springs,
Collaborative work,
International collaboration,
Materials testing,
Information science,
Technology management,
Feedback,
Stress,
Internet,
Peer to peer computing"
Mining market value functions for targeted marketing,"Targeted marketing typically involves the identification of customers or products having potential market values. We propose a linear model for solving this problem by drawing and extending results from information retrieval. It is assumed that each object is represented by values of a finite set of attributes. A market value function, which is a linear combination of utility functions on attribute values, is used to rank objects. Several methods are examined for mining market value functions. The main advantage of the model is that one can rank objects. of interest according to their market values, instead of classifying the objects. The theoretical results reported in this paper establish a basis on which further studies and experimental evaluation can be carried out.",
Packet based telephony,"This paper describes methods used to place telephone calls over packet-based networks with emphasis on Internet point-to-point communication. Two approaches will be described-H.323 and SIP (session initiation protocol). The first is an ITU standard and is already widely deployed by commercial vendors. The second, SIP, was developed by IETF activists and represents an ""Internet approach"" to telephony. First, a brief overview of reasons for the growing importance of IP telephony is given. It is followed by a short introduction to voice transmission over packet networks and an overview of the existing standards. In the second part of the paper, short examples of H.323 and SIP call setup are given.","Internet telephony,
IP networks,
Protocols,
Quality of service,
Computer science,
Electronic mail,
Laboratories,
Computer architecture,
Communication system control,
Circuits"
Efficient multicast algorithms for switch-based irregular heterogeneous networks of workstations,"This paper considers the problem of efficient multicast on worm-hole routed irregular heterogeneous networks of workstations, using multiple unicast messages. The fundamental issues are the avoidance of link contention and effective use of the faster nodes in the system for distributing the multicast message. Previously proposed schemes have either considered optimization for heterogeneity or elimination of contention, but not both together. We present two algorithms that addresses both issues and demonstrate their superiority through simulation studies.",
Efficient transparency extraction and utilization in hierarchical test,"We introduce a methodology for identifying transparency behavior appropriate for hierarchical test, based on the theoretical principles of transparency composition. Unlike high level approaches that identify limited, coarse transparency behavior, the proposed methodology is capable of extracting a wide class of fine grained transparency functions for arbitrary sub-word bit clusters. The functions in this class can furthermore be rapidly extracted on the fly and efficiently utilized for hierarchical test translation, thus alleviating the exponential extraction time and storage space requirements of exhaustive approaches. The twin benefits of rapid, automated extraction coupled with the expansion of utilizable transparency scope deliver reduced DFT while enabling cost-effective hierarchical test of high quality.",
Using tree topology for multicast congestion control,"Multicast is a promising technique for mass distribution of streaming media. However, the inherent heterogeneity of the Internet poses several challenges. A major challenge is to develop a congestion control mechanism that is efficient, flexible (to administrative heterogeneity) and deployable. Many approaches using a layered encoding scheme have been proposed to address this problem. In parallel, many tools are being developed which provide a snapshot of network internals. Of particular interest are multicast topology discovery tools. The existence of such tools motivates the possibility of using tree topology information for multicast congestion control. In this paper we seek to understand the benefits of such a mechanism and the challenges in its practical implementation. We develop an algorithm, called TopoSense, which uses topology information and layered streams to control congestion within an administrative domain. Our algorithm presents a new model for multicast congestion control as it does not involve on-router computation as opposed to other approaches which require router support. We evaluate our algorithm using ns, the network simulator. Our results indicate that topology information is very useful in understanding the dynamics of multicast congestion and can be used for efficient traffic management.","Streaming media,
Network topology,
Internet,
Multicast algorithms,
Traffic control,
Computer science,
Communication system traffic control,
Computational modeling,
Explosives,
Unicast"
Unifying stabilization and termination in message-passing systems,"We dispel the myth that it is impossible for any stabilizing message passing program to be terminating. We identify fixpoint-symmetry as a necessary condition for a message passing stabilizing program to be terminating. Our results do confirm that a number of well-known input-output problems (e.g., leader election and consensus) do not admit a terminating and stabilizing solution. On the flip side, they show that reactive problems such as mutual exclusion and reliable-transmission do admit such solutions. We go on to present stabilizing and terminating programs for both problems. Also, we describe a way to add termination to a stabilizing program, and demonstrate it in the context of our design of a solution to the reliable-transmission problem.","Information science,
Nominations and elections,
Context,
Computer crashes,
Mathematics,
Computer science"
Fast and scalable algorithms for the euclidean distance transform on the LARPBS,,"Euclidean distance,
Pixel,
Phase change random access memory,
Computer science,
Software engineering,
Image converters,
Application software,
Computer vision,
Image processing,
Optical arrays"
A graphical class representation for integrated black- and white-box testing,"Although both black- and white-box testing have the same objective, namely detecting faults in a program, they are often conducted separately. In our opinion, the reason is the lack of techniques and tools integrating both strategies, although an integration can substantially decrease testing costs. Specifically, an integrated technique can generate a reduced test suite, as single test cases can cover both specification and implementation at the same time. The paper proposes a new graphical representation of classes, which can be used for integrated class-level black-and white-box testing. Its distinguishing feature from existing representations is that each method of a class is shown from two perspectives, namely the specification and implementation view. Both the specification of a method and its implementation are represented as control flow graphs, which allows black- and white-box testing by structural techniques. Moreover, a test suite reduction technique has been developed for adjusting white-box test cases to black-box testing.",
Designing components versus objects: a transformational approach,"A good object oriented design does not necessarily make a good component based design, and vice versa. What design principles do components introduce? The paper examines component based programming and how it expands the design space in the context of an event based component architecture. We present a conceptual model for addressing new design issues these components afford, and we identify fundamental design decisions in this model that are not a concern in conventional object oriented design. We use JavaBeans based examples to illustrate concretely how expertise in component based design, as embodied in a component taxonomy and implementation space, impacts both design and the process of design. The results are not exclusive to JavaBeans; they can apply to any comparable component architecture.","Object oriented modeling,
Java,
Process design,
Component architectures,
Taxonomy,
Connectors,
Boolean functions,
Educational institutions,
Computer science,
Software engineering"
Typechecking XML views of relational databases,"Motivated by the need to export relational databases as XML data in the context of the World Wide Web, we investigate the type-checking problem for transformations of relational data into tree data (i.e. XML). The problem consists of statically verifying that the output of every transformation belongs to a given output tree language (specified for XML by a document type definition), for input databases satisfying given integrity constraints. The type-checking problem is parameterized by the class of formulas defining the transformation, the class of output tree languages and the class of integrity constraints. While undecidable in its most general formulation, the type-checking problem has many special cases of practical interest that turn out to be decidable. The main contribution of this paper is to trace a fairly tight boundary of decidability for type-checking in this framework. In the decidable cases, we examine the complexity and show lower and upper bounds. We also exhibit a practically appealing restriction for which type-checking is in PTIME.","XML,
Relational databases,
Industrial relations,
Web sites,
Markup languages,
Medical services"
"Three-dimensional motion system (""data-gloves""): application for Parkinson's disease and essential tremor","The accurate modelling and analysis of medical treatment plays an important role in modern medical science. This paper describes an attempt to apply current sensor and control techniques to quantitatively measure significant motion of the human body. By modelling the movement of the human hand this system can accurately measure the hand posture and all the phalanges, store the measurements as X,Y,Z absolute positions and the computer can (offline) compute the movement of the hand and phalanges. By applying dedicated embedded electronics a portable and networked system can be realized which may be used in the clinical assessment of stroke patients or medical conditions such as Parkinson's disease. It can be used as both a training instrument and a tool for medical diagnosis and research, leading to objective analysis of medical conditions involving accurate posture measurement of the human body.","Humans,
Biological system modeling,
Position measurement,
Medical conditions,
Medical treatment,
Medical diagnostic imaging,
Sensor phenomena and characterization,
Motion control,
Medical control systems,
Current measurement"
A new technology mapping for CPLD under the time constraint,"In this paper, we proposed a new technology mapping algorithm for CPLD under the time constraint (TMCPLD-II). In our technology mapping algorithm, we generate the feasible clusters from a given Boolean. The generated feasible clusters create clusters with minimum area under the time constraint. A covered Boolean network is transformed to a Boolean equation. The transformed equations are reconstructed in order to fit to an architecture of selected target CPLD by using collapsing and bin-packing. To demonstrate the efficiency of our approach, we applied our algorithm to MCNC benchmarks and compared the results with those of the existing algorithms. The experimental results show that our approach is better than any of the existing algorithms in the number of logic blocks.","Time factors,
Clustering algorithms,
Logic functions,
Circuit synthesis,
Equations,
Field programmable gate arrays,
Digital circuits,
Combinational circuits,
Computer science,
Educational technology"
Collaborative filtering with automatic rating for recommendation,"In this paper, the authors describe a recommendation system designed to suggest new products to Web shopping mall customers. The recommender is meant to provide alternatives or new products that actively suit a user's tastes and meets his/her needs. Most previously proposed recommendation systems that use collaborative filtering can cause problems when there are insufficient user ratings. The authors address this problem by combining content-based filtering and collaborative filtering. Using an automatic rating method instead of a users' explicit rating, the inaccuracy of rating data is decreased.",
A distributed ladder transportation algorithm for two robots in a corridor,"We consider the problem of transporting a long object, such as a ladder, through a 90 degree corner in a corridor using two omnidirectional robots that do not necessarily have identical characteristics. A distributed algorithm is presented in which each robot computes its own motion based on the current and goal positions of the ladder, the locations of the walls, and the motion of the other robot observed indirectly through the link between the robot and the ladder. We evaluate the performance and robustness of the algorithm using extensive computer simulation by changing several parameter values that affect the key characteristics of the robots, including the maximum speed, the guide path through a corner, and the sensitivity and reaction to the motion of the other robot. The simulation results indicate that if the parameter values are chosen within certain reasonable ranges, then overall the algorithm works quite well even for robots having difficult characteristics. It is also shown that the robustness of the algorithm critically depends on the differences between the robots in the values of two parameters.","Transportation,
Robot sensing systems,
Computer science,
Distributed algorithms,
Distributed computing,
Robustness,
Computer simulation,
Computational modeling,
Robust control,
Robot control"
Structuring the disciplines related to software engineering: a general model,"The paper examines the relationship between software engineering and the subjects that comprise its related disciplines, taking as a motivating example the role of the topic of data structures and algorithms in an undergraduate degree programme in software engineering. The paper examines current models for the relationship between disciplines such as software engineering and the knowledge areas that comprise them, and it identifies the main weakness of these models. It then goes on to propose a new general model for this relationship, and it describes how this was applied in redesigning a course on data structures and algorithms within a software engineering degree programme. A qualitative evaluation is made of this redesign, from which conclusions are drawn about how this model might be applied to other topics and disciplines.","Software engineering,
Object oriented modeling,
Data structures,
Computer science,
Software algorithms,
Telephony,
Knowledge engineering,
Information systems"
Text area localization under complex-background using wavelet decomposition,"In this paper, we propose a novel approach to determine the positions of text areas in images with complex background using wavelet decomposition and pseudo-motion of images. In our method, a fixed image is translated and provides a sequence of moving images-the pseudomotion sequence of image. In fact, we can consider the translation of an image to be a motion of eyeshot. When an image is translated, its wavelet coefficients will oscillate. From this property, we can locate the text areas in complex-background images. Experiments were conducted to demonstrate the performance of the method In the experiments, we detect the text areas of several different types of characters in images with multi-gray level complex backgrounds.","Data mining,
Pattern recognition,
Text recognition,
Image retrieval,
Mathematics,
Image recognition,
World Wide Web,
Videos,
Computer science,
Fourier transforms"
A 50nm channel vertical MOSFET concept incorporating a retrograde channel and a dielectric pocket,,"MOSFET circuits,
Dielectrics,
Doping,
Threshold voltage,
Epitaxial layers,
Computer science,
Computer architecture,
Numerical simulation,
Epitaxial growth,
Logic devices"
Safety and liveness in branching time,"Extends B. Alpern & F.B. Schneider's linear time characterization of safety and liveness properties to branching time, where properties are sets of trees. We define two closure operators that give rise to the following four extremal types of properties: universally safe, existentially safe, universally live and existentially live. The distinction between universal and existential properties captures the difference between the CTL (computation tree logic) path quantifiers /spl forall/ (for all paths) and /spl exist/ (there is a path). We show that every branching time property is the intersection of an existentially safe property and an existentially live property, a universally safe property and a universally live property, and an existentially safe property and a universally live property. We also examine how our closure operators behave on linear-time properties. We then focus on sets of finitely branching trees and show that our closure operators agree on linear-time safety properties. Furthermore, if a set of trees is given implicitly as a Rabin tree automaton /spl Bscr/, we show that it is possible to compute the Rabin automata corresponding to the closures of the language of /spl Bscr/. This allows us to effectively compute /spl Bscr//sub safe/ and /spl Bscr//sub live/ such that the language of /spl Bscr/ is the intersection of the languages of /spl Bscr//sub safe/ and /spl Bscr//sub live/. As above, /spl Bscr//sub safe/ and /spl Bscr//sub live/ can be chosen so that their languages are existentially safe and existentially live, universally safe and universally live, or existentially safe and universally live.","Safety,
Automata,
Control systems,
Protocols,
Boolean algebra,
Logic functions"
Application of parametric model checking - the Root Contention protocol,"Presents an application of formal verification which was carried out using a new implemented version of the LPMC model checker tool. The focus is on the modeling and the automatic verification of a protocol contained in the IEEE 1394 standard, the Root Contention protocol. This protocol involves both real time and randomization. This is an illustrative case study which fully demonstrates the use of the new LPMC tool's capability of handling linear constraints in order to exploit parametric real-time model checking.","Parametric statistics,
Protocols,
Firewire,
Network topology,
Application software,
Formal verification,
Computer science,
Information technology,
Real time systems,
High performance computing"
Fast computation of the 3-D Euclidean distance transform on the EREW PRAM model,"In a two or three-dimensional image array, the computation of Euclidean distance transform (EDT) is an important task. With the increasing application of 3D voxel images, it is useful to consider the distance transform of a 3D digital image array. Because the EDT is a global operation, it is prohibitively time consuming when performing the EDT for image generation. In order to provide the efficient transform computations, parallelism is employed. In this paper we first derive several important geometry relations and properties among parallel planes. We then develop a parallel algorithm for the three-dimensional Euclidean distance transform (3D-EDT) on the EREW PRAM computation model. The time complexity of our parallel algorithm is O(log/sup 2/ N) for an N/spl times/N/spl times/N image array.","Euclidean distance,
Phase change random access memory,
Pixel,
Computer science,
Concurrent computing,
Parallel algorithms,
Image converters,
Application software,
Digital images,
Image generation"
Teams of agents,"The paper presents a discussion about necessary and sufficient conditions for the existence of a ""team"". It looks at the nature of the common mental state required of the agents forming the team, in particular, what an intention is, and various measures of how ""team-like"" a group of agents is.","Humans,
Permission,
Computer science,
Sufficient conditions,
Particle measurements,
Books,
Software agents,
Insects"
Sequence adaptation for interference reduction and capacity maximization in DS-CDMA systems,"The relation between interference reduction and capacity maximization in DS-CDMA systems is investigated. We show that the solution which minimizes the total weighted squared correlation (TWSC) happens to maximize the sum capacity of the system. Furthermore, we show that a distributed sequence adaptation algorithm based on the MMSE receiver reduces TWSC and increases sum capacity simultaneously at each iteration step. Simulation results show that the algorithm converges to the optimal solution.","Interference,
Multiaccess communication,
Correlation,
Code division multiplexing,
Distributed algorithms,
AWGN,
Computer science,
Power system modeling,
Base stations,
Additive white noise"
Semisentient robots: routes to integrated intelligence,"Complex systems are getting to the point where it almost feels as if ""someone"" is there behind the interface. This impression comes across most strongly in the field of robotics because these agents are physically embodied, much as humans are. We believe that this phenomenon has four primary components: a system must be able to act in some reasonably complicated domain; communicate with humans using a language-like modality;. reason about its actions at some level so that it has something to discuss, and, learn and adapt to some extent on the basis of human feedback. These aspects are discussed in the article.","Intelligent robots,
Humans,
Artificial intelligence,
Intelligent systems,
Animation,
Feedback,
Psychology,
Cognitive science,
Animals,
Medical expert systems"
Combined M/G/1-G/M/1 type structured chains: a simple algorithmic solution and applications,We consider combined M/G/1-G/M/1 type Markov chains with block-structured transition. It is assumed that all the blocks are generated with rational generating matrices. We provide an algorithmic approach to find the stationary probability distribution based on well-known/concepts in linear system theory. These chains arise in the (correlated) G/G/1 queueing systems and known structured Markov chains such as canonical and non-canonical M/G/1 and G/M/1 types which frequently arising in teletraffic analysis of computer and communications networks are special cases. We provide a truncation-free algorithmic solution in a simple geometric form of the the stationary probability vector of the chain taking full advantage of the rational generating matrices.,"Application software,
Cities and towns,
Stochastic processes,
Computer science,
Probability distribution,
Linear systems,
Queueing analysis,
Computer networks,
Communication networks,
Solid modeling"
DDDDRRaW: A prototype toolkit for distributed real-time rendering on commodity clusters,"We describe DDDDRRaW, a prototype toolkit for distributed real-time rendering on commodity clusters. In constrast to most work on cluster computing, DDDDRRaW supports a repeated, low-latency computation, the drawing of frames which must take place on a time scale of 30-100 ms. DDDDRRaW employs image layer decomposition, a rendering-specific work partitioning algorithm described and evaluated using simulation. In this paper we address implementation issues. In particular, one important issue we explore is how to exploit the potential parallelism afforded by the multiple hardware resources of each node: the CPU, the network adapter and the video card. We evaluate DDDDRRaW's live performance on two small workstation clusters representing different points in the technology spectrum. Our results show that DDDDRRaW effectively exploits cluster resources to improve real-time rendering performance and should scale well to moderately sized clusters.","Prototypes,
Rendering (computer graphics),
Hardware,
Layout,
Computer science,
Peer to peer computing,
Workstations,
Personal communication networks,
Graphics,
Delay"
Distributed sequence adaptation for capacity maximization of DS-CDMA systems,"A game theoretic approach is used to investigate the stability of distributed adaptation algorithms. We show that for cellular CDMA systems, Nash equilibria may not exist. On the other hand, for single-cell systems, there are multiple Nash equilibria. None of them is better than the others in the Pareto sense. Furthermore, the behavior of a distributed algorithm based on an MMSE receiver is studied. Simulation results show that it converges to the Nash equilibrium which maximizes sum capacity.",
Computing executable slices for concurrent logic programs,"Program slicing has many applications in software engineering activities. However, until recently, no slicing algorithm has been presented that can compute executable slices for concurrent logic programs. We present a dependence-graph based approach to computing executable slices for concurrent logic programs. The dependence-based representation used in the paper is called the Argument Dependence Net which can be used to explicitly represent various types of program dependences in a concurrent logic program. Based on the ADN, we can compute static executable slices for concurrent logic programs at argument level.","Concurrent computing,
Logic programming,
Software engineering,
Computer science,
Information technology,
Application software,
Performance evaluation,
Debugging,
Software testing,
Software measurement"
Detecting noise trading using fuzzy exception learning,"Th paper analyses noise trading, a phenomenon observed in financial markets when technical traders forecast financial price movements based on recent prices and volumes. Due to noise trading, financial returns show (during a few and unknown periods in time), certain deterministic behavior besides the usual random behavior predicted by the efficient market hypothesis. Our goal is to unmask the (fuzzy) deterministic part, that is, to discover the special circumstances called 'regimes', under which noise trading takes place. To reach our goal, we use the Competitive Fuzzy Exception Learning Algorithm (CELA) as introduced by W.M. van den Bergh, and J. van den Berg (2000), J. van den Berg, and W.M. van den Berg, (2000). In order to analyze the properties of our method, we apply it on an artificially made, yet quite hard to analyze financial time series. Even in a very general setting, CELA appears to be able to discover various important 'regimes' corresponding to exceptional price developments. These occurrences are collected in a fuzzy rule base.",
Learning similarity measure for natural image retrieval with relevance feedback,"A new scheme of learning similarity measure is proposed for content-based image retrieval (CBIR). It learns a boundary that separates the images in the database into two parts. Images on the positive side of the boundary are ranked by their Euclidean distances to the query. The scheme is called restricted similarity measure (RSM), which not only takes into consideration the perceptual similarity between images, but also significantly improves the retrieval performance based on the Euclidean distance measure. Two techniques, support vector machine and AdaBoost, are utilized to learn the boundary, and compared with respect to their performance in boundary learning. The positive and negative examples used to learn the boundary are provided by the user with relevance feedback. The RSM metric is evaluated on a large database of 10,009 natural images with an accurate ground truth. Experimental results demonstrate the usefulness and effectiveness of the proposed similarity measure for image retrieval.",
Mining frequent closed itemsets with the frequent pattern list,"The mining of a complete set of frequent itemsets will lead to a huge number of itemsets. Fortunately, this problem can be reduced to the mining of frequent closed itemsets (FCIs), which results in a much smaller number of itemsets. The approaches to mining frequent closed itemsets can be categorized into two groups: those with candidate generation and those without. In this paper, we propose an approach to mining frequent closed itemsets without candidate generation with a data structure called the frequent pattern list (FPL). We designed the algorithm FPLCI-mining to mine the FCIs. Experimental results show that our method is faster than previous ones.","Data mining,
Itemsets,
Data structures,
Algorithm design and analysis,
Transaction databases,
Computer science,
Association rules,
Filtering"
Multiring Fiducial Systems for Scalable Fiducial-Tracking Augmented Reality,"In augmented reality (AR), a user can see a virtual world as well as the real world. To avoid registration problems between the virtual world and the real world, the user's viewing pose in both worlds should be kept the same. Fiducial-tracking AR is an attractive approach to the registration problem. However, most of the developed fiducialtracking AR systems have restricted workspaces. To provide a wide range of workspaces (from a small-scale desktop space to a large-scale space) and a wide range of views (from far views to detailed views), an AR system should have scalability. In this paper, we present multiring color fiducial systems and a real-time fiducial detection method for scalable fiducial-tracking AR. We analyze the optimal ring width and develop formulas to obtain the optimal fiducial set with applicationspecific inputs. We develop a real-time ring-detection method that converts the five-DOF ellipse-detection problem to a series of simple steps with a 1-D segmentfilter and multithreshold segmentation. The results lead to a simple and inexpensive means of achieving scalable-area tracking for AR and an approach that is suitable as an optical tracking method for VR as well.",
Location update generation in cellular mobile computing systems,,
Digital pulse mode neural network with simple synapse multiplier,"This paper proposes a new type of digital pulse mode neural network with very simple synapse multiplier. Combined with the pulse mode operation, the multiplier is implemented with a set of AND gates. The feasibility of the proposed architecture is verified by computer simulations and experiments. The results show that the proposed MNN can be realize with much less hardware resource while providing the same performance as the conventional architecture.","Neural networks,
Neurons,
Multi-layer neural network,
Pulse circuits,
Registers,
Computer architecture,
Frequency,
Pulse modulation,
Electronics packaging,
Computer science"
The recognition of facial expressions from video frames,"Several researchers have attempted automatic recognition and tracking of facial expressions in video frames of neutral and expressive faces. However, it is very hard to detect a neutral face in practical applications because the distinction between facial movement and head movement is difficult to make. In order to solve this distinction problem, the initialization of the head position using the central point of each eye and the tip of the nose is first made. Facial movements are then extracted. Based on the detected neutral faces, the changing processes of facial expressions can be recognized using the recognition algorithm for facial expressions which is described based on the fuzzy expert system. Computer simulation using 48 expressive facial videos shows more than 77% accuracy on average.","Face recognition,
Face detection,
Image recognition,
Head,
Nose,
Hybrid intelligent systems,
Tracking,
Mouth,
Informatics,
Computer science"
High performance computing with microsoft windows 2000,,
Seamless integration of control flow and data flow in a visual language,"In the visual programming domain, the stress of research is laid on the use of visual formalism, which is considered to be more intuitive than the textual formalism, in the programming task. Some visual languages are based purely on data flow. With such languages, the execution order depends on the availability of data and it is therefore difficult to specify some programming constructs such as control structures. On the other hand, a pure control flow based language has shortcomings with respect to data processing. Many visual languages based on the data flow paradigm are supplemented with control structures to specify repetitive behavior in programs. In our case, the visual language initially used the control flow paradigm. We then enriched this language with the integration of data flow. The article presents the advantages of this integration. Then, it explains (mostly from the visual formalism point of view) how we decided to handle the combination of the two paradigms in our language, named DIVA-cd. The article presents through examples the specifications of various control structures in the DIVA-cd language.","Computer science,
Computer languages,
Stress,
Programming profession,
Flowcharts,
Petri nets,
Flow graphs,
Testing,
Data structures,
Software engineering"
Design of a prototype real-time image reconstruction system for PET imaging,"Real-time reconstruction of the acquired data in positron emission tomography (PET) imaging would facilitate positioning of the subject and could serve to monitor image quality and detect potential problems during the acquisition. A real-time image reconstruction prototype was developed as an extension of the data acquisition system of the Sherbrooke Animal PET scanner using a network of field programmable gate arrays (FPGA), digital signal processors (DSP) and a fast PET image reconstruction algorithm based on SVD decomposition of the system matrix. Two different implementation of the reconstruction system were investigated in order to obtain the fastest reconstruction times for images of at least 64/spl times/64 pixels. Initial results demonstrate that the acquired image can be updated every second using a 600 MHz personal computer for the reconstruction calculations while using the DSPs to pre-process the incoming data.",
Lessons learned in data reverse engineering,"Reverse engineering of data has been performed in one form or another for over twenty-five years (1976-2001 approx.). The author describe the lessons learned in data reverse engineering (DRE) as contributed in a survey of data reverse engineers. Interesting is the fact that some of the lessons learned tell us how we are doing in the process of initial database design as well as how difficult the DRE process really is. It is hoped that from these lessons learned, we can assist in the suggestion of the next steps that are needed in the DRE area and promote discussion among the DRE community.","Reverse engineering,
Databases,
Software systems,
Computer science,
Buildings,
Application software,
Data analysis,
Data mining,
Data structures,
Documentation"
Support vector domain description for speaker recognition,"A novel approach to speaker recognition is presented. The method, called Support Vector Data Description (SVDD), was originally suggested by Vapnik, interpreted as a novelty detector by D. Tax and R. Duin (1999). In this paper, we use this data domain description as a classifier. It contains support vectors describing the sphere separating the samples. With a minimal radius R, this classifier achieves good performance in finding abnormal samples within the open set test. We use it in the speaker identification application. The results on YOHO database are presented.","Speaker recognition,
Detectors,
Databases,
System testing,
Computer science,
Speech,
Vector quantization,
Kernel,
Classification algorithms,
Chromium"
"Orientation, Scale, and Discontinuity as Emergent Properties of Illusory Contour Shape","A recent neural model of illusory contour formation is based on a distribution of natural shapes traced by particles moving with constant speed in directions given by Brownian motions. The input to that model consists of pairs of position and direction constraints, and the output consists of the distribution of contours joining all such pairs. In general, these contours will not be closed, and their distribution will not be scaleinvariant. In this article, we show how to compute a scale-invariant distribution of closed contours given position constraints alone and use this result to explain a well-known illusory contour effect.",
An agglomerative hierarchical clustering using partial maximum array and incremental similarity computation method,"As the tractable amount of data grows in the computer science area, fast clustering algorithms are required, because traditional clustering algorithms are not feasible for very large and high-dimensional data. Many studies have been reported on the clustering of large databases, but most of them circumvent this problem by using an approximation method, resulting in the deterioration of accuracy. In this paper, we propose a new clustering algorithm by means of a partial maximum array, which can realize agglomerative hierarchical clustering with the same accuracy as the brute-force algorithm and has O(N/sup 2/) time complexity. We also present an incremental method of similarity computation which substitutes a scalar calculation for the time-consuming calculation of vector similarity. Experimental results show that clustering becomes significantly fast for large and high-dimensional data.","Clustering algorithms,
Databases,
Clustering methods,
Iterative algorithms,
Merging,
Nearest neighbor searches,
Approximation methods,
Computational efficiency,
Machine intelligence,
Computer science"
Fuzzy frequent episodes for real-time intrusion detection,Data mining methods including association rule mining and frequent episode mining have been applied to the intrusion detection problem. We describe an extension that uses fuzzy frequent episodes for near real-time intrusion detection. We first define fuzzy frequent episodes and then describe experiments that explore their applicability for real-time intrusion detection. Experimental results indicate that fuzzy frequent episodes can provide effective approximate anomaly detection.,"Intrusion detection,
Data mining,
Association rules,
Frequency,
Computer networks,
Quantization,
Bridges,
Computer science,
Modems,
IP networks"
Separation of NP-completeness notions,"We use hypotheses of structural complexity theory to separate various NP-completeness notions. In particular, we introduce a hypothesis from which we describe a set in NP that is /spl les//sub T//sup P/-complete but not /spl les//sub tt//sup P/-complete. We provide fairly thorough analyses of the hypotheses that we introduce.","Polynomials,
Computer science,
Turing machines,
NP-complete problem"
PANDORA: a multi-agent system using paraconsistent logic,"This work is part of the Multicheck Project that defines architecture of cognitive and independents agents for the automatic treatment of handwritten Brazilian bank checks. The concept of autonomous agents allows us to organize the application knowledge and brings several own benefits to the approach. The choice of this approach is supported in a triple hypothesis. First, the nature of the problem in question allows decomposition in well-defined tasks, and each of them can be encapsulated in an independent agent. Second, the natural capability of interaction of the agents makes the check treatment process more robust, solving situations apparently difficult. Third, the natural parallelism between the agents can contribute to implement an application with high performance.","Multiagent systems,
Logic,
Image segmentation,
Image recognition,
Image analysis,
Electrical capacitance tomography,
Information analysis,
Computer science,
Robustness,
Costs"
PSP/sup SM/ in the large class,"Describes our experience with teaching some elements of the Personal Software Process/sup SM/ as part of a second programming course. A distinctive feature was the class size of more than 360 students. The goals were to help students develop good software development habits early, and to encourage them to see software development as a systematic discipline rather than a trial-and-error activity. The results indicate partial success, with indicators for future improvement. We hope that other people will be able to build on our experience of teaching the concepts of PSP to large groups of students.","Samarium,
Programming profession,
Java,
Computer science,
Computer science education,
Australia,
Computer languages,
Computer errors,
Concrete,
Software engineering"
On the decision problem for the guarded fragment with transitivity,"The guarded fragment with transitive guards, [GF+TG], is an extension of GF in which certain relations are required to be transitive, transitive predicate letters appear only in guards of the quantifiers and the equality symbol may appear everywhere. We prove that the decision problem for [GF+TG] is decidable. This answers the question posed in (Ganzinger et al., 1999). Moreover, we show that the problem is 2EXPTIME-complete. This result is optimal since the satisfiability problem for GF is 2EXPTIME-complete (Gradel, 1999). We also show that the satisfiability problem for two-variable [GF+TG] is NEXPTIME-hard in contrast to GF with bounded number of variables for which the satisfiability problem is EXPTIME-complete.",
Data mining for intelligent Web caching,"Presents a vertical application of data warehousing and data mining technology: intelligent Web caching. We introduce several ways to construct intelligent Web caching algorithms that employ predictive models of Web requests; the general idea is to extend the LRU (least recently used) policy of Web and proxy servers by making it sensible to Web access models extracted from Web log data using data mining techniques. Two approaches have been studied, in particular one based on association rules and another based on decision trees. The experimental results of the new algorithms show substantial improvements over existing LRU-based caching techniques in terms of the hit rate, i.e. the fraction of Web documents directly retrieved in the cache. We designed and developed a prototypical system, which supports data warehousing of Web log data, extraction of data mining models and simulation of the Web caching algorithms, around an architecture that integrates the various phases in the knowledge discovery process. The system supports a systematic evaluation and benchmarking of the proposed algorithms with respect to existing caching strategies.",
An end-end approach to wireless Web access,"We propose a lightweight scheme for negotiating client capabilities in the context of end-end content adaptation for wireless Web access. Our method is much less complex then W3C's proposed CC/PP framework. We suggest that for the purposes of content negotiation, all (mobile) clients could be grouped into a few relatively large categories. With this assumption, we simplify the CC/PP protocol and implement it as an Apache module. The paper describes the simple CC/PP protocol, issues related to its implementation, and performance measurements.","Personal digital assistants,
Bandwidth,
Access protocols,
Portable computers,
Displays,
HTML,
Computer science,
Measurement,
Information security,
Distributed computing"
Analysis and synthesis of facial expressions with hand-generated muscle actuation basis,"We present a performance-driven facial animation system for analyzing captured expressions to find muscle actuation and synthesizing expressions with the actuation values. A significantly different approach of our work is that we let artists sculpt the initial draft of the actuation basis: the basic facial shapes corresponding to the isolated actuation of individual muscles, instead of calculating skin surface deformation entirely, relying on mathematical models such as finite element methods. We synthesize expressions by linear combinations of the basis elements, and analyze expressions by finding the weights for the combinations. Even though the hand-generated actuation basis represents the essence of the subject's characteristic expressions, it is not accurate enough to be used in the subsequent computational procedures. We also describe an iterative algorithm to increase the accuracy of the actuation basis. The experimental results suggest that our artist-in-the-loop method produces a more predictable and controllable outcome than pure mathematical models, and thus can be a quite useful tool in animation productions.","Muscles,
Mathematical model,
Shape,
Facial animation,
Finite element methods,
Skin,
Production,
Computer science,
Iterative algorithms,
Humans"
Efficient Algorithms for Persistent Storage Allocation,Efficient disk storage is a crucial component formany applications. The commonly used method of storing data on disk using file systems or databases incurs significant overhead which can be a problem for applications which need to frequently access and update a large number of objects. This paper presents efficient algorithms for managing persistent storage which usually only require a single seek for allocations and deallocations and allow the state of the system to be fully recoverable in the event of a failure. Our system has been deployed for persistently storing data at the most accessed sport and event Web site hosted by IBM and results in considerable performance improvements over databases and file systems forWeb-related workloads.,
Classical Gentzen-type methods in propositional many-valued logics,"A classical Gentzen-type system is one which employs two-sided sequents, together with structural and logical rules of a certain characteristic form. A decent Gentzen-type system should allow for direct proofs, which means that it should admit some useful forms of cut elimination and the subformula property. In this tutorial we explain the main difficulty in developing classical Gentzen-type systems with these properties for many-valued logics. We then illustrate with numerous examples the various possible ways of overcoming this difficulty. Our examples include practically all 3-valued logics, the most important class of 4-valued logics, as well as central infinite-valued logics (like Godel-Dummett logic, S5 and some substructural logics).","Multivalued logic,
Computer science,
Data structures"
Reasoning about agents in the KARO framework,"This paper proposes two methods for realising automated reasoning about agent-based systems. The framework for modelling intelligent agent behaviour that we focus on is a core of KARO logic, an expressive combination of various modal logics including propositional dynamic logic, a modal logic of knowledge, a modal logic of wishes, and additional non-standard operators. The first method we present is based on a translation of core KARO logic to first-order logic combined with first-order resolution. The second method uses an embedding of core KARO logic into a combination of branching-time temporal logic CTL and multi-modal S5 plus a clausal resolution calculus for these combined logics. We discuss the advantages and shortcomings of each approach and suggest ways to extend each variant to cover more of the KARO framework.","Logic,
Computer science,
Intelligent agent,
Page description languages,
Calculus"
Hierarchical pre-segmentation without prior knowledge,"A new method to pre-segment images by means of a hierarchical description is proposed. This description is obtained from an investigation of the deep structure of a scale space image-the input image and the Gaussian filtered ones simultaneously. We concentrate on scale space critical points-points with vanishing gradient with respect to both spatial and scale direction. We show that these points are always saddle points. They turn out to be extremely useful, since the iso-intensity manifolds through these points provide a scale space hierarchy tree and induce a segmentation without a priori knowledge. Moreover, together with the so-called catastrophe points, these scale space saddles form the critical points of the parameterised critical curves-the curves along which the spatial saddle points move in scale space. Experimental results with respect to the hierarchy and segmentation are given, based on an artificial image and a simulated MRI.","Filters,
Image resolution,
Computer science,
Gaussian processes,
Image segmentation,
Magnetic resonance imaging,
Image analysis,
Spatial resolution,
Mathematics,
Joining processes"
"Telme: a personalized, context-aware communication support system","True global communication will require more than just language translation technologies. To fully understand each other, people also need context-specific information. The authors have developed Telme, a support system that gives users real-time information to help bridge the knowledge and experience gap. The authors explain the Telme framework for wearable computers connected to a central knowledge base server. The server controls a background knowledge database and downloads data on user request.","Wearable computers,
Computer displays,
Real time systems,
Space technology,
Intelligent systems,
Natural languages,
Speech,
Cognitive science,
Context modeling,
Humans"
A spectral histogram model for textons and texture discrimination,"Based on a local spatial/frequency representation, the spectral histogram of an image is defined as the marginal distribution of responses from a bank of filters. We propose the spectral histogram as a quantitative definition for textons. The spectral histogram model avoids rectification and spatial pooling, two commonly assumed stages in texture discrimination models. By matching spectral histograms, an arbitrary image can be transformed via statistical sampling to an image with similar textons to the observed. Texture synthesis is employed, to verify the adequacy of the model. Building on the texton definition, we use the /spl chi//sup 2/-statistic to measure the difference between two spectral histograms, which leads to a texture discrimination model. The performance of the model well matches psychophysical results on a systematic set of texture discrimination data. A quantitative comparison with the Malik-Perona model is given, and the biological plausibility of the model is discussed.",
Muteness-based audio watermarking technique,"Audio watermarking is a promising approach to copyright protection of audio data, especially music and songs. Several watermarking techniques have been developed and commercialized. The watermarks produced by those techniques can withstand a number of single attacks such as MPEG, resampling, filtering, and quantization. However, the watermarks are easily destroyed if subjected to chopping or multiple attacks. We present a new audio watermarking technique that provides watermark robustness not only to single attacks but also to multiple attacks and chopping.","Watermarking,
Filtering,
Data encapsulation,
Speech,
Computer science,
Copyright protection,
Commercialization,
Quantization,
Noise robustness,
Signal processing"
Towards automatic shaping in robot navigation,"Shaping is a potentially powerful tool in reinforcement learning applications. Shaping often fails to function effectively because of a lack of understanding about its effects when applied in reinforcement learning settings and the use of inadequate algorithms in its implementation. Due to these difficulties current shaping techniques require some form of manual intervention. We examine some of the principles involved in shaping and present a new algorithm for automatic transferral of knowledge, which uses the Q-values established in a previous task to guide exploration in the learning of a new task. This algorithm is applied to two different but related robot navigation tasks.","Robotics and automation,
Navigation,
Learning,
Robot sensing systems,
Function approximation,
Animals,
Computer science,
Application software,
Manuals,
Algorithm design and analysis"
Comparison of the analytic N-burst model with other approximations to telecommunications traffic,"A wide variety of traffic models are presently used to study the performance of telecommunications networks. These are shown to be limiting cases of N-burst/G/1 queues. The analytic N-burst model describes traffic as superposition of N packet streams of ON/OFF type. When using power-tail distributions for the duration of the ON periods, self-similar properties, which are critical for understanding tele-traffic, are observed. For very low intraburst packet rates, the N-burst/G/1 model reduces to an M/G/1 queue. For /spl lambda//sub p/ /spl rarr/ /spl infin/ all packets in a burst arrive simultaneously and the model becomes a bulk arrival, or M/sup (X)//G/1, queue. In the same limit, the packet-based model can be compared to a model of the burst level, an M/G/1 queue where the individual customers represent complete bursts rather than individual packets. Thus the mean system time describes the mean delay for the last packet in a burst rather than the average over all packets. The continuous flow model is also shown to be a limiting case of the N-burst model by letting the number of packets in a burst, n/sub p/, and the router's packet service rate, /spl nu/, go to infinity while holding their ratio constant. Numerical results are presented comparing the steady-state results for mean packet delay and for buffer overflow probabilities of the different analytic models. They collectively show the critical importance of the burstiness parameter. The N-burst/M/1 model with self-similar properties shows drastically changing steady-state performance for specific values of the burstiness parameter. The limiting models are incapable of describing the detailed structure of the performance in this transition region.","Telecommunication traffic,
Traffic control,
Steady-state,
Switches,
Delay effects,
H infinity control,
Computer science,
Computer applications,
Tail,
Telecommunication standards"
Development of a self-adaptive Web search engine,"As the Web evolves towards the direction of providing more and more information, locating the desired information efficiently becomes a very important issue. Web search engines are very useful information search tools in the Internet. Current Web search engines produce search results relating to the search terms and the actual information collected by them. Since the selections of the search results cannot affect the future ones, they may not cover most people's interests. Feedback information produced by users' accessing lists can influence the search results. Thus the search engines can provide self-adaptability.","Web search,
Search engines,
Output feedback,
Computer science,
World Wide Web,
Web and internet services,
Privacy,
IP networks,
Robots,
Indexes"
A new image flux conduction model and its application to selective image smoothing,"A discrete image flux conduction equation which is completely new in this field is proposed. The new approach starts with formulating a discrete image flux conduction equation based on the concept of heat conduction theory. Based on this discrete equation, the status change at a time point can be directly computed from its spatial neighborhood. To more accurately estimate an image flux, we have used an orthogonal wavelet basis to approximate the gradient of the intensity at each point. Since the proposed approach is discrete by nature, it is not necessary to formulate a continuous PDE to fit the discrete image data set. Furthermore, introduction of different numerical methods to solve the PDE can also be avoided. Since the proposed approach does not require that a PDE be solved, it is therefore more efficient and accurate than the conventional methods. Experimental results obtained using both synthetic signals and real images have demonstrated that the proposed model could effectively handle the selective image smoothing problem.","Smoothing methods,
Filtering,
Gaussian processes,
Nonlinear equations,
Conductivity,
Frequency,
Information science,
Computer science,
Filters,
Image processing"
A flexible curriculum for computer science undergraduate major,"This paper describes an innovative approach to establish a computer science curriculum, aiming flexibility and minimization of the time spent in the classrooms. This approach has been developed at the Paulista State University - Unesp - at Sao Jose do Rio Preto, Brazil, and is producing very interesting results. The load reduction is achieved through a series of fundamental core and breadth courses that precede depth courses in specific areas. The flexibility comes as a side effect of the depth courses, which can be adapted without any changes in the core courses. In this paper, the authors fully describe their motivations, actions and results.","Computer science,
History,
Hardware,
Computer networks,
Web sites,
Application software,
Educational technology,
Computer science education,
Assembly"
RECOM: a reflective architecture of middleware,"Current middleware is limited in its flexibility and adaptability in the face of varying environment and different user requirements. Applying the reflection technology to the middleware design has become a new research field. First, the concepts of reflection and reflective middleware are introduced, and what benefits from reflective middleware are pointed out. This paper compares the processing of middleware and the reflection mechanism, and then the reflective view of middleware is yielded. Based on this, the idea of employing binding-reification reflective model in middleware design is proposed and used in the design of a reflective middleware prototype named RECOM. Whereafter, this paper details the implementation of RECOM about its binding factories, reflective structure, and configurable reflective layers. Finally, some related work is discussed, and some concluding remarks and topics for further study are presented.","Middleware,
Reflection,
Prototypes,
Bandwidth,
Production facilities,
Educational institutions,
Computer science,
Design engineering,
Software design,
Technology management"
QoS routing with mobility prediction in MANET,"A Mobile Ad hoc NETwork (MANET) is a collection of wireless mobile hosts that form a temporary network without a centralized administration or wired infrastructure. Because of its dynamic topology, a MANET's routing protocol is different from other networks. Network control with Quality of Service (QoS) support is a key issue for multimedia applications in MANET. This paper proposes QoS Routing with Mobility Prediction (QRMP) protocol. This protocol selects the most stable path based on mobility prediction and QoS requirements on bandwidth and delay. The impact of our improvements is evaluated through simulations. The protocol is expected to efficiently support real-time multimedia traffic with different QoS requirements.",
Face detection based on template matching and support vector machines,"A face detection algorithm integrating template matching and support vector machines (SVM) is presented. Two types of templates: eyes-in-whole and face itself, are used for coarse filtering, and the SVM classifier is used for classification. A bootstrap method is used to collect non-face samples for SVM training under a template matching constrained subspace, which greatly reduces the complexity of training the SVM. Comparative experimental results demonstrate its effectiveness.","Face detection,
Support vector machines,
Support vector machine classification,
Clustering algorithms,
Matched filters,
Filtering algorithms,
Risk management,
Computer science,
Filtration,
Subspace constraints"
UML: an evaluation of the visual syntax of the language,"Examination of the UML indicates weaknesses in its graphic syntax which undermine its structure as a visual language. Although the UML Notation claims to provide a ""canonical notation"", there are insufficient rules governing the graphic constructs used to produce the essential 'signifiers' of this visual language and to define their permissible combinations. The nature and composition of the graphical elements actually shown is a fundamental consideration, separate from the underlying constructs that they may signify. A much earlier formulation for notational systems, that provided by Nelson Goodman, clarifies the issues involved and makes it possible to set basic tests for a notational scheme, such as the UML, which require syntactic disjointness and differentiability. Application of these tests (plus others) to graphical primitives, simple characters and diagrams shows a variety of failures that lead to a fundamental questioning of the graphical syntax which forms part of the UML structure as a language.","Unified modeling language,
Object oriented modeling,
Graphics,
System testing,
Computer languages,
Software engineering,
Software systems,
Humans"
Observational emergence of a fuzzy controller evolved by genetic algorithm,"Explaining emergence is a difficult work, such that there are many arguments on what it is or how it can be explained. Nonetheless, it is frequently referred to in many fields, such as behavior-based robotics, artificial life and complex systems, without any formal definition. In this paper, we develop a fuzzy logic controller for a simulated mobile robot with a genetic algorithm and analyze the behavior of the controller from the perspective of observational emergence. The analysis shows that the fuzzy logic controller has acquired emergent behavior through the interactions of the underlying fuzzy rules.","Fuzzy control,
Genetic algorithms,
Fuzzy logic,
Infrared sensors,
Testing,
Robot sensing systems,
Computer science,
Mobile robots,
Algorithm design and analysis,
Connectors"
"Satisfaction measure for result in fuzzy reasoning and retrieval - an attempt towards the application of fuzzy logic as the ""brainware"" of the Internet","Reasoning and retrieval based on uncertain information and imprecise knowledge is often requested in designing and developing intelligent information systems. As a recent trend, the ideas of fuzzy logic have also been widely used in searching the Internet. An important issue for these applications is to equip the reasoning and retrieval with the capability of transmitting the uncertainty from imprecise requirements to a conclusion at a reasonable level of belief and satisfaction. In this paper, we discuss the concept of a satisfaction measure in fuzzy reasoning and retrieval.","Fuzzy reasoning,
Information retrieval,
Internet,
Application software,
Fuzzy logic,
Fuzzy sets,
Computer science,
Intelligent systems,
Information systems,
Databases"
Using orthogonal visual servoing errors for classifying terrain,"A novel, centimeter-scale crawling robot has been developed to address applications in surveillance, search-and-rescue, and planetary exploration. This places constraints on size and durability that minimizes the mechanism. As a result, a dual-use design employing two arms for both manipulation and locomotion was conceived. In a complementary fashion, this paper investigates the dual-use of visual servoing error. Visual servoing can be used by a mobile robot for homing and tracking. But because ground-based mobile robots are inherently planar, the control methodology (steering) is one-dimensional. The two-dimensional nature of image-based servoing leaves additional information content to be used in other contexts. We explore this information in the context of classifying terrain conditions. An outline for gait adaptation based on this is suggested for future work.","Visual servoing,
Arm,
Mobile robots,
Manipulators,
Orbital robotics,
Computer science,
Surveillance,
Mechanical engineering,
Computer errors,
Application software"
A capacity and utilization study of mobile ad hoc networks,"We develop an empirical technique to determine the capacity of a mobile ad hoc network. We assume that the network runs on-demand routing and carrier sense-based medium access protocols. The technique, however, is general and should apply to other types of protocols. We develop a tool that determines the network capacity given session-level traffic and node mobility traces. We compare the network capacity so determined with the actual network utilization from output statistics generated by a comprehensive simulator. It is observed that network capacity increases with node mobility; but the routing and medium access protocols fail to take advantage of the increased capacity. Even with high loads, a significant portion of the network capacity is not utilized, while the routing performance remains poor. This study indicates that there is a significant scope for designing aggressive routing protocols that utilize the network capacity better to improve routing performance.","Mobile ad hoc networks,
Routing protocols,
Access protocols,
Media Access Protocol,
Ad hoc networks,
Telecommunication traffic,
Computer science,
Statistics,
Base stations,
Spread spectrum communication"
Efficient metering schemes with pricing,"In order to decide on advertisement fees for Web servers, Naor and Pinkas (see Proc. Advances in Cryptology-EUROCRYPT'98 (Lecture Notes in Computer Science). New York: Springer-Verlag, vol.1403, p.576-590, 1998) introduced metering schemes. They proposed metering schemes in which any server is able to compute a proof to be sent to an audit agency if and only if it has been visited by at least a certain number, say h, of clients. In such schemes, any server which has been visited by less than h clients has no information about the proof; consequently, it does not receive any money from the audit agency. In order to have a more flexible payment system, Blundo, De Bonis, and Masucci (see Proc. 4th Int. Symp. Distributed Computing-2000 (Lecture Notes in Computer Science). New York: Springer-Verlag, vol.1914, p.194-208,2000) introduced metering schemes with pricing. These schemes allow different rates of payments based on the number of visits that each server has received. In this paper, we are interested in the efficiency of metering schemes with pricing. We propose a new model for metering schemes with pricing and we provide lower bounds on the size of the information distributed to clients and servers, and on the number of random bits needed by the audit agency to set up a metering scheme with pricing. These bounds are tight, as we provide a scheme which achieves them with equality. Compared to the scheme presented by Blundo, De Bonis, and Masucci, our scheme distributes less information to clients and servers. The drawback of our scheme is that it requires servers to interact with the audit agency in order to compute their proofs.",
Efficient computation of marginal reliability-importance for reducible/sup +/ networks,"Marginal reliability importance (MRI) of a link with respect to terminal-pair reliability (TR) is the rate to which TR changes with the modification of the success probability of the link. It is a quantitative measure reflecting the importance of the individual link in contributing to TR of a given network. Computing MRI for general networks is an NP-complete problem. Attention has been drawn to a particular set of networks (reducible networks), which can be simplified to source-sink (2-node) networks via 6 simple reduction rules (axioms). The computational complexity of the MRI problem for such networks is polynomial bounded. This paper proposes a new reduction rule, referred to as triangle reduction. The triangle reduction rule transforms a graph containing a triangle subgraph to that excluding the base of the triangle, with constant complexity. Networks which can be fully reduced to source-sink networks by the triangle reduction rule, in addition to the 6 reduction rules, are further defined as reducible/sup +/ networks. For efficient computation of MRI for reducible/sup +/ networks, a 2-phase (2-P) algorithm is given. The 2-P algorithm performs network reduction in phase 1. In each reduction step, the 2-P algorithm generates the correlation, quantified by a reduction factor, between the original network and the reduced network. In phase 2, the 2-P algorithm backtracks the reduction steps and computes MRI, based on the reduction factors generated in phase 1 and a set of closed-form TR formulas. As a result, the 2-P algorithm yields a linearly bounded complexity for the computation of MRI for reducible/sup +/ networks. Experimental results from real networks and benchmarks show the superiority, by two orders of magnitude, of the 2-P algorithm over the traditional approach.","Computer networks,
Magnetic resonance imaging,
Reliability engineering,
NP-complete problem,
Computational complexity,
Polynomials,
Computer network reliability,
Computer science,
Joining processes,
Helium"
A multiconstraint QoS routing scheme using the depth-first search method with limited crankbacks,"We propose a multiconstraint QoS routing scheme using the DFS (depth-first search) method to sequentially search for a feasible routing path. Although the standard sequential search approach has several advantages, it has not been explored for multiconstraint QoS routing in the literature due to its asymptotic exponential worst-case time complexity. Our scheme limits the number of crankbacks per node (crankback degree) to control the worst-case time complexity. The scheme uses a metric, called the normalized margin, to reduce the average-case time complexity by first exploring the paths that are most likely to be feasible. We further tune the crankback degree based on correlation coefficients between QoS attributes to reduce the worst-case time complexity. We use simulations to show that our scheme solves multiconstraint QoS routing problems with a small probability of missing a feasible path while keeping the average-case time complexity low.","Routing,
Search methods,
Computer science education,
Computer security,
Information security,
Operating systems,
Polynomials,
Indium tin oxide,
Contracts,
Chromium"
Teaching computational methods for partial differential equations using the Web,"Computational methods are part of the problem solving skills that professionals working in quantitative fields need. Advanced text-books provide the mathematical foundation for one specific approach, however, they miss the overview and examples that are generally necessary to choose the right method and implement a practical solution. Internet technology can be of great value in this context. Thus, we created a problem-based learning environment where knowledge is acquired by performing well-defined tasks. We also switched from a teaching-centered course to a learning-centered course; the students choose their own focus, order, and pace as they explore the material (http://pde.fusion.kth.se). Since 1997, the concept has been validated three times in summer courses with about 15 participants geographically dispersed around Stockholm and Goteborg (Sweden). It is now being tested year round with distance learning students on the Web.","Education,
Partial differential equations,
Monte Carlo methods,
Java,
Problem-solving,
Joining processes,
Algebra,
Gaussian distribution,
Electronic publishing,
Algorithm design and analysis"
PROFS-performance-oriented data reorganization for log-structured file system on multi-zone disks,"I/O is a major performance bottleneck in modern computer systems. Modern disks use zone-bit-recording (ZBR) technology to increase the capacity. A direct consequence of ZBR is that outer tracks have higher data transfer rates. Because LFS uses large disk transfers, access times are mainly determined by the data transfer rate. This paper presents a novel performance-oriented data reorganizing scheme, called PROFS, which boosts the I/O performance of LFS (log-structured file system). Our scheme reorganizes data on the disk during LFS garbage collection and system idle periods. By putting active data in the faster zones and inactive data in the slower zones, we can significantly improve the read and write performance. The reorganization overhead is minimal. We studied three different algorithms that are read-optimized, write-optimized and balanced. Simulation results based on five real-world and two synthetic traces showed that such algorithms can dramatically reduce the I/O response time for both reads and writes compared with the previous LFS systems with adaptive methods. Our algorithms improve the LFS system's write latency by up to 24.5% and the read latency by up to 21.3%. They also improve the file system throughput by up to 32%.","File systems,
Delay,
Disk recording,
Computer science,
Adaptive systems,
Throughput,
Disk drives,
Engine cylinders,
Performance gain,
Knowledge based systems"
3DE: an environment for the development of learner-centered custom educational packages,"The usual approach for interactive multimedia courses is to build teacher or institution-driven 'commodity' courses. Better results can be achieved with courses customized for the requirements and needs of each learner. This is the approach of 3DE (Design, Development and Delivery-Electronic Environment for Educational Multimedia), a project aimed to define, design, and build personalized learning packages, tailor-made for individual learners. These custom courses are built from a library of micromodules worked out specifically for different learning styles. The goals are to improve the effectiveness of computer-based teaching and training packages, and to reduce the cost for the development of high-quality interactive multimedia educational packages, thanks to reuse of micromodules and the automated course assembly procedures. The environment include tools to analyze learning style preferences, competence, and educational goals in terms of final competence and skills.","Costs,
Consumer electronics,
Electronics packaging,
Libraries,
Education,
Assembly,
Production,
Multimedia systems,
Design engineering,
Standards development"
Stability analysis of the sequential partial update LMS algorithm,"Partial updating of LMS filter coefficients is an effective method for reducing the computational load and the power consumption in adaptive filter implementations. The sequential partial update LMS algorithm is one popular algorithm in this category. A first-order stability analysis of this algorithm was performed (Douglas, 1997) on wide sense stationary signals under the restrictive assumption of small step size parameter /spl mu/. The necessary and sufficient condition derived on /spl mu/ for convergence in the mean was identical to the one for guaranteeing stability in the mean of LMS. First-order sufficient conditions were derived (Godavarti et al., 1999) for stability without the aforementioned small /spl mu/ assumption. The sufficient region of convergence derived was smaller than that of regular LMS. In this paper, we establish that for stationary signals the sequential algorithm converges in mean for the same values of the step size parameter /spl mu/ for which the regular LMS does. In other words, we show that the conclusion drawn Douglas holds without the restrictive assumption of small /spl mu/. We also derive sufficient conditions for stability on /spl mu/ for cycle-stationary signals.",
"Kara, finite state machines, and the case for programming as part of general education","As a major evolutionary step in computer technology, users have come to rely on ready-made application software, rather than writing their own programs. If computer users no longer program, does it follow that the art of programming should only be taught to computing professionals? We argue the case for programming as a component of general education - not because of any direct utilitarian benefit, but in order to gain a personal experience as to what it means, and what it takes, to specify processes that evolve over time. An analogy to mathematics education shows that schools teach the concept of ""proof"", although in daily life people use mathematical formulas without knowledge of their proof. Programming practiced as an educational exercise, free from utilitarian constraints, is best learned in a toy environment, designed to illustrate selected concepts in the simplest possible setting. As an example, we present the programming system Kara based on the concept of finite state machines.",
3D edge detection to define landmarks for point-based warping in brain imaging,"The accurate comparison of inter-individual 3D image datasets of brains requires warping techniques to reduce geometric variations. In this study we use a point-based method of warping with weighted sums of displacement vectors, which is extended by an optimization process. To improve the practicability of 3D warping, we investigate 3D operators as landmark detectors for the applicability to our image datasets. The combined approach was tested on 3D autoradiographs of brains of Mongolian gerbils. The warping function is distance-weighted with landmark-specific weighting factors. These weighting factors are optimized by a computational evolution strategy. Within this optimization process the quality of warping is quantified by the sum of spatial differences of manually predefined registration points (registration error). The described approach combines a highly suitable procedure to detect landmarks in brain images and a point-based warping technique, which optimizes local weighting factors.","Image edge detection,
Brain,
Optimization methods,
Humans,
Acoustic imaging,
Visualization,
Detection algorithms,
Animals,
Computer science,
Detectors"
Integrated floorplanning and power supply planning,"The power supply planning is very important for high performance VLSI design. An algorithm is proposed, which deals with the design and optimization of tree-based power/ground network in the BBL-based VLSIs. The object of the algorithm is to minimize the routing area used by a power tree. This paper presented an integrated floorplanning and power supply planning algorithm for BBL-based VLSI, which minimizes the chip area used by both blocks and power networks. The algorithm solves two problems. The first is the overestimation problem: only routing area used by the power tree is reserved. The second is the constraints dissatisfactory problem: if no feasible power tree satisfies the constraints in a floorplan, the floorplan will be changed. Experimental results on MCNC benchmarks show promising performance.","Power supplies,
Routing,
Very large scale integration,
Design optimization,
Computer science,
Algorithm design and analysis,
Circuits,
Path planning,
Voltage,
Network topology"
Flexible geometries for hand-held PET and SPECT cameras,"Minimally invasive surgery has spurred development of portable nuclear medicine nonimaging detection products (i.e., probes). Although these probes function well in simple anatomic regions (e.g., extremities), more complicated regions (e.g., head & neck, axilla) can present surgeons with difficulties due to the presence of overlapping tissue activity. Conventional gamma cameras, with large camera heads, are not ideal for intraoperative applications. Due to the bulky camera size, PET and SPECT is difficult to implement in OR settings. Although small gamma cameras may be helpful when a lesion is already identified, surveillance of larger areas (e.g., for peritoneal evaluation) can be difficult due to limited field-of-view. Ideally, the information from small hand-held gamma cameras would be integrated from multiple views taken at arbitrary positions and angles. We present a method of creating an image with 3D information from one or more portable hand-held gamma cameras held in arbitrary positions and angles near a radioactive source (patent pending). The method takes advantage of recent developments in iterative reconstruction algorithms, computer speed, and position sensing devices. Corrections were implemented for inhomogeneous sampling in time and space.","Geometry,
Positron emission tomography,
Cameras,
Minimally invasive surgery,
Probes,
Head,
Nuclear medicine,
Extremities,
Neck,
Axilla"
Bridging the cultural chasm: Improving collaboration and cooperation between the computer and social sciences,"This paper examines the cultural chasm between computer scientists and social scientists and suggests ways and benefits of closer interaction and possible collaboration. Specifically, it presents exploratory research on the attitudes of computer scientists concerning the need for social research in computer system design, the efficacy of social science and the utility of social scientists in this endeavor, and finally the real barriers to crossing what has been termed the great divide. It is based on in-depth, ethnographic interviews conducted in 2001 with 30 academics and practitioners from a research university, a national laboratory, and industry sites in new Mexico. It finds computer scientists, though they recognize the benefits of social science, seldom seek out social scientists to collaborate and know little about social science.","Cultural differences,
Collaboration,
Information technology,
Robotics and automation,
Humans,
Telecommunication computing,
Business communication,
Environmental management,
Technology management,
Government"
New iterative algorithms and architectures of modular multiplication for cryptography,"Algorithms and architectures for performing modular multiplication operations, which is central to crypto-system and authentication schemes, are important in today's needs of secure communications. This paper presents two new iterative algorithms for modular multiplication. The implementation of these algorithms yields to scalable architectures that can be used for any modulus without altering the design. In addition, the Radix-2 algorithm shows almost similar features when compared with similar architectures available in the literature. Furthermore, the radix-4 algorithm can be used to develop higher radix algorithms since it only requires the use of powers of two of the modulus.",
Quality of service and object-oriented middleware - multiple concerns and their separation,"Quality of service (QoS) is an important requirement of today's distribution infrastructures. The popularity of object-oriented middleware makes QoS provision in those infrastructures desirable. The system-dependent nature of QoS necessitates a clear separation of concerns and insulation from the application objects. In this paper, QoS integration is investigated from two viewpoints. First, based on the aspect-oriented programming, the integration of QoS mechanisms into the application objects is presented, and second, the hierarchical structure of QoS mechanisms is addressed in a reflection-based approach.","Quality of service,
Middleware,
Application software,
Computer science,
Real time systems,
Fault tolerant systems,
Logic,
Programming profession,
Mechanical factors,
Quality management"
M-commerce - mobile commerce: a new frontier for E-business,,"Business,
Mobile computing,
Paper technology,
Information systems,
Space technology,
Multiaccess communication,
Wireless application protocol,
Security,
Design methodology,
Computer science"
Reliable multicast via satellites,"Automatic repeat request (ARQ) is a well-known technique to provide error control. In reliable satellite multicasting, ARQ may reduce system throughput as the number of receivers increases since the satellite has to retransmit a packer until all receivers correctly receive it. This performance degradation might be alleviated substantially by conducting retransmissions through terrestrial paths from the sender to each receiver instead of through the multicast satellite link. However, this approach may not be possible when some receivers do not have terrestrial connections to the sender. Also, a large volume of feedback traffic from the receivers to the sender may cause a feedback implosion at the sender. In this paper, we propose a scalable framework to solve these problems. Our solution enables receivers to form groups to avoid unnecessary retransmissions as long as a correct packet call be found inside the group. Using analytical models, we demonstrate that the proposed solution can significantly improve system throughput.",
Abstraction of word-level linear arithmetic functions from bit-level component descriptions,RTL descriptions for word-level arithmetic components typically specify the architecture at the bit-level of the registers. The problem studied in this paper is to abstract the word-level functionality of a component from its bit-level specification. This is particularly useful in simulation since word-level descriptions can be simulated much faster than bit-level descriptions. Word-level abstractions are also useful for reducing the complexity of component matching since the number of words is significantly smaller than the number of bits. This paper presents an algorithm for abstraction of word-level linear functions from bit-level component descriptions. We also present complexity results for component matching which justifies the advantage of performing abstraction prior to component matching.,
Protocol switching: exploiting meta-properties,"As we see a growing variety of network and application behaviors, it becomes more important that protocols adapt to their surroundings. Building adaptive protocols is complicated, and therefore we have considered building hybrid protocols that switch between specialized protocols. We show for which communication properties this is a correct solution, and classify these using a new concept called meta-properties. We also show how well these switches perform.","Communication switching,
Switches,
Runtime,
Logic,
Multicast protocols,
Computer science,
Windows,
Energy consumption,
Communication system security,
Intrusion detection"
Rapid prototyping of computer systems: experiences and lessons,"Carnegie Mellon University has developed a user-centered interdisciplinary concurrent system design methodology (UICSM) that takes teams of electrical engineers, mechanical engineers, computer scientists, industrial designers and human-computer interaction students that work with an end-user to generate a complete prototype system during a four-month-long course. The methodology is Web-supported and defines intermediary design products that document the evolution of the design. These products are posted on the Web so that even remote designers and end-users can participate in the design activities. The design methodology proceeds through three phases: conceptual design, detailed design and implementation. End-users critique the design at each phase. In addition, simulated and real application tasks provide further focus for design evaluation. The methodology has been used by the class, in designing over a dozen wearable computers, with diverse applications ranging from inspection and maintenance of heavy transportation vehicles to augmented reality in manufacturing and plant operations. The methodology includes monitoring and evaluation of the design process. This methodology is illustrated through a description of developing pervasive computing applications in collaboration with IBM during the Spring 2000 course.","Prototypes,
Application software,
Design engineering,
Concurrent computing,
Computer industry,
Electrical products industry,
Product design,
Design methodology,
Computational modeling,
Wearable computers"
Learning statistically efficient features for speaker recognition,"We apply independent component analysis for extracting an optimal basis to the problem of finding efficient features for a speaker. The basis functions learned by the algorithm are oriented and localized in both space and frequency, bearing a resemblance to Gabor functions. The speech segments are assumed to be generated by a linear combination of the basis functions, thus the distribution of speech segments of a speaker is modeled by a basis, which is calculated so that each component should be independent upon others on the given training data. The speaker distribution is modeled by the basis functions. To assess the efficiency of the basis functions, we performed speaker classification experiments and compared our results with the conventional Fourier-basis. Our results show that the proposed method is more efficient than the conventional Fourier-based features, in that they can obtain a higher classification rate.",
An efficient QoS routing algorithm for quorumcast communication,"This paper extends the concept of multicast to quorumcast, a generalized form of multicast communication. The need of quorumcast communication arises in a number of distributed applications. Little work has been done on routing quorumcast messages. The objective of previous research was to construct a minimum cost tree spanning the source and the quorumcast group members. We further consider the path quality of a constructed spanning tree in terms of delay constraints required by applications that use the tree. As the delay-constrained quorumcast routing problem is NP-complete, we propose an efficient heuristic QoS routing algorithm. We also consider how a loop is detected and removed in the course of tree construction and how to deal with members joining/leaving the quorumcast pool. Our simulation study shows that the proposed algorithm performs well and constructs a quorumcast tree whose cost is close to that of the ""optimal"" routing tree.","Routing,
Delay,
Costs,
Multicast algorithms,
Computer science,
Multicast communication,
Distributed databases,
Quality of service,
Application software,
Heuristic algorithms"
Formal verification of a microprocessor control,"The complexity of the instruction set of modern processors often leads to faults in the microinstruction sequencing, and timing errors, which are difficult to detect with conventional simulation methods. Formal verification offers a powerful alternative for dealing with these problems. In this paper we present a mathematical model of the microcode of a transputer-like microprocessor, and demonstrate how to test for the satisfaction of desired properties and the absence of improper microinstruction sequencing. The verification is based on a recently introduced technique using the inductively defined notion of series parallel posets, which offers low time and space complexity.","Formal verification,
Microprocessors,
Timing,
Mathematical model,
Power system modeling,
Protocols,
Modems,
Circuit faults,
Hardware,
Computer science"
The lightweight protocol CLIC: performance of an MPI implementation on CLIC,,
Efficient generalized deadlock detection and resolution in distributed systems,"Presents a distributed algorithm for detecting generalized deadlocks in distributed systems. The algorithm constructs a distributed spanning tree through the propagation of probes and receiving replies from those probes. The initiator of the algorithm builds a local wait-for graph to determine the existence of deadlock. A scheme for encoding the path information from the initiator to each process is developed so that ancestor-descendant relationships between the tree nodes are not explicitly sent to the initiator but are inferred at the initiator. The advantages of the proposed algorithm are: (1) all deadlocks reachable from the initiator are resolved, whereas current algorithms detect deadlock only if the initiator is in deadlock; (2) deadlock resolution is simplified without additional message transmission, due to the availability of dependency relations among processes at the initiator; and (3) a unique property of the algorithm is that it handles concurrent algorithm executions and prevents duplicate deadlock detection which may cause false deadlock resolution, whereas most deadlock detection algorithms ignore this issue and deal with a single execution of the algorithm. In addition, our scheme provides a solution to the problem of G. Bracha et al.'s (1987) algorithm that may not detect a deadlock if the lower-priority execution is simply discarded. Our algorithm performs better than or comparably to the current best algorithms in terms of both message and time complexities.","System recovery,
Computer science education,
Tree graphs,
Detection algorithms,
Distributed computing,
Postal services,
Distributed algorithms,
Probes,
Encoding,
Systems engineering and theory"
Length-restricted coding in static and dynamic frameworks,This paper describes variants of a recent length-restricted coding technique for use in static and dynamic frameworks. The resulting compression systems are shown to have identical asymptotic time complexity and also competitive performance to the corresponding unrestricted systems.,"Arithmetic,
Computer science,
Software engineering,
World Wide Web,
Encoding,
Decoding,
Packaging,
Frequency,
Probability distribution,
Australia"
Fuzzy constraints using the enhanced entity-relationship model,"Our aim is to relax the constraints which can be expressed in a conceptual model using the enhanced entity-relationship (EER) modelling tool, so that these constraints can be made more flexible. To do so, we use the fuzzy quantifiers which have been widely studied in the context of fuzzy sets and fuzzy consultation systems for databases. We also examine the representation of these constraints in an EER model and their practical repercussions. The constraints proposed are as follows: a fuzzy participation constraint, a fuzzy cardinality constraint, a fuzzy completeness constraint in the representation of classes and sub-classes (and categories or union types) and a fuzzy cardinality constraint in overlapping specialization. We also demonstrate how a fuzzy (min, max) notation can substitute for the fuzzy cardinality constraints. All these fuzzy constraints have a novel meaning and offer greater expressiveness in conceptual design.",
How to teach electronics packaging technology?,"The preparation of engineers for the 21st century's requirements of rapidly developing information technology as well as microelectronics and electronics packaging technology is a real challenge for education. An approach that teaches electronics packaging technology and associated topics, providing lecture courses, lab sessions and independent research studies supported by a prototype manufacturing environment can hopefully achieve the main goals. Supporting projects are also essential for the department in order to develop the prototyping laboratories. From a teaching viewpoint, the advanced educational structure and the application of appropriate didactic methods has great importance. The Faculty of Electrical Engineering and Informatics at the Budapest University of Technology and Economics retained both the electronics and the technical informatics topics in the curriculum that enables teaching of both topics on a wider horizon. The curriculum has three levels. In the first semester, natural sciences and computer programming are taught. In the middle of the curriculum, emphasis is laid on the core knowledge of electronics and information technology. In the final semester, specific courses are organized to teach current information on selected subjects. The teaching of electronics packaging technology meets the requirements of this three-level approach. A method developed by the Department of Electronics Technology (BME-ETT) is also demonstrated which makes education in electronics packaging technology more efficient.",
Efficient recovery information management schemes for the fault tolerant mobile computing systems,"This paper presents region-based storage management schemes, which support the efficient implementation of checkpointing and message logging for fault tolerant mobile computing systems. In the proposed schemes, a recovery manager assigned for a group of cells takes care of the recovery for the mobile hosts within the region. As a result, the recovery information of a mobile host, which may be dispersed over the network due to the mobility of the host, can efficiently be handled.","Information management,
Fault tolerant systems,
Mobile computing,
Costs,
Checkpointing,
Mobile communication,
Environmental management,
Computer science,
Engineering management,
Space stations"
Beyond the Informedia digital video library: video and audio analysis for remembering conversations,"The Informedia Project digital video library pioneered the automatic analysis of television broadcast news and its retrieval on demand. Building on that system, we have developed a wearable, personalized Informedia system, which listens to and transcribes the wearer's part of a conversation, recognizes the face of the current dialog partner and remembers his/her voice. The next time the system sees the same person's face and hears the same voice, it can retrieve the audio from the last conversation, replaying in compressed form the names and major issues that were mentioned. All of this happens unobtrusively, somewhat like an intelligent assistant who whispers to you: ""That's Bob Jones from Tech Solutions; two weeks ago in London you discussed solar panels"". This paper outlines the general system components as well as interface considerations. Initial implementations showed that both face recognition methods and speaker identification technology have serious shortfalls that must be overcome.","Software libraries,
Digital video broadcasting,
Digital audio broadcasting,
Face recognition,
Speech recognition,
Space technology,
Global Positioning System,
Computer science,
TV broadcasting,
Automatic speech recognition"
A graph-spectral method for surface height recovery from needle-maps,"The paper describes a graph-spectral method for 3D surface integration. The algorithm takes as its input a 2D field of surface normal estimates, delivered, for instance, by a shape-from-shading or shape-from-texture procedure. The method borrows ideas from routing theory. We exploit the well-known fact that the leading eigenvector of a Markov chain transition probability matrix is the steady-state random walk on the equivalent weighted graph. We use this property to find the minimum total curvature path through the available surface normals. To do this, we construct a transition probability matrix whose elements are related to the differences in surface normal direction. By threading the surface normals together along the path specified by the magnitude order of the components of the leading eigenvector, we perform surface integration. The height increments along the path are simply related to the traversed path length and the slope of the local tangent plane. The method is evaluated on data delivered by a shape-from-shading algorithm.","Surface reconstruction,
Routing,
Computer science,
Surface structures,
Image reconstruction,
Integral equations,
Constraint theory,
Shape,
Graph theory,
Information retrieval"
Fading timescales associated with GPS signals and potential consequences,"The effect of equatorial ionospheric scintillations on the operation of GPS receivers is investigated, with special attention given to the effect of scintillation timescales on the code division multiple access (CDMA) protocol used by GPS. We begin by examining the timescales of scintillation fades modeled as a horizontally drifting pattern whose timescales are determined by the Fresnel length and the drift speed. The model is tested by comparing the speed, determined by dividing the Fresnel length by the autocorrelation time (width), with the speed estimated using spaced receivers, and the two independent estimates of speed are shown to possess a linear relationship. Next we show that the scintillation pattern drift speed is given by the difference of the ionospheric drift and the speed of the GPS signal F region puncture point. When the ionosphere and GPS signal puncture point speeds match, the fade timescales lengthen.; Additionally, if the fade depth is adequate, during periods of longer fade times the loss of receiver lock on GPS signals is more likely, as shown in several examples; that is, both larger fade depths and longer fade timescales are required to produce loss of tracking. We conclude by demonstrating that speed matching or resonance between the ionosphere and receiver is most likely when the receiver is moving from west to east at speeds of 40-100 m/s (144-360 km/h). This is in the range of typical aircraft speeds.",
Weighting of constraints in fuzzy optimization,"Many practical optimization problems are characterized by some flexibility in the problem constraints, where this flexibility can be exploited for additional trade-off between improving the objective function and satisfying the constraints. Fuzzy sets have proven to be a suitable representation for modeling this type of soft constraints. The paper proposes an extension of this model for satisfying the problem constraints and the goals, where preference for different constraints and goals can be specified by the decision-maker. The difference in the preference for the constraints is represented by a set of associated weight factors, which influence the amount of trade-off between improving the optimization objectives and satisfying various constraints. Simultaneous weighted satisfaction of the problem constraints and goals are demonstrated by using a fuzzy linear programming problem.","Constraint optimization,
Fuzzy sets,
Linear programming,
Constraint theory,
Decision making,
Fuzzy set theory,
Computer science,
Mechanical engineering,
Optimization methods,
Design optimization"
JR: flexible distributed programming in an extended Java,"Java provides a clean object-oriented programming model and allows for inherently system-independent programs. Unfortunately, Java has a limited concurrency model, providing only threads and remote method invocation (RMI). The JR programming language extends Java to provide a rich concurrency model. JR provides dynamic remote virtual machine creation, dynamic remote object creation, remote method invocation, asynchronous communication, rendezvous, and dynamic process creation. JR programs are written in an extended Java and then translated into standard Java programs. The JR run-time support system is also written in standard Java. This paper describes the JR programming language and its implementation. Some initial measurements of the performance of the implementation are also included.","Java,
Object oriented modeling,
Concurrent computing,
Strontium,
Object oriented programming,
Computer languages,
Dynamic programming,
Yarn,
Virtual machining,
Computer science"
Storing semistructured data in relational databases,,"Relational databases,
Data mining,
Web pages,
XML,
Weather forecasting,
HTML,
Assembly,
Computer science,
Data models,
Information retrieval"
Compositional abstractions of hybrid control systems,"Abstraction is a natural way to hierarchically decompose the analysis and design of hybrid systems. Given a hybrid control system and some desired properties, one extracts an abstracted system while preserving the properties of interest. Abstractions of purely discrete systems is a mature area, whereas abstractions of continuous systems is a recent activity. We present a framework for abstraction that applies to abstract control systems capturing discrete, continuous, and hybrid systems. Parallel composition is presented in a categorical framework and an algorithm is proposed to construct abstractions of hybrid control systems. Finally, we show that our abstractions of hybrid systems are compositional.",
The Women in Applied Science and Engineering Summer Bridge Program: easing the transition for first-time female engineering students,"The Women in Applied Science and Engineering (WISE) Summer Bridge Program is designed to prepare incoming female students for the transition from high school to the College of Engineering and Applied Sciences (CEAS) at Arizona State University (ASU). This program offers academic reviews in courses such as mathematics, physics, and chemistry. Computer programming tutorials are also offered in Excel and HTML to better prepare students for their freshman introductory engineering course. Participants acclimate to the campus by receiving general information concerning the university, financial aid, and departmental advising. Students attending the program become familiar with the campus, have a head start on their freshman engineering classes, and have a chance to meet other female students. An overview of the WISE Summer Bridge Program is presented as well as retention data for 1998 and 1999 program participants. In addition, the paper discusses the need for and impact of bridge programs specifically geared toward female students. Further, the paper investigates other life circumstances, such as level of involvement in student activities, living situation, and employment that impact retention of these students. Finally, future projections of implementation and direction of student retention programs are explored.","Bridges,
Design engineering,
Educational institutions,
Mathematics,
Physics,
Chemistry,
Programming,
HTML,
Student activities,
Employment"
Programmability and performance of M++ self-migrating threads,,"Yarn,
Biological system modeling,
Computational modeling,
Computer networks,
Parallel programming,
Computer science,
Biology computing,
Scalability,
Switches,
Software systems"
An evolutionary method of training topography-preserving maps,"In this paper, we introduce an evolutionary training method that can be used either to replace standard self-organizing map (SOM) training or to post-process a trained SOM. The approach is motivated by a desire to improve the way a map preserves relationships in the data beyond the presentation of continuity. There are two stages in the algorithm, prompted by the observation that there is conflict in standard SOM training between the goal of representing the probability distribution of the data and the goal of preserving topology between input and output (a conflict between competition and cooperation). By pursuing these two goals in two separate stages of training, we are able to focus on each goal individually and prevent each from impeding the other. The use of a genetic algorithm in the second stage determines the adjacencies of neurons in the output map grid and allows greater control over the way relationships between output neurons preserve the relationships found in the input data. This is important because it enhances the ability of the map to more accurately represent the structure of the input data. It may prove especially valuable when dealing with high-dimensional data, when one cannot visually inspect the map plotted in the data space to verify the quality of the mapping.",
Efficient fault-tolerant protocol for mobility agents in mobile IP,,"Fault tolerance,
Protocols,
Mobile computing,
Delay,
Forward contracts,
Scalability,
Peer to peer computing,
Checkpointing,
Intelligent networks,
Computer science"
The Center for Engineering Education at the Colorado School of Mines: using Boyer's four types of scholarship,"The Center for Engineering Education (CEE) at the Colorado School of Mines promotes both educational research and improvements in teaching. In order to connect the potentially competing activities of research and teaching, CEE has used Ernest Boyer's model of the four forms of scholarship to describe the Center's activities. According to Boyer, the word ""scholarship"" should not only describe the activities of those conducting original research (which he called the scholarship of discovery) but should also be extended to include the scholarships of integration, application, and teaching. This paper describes Boyer's model and provides examples of how these four forms of scholarship display themselves in CEE-sponsored projects.","Engineering education,
Scholarships,
Seminars,
Educational institutions,
Displays,
Technological innovation,
Systems engineering education,
Mathematical model,
Chemical engineering,
Computer science education"
DCAPS-architecture for distributed computer aided prototyping system,"This paper describes the architecture for the distributed CAPS system (DCAPS). The system accomplishes distributed software prototyping with legacy module reuse. Prototype System Description Language (PSL), the prototyping language, is used to describe real-time software in the DCAPS system. PSDL specifies not only real-time constraints, but also the connection and interaction among software components. Automatic generation of software wrappers and glue is applied for the normalization of data transfer between legacy systems. Implementation of the DCAPS communication layer is based on the JavaSpaces/sup TM/ library. DCAPS supports collaborative prototype design in a distributed environment.","Computer architecture,
Distributed computing,
Prototypes,
Software prototyping,
Programming,
Military computing,
Real time systems,
Control systems,
Computer science,
Software libraries"
Variational classification for visualization of 3D ultrasound data,"We present a new technique for visualizing surfaces from 3D ultrasound data. 3D ultrasound datasets are typically fuzzy, contain a substantial amount of noise and speckle, and suffer from several other problems that make extraction of continuous and smooth surfaces extremely difficult. We propose a novel opacity classification algorithm for 3D ultrasound datasets, based on the variational principle. More specifically, we compute a volumetric opacity function that optimally satisfies a set of simultaneous requirements. One requirement makes the function attain nonzero values only in the vicinity of a user-specified value, resulting in soft shells of finite, approximately constant thickness around isosurfaces in the volume. Other requirements are designed to make the function smoother and less sensitive to noise and speckle. The computed opacity function lends itself well to explicit geometric surface extraction, as well as to direct volume rendering at interactive rates. We also describe a new splatting algorithm that is particularly well suited for displaying soft opacity shells. Several examples and comparisons are included to illustrate our approach and demonstrate its effectiveness on real 3D ultrasound datasets.","Data visualization,
Ultrasonic imaging,
Rough surfaces,
Surface roughness,
Speckle,
Surface cleaning,
Data mining,
Isosurfaces,
Integral equations,
Computer science"
An intelligent tutoring system for teaching and learning Hoare logic,"Significant areas of the computer science curriculum are constantly and rapidly changing as new technologies (e.g., multimedia, WWW, Java) are adopted. This presents a challenge to our current education system: Intelligent Tutoring Systems (ITSs) for teaching and learning in Computer Science must be able to assist not only their student users but also the teachers in developing and managing courses to the best advantage of the students. We developed an ITS for teaching and learning Hoare logic. In this paper, we describe the practical use of the system in a computation theory course.","Intelligent systems,
Education,
Logic,
Computer science,
World Wide Web,
Java,
Sun,
Rain,
Tellurium,
Mathematics"
Finding code on the World Wide Web: a preliminary investigation,"To find out what kind of design structures programmers really use, we need to examine a wide variety of programs. Unfortunately, most program source code is proprietary and is unavailable for analysis. The World Wide Web (Web) potentially can provide a rich source of programs for study. The freely available code on the Web, if in sufficient quality and quantity, can provide a window into software design as it is practiced today. In a preliminary study of source code availability on the Web, we estimate that 4% of URLs contain object-oriented source code, and 9% of URLs contain executable code: either binary or class files. This represents an enormous resource for program analysis. We can, with some risk of inaccuracy, conservatively project our sampling results to the entire Web. Our estimate is that the Web contains at least 3.4 million files containing either Java, C++, or Perl source code, 20.3 million files containing C source code, and 8.7 million files containing executable code.","Web sites,
Application software,
Programming profession,
Uniform resource locators,
Sampling methods,
Computer science,
Availability,
Java,
Software quality,
Graphical user interfaces"
Experience-based learning of task representations from human-robot interaction,"We present an approach that allows a robot to learn task representations from its own experiences of interacting with a human. The robot follows a human teacher and maps its own observations of the environment into a representation of what has constituted the human's demonstration. The robot then builds a representation of the experienced task in the form of a behavior network. To enable this we introduce an architecture that extends the capabilities of behavior-based systems by allowing the representation and execution of complex and flexible sequences of behaviors. We demonstrate this architecture in a set of experiments in which a mobile robot learns representations for multiple tasks and is able to execute the tasks, even in changing environments.","Mobile robots,
Robotics and automation,
Human robot interaction,
Educational robots,
Computer architecture,
Computer science,
Education,
Automatic control,
Robustness,
Mars"
Efficient processing of large 3D meshes,"Due to their simplicity triangle meshes are often used to represent geometric surfaces. Their main drawback is the large number of triangles that are required to represent a smooth surface. This problem has been addressed by a large number of mesh simplification algorithms which reduce the number of triangles and approximate the initial mesh. Hierarchical triangle mesh representations provide access to a triangle mesh at a desired resolution, without omitting any information. In this paper we present an infrastructure for mesh decimation, geometric mesh smoothing, and interactive multiresolution editing of arbitrary unstructured triangle meshes. In particular, we demonstrate how mesh reduction and geometric mesh smoothing can be combined to provide a powerful and numerically efficient multiresolution smoothing and editing paradigm.","Smoothing methods,
Surface reconstruction,
Shape,
Data structures,
Computer science,
Graphics,
Reconstruction algorithms,
Topology,
Solid modeling,
Mesh generation"
Modelling with queues: an empirical study,"We review the principles of computer performance modelling with queues, and validate the models using the empirical data. We focus on modelling a special configuration of two computers working in tandem: a mirroring system. Distributed system under investigation has implemented as a series of Java applets communicating using the functions of java net.* package. The mirroring aspect of the system is modelled analytically with a difference queue. The difference queue is defined for a network of two queues in parallel, as the queue consisting of elements present in one queue and absent from the other queue. The queueing models under investigation are M/M/1, and M/D/1 queues. In our study we have observed that the service times were not completely random on a typical server and could not be approximated well by exponential distribution. Our results have shown that the service was almost (i.e. with a very small variance) deterministic. We have also found that the d-queue model approximates well the behaviour of the mirroring system under low load conditions, and also provides an upper bound on the delays under high load conditions.",
New architectures for serial-serial multiplication,"Traditional serial-serial multiplier structures suffer from an inefficient generation of partial products, which leads to hardware overuse and slow speed systems. In this paper, two new architectures for fully serial multiplication are presented. To the best of our knowledge, the first structure is the first fully serial multiplier reported in the literature with comparable performance-in terms of speed-to existing serial-parallel multipliers. The second structure requires an extra multiplexer in the clock path thus making it slower, but has the merit of reducing the latency of the multiplier. Both structures are systolic and need near communication links only. Compared with available architectures, an FPGA based implementation has shown an increase in the speed of the multipliers by about 200% for the first structure and 150% for the second structure.","Pins,
Field programmable gate arrays,
Digital signal processing,
Logic devices,
Computer architecture,
Hardware,
Clocks,
Delay,
Signal processing,
Computer science"
FDIMLP: A new neuro-fuzzy model,We present a neuro-fuzzy model called fuzzy discretized interpretable multi-layer perceptron (FDIMLP). Fuzzy rules are extracted in polynomial time with respect to the size of the problem and the size of the network. We applied our model to three classification problems of the public domain. It turned out that FDIMLP networks compared favorably with respect to EFuNN and ANFIS neuro-fuzzy systems.,"Neurons,
Multilayer perceptrons,
Humans,
Polynomials,
Fuzzy neural networks,
Uncertainty,
Fuzzy logic,
Computer science,
Intelligent systems,
Artificial intelligence"
Linear-time recognition of circular-arc graphs,"A graph G is a circular-arc graph if it is the intersection graph of a set of arcs on a circle. That is, there is one arc for each vertex of G, and two vertices are adjacent in G if the corresponding arcs intersect. We give a linear time bound for recognizing this class of graphs. When G is a member of the class, the algorithm gives a certificate in the form of a set of arcs that realize it.","Computer science,
Genetics,
DNA,
Testing"
Optimizations enabled by a relational data model view to querying data streams,"We postulate that the popularity and efficiency of SQL for querying relational databases makes the language a viable solution to retrieving data from data streams. In response, we have developed a system, dQUOB, that uses SQL queries to extract data from streaming data in real time. The high performance needs of applications such as scientific visualization motivates our search for optimizations to improve query evaluation efficiency. The purpose of this paper is to discuss the unique optimizations we have realized by a database point of view to streaming data and to show that the enhanced conceptual model of viewing data streams as relations has reasonable overhead.",
Improving Web database access using decision diagrams,"In some areas of management and commerce that are accelerated by advances in Web technologies, especially in electronic commerce (e-commerce), it is essential to support the decision-making process using formal methods. The problems of e-commerce applications include reducing the time of data access so that huge databases can be searched quickly, decreasing the cost of database design, etc. We present a decision diagram design application using an information-theoretical approach to improve database access speeds. We show that such a utilization provides systematic and visual ways of applying decision-making methods to simplify complex Web engineering problems.","Visual databases,
Decision making,
Application software,
Data mining,
Modems,
Artificial intelligence,
Computer science,
Technology management,
Business,
Electronic commerce"
A robust and fast watermarking scheme for compressed audio,,"Robustness,
Watermarking,
Decoding,
Internet,
Data security,
Masking threshold,
Digital audio players,
Psychoacoustic models,
Protection,
Computer science"
An approach to minimization of decision diagrams,"One of the most promising concepts which has been developed for efficient representation functions is Linearly Transformed Binary Decision Diagram (LTBDD). We present extensions to LTBDDs called Function-driven Decision Diagrams (fDDs). The notion of fDDs is based on using simple balanced (including nonlinear) Boolean functions for defining transformations of decision diagrams. In this context a new scheme of preprocessing which corresponds to inverse transformations as well as using composition of transformations are very efficient for minimization of fDDs. The first experimental results show that fDDs driven by nonlinear Boolean functions can be more compact than LTBDDs, with a reasonable cost. Further extensions of fDDs are also mentioned such as Function-driven Kronecker Functional Decision Diagrams and Multiple-Valued Function-driven Decision Diagrams.","Boolean functions,
Binary decision diagrams,
Data structures,
Cost function,
Circuit testing,
Computer science,
Information technology,
Minimization,
Design automation,
Digital circuits"
Introduction of mechatronics in pre-college programs and freshman design course in an active and cooperative learning framework,"The burgeoning field of ""Mechatronics"" has been evolving over the last few decades as a new field of pedagogy within the engineering curricula. ""Mechatronics"" synergistically combines engineering mechanics, electronics, and computer technology. The development may be attributed to the growing number of applications and product development at the interfaces of traditional disciplinary boundaries within the broad area of engineering education. Accreditation Board of Engineering and Technology (ABET) is facilitating such efforts by encouraging engineering programs to promote design activity in multidisciplinary teams among engineering students. In this paper student activities related to Summer Engineering Bridge Program (SEBP) and ""Introduction to Engineering Design"", a required course for all freshman engineering majors at University of Maryland Eastern Shore (UMES) are discussed. SEBP is a NASA sponsored pre-college program for high school students in transition to college. The program is designed for prospective engineering majors and is designed to bridge the gap between high school education and expected standards for freshman engineering majors. Both the SEBP and the Introduction to Engineering Design course requires students to participate in a ""mechatronic"" design project while cooperating with one another in teams of four or five members.","Mechatronics,
Pre-college programs,
Design engineering,
Educational institutions,
Bridges,
Application software,
Product development,
Engineering education,
Accreditation,
Engineering students"
Co-evolution in negotiation games,"In this paper, we focus on the mechanism of co-evolution in the negotiation situations. We formulate negotiation situations as hawk-dove games. It is an interesting question to answer how the society gropes its way towards equilibrium in an imperfect world when self-interested agents learn each other in order to improve the rules of interaction. It is known the mixed strategy will result in equilibrium in hawk-dove games, and both hawks and doves coexist. In this paper, we consider evolutionary dynamics with local matching and investigate the role of the mutual learning. We show that all agents gradually learn to behave as doves, which result in social efficiency.",
A crowd of Little Man Computers: visual computer simulator teaching tools,"This paper describes the use of a particular type of computer simulator as a tool for teaching computer architecture. The Little Man Computer (LMC) paradigm was developed by Stuart Madnick of MIT in the 1960s and has stood the test of time as a conceptual device that helps students understand the basics of how a computer works. With the success of the LMC paradigm, LMC simulators have also proliferated. We compare and contrast the current crowd of LMC simulators highlighting visual features. We found unexpected insights since despite starting with the same paradigm with the same goals, each implementation is distinct with different strengths and weaknesses. It is our intention that interested educators will find this a useful starting point or useful reference for incorporating simulation into their courses.",
Collaboration-based design - exemplified by the Internet Session Initiation Protocol (SIP),"The concept of collaborations capturing dynamic aspects of a distributed system across agent boundaries is introduced. Some ways of composing collaborations are illustrated, with collaborations being implicitly represented as state machine fragments. The concepts are exemplified by the Internet Session Initiation Protocol (SIP) and consequences for a potential SIP implementation are discussed.","Collaboration,
Internet,
Protocols,
Collaborative software,
Object oriented modeling,
Collaborative work,
Automata,
Software architecture,
Computer science,
Object oriented programming"
On complex fuzzy sets,"The innovative concept of complex fuzzy sets is introduced. The novelty of the complex fuzzy set lies in the range of values its membership function may attain. In contrast to a traditional fuzzy membership function, this range is not limited to [0,1] but extended to the unit circle in the complex plane. Thus, the complex fuzzy set provides a mathematical framework for describing membership in a set in terms of a complex number. A study of this original concept is presented, including examples of possible applications, which demonstrate the new theory.","Fuzzy sets,
Fuzzy set theory,
Fuzzy systems,
Physics,
Computer science,
Set theory,
Multivalued logic"
Project Catalyst: promoting systemic change in engineering education,"Project Catalyst is an NSF-funded initiative to promote systemic change in engineering education by integrating instructional design techniques, transforming the classroom into a cooperative learning environment, and incorporating the use of information technology in the teaching/learning process. A conceptual framework is described to aid in shifting and supporting student's and instructor's activities in a transition from a traditional mode to a collaborative mode of instruction. In the first year of Project Catalyst, a core group of engineering faculty has begun implementing this focused shift by introducing a greater emphasis on team building, teamwork, cooperative learning, problem-based learning, and information technology. This paper discusses our enhanced instructional model and the supplementary skills modules that we will develop and use to implement this model. It concludes with the future work for the remaining two years of the NSF-funded project.","Engineering education,
Information technology,
Chemical engineering,
Teamwork,
Computer science,
Collaboration,
Educational technology,
Educational institutions,
Laboratories,
Books"
A novel proximity coupled patch antenna for active circuit integration,"A novel design and measured results of a proximity coupled (PC) patch antenna for integration with a compact feed network on high dielectric constant, thin substrates are presented. A new type of antenna feed structure is proposed. Measured results with various modified feedlines are summarized. This approach facilitates a circuit designer to select a desired feedline substrate without compromising the radiation characteristics of the antenna in active integrated antenna applications.","Coupling circuits,
Patch antennas,
Active circuits,
Antenna feeds,
Dielectric substrates,
Dielectric constant,
Impedance,
Antenna measurements,
USA Councils,
Computer networks"
Science and Technology Knowledge Platform: a novel tool for science and technology policy in Japan,"A new system named Science and Technology Knowledge Platform (STKP) is being realized in Japan with support of the national government. This paper describes the architecture of STKP and compares it with the old national science and technology (S&T) system. STKP provides an open platform for communication among scientists; engineers and even investors. They can enter STKP after registration from the portal site provided by the Research Institute of Economy, Trade and Industry (RIETI). They submit S&T information to STKP via the portal, and the information is stored in a database. Users may search and obtain useful information from the database usually for no charge. STKP participants can freely create discussion forums on not only technical but also policy issues. The Engineering Academy of Japan and related S&T institutions have collaborated to create STKP.","Government,
Portals,
Companies,
Humans,
Computer industry,
Petroleum industry,
Databases,
Genomics,
Bioinformatics,
Global communication"
Minimally supervised acquisition of 3D recognition models from cluttered images,"Appearance-based object recognition systems rely on training from imagery, which allows the recognition of objects without requiting a 3D geometric model. It has been little explored whether such systems can be trained from imagery that is unlabeled, and whether they can be trained from imagery that is not trivially segmentable. In this paper we present a method for minimally supervised training of a previously developed recognition system from unlabeled and unsegmented imagery. We show that the system can successfully extend an object representation extracted from one black background image to contain object features extracted from unlabeled cluttered images and can use the extended representation to improve recognition performance on a test set.","Image recognition,
Object recognition,
Image segmentation,
Image databases,
Shape,
Surface cleaning,
Computer science,
Solid modeling,
Feature extraction,
System testing"
Parallel parsing of MPEG video,"Video parsing refers to the detection and classification of abrupt and gradual scene changes in a video stream and constitutes an important preprocessing step in applications that treat video streams as sources of information. Parallel processing is proposed as a means of dealing with the high computational demands of video parsing. Parallel versions of two algorithms that detect scene transitions in compressed video streams are proposed. Three granularities of parallelism are investigated; Group of Pictures (GOP), frame and slice. Results show that the GOP-level implementation, which represents the coarsest granularity of task and data decomposition, always performs the best. The slice and frame levels of granularity take the second and third place respectively. The speedup a's shown to be almost linear in the case of the GOP level of granularity, whereas the Synchronization overheads are seen to be high for the frame and slice levels of granularity.",
Enhanced static Fano coding,"Statistical coding techniques have been used for a long time in lossless data compression, using methods such as Huffman's algorithm, arithmetic coding, Shannon's method, Fano's method, etc. Most of these methods can be implemented either statically or adaptively. Canonical codes, in which the code words are arranged in a lexicographical order, are advantageous because they can be decoded extremely expediently. Although Huffman's algorithm is optimal, the generation of a canonical Huffman code is not straightforward. Conversely, while the Fano coding is sub-optimal, it can lead to canonical codes. In this paper, we resolve the dilemma by focusing on the static implementation of Fano's method. By taking advantage of the properties of the encoding schemes generated by this method, and the concept of ""code word arrangement"", we present an enhanced version of the static Fano's method, namely Fano/sup +/. We formally analyze Fanol by presenting some properties of Fano trees, and the theory of list rearrangements. Our enhanced algorithm achieves compression ratios arbitrarily close to those of Huffman's algorithm. Empirical results on files of the Canterbury corpus corroborate the almost-optimal efficiency of our enhanced algorithm and its canonical nature. We believe that the compression efficiency of Fano+ can be made to attain the compression ratios of the best known schemes if a structure model of the data is also incorporated.",
Fuzzy logic based handwritten character recognition,"This paper presents an innovative approach called box method for feature extraction for the recognition of handwritten characters. In this approach, the character image is partitioned into a fixed number of sub images called boxes. The features consist of normalized vector distance (/spl gamma/) and angle (/spl alpha/) from each box to a fixed point. The recognition schemes used are back propagation neural network (BPNN) and fuzzy logic. The recognition rate is found to be around 100% with the fuzzy based approach on the standard database.","Fuzzy logic,
Character recognition,
Feature extraction,
Handwriting recognition,
Neural networks,
Optical character recognition software,
Shape,
Multi-layer neural network,
Computer science,
Image databases"
Packet data capacity in an integrated network,"The challenge for 3G cellular is not only to support varied services at speeds to 2 Mbit/s, but to support these services efficiently and with good quality. A method to optimize the capacity available to high-speed data is to consider placing dual mode circuit-switched calls on an alternate cellular network (ie, 2G or 3G). For a mature UMTS network with high data demand, more capacity becomes available for 3G packet (and high-rate circuit) data, if dual mode 3G speech is preferably placed on the GSM network Our integrated network solution shows that 2G telephones may be serviced fairly, by overflowing calls between the 2G and 3G networks, if the 2G network is at capacity.","Intelligent networks,
Speech,
Switching circuits,
Land mobile radio cellular systems,
3G mobile communication,
GSM,
Bandwidth,
Circuit testing,
Computer science,
Integrated circuit technology"
On learning and computational complexity of FIR radial basis function networks. Part II. Complexity measures,"For pt. I see ibid., vol.II, p.1321-4(2001). Recently, the complexity control of dynamic neural models has gained significant attention from the signal processing community. The performance of such a process depends highly on the applied definition of ""model complexity"", i.e. complexity models that give simpler networks with better model accuracy and reliability are preferred. The learning theory creates a framework to assess the learning properties of models. These properties include the required size of the training samples as well as the statistical confidence over the model. In this paper, we apply the learning properties of two families of FIR radial basis function networks (RBFN) to introduce new complexity measures that reflect the learning properties of such neural models. Then, based on these complexity terms, we define cost functions, which provide a balance between the training and testing performances of the model, and give desirable levels of accuracy and confidence.","Computational complexity,
Finite impulse response filter,
Radial basis function networks,
Cost function,
Testing,
Computer science,
Cities and towns,
Performance evaluation,
Computer networks,
Neural networks"
An unsupervised segmentation framework for texture image queries,"In this paper a novel unsupervised segmentation framework for texture image queries is presented. The proposed framework consists of an unsupervised segmentation method for texture images, and a multi-filter query strategy. By applying the unsupervised segmentation method on each texture image, a set of texture feature parameters for that texture image can be extracted automatically. Based upon these parameters, an effective multi-filter query strategy which allows the users to issue texture-based image queries is developed The test results of the proposed framework on 318 texture images obtained from the MIT VisTex and Brodatz database are presented to show its effectiveness.","Image segmentation,
Image databases,
Image retrieval,
Multimedia systems,
Information systems,
Laboratories,
Computer science,
Spatial databases,
Image texture analysis,
Information retrieval"
Emphasizing formal analysis in a software engineering curriculum,"The integration of formal method application throughout a six course software engineering curriculum is outlined. Formal analysis skills were included in order to increase the complex program solving skills of the student. The five instruction-oriented courses presented highlight how formal analysis was introduced in and applied to the corresponding subject material. The materials presented, along with an accounting of the experiment, provide a basis for other academicians to teach formal analysis at their own institutions.","Computer science education,
Software engineering"
Implicit extrusion fields: general concepts and some simple applications,"Presents a new interpretation of the binary blending operator of implicit modeling. Instead of considering the operator as a composition of potential functions, we propose to consider it as an implicit curve extruded in an implicit extrusion field. An implicit extrusion field is a 2D space for which each coordinate is a potential field. The study of general concepts around implicit extrusion fields allows us to introduce the theoretical notion of free-form blending, controlled point-by-point by the user. Through the use of functional interpolation functions, we propose modeling tools to create, sculpt or combine implicit primitives by extrusion of a profile in an implicit extrusion field.","Shape control,
Equations,
Skeleton,
Application software,
Automatic control,
Computer science,
Interpolation,
Assembly,
Anisotropic magnetoresistance,
Visualization"
A simulation environment for analyses of quality of service in mobile cellular networks,"This paper presents a simulation environment for mobile cellular networks called CELSA, which implements a mobility model based on users, regions and time period characteristics. This environment was used to obtain quality of service (QoS) parameters of a mobile cellular network and to evaluate the performance of a non-uniform channel allocation scheme called CA-STV compared to fixed and dynamic channel allocation.",
Correlation matching approach to source separation in the presence of spatially correlated noise,"This paper addresses a new method of source separation that is robust in the presence of spatially correlated but temporally white noise. The method exploits the non-vanishing temporal structure of sources to reduce the effect of noise. In the framework of correlation matching, we develop two efficient least squares algorithms. Useful behavior of the proposed algorithms is confirmed by numerical experiments.",
Minimum dynamic update for shortest path tree construction,"Shortest path tree (SPT) computation is the major over-head for routers using any link-state routing protocols including the most widely used OSPF and IS-IS. Changes of link states are nowadays commonly occurred. It is not efficient and stable for network routing to use traditional static SPT algorithms to recompute the whole SPT whenever a change happens. We present new dynamic algorithms to compute and update the SPT with the minimum computational overhead. Routing stability is achieved by having the minimum changes in the topology of an existing SPT when some link states are changed. To the authors' knowledge, our algorithms outperform the best existing ones in the literature.",
Distance education and its impact on computer engineering laboratories,"As technology has evolved with the rapid growth of the Internet, it is only natural that the standard computer engineering curriculum must also adapt and change as well. As a result of this new technology and proliferation of Internet access to the general public, there have been strong pushes to disseminate knowledge using these new technologies. However, delivering the hands-on laboratories normally associated with computer engineering is a nontrivial task. This paper investigates the difficulties and potential solutions for providing high quality laboratories through distance education. The paper also investigates a case study in adapting a computer engineering laboratory for distance education.",
World knowledge for the domain of your choice,"Modern Web search engines access large parts of the publicly indexable Web. Relevant sites can be found easily thanks to advanced techniques such as Google's PageRank algorithm. However, a common problem remains the large number of matching documents being returned even for fairly specific queries. The same problem can be observed in domains that are more limited like intranets or local Web sites. By enriching a search engine with knowledge about the domain one could provide much more feedback for a query than just a list of matches, such as a number of useful discriminating terms, that would allow the user to constrain the query. We present a way of building such a domain model automatically by analyzing the markup of the source data. We will illustrate this with some examples taken from our sample domain.",
Improving the quality of object-oriented programs,"Object-oriented languages originated in the late '60s and entered widespread industrial use in the early '90s. With this technology's advent came the hope and belief that life from a programming perspective would generally improve. We believed that these languages would foster reuse and, consequently, a reduction in the quantity of code written. We also believed that inheritance, an object-oriented language feature, would result in less testing effort, although D. Perry and G. Kaiser (1990) dispelled this notion. So, if we use object-oriented technology, what should we expect in terms of quality? What can we do to increase our chances of producing highly reliable software? This paper presents four ideas that will improve these chances regardless of the object-oriented language used.","Aircraft,
Data structures,
Underwater vehicles,
Marine vehicles,
Java,
Libraries,
Computer science,
Information technology,
Software testing"
Participatory business process reengineering design: generating solutions,"PAWS (Participatory Approach to Workgroup Systems) is an alternative approach to radical BPR (business process reengineering). Its main feature is to include participatory design by employees in the reengineering activity. It consists of a set of sequential stages. The third of these stages is intended to obtain the largest possible number of alternative solutions to the previously identified problems of the organization during the second stage of the PAWS method. Since the idea is to make employees participate, the consultant must be able to motivate workers to generate ideas. It will probably take several rounds of participatory work and individual sessions to achieve this generation. Mechanisms to motivate participation may be designed; many of them can be supported by a computer system. Three aspects of the work related to PAWS Stage 3 are presented. The first is a participatory model, making explicit the elements which motivate people to participate in a collaborative work environment. The second is the computer mechanisms designed to motivate participation. Finally, the third is a software prototype based on the proposed GPS (Generation oPtions Support) participatory model.","Business process re-engineering,
Process design,
Plasma welding,
Automation,
Proposals,
Collaborative work,
Software prototyping,
Information technology,
Logic,
Tree graphs"
A platform-independent API for quality of service management,"The differentiated services approach to QoS in the Internet poses new challenges on the configuration and service provisioning side. The approach relies on an entity often referred to as a bandwidth broker. It configures the network elements so that guaranteed networking services are provided to customers. However, the DiffServ capable routers have a large variety of hardware configurations and different configuration interfaces (e.g. SNMP, CORBA, CLI). Therefore, we propose a QoS management API which is used by bandwidth broker implementations in order to configure the underlying routers. In our prototype we implemented a proprietary configuration interface to our Linux-based DiffServ router implementation.","Quality of service,
Quality management,
Bandwidth,
Laser sintering,
Diffserv networks,
Hardware,
Application software,
Computer science,
Mathematics,
Laboratories"
Optimistic simulation of parallel message-passing applications,"Optimistic techniques can improve the performance of discrete-event simulations, but one area where optimistic simulators have been unable to show performance improvement is in the simulation of parallel programs. Unfortunately, parallel program simulation using direct execution is difficult: the use of direct execution implies that the memory and computation requirements of the simulator are at least as large as that of the target application which restricts the target systems and application problem sizes that can be studied. Memory usage is especially important for optimistic simulators due to the need for periodic state-saving and rollback. In our research we addressed this problem and have implemented a simulation library running a Time-Warp-based optimistic engine that uses direct execution to simulate and predict the performance of parallel MPI program while attaining good simulation speedup. For program with data sets too large to be directly executed with our optimistic simulator, we reduced the memory and computational needs of these programs by utilizing a static task graph and code-slicing methodology an approach which also exhibited good performance speedup.","Computational modeling,
Engines,
Application software,
Optimization methods,
Computer architecture,
Concurrent computing,
Communication system control,
Computer science,
Computer simulation,
Predictive models"
An effective testing technique for component composition in EJBs,"This paper proposes a new testing technique for component composition of EJBs. We define components made by a current developer as white box components and components made by another developer as black box components. Software from CBSD consists of black box components and white box components, and composition errors result from the interaction between black box components and white box components, or the interaction between two white box components. Our technique tests these composition errors. We select test cases by injecting a fault into a specific parts of the white box component. The specific parts we define in this paper lead to the high effectiveness of our technique. We evaluate this effectiveness through an experiment and a theorem. In addition, we provide an example in Enterprise JavaBeans.","Testing,
Costs,
Assembly,
Computer science,
Java,
Programming,
Qualifications,
HTML"
Improved shape-from-shading using Darboux smoothing,"This paper describes a new surface normal smoothing process which can be used in conjunction with shape-from-shading. Rather than directly smoothing the surface normal vectors, we exert control over their directions by smoothing the field of principal curvature vectors. To do this we develop a topography sensitive smoothing process which overcomes the problems of singularities in the field of principal curvature directions at the locations of umbilics and saddles.",
Similarity-based approach to earthenware reconstruction,"This paper proposes an earthenware reconstruction system, which can automatically reconstruct some earthenware from numerous given potsherds in two-dimensional grayscale images. The system supposes that given potsherds are thin and moderately flat and small. Some earthenware having three-dimensional shape, such as crocks, can be reconstructed by the system within the supposition. The system performs earthenware reconstruction through two phases. At the first phase, potsherds are joined automatically in two dimensions. An efficient joint detection algorithm using surface pattern and shape similarity is proposed at this phase. At the second phase, three-dimensional shape is recovered by an adequate three-dimensional transformation. Some experimental results of reconstruction from numerous potsherds are also reported.",
Application of a breeder genetic algorithm for system identification in an adaptive finite impulse response filter,"We describe in this paper the application of a breeder genetic algorithm to the problem of parameter identification for an adaptive finite impulse filter. A breeder genetic algorithm was needed due to the epistiasis phenomena, which is present for this type of adaptive filter. The results of the genetic algorithm were compared to the traditional statistical method and, we found that the breeder genetic algorithm was clearly superior (in accuracy) in most of the cases. However, the statistical least mean squares method is faster then the genetic algorithm. For this reason we suggest using the genetic algorithm for off-line adaptation. Ay hybrid method combining the advantages of both methods is proposed for real world applications.","Genetic algorithms,
System identification,
Parameter estimation,
Mathematical model,
Training data,
Testing,
Application software,
Statistical analysis,
Finite impulse response filter,
Computer science"
A method for automatic optimization of dynamic memory management in C++,"In C++, the memory allocator is often a bottleneck that severely limits performance and scalability on multiprocessor systems. The traditional solution is to optimize the C library memory allocation routines. An alternative is to attack the problem on the source code level, i.e. modify the applications source code. Such an approach makes it possible to achieve more efficient and customized memory management. To implement and maintain such source code optimizations is however both laborious and costly, since it is a manual procedure. Applications developed using object-oriented techniques, such as frameworks and design patterns, tend to use a great deal of dynamic memory to offer dynamic features. These features are mainly used for maintainability reasons, and temporal locality often characterizes the run-time behavior of the dynamic memory operations. We have implemented a pre-processor based method, named Amplify, which is a completely automated procedure optimizes (object-oriented) C++ applications to exploit the temporal locality in dynamic memory usage. Test results show that Amplify can obtain significant speed-up for synthetic applications and that it was useful for a commercial product.","Optimization methods,
Memory management,
Power system management,
Programming profession,
Computer science,
Scalability,
Multiprocessing systems,
Libraries,
Runtime,
Testing"
Effect of using computer graphics animation in programming education,"The authors developed a novel lecture style for elementary programming education named program reading practice by using computer graphics animation programs (named WinTK). In this practice, students read the source codes of various kinds of computer graphics animation programs and are challenged to modify or extend these programs instructed by the teacher (program re-write practice). We have done this practice for six years and investigated various kinds of results in order to acquire the effect objectively. The authors describe the investigation technique and several results of our practice.",
Reasoning in higraphs with loose edges,"Harel (1988) introduces the notion of zooming out as a useful operation in working with higraphs. Zooming out allows one to consider less detailed versions of a higraph by dropping some detail from the description in a structured manner. Although this is a very useful operation it seems it can be misleading in some circumstances by allowing the user of the zoomed out higraph to make false inferences given the usual transition system semantics for higraphs. We consider one approach to rectifying this situation by following through Harel's suggestion that, in some circumstances, it may be useful to consider higraphs with edges that have no specific origin or destination. We call these higraphs loose higraphs and show that an appropriate definition of zooming on loose higraphs avoids some of the difficulties arising from the use of zooming. We also consider a logic for connectivity in loose higraphs.",
Preliminary results from the deployment of integrated teleconsultation services in rural Crete,"Teleconsultation services for cardiology patients have been installed and have been in routine use since December 2000, connecting a primary health centre in rural Crete to a regional hospital. Since efficiency and effectiveness are key factors in the acceptance of the service, integration of the services with the primary health record, support of clinical protocols and guidelines, and continuous evaluation of the services are primary foci of the overall effort. This paper discusses our evaluation approach and presents preliminary results from the utilization of the service between December 2000 and June 2001. The presented results are encouraging, suggesting that the wide deployment of the service will benefit not only the patients but also the GPs, the cardiologists and the health care system.",
Incremental hierarchical discriminating regression for indoor visual navigation,"In this paper, we investigate vision-based navigation using the incremental hierarchical discriminating regression (IHDR) algorithm. Based on the learned experience, the system gradually improves its performance through online interaction with environment. The learning process is interactive and on-line. The hierarchical structure of the IHDR algorithm allows each associative recall to be completed in O(log n) time, where it is the number of cases learned. This makes real time performance possible. The IHDR learns incrementally: each learning sample is learned or rejected based on the real time response. The proposed scheme has been successfully applied to indoor navigation.","Navigation,
Clustering algorithms,
Roads,
Image edge detection,
Humans,
Decision trees,
Robot vision systems,
Cameras,
Computer science,
Query processing"
Communication with threads in softw are DSMs,,"Yarn,
Delay,
Software performance,
Workstations,
Application software,
Electronic mail,
Protocols,
Communication switching,
Hardware,
Communication system software"
A new method to evaluate a trained artificial neural network,"In comparison with traditional local sample testing methods, this paper proposes a new approach to evaluate a trained neural network. A new parameter is defined to identify the different potential roles of the individual input factors based on the trained connections of the nodes in the network. Compared with field-specific knowledge, the dominance of individual input factors can be checked and then false mappings satisfying only the specific data set may be avoided.","Artificial neural networks,
Testing,
Neural networks,
Knowledge engineering,
Computer science,
Data engineering,
Knowledge based systems,
Civil engineering,
Transportation"
On the complexity of gate duplication,"In this paper, we show that both the global and local gate duplication problems for delay optimization are NP-complete under certain delay models.","Delay,
Circuits,
Capacitance,
Very large scale integration,
Algorithm design and analysis,
Load modeling,
Computer science,
Equations,
Optimization methods"
Low-bandwidth Internet streaming of multimedia lectures,"Traditional modes of communications are inadequate for distance learning in the information age. With advances in computer technology, the Internet offers great opportunities for educational exchange of ideas across vast distances. This paper proposes an end-to-end solution for streaming 'live' and recorded multimedia lectures over the Internet. The technical issues involved are first discussed and the importance of application-layer QoS control in ensuring an acceptable quality of multimedia lecture play-out emphasised. A hybrid architecture that uses an exchange server to handle communications and represents an optimal balance between several conflicting requirements is then proposed. In this architecture video/audio data are transferred directly between the streaming servers and clients. Objective and subjective tests have validated the effectiveness of the design.","quality of service,
multimedia communication,
Internet,
distance learning,
educational computing"
Cluster rolling upgrade using multiple version support,,"Availability,
Operating systems,
Hardware,
Costs,
Sun,
Computer networks,
Scalability,
Fault tolerant systems,
Time factors"
DFSLIF: dynamical fuzzy system with linguistic information feedback,"We propose a new dynamical fuzzy system with linguistic information feedback (DFSLIF). Instead of crisp system output, the delayed conclusion fuzzy membership function in the consequence part is fed back locally with adjustable scaling and shifting in order to overcome the static mapping drawback of conventional fuzzy systems. We give a detailed description of the corresponding structure and algorithm. Our novel scheme has the advantage of inherent dynamics, and is therefore well suited for handling temporal problems like dynamical system identification, control, and filtering. Simulation experiments have been carried out to demonstrate its effectiveness.","Fuzzy systems,
Feedback,
Fuzzy logic,
Fuzzy sets,
Filtering,
Modeling,
Nonlinear dynamical systems,
Engines,
Power electronics,
Computer science"
An editing system for working processes,"Business processes are often complex and varying. Workflow technologies help people minimize errors and delays in the progress of a process within an enterprise. Within a workflow management system (WFMS), a process or even a project can be specified and interpreted to work directly. In this paper, we present an editing system and its two major components, PDE (Process Definition Editor) and FormDesigner, for the specification of a process. PDE is a graphical editor for constructing /modifying process definitions. FormDesigner assumes the responsibility of integrating software components for building/modifying active application forms. With PDE and FormDesigner, a workflow application system can be constructed/modified rapidly and easily so that the WFMS is allowed to run smoothly.","Workflow management software,
Object oriented modeling,
Application software,
Buildings,
Software engineering,
Management information systems,
Computer science,
Computer errors,
Delay,
Business process re-engineering"
Democratic data fusion for information retrieval mediators,,"Information retrieval,
Computer science,
Metasearch,
Engines,
Fuses,
Nominations and elections,
Voting,
Indexing,
Bandwidth,
Costs"
Characterization of the output of an ATM output buffer receiving self-similar traffic,We investigate the departure process of an ATM output buffer intaking asymptotically second-order self-similar traffic using matrix-analytic technique. The results obtained include the marginal and joint distribution of interdeparture times. We show that output and input processes asymptotically possess the same tail and smoothing effect takes place only for traffic with small Hurst parameters.,"Traffic control,
Asynchronous transfer mode,
Telecommunication traffic,
Performance analysis,
Queueing analysis,
Tail,
Communication system traffic control,
Switches,
Computer science,
Smoothing methods"
Coherence analysis of the ongoing EEG by means of microstates of synchronous oscillations,"The sensitivity of instantaneous EEG coherence for the investigation of elementary thinking processes was shown by several authors. Similarly to EP analysis the statistical validation of the results is based on the multiple repetition of the same task. This approach is not possible for complex unreproducible thinking processes of long duration as e.g., solving mathematical problems. Classical coherence analysis of long periods of the EEG suffers from the loss of temporal information. The paper presents a temporal coherence analysis of the ongoing EEG preserving the temporal information of long cognitive processes. The main point of the method is the decomposition of the whole time interval in microstates of synchronous oscillations. The calculation of instantaneous coherence for multiple electrode pairs yields in a time-dependent high-dimensional coherence vector. A segmentation algorithm dissects the whole process into time intervals with stable coherence vectors the so-called microstates of oscillations. A subsequent clustering procedure into a small number of classes results in a sequence of prototypical microstates, which may be modeled by a Markovian process. Special entropy parameters characterize the strength of concatenation of different microstates. The method was applied in order to understand the special brain functioning of mathematically highly gifted subjects.","Electroencephalography,
Coherence,
Brain modeling,
Entropy,
Information analysis,
Prototypes,
Computer science,
Clustering algorithms,
Frequency synchronization,
Frequency estimation"
New techniques for collective communications in clusters: a case study with MPI,"The paper describes new techniques to increase the performance of collective communication operations in clusters. These techniqnes are based in multithreading operations and on-line data compression. The techniques proposed have been implemented in MiMPI, a thread-safe implementation of MPI. We have evaluated, and compared, the performance of MiMPI with other implementations of MPI available for clusters with Linux and Windows 2000. The benchmark used has been MPBench, a flexible and portable framework to allow benchmarking of MPI implementations.","Computer aided software engineering,
Yarn,
Data compression,
Multithreading,
Linux,
Libraries,
Operating systems,
Sockets,
Computer architecture,
Computer science"
Evolution and analysis of mixed mode neural networks for walking: mixed pattern generators,"This paper summarizes the results of the dynamical systems analysis of nearly four hundred continuous-time recurrent neural network (CTRNN) single-leg locomotion controllers evolved under conditions where sensory information was unreliable and in which the body the controller was embedded in could change its physical properties. The general principles underlying the operation of all the resulting mixed pattern generators (MPGs) are discussed. Several MPG operational features are explained and verified. Finally, discussion is made of future extensions of this research.",
Component-oriented simulation architecture: Toward interoperability and interchangeability,"In this paper we investigate two issues at the heart of simulation reusability: interoperability and interchangeability. Their implications for simulation technology are discussed. Based on our previous work on simulation component oriented world view and simulation component classification, the Component-Oriented Simulation Architecture (CORSA) is devised to address both issues. The ideas and considerations which motivated us in developing CORSA are discussed. The design and implementation of a prototype are also described briefly. A sequential PCS simulation has been developed using CORSA.. This exercise demonstrated several advantages of the component-based approach: flexibility, extensibility, and reusability. Experimental results show that the component-based approach is only slightly slower than the monolithic approach, whose complexity quickly grows to nearly unsurmountable proportions with the growth of complexity of the simulated system.","Computational modeling,
Discrete event simulation,
Prototypes,
Personal communication networks,
Computer architecture,
Computer science,
Kernel,
Publishing,
Subscriptions,
Buildings"
Deadline based channel scheduling,"The use of deadline based channel scheduling in support of real time delivery of application data units (ADU's) is investigated. Of interest is priority scheduling where a packet with a smaller ratio of delivery deadline over number of hops to destination is given a higher priority. It has been shown that a variant of this scheduling algorithm, based on head-of-the-line priority, is efficient and effective in supporting real time delivery of ADU's. In this variant, packets with a ratio smaller than or equal to a given threshold are sent to the higher priority queue. We first present a technique to select this threshold dynamically. The effectiveness of our technique is evaluated by simulation. We then study the performance of deadline based channel scheduling for large networks, with multiple autonomous systems. For this case, accurate information on number of hops to destination may not be available. A technique to estimate this distance metric is presented. The effectiveness of our algorithm with this estimated distance metric is evaluated. In addition, we study the performance of a multi-service scenario where only a fraction of the routers deploy deadline based channel scheduling.",
"Using case studies to increase awareness of, and improve resolution strategies for, ethical issues in engineering","Case studies in engineering ethics were integrated into a first course in Electrical and Computer Engineering at Worcester Polytechnic Institute with the primary goals of increasing students' awareness of (1) ethical issues in the workplace, and (2) the number of different courses of action that one might take to resolve the ethical issues. During a three-hour ""laboratory"" period, students read and discussed four short case studies in engineering ethics. Discussion was guided to focus on understanding the (often conflicting) viewpoints of individuals within a case and to look for multiple courses of action for resolving the issue. The second learning goal was assessed prior to the case study laboratory and on two occurrences after the laboratory. Assessment results showed no changes in the number of different courses of action that students could enumerate to resolve ethical issues. Nonetheless, this laboratory might still (1) encourage students to take a full-semester ethics course (which could lead to measurable results), (2) contribute to progressive learning in this area, and (3) begin (in a way we have yet to measure) to convince students that ethics is an important aspect of their career paths.","Computer aided software engineering,
Ethics,
Laboratories,
Employment,
Area measurement,
Engineering education,
Electronic circuits,
Engineering profession,
Explosions,
Space shuttles"
A package for filter design based on MATLAB,"Electric filters have a relevant importance in electronic systems because they are present in almost any electronic system. For example, communication systems, as many other electric systems, make intensive use of filtering to separate unwanted noise from the desired signal. Unfortunately, filter design is an intensive computational task requiring a significant amount of numerical calculations to obtain either the parameters of a filter transfer function or the element values for a filter circuit realization. This paper describes a software package whose purpose is to provide a tool to be used as a teaching aid in analog and digital filter design courses. The feature of this package is that it uses MATLAB for the numerical computations. The main advantage of the filter design software package described in this paper is that it makes uses of one of the MATLAB toolboxes, the signals toolbox (which is used for analog and digital filter design), but used with an interface that makes it possible even to the novice user to readily design filters, either analog or digital, without any previous knowledge of MATLAB or the signals toolbox.","MATLAB,
Digital filters,
Signal design,
Software packages,
Electronics packaging,
Filtering,
Circuit noise,
Transfer functions,
Education,
Software design"
Soft counting networks for bone marrow differentials,"Differential white cell counts from bone marrow preparations are very useful in evaluation of various hematologic disorders. It is tedious to locate, identify, and count these classes of cells, even by skilled hands. Automation of classification and counting would be of great benefit. However, the class structure of bone marrow or peripheral blood cells is not discrete; it represents a biological continuum of maturation levels. Because of this, there is uncertainty and overlap in characteristics of adjacent cell classes such that traditional pattern recognition techniques have difficulty in arriving at accurate cell counts. The authors investigate soft counting networks that are trained to produce accurate overall class counts by allowing cells to have degrees of membership in multiple cell classes. This approach is applied to a bone marrow cell library and is compared with other standard recognition algorithms.","Bones,
Cells (biology),
Image segmentation,
Humans,
White blood cells,
Neural networks,
Computer science,
Pathology,
Automation,
Uncertainty"
PVL: an object oriented software library for parallel signal processing,,"Software libraries,
Signal processing,
Video signal processing,
Signal processing algorithms,
Concurrent computing,
Military computing,
Laboratories,
Microprocessors,
Application software,
Wireless communication"
VXT: Visual XML Transformer,"The ever-growing amount of heterogeneous data exchanged via the Internet, combined with the popularity of XML, makes structured document transformations an increasingly important application domain. Most of the existing solutions for expressing XML transformations are textual languages, such as XSLT (Extensible Stylesheet Language Transformations) or DOM (Document Object Model), combined with a general-purpose programming language. Several tools build on top of these languages, providing a graphical environment and debugging facilities. Transformations are, however, still specified in a textual way using the underlying language (often XSLT), thus requiring users to learn it. We believe that visual programming techniques are well-suited to representing XML structures and make the specification of transformations simpler. We propose a visual language, called VXT (Visual XML Transformer), for the specification of XML transformations in an interactive environment based on a zoomable user interface toolkit and on two target languages specialised in structure transformations: Circus and XSLT.","XML,
Data mining,
Data structures,
Cognitive science,
Postal services,
Internet,
Debugging,
User interfaces,
Computer languages,
Tree data structures"
Fault tolerance as an aspect using JReplica,"Reliability and availability are very important trends in the development process of distributed systems. In order to improve these features, object replication mechanisms have been introduced. Programming replication policies for a given application is not an easy task, and this is the reason why transparency for the programmer has been one of the most important properties offered by all replication models. However, this transparency for the programmer is not always desirable. In this paper we present a replication model, JReplica, based on Aspect Oriented Programming (AOP). JReplica allows the separated specification of the replication code from the functional behaviour of objects, providing not only a high degree of transparency, as done by previous models, but also the possibility for programmers to introduce new behaviour to specify different fault tolerance requirements. Moreover, the replication aspect has been introduced at design time, and in this way, UML has been extended in order to consider replication issues separately when designing fault tolerance systems.","Fault tolerance,
Object oriented modeling,
Programming profession,
Fault tolerant systems,
Computer science,
Unified modeling language,
Middleware,
Protocols,
Contracts,
Availability"
The Event Calculus assessed,"The range of applicability of the Full Event Calculus is proven to be the Ksp-IA class in the features and fluents taxonomy. The proof is given with respect to the original definition of this preference logic, where no adjustments of the language or reasoning method were necessary. The result implies that the claims on the expressiveness and problem-solving power of this logic were indeed correct.","Calculus,
Problem-solving,
Taxonomy,
Logic design,
Computer science,
Educational institutions,
Licenses,
Warranties,
Software standards"
Sensor placement in distributed sensor networks using a coding theory framework,"An important issue in the design of distributed sensor networks is the optimal placement of sensors for target location. If the surveillance region, also referred to as the sensor field, is represented as a grid (two- or three-dimensional) of points (coordinates), target location refers to the problem of pin-pointing a target at a grid point at any point in time. For enhanced coverage, a large number of sensors are typically deployed in the sensor field, and if the coverage areas of multiple sensors overlap, they may all report a target in their respective zones. The precise location of the target must then be determined by examining the location of these sensors. Target location can be simplified considerably if the sensors are placed in such a way that every grid point in the sensor field is covered by a unique subset of sensors. The sensor placement problem for target location is closely related to the alarm placement problem, which refers to the problem of placing ""alarms"" on the nodes of a graph G such that a single fault in the system can be diagnosed. The alarms are therefore analogous to sensors in a sensor field. It was shown by Rao (1993) that the alarm placement problem is NP-complete for arbitrary graphs. However, for restricted topologies, e.g. a set of grid points in a sensor field, a coding theory framework can be used to efficiently determine sensor placement. The sensor locations correspond to codewords of an identifying code constructed over the grid points in the sensor field.","Intelligent networks,
Codes,
Computer science,
Surveillance,
Sensor systems,
Topology"
Form document identification using line structure based features,"Form recognition is one of the special applications of document analysis (DA). We present a novel form recognition method by analyzing the line structure embedded in an input form document. First, all vertical and horizontal lines embedded in the form image are extracted. By analyzing the crossing relationships among horizontal lines and vertical lines, a line crossing relationship matrix can be built with each row corresponding to one horizontal line and each column corresponding to one vertical line. Moreover two line distance relationship matrices, horizontal and vertical line distance relationship matrices, are built by analyzing the distance relationships among horizontal lines and vertical lines, respectively. Last, the recognition task is performed by matching these three matrices. Experimental results reveal the feasibility and efficiency of our proposed method in recognizing form documents.","Character recognition,
Image storage,
Optical devices,
Pattern recognition,
Storage automation,
Computer science,
Application software,
Text analysis,
Information analysis,
Office automation"
The ANN-tree: an index for efficient approximate nearest neighbor search,"We explore the problem of approximate nearest neighbor searches. We propose an index structure, the ANN-tree (approximate nearest neighbor tree) to solve this problem. The ANN-tree supports high accuracy nearest neighbor search. The actual nearest neighbor of a query point can usually be found in the first leaf page accessed. The accuracy increases to near 100% if a second page is accessed. This is not achievable via traditional indexes. Even if an exact nearest neighbor query is desired, the ANN-tree is demonstrably more efficient than existing structures like the R*-tree. This makes the ANN-tree a preferable index structure for both exact and approximate nearest neighbor searches. We present the index in detail and provide experimental results on both real and synthetic data sets.","Nearest neighbor searches,
Computer science,
Indexes,
Databases,
Web search,
Search engines,
Information retrieval,
Information systems,
Multidimensional systems,
Algorithm design and analysis"
"On separators, segregators and time versus space","Gives an extension of the result due to Paul, Pippenger, Szemeredi and Trotter (1983) that deterministic linear time (DTIME) is distinct from nondeterministic linear time (NTIME). We show that NTIME[n/spl radic/log*(n)] /spl ne/ DTIME[n/spl radic/log*(n)]. We show that if the class of multi-pushdown graphs has {o(n), o[n/log(n)]} segregators, then NTIME[n log(n)] /spl ne/ DTIME[n log(n)]. We also show that at least one of the following facts holds: (1) P /spl ne/ L, and (2) for all polynomially bounded constructible time bounds t, NTIME(t) /spl ne/ DTIME(t). We consider the problem of whether NTIME(t) is distinct from NSPACE(t) for constructible time bounds t. A pebble game on graphs is defined such that the existence of a ""good"" strategy for the pebble game on multi-pushdown graphs implies a ""good"" simulation of nondeterministic time-bounded machines by nondeterministic space-bounded machines. It is shown that there exists a ""good"" strategy for the pebble game on multi-pushdown graphs if the graphs have sublinear separators. Finally, we show that nondeterministic time-bounded Turing machines can be simulated by /spl Sigma//sub 4/ machines with an asymptotically smaller time bound, under the assumption that the class of multi-pushdown graphs has sublinear separators.","Particle separators,
Turing machines,
Polynomials,
Legged locomotion,
Computer science,
Diffusion tensor imaging,
Complexity theory,
Computational modeling"
"Traveling with a Pez dispenser (or, routing issues in MPLS)","MultiProtocol Label Switching (MPLS) is a routing model proposed by the IETF for the Internet, and is becoming widely popular. In this paper, we initiate a theoretical study of the routing model, and give routing algorithms and lower bounds in a variety of situations. We first study the routing problems on the line. We then build up our results from paths through trees to more general graphs. The basic technique to go to general graphs is that of finding a tree cover, which is a small set of subtrees of the graph such that for each pair of vertices, one of the trees contains a shortest (or near-shortest) path between them. The concept of tree covers appears to have many interesting applications.","Multiprotocol label switching,
Tree graphs,
Internet,
Routing protocols,
Data mining,
Trademarks,
Computer science,
Performance analysis,
Switches,
Information analysis"
Geometric hashing techniques for watermarking,"In this paper we introduce the idea of using computer vision techniques for improving and enhancing watermarking capabilities. Specifically, we incorporate geometric hashing techniques into the watermarking methodology. Geometric hashing was developed to detect objects in a visual scene under a class of geometric transformations. The technique is incorporated into watermarking to detect watermarks encoded under transformations. This allows randomization of the watermark code without the need of maintaining the random generator seed. In turn, this randomization increases robustness under attacks such as collusion (determining the watermark from multiple watermarked examples). Depending on the embedding domain, robustness of the watermark under geometric attacks can be achieved.","Watermarking,
Layout,
Robustness,
Computer vision,
Computer science,
Object detection,
Decoding,
Digital images,
Frequency,
Shearing"
Introducing 9-12 grade students to electrical engineering technology through hands-on laboratory experiences,"At Penn State Erie, The Behrend College, an outreach project has been designed and implemented to provide 9th through 12th grade students with electrical engineering technology (EET) experiences. The high school students are brought onto the college campus and given a ten-minute presentation on the EET program and career opportunities. The students then participate individually in two thirty-minute hands-on laboratory experiences, one introducing the students to PSpice(R) and the other laboratory introducing the students to LabVIEW/sup TM/. Given two electronic circuits, the students are introduced to PSpice(R) for simulation. In the LabVIEW/sup TM/ experience the presentation begins with a tone generator program. The graphical user interface allows the students to control the amplitude and frequency of the waveform. In a second program the students simulate the functions of a touch tone telephone. The workshops have been conducted for high schools resulting in very positive feedback.","Electrical engineering,
Educational institutions,
Laboratories,
Circuit simulation,
Engineering profession,
Electronic circuits,
Graphical user interfaces,
Frequency,
Telephony,
Feedback"
A dichotomy in the complexity of propositional circumscription,"The inference problem for propositional circumscription is known to be highly intractable and, in fact, harder than the inference problem for classical propositional logic. More precisely, in its full generality this problem in /spl Pi//sub 2//sup P/-complete, which means that it has the same inherent computational complexity as the satisfiability problem for quantified Boolean formulas with two alternations (universal-existential) of quantifiers. We use T.J. Schaefer's (1978) framework of generalized satisfiability problems to study the family of all restricted cases of the inference problem for propositional circumscription. Our main result fields a complete classification of the ""truly hard""(/spl Pi//sub 2//sup P/-complete) and the ""easier"" cases of this problem (reducible to the inference problem for classical propositional logic). Specifically, we establish a dichotomy theorem which asserts that each such restricted case is either /spl Pi//sub 2//sup P/-complete or is in co-NP. Moreover, we provide efficiently checkable criteria that tell apart the ""truly hard"" cases from the ""easier"" ones.","Logic,
Polynomials,
Informatics,
Computer science,
Prototypes,
Reflection"
Deterministic path planning for planar assemblies,"I present a deterministic path planning algorithm for an assembly of rigid planar parts connected by joints. The part boundaries are comprised of line segments and circular arcs. The algorithm is based on configuration space computation and search. It is complete for one moving part and is heuristic for multiple parts. It is the first complete algorithm that is practical for real-world applications. It is more reliable than randomized algorithms, which are inherently incomplete. It solves problems with many parts, crowded environments, tight part fits, and closed chains.","Path planning,
Orbital robotics,
Robot kinematics,
Robotic assembly,
Mobile robots,
Legged locomotion,
Computational geometry,
Mechanical systems,
Manipulators,
Computer science"
Nonparametric training of snakes to find indistinct boundaries,"We enable highly improved performance of deformable model (snake) segmentation of a known type of object (human bladder) with unclear edges in a cluttered domain (abdominal CT scans). This is accomplished by learning an objective function from ground-truth contours in test images, using a nonparametric estimator of the distributions of chosen image quantities (intensity on the boundary and image gradient perpendicular to it). The Parzen-window estimator is found to reward correct contours much more accurately than a model based on means and covariances. This latter Gaussian model, in turn, performs adequately where a traditional a priori objective function does not. Performance of objective functions is measured by checking the fraction of incorrect contours that score better than ground truth (false positives), and the deviation of plots of shape incorrectness vs. objective function value from the closest strictly increasing function.","Deformable models,
Image segmentation,
Testing,
Computer science,
Bladder,
Computed tomography,
Shape measurement,
Cities and towns,
Educational institutions,
Abdomen"
Effect of different marking strategies on explicit congestion notification (ECN) performance,"The congestion control mechanisms built into the transmission control protocol (TCP) use packet drops as a means to detect congestion occurring in the network. Unnecessary packet drops lead to poor performance for low-bandwidth delay-sensitive applications. Explicit congestion notification (ECN) is proposed as a mechanism to provide feedback to the sources about impending congestion in the routers without the need to drop packets. This requires the ECN bit of the IP packet to be marked at the router based on mechanisms like random early detection (RED) to identify congestion. We examine three different marking strategies, viz., mark-tail, mark-front and mark-random. The throughput performance of ECN flows and the unfairness among the ECN flows are examined. We also study the interaction between ECN and non-ECN flows.","Delay,
Feedback,
Transport protocols,
Computer science,
Application software,
Internet,
Traffic control,
Councils"
Semantics of name and value passing,"Provides a semantic framework for (first-order) message-passing process calculi by combining categorical theories of abstract syntax with binding and operational semantics. In particular, we obtain abstract rule formats for name and value passing with both late and early interpretations. These formats induce an initial-algebra/final-coalgebra semantics that is compositional, respects substitution and is fully abstract for late and early congruence. We exemplify the theory with the /spl pi/-calculus and value-passing CCS (calculus of communicating systems).","Carbon capture and storage,
Laboratories,
Concurrent computing,
Informatics,
Message passing,
Concrete"
Relation between weight initialization of neural networks and pruning algorithms: case study on Mackey-Glass time series,"The implementation of weight initialization is directly related to the convergence of learning algorithms. We made a case study on the Mackey-Glass time series problem in order to try to find some relations between weight initialization of neural networks and pruning algorithms. The pruning algorithm used in simulations is the Laplace regularizer method, that is, the backpropagation algorithm with Laplace regularizer added to the criterion function. Simulation results show that different kinds of initialization weight matrices display almost the same generalization ability when using the pruning algorithm, at least for the Mackey-Glass time series.","Neural networks,
Computer aided software engineering,
Backpropagation algorithms,
Data mining,
Convergence,
Concrete,
Intelligent control,
Laboratories,
Information science,
Glass"
A data delivery strategy in ubiquitous computing systems,"Browsing Web pages through wireless devices will become more and more popular in the near future. However, accessing a large chunk of Web data incurs a great amount of access latency while a mobile user migrates across cells. This paper investigates this problem and proposes an effective way of delivering such data to a mobile user in a ubiquitous computing system. Our data delivery (/spl Dscr//sup 2/) strategy is based on the dynamic programming skill needed to pre-assign the user-requested data on the possible cells to be visited in order to reduce the access latency. We first design an encoding method for the cells and then derive the properties of the encoding scheme. These properties are then used in our algorithm to deliver the data. Finally, experiments are conducted to demonstrate the feasibility and efficiency of the proposed method.","Ubiquitous computing,
Bandwidth,
Web pages,
Delay,
Data communication,
Chaos,
Computer science,
Dynamic programming,
Design methodology,
Information retrieval"
Performance driven optimization for MUX based FPGAs,"Logic synthesis and technology mapping for Multiplexor (MUX) based field programmable gate arrays (FPGAs) have widely been considered. In this paper, an algorithm is proposed that applies techniques from logic synthesis during mapping to minimize the delay. By this, the target technology is considered during the minimization process. Binary decision diagrams (BDDs) are used as an underlying data structure for netlists, combining both structural and functional properties. To evaluate a netlist, a fast technology mapper is used. Since most of the changes to a netlist are local, re-mapping can be done locally, allowing a fast but reliable evaluation after each modification. We give experimental results that show large improvements for most of the instances we considered.",
Use of local memory for efficient Java execution,"Java has become a popular choice for implementing various applications that run on mobile and hand-held devices. Optimizing the energy consumption in mobile environments is of critical importance to prolong the battery life. In this paper, we propose an object allocation strategy to reduce the energy consumption of Java applications. This object allocation strategy uses a part of the on-chip memory resources as a local memory to achieve better performance than a cache only architecture. The object allocation strategy has been implemented using an annotation based approach and shown to be effective in improving performance and reducing the memory system energy using the SPECJvm98 benchmarks.","Java,
Energy consumption,
Batteries,
Random access memory,
Frequency,
Computer science,
Power engineering and energy,
Application software,
Mobile computing,
Virtual machining"
"A method for asynchronous, web-based lecture delivery","Computer-based distance education has been growing enormously in recent years due to advances in key technologies: telecommunications, computer technology, graphical user interfaces, and the Internet. Distance education materials can be presented either synchronously, creating the illusion of a classroom on the computer, or asynchronously, allowing the students access-on-demand. We describe the development of a system to solve part of the asynchronous education problem-delivery of pre-recorded multimedia lectures via a web browser. We built a demonstration system using off-the-shelf technology making it possible to make presentations using commercial tools and to display them on standard systems. We designed the system with lectures composed of ""slides"" accompanied by audio commentary. These presentations can be delivered via low-speed modem links-an important consideration in places without a well-developed infrastructure. We also believe that computer-based education is often so complicated to use that operation interferes with education. In order to avoid this, we evaluated a prototype system to obtain feedback about difficulties in operation and to correct them.",
Building a scalable web server with global object space support on heterogeneous clusters,,"Web server,
Peer to peer computing,
Cooperative caching,
Network servers,
Delay,
IP networks,
Service oriented architecture,
File systems,
Wide area networks,
Computer science"
A visualization tool for analyzing cluster performance data,,
Form Invariance and Implicit Parallelism,"Holland's schema theorem (an inequality) may be viewed as an attempt to understand genetic search in terms of a coarse graining of the state space. Stephens and Waelbroeck developed that perspective, sharpening the schema theorem to an equality. Of particular interest is a form invariance of their equations; the form is unchanged by the degree of coarse graining. This paper establishes a similar form invariance for the more general model of Vose et al. and uses the attendant machinery as a springboard for an interpretation and discussion of implicit parallelism.","schemata,
Course graining,
form invariance,
genetic algorithms,
implicit parallelism,
intrinsic parallelism,
mixing scheme"
Modular microprocessor kit for undergraduate laboratory on industrial automation,"Teaching microprocessors to mechanical engineering students has become part of the new curriculum of the Escola Politecnica of the University of Sao Paulo, since 1988. The theoretical discipline, where microprocessor small systems design is taught, is complemented with laboratory activities where the student faces a one-semester project for solving a problem with a modular microprocessor kit on the subject of industrial automation. The concept of modularity introduced in this microprocessor kit allows the student to propose his or her own project and configure a system that allows the students to have contact with three basic issues concerning industrial automation: actuation, sensing and I/O commands. The kit is composed of several modules, that can be interconnected between themselves and a linear position system with two actuators versions, one with DC motor and encoder and other with stepping motor. Each module consists of a PC board of 100/spl times/130 mm of dimensions and, except for the motor's drivers, they can be connected to each other by flat cable or by the piggy-back method which creates a stack of boards. This paper describes the modular microprocessor kit and its application to an undergraduate laboratory to solve industrial automation problems.",Mechanical engineering
Recent advances in conductivity and EOS modeling for high energy density physics simulations near the metal-insulator transition,"Summary form only given. Many present day high energy density physics experiments, such as wire-array Z pinches, imploding liners, and exploding wires, evolve through complex conductivity and equation of state regimes, starting from solid density and rapidly transitioning through liquid and vapor to plasma. Of particular importance for the accurate computer simulation of the early time evolution of these experiments is the phase space in the vicinity of the metal-insulator transition and the solid-liquid-vapor transitions. The commonly used Lee-More and Rinker (SESAME) conductivities, primarily developed for higher density and temperature regimes, are inaccurate at these lower temperatures and densities, often differing with experimental data by several orders of magnitude. A recently developed set of modifications (Desjarlais, 2001) to the Lee-More algorithm provide greatly improved agreement with experimental data and have permitted significant improvements to the correspondence between simulations and experiments. We have extensively studied the Cornell exploding wire experiments with particular emphasis on the role of the conductivity model and the equation of state in the wire evolution. Simulations from the Mach II and ALEGRA codes, with elaborate phase space diagnostics, have helped identify critical regions in phase space and have suggested directions for further improvement in the transport and EOS modeling.","Conductivity,
Earth Observing System,
Plasma temperature,
Equations,
Wire,
Physics,
Solids,
Plasma density,
Plasma simulation,
Computer simulation"
Software resource architecture and performance evaluation of software architectures,"Performance is determined by a system's resources and its workload. Some of these resources are software resources which are embedded in the software architecture; some of them are even created by the software architecture. This paper considers software resources and resource architecture, as an aspect of software architecture. It considers how resource architecture emerges, the relationship of software and hardware resources, some classes of resource architecture, and what they can tell us about system performance.","Software performance,
Computer architecture,
Software architecture,
Databases,
Delay,
Multicast protocols,
Access protocols,
Buffer storage,
Systems engineering and theory,
Tellurium"
Web based VRML modeling,"Presents a method to connect VRML (Virtual Reality Modeling Language) and Java components in a Web page using EAI (External Authoring Interface), which makes it possible to interactively generate and edit VRML meshes. The meshes used are based on regular grids, to provide an interaction and modeling approach that uses the internal semantics of such a mesh by linking the available modeling operations to either single vertices, the vertices of a ring or column, or all the vertices from the VRML mesh. Our method permits strict mesh complexity control and scales the operations according to the mesh properties. We describe the structure of the system and provide a few examples of the meshes that were created.","Java,
Virtual reality,
Writing,
Computer science,
Web pages,
Joining processes,
World Wide Web,
Virtual environment,
Prototypes,
Smoothing methods"
An evolving instructional design model for designing Web-based courses,"The complexity of design for Web-based courses requires the incorporation of instructional design principles. However, the majority of the existing models were not adequate to address the needs of Web based learning. This project therefore stems from the fact that a specific instructional design model is needed to guide designers in developing Web based courses. A study was conducted in a multimedia course offered by the Multimedia University, Malaysia to investigate the scaffoldings for Web based learning. The findings from the study conform to the components in which the SWLing instructional design model was developed.","Electronic mail,
Web sites,
Feedback,
Process design,
Design methodology,
Context modeling,
Computer science education,
Computer graphics,
Application software,
Frequency"
Educational opportunities for US students abroad: how to internationalize and diversify your university,"Summary form only given. This paper discusses practical and effective models to help students study abroad and attract international students from around the world -with no lost revenue for educational institutions. The authors share insights into study abroad opportunities for US based engineering students, and describe some of the challenges posed by study abroad based on the experiences of the Global Engineering Education Exchange (Global E3) consortium. Topics covered include: how to promote study abroad to faculty and students; where students can study engineering overseas; financial aid and scholarships; and how to prepare students for the study abroad experience. The discussion also explores ways to welcome international students and how to advise them on academic and internship opportunities. The Global Engineering Education Exchange (Global E3) is an international exchange program that allows engineering students from 30+ US consortium-member institutions to study abroad and for students from international institutions in 14 countries to study at US member institutions. Through Global E3, students take courses overseas for credit at their home institutions, and receive practical training in another country, if desired. At the same time, the Global E3 program strives to minimize the increased costs associated with most study abroad programs.",
Counterpropagation neural networks for trademark recognition,"This work considers the possibility of using the connected component algorithm for segmentation to extract the features inherent in the studied objects and counterpropagation neural networks (CPN) for the learning capability for object recognition. Neural networks do not need any mathematical model to determine the system output depending upon the given inputs. Instead they behave as model free estimators and their output is closest to the already ""learned"" patterns. Neural networks have conventionally been used for a variety of automatic target detection, character recognition, control etc., but in the case of multiple integrated object matching, such as trademarks, solutions have yet to be found due to the complex mixture of graphics and texts comprised in the logo. CPN operates on the principle of closeness in the n-dimensional Euclidian space. Very encouraging results are observed.","Counting circuits,
Neural networks,
Trademarks,
Feature extraction,
Image segmentation,
Shape measurement,
Signal processing algorithms,
Computer science,
Information systems,
Data mining"
GTREP: a new model for expanding the availability of engineering education,"The Georgia Tech Regional Engineering Program (GTREP) expands engineering education opportunities by offering Georgia Tech undergraduate engineering degrees in southeast Georgia in collaboration with Armstrong Atlantic State University, Georgia Southern University and Savannah State University. While branch campuses and remote programs are not new, GTREP incorporates a number of innovative elements. Students initially enroll at one of the three regional institutions, then become Georgia Tech students for their junior and senior years, while remaining in Savannah or Statesboro and taking courses taught by locally-resident Georgia Tech faculty. Distance learning technology, including two-way video and Internet-based courses, increases the available course offerings and fosters interactions between the main and southeast campuses. GTREP faculty members in southeast Georgia are regular tenure-track Georgia Tech faculty, engaged in both instruction and research. This paper addresses the challenges and innovative solutions associated with bringing GTREP from concept to initial graduates in less than four years.","Engineering education,
Agricultural engineering,
Computer aided instruction,
Collaboration,
Internet,
Educational institutions,
Civil engineering,
Job design,
Broadband communication,
Communications technology"
Local-area mobility support through cooperating hierarchies of mobile IP foreign agents,"Mobile IP presents an efficient solution to the wide-area host mobility problem in the Internet. Nevertheless, its home registration process introduces an overhead in the local-area mobility case. A mobile host is required to register with its possibly distant home agent, whenever it changes its point of attachment to the network. Foreign agent hierarchies have been introduced to perform regional mobile IP registration to minimize the home registration signaling overhead. We propose a novel configurable architecture to organize cooperating foreign agent hierarchies in the foreign domain. An attempt is made not to change the mobile host home registered care-of address as long as it is within the foreign domain. In such a manner, home registration signaling overhead is minimized, and the home agent is isolated from any local-area movement by the mobile host.","IP networks,
Protocols,
Mobile computing,
Wireless communication,
Computer networks,
Portable computers,
Home automation,
Wireless networks,
Computer science,
Internet"
A binary-categorization approach for classifying multiple-record Web documents using application ontologies and a probabilistic model,"The amount of information available on the World Wide Web has been increasing dramatically in recent years. To enhance speedy searching and retrieving Web documents of interest, researchers and practitioners have partially relied on various information retrieval techniques. We propose a probabilistic model to classify Web documents into relevant documents and irrelevant documents with respect to a particular application ontology, which is a conceptual-model snippet of standard ontologies. Our probabilistic model is based on multivariate statistical analysis and is different from the conventional probabilistic information retrieval models. The experiments we have conducted on a set of representative Web documents indicate that the proposed probabilistic model is promising in binary-categorization of multiple-record Web documents.","Information retrieval,
Ontologies,
Application software,
Web sites,
Internet,
Computer science,
Statistical analysis,
Organizing,
Decision theory,
Probability"
The estimated cost of a search tree on binary words,"The problem of constructing a binary search tree for a set of binary words has wide applications in computer science, biology, mineralogy, etc. Shannon considered a similar statement in his optimal coding theorem. It is NP-complete to construct a tree of minimum cost; therefore, the problem arises of finding simple algorithms for constructing nearly optimal trees. We show that there is a simple algorithm for constructing search trees sufficiently close to the optimal tree on average. By means of this algorithm we prove that for the optimal tree the average number of bits to be checked is near to its natural lower bound, i.e., the binary logarithm of the number of given words: their difference is less than 1.04.",Tree searching
Analysis on impact propagation of docking platform for Spacecraft,"When spacecraft dock with each other, impact will rise at the docking point. It is assumed in this paper that a docking platform having the shape of Stewart platform is attached at the one of the docking spacecraft to reduce the effect of impact. Although the impact between two spacecraft at the contact point of the upper plate of the platform is very large, the impact is absorbed by the docking platform and a negligible impact is transmitted to the spacecraft. To show this, the modeling of the internal impulses at the joints of multibody system is introduced and the internal impulses of the docking platform are analyzed by the modeling method.","Space vehicles,
Manipulators,
Birds,
Robot kinematics,
Motion analysis,
Force control,
Equations,
Mechanical engineering,
Computer science,
Shape"
BClassifier: a personal agent for bookmark classification,"For the study of software systems helping users reduce personal tasks, we implemented an agent, called BClassifier, which classifies bookmarks on behalf of the user. The BClassifier is an agent based on a naive Bayesian learning algorithm which acquires knowledge through training with examples. We present the findings of experimentation where the BClassifier consistently exhibits success rates over 70%. Two other types of learning methods are compared with the naive Bayesian technique.","Bayesian methods,
Software systems,
Uniform resource locators,
Computer science,
Software algorithms,
Learning systems,
Microcomputers,
Communication networks,
Acceleration,
IP networks"
Towards efficient resource allocation in distributed systems management,,"Resource management,
Availability,
Monitoring,
Control systems,
Environmental management,
Data analysis,
Distributed computing,
Computer science,
Distributed control,
Instruments"
The design and implementation of automata-based testing environment for Java multi-thread programs,"Deterministic execution testing has been considered a promising method for concurrent program testing because of its reproducibility. However, since deterministic execution requires that a synchronization sequence to be replayed is feasible and valid, it is not directly applicable to a situation in which synchronization sequences, being valid but infeasible, are taken into account. To resolve this problem, we proposed automata-based testing in previous work, where a concurrent program is executed according to one sequence accepted by the automaton recognizing all sequences semantically equivalent to a given sequence. In this paper, we present the automata-based testing environment for Java multi-thread programs, and design and implement key components-automata generator, program transformer and replay controller. Algorithms for generating the equivalence automaton of a given sequence are presented and a program transformation method is suggested in order to guide a program to be executed according to the sequence accepted by the automaton. The replay controller is also redesigned and implemented to adopt the automaton. By illustrating automata-based testing procedures with the gas station example, we show how the proposed approach works in Java multi-threaded programs.",
Deadlock avoidance in manufacturing systems with non-deterministic part routings,"A deadlock avoidance algorithm for flexible manufacturing systems containing both multiple capacity resources and mixed choices in part routing is presented. The method determines whether moving a part to its next step is safe, unsafe, or undetermined. That classification is linear in complexity. Undetermined part movements are further analyzed using another procedure, which attempts to empty the system virtually to determine whether the move is safe or not. It is polynomial in complexity. An example is provided to illustrate how the method can be applied.",
NASA Goddard Space Flight Center Virtual System Design Environment,"The Virtual System Design Environment (VSDE) is one of a number of initiatives under development through NASA Goddard Space Flight Center's (GSFC's) Advanced Engineering Services and Environments (AESE) Office. The AESE Science Team, a broad cross section of Earth and space scientists, identified needs and challenges they face in their daily work and collaboration with GSFC engineers. Prominent amid these challenges were the tasks and the engineering support necessary to effectively perform system concept (mission, instrument and science measurement concepts) development and formulation. They identified that the work and collaborative efforts with other scientists, engineers, and team members who are geographically distributed could be significantly enhanced, greatly increasing science value. The VSDE users are scientists, engineers, team leaders and project managers, working on system development efforts. The VSDE has been developed in close collaboration with these customers and users. The initial deployment has focused on supporting a collaborative design activity to modify a spacecraft structural component Use of VSDE facilitates capturing the effects within other subsystems through the VSDE portal notification mechanism while maintaining configuration management and version control over the entire design cycle. In this case, engineers used both the collaboration services such as white boarding to mark up 3-D Computer Aided Design (CAD) models, and analysis tools, such as Matlab and ProE, within the VSDE Portal environment to collaborate on the design problem In this paper we present the VSDE project system capabilities and technologies, a case study and the tools used, and provide a discussion of metrics collected during the initial deployment to feature user feedback, design cycle time savings, and current as well as future capabilities.","NASA,
Aerospace engineering,
Collaborative work,
Design engineering,
Portals,
Collaborative tools,
Geoscience,
Performance evaluation,
Instruments,
Project management"
An adaptive person recognition system,"We propose a framework for integrating the processes of object recognition and knowledge adaptation. This framework acknowledges that the performance of the object recognition process depends directly on the state of the system's internal knowledge, i.e., its memory, and conversely, that the efficacy of the system's knowledge adaptation process is enhanced by its ability to recognize objects with greater accuracy. Thus, object recognition and knowledge adaptation are inseparable aspects of the same cognitive task, and must be co-ordinated if the system is to be robust in the context of objects that present variations in illumination, scale, shift, and other temporal variations. Specifically, the presented system combines a multiple-cue person recognition component and an example-based knowledge adaptation component, and is applied to the task of automatic video indexing of personal appearance events. We present the details of this integrated framework and demonstrate successful experimental results.","Adaptive systems,
Object recognition,
Indexing,
Streaming media,
Pattern recognition,
Spatial databases,
Computer science,
Robustness,
Lighting,
Face recognition"
Object distribution with local information,"We investigate the problem of distributing communicating objects across wide-area environments. Our goals are to balance load, minimize network communication, and use resources efficiently. However, applications running in such environments are often dynamic and highly unpredictable. Furthermore, synchronous communication is usually too expensive to be used in disseminating load information. We therefore investigate policies that use local information to approximate desired global behaviors. Our results with Java applications show that simple, local approaches are surprisingly effective in capturing load information and object relationships, and in making migration and clustering decisions based on profiled information.","Computer science,
Educational institutions,
Java,
Parallel processing,
Application software,
Metacomputing,
Large-scale systems,
Testing,
Costs,
Computer networks"
Copy-based versus edit-based version management schemes for structured documents,"Managing multiple versions of XML documents and semistructured data represents a problem of growing interest. Traditional version control methods, such as RCS, use edit scripts representing changes in the document to support the incremental reconstruction of different versions. The edit-based approaches have been enhanced with a replication scheme called UBCC (Chien et al., 2000). UBCC is based on the notion of page usefulness and ensures effective management for multi-version documents in terms of both retrieval and storage cost. These improvements notwithstanding, the edit-based representation suffers from limited generality and flexibility-e.g., it cannot represent changes such as rearranging the document or duplicating parts of its content. To solve these problems, the paper proposes a copy-based UBCC versioning scheme, which also provides a simpler format for the electronic exchange of multi-version documents. With the objective of matching the performance of the edit-based UBCC technique, we develop algorithms that enhance the copy-based UBCC scheme with page usefulness management. We also present results of various experiments that test the storage and retrieval performance of the new copy-based approach, and compare it with that of the edit-based UBCC approach.","Costs,
XML,
Computer science,
Testing,
Content management,
Collaborative work,
Standards development,
Protection,
History,
Intrusion detection"
Shape-based interpolation using morphological morphing,We propose an interpolation algorithm using a morphological morphing approach. The aim of this algorithm is to reconstruct an n-dimensional object from a group of (n-1)-dimensional sets representing object sections. The morphing transformation modifies consecutive sets so that they approach in shape and size. When the two morphed sets become idempotent we generate a new set. The entire object is modeled by successively interpolating a certain number of intermediary sets between each two consecutive initial sets. The interpolation algorithm is used for 3D tooth reconstruction.,"Shape,
Interpolation,
Image reconstruction,
Computer science,
Teeth,
Magnetic resonance,
Tomography,
Visualization,
Skeleton,
Informatics"
Visually testing recursive programs in spreadsheet languages,"Although there has been recent research into ways to design visual programming languages and environments, little attention has been given to systematic testing in these languages, and what work has been done does not address ""power"" features such as recursion. In this paper, we discuss two possible ways the ""What You See Is What You Test"" methodology could be extended to accommodate recursion. The approaches are presented in terms of their testing theoretic aspects and then implementation strategies and algorithms. Since the goal is to help the people using these languages, we also present an empirical study and use its results to inform our choice as to which of the two approaches to adopt.","Programming profession,
Computer languages,
Logic testing,
Feedback,
Pipelines,
Computer science,
System testing,
Software prototyping,
Prototypes,
Data visualization"
Error masking probability of 1's complement checksums,"In the transport layer of the TCP/IP protocol suite, both TCP and UDP use Internet checksum to protect headers and data. Internet checksum uses 1's complement arithmetic to detect errors in the content delivered by the data-link layer. Both research and experience have shown that there are a wide variety of error sources which cannot be detected by this lower layer. The error detecting performance of 1's complement checksum determines how many of these errors will be passed to higher layers, including the application layer. The performance analysis also influences protocol design and improvement, for example, header compression. Unfortunately, previous work on this topic only determined the number of error passing patterns and the probability for 2 and 3 bit errors, and the method used for determining the probability is hard to extend to more bit errors. We present a method to generate the formula of error passing probability. When too much calculation is needed to compute an exact result, we achieve a better estimation of the probability, which is around 3 percent of the upper bound achievable with previous techniques when 1's complement checksum is used in TCP/UDP.","Cyclic redundancy check,
Internet,
TCPIP,
Computer errors,
Arithmetic,
Performance analysis,
Hardware,
Computer science,
Transport protocols,
Protection"
Teaching computer systems performance analysis,"A course on computer systems performance analysis has been adapted for several different distance education delivery options, including an interactive television system, face-to-face presentation at a satellite campus, and delivery over the Internet to independent study students. Of the 122 students who have enrolled in this graduate-level course for a grade over the three-year period analyzed, half have been nontraditional students who never set foot on campus. These remote students have a substantially higher drop-out rate than the traditional on-campus students, and frequently indicate a strong preference for face-to-face instruction in a traditional classroom setting. Nevertheless, due to significant differences in the characteristics of the two student groups, the remote students typically earn higher final course grades than the on-campus students. While there is a strong demand for delivery of this type of advanced course to remote students, more still needs to be done to effectively engage these students in the learning process.",Computer science education
Protection of keys against modification attack,"Anderson and Kuhn (1997) described an attack against tamper-resistant devices wherein a secret key stored in EEPROM is compromised using a simple and low-cost attack. The attack consists of setting bits in the EEPROM using low-cost probes and observing the effect on the output of the device. These attacks are extremely general, as they apply to virtually any cryptosystem. The objective of the present work is to explore cryptographic techniques with the goal of raising the cost (in terms of time and money) of carrying out the EEPROM modification attack by Class I attackers, at least to a point where it is as prohibitive as the cost of purchasing more expensive equipment. We propose the m-permutation protection scheme in which the key will be encoded in a special way and burned into the EEPROM of the device. To attack the scheme, the attacker needs to be able to solve for K in the equation K=/spl oplus//sub i=1//sup m/P/sub i/ in which P/sub i/'s are unknown. It is observed that the m-permutation protection scheme does not distribute the key K uniformly. Analysis shows that m=3 or m=5 are already good enough practically to provide strong security if the encoding is done properly and that m>5 may not give significant improvement to the security of the scheme.","Protection,
EPROM,
Costs,
Cryptography,
Computer science,
Water storage,
Security,
Needles,
Read only memory,
Probes"
New paradigm for segmentation and recognition of handwritten numeral string,"String recognition is rather paradoxical problem because it requires the segmentation of the string into understandable units, but proper segmentation needs a-priori knowledge of the units and this implies a recognition capability. To solve this dilemma therefore, both a-priori knowledge of meaningful units and a segmentation method have to be used together, and they should dynamically interact with each other. In other words, the results of segmentation are used as fundamental information to suppose what is most likely to be, and then its a-priori knowledge is used to help the segmentation. This model makes explicit segmentation unnecessary because it does not speculate on possible break positions. It is also possible to recognize a digit even if it contains strokes that do not belong to to it. Using this paradigm for 100 handwritten numeral strings belonging to the NIST database has resulted in 95% recognition.",
Directory based composite routing and scheduling policies for dynamic multimedia environments,"In this paper, we present and evaluate algorithms to address combined path and server selection (CPSS) problems in highly dynamic multimedia environments. Our goal is to ensure effective utilization of network and server resources while tolerating imprecision in system state information. Components within the framework implement the optimized scheduling policies as well as collect/update the network and server parameters using a directory service. We present and analyze multiple policies to solve the combined path and server selection (CPSS) problem. In addition, we study multiple techniques for updating the directory service with system state information. We further evaluate the performance of the CPSS policies under different update mechanisms and study the implications of the CPSS policies on directory service management.","Routing,
Dynamic scheduling,
Network servers,
Processor scheduling,
Computer science,
Electrical capacitance tomography,
Web and internet services,
Resource management,
Load management,
Delay"
CTL and ACTL patterns,"Model checking has become a widely used technique for formal verification of concurrent systems, such as communication protocols. However its rise is still much restricted to scientists with high mathematical education because temporal logic formulae are difficult to understand and even more difficult to create. Therefore, many projects have been started to find out how computers can help engineers in specifying system properties. This paper reports on using patterns of temporal logic formulae, which facilitate the usage of model checking.","Computer science,
Systems engineering and theory,
Formal verification,
Protocols,
Formal languages,
Algebra,
Natural languages,
Automatic logic units"
Rotating ultrasonic signal vectors with a word-parallel CORDIC processor,"The CORDIC algorithm is an iterative method for the efficient computation of vector rotations and several other trigonometric and hyperbolic functions. We have developed a fast, redundant, constant-scale factor, word-parallel implementation of a CORDIC algorithm to rotate ultrasonic signal vectors. The implementation is an improvement of a similar 1996 algorithm know as the differential CORDIC. The CORDIC processor is a part of an ultrasonic imaging system under development and has been implemented using logic synthesis of VHDL descriptions on a Xilinx Virtex 800 FPGA. The algorithm has been simulated with MATLAB. The results of simulation and testing of the CORDIC rotator using a novel VHDL testbench have been presented. The error resulting from truncations is well within the expected limit.","Signal processing,
Sensor arrays,
Signal processing algorithms,
Iterative algorithms,
Ultrasonic imaging,
Testing,
Frequency,
Phased arrays,
Ultrasonic transducer arrays,
Computer science"
A study of synthetic creativity through behavior modeling and simulation of an ant colony,"The objective is to scientifically study the nature of creativity by modeling synthetic creativity in an ant colony, simulating it on a computer system, and measuring its impact on performance through innovative metric design. In this study, two synthetic creative traits are introduced into select individual ants of a colony by imparting to them a foraging behavior that is radically different from the normal behavior. Analysis of the simulation results reveal that a creative trait coupled with the underlying parameters of the ant colony may cause the foraging completion time metric, i.e. the time to collect food from all sources, to be marginally better or weaker than the normal colony. Contrary to intuition, the completion time metric worsens when, in a fixed sized colony, the number of creative ants, relative to normal ants, becomes excessive.",
E-ADOME: enacting composite E-services in an advanced workflow environment,"E-services refer to services provided via the Internet. To provide value-added and composite e-services, we need an information system that can coordinate API-based, interactive web-based and human agents, internal and external to an organization. We present an environment to model and enact composite e-services as workflows, by extending our ADOME-WFMS (Advanced Object Modeling Environment-Workflow Management System) to E-ADOME, which supports composing new e-services and effective management of agents for enacting e-service.","Web and internet services,
Humans,
Packaging,
Web server,
Computer science,
Environmental management,
Workflow management software,
Information technology,
Management information systems,
Home appliances"
Dynamic cluster configuration and management using JavaSpaces,,"Java,
Resource management,
Space technology,
Conference management,
Computer science,
Informatics,
Dynamic scheduling,
Control systems,
Availability,
Prototypes"
WWG: a wide-area infrastructure for group work,"Group learning at Internet scale is becoming more frequent in university courses. This complex process requires support by distributed computing learning infrastructures. This paper describes the design of WWG: a distributed and decentralized infrastructure with the aim of supporting distributed group learning and team work, centered on the distribution of events, so that every participant can be notified and thus be aware of the actions, changes, progress of the groups he or she belongs to - synchronous awareness for asynchronous work. The design issues, requirements and the resulting architecture are presented. WWG is based on a multicast mechanism for event distribution with meta-information agents responsible for the dissemination and transformation of events, repository agents responsible for the storage of group information and user agents responsible for the representation of users (sources and sinks of events).","Internet,
Distributed computing,
Virtual groups,
Educational institutions,
Professional activities,
Computer aided instruction,
Computer science education,
Engineering education,
Proposals,
Scalability"
Efficient Network and I/O Throttling for Fine-Grain Cycle Stealing,"This paper proposes and evaluates a new mechanism, rate windows, for I/O and network rate policing. The goal of the proposed system is to provide a simple, yet effective way to enforce resource limits on target classes of jobs in a system. This work was motivated by our Linger Longer infrastructure, which harvests idle cycles in networks of workstations. Network and I/O throttling is crucial because Linger Longer can leave guest jobs on non-idle nodes and machine owners should not be adversely affected. Our approach is quite simple. We use a sliding window of recent events to compute the average rate for a target resource. The assigned limit is enforced by the simple expedient of putting application processes to sleep when they issue requests that would bring their resource utilization out of the allowable profile. Our I/O system call intercept model makes the rate windows mechanism light-weight and highly portable. Our experimental results show that we are able to limit resource usage to within a few percent of target usages.","Workstations,
Computer science,
Educational institutions,
Permission,
Resource management,
Communication system control,
Processor scheduling,
Delay,
Central Processing Unit"
Optimized address assignment for DSPs with SIMD memory accesses,"This paper deals with address assignment in code generation for digital signal processors (DSPs) with SIMD (single instruction multiple data) memory accesses. In these processors data are organized in groups (or partitions), whose elements share one common memory address. In order to optimize program performance for processors with such memory architectures it is important to have a suitable memory layout of the variables. We propose a two-step address assignment technique for scalar variables using a genetic algorithm based partitioning method and a graph based heuristic which makes use of available DSP address generation hardware. We show that our address assignment techniques lead to a significant code quality improvement compared to heuristics.","Digital signal processing,
Registers,
Digital signal processors,
Genetic algorithms,
Assembly,
Digital signal processing chips,
Computer science,
Signal generators,
Memory architecture,
Hardware"
Applying WinWin to quality requirements: a case study,"Describes the application of the WinWin paradigm to identify and resolve conflicts in a series of real-client, student-developer digital library projects. The paper is based on a case study of the statistical analysis of 15 projects and an in-depth analysis of one representative project. These analyses focus on the conflict resolution process, stakeholders' roles and their relationships to quality artifacts, and tool effectiveness. We show that stakeholders tend to accept satisfactory rather than optimal resolutions. Users and customers are more proactive in stating win conditions, whereas developers are more active in working toward resolutions. Further, we suggest that knowledge-based automated aids have potential to significantly enhance process effectiveness and efficiency. Finally, we conclude that such processes and tools have theoretical and practical implications in the quest for better software requirements elicitation.","Computer aided software engineering,
Software quality,
Costs,
Degradation,
Computer science,
Software engineering,
Application software,
Software libraries,
Statistical analysis,
Software tools"
Applied knowledge management in innovation processes,"While the efficiency of business processes like order fulfillment or customer service relies mainly on the exploitation of existing knowledge or the dissemination of new knowledge, innovation processes have to focus on the efficient creation of new knowledge. This paper is guided by an analysis of the knowledge creation process in innovation processes.","Knowledge management,
Technological innovation,
Electronic switching systems,
Companies,
Technology management,
Computer science education,
Educational technology,
Computer networks,
IP networks,
Explosions"
Multicast-based Distributed LVS (MD-LVS) for improving scalability and availability,"Clustering with a single-system image view is the most commonly used approach to increase throughput of a Web site. There are two types of clustering architecture: centralized IP cluster and distributed IP cluster. In the centralized IP cluster the load balancer distributes incoming requests of clients to an appropriate real server based on load characteristics. However the availability and scalability of the cluster system is low, since this kind of solution creates a single-point-of-failure and the total throughput of the cluster is limited by the performance of the load balancer. We propose the Multicast-based Distributed LVS (MD-LVS), which combines the centralized IP cluster and the distributed IP cluster. There are multiple load balancers, and each of the load balancers has an individual and independent cluster group, which consists of several real servers. This mechanism efficiently can improve scalability and availability of the cluster system, since it is easy to expand the cluster system and may avoid the entire system failure.","Scalability,
Web server,
Load management,
Throughput,
Internet,
Distributed computing,
Computer science,
Computer architecture,
Explosives,
Web sites"
A packet scheduling algorithm in high performance routers,"The Internet is facing two problems simultaneously: there is a need for a faster switching/routing infrastructure and a need to introduce guaranteed qualities-of-service (QoS). Each problem can be solved independently: high performance routers can be made faster by using input-queued crossbars instead of shared memory systems; QoS can be provided using packet fair queueing (PFQ) algorithms. Until now, however, the two solutions have been mutually exclusive-all of the work on PFQ algorithms has required that routers use output-queueing or centralized shared memory. In this paper, we design and implement a packet scheduling algorithm, DF/sup 2/Q (distributed feedback fair queueing), on the basis of CIOQ (combined input output queueing) architecture. The most important feature of this scheduling algorithm is the introduction of a feedback mechanism. We analyze and discuss the performance of DF/sup 2/Q, and the experimental results show that it can avoid internal congestion effectively and improve the efficiency of resource utilization.",
A global optimisation approach to classification in medical diagnosis and prognosis,Global optimisation based techniques are studied in order to increase the accuracy of medical diagnosis and prognosis with FNA image data from the Wisconsin Diagnostic and Prognostic Breast Cancer databases. First we discuss the problem of determining the most informative features for the classification of cancerous cases in the databases under consideration. Then we apply a technique based on convex and global optimisation to breast cancer diagnosis. It allows the classification of benign cases and malignant ones and the subsequent diagnosis of patients with very high accuracy. The third application of this technique is a method that calculates centres of clusters to predict when breast cancer is likely to recur in patients for which cancer has been removed. The technique achieves higher accuracy with these databases than reported elsewhere in the literature.,"Medical diagnosis,
Breast cancer,
Spatial databases,
Image databases,
Cancer detection,
Optimization methods,
Mammography,
Oncological surgery,
Information technology,
Computer science"
A novel low power multiplexer-based full adder cell,"A novel low-power multiplexer-based 1-bit full adder cell that uses 12 transistors (MBA-12T) is presented here. MBA-12T is tested along with four other low-power 10-transistor 1-bit full adders that were shown to have more than 26% in power-savings over the conventional 28-transistor CMOS cell. The testing consists of simulating using HSpice under 6 frequencies, and 6 different loads. The testing result shows that the MBA-12T exhibits at least 23% in power-savings over the least power-consuming 10-transistor cell and a minimum of 64% in speed improvement.","Adders,
Testing,
Circuits,
Multiplexing,
Frequency,
Throughput,
Computer science,
Power engineering and energy,
Transistors,
Explosives"
Rod pinch radiography source optimization at 2.3 MV,"Summary form only given, as follows. Rod pinch diodes have shown considerable promise as high-brightness flash X-ray sources for penetrating dynamic radiography for a variety of DOE Defense Programs applications. The rod pinch diode uses a small diameter (0.4 - 2 mm) anode rod extended through a cathode aperture. When properly configured, the electron beam born off of the aperture edge can self-insulate and pinch onto the tip of the rod creating an intense, small X-ray source. Experiments have been performed on Sandia's SABRE accelerator (2.3 MV, 40 Ohms, 60 ns) to search a wide parameter space to optimize the source by maximizing the figure of merit (dose/spot diameter/sup 2/) and minimizing the diode impedance droop. Many diode parameters have been examined in detail including rod diameter, rod length, rod material, cathode aperture diameter, and vacuum pressure. The best configuration tested so far uses a 0.5 mm diameter gold rod, a 6 mm rod extension beyond the cathode aperture (diameter = 8 mm), to produce 3.5 rad at 1 m from a 0.8 mm X-ray spot. A large experimental database has been developed to benchmark computer codes so that physics details can be extracted in this complex beam-plasma environment. Further physical insight is necessary to extend the rod pinch operations to higher voltages.",
An evaluation model for Web-based instruction,"Recently Web-based instruction (WBI) has been adopted for many educational systems in order to support distance education. WBI has become popular in that it overcomes time and space limitation in traditional educational systems. But due to lack of face-to-face communication, it is crucial that WBI provide interactivity and motivation for students. This paper introduces a formal model that evaluates interactivity and motivation for courses based on WBI. The model is comprehensive and objective so that it can be used to evaluate any course. Based on the model, the paper selects some WBI courses and compares them for their interactivity and motivation. Finally, the paper concludes with a discussion of further research issues.",Computer aided instruction
Approximating large convolutions in digital images,"Computing discrete two-dimensional (2-D) convolutions is an important problem in image processing. In mathematical morphology, an important variant is that of computing binary convolutions, where the kernel of the convolution is a 0-1 valued function. This operation can be quite costly, especially when large kernels are involved. We present an algorithm for computing convolutions of this form, where the kernel of the binary convolution is derived from a convex polygon. Because the kernel is a geometric object, we allow the algorithm some flexibility in how it elects to digitize the convex kernel at each placement, as long as the digitization satisfies certain reasonable requirements. We say that such a convolution is valid. Given this flexibility we show that it is possible to compute binary convolutions more efficiently than would normally be possible for large kernels. Our main result is an algorithm which, given an m/spl times/n image and a k-sided convex polygonal kernel K, computes a valid convolution in O(kmn) time. Unlike standard algorithms for computing correlations and convolutions, the running time is independent of the area or perimeter of K, and our techniques do not rely on computing fast Fourier transforms. Our algorithm is based on a novel use of Bresenham's (1965) line-drawing algorithm and prefix-sums to update the convolution incrementally as the kernel is moved from one position to another across the image.","Digital images,
Kernel,
Morphology,
Computer science,
Two dimensional displays,
Image processing,
Automation,
Fast Fourier transforms,
Geometry,
Mathematics"
Modular neural networks exploit large acoustic context through broad-class posteriors for continuous speech recognition,"Traditionally, neural networks such as multi-layer perceptrons handle acoustic context by increasing the dimensionality of the observation vector, in order to include information of the neighbouring acoustic vectors, on either side of the current frame. As a result the monolithic network is trained on a high multi-dimensional space. The trend is to use the same fixed-size observation vector across the one network that estimates the posterior probabilities for all phones, simultaneously. We propose a decomposition of the network into modular components, where each component estimates a phone posterior. The size of the observation vector we use, is not fixed across the modularised networks, but rather accounts for the phone that each network is trained to classify. For each observation vector, we estimate very large acoustic context through broad-class posteriors. The use of the broad-class posteriors along with the phone posteriors greatly enhance acoustic modelling. We report significant improvements in phone classification and word recognition on the TIMIT corpus. Our results are also better than the best context-dependent system in the literature.","Neural networks,
Training data,
Multi-layer neural network,
Multilayer perceptrons,
Hidden Markov models,
Detectors,
Speech recognition,
Computer science,
Speech processing,
Computer architecture"
Defining computing curricula for the modern age,"In 1998, a joint task force of the IEEE Computer Society and the Association for Computing Machinery set out to develop a set of undergraduate curricular guidelines that would match the latest developments of computing technologies in the past decade and endure through the next decade. To meet this daunting challenge, the task force decided that, instead of attempting a definitive work that covered all aspects of computing, they would prepare an overview volume and create or solicit curricular guides for a number of subfields. The Ironman Draft of Computing Curricula 2001 () consists of a detailed review of curricula which are appropriate for computer science programs, as well as the basic structure for the component volumes.",
"Convergent evolution of protein structure prediction and computer chess tournaments: CASP, Kasparov, and CAFASP","Predicting the three-dimensional structure of a protein from its amino acid sequence is one of the most important current problems of modern biology. The CASP (Critical Assessment of Structure Prediction) blind prediction experiments aim to assess the prediction capabilities in the field. A limitation of CASP is that predictions are prepared and filed by humans using programs, and thus, what is being evaluated is the performance of the predicting groups rather than the performance of the programs themselves. To address this limitation, the Critical Assessment of Fully Automated Structure Prediction (CAFASP) experiment was initiated in 1998. In CAFASP, the participants are programs or Internet servers, and what is evaluated are their automatic results without allowing any human intervention. In this paper, we review in brief the current state of protein structure prediction and describe what has been learned from the CAFASP1 experiment, the evolution toward CAFASP2, and how we foresee the future of automated structure prediction. We observe that the histories of in silico structure prediction experiments and computer chess tournaments show some striking similarities as well as some differences. We question whether the major advances in automated protein structure prediction stem from novel insights of the protein folding problem, of protein evolution and function, or merely from the technical advances in the ways the evolutionary information available in the biological databases is exploited. We conclude with a speculation about the future, where interesting chess might only be observed in computer games and where the interpretation of the information encoded in the human genome may be achieved mainly through in silico biology.",
A lattice based framework of shared memory consistency models,"A memory consistency model specifies certain aspects of the behavior of a memory system. Stronger consistency models are easier for programmers to use, but provide less flexibility for optimizing the memory implementation. More relaxed consistency models are just the opposite. The goal of our work is to develop a framework that captures the relationships among existing models, and maps the territory of possible models that have not yet been discovered. This work is based on the idea of orthogonal consistency properties. hypothesize that all consistency models can be represented by different combinations of a few primitive properties. The work comes from examining the PRAM, cache, processor, and causal consistency models. Processor is a combination of PRAM and cache, and causal is a combination of PRAM plus an additional requirement. These factors suggest an underlying structure to the models.","Lattices,
Phase change random access memory,
Computer science,
Programming profession,
Databases,
Read-write memory"
A new camera calibration method for robotic applications,"In this paper, we present a camera calibration method using two views of three pairs of concentric circles with known sizes. A new invariant property for circle is introduced to determine the position of the center of the projected circle. Given two image ellipses and their corresponding centers, a cross-ratio based method estimates the center of projected circles using the new invariant property. The accurate center of projected concentric circles provides correct correspondences between ellipses in the image plane and circles in 3D plane. We also demonstrate that two views of three pairs of coplanar concentric circles are enough to determine the intrinsic camera parameters, such as the focal length, the aspect ratio, and the principal point. We validate the performance of the method using both synthetic and real images. Our method shows a comparable performance with respect to similar calibration methods using planes. The use of concentric circles, however, provides correct correspondences between 3D target points and their image points, and greatly simplifies the calibration problem.","Robot vision systems,
Cameras,
Calibration,
Mobile robots,
Application software,
Computer vision,
Laboratories,
Computer science,
Layout,
Robot kinematics"
On compression of parse trees,"We consider methods for compressing parse trees, especially techniques based on statistical modeling. We regard a sequence of productions corresponding to a sum of the path from the root of a tree to a node x as the context of a node x. The contexts are augmented with branching information of the nodes. By applying the text compression algorithm PPMon such contexts we achieve good compression results. We compare experimentally the PPMapproach with other methods.","Production,
Stochastic processes,
Compression algorithms,
Context modeling,
Computer science,
Printers,
Decoding,
Computer languages,
Statistical analysis,
Natural languages"
Modified brain model hyper communication mechanisms,"Proposes a new communication technique, which is named the ""modified brain model hyper-communication"" (MBMHC) mechanism. The hyper-communication technique deals with almost every kind of data about the environment, i.e. information from the five senses and more. Hyper-communication needs two mechanisms. One mechanism analyses and classifies all the environmental data and the other mechanism integrates and processes the data selectively. The human brain analyses many kinds of data about the environment and it integrates data selectively and hierarchically. We modelled the brain's processing mechanism, especially its vision mechanism, in order to establish the hyper-communication. We propose the MBMHC mechanism, which analyses environmental data and integrates it selectively and hierarchically. We also add a feedback loop and a direct pathway from a sender to a receiver as the goal of the information processing. The MBMHC mechanism is expected to be applied to next-generation network systems.","Brain modeling,
Humans,
Collaborative work,
Retina,
Feedback loop,
Information processing,
Transmitters,
Temperature sensors,
Cooperative systems,
Information science"
Execution-driven simulation of IP router architectures,"A number of approaches have been proposed by different vendors for the next generation Internet router architectures, capable of processing millions of packets per second. Most of this processing speed stems from employing latest high-performance network processor or multiprocessors as the forwarding engine of the router However, all these improvements have been proposed without any detailed study in performance evaluation. The impact of instruction level parallelism, branch prediction, multiprocessing, and cache architectures on the performance of routers is not known. In the paper a methodology is proposed, which extends an execution-driven simulator to evaluate router architectures. We incorporate the exact model of an IP router into RSIM to analyze its performance and also develop a framework for feeding real Internet traces to the simulator Our work enables us to vary system parameters to simulate and analyze designs of realistic system with a range of traces. It is shown that the performance of Internet routers can be dramatically enhanced by using multiprocessor architectures. The router design also considers various cache replacement policies and router arbitration policies.","Internet,
Computer architecture,
Computer science,
Analytical models,
Aggregates,
Bandwidth,
Hardware,
Telecommunication traffic,
Traffic control,
Delay"
MSC/sup +/: From requirement to prototyped systems,"Message Sequence Charts (MSCs) have gained wide acceptance for scenario-based specification of component behaviors. MSCs are very useful during requirements capture phase of the software development process and reveal errors in requirement specifications when used in early stages. As MSCs have found widespread usage, there have been several extensions to overcome its' shortcomings for a spectrum of applications keeping the rationale of MSCs invariant. In this paper, we propose (a) An extension of hierarchical MSCs (hMSC for short), called MSC/sup +/, keeping in view the need of complex reactive system specifications; it has new additional features such as watching (preemptive) construct, generalized coregions, and includes features for the specifications of live and forbidden scenarios. (b) A formal translation of MSC/sup +/, to the synchronous language ESTEREL is also provided, This feature enables validating requirement specifications and also to obtain a prototype for synchronous MSC/sup +/ specifications. Apart from obtaining a prototype, the translation of MSC/sup +/ to ESTEREL (that has clean and mathematical semantics) provides a clear semantic definition for the synchronous MSC/sup +/ specifications, In the paper, we describe, the design and implementation of MSC/sup +/ followed by the translation of MSC/sup +/, to ESTEREL leading to prototyping of systems. Examples are used to highlight characteristic features of the language, system and applications.","Prototypes,
Computer science,
Software prototyping,
Programming,
Application software,
Layout,
Communication industry,
Design methodology,
Visualization,
Testing"
On the covering radius of ternary negacyclic codes with length up to 26,The covering radius of all ternary negacyclic codes of even length up to 26 is given. The minimum distances and weight distributions of all codes were recalculated. Seven of the open cases for the least covering radius of ternary linear codes were solved and for the other three cases upper bounds were improved.,Linear codes
Noise in bilinear problems,"Despite the wide application of bilinear problems to problems both in computer vision and in other fields, their behaviour under the effects of noise is still poorly understood. In this paper, we show analytically that marginal distributions on the solution components of a bilinear problem can be bimodal, even with Gaussian measurement error. We demonstrate and compare three different methods of estimating the covariance of a solution. We show that the Hessian at the mode substantially underestimates covariance. Many problems in computer vision can be posed as bilinear problems: i.e. one must find a solution to a set of equations of the form.","Gaussian noise,
Optical scattering,
Computer vision,
Noise measurement,
Image reconstruction,
Computer science,
Application software,
Computer errors,
Equations,
Motion analysis"
A router-based technique for monitoring the next-generation of Internet multicast protocols,"Most of the tools available for monitoring multicast have only marginal utility in today's multicast infrastructure. These tools lack the ability to handle the latest routing protocols because they rely on application layer data like the Realtime Transport Protocol (RTP). Our goal is to develop a system that monitors multicast at the network layer; that can provide functionality for all of the network management tasks; and that can be easily extended to monitor the evolving set of multicast protocols. To this end, we have developed Mantra, a tool for collecting data from multicast routers, analyzing the collected data and presenting realtime results. Using collected data, Mantra creates interactive graphs depicting the state of the multicast infrastructure. Results from Mantra are being used to monitor multicast usage and deployment, as well as routing protocol deployment and performance. In addition, the results are being used to detect common anomalies and to troubleshoot frequent routing problems. In this paper, we discuss the issues related to multicast monitoring, describe the existing efforts in the field and present the design of Mantra. In addition, we present results based on data collected over a 6 months period by monitoring the multicast traffic at two points: (1) at the Federal IntereXchange-West (FIXW) router, and (2) an on-campus router.","Internet,
Multicast protocols,
Computerized monitoring,
Routing protocols,
Transport protocols,
Computer science,
Data analysis,
Telecommunication traffic,
Bandwidth,
Debugging"
An 8.61 Tflop/s Molecular Dynamics Simulation for NaCl with a Special-Purpose Computer: MDM,"We performed molecular dynamics (MD) simulation of 33 million pairs of NaCl ions with the Ewald summation and obtained a calculation speed of 8.61 Tflop/s. In this calculation we used a special-purpose computer, MDM, which we have developed for the calculations of the Coulomb and van der Waals forces. The MDM enabled us to perform large scale MD simulations without truncating the Coulomb force. It is composed of MDGRAPE-2, WINE-2 and a host computer. MDGRAPE-2 accelerates the calculation for real-space part of the Coulomb and van der Waals forces. WINE-2 accelerates the calculation for wavenumber-space part of the Coulomb force. The host computer performs other calculations. With the completed MDM system we performed an MD simulation similar to what was the basis of our SC2000 submission for a Gordon Bell prize. With this large scale MD simulation, we can dramatically decrease the fluctuation of the temperature less than 0.1 Kelvin.","Computational modeling,
Computer simulation,
High performance computing,
Large-scale systems,
Costs,
Acceleration,
Bonding forces,
Permission,
Informatics,
Genomics"
"Investigation of the (/spl mu/, /spl lambda/)-ES in the presence of noise","While in the absence of noise no improvement in local performance can be gained from retaining but the best candidate solution found so far, it has been shown experimentally that, in the presence of noise, operating with a non-trivial population of candidate solutions can have a marked and positive effect on the local performance of evolution strategies (ES). In this paper, we attempt to shed some light on the reasons for the potential performance improvement. In particular, we derive a progress law for the (/spl mu/, /spl lambda/)-ES on a noisy linear fitness function and both numerically and empirically study its implications. We then discuss the significance of the progress coefficients that have been obtained on the linear function for the quadratic sphere, Comparisons of the local performance of the (/spl mu/, /spl lambda/)-ES and of the (1+1)-ES and the (1, /spl lambda/)-ES are presented.",
Defining spatial context for focused image analysis,"Most human commonsense problem solving is done within a context, which constrains the solution space, whether it involves perception and image interpretation or not. Yet most research in image analysis still assumes context is defined a-priori by the investigator and is external to the computational image analysis system. Where explicit focus of attention and spatial contexts are used, as in active vision systems, these are problem-specific. We report on a new approach to image analysis, which includes a general framework for defining spatial context in terms of reference objects and their spatial relationships to other objects in a scene. Image analysis problems are decomposed into a sequence of sub-problems corresponding to determining a sequence of spatial contexts based on the set of dynamically chosen reference objects. The experimental results with medical image analysis and interpretation have demonstrated that using reference objects to define spatial contexts is a very effective strategy for computer image analysis systems that can purposely focus the image analysis effort on the most promising part of the entire image, such that the target object can be quickly and accurately localized by eliminating most potential false positives.","Focusing,
Image analysis,
Image sequence analysis,
Layout,
Humans,
Problem-solving,
Biomedical imaging,
Machine vision,
Computer science,
Artificial intelligence"
A flexible control scheme for current wave forming using multiple capacitor bank units,"Summary form only given. The key issues in high power, high energy applications such as electromagnetic launchers include safety, reliability, flexibility, efficiency, compactness, and cost. To explore some of the issues, a control scheme for a large current wave forming was designed, built and experimentally verified using a 2.4 MJ capacitor bank. The capacitor bank was made up of eight capacitor bank unit, each containing six capacitors connected in parallel. Therefore there were 48 capacitors in total, with ratings of 22 kV and 50 kJ each. Each unit is charged through a charging switch that is operated by air pressure. For discharging each unit has a triggered vacuum switch (TVS) with ratings of 200 kA and 25 kV. Hence, flexibility of wave forming can be obtained by controlling the charging voltage and the discharging timings. The whole control system includes a personal computer, RS232 and RS485 converters, electrical/optical signal converters and eight 80196 micro controller based capacitor-bank unit controllers. Hence, the PC based controller can set the capacitor charging voltages and the TVS triggering timings of each capacitor-bank unit controller for the current wave forming. It also monitors and records the system status data. The RS232 and 485 converters minimize the use optical cables without reducing EMI noise immunity and reliability, this resulting in cost reduction. The paper contains the complete control scheme and details of each subsystem unit. Some experimental current wave forming results are also included.","Capacitors,
Switches,
Voltage control,
Optical control,
Control systems,
Costs,
Timing,
Optical recording,
Optical noise,
Electromagnetic launching"
An implemented planner for manipulating a polygonal object in the plane with three disc-shaped mobile robots,"Presents an implementation of a planner that uses three disc-shaped robots to manipulate a polygonal object in the plane in the presence of obstacles. The approach is based on the computation of the maximal discs (maximal independent capture discs or MICaDs) where the robots can move independently while preventing the object from escaping their grasp. It has been shown that, in the absence of obstacles, it is always possible to bring a polygonal object from any configuration to any other one with robot motions constrained to lie in a set of overlapping MICaDs. This approach is generalized to the case where obstacles are present by decomposing the motion planning task into (1) the construction of a collision-free path for a modified form of the object, and (2) the execution of this path by a sequence of simultaneous and independent robot motions within overlapping MICaDs. The approach is guaranteed to work provided a collision free path exists for the modified form of the object. Experiments with Nomadic Scouts and a visual localization system are presented.","Mobile robots,
Robot motion,
Motion planning,
Path planning,
Fingers,
Computer science,
Grippers,
Controllability,
Trajectory,
Friction"
A note on complexity of OBDD composition and efficiency of partitioned-OBDDs over OBDDs,"We discuss an open problem with constructing an OBDD using composition and prove that the worst case complexity of the construction is truly cubic. Using this insight, we show compactness of partitioned-OBDD over monolithic OBDD.",
Research on cooperative template design,"A design method based on a template uses decomposition, combination, replacement and merging of the template for agility and variation of a model in global and local design. The authors have researched the fields of mechanical CAD and engineering CAD systems. Based on these works a concept of a cooperative template is presented in order to control and manage design tasks on the Net. It can inherit components that have been designed and give a nimble response for new projects if template technology is introduced into the public area of cooperative design. The paper discusses the work process of the template, its elements based on a cooperative mechanism and combines lock and semaphore functions and expands TDL (Template Design Language) using a cooperating primitive. Users are allowed to compete for design elements in the framework of visualization. Some technical problems concerning XML and JSP+Java Servlet are also discussed.","Design automation,
Design methodology,
Design engineering,
Concurrent engineering,
Shape,
Hardware,
Circuits,
Plasma applications,
Nuclear and plasma sciences,
Physics"
Probabilistic fair queuing,"Packet scheduling constitutes the core problem in efficient fair allocation of bandwidth to competing flows. To date, numerous algorithms for packet scheduling have been suggested and tested. However, only a few of them are currently deployed. One of the key reasons for rarity of applied packet scheduling methods lies in the complexity of their implementation. This paper describes a family of randomized algorithms for packet scheduling. These algorithms are simple to implement and require small amounts of computation time. Specifically we present an O(1) probabilistic weighted fair queuing algorithm that emits packets from flows with an improved delay jitter. Experimental results of the proposed randomized algorithms suggest that the randomized approach is a viable alternative to the currently deployed deterministic fair queuing algorithms.","Scheduling algorithm,
Bandwidth,
Delay,
Global Positioning System,
Jitter,
Computer science,
Testing,
Resource management,
Processor scheduling,
Channel allocation"
Providing differentiated services to Mobile IP users,"To support quality-of-service (QoS) provisioning in highly dynamic mobile environments, we propose a signaling protocol allowing mobile users to contact a Differentiated Services bandwidth broker for QoS negotiation. The protocol can also be used for QoS negotiations between bandwidth brokers.","Bandwidth,
Diffserv networks,
Routing,
Laser sintering,
Mobile computing,
Access protocols,
Bidirectional control,
Computer science,
Mathematics,
Quality of service"
Efficient categorization of memory sharing patterns in software DSM systems,"This work introduces a new technique that enables SDSMs to categorize dynamically and accurately memory sharing patterns in both classes of regular and irregular applications. The categorization is carried out automatically at run-time on a per-page basis, requiring no user or compiler assistance. We evaluate the potential benefits of our technique using execution-driven simulations of 8 applications running on TrendMarks on a network of 8 workstations. Surprisingly, we found that producer-consumer(s) and migratory are the dominant patterns even in irregular applications. Preliminary results suggest that the categorization technique we propose is a promising option to further improve the performance of current adaptive SDSM systems.","Software systems,
Workstations,
Adaptive systems,
Prefetching,
Access protocols,
Computer science,
Systems engineering and theory,
Runtime,
Modeling,
Application software"
Case study: interacting with cortical flat maps of the human brain,"The complex geometry of the human brain contains many folds and fissures, making it impossible to view the entire surface at once. Since most of the cortical activity occurs on these folds, it is desirable to be able to view the entire surface of the brain in a single view. This can be achieved using quasi-conformal flat maps of the cortical surface. Computational and visualization tools are now needed to be able to interact with these flat maps of the brain to gain information about spatial and functional relationships that might not otherwise be apparent. Such information can contribute to earlier diagnostic tools for diseases and improved treatment. Our group is developing visualization and analysis tools that will help elucidate new information about the human brain through the interaction between a cortical surface and its corresponding quasiconformal flat map.",
A novel fast motion estimation algorithm using fixed subsampling pattern and multiple local winners search,"In this paper, a novel fast algorithm for block motion estimation is proposed. The reduction of computation complexity is obtained from the use of pixel sub-sampling on block matching instead of limiting the number of searching locations. Further, in order to get the better quality, this algorithm uses a technique of multiple local winners. From the experimental result, the proposed algorithm produces better performance than some other fast block-matching algorithms like three-step search (TSS), four-step search (FSS), new three-step search (NTSS), new diamond search (DS), hybrid search (HBS) and four-step genetic algorithm (4GA). That is, the performance of proposed algorithm not only reduces the computational time but also improves the accuracy of motion vectors.",
Performance evaluation of parallel file systems for PC clusters and ASCI red,,"File systems,
Concurrent computing,
Linux,
Throughput,
High performance computing,
Application software,
Bandwidth,
Hardware,
Extraterrestrial measurements,
Velocity measurement"
On the use of mutations in Boolean minimization,"The paper presents a new method of Boolean function minimization based on an original approach to implicant generation by inclusion of literals. The selection of these newly included literals, as well as the subsequent rejection of some others to obtain prime implicants, is based on heuristics working with the frequency of literal occurrence. Instead of using this data directly, some mutations are used in certain places in the algorithm. The technique of mutations and their influence on the quality of the result obtained is evaluated. The BOOM system implementing the proposed method is efficient especially for functions with several hundreds of input variables, whose values are defined only for a small part of their range. It has been tested both on standard benchmarks and on problems of a much larger dimension, generated randomly. These experiments proved that the new algorithm is very fast and that for large circuits it delivers better results than the state-of-the-art ESPRESSO.",
Towards a temporal World-Wide Web: a transaction-time server,"Transaction time is the time of a database transaction, i.e., an insertion, update, or deletion. A transaction time database stores the transaction-time history of a database and supports transaction timeslice queries that retrieve past database states. The paper introduces transaction time to the World-Wide Web. In a Web context, transaction time is the modification time of a resource such as an XML document. A transaction-time Web server archives resource versions and supports transaction timeslice. Unlike a database server, a Web server is typically uninvolved in the update of a resource, instead it is only active when a resource is requested. The paper describes a lazy update protocol that enables a Web server to manage resource versions during resource reads. An important benefit of our approach is that transaction-time can be supported by a transparent, minimal Web server extension; no changes to legacy resources, HTTP, XML, or HTML are required. Furthermore, a Web server can seemlessly become a transaction-time server at any time without affecting or modifying the resources it services or other Web servers.","Transaction databases,
Web server,
Information retrieval,
XML,
Search engines,
Computer science,
Uniform resource locators,
History,
Protocols,
Resource management"
Quantum Computation,,"Quantum computing,
Physics computing,
Public key cryptography,
Laboratories,
Quantum mechanics,
Electrons,
Fellows,
Information science,
Voltage,
Logic circuits"
The design of a web-based computer proficiency examination,"In 1996, the Kansas Board of Regents passed a requirement that, beginning in 2001, a student applying for admission to a Kansas university must have earned one high school unit of computer technology. One option a student has for satisfying this requirement is to pass a computer proficiency examination. This paper reports on the development of a web-based proficiency test designed and developed at the University of Kansas. It discusses initial design decisions that were made as well as unanticipated problems which arose that had to be resolved in order to bring this project to fruition. Two types of security issues had to be addressed-maintaining the integrity of the question source and ensuring that the testing environment at the remote site was strictly monitored. Having to deal with different versions of browsers as well as different computing platforms presented additional challenges.","Testing,
Educational institutions,
Text processing,
Computer networks,
Computerized monitoring,
Remote monitoring,
Computer science education,
Performance evaluation,
Processor scheduling,
Operating systems"
Consumer online-privacy and anonymity protection using infomediary schemes,"The rapid evolution of the Internet may largely depend on gaining and maintaining the trust of users. This possibility may especially rule enterprises whose financial viability depends on electronic commerce. Customers will neither have the time, the ability or the endurance to work out the best deals with vendors, nor will vendors have time to bargain with every customer. In order for customers to strike the best bargain with vendors, they need a privacy supporter, an information intermediary or ""infomediary"". Infomediaries will become the custodians, agents and brokers of customer personal information exchanged via the Internet, while at the same time protecting their privacy. There is a scale between security and privacy that currently leans towards security; security adopts strong user authentication mechanisms. In order to control access to personal data, while privacy requires loose authentication in order to provide user anonymity. In this paper, we introduce a new infomediary-based privacy-enhancing business model, which is capable of providing anonymity, privacy and security to customers and vendors of e-commerce. Using this model, customers can buy goods or services without revealing their real identity or preferences to vendors, and vendors can sell or advertise goods or services without violating the privacy of their customers.",Protection
Optimal prefix-free codes that end in a specified pattern and similar problems: the uniform probability case,"In this paper we discuss the problem of constructing minimum-cost, prefix-free codes for equiprobable words under the assumption that all codewords are restricted to belonging to an arbitrary language L. We examine how, given certain types of L, the structure of the minimum-cost code changes as n, the number of codewords, grows.",
Inference of regular languages using model simplicity,"We describe an approach that is related to a number of existing algorithms for the inference of a regular language from a set of positive (and optionally also negative) examples. Variations on this approach provide a family of algorithms that attempt to minimise the complexity of a description of the example data in terms of a finite state automaton model. Experiments using a standard set of small problems show that this approach produces satisfactory results when positive examples only are given, and can be helpful when only a limited number of negative examples is available. The results also suggest that improved algorithms will be needed in order to tackle more challenging problems, such as data mining and exploratory sequential analysis applications.","Inference algorithms,
Automata,
Data mining,
Sequential analysis,
Artificial intelligence,
Pattern recognition,
Convergence,
Heuristic algorithms"
An interactive multimedia-board,"The multimedia-board is a CSCW application in long-distance education and a key technology to realize long-distance classrooms and real time multimedia answering systems. In order to meet the requirements of large-scale users' cooperation with real time multimedia in an IP network, the paper presents the concept of multimedia-board and the definition of a multimedia-board system. It goes on to describe a conversation model with a Petri net and provides a concurrent control mechanism and algorithm which may be applied to the multimedia-board and other CSCW systems. Finally, the authors present the design of a multimedia-board with B/S structure and its perspective in practice.","IP networks,
Education,
Multimedia systems,
Educational technology,
Satellite broadcasting,
Internet,
Displays,
Computer science,
Large-scale systems,
Real time systems"
Confined mobile functions,,"Information security,
Distributed computing,
Programming profession,
Computer languages,
Data security,
Laboratories,
Computer science,
Informatics,
Distributed control,
Runtime"
Cost-based replacement policy for multimedia proxy across wireless Internet,"The multimedia proxy plays an important role in media streaming applications. In this paper, we propose an architecture for a multimedia proxy across the wireless Internet. By considering multiple objectives of the multimedia proxy, we design a unified cost metric to measure proxy performance in the wireless Internet. Furthermore, we propose a cost-based replacement policy for the wireless Internet to improve performance such as throughput, media quality, and startup delay. Simulation results demonstrate that our policy achieves significantly better performance than the conventional replacement policy.","Internet,
Streaming media,
Costs,
Delay,
Error correction,
Distortion measurement,
Error correction codes,
Computer science,
Application software,
Computer architecture"
Using edit distance in point-pattern matching,,"Proteins,
Time measurement,
NP-complete problem,
Computer science,
Speech processing,
Extraterrestrial measurements,
Dynamic programming,
Computational geometry,
Robustness,
Voting"
Cache-on-demand: recycling with certainty,"Queries posed to a database usually access some common relations, or share some common sub-expressions. We examine the issue of caching using a novel framework, called cache-on-demand (CoD). CoD views intermediate/final answers of existing running queries as virtual caches that an incoming query can exploit. Those caches that are beneficial may then be materialized for the incoming query. Such an approach is essentially nonspeculative: the exact cost of investment and the return on investment are known, and the cache is certain to be reused. We address several issues for CoD to be realized. We also propose two optimizing strategies, Conform-CoD and Scramble-CoD, and evaluate their performance. Our results show that CoD-based schemes can provide substantial performance improvement.",
Local defect study of membrane antennas and reflectors,"One of the great enabling technologies for 21st Century space science missions will be gossamer spacecraft. Since resolution is proportional to diameter at the diffraction limit, larger antennas and optic apertures mean greater opportunities for increasing scientific knowledge. Due to finite launch vehicle capacity (launch mass and volume), these large (>12 m) apertures must also be ultra-low mass. This implies some sort of membrane/ inflatable structure. The current paper discusses the effects of local defects on reflector performance. Unlike classical glass optics for example, membrane apertures cannot be ground and polished to precision tolerances. The manufacturing process must account for minimum thresholds of surface smoothness and mechanical property irregularities. For example, sufficient numbers of small regions of thickness or Young's modulus irregularities can lead to unacceptable surface error. This paper reports on analysis of such local defects on the surface precision of gossamer apertures. A nonlinear finite element code is used to model the effect of single and multiple defects in curved membranes. Two measures of performance are used. First, we compute the deviation of the local slope for a given defect geometry and property irregularity. Secondly, we compute the spatial influence function of the defect both on the neighboring uniform membrane, as well as on nearby like defects. Indications of manufacturing tolerances required to achieve minimum acceptable performance are discussed.","Biomembranes,
Reflector antennas,
Space technology,
Nonlinear optics,
Space missions,
Space vehicles,
Optical diffraction,
Aperture antennas,
Glass,
Manufacturing processes"
A concurrency test tool for Java monitors,"The Java programming language supports monitors. Monitor implementations, like other concurrent programs, are hard to test due to the inherent non-determinism. This paper presents the ConAn (Concurrency Analyser) tool for generating drivers for the testing of Java monitors. To obtain adequate controllability over the interactions between Java threads, the generated driver contains processes that are synchronized by a clock. The driver automatically executes the calls in the test sequence in the prescribed order and compares the outputs against the expected outputs specified in the test sequence. The method and tool are illustrated on an asymmetric producer-consumer monitor and their application to two other monitors is discussed.","Java,
Yarn,
Clocks,
Automatic testing,
Synchronization,
Concurrent computing,
Computer displays,
Computer science,
Computer languages,
Controllability"
Integrating the Freshman seminar and Freshman Problem Solving courses,"The transition from high school to college can be very difficult for many students. At the University of Pittsburgh, we have a system of courses and academic counseling that is designed to address these issues and help the student in this transition. One major component is a series of mentoring courses that the entering student can select for the first semester. These courses are designed to help the freshman make this major transition. The student must also enroll in a Freshman Problem Solving course that details the use of various computer tools. This paper discusses how these courses are integrated and describes the interaction of counseling with the first semester engineering problem solving course. The paper also discusses the mentor selection process, the mentor-training program, and the topics covered in the mentoring sessions.","Seminars,
Problem-solving,
Educational institutions,
Employee welfare,
Mathematics,
Design engineering,
Programming profession,
Teamwork,
Web page design,
Internet"
Performance evaluation of HiperLAN type 2 with voice and Web data traffic,"HiperLAN (HIgh-PErformance Radio Local Area Network) type 2 is a new standard from ETSI (European Telecommunications Standards Institute) for high-speed wireless LANs, interconnecting portable devices to broadband core networks based on different networking technologies, such as IP, ATM, UMTS, etc. This paper carries out a performance evaluation of the MAC protocol operating within a HiperLAN/2 system. Specifically, the paper focuses on the mechanisms provided by HiperLAN/2 to manage bandwidth resource request/granting and scheduling, in order to assess their flexibility and efficiency in supporting delay-sensitive (voice) and computer data traffic of a bursty nature (Web traffic).","Telecommunication standards,
Telecommunication traffic,
Local area networks,
Wireless LAN,
LAN interconnection,
3G mobile communication,
Media Access Protocol,
Resource management,
Bandwidth,
Processor scheduling"
"A hierarchical cluster algorithm for dynamic, centralized timestamps","Partial-order data structures used in distributed-system observation tools typically use vector timestamps to efficiently determine event precedence. Unfortunately all current dynamic vector-timestamp algorithms either require a vector of size equal to the number of processes in the computation or require a graph search operation to determine event precedence. This fundamentally limits the scalability of such observation systems. In this paper we present an algorithm for hierarchical, clustered vector time-stamps. We present results for a variety of computation environments that demonstrate such timestamps can reduce space consumption by more than an order-of-magnitude over Fidge/Mattern timestamps while still providing acceptable time bounds for computing timestamps and determining event precedence.","Clustering algorithms,
Heuristic algorithms,
Distributed computing,
Data structures,
Control systems,
Monitoring,
Scalability,
Data visualization,
Computer science,
Costs"
Use of colour in form layout analysis,"Colour has long been viewed as one of the unnecessary features in any form processing system, due not only to the large storage requirement and computational cost its inclusion imposes but also to the complexities of hue, chroma and brightness variation. However, as technology has advanced and computing costs have reduced, the processing of documents in colour has now become practical. This paper describes a prototype form extraction system that utilises colour information to help facilitate data extraction from a form. Blank forms are first automatically analysed to obtain their layout, colour and statistical information. The filled data is then extracted from the filled forms using techniques based upon the colour characteristic changes that have occurred with respect to the blank form. The improved performance of the proposed method has been verified by comparing the processing time, data extraction precision and recall rate of the proposed system to that of an archetypal black and white form extraction system.","Data mining,
Costs,
Image color analysis,
Iris,
Computational efficiency,
Brightness,
Information analysis,
Finance,
Computer science education,
Medical services"
Logical operations and Kolmogorov complexity. II,"For Part I, see Theoretical Computer Science (to be published). Investigates the Kolmogorov complexity of the problem (a/spl rarr/c)/spl and/(b/spl rarr/d), defined as the minimum length of a program that, given a, outputs c and, given b, outputs d. We prove that, unlike all known problems of this kind, its complexity is not expressible in terms of the Kolmogorov complexity of a, b, c and d, their pairs, triples, etc. This solves the problem posed in Part I. We then consider the following theorem: there are two strings, whose mutual information is large but which have no common information in a strong sense. This theorem was proven by A. Muchnik et al. (1999) via a non-constructive argument. We present a constructive proof, thus solving a problem posed by Muchnik et al. We give also an interpretation of both results in terms of Shannon entropy.","Upper bound,
Logic,
Mutual information,
Entropy"
The human-computer interface is the system; a plea for a poor man's HCI component in software engineering curricula,"Most software engineering approaches restrict the user interface to everything a user may perceive or experience. As a result, it is often designed rather independently of the system functionality. Chances are then that it does not get the attention it deserves. In the approach to software development we sketch, the design of the user interface and the design of the functionality go hand in hand. We give a number of examples of user interface problems, and illustrate how these can be caught early if a more integrated approach is taken. We conclude with an outline of a minimal course on human-computer interaction that we feel should be part of everyone's software engineering curriculum.","Human computer interaction,
Engines,
User interfaces,
Software engineering,
Application software,
Mathematics,
Computer science,
Programming,
Interactive systems,
Usability"
Expanding generalized Hadamard matrices over Gm by substituting several generalized Hadamard matrices over G,"Over an additive abelian group G of order g and for a given positive integer , a generalized Hadamard matrix GH(g, ) is defined as a g  g matrix [h(i, j)], where 1  i  g and 1  j  g, such that every element of G appears exactly  times in the list h{i1, 1)  h(i2,1), h(i1, 2)  h(i2, 2), , h(i1, g)  h(i2, g), for any i1  i2. In this paper, we propose a new method of expanding a GH(gm, 1) = B = [-Bij] over Gm by replacing each of its m-tuple Bij with Bij  GH(g, 2) where m = g2. We may use gm 1 (not necessarily all distinct) GH(g, 2)'s for the substitution and the resulting matrix is defined over the group of order g.","Error correction codes,
Error correction,
Matrices,
Educational institutions,
Additives,
Presses"
New approach for mobile multicast based on SSM,"The issue for mobile multicast is classified as two trends. One is route optimization and the other is how fast the multicast routing tree can be reconstructed. In the case of reconstructing the multicast tree, the FA must join the multicast group whenever the attachment point is changed. This mechanism has the advantage of route optimization for transmitting multicast data. However, the tree reconstruction may have occurred whenever the mobile host moves. Its overhead may be increased rapidly. In order to service multicast, the other way is the use of tunneling without tree reconstruction. Because the HA (home agent) is already being subscribed in the multicast session, the multicast datagram is transmitted to the HA. The HA tunnels it to the current FA (foreign agent), to which the mobile host is attached. When this mechanism is applied, the tree reconstruction is not needed. However, the path from the HA to FA may not be optimized. The longer the path distance is, the more the packet delay may increase. If the FA does not support IGMP, a more serious problem is caused. As stated above, the two mechanisms may not be implemented easily on the current network. We propose a mobile multicast mechanism based on SSM (source-specific multicast), which is proposed in order to support one-to-many transmission efficiently. However, automatic tunneling is used instead of the source-based tree. The key feature of this mechanism is to remove the overload caused by tree reconstruction and to optimize the multicast path simultaneously when the handoff occurred.","Tunneling,
Routing,
Computer science,
Internet,
Broadcasting,
Mobile communication,
Delay,
Resource management,
Scalability"
Distributed projects tackle protein mystery,"A computing paradigm popularized by the search for life in outer space is now being directed at understanding life from the inside. Two recently launched programs, one independent, the other under the auspices of Stanford University, have developed SETI@home-like distributed computing programs that rely on lay participants' computing power to help them unlock the biological mystery of how proteins fold. More specifically, the Folderol and Folding@home projects are attempting to shed light on the protein-folding mystery using the screensaver-based distributed computing model popularized by the SETI@home project. Although each project takes a different approach to the folding problem, both rely on ever-increasing PC performance and the willingness of lay users to participate in solving this biological mystery.","Proteins,
Biology computing,
Amino acids,
Biological system modeling,
Engineering profession,
Collaboration,
Workstations,
Testing,
Web server,
Entropy"
Parallel complete remeshing for adaptive schemes,"In order to improve the convergence ratio and to automate Finite Element Methods, several strategies have been introduced. One of these is the adaptive scheme. This approximation presents limitations for parallelism since the generation of a conformal, valid and well conditioned finite element mesh is a time consuming task, and now it appears as a main task in each iteration of the adaptation procedure. This work is motivated by the use of the h-adaptive method in its most flexible form, where a complete reconstruction of the whole mesh have to be performed whenever a solution over the current mesh has been obtained and until error criteria are achieved. We focused on the problem of the fast generation of tetrahedral unstructured meshes in a parallel fashion over geometric models with some given refinement criteria. The chosen strategy implies the use of an octal tree, octree, as a key hierarchical data structure to guide the algorithm. The codes have been developed using the MPI library in a SGI Origin 200 multiprocessor.","Numerical simulation,
Mesh generation,
Solid modeling,
Data structures,
Computer science,
Convergence,
Finite element methods,
Parallel processing,
Tree data structures,
Libraries"
On the Phase-Space Dynamics of Systems of Spiking Neurons. I: Model and Experiments,"We investigate the phase-space dynamics of a general model of local systems of biological neurons in order to deduce the salient dynamical characteristics of such systems. In this article, we present a detailed exposition of an abstract dynamical system that models systems of biological neurons. The abstract system is based on a limited set of realistic assumptions and thus accommodates a wide range of neuronal models. Simulation results are presented for several instantiations of the abstract system, each modeling a typical neocortical column to a different degree of accuracy. The results demonstrate that the dynamics of the systems are generally consistent with that observed in neurophysiological experiments. They reveal that the qualitative behavior of the class of systems can be classified into three distinct categories: quiescence, intense periodic activity resembling a state of seizure, and sustained chaos over the range of intrinsic activity typically associated with normal operational conditions in the neocortex. We discuss basic ramifications of this result with regard to the computational nature of neocortical neuronal systems.",
A split operator for now-relative bitemporal databases,"The timestamps of now-relative bitemporal databases are modeled as growing, shrinking or rectangular regions. The shape of these regions makes it a challenge to design bitemporal operators that (a) are consistent with the point-based interpretation of a temporal database, (b) preserve the identity of the argument timestamps, (c) ensure locality and (d) perform efficiently. We identify the bitemporal split operator as the basic primitive to implement a wide range of advanced now-relative bitemporal operations. The bitemporal split operator splits each tuple of a bitemporal argument relation, such that equality and standard nontemporal algorithms can be used to implement the bitemporal counterparts with the aforementioned properties. Both a native database algorithm and an SQL implementation are provided. Our performance results show that the bitemporal split operator outperforms related approaches by orders of magnitude and scales well.","Transaction databases,
History,
Computer science,
Shape,
Employee rights,
EMP radiation effects"
Exploiting structure for intelligent Web search,"Together with the rapidly growing amount of online data, we register an immense need for intelligent search engines that access a restricted amount of data as found in intranets or other limited domains. These sorts of search engine must go beyond simple keyword indexing/matching, but they also have to be easily adaptable to new domains without huge costs. The paper presents a mechanism that addresses both of these points: first of all, the internal document structure is being used to extract concepts which impose a directory-like structure on the documents, similar to those found in classified directories. Furthermore, this is done in an efficient way which is largely language independent and does not make assumptions about the document structure.","Intelligent structures,
Web search,
Indexing,
Search engines,
Data mining,
Costs,
Web pages,
Computer science,
Registers,
Pattern matching"
Shape control of cubic B-spline and NURBS curves by knot modifications,"Presents shape control methods for cubic B-spline and NURBS curves by the modification of their knot values and by the simultaneous modification of weights and knots. Theoretical aspects of knot modification are also discussed, concerning the paths of points on a curve and the existence of an envelope for the family of curves resulting from a knot modification for curves of degree k.","Spline,
Shape control,
Surface topography,
Surface reconstruction,
Design automation,
Books,
Mathematics,
Computer science,
Computational geometry,
Educational institutions"
Extensible signaling for temporal resource sharing,"The Internet is rapidly evolving from a network that provides basic best-effort communication service to an infrastructure capable of supporting complex value-added services. These services typically have multiple fluffs with interdependent resource requirements. These dependencies provide opportunities to share the same set of resources among related flows over time leading to significant resource gains. We call this type of sharing temporal resource sharing. Exploiting temporal sharing requires support in the signaling protocol that performs resource allocation for the related flows. We examine the problem of supporting temporal sharing in a signaling protocol. This paper makes the case that temporal sharing support must be designed to be extensible, so that service providers can define and implement new sharing behaviors without having to modify the signaling protocol. We motivate the need for an extensible design by showing that the range of possible temporal sharing behaviors is large and supporting the most general forms of temporal sharing is computationally expensive. We then present a design for extensible signaling support for temporal sharing. We have implemented the temporal sharing design presented in this paper in the Beagle signaling protocol. We present an evaluation of the Beagle design and contrast it with other signaling protocols like RSVP and Tenet-2.","Resource management,
Protocols,
Signal design,
Web and internet services,
IP networks,
Helium,
Packet switching,
Switches,
Computerized monitoring,
Computer science"
End-user ethics teaching: issues and a solution based on universalization,"The ethical aspects of computing have gained increasing attention at the professional level of education in universities. As a result, several works have been produced relating to computer ethics education at this level. However, the ever-increasing role and usage of computer technology means that ethical education related to computing is also necessary for non-professional/non-major computing/information systems students. Due to the differences between professional and non-professional education in terms of substance, along with pragmatic reasons (e.g. lack of resources), the ordinary end-users need a different educational program. This paper first explores issues (i.e. challenges and problems to overcome) of end-user ethics teaching, second puts forward a solution based on a process of universalization, third argues that the universality thesis is adequate for the purpose of end-user education, and finally demonstrates, with the help of three imaginary cases, how the chosen solution can be used to tackle the issues and educate ordinary end-users in universities.",
Realization of a genetic-algorithm-optimized wire antenna with 5:1 bandwidth,"Techniques for developing and realizing a broadband, loaded wire antenna with a transmission line transformer matching network are presented. The component values and position of a parallel inductor-resistor load circuit are optimized via a genetic algorithm to improve the bandwidth of a straight-wire monopole antenna. Candidate antennas are analyzed by solving the electric field integral equation with a lumped-load model included. This model has been enhanced with fast algorithms from the literature in order to improve computational efficiency during optimization. Subsequent to optimization, the curved-wire integral equation solution is used to more closely model a real-world monopole loaded with a helical coil and parallel resistor. The measured input voltage standing wave ratio of the loaded antenna with matching network is below 3.5 over a 5:1 bandwidth (2001000 MHz). The computed system gain is above 4.0 dBi over the band 2201000 MHz.",
"Analyzing the relation between heart rate, problem behavior, and environmental events using data mining system LERS","The relation between physiological events, environmental factors and the occurrence of problem behavior in natural settings was analyzed using the data mining system LERS (Learning from Examples based on Rough Sets). Heart-rate data were linked to environmental and behavioral data coded from videotapes of one adult subject diagnosed with severe mental retardation and who engaged in problem behavior. The results of the analysis suggest that using data mining system LERS will be a valuable strategy for exploring large data sets that include heart rate, environmental and behavioral measures.","Heart rate,
Data mining,
Environmental factors,
Pediatrics,
Computer science,
Data analysis,
Rough sets,
Problem-solving,
Transmitters,
Belts"
Ball lightning explained as a stable plasma toroid,"Summary form only given, as follows. Describes spinning plasma toroids that are created using high power electric arcs similar to lightning bolts. The spinning toroids are observed to be stable in atmosphere with no confining magnetic fields. Spinning toroids have the appearance of spheres, or balls, and create bright light through collisions with neutrals in the atmosphere. The spinning toroids are observed to last for more than 200 milliseconds in partial atmosphere. The paper describes the initiation apparatus and parameters. An explanation for the plasma toroid is presented that it is a hollow toroid of electrons where all the electrons travel in parallel paths orthogonal to the toroid circumference and reside in a thin outer shell of the toroid. The electron motion creates a current in the surface that in turn creates an internal magnetic field. Equations are presented detailing the initiation of the plasma toroid, and detailing the plasma toroid itself. The stability condition for the plasma toroid has been identified that explains how the plasma toroid remains stable in atmosphere. The observations of the plasma toroids match computer simulations of spinning toroids with constant luminosity. The spinning plasma toroid has the appearance of ball lightning in observations and in computer simulations. The plasma toroid explains how a plasma ring can be stable in atmosphere with no external magnetic fields, and how it can contain many electrons with high energy. Ball lightning is often reported as a ring current, in toroid shape, and since a spinning ring appears as a sphere or ball, the spinning plasma toroid provides an explanation for ball lightning. The technology of the stable plasma toroid has potential for new applications in propulsion, energy generation, and energy storage.","Lightning,
Plasma stability,
Spinning,
Plasma simulation,
Atmosphere,
Electrons,
Plasma confinement,
Toroidal magnetic fields,
Computer simulation,
Plasma applications"
Active distributed monitoring for dynamic large-scale networks,"Networks offering services of high availability and quality need to be carefully monitored. Their increasing size and complexity stresses the ability of currently used static centralized systems. Decentralized approaches are possible and a key issue is the placement of area monitoring stations for optimal operation. Previous research has resulted in computationally expensive algorithms that require a global centralized network view. In this paper we propose a much simpler distributed algorithm and show that it performs as well as existing near-optimal but expensive, centralized algorithms. In addition, we propose that area monitoring stations are mobile agents, cloning and optimally placing themselves by executing the proposed algorithm. As network conditions change, e.g. through faults or persisting congestion, agents can adapt and migrate to new locations. We quantify the benefits of our approach against both the centralized and centrally-computed static distributed approaches.","Monitoring,
Large-scale systems,
Communication system traffic control,
Telecommunication traffic,
Software agents,
Delay,
Context,
Computer science,
Educational institutions,
Optical wavelength conversion"
A closed-form solution for optical flow by imposing temporal constraints,"We present a well-constrained formulation for estimation of optical flow in a video sequence, which yields a closed-form solution. Unlike other existing closed-form approaches, our solution does not require second order spatial derivatives. It relies only on first order and cross-space-time derivatives which can be computed more reliably. Instead of the commonly used spatial smoothness constraint, we propose a temporal smoothness constraint which has a clear physical interpretation. Our formulation also allows for accurate analysis of sources of error and ill-conditioning, leading to a more tractable implementation and mathematically justifiable thresholding values.",
Lightweight analysis of operational specifications using inference graphs,"The Amalia framework generates lightweight components that automate the analysis of operational specifications and designs. A key concept is the step analyzer, which enables Amalia to automatically tailor high-level analyses, such as behavior simulation and model checking, to different specification languages and representations. A step analyzer uses a new abstraction, called an inference graph, for the analysis. It creates and evaluates an inference graph on-the-fly during a top-down traversal of a specification to deduce the specification's local behaviors (called steps). The nodes of an inference graph directly reify the rules in an operational semantics, enabling Amalia to automatically generate a step analyzer from an operational description of a notation's semantics. Inference graphs are a clean abstraction that can be formally defined. The paper provides a detailed but informal introduction to inference graphs. It uses example specifications written in LOTOS for purposes of illustration.","Algorithm design and analysis,
Pattern analysis,
Computer science,
Design engineering,
Analytical models,
Automatic testing,
Automatic test pattern generation,
Test pattern generators,
Packaging,
Engines"
Fast specification of cycle-accurate processor models,"This paper introduces a new specification style for processor microarchitectures. Our goal is to produce very simple, compact, but cycle-accurate descriptions, in order to enable early exploration of different microarchitectures and their performance. The key idea behind our approach is that we can derive the difficult-to-design forwarding and stall logic completely automatically. We have implemented a specification language for pipelined processors, along with an automatic translator that creates cycle-accurate software simulators from the specifications. We have specified a pipelined MIPS integer core in our language. The entire specification is less than 300 lines long and implements all user mode instructions except for coprocessor support. The resulting, automatically-generated, cycle-accurate simulator achieves over 240,000 instructions per second simulating MIPS machine code. This performance is within an order of magnitude of large, hand-crafted, cycle-accurate simulators, but our specification is far easier to create, read, and modify.","Microarchitecture,
Logic,
Writing,
Computer science,
Tires,
Coprocessors,
Machine intelligence,
Pipelines,
Delay,
Automatic control"
Implementation and performance evaluation of TeleMIP,"We present our implementation of TeleMIP, a two-level architecture for IP-based mobility management. TeleMIP essentially uses an intra-domain mobility management protocol (IDMP) for managing mobility within a domain, and mobile IP for supporting inter-domain (global) mobility. Unlike other proposed schemes for intra-domain mobility management, IDMP uses two care-of addresses for mobility management. The global care-of address is relatively stable and identifies the mobile node's current domain, while the local care-of address changes every time the mobile changes subnets and identifies the mobile's current point of attachment. The paper describes our TeleMIP implementation based on enhancements to the Stanford University mobile IP Linux code and presents performance results obtained through experiments on our test-bed. Finally, we use analysis to accurately quantify the savings in signaling overhead obtained when TeleMIP is used in environments where mobiles change subnets relatively rapidly.","Mobile radio mobility management,
Land mobile radio cellular systems,
Protocols,
Testing,
Linux,
US Government,
Educational institutions,
Computer science,
Computer architecture,
Signal analysis"
Evaluation of femoral head necrosis using a volumetric method based on MRI,"Most studies agree that the fate of femoral head osteonecrosis is associated with the size and location of the necrotic lesion. We present a volumetric method for assessment of osteonecrosis, based on magnetic resonance imaging. The method evaluates the percentage of the necrotic volume in the femoral head and in each of the head segments (octants). Along with the method, we present a classification system based on the geometrical features of the lesion. The system was used to classify 106 hips with osteonecrosis before treatment with vascularized fibular grafting. The hips were evaluated using the volumetric method. The follow up study indicates that the proposed method and the classification system can be reliable tools for the assessment of femoral head necrosis.","Magnetic resonance imaging,
Magnetic heads,
Lesions,
Hip,
Biomedical imaging,
Medical diagnostic imaging,
Image segmentation,
Mice,
Physics,
Computer science"
Teaching distributed and parallel computing with Java and CSP,We discuss the advantages of using Java with a CSP library (JCSP) for teaching concurrent and parallel computing. We describe an extension to JCSP that allows channels to be extended across a network using TCP/IP sockets. Three examples of concurrent programs using JCSP are presented to show how different concepts of concurrent and parallel computing can be presented with these tools.,
Slicing floorplan with clustering constraints,"In floorplan design it is useful to allow users to specify placement constraints in the final packing. Clustering constraint is one kind of placement constraint in which a given set of modules are restricted to be geometrically adjacent to one another. The wiring cost can be reduced by putting modules with a lot of connections close together. Designers may also need this type of placement constraint to pack the modules according to their functionality. In this paper, a method addressing clustering constraint in slicing floorplan is presented. A linear time algorithm is devised to locate neighboring modules in a normalized Polish expression and re-arrange the modules in order to satisfy the constraints. Experiments were performed on some benchmarks and the results are promising.","Circuit optimization,
Costs,
Shape,
Design optimization,
Integrated circuit interconnections,
Delay,
Computer science,
Design engineering,
Wiring,
Energy consumption"
Efficient use of the BlobTree for rendering purposes,"One of the major applications of implicit surface modeling systems has been the generation of cartoon-like characters. Recently, additional modeling methods have been combined with implicit surfaces to create much more complex models. These methods include constructive solid geometry (CSG), warping, and two-dimensional texture mapping (among others). The BlobTree has been introduced to organize all of these elements into a single structure which allows both local and global applications of each of these techniques in a general and intuitive fashion. The BlobTree lends itself well to rapid and direct specification of complex models, however current implementations of the BlobTree have not been engineered for efficiency, and perform poorly when attempting to render large models. In this work we apply established techniques, such as spatial subdivision and tree optimization, to the BlobTree. The objective is to increase efficiency during rendering without restricting the functionality of the BlobTree as a modeling tool.","Ray tracing,
Solids,
Facial animation,
Rendering (computer graphics),
Visualization,
Computer science,
Application software,
Motion pictures,
Geometry,
Organizing"
The communication revolution and its effects on 21/sup st/ century engineering education,"The communication revolution that began in the latter portion of the 20/sup th/ century has brought a new focus on communication to 21/sup st/ century students both in and out of the classroom. In the classroom, student communication is rapidly increasing through the application of active learning strategies and cooperative learning. No longer are students expected to learn in isolation. Outside the classroom, technology has brought the classroom to the student in a variety of ways now centered on the World Wide Web. Students have access to e-mail and information 24-hours a day and are often expected to collaborate and communicate with classmates in design teams and on homework. This paper explores the nearly all-encompassing focus on communication as a cornerstone in the education of today's engineering students.","Engineering education,
Web sites,
Internet,
Engineering students,
Teamwork,
Video compression,
Laboratories,
Isolation technology,
Collaboration,
Employment"
Infrastructure for the electronic business on the internet,,"Internet,
Tellurium,
Computer science,
Virtual manufacturing,
Application software,
Research and development,
Laboratories,
Electronic switching systems,
XML,
Mediation"
Establishing enterprise communities,"One of the most important challenges facing the builders of enterprise software is the reliable implementation of the policies that are supposed to govern the various communities operating within an enterprise. Such policies are widely considered fundamental to enterprise modeling, and their specification were the subject of several recent investigations. But specification of the policy that is to govern a given community is only the first step towards its implementation; the second, and more critical step is to ensure that all members of the community actually conform to the specified policy. The conventional approach to the implementation of a policy is to build it into all members of the community subject to it. But if the community in question is large and heterogeneous, and if its members are dispersed throughout a distributed enterprise, then such ""manual"" implementation of its policy would be too laborious and error-prone to be practical. Moreover, a policy implemented in this manual manner would be very unstable with respect to the evolution of the system, because it can be violated by a change in the code of any member of community subject to it. It is our thesis that the only reliable way for ensuring that an heterogeneous distributed community of software modules and people conforms to a given policy is for this policy to be strictly enforced. A mechanism for establishing enterprise communities by formally specifying their policies, and by having these policies enforced is the subject of the paper.",
Massively parallel distributed feature extraction in textual data mining using HDDI/sup TM/,"One of the primary tasks in mining distributed textual data is feature extraction. The widespread digitization of information has created a wealth of data that requires novel approaches to feature extraction in a distributed environment. We propose a massively parallel model for feature extraction that employs unused cycles on networks of PCs/workstations in a highly distributed environment. We have developed an analytical model of the time and communication complexity of the feature extraction process in this environment based on feature extraction algorithms developed in our textual data mining research with HDDI/sup TM/ (Hierarchical Distributed Dynamic Indexing). We show that speedups linear in the number of processors are achievable for applications involving reduction operations based on a novel, parallel pipelined model of execution. We are in the process of validating our analytical model with empirical observations based on the extraction of features from a large number of pages on the World Wide Web.","Feature extraction,
Data mining,
Analytical models,
Information management,
Personal communication networks,
Workstations,
Complexity theory,
Distributed processing,
Computer science,
Data engineering"
Microelectronics education at Ngee Ann Polytechnic,"The quality of engineering courses comes from the combination of the theoretical and practical components of the subject. The hands-on aspect of their education would equip graduates with the necessary practical knowledge as they enter the manufacturing industries. However, the problem is aggravated in microelectronic education by the high cost of the equipment and its maintenance. A six-month full-time industrial attachment was included in the curriculum to give the students hands-on experience. In addition, an e-learning module was introduced to ensure that the theoretical content of the course is not reduced as a result of the industrial attachment.",
An energy-efficient initialization protocol for wireless sensor networks,"A wireless sensor network (WSN, for short) is a distributed system consisting of n sensor nodes and a base station. We propose an energy-efficient protocol to initialize sensor nodes in WSNs, that is, to assign a unique ID to each sensor node. We show that if the number n of sensor nodes is known beforehand, for any f/spl ges/1 and any small /spl mu/ (O",
Adaptive parameter collection in dynamic distributed environments,"Cost-effectively collecting distributed state information is a challenging problem. There is perhaps no single perfect solution since different distributed application environments pose different requirements from the information collection process. Knowledge of the environment, in terms of traffic conditions, load models etc., plays a key role in determining tradeoffs between accuracy (needed to ensure quality-of-service requirements) and cost-effectiveness. We develop an adaptive information collection algorithm that utilizes network traffic knowledge characterized using a time series model. The algorithm utilizes an information collection architecture consisting of a directory service integrated into the middleware layer with monitoring modules distributed across the network. The cost-effectiveness of the proposed information collection algorithm proposed is verified in simulations over diverse network traffic patterns, i.e. Internet WAN (TCP), MPEG (multimedia) and Web access traffic traces. Our results show that the proposed adaptive information collection algorithm compensates for inaccuracies in network traffic predictions in a cost-effective manner.","Traffic control,
Telecommunication traffic,
Communication system traffic control,
Network servers,
Monitoring,
Load management,
Protocols,
Computer science,
Application software,
Load modeling"
Bibel's matrix connection method in paraconsistent logic: general concepts and implementation,"Bibel's matrix connection method is an alternative to resolution for the mechanized proof of logical statements. Bibel's method was originally defined for classical logic. In this work, an adaptation of the method for annotated propositional logic is given, followed by a simple case study. Some implementation details are also presented.","Logic,
Lattices"
Question model for intelligent questioning systems in engineering education,"This paper describes the development of a question model to be used with an intelligent questioning system. The purpose of the intelligent questioning system is to improve the educational process in engineering courses by allowing students to learn more in less time, to understand more deeply, and to enjoy their learning experience. Key elements of this system are a question model and an adaptive question management system that uses a hierarchical knowledge map to direct the learning process based on the student's degree of understanding of individual or grouped concepts. Although there are several online computer-based questioning systems, they typically have no built-in help, no guidance if questions are answered incorrectly, no method for selecting questions based on the students needs, and no comprehensive monitoring of a student's progress through a knowledge map not only of the course but also of the curriculum.","Intelligent systems,
Engineering education,
Educational technology,
Adaptive systems,
Knowledge management,
Educational programs,
Continuing education,
Databases,
Computerized monitoring,
Educational products"
Probabilistic polynomial-time process calculus and security protocol analysis,,"Polynomials,
Calculus,
Security,
Cryptography,
Cryptographic protocols,
Testing,
Logic design,
Standards development,
Body sensor networks,
Contracts"
Hierarchical fuzzy-KNN networks for news documents categorization,"In this paper, we present a document categorization method based on the hierarchical fuzzy networks. The proposed model employs the divide-and-conquer principle to resolve documents categorization problem based on a predefined hierarchical structure. The final classification framework can be interpreted as a hierarchical array of non-linear decision tree. Each node in the tree represents one filter. The fuzzy K-nearest-neighbor (KNN)-based filter decides that the unknown document belongs to the corresponding category or not. We use the Reuters-21578 news data set to evaluate the performance of the proposed method.",
Commitment-based interoperation for e-commerce,"Successful e-commerce presupposes techniques by which autonomous trading entities can interoperate. Although much progress has been made on data exchange and payment protocols, interoperation in the face of autonomy is still inadequately understood. Current techniques, designed for closed environments, support only the simplest interactions. We develop a multiagent approach for interoperation in e-commerce. This approach consists of: (1) a behavioral model to specify autonomous, heterogeneous agents representing different trading entities (businesses, consumers, brokers), (2) a metamodel that provides a language (based on XML) for specifying a variety of service agreements and accommodating exceptions and revisions, and (3) an execution architecture that supports persistent and dynamic (re)execution.",
The Crane Beach Conjecture,"A language L over an alphabet A is said to have a neutral letter if there is a letter e/spl isin/A such that inserting or deleting e's from any word in A* does not change its membership (or non-membership) in L. The presence of a neutral letter affects the definability of a language in first-order logic. It was conjectured that it renders all numerical predicates apart from the order predicate useless, i.e., that if a language L with a neutral letter is not definable in first-order logic with linear order then it is not definable in first-order. Logic with any set /spl Nscr/ of numerical predicates. We investigate this conjecture in detail, showing that it fails already for /spl Nscr/={+, *}, or possibly stronger for any set /spl Nscr/ that allows counting up to the m times iterated logarithm, 1g/sup (m)/, for any constant m. On the positive side, we prove the conjecture for the case of all monadic numerical predicates, for /spl Nscr/={+}, for the fragment BC(/spl Sigma/) of first-order logic, and for binary alphabets.",
A webcast virtual laboratory on a frequency modulation experiment,"A number of Internet remote experimentation has been successfully developed for teaching and research purposes in the National University of Singapore (NUS). As only one user can assume control of the apparatus in any physical or remote experiment, the access to these experiments has hitherto been only been limited to single users one at a time. Without any increase in the hardware experimental apparatus needed, this paper presents a new webcast based approach for remote experimentation that allows several observing users to view an existing remote experimental session while it is being conducted by a main user. Multicast, the state of the art technology, is being adopted to implement the webcasting capability for remote experimentation purposes with some degree of reliability provided. In multicast, only one copy of the same data is sent to a group address, reaching all the observing users in a particular remote laboratory session. Thus, the system can be accessed by as many users as possible without overloading network and server resources. This is particularly useful as the number of users that may want to observe the experiment cannot be predicted in advance.",
TSFD: two stage frame dropping for scalable video transmission over data networks,"Scalable video transmission is used to adjust the rate of video depending on the level of network congestion. Previous studies on scalable video transmission of MPEG over ATM ABR service required major changes in the network protocols and did not provide methods to determine the ABR connection parameters. We propose a new scalable video transmission scheme which does not require major changes in network protocols. In our proposed scheme, frames are dynamically dropped either by the source or the network depending on the level of network congestion.","Matrix decomposition,
Bit rate,
Video on demand,
Streaming media,
Protocols,
Bandwidth,
Network servers,
Computer networks,
Computer science,
Standards development"
Compaction techniques for nextword indexes,,
"OSCAR and the Beowulf arms race for the ""cluster standard""",,
A probabilistic framework for graph clustering,"The paper describes a probabilistic framework for graph clustering. We commence from a set of pairwise distances between graph structures. From this set of distances, we use a mixture model to characterize the pairwise affinity of the different graphs. We present an EM-like algorithm for clustering the graphs by iteratively updating the elements of the affinity matrix. In the M-step we apply eigendcomposition to the affinity matrix to locate the principal clusters. In the M-step we update the affinity probabilities. We apply the resulting unsupervised clustering algorithm to two practical problems. The first of these involves locating shape-categories using shock trees extracted from 2D silhouettes. The second problem involves finding the view structure of a polyhedral object using the Delaunay triangulation of corner features.","Clustering algorithms,
Iterative algorithms,
Tree graphs,
Computer science,
Electric shock,
Machine learning,
Unsupervised learning,
Knowledge engineering,
Pattern recognition,
Computer vision"
Run-time characterization of irregular accesses applied to parallelization of irregular reductions,"Irregular reduction operations are the core of many large scientific and engineering applications. There are, in the literature, different methods to solve these operations in parallel. In this paper we discuss a new technique which improves performance significantly, both in terms of execution time and memory overhead. These improvements are achieved in the preprocessing as well as in the resulting parallel code. Our proposal is based on the use of the Irregular Access Region Descriptor (IARD). This data structure is a compact characterization of indirectly accessed arrays that can be used for the efficient parallelization of a wide spectrum of irregular codes. In this paper we present its application to parallelize irregular reduction operations.","Runtime,
Data structures,
Sparse matrices,
Proposals,
Computer science,
Application software,
Libraries,
Costs,
Kernel,
Polarization"
Integrating voice and data services for mobile internet collaboration with the MOVE middleware architecture,"Voice and data services for mobile collaboration have hitherto been offered only in isolation or loosely coupled, at best. The MOVE mobile middleware architecture enables the integration of voice and data services to a much higher degree. This paper is an experience report about the design and the use of this architecture to enhance a multimedia service for mobile collaboration, with voice services typically provided by call-centre organisations, and data services typically provided by collaboration facilities of commercial web browsers.","Web and internet services,
Collaboration,
Middleware,
Mobile computing,
Application software,
Mobile communication,
Streaming media,
Service oriented architecture,
Availability,
Computer science"
Towards the fault tolerant software: fuzzy extension of crisp equivalence voters,"Redundancy, on which fault tolerance is based, can be achieved through hardware, software, information and time. With respect to different version outputs from redundant software versions, voting strategies are separated into two classes. Voting strategies are either based on output classification, partitioning of the outputs, or on convergence functions. The traditional equivalence relation does not enable gradual comparisons below the fixed threshold. Fuzzy extension of classical numerical equivalence relation, proposed in the paper, overcomes those potential problems. Test examples are graphically illustrated.","Fault tolerance,
Voting,
Redundancy,
Hardware,
Computer science,
Convergence,
Testing,
Reliability,
History,
Industrial Electronics Society"
Optimal maximal encoding different from Huffman encoding,"Novel maximal encoding, encoding, and maximal prefix encoding different from Huffman encoding are introduced. It is proven that for finite source alphabets all Huffman codes are optimal maximal codes, codes, and maximal prefix codes. Conversely, the above three types optimal codes need not to be the Huffman codes. Completely similar to Huffman codes, we prove that for every random variable with a countably infinite set of outcomes and with finite entropy there exists an optimal maximal code (code, maximal prefix code) which can be constructed from optimal maximal codes (codes, maximal prefix codes) for truncated versions of the random variable, and furthermore, that the average code word lengths of any sequence of optimal maximal codes (codes, maximal prefix codes) for the truncated versions converge to that of the optimal maximal code (cone, maximal prefix code).","Encoding,
Random variables,
Computer science,
Entropy,
Length measurement"
A hybrid approach to video retrieval in a generic video management and application processing framework,,"Information retrieval,
Content based retrieval,
Streaming media,
Layout,
Application software,
Computer science,
Object oriented databases,
Spatiotemporal phenomena,
Councils,
Software prototyping"
Coupling computation of the BEM and FDM in 3D capacitance extraction,"When using the boundary element method (BEM) to calculate the capacitance of three-dimensional VLSI interconnects with multiple dielectrics, a significant error can occur if some dielectrics do not contain a conductor. We present a new algorithm of coupling computation of BEM and the finite difference method (FDM) to resolve the problem. This algorithm still uses BEM for dielectrics which contain conductors, but employs FDM for those without. Then we couple these equations to calculate the capacitance. The numerical results indicate that this algorithm reduces calculation error greatly.",
How to design Web-based counseling systems,"The purpose of a Web based counseling and evaluation system is to serve students and other interest groups of a virtual university. To develop a competent system requires careful design. We describe the requirements of our system, and specifically we discuss the criteria we have created to evaluate existing counseling systems. Criteria work as guidelines when designing a counseling system. The evaluation process helps us to construct technically more solid solutions for the needs of the Finnish virtual university counseling and evaluation system.","Employee welfare,
Guidelines,
Computer science,
Solids,
Personnel,
Research and development,
Computer aided instruction,
Information analysis,
Process design,
Navigation"
Position summary. DiPS: a unifying approach for developing system software,"In this position paper we unify three essential features for flexible system software: a component oriented approach, self-adaptation and separation of concerns. We propose DiPS (Distrinet Protocol Stack), a component framework, which offers components, an anonymous interaction model and connectors to handle non-functional aspects such as concurrency.","Electronics packaging,
System software,
Operating systems,
Protocols,
Concurrent computing,
Computer science,
Connectors,
Runtime,
Prefetching,
Software maintenance"
Design of electric filters in MAPLE and through WWW interface,"This paper presents a system for analog filter design. The system consists of a special library of functions, SYNTFIL, for the MAPLE environment which allows both approximation, computation and consecutive synthesis of an electrical circuit. A WWW interface for the complex design of filters is then presented. The WWW interface uses the above mentioned MAPLE software. The system is proposed for education in the CTU, Faculty of Electrical Engineering.",
Mining decision trees from data streams in a mobile environment,"This paper presents a novel Fourier analysis-based technique to aggregate, communicate and visualize decision trees in a mobile environment. A Fourier representation of a decision tree has several useful properties that are particularly useful for mining continuous data streams from small mobile computing devices. This paper presents algorithms to compute the Fourier spectrum of a decision tree and vice versa. It offers a framework to aggregate decision trees in their Fourier representations. It also describes a touchpad/ticker-based approach to visualize decision trees using their Fourier spectrum and an implementation for PDAs.","Decision trees,
Data mining,
Data visualization,
Personal digital assistants,
Aggregates,
Mobile computing,
Time factors,
Wireless networks,
Cellular phones,
Computer science"
Research on role-based learning technologies,"One of the goals of science education is to familiarize students with an intellectual framework based on established scientific principles and general approaches that can later be used to solve science-based problems. Science is also content-based, and students must master the content of a discipline in order to succeed. The challenge for science educators is to develop educational tools and methods that deliver the principles but at the same time teach the important content material, but in a meaningful way. The paper describes research based on experimental virtual role-based environments built to explore the following beliefs: educational technology should capitalize on the natural human propensity for role-playing; students will be willing to assume roles if the environment makes it easy to do, and if the environment reinforces role-playing through careful crafting of explicit tutorial components; that educational software should be engaging, entertaining, attractive, interactive, and flexible: in short, game-like. The experiences provided to the student within these virtual worlds can be both meaningful and authentic, although some trade-offs are required to make them fun, challenging, and occasionally unpredictable.","Computer science,
Geology,
Object oriented modeling,
Multiuser detection,
Computer science education,
Educational technology,
Buildings,
Biology,
Humans,
Web sites"
Optimal checkpoint interval analysis using stochastic Petri net,"While various checkpointing schemes have been widely used to reduce the recovery time when a fault occurs, the problem of evaluating the optimal checkpoint interval that maximizes the availability of the system has been a critical research issue for decades. The evaluation can be done by developing analytical models with restrict assumptions. However, the analytical model has reached its limitations as the checkpointing schemes become complicated. This paper proposes to use stochastic Petri net model for the evaluation and shows the effectiveness of the approach using case studies. The paper develops stochastic Petri net models and shows how to obtain the optimal checkpoint intervals for systems employing two widely used checkpointing schemes: Checkpoint with Rollback Recovery scheme for uniprocessor systems and Primary Site Approach for multiprocessor systems.","Stochastic processes,
Checkpointing,
Analytical models,
Stochastic systems,
Multiprocessing systems,
Computer science,
Statistics,
Availability,
Fault detection,
Random variables"
MOOSE - a task-driven program comprehension environment,"Many tools have been developed to derive abstract representations from existing source code. Yet, most of these tools provide only little help in providing an encompassing picture of the system under examination. Graphical visualization techniques derived from reverse engineered source code have long been recognized for their impact on improving the comprehensibility of software systems and their source code. In this paper, we present a task-oriented approach to software comprehension by introducing our MOOSE (Montreal Object-Oriented Slicing Environment) environment that provides a task-driven wizard approach that supports a cognitive comprehension model combined with reverse engineering techniques, algorithmic and visualization support. We close our discussion with a brief overview of typical software comprehension tasks and how the MOOSE environment will benefit users during these comprehension tasks.","Reverse engineering,
Object oriented modeling,
Software maintenance,
Software systems,
Computer science,
Design methodology,
Data visualization,
Pattern matching,
Pattern recognition,
Software quality"
Parallel simulated annealing for the delivery problem,"A delivery problem which reduces to an NP-complete set-partitioning problem is considered. Two algorithms of parallel simulated annealing, i.e. the simultaneous independent searches and the simultaneous periodically interacting searches are investigated. The objective is to improve the accuracy of solutions to the problem by applying parallelism. The accuracy of a solution is meant as its proximity to the optimum solution. The empirical evidence supported by the statistical analysis indicates that the interaction of processes in parallel simulated annealing can yield more accurate solutions to the delivery problem as compared to the case when the processes run independently.","Simulated annealing,
Computational modeling,
Vehicles,
Message passing,
Temperature,
Computer simulation,
Computer science,
Statistical analysis,
Analytical models,
Concurrent computing"
Implementation of object attachments by cellular modeling,"We research the defects of geometric modeling in representing object attachments. It is difficult to represent different types of object attachments such as glueing or fusing in current computer graphics. We consider two types of different attachments such that an object is ""put"" on the top of another object, and an object is ""fused"" to the top of another object. To represent the relationships of object attachments, we assume a hypothesis such that we can represent the information of object attachments in computer graphics based on the cellular models, and consider the real implementation in computer graphics for proving that the cellular model of object attachments meet the hypothesis. The results of our research are expected to influence major applications including computer integrated manufacturing (CIM).","Computer graphics,
Solid modeling,
Computer aided manufacturing,
Cities and towns,
Design automation,
CADCAM,
Manufacturing processes,
Computer science,
Computer integrated manufacturing"
Uniform circuits for division: consequences and problems,"Integer division has been known to lie in P-uniform TC/sup 0/ since the mid-1980s, and recently this was improved to L-uniform TC/sup 0/. At the time that the results in this paper were proved and submitted for conference presentation, it was unknown whether division lay in DLOGTIME-uniform TC/sup 0/ (also known as FOM). We obtain tight bounds on the uniformity required for division, by showing that division is complete for the complexity class FOM+POW obtained by augmenting FOM with a predicate for powering modulo small primes. We also show that, under a well-known number-theoretic conjecture (that there are many ""smooth"" primes), POW (and hence division) lies in FOM. Building on this work, Hesse has shown recently that division is in FOM [17]. The essential idea in the fast parallel computation of division and related problems is that of Chinese remainder representation (CRR)-storing a number in the form of its residues modulo many small primes. The fact that CRR operations can be carried out in log space has interesting implications for small space classes. We define two versions of s(n) space for s(n)=o(log n): dspace(s(n)) as the traditional version where the worktape begins blank, and DSPACE(s(n)) where the space bound is established by endmarkers before the computation starts. We present a new translational lemma, and derive as a consequence that (for example), if one can improve the result of Hartmanis and Berman (1976) that {0/sup n/: n is prime} /spl notin/ dspace (log log n) to show that {0/sup n/: n is prime} /spl notin/ DSPACE (log log n), it would follow that L/spl ne/NP.","Circuits,
Computer science,
Concurrent computing,
Polynomials,
Turing machines"
An efficient test vector compression technique based on geometric shapes [system-on-a-chip],"One of the prime challenges of testing a system-on-a-chip (SOC) is to reduce the required test data size. In this paper, we introduce a novel geometric shapes based compression/decompression scheme that substantially reduces the amount of test data and hence reduces test time. The proposed scheme is based on reordering the test vectors in such a way that it enables the generation of geometric shapes that can be highly compressed via perfect lossless compression. Experimental results on ISCAS benchmark circuits demonstrate the effectiveness of the proposed technique in achieving very high compression ratio. Compared to published results, our technique achieves significantly higher compression ratio.","Shape,
Circuit testing,
System-on-a-chip,
System testing,
Costs,
Hardware,
Computer science,
Information systems,
Petroleum,
Minerals"
Generic description of a software document environment,UQ* is an evolving generic language-based environment for manipulation of structured documents. The environment is intended to capture both syntactic and relational structure within and between documents and to support user interaction via both textual and diagrammatic views. This paper illustrates the innovative features of the environment description language used to instantiate a UQ* environment.,"Computer languages,
Mathematics,
Computer science,
Application software,
Software engineering,
Programming environments,
Program processors,
Costs,
Navigation,
Technological innovation"
How children understand concurrent comics: experiences from LOFI and HIFI prototypes,"In a study of how ten to eleven year old children understand program representations based on comic strips, it turned out that narrative interpretations were more common when using a low fidelity paper prototype than when using a high fidelity computer prototype. One explanation for this is that a computer prototype ""sets the rules"" to a much greater extent than a paper prototype, thus narrowing the set of plausible interpretations.","Prototypes,
Strips,
Runtime,
Mood,
Computer science,
Programming profession,
Animation,
Mice,
Concurrent computing,
Arithmetic"
Extraction of rotation invariant signature based on fractal geometry,"A new method of feature extraction with a rotation invariant property is presented. One of the main contributions of this study is that a rotation invariant signature of 2D contours is selected based on fractal theory. The rotation invariant signature is a measure of the fractal dimensions, which is rotation invariant based on a series of central projection transform (CPT) groups. As the CPT is applied to a 2D object, a unique contour is obtained. In the unfolding process, this contour is further spread into a central projection unfolded curve, which can be viewed as a periodic function due to the different orientations of the pattern. We consider the unfolded curves to be non-empty and bounded sets in IR/sup n/, and the central projection unfolded curves with respect to the box computing dimension are rotation invariant. Some experiments with positive results have been conducted. This approach is applicable to a wide range of areas such as image analysis, pattern recognition etc.","Fractals,
Pattern recognition,
Feature extraction,
Computational geometry,
Computer science,
Rotation measurement,
Shape,
Image analysis,
Image processing,
Optical computing"
Data in your space [wireless access],"The wireless world of the future will disseminate data through both push and pull technologies. In this paper, we propose an architecture which facilitates global wireless access to data using both push and pull techniques. By combining these techniques on the wireless channels, a more efficient and effective data access paradigm is developed.","Broadcasting,
Space technology,
Cities and towns,
Availability,
Computer science,
Database systems,
Mobile computing,
Maintenance engineering,
Proposals,
Transaction databases"
Combining generality and practicality in a conit-based continuous consistency model for wide-area replication,"Replication is a key approach to scaling wide-area applications. However, the overhead associated with large-scale replication quickly becomes prohibitive across wide-area networks. One effective approach to addressing this limitation is to allow applications to dynamically trade reduced consistency for increased performance and availability. Although extensive study has been performed on relaxed consistency models in traditional replicated databases, none of the models can simultaneously achieve the following two typically conflicting requirements imposed by wide-area applications: generality (capturing application-specific consistency semantics) and practicality (enabling efficient application-independent consistency protocols to be designed and providing natural ways to express application semantics). We propose a conit-based continuous consistency model designed to simultaneously achieve generality and practicality. Our conit theory provides generality, where application-specific consistency requirements are exported as conits. Practicality is achieved by using a simple, spanning set of metrics for conit consistency and by using a per-write weight specification. We demonstrate the generality of our model through representative wide-area applications and by showing that a number of existing models can be expressed as instances of our model. Our efficient, application-independent consistency protocols and prototype implementation verify its practicality.","Application software,
Programming profession,
Computer science,
Prototypes,
Access protocols,
Hafnium,
Databases,
Collaboration,
Sensor systems and applications,
Large-scale systems"
The Logistical Session Layer,The Logistical Session Layer is a system to enable enhanced functionality to distributed programming systems. The term Logistical refers to the fact that we enhance the traditional client-server model to allow for intermediate systems which are neither. This system generalizes the notion of caches but represents a cleaner architecture in that it explicitly declares itself to be a session layer protocol.,"Distributed computing,
Logistics,
Computer networks,
Communication system control,
Grid computing,
Bandwidth,
Computer science,
Functional programming,
Access protocols,
Collaborative work"
An architecture for multidatabase systems based on CORBA and XML,"A multidatabase system is an effective approach to data sharing and interoperability among many distributed and heterogeneous data sources. A CORBA-based architecture model of multidatabase system is introduced, and an XML-oriented common data model, named XIDM, is presented. These models conform to the characteristics of multidatabase systems, such as autonomy, distribution and heterogeneity. Panorama, a prototype system on the basis of these models, is also introduced.","XML,
Management information systems,
Satellite broadcasting,
Computer architecture,
Prototypes,
File systems,
Educational institutions,
Computer science,
Data models,
Computer networks"
The confluence of ground term rewrite systems is decidable in polynomial time,"The confluence property of ground (i.e., variable-free) term rewrite systems (GTRS) is well-known to be decidable. This was proved independently by M. Dauchet et al. (1987; 1990) and by M. Oyamaguchi (1987) using tree automata techniques and ground tree transducer techniques (originated from this problem), yielding EXPTIME decision procedures (PSPACE for strings). Since then, it has been a well-known longstanding open question whether this bound is optimal. The authors give a polynomial-time algorithm for deciding the confluence of GTRS, and hence alsofor the particular case of suffix- and prefix string rewrite systems or Thue systems. We show that this bound is optimal for all these problems by proving PTIME-hardness for the string case. This result may have some impact on other areas of formal language theory, and in particular on the theory of tree automata.",
"Neural network modeling study of one dimension gray problem GNNM (1, 1)","The study on representing and processing uncertain information is an important subject in information era. This paper combines gray system with neural network in processing one dimension gray problem, which is common in practice. Based on analyzing the feature of gray system and neural network, the initial model is discussed at first. The gray neural network model GNNM (1, 1) is given after improving the initial model including its theory, gray neural network model GNNM (1, 1) and the learning algorithm. In addition, GNNM (1, 1) is applied in a sample of material science to forecast the abrasion rate and it has proved GNNM (1, 1) is feasible and correct which is much better than tradition methods.","Neural networks,
Differential equations,
Cities and towns,
Predictive models,
Computer science,
Economic forecasting,
Materials science and technology,
Information representation,
Eigenvalues and eigenfunctions,
Time factors"
Adding security and privacy to agents acting in a marketplace: a trust model,"A general trust model for secure electronic agent-based marketplaces is described. The trust is presented as a dimensional space covering from physical security to high level trust relationships. A specific scenario has been chosen to show an implementation of the trust model: a secure multi-agent marketplace designed to manage resources in future mobile communications networks (J. Bigham et al., 2000). The multi-agent system is being developed as part of the IST SHUFFLE project (http://www.ist-shuffle.org).","Privacy,
Cryptographic protocols,
Consumer electronics,
Multiagent systems,
Artificial intelligence,
Computer security,
Computer science,
Resource management,
Public key cryptography,
Voting"
Scouting context-sensitive components,"Nature's gadgets are implemented without being planned and therefore can utilize context-sensitive components. Thus functionality that would require extensive networks of context-free components can be elicited from a minimum of material. Proteins can serve as context-sensitive components for pattern processing applications. We here describe an evolutionary search strategy currently under investigation for its potential use in conjunction with computer controlled fluidics to evaluate the computational capabilities of proteins. Our algorithm employs evolutionary search not to seek an optimum, but to seek surprises. It directs experiments and incrementally constructs an empirical model from their outcome. Reward is given for discovering conditions that exhibit a discrepancy between the prediction of the current model and the experimental result. As unexpected observations are incorporated into the model, the reward associated with them vanishes. Results obtained so far indicate that evolutionary search is a useful paradigm for characterizing the phenomenology of context-sensitive components.",
Small signal model and efficient parameter extraction technique for deep submicron MOSFETs for RF applications,A new small-signal model for deep submicrometre MOSFETs is proposed for accurately predicting MOS transistor behaviour up to 15 GHz. It is a unified model suitable for both baseband and RF simulation and valid in both the triode and saturation regions. The model is implemented as a macromodel with parasitic elements added to the BSIM3v3 core. The BSIM3v3 model is the ideal basis for RF simulations as it consists of an accurate nonquasistatic model and a capacitance model. The parameter extraction methodology is analogous to the 'divide and conquer' strategy used in computer science. This approach is superior to the traditional method of optimising the entire model to fit the measured S-parameters. It has shown an increase in accuracy and optimisation speed while reducing convergence problems caused by global optimisation. Excellent agreement between measured and simulated S-parameters at different biasing conditions in the range from 50 MHz to 15 GHz has been obtained.,"S-parameters,
MOSFET,
semiconductor device models,
divide and conquer methods"
A flexible polygon representation of multiple overlapping regions of interest for wavelet-based image coding,"Image transmission over low-bandwidth channels can be speeded up if the image coding mechanism supports regions of interest (RoIs). By such a scheme, image parts not belonging to RoIs can be encoded at a lower bitrate. This paper describes a flexible dynamic RoI scheme which supports the definition of arbitrarily-shaped RoIs before the start of the transmission process and the refinement of a partially transmitted image by the definition of new RoIs at any time during image transmission. The overhead to represent RoIs is kept as small as possible by exploiting a polygon-based RoI representation. Span arithmetic on a multiresolution grid is used to support the redundancy-free transmission of overlapping RoIs.","Image coding,
Image communication,
Shape,
Arithmetic,
Wavelet coefficients,
Computer science,
Computer graphics,
Computed tomography,
Bit rate,
Lapping"
Open protocols for web-based educational materials,"Using computers to deliver instructional material should be an excellent match of technology to education. Countless products meant to enhance or replace the classroom experience are currently available, yet few are utilized in spite of the national drive to increase the use of educational technology in our schools. Factors behind this under-utilization are complex but include expense, lack of flexibility, the proprietary nature of the products, and lack of teacher training, support, and time. Any computer-based instructional tool should incorporate the following characteristics in order to maximize utility and flexibility while minimizing cost and time: (1) be cross-platform, (2) separate content from delivery, (3) provide creation, editing, and delivery capabilities, and (4) use open standards. Our work proposes partitioning educational software along lines of expertise: open source authoring and presentation tools, open protocols for instructional design, and separate content created using the most appropriate application programs. We show the feasibility of meeting all the requirements with this approach using XML and Java.","Protocols,
Computer aided instruction,
Educational technology,
Computer science education,
Educational products,
Educational institutions,
Costs,
Open source software,
Software tools,
Application software"
Failure mechanisms of SOI high-voltage LIGBTs and LDMOSes under unclamped inductive switching,"This paper examines the behaviour of silicon-on-insulator (SOI) LIGBTs and LDMOSes under unclamped inductive switching (UIS). Surprisingly, it is found that LIGBTs can absorb much less UIS current than LDMOSes, specifically only between one-half and one-third. Two-dimensional device simulation showed that this was because hole injection from the LIGBT anode during turn-off changed the potential distribution within the device, leading to field concentration beneath the gate and hence premature failure of the gate. This has an important impact on the choice of a power device in a power integrated circuit, and its design.","Failure analysis,
Robustness,
Power integrated circuits,
Computer science,
Silicon on insulator technology,
Computational modeling,
Anodes,
Integrated circuit manufacture,
Breakdown voltage,
Magnetic switching"
Algorithms that compute test drivers in object oriented testing,"Program testing has been a successfully established and implemented issue for the imperative paradigm. On the other hand, program testing for object-oriented (OO) is being constantly revisited and discussed. In this paper we present algorithms that compute the number of test drivers needed in testing OO systems. Testing OO programs would have been a done issue a long time ago if the imperative techniques were sufficient for that paradigm. However, more powerful and rigorous algorithms are significantly needed for OO programs since they differ in features and concepts than the imperative ones. In this study we have intended to make such a need, a primary goal.",
A rigorous method for testing real-time reactive systems,"Real-time reactive systems are complex systems to design and verify. Rigorous testing of real-time reactive systems complement the more difficult and expensive formal verification process. This paper discusses a rigorous method for block-box testing of real-time reactive systems, whose design specifications are given in the timed reactive object model (TROM) formalism.","System testing,
Real time systems,
Automata,
Object oriented modeling,
Clocks,
Formal specifications,
Synchronization,
Time factors,
Computer science,
Safety"
Predicting the impact of advertising: a neural network approach,"This paper studies if neural networks using the temporal structure of the domain can raise the accuracy when predicting the outcome of investments in advertising (both on monthly and yearly basis), compared to the methods used today. The focus has been to investigate if future publicity can be predicted from historical outcome and planned future media investments. The domain is the car industry. This paper contains a case study where ANNs utilize time series effects for accurate prediction (the effect of advertising has a temporal signature). It is a comparative study between different network architectures that conclusively show that sequential and recurrent approaches exploit the time series dependencies and yield a performance, which supersede approaches traditionally used.",
Use of engineering case studies to teach associate degree electrical engineering technology students,"This paper describes the use of engineering case studies to teach associate degree electrical engineering technology (2EET) students at the Penn State University, Altoona College (Penn State Altoona). Engineering case studies are used in several 2EET courses such as digital electronics (EET 117), semiconductors (EET 210), and electrical machines (EET 213 W). The paper begins with a description of the need for the use of engineering case studies to teach engineering technology students. Development and implementation of the case study based instructional model used in several 2EET courses is described next. The results of implementation of this instructional technique in the 2EET courses are presented in the paper.",
Integrating object oriented design with concurrency using Petri nets-A case study of a banking system and the Syroco-Macao environment,"Petri nets provide a well understood formal method for modeling concurrent systems, in all of their complexity. Petri nets, however, do not directly support the concepts of modularization, encapsulation, and information hiding that are essential to the successful abstraction of complexity provided by the OO methodology. The Petri net model of even a simple system can become overwhelmingly complex. The first goal of this paper was to survey a number of tools combining the OO technique with Petri nets, to pick one, and to use it in an implementation of an OO design. There was a great desire to utilize a tool having a graphical interface for the Petri net presentation. It was also highly desirable to utilize colored Petri nets. The design chosen was that of a bank ATM machine. The tool selected for use was SYROCO. This tool provides a means to define Cooperative objects (COO), which are translated into C++ classes. The COO language uses high level Petri nets to model the internal operation of objects, and provides a mechanism to connect together the Petri nets of distinct objects via the normal OO interface.",
An empirical study of facilitation of computer-mediated distributed requirements negotiations,Group facilitation is an important element of group approaches to requirements engineering (RE). The facilitation in traditional face-to-face groups is challenged by the increased globalization of the software industry. Thorough empirical investigation of human facilitation in computer-mediated requirements meetings is needed. This paper presents findings about the facilitation of distributed group settings in a controlled environment. Three professional facilitators mediate 15 three-person groups negotiating software requirements. Facilitation in face-to-face meetings is contrasted with four group settings in which the facilitator is physically separated from the group or co-located with key stakeholders. Rich qualitative and behavioral data enables an understanding of differences and similarities in the facilitation of the distributed groups and of aspects that were detrimental or beneficial to their facilitation. The empirical evidence indicates a reduced richness of social behaviors in computer-mediated group settings which: made the group facilitation problematic; but also enabled certain facilitation support in the medium itself.,
Vague regions and spatial relationships: a rough set approach,Uncertainty management is necessary for spatial data and GIS applications. This paper focuses on topological relationships and uncertainty in spatial data regions. We discuss the representation of vague regions using the RCC-8 theory and show how rough sets can improve on this methodology through the use of its indiscernibility relation and approximation regions.,"Uncertainty,
Rough sets,
Spatial databases,
Relational databases,
Geographic Information Systems,
Management information systems,
Topology,
Computer science,
Application software,
Artificial intelligence"
A distributed formation of orthogonal convex polygons in mesh-connected multicomputer,"The rectangular faulty block model is the most commonly used fault model in designing a fault-tolerant and deadlock-free routing algorithm in mesh-connected multicomputers. The convexity of a rectangle facilitates simple and efficient ways to route messages around fault regions using relatively few virtual channels to avoid deadlock. However, such a faulty block may include many nonfaulty nodes which are disabled, i.e. they are not involved in the routing process. Therefore, it is important to define a fault region that is convex, and at the same time, to include a minimum number of nonfaulty nodes. In this paper we propose a simple and efficient distributed algorithm that can quickly construct a set of special convex polygons, called orthogonal convex polygons, from a given set of rectangular faulty blocks in a 2-D mesh (or 2-D torus). The formation of orthogonal convex polygons is done through a labeling scheme based on iterative message exchanges among neighboring nodes.",
Space complexity of random formulae in resolution,"We study the space complexity of refuting unsatisfiable random k-CNFs in the resolution proof system. We prove that for any large enough /spl Delta/, with high probability a random k-CNF over n variables and /spl Delta/n clauses requires resolution clause space of /spl Omega/(n/spl middot//spl Delta//sup -1+/spl epsiv//k-2-/spl epsiv//), for any 0>/spl radic/n. This bound is nearly tight. Specifically, we show that with high probability, a random 3-CNF with /spl Delta/n clauses requires tree-like refutation size of exp(/spl Omega/(n//spl Delta//sup 1+/spl epsiv//1-/spl epsiv//)), for any 0","Time measurement,
Complexity theory,
Polynomials,
Bipartite graph,
Graph theory,
Size measurement,
Computer science,
Mathematics,
Tree graphs,
Scholarships"
Efficient metering scheme in the WWW,"In this paper, we present a secure and efficient metering scheme to measure the interaction between clients and servers in Web advertising. In most cases, Web advertising is composed of advertisers, clients, servers and an audit agency that collects metering information about the number of clients that are served by each server. The metering scheme should always be secure against fraud attempts by servers that maliciously try to inflate the number of their clients and against clients that attempt to disrupt the metering process, so we suggest secure and efficient metering schemes based on some cryptographic techniques.","World Wide Web,
Advertising,
Web server,
Cryptography,
Time measurement,
Computer science,
Information security,
Polynomials,
Scalability"
Rapid application development of middleware components by using XML,"Towards a general service infrastructure of cooperating services and devices, the gap between different systems and programming languages has to be overcome. As the tremendous success of the World Wide Web mainly depends on open standards, it is necessary to create an open standard regarding distributed interacting services. To create system and language independent services the presentation of services at a meta level is inevitable. The main goal of the presented system is the dynamic creation of system independent XML applications to automatically generate programming language and middleware dependent components.","Middleware,
XML,
Application software,
Computer languages,
Software systems,
Computer architecture,
Java,
Computer science,
Web sites,
Dynamic programming"
Performance of movement-based location update and one-step paging in wireless networks with sparsely underlaid microcells,"We study the performance of movement-based location update with one-step paging in a macrocell/microcell overlaid network. The network covers a whole service area with macrocells and high traffic density areas with microcells. The location tracking scheme requires each mobile platform (MP) to count the number of macrocell boundary crossings from the last location update and use the number, compared with two thresholds, to determine which tier to report to. If it is greater than or equal to the lower threshold the MP is allowed to update location information but report only to a low-tier microcell. If it reaches the higher threshold the MP reports immediately to the overlaying macrocell and resets the counter. For the scheme combined with one-step paging, expected total tracking cost per call is analyzed based on the MP mobility modelled as a semi-Markov chain with uniform transitions between neighboring microcell areas and the presence of a microcell determined by a Bernoulli trial with probability p of success. Results, corroborated by simulations, show that the analysis yields accurate performance bounds for a wide range of parameters. This allows us to study the effects of several critical factors on tracking costs.","Paging strategies,
Intelligent networks,
Wireless networks,
Microcell networks,
Macrocell networks,
Costs,
Performance analysis,
Computer science education,
Telecommunication traffic,
Tracking"
"Softening the object-oriented database model: imprecision, uncertainty, and fuzzy types","Object-oriented databases have proved to be a good alternative to the relational ones of Codd when dealing with applications characterized by their complexity and dynamism. A big part of the effort of researchers in the field of object-oriented databases (OODB) has been focused on the study of the addition of vagueness to this database model. There are different levels where vagueness can arise: uncertain and imprecise attribute values, fuzzy extents in classes, vague relationships between classes (including inheritance), and soft type definitions. We summarize our proposal in this area, showing how these different sources of vagueness can be managed over a traditional OODB system. We explain the new structures to be considered in order to incorporate vagueness and we use the Unified Modeling Language (UML) to make the conceptual representation of this structures clear because of its direct translation to an object-oriented model.","Softening,
Object oriented databases,
Object oriented modeling,
Uncertainty,
Relational databases,
Proposals,
Unified modeling language,
Computer science,
Application software,
Standards development"
Introduction to web computing,,"Java,
Portals,
Computer networks,
Libraries,
Distributed computing,
Databases,
Power system modeling,
Computer interfaces,
Grid computing,
Information technology"
Efficient sequenced temporal integrity checking,"Primary key and referential integrity are the most widely used integrity constraints in relational databases. Each has a sequenced analogue in temporal databases, in which the constraint must apply independently at every point in time. In this paper, we assume a stratum approach, which expresses the checking in conventional SQL, as triggers on period-stamped relations. We evaluate several novel approaches that exploit B/sup +/-tree indexes to enable efficient checking of sequenced primary key (SPK) and sequenced referential integrity (SRI) constraints. We start out with a brute-force SPK algorithm, then adapt the relational interval-tree overlap algorithm. After that, we propose a new method, the straight traversal algorithm, which utilizes the B/sup +/-tree more directly in order to identify when multiple key values are present. Our evaluation, on two platforms, shows that the straight traversal algorithm approaches the performance of built-in nontemporal primary key and referential integrity checking, with a constant time per tuple.","Relational databases,
Transaction databases,
Computer science,
Presses"
Resource Manager for distance education systems,"The authors extend the concept of distance education with adding a new service, that we call Resource Manager. The Resource Manager offers a possibility to the attendees to share different resources out of time and space boundaries. It enables geographically separated users to effectively facilitate remote access to various, presumably diverse, (real) resources. The Resource Manager has to provide an efficient sharing of resources among distance learning students according to the student profiles. We defined several protocols in XML suitable for communication between agents. The first experiments show that users are satisfied with the Resource Manager's usability. They found Resource Manager to be a very convenient service within the distance education systems.","Resource management,
Distance learning,
XML,
Collaborative software,
Laboratories,
Internet,
Protocols,
Computer science,
Engineering management,
Computer aided instruction"
A neural network approach for estimating large K distribution parameters,"The K distribution has been proposed in the literature as a general speckle model for ultrasonic backscatter. The shape parameter of this distribution can be used to provide clinically important information on tissue density and regularity. A neural approach for parameter estimation is proposed, specifically for large values of the shape parameter. Experimental results on simulated images show that this approach compares favorably with other methods. Thus, neural networks can be used in conjunction with other approaches to accurately model speckle, and thereby to classify tissue.","Neural networks,
Speckle,
Shape,
Rayleigh scattering,
Biomedical imaging,
Ultrasonic imaging,
Computer science,
Distributed computing,
Parameter estimation,
Frequency"
Mosaic-based clustering of scene locations in videos,"We present an approach for compact video summaries that allows fast and direct access to video data. The video is segmented into shots and scenes using a previously proposed method, then motion analysis is used to select representative shots for each scene. In contrast to approaches to video indexing which are based on key-frames, we use mosaics constructed from the representative shots for an efficient mosaic-based scene representation. We use a novel method for mosaic comparison to spatially cluster scenes and create a highly compact non-temporal representation of video. Our scene-based representation allows accurate comparison of scenes across different video data, and serves as a basis for indexing of whole video sequences.","Layout,
Indexing,
Video sequences,
Cameras,
Gunshot detection systems,
Histograms,
Computer science,
Motion analysis,
Clustering algorithms,
Organizing"
A performance analysis of the active memory system,"One major problem of using Java in real-time and embedded devices is the non-deterministic turnaround time of dynamic memory management systems (memory allocation and garbage collection). For the allocation, the nondeterminism is often contributed by the time to perform searching, splitting, and coalescing. For the garbage collection, the turnaround time is usually determined by the size of the heap, the number of live objects, the number of object collected, and the amount of garbage collected Even with the current state-of-the-art garbage collectors (generational and incremental schemes), they may or may not guarantee the worst case latency. Moreover such schemes often prolong overall garbage collection time. In this paper, the performance analysis of the proposed Active Memory Module (AMM) for embedded systems is presented Unlike the software counterparts, the AMM can perform a memory allocation in a predictable and hounded fashion (14 cycles). Moreover it can also yield a bounded sweeping time regardless of the number of live objects or heap size. By utilizing the proposed system, the overall speed-up can be as high as 23% over the JDK 1.2.2 running in classic mode.","Performance analysis,
Memory management,
Delay,
Embedded system,
Java,
Real time systems,
Runtime,
Hardware,
Computer science,
Technology management"
Lazy training: improving backpropagation learning through network interaction,"Backpropagation, similar to most high-order learning algorithms, is prone to overfitting. We address this issue by introducing interactive training (IT), a logical extension to backpropagation training that employs interaction among multiple networks. This method is based on the theory that centralized control is more effective for learning in deep problem spaces in a multi-agent paradigm. IT methods allow networks to work together to form more complex systems while not restraining their individual ability to specialize. Lazy training, an implementation of IT that minimizes misclassification error, is presented. Lazy training discourages overfitting and is conducive to higher accuracy in multiclass problems than standard backpropagation. Experiments on a large, real world OCR data set have shown interactive training to significantly increase generalization accuracy, from 97.86% to 99.11%. These results are supported by theoretical and conceptual extensions from algorithmic to interactive training models.","Backpropagation algorithms,
Optical character recognition software,
Training data,
Network topology,
Computer science,
Centralized control,
Artificial neural networks,
Robustness,
Databases,
Gaussian noise"
Agile monitoring for cyber defense,"The Monitoring, Analysis, and Interpretation Tool Arsenal (MAITA) seeks to support rapid construction and empirical reconfiguration of cyber defense monitoring systems inside the opponent decision cycle through a set of mechanisms including a flexible infrastructure for distributed monitoring processes and signal flows, a monitoring executive that coordinates resource allocation and systemic self-monitoring, and a library of monitoring process types, event descriptions, event recognition methods, alerting decision models, and other forms of monitoring knowledge.","Laboratories,
Computerized monitoring,
Computer science,
Libraries,
Condition monitoring,
Hospitals,
Artificial intelligence,
Signal analysis,
Signal processing,
Resource management"
Distributed QoS monitoring and edge-to-edge QoS aggregation to manage end-to-end traffic flows in Differentiated Services networks,"The Differentiated Services (DiffServ) framework has been proposed by the IETF as a simple service structure that can provide different Quality of Service (QoS) to different classes of packets in IP networks. IP packets are classified into one of a limited number of service classes, and are marked in the packet header for easy classification and differentiated treatments when transferred within a DiffServ domain. The DiffServ framework defines simple and efficient QoS differentiation mechanisms for the Internet. However, the original DiffServ concept does not provide a complete QoS management framework. Since traffic flows in IP networks are unidirectional from one network point to the other and routing paths and traffic demand get dynamically altered, it is important to monitor end-to-end traffic status, as well as traffic status in a single node. This paper suggests a distributed QoS monitoring method that collects the statistical data of each service class in every DiffServ router and calculates edge-to-edge QoS of the aggregated IP flows by combining routing topology and traffic status. A formal modeling of edge-to-edge DiffServ flows and algorithms for aggregating edge-to-edge QoS is presented. Also an SNMP-based QoS management prototype system for DiffServ networks is presented, which validates our QoS management framework and demonstrates useful service management functionality.","Diffserv networks,
Quality of service,
Monitoring,
Routing,
Topology,
Throughput,
Mathematical model"
Introduction of system level architecture exploration using the SpecC methodology,"To implement chip design on satisfactory target architectures, architecture exploration should be done at higher levels of abstraction, in the earliest design stages. Using the SpecC language, an executable system level design language, system level architecture exploration can proceed easily and smoothly as the system specification is being created. A SpecC methodology of system level architecture exploration is introduced within this paper to illustrate this process. The design of a JPEG encoder is used as an example to illustrate the system level architecture exploration methodology.","Computer architecture,
System-level design,
Process design,
Specification languages,
Design methodology,
Transform coding,
Computer science,
Chip scale packaging,
Time to market,
System testing"
Camera calibration with genetic algorithms,"We present a novel approach based on genetic algorithms for performing camera calibration. Contrary to the classical nonlinear photogrammetric approach, the proposed technique can correctly find the near-optimal solution without the need of initial guesses (with only very loose parameter bounds) and with a minimum number of control points (7 points). Results from our extensive study using both synthetic and real image data as well as performance comparison with Tsai's procedure demonstrate the excellent performance of the proposed technique in terms of convergence, accuracy, and robustness.","Cameras,
Calibration,
Genetic algorithms,
Robot vision systems,
Robustness,
Optimization methods,
Genetic mutations,
Computer science,
Genetic engineering,
Systems engineering and theory"
Design of multiple attractor GF(2/sup p/) cellular automata for diagnosis of VLSI circuits,"This paper introduces an efficient diagnosis scheme for VLSI circuits. A special class of non-group CA referred to as multiple attractor cellular automats (MACA) is introduced to diagnose the faulty block of a circuit under test (CUT). The scheme employs significantly lesser memory than the existing methods reported so far. Experimental results establish the efficiency of the scheme in terms of saving in memory space and execution time and enhanced diagnostic resolution. Rather than GF(2) CA where each CA cell handles GF(2) elements (0 and 1), the GF(2/sup p/) CA is employed to reduce the processing time.","Very large scale integration,
Circuit faults,
Circuit testing,
Fault diagnosis,
Dictionaries,
Sections,
Polynomials,
Computer science,
Educational institutions,
Automatic testing"
The toolkit for development of hybrid expert systems,"The architecture of the toolkit ESWin (Expert Systems under Windows) and hybrid expert systems supported by it are described. Knowledge representation by rules and frames is used. The possibility of keeping and extracting data from external databases by external programs is provided. Frames allow one to describe an application domain in terms of hierarchies of classes and owners. Frames consist of slots, which can be symbols, numerical or linguistic variables, or date and time. Frames can be joined with rules and procedures, processing determined events. Three types of frames are used: frame classes; frame examples, in which facts are described; and frame patterns, in which situations (events, examples, etc.) are described. Inverse fuzzy inference is used as the main method of knowledge processing. Fuzzy rules are assigned and processed, on the one hand, by using certain factors of conclusions of rules, and, on the other hand, by using linguistic variables. A user can enter both numerical and fuzzy symbol values of corresponding slots using linguistic variables. The ESWin software consists of three modules - a shell-interpreter, an editor-constructor of knowledge bases and a program for the viewing and diagnosis of knowledge bases.",
Vision model based perceptual coding of digital images,"The paper presents a perceptual coder based on the Embedded Block Coding with Optimised Truncation (EBCOT) structure which visually outperforms JPEG2000 VM 8.0 utilising mean-square-error (MSE) criterion. Furthermore, the proposed perceptual coder shows a performance comparable to or better than CVIS, the EBCOT implementation with visual masking. This performance gain is attributed to more advanced vision modelling which includes intra-band and cross-orientational masking.","Image coding,
Digital images,
Humans,
Virtual manufacturing,
Machine vision,
Distortion measurement,
Frequency,
Computer science,
Software,
OFDM modulation"
Quantifying the impact of architectural scaling on communication,"This work quantifies how persistent increases in processor speed compared to I/O speed reduce the performance gap between specialized, high performance messaging layers and general purpose protocols such as TCP/IP and UDP/IP. The comparison is important because specialized layers sacrifice considerable system connectivity and robustness to obtain increased performance. We first quantify the scaling effects on small messages by measuring the LogP performance of two Active Message II layers, one running over a specialized VIA layer and the other over stock UDP as we scale the CPU and I/O components. We then predict future LogP performance by mapping the LogP model's network parameters, particularly overhead into architectural components. Our projections show that the performance benefit afforded by specialized messaging for small messages will erode to a factor of 2 in the next 5 years. Our models further show that the performance differential between the two approaches will continue to erode without a radical restructuring of the I/O system. For long messages, we quantify the variable per-page instruction budget that a zero-copy messaging approach has for page table manipulations if it is to outperform a single-copy approach. Finally we conclude with an examination of future I/O advances that would result in substantial improvements to messaging performance.","Protocols,
Robustness,
Performance loss,
Communication system control,
Hardware,
Switches,
Computer science,
TCPIP,
Predictive models,
Costs"
Communication-efficient bitonic sort on a distributed memory parallel computer,"Sort can be speeded up on parallel computers by dividing and computing data individually in parallel. Bitonic sorting can be parallelized, however, a great portion of execution time is consumed due to O(log/sup 2/P) time of data exchange of N/P keys where P, N are the number of processors and keys, respectively. This paper presents an efficient way of data communication in bitonic sort to minimize the interprocessor communication and comparison time. Before actual data movement, each pair processor exchanges the minimum and maximum in its list of keys to determine which keys are to be sent to its partner. Very often no keys need to exchange, or only a fraction of them are exchanged. At least 20% or greater of execution time could be reduced on the T3E computer in our experiments. We believe the scheme is a good way to shorten the communication time in similar applications.","Concurrent computing,
Distributed computing,
Sorting,
Application software,
Partitioning algorithms,
Data communication,
Merging,
Load management,
Costs,
Information science"
Implementation of sequence patterns mining in network intrusion detection system,"In this paper we present a frequent sequence pattern mining-based algorithm used for network intrusion detection, which is an application and extension of the SPADE algorithm. It is based on the idea that much behavior on the network appears as sequences of activities, according to the sequence patterns we computed, we can construct the intrusion rule base and legal action rule base, then we can detect known and novel intrusion activities by rule matching. In addition, when the system is running, we use an incremental sequence pattern mining algorithm to complement the rule library in order to avoid re-executing the algorithm on the entire dataset, thereby reducing execution time. The experimental results indicate that this algorithm is efficient enough to meet the needs for active detection of intrusion. Compared with most existing methods used in commercial systems which are built using purely knowledge engineering approaches, our algorithm is more intelligent and adaptive.","Intrusion detection,
Law,
Legal factors,
Transaction databases,
Computer networks,
Knowledge engineering,
Information security,
Change detection algorithms,
Computer science,
Application software"
Cerenkov generators with overmoded one-sectional slow-wave structures,"Summary form only given, as follows. Results of investigations of physical processes of interaction of a high-current electron beam and, field of overmoded one-sectional slow-wave structures (SWS) in a three-centimeter wavelength range are presented. A 3D computer code, MULTIWAVES 5.2, based on the previously developed electromagnetic linear theory of Cerenkov devices has been developed for this purpose. Dispersion diagrams of electron (including the space charge waves, cyclotron and synchronous ones) and electromagnetic waves, transmission and reflection coefficients, starting currents of symmetric and non-symmetric modes can be calculated by means of this code. Interaction of longitudinal symmetric modes with a hollow electron beam is investigated by means of a nonlinear time-domain hybrid code. The analysis of Cerenkov generators with the beam current of 10 kA formed at the diode voltage of 500 kV has been made by means of the linear code. It has been shown that depending on the beam radius, the BWO-TWT or TWT-BWO generation regimes were realized. The starting parameters for the modes TM01 and HE11 have been calculated. It has been shown that the selection of the operating mode TM01 was possible due to the beam radius variation. The influence of cyclotron absorption of the TM0m-modes (m=1-4) on the generator starting current has been studied. The generation efficiency has been calculated by means of a nonlinear code. The dependence of the generation efficiency on the SWS length has maximum. It has been shown that in the radiated power the TM01-mode was up to 90% of the total power. Numerical simulation results are compared with the experimental ones.",
The design and implementation of a visual MPEG-4 scene-authoring tool,"We have implemented an MPEG-4 scene-authoring tool for complete profile in our laboratory. This tool is the extension of the results from our laboratory last year. We changed the system architecture and design for 3D scenes, the event in/out mechanism, visual editing, and friendlier user interface. We believe that such a tool can allow users to produce MPEG-4 contents easily and thus can push the MPEG-4 standard to be widely used. In this paper, we describe the design and implementation of an MPEG-4 scene-authoring tool, and we also introduce some necessary components for an MPEG-4 scene-authoring tool.","MPEG 4 Standard,
Layout,
Laboratories,
Facial animation,
Streaming media,
Multimedia communication,
Computer science,
Electronic mail,
User interfaces,
Digital TV"
Fuzzy cognitive state map vs markovian modeling of user's web behavior,"The vast majority of college students have been reared as researchers in an environment where boundaries for information have been clearly marked, i.e., that of books and paper text. Increasingly, however, they are called upon to perform tasks in an environment not clearly bounded, that of hyperspace. How do we learn to surf in this unfamiliar medium? What strategies do people use when surfing through the unbounded space of hyperlinks or the World Wide Web (WWW)? In order to effectively teach students new surfing skills we must be able to understand how neophyte web users form the cognitive neurological networks that result in a mental pathway, or cognitive map, that makes more navigable the route to further information as well as the information they set out to find. A markovian modeling of users behavior is introduced and compared to a fuzzy cognitive map (FCM) that represents the opinions of experts on how users surf the web. Experts are divided on what causes users to fail their queries on the web. This paper shows that a viable FCM model can be developed and some limit-cycle equilibria are uncovered. A FCM limit cycle repeats a sequence of events and actions. Limit cycles can reveal cognitive and behavioral patterns of users on the web. An adaptive FCM is built to reflect its causal behavior in time. This change reflects the users behavior as their knowledge of the web increases with time. The causal behavior learns from data. Users lean new patterns and reinforce old ones.","World Wide Web,
Limit-cycles,
Information retrieval,
Computer science,
Paper technology,
Educational institutions,
Books,
Web sites,
Fuzzy cognitive maps,
Web search"
Components + security = OS extensibility,"Component-based programming systems have shown themselves to be a natural way of constructing extensible software. Well-defined interfaces, encapsulation, late binding and polymorphism promote extensibility, yet despite this synergy, components have not been widely employed at the systems level. This is primarily due to the failure of existing component technologies to provide the protection and performance required of systems software. In this paper we identify the requirements for a component system to support secure extensions, and describe the design of such a system on the Mungi OS.","Protection,
Safety,
Operating systems,
Chaos,
Computer science,
Australia,
Encapsulation,
System software,
Dynamic compiler,
Programming profession"
Design of scalable interdomain IP multicast architecture,"The current IP multicast architecture does not have enough scalability and is not suitable for the whole Internet, because routing information is too large. We propose the scalable interdomain IP multicast architecture in order to achieve IP multicast communications over the whole Internet. The proposed architecture provides methods to reduce the routing information. We define two kind of multicast addresses, which are called VMA (virtual multicast address) and MAR (multicast address for routing). The VMA is used to identify the multicast group in the intra-domain and MAR is used to forward multicast packets between the interdomain. With the definition of two multicast addresses, multicast address allocation can be more flexible, and routing information can be reduced with address aggregation. We estimate the amount of reduced routing information with our architecture, and evaluate the proposed architecture.","Multicast protocols,
Routing protocols,
Multicast communication,
Computer architecture,
Web and internet services,
Unicast,
Bandwidth,
Information science,
Electrical capacitance tomography,
Electronic mail"
Conflict detection and planar resolution for air traffic control,"An algorithm for planar (i.e. no altitude change) conflict resolution in air traffic control (ATC) is presented. By using the point-line duality in 2D of the parallel coordinates methodology, and a field of particles incorporating the motion constraints associated with the aircraft, a ""map"" in time-and-space of the conflict as well as the conflict-free regions is obtained. Then the algorithm constructs maneuvers, satisfying the constraints, which move the aircraft in conflict to the nearest available conflict-free trajectories. In the process, it is ensured that the maneuvers do not generate new conflicts. The resolution problem, being in general NP-hard, may require very high complexity even in real situations. The need for real-time solutions is handled by cascading (or running in parallel) the algorithm in levels of increasing complexity O(qN/sup (p+1)/logN), where q and N are the number of allowable maneuvers and the number of aircraft respectively, and where the minimum p=1, 2, ..., R needed to resolve a complex conflict scenario is a measure of the scenarios' (i.e. input) complexity. As an illustration, a complex conflict scenario is resolved with complexity O(N/sup 2/logN). The resolution problem is in a sense ""dual"" to the interception problem. An example is given where the aforementioned time-space map provides specific solutions of the One-Shot problem, which is in general NP-complete. That is ""shots"", if they exist, are found which intercept all the aircraft.","Air traffic control,
Protection,
Aircraft manufacture,
USA Councils,
Traffic control,
Computer science,
Change detection algorithms,
Safety,
Shape,
Trajectory"
Building intelligent systems for mining information extraction rules from web pages by using domain knowledge,"Previous research on automatic information extraction experienced difficulties in acquiting and representing useful domain knowledge and in coping with the structural heterogeneity among different information sources. As a result, many real-world information sources with complex document structures could not be correctly analyzed. In order to resolve these problems, this paper presents a method of building intelligent systems for mining information extraction rules from semi-structured Web pages by using domain knowledge. This system automatically generates a wrapper for each information source and performs information extraction and information integration by applying this wrapper to the corresponding source. Both the domain knowledge and the wrapper are represented by ML documents to increase flexibility and interoperability. By testing our prototype system on several real-estate information sites, we can claim that it creates the correct wrappers for most Web sources and consequently facilitates effective information extraction for heterogeneous information sources.","Intelligent systems,
Intelligent structures,
Data mining,
Web pages,
XML,
HTML,
Human computer interaction,
Computer science,
Knowledge engineering,
Information analysis"
Length-restricted coding using modified probability distributions,"The use of data compression has long been a central part of text databases and fast communication protocols. In many contexts, effective compression techniques use a minimum-redundancy prefix code. However, if the length of a codeword exceeds the machine word size, the decoding routines must be altered and lose efficiency. To avoid these complications, it is desirable to produce a prefix code with the constraint that no codeword should be longer than some constant. L.L. Larmore and D.S. Hirschberg's (1990) package-merge algorithm is a well-known method for producing minimum-redundancy length-restricted prefix codes, although other methods exist. In this paper, we present an alternative method for length-restricted coding which calculates an approximate code, rather than an optimal code, but which can be implemented to operate in linear time. This approach also has applications to non-length-restricted coding.","Decoding,
Data compression,
Databases,
Protocols,
Context,
Computer science,
Software engineering,
Packaging machines,
Arithmetic,
Table lookup"
Task assignment heuristics for distributed CFD applications,"This paper proposes a task graph (TG) model to represent a single discrete step of a computational fluid dynamics (CFD) application. The TG model is then used to predict the performance of a set of task assignment heuristics developed based on the constraints inherent in the CFD problem. Two primitive assignments, the Largest Task First and the Minimum Task First, are systematically enhanced by the integration of the status of the processing units and communication costs. Evaluation is performed on a synthetic TG with tasks defined in terms of the number of gridpoints in predetermined grid zones. A realistic problem with eight million gridpoints is also used as a test case.","Computational fluid dynamics,
Costs,
Computational modeling,
Application software,
Aerodynamics,
Grid computing,
Mesh generation,
Computer science,
Computer applications,
Predictive models"
Parallel standard cell placement on a cluster of workstations,,"Workstations,
Clustering algorithms,
Circuits,
Iterative algorithms,
Design automation,
Parallel processing,
Iterative methods,
Design optimization,
Very large scale integration,
Space technology"
A comparison of certain quasi-velocities approaches in PD joint space control,"This paper presents a comparison of PD controls in joints space for serial manipulators whose dynamics is expressed in terms of two different kinds of quasi-velocities. Robot dynamic algorithms in terms of so called normalized and unnormalized quasi-velocities are recursive in nature and consists of two recursions: one starts from a base of the manipulator towards its tip, and the other in an opposite direction. Both recursions are described by using a vector-matrix notation. An instantaneous eigenstructure quasi-velocity formulation, as a result of numerical mass matrix factorization, is introduced. The two PD controls were tested on the model of a manipulator with two degrees of freedom. The standard control algorithms known in robotic literature are considered for a case when the robot dynamics is formulated in terms of quasi-velocities.","PD control,
Manipulator dynamics,
Space technology,
Orbital robotics,
Robot control,
Computer science,
Heuristic algorithms,
Testing,
Differential equations,
Couplings"
Coupling computer-supported co-operative work- and hypermedia technology for distance education solutions,"Searching for highly interactive and cooperative technologies in the field of distance learning has led to the development of a novel solution. It overcomes existing deficiencies with respect to context-sensitive and individualized interaction in shared learning spaces among learners and teachers. In this contribution we present the results and benefits of integrating computer-supported-cooperative-work- and hypermedia technology at the methodological and conceptual layer, and, based on that, at the implementation level. The application of concepts, such as profiling, has led to several positive effects in practice. We also report on the problems when providing the proper technology support. However, first results from evaluation indicate an increase in usability with respect to the acceptance of electronic media for interactive learning support in distributed environments.","Educational technology,
Distance learning,
Knowledge transfer,
Space technology,
Computer aided instruction,
Collaboration,
Collaborative work,
Usability,
Employee welfare,
Business communication"
Integrating adaptive mutations and family competition with differential evolution for flexible ligand docking,A flexible ligand docking protocol based on evolutionary algorithms is investigated. The proposed approach integrates decreasing-based mutations and self-adaptive mutations with differential evolution. This approach possesses global and local search strategies to balance the trade-off between exploitation and exploration of the search. The proposed approach is applied to a dihydrofolate reductase enzyme with the anti-cancer drug methotrexate and two analogues of antibacterial drug trimethoprim. Numerical results indicate that the new approach is very robust.,
Improving the performance of TCP Vegas in a heterogeneous environment,"Many results indicate that TCP Vegas exhibits better throughput and higher stability than TCP Reno in homogeneous cases where a single version exists, but it performs poorly in heterogeneous cases where two versions coexist. Hence users are delaying, even protesting, the adoption of TCP Vegas. The difference in performance is due to the fact that Reno vibrates in the high throughput level and Vegas almost stabilizes in the low level. We propose two simple approaches to address this problem, the RED (random early discard) approach and parameter adjustments to Vegas. These approaches enable Vegas to achieve its fair share of bandwidth, even gaining an advantage over Reno, encouraging users to switch their TCP from Reno to Vegas.","Bandwidth,
Throughput,
Switches,
Computer science,
Internet,
Protocols,
Propagation delay"
Multimedia design and development for distance teaching of electronics,"Distance teaching and learning involve some definite problems. One of them has to deal with the loneliness that the student faces and that he has to assume the whole responsibility of the learning process. This requires considerable motivation and a strong tendency to exert himself. The CAEE workgroup in the Electric Engineering and Computer Department of the Spanish University for Distance Education is working to design and develop multimedia applications for distance teaching of electronics engineering and checking its appropriateness. The main target of this work is to profit from the advantages of the multimedia applications to the transfer of information and to the learning process. It is verified that a correctly designed multimedia application produces reinforcement, and a greater and better assimilation in the learning process. Therefore, in order to ensure the correct profit of the multimedia applications advantages, developers must be very careful in using the right methodology.",
