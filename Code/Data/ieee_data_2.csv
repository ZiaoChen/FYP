Title,Abstract,Keywords
Learning to Learn: Dynamic Runtime Exploitation of Various Knowledge Sources and Machine Learning Paradigms,"The ability to learn at runtime is a fundamental prerequisite for self-adaptive and self-organising systems that allows for dealing with unanticipated conditions and dynamic environments. Often, this machine learning process has to be highly or fully autonomous. That is, the degree of interaction with humans must be reduced to a minimum. In principle, there exist various learning paradigms for this task such as transductive learning, reinforcement learning, collaborative learning, or - if interaction with humans is allowed but has to be efficient - active learning. These paradigms are based on different knowledge sources such as appropriate sensor measurements, humans, or databases as well as access models considering e.g., availability or reliability. In this article, we propose a novel meta learning approach that aims at dynamically exploiting various possible combinations of knowledge sources and machine learning paradigms at runtime. The approach is learning in the sense that it self-optimises a certain objective function (e.g., it maximises a classification accuracy) at runtime. We present an architectural concept for this learning scheme, discuss some possible use cases to highlight the benefits, and derive a research agenda for future work in this field.","Learning systems,
Runtime,
Adaptation models,
Learning (artificial intelligence),
Conferences,
Internet"
High Performance Machine Learning (HPML) Framework to Support DDDAS Decision Support Systems: Design Overview,"This paper presents a design for a High Performance Machine Learning (HPML) framework to support DDDAS decision processes. The HPML framework can provide a high performance computing environment to implement large scale machine learning algorithms that leverages Big Data tools (e.g., SPARK, Hadoop), parallel algorithms, and MapReduce programming paradigm. The framework provides the following capabilities: • High Performance Parallel Algorithms: For a suite of important ML, we will develop three parallel implementations of each algorithm that are based on Message Passing Interface (MPI), Shared Memory (SM) and MapReduce programming model. • High Performance and Scalable Platforms: This will enable us to identify the best high performance platform that maximizes performance and scalability of the parallel ML methods. We will experiment with and evaluate the performance and scalability of different parallel architectures (shared memory and message passing), Clusters of GPUs, and cloud computing systems. By leveraging the emerging Big Data tools and high performance computing algorithms (traditional and emerging paradigm such as MapReduce), we will be able to achieve the following: 1) reduce significantly the ML processing time, 2) enable StreamlinedML users to leverage Big Data tools to perform large scale ML tasks over structured and non-structured data sets; and 3) enable users to identify the best parallel platform and storage allocation and distribution that maximize performance and scalability of the selected ML algorithms.""","Machine learning algorithms,
Computational modeling,
Scalability,
Programming,
Data models,
Classification algorithms"
A Host-Independent Supervised Machine Learning Approach to Automated Overload Detection in Virtual Machine Workloads,"This paper evaluates a mechanism for applying machine learning (ML) to identify over-constrained IaaS virtual machines (VMs). Herein, over-constrained VMs are defined as those who are not given sufficient system resources to meet their workload specific objective functions. To validate our approach, a variety of workload-specific benchmarks inspired by common Infrastructure-as-a-Service (IaaS) cloud workloads were used. Workloads were run while regularly sampling VM resource consumption features exposed by the hypervisor. Datasets were curated into nominal or over-constrained and used to train ML classifiers to determine VM over-constraint rules based on one-time workload analysis. Rules learned on one host are transferred with the VM to other host environments to determine portability. Key contributions of this work include: demonstrating which VM resource consumption metrics (features) prove most relevant to learned decision trees in this context, and a technique required to generalize this approach across hosts while limiting required up front training expenditure to a single VM and host. Other contributions include a rigorous explanation of the differences in learned rulesets as a function of feature sampling rates, and an analysis of the differences in learned rulesets as a function of workload variation. Feature correlation matrices and their corresponding generated rule sets demonstrate individual features comprising rule sets tend to show low cross-correlation (below 0.4) while no individual feature shows high direct correlation with classification. Our system achieves workload-specific error percentages below 2.4% with a mean error across workloads of 1.43% (and strong false negative bias) for a variety of synthetic, representative, cloud workloads tested.","Monitoring,
Virtual machine monitors,
Kernel,
Databases,
Conferences,
Virtual machining,
Cloud computing"
A Deep Machine Learning Method for Classifying Cyclic Time Series of Biological Signals Using Time-Growing Neural Network,"This paper presents a novel method for learning the cyclic contents of stochastic time series: the deep time-growing neural network (DTGNN). The DTGNN combines supervised and unsupervised methods in different levels of learning for an enhanced performance. It is employed by a multiscale learning structure to classify cyclic time series (CTS), in which the dynamic contents of the time series are preserved in an efficient manner. This paper suggests a systematic procedure for finding the design parameter of the classification method for a one-versus-multiple class application. A novel validation method is also suggested for evaluating the structural risk, both in a quantitative and a qualitative manner. The effect of the DTGNN on the performance of the classifier is statistically validated through the repeated random subsampling using different sets of CTS, from different medical applications. The validation involves four medical databases, comprised of 108 recordings of the electroencephalogram signal, 90 recordings of the electromyogram signal, 130 recordings of the heart sound signal, and 50 recordings of the respiratory sound signal. Results of the statistical validations show that the DTGNN significantly improves the performance of the classification and also exhibits an optimal structural risk.","Hidden Markov models,
Time series analysis,
Biological system modeling,
Neural networks,
Phonocardiography,
Medical services,
Brain modeling"
Knowledge Transfer Through Machine Learning in Aircraft Design,"The modern aircraft has evolved to become an important part of our society. Its design is multidisciplinary in nature and is characterized by complex analyses of mutually interdependent disciplines and large search spaces. Machine learning has, historically, played a significant role in aircraft design, primarily by approximating expensive physics-based numerical simulations. In this work, we summarize the current role of machine learning in this application domain, and highlight the opportunity of incorporating recent advances in the field to further its impact. Specifically, regression models (or surrogate models) that represent a major portion of the current efforts are generally built from scratch assuming a zero prior knowledge state, only relying on data from the ongoing target problem of interest. However, due to the incremental nature of design processes, there likely exists relevant knowledge from various related sources that can potentially be leveraged. As such, we present three relatively advanced machine learning technologies that facilitate automatic knowledge transfer in order to improve design performance. Subsequently, we demonstrate the efficacy of one of these technologies, i.e. transfer learning, on two use cases of aircraft engine design yielding noteworthy results. Our aim is to unveil this new application as a well-suited arena for the salient features of knowledge transfer in machine learning to come to the fore, thereby encouraging future research efforts.","Computational modeling,
Numerical models,
Aircraft,
Mathematical model,
Atmospheric modeling,
Data models,
Numerical simulation"
Supervised Machine Learning for Estimation of Target Aspect Angle From Bistatic Acoustic Scattering,"When an aspect-dependent target is insonified by an acoustic source, distinct features are produced in the resulting bistatic scattered field. These features change as the aspect between the source and the target is varied. This paper describes the use of these features for estimation of the target aspect angle using data collected by an autonomous underwater vehicle (AUV). An experiment was conducted in November 2014 in Massachusetts Bay to collect data using a ship-based acoustic source producing 7–9-kHz linear frequency modulation (LFM) chirps insonifying a steel pipe. The true target orientation was unknown, as the target was dropped from the ship with no rotation control. The AUV Unicorn, fitted with a 16-element nose array, was deployed in data collection behaviors around the target, and the ship was moved to create two target aspects. A support vector machine regression model was trained using simulated scattering bistatic field data. This model was then used to estimate the target aspect angle from the data collected during the experiment. The difference between the estimates was consistent with experimental observations of relative source positioning. The simulation-based model appeared successful in estimating the target aspect angle despite uncertainties in target and source location and mismatch between true environment and simulation parameters.","Scattering,
Underwater acoustics,
Marine vehicles,
Aquatic robots,
Machine learning,
Underwater autonomous vehicles"
Soil Moisture Retrieval using UWB Echoes via Fuzzy Logic and Machine Learning,"Soil moisture (SM) retrieval using wireless signals has become a research focus with the development of sensor devices in internet of things (IOT). Studies applying ground-penetrating radar (GPR) have improved the accuracy of SM retrieval, however the field-scaled data are hardly satisfactory mainly due to the frequency response of the antenna, and it’s not cost-effective for farmers to monitor the soil conditions. In this paper, we compare two fuzzy logic systems (FLS) -type-1 fuzzy logic system (T1FLS) and adaptive network-based fuzzy inference system (ANFIS) to extract fuzzy parameters of soil. Moreover, two machine learning (ML) algorithms - random forest (RF) and artificial neural network (ANN) with principal component analysis (PCA) are applied in the SM classifications. 9 types of UWB soil echoes of different texture and volume water content (VWC) are collected and investigated using our approaches. Final analysis shows that ANFIS with RF provides the best VWC correct recognition rate (CRR) compared to other algorithms.","Soil,
Fuzzy logic,
Feature extraction,
Soil measurements,
Radio frequency,
Internet of Things,
Ground penetrating radar"
Integrating Heuristic and Machine-Learning Methods for Efficient Virtual Machine Allocation in Data Centers,"Modern cloud data centers (DCs) need to tackle efficiently the increasing demand for computing resources and address the energy efficiency challenge. Therefore, it is essential to develop resource provisioning policies that are aware of virtual machine (VM) characteristics, such as CPU utilization and data communication, and applicable in dynamic scenarios. Traditional approaches fall short in terms of flexibility and applicability for large-scale DC scenarios. In this paper we propose a heuristic-and a machine learning (ML)-based VM allocation method and compare them in terms of energy, quality of service (QoS), network traffic, migrations, and scalability for various DC scenarios. Then, we present a novel hyper-heuristic algorithm that exploits the benefits of both methods by dynamically finding the best algorithm, according to a user-defined metric. For optimality assessment, we formulate an integer linear programming (ILP)-based VM allocation method to minimize energy consumption and data communication, which obtains optimal results, but is impractical at runtime. Our results demonstrate that the ML approach provides up to 24% server-to-server network traffic improvement and reduces execution time by up to 480x compared to conventional approaches, for large-scale scenarios. On the contrary, the heuristic outperforms the ML method in terms of energy and network traffic for reduced scenarios. We also show that the heuristic and ML approaches have up to 6% energy consumption overhead compared to ILP-based optimal solution. Our hyper-heuristic integrates the strengths of both the heuristic and the ML methods by selecting the best one during runtime.","Servers,
Resource management,
Energy consumption,
Correlation,
Data communication,
Quality of service"
Imbalance Learning Machine-Based Power System Short-Term Voltage Stability Assessment,"In terms of machine learning-based power system dynamic stability assessment, it is feasible to collect learning data from massive synchrophasor measurements in practice. However, the fact that instability events rarely occur would lead to a challenging class imbalance problem. Besides, short-term feature extraction from scarce instability seems extremely difficult for conventional learning machines. Faced with such a dilemma, this paper develops a systematic imbalance learning machine for online short-term voltage stability assessment. A powerful time series shapelet (discriminative subsequence) classification method is embedded into the machine for sequential transient feature mining. A forecasting-based nonlinear synthetic minority oversampling technique is proposed to mitigate the distortion of class distribution. Cost-sensitive learning is employed to intensify bias toward those scarce yet valuable unstable cases. Furthermore, an incremental learning strategy is put forward for online monitoring, contributing to adaptability and reliability enhancement along with time. Simulation results on the Nordic test system illustrate the high performance of the proposed learning machine and of the assessment scheme.","Power system stability,
Voltage measurement,
Stability criteria,
Power system dynamics,
Thermal stability,
Feature extraction"
Predicting radiation protection and toxicity of p53 targeting radioprotectors using machine learning,"This paper explores machine learning application in the case of drug discovery. We apply extreme gradient boosting and K-nearest neighbor to biomedical data and it signiflcantly outperform former studies using feature selection and proper tuning parameters. The novel application motivated by a recent circumstance that there is a need for rapid development of radio-protectors. It mainly targets the DNA of growing cancer cells, whereas it has adverse side effects, including p53-induced apoptosis of normal tissues and cells. It considered that p53 would be a target for therapeutic and mitigated radioprotection to escape from the apoptotic fate. On the other hand, many types of compounds contain several level of toxicity, so it is important to consider not only radiation protection but also the level of toxicity of candidate compounds for radioprotectors. Compounds of radio-protectors that have low toxicity and high radiation protection are expected. It is possible to do efficiently the compounds discovery using machine learning. This study predicts compounds of radioprotectors using plural machine learning methods, Extreme Gradient Boosting, K-nearest neighbor, SVM and Random Forest. We compare these methods and suggest proper methods to predict radioprotectors.","Compounds,
Boosting,
Support vector machines,
Decision trees,
Drugs,
Cancer"
Android mobile security by detecting and classification of malware based on permissions using machine learning algorithms,"Android occupies a major share in the mobile application market. Android mobiles have become an easy target for the attackers. The main reason is the user ignorance in the process of installing and usage of the apps. Android malware can be detected based on the permissions it requests from the user. Several machine learning algorithms are being used in the detection of android malware based on the list of permissions enabled for each app. This paper makes an attempt to study the performance of some of the machine learning algorithms, viz., naïve Bayes, J48, Random Forest, Multi-class classifier and Multi-layer perceptron. Google play store 2015 and 2016 app data are used for normal apps and standard malware data sets are used in the evaluation. Multi-class classifier was found to be outperforming the other algorithms in terms of classification accuracy. Naïve Bayes classifier has outperformed as far as model construction time is concerned.","Malware,
Androids,
Humanoid robots,
Machine learning algorithms,
Mobile communication,
Feature extraction,
Data mining"
Efficient extreme learning machine with transfer functions for filter design,"This paper proposes a model based on a machine learning algorithm, extreme learning machine (ELM), and the pole-residue-based transfer function (TF) for parametric modeling of electromagnetic behavior of microwave components. Compared with the model based on the artificial neural network, the proposed ELM model can obtain the accurate results for microwave passive component design with the small training datasets due to its good iterative learning ability. The validity and efficiency of this proposed model is confirmed by a triple-mode filter.","Data models,
Training,
Microwave filters,
Optimization,
Testing,
Transfer functions,
Computational modeling"
Predictive analysis of diabetic patient data using machine learning and Hadoop,"Now days from health care industries large volume of data is generating. It is necessary to collect, store and process this data to discover knowledge from it and utilize it to take significant decisions. Diabetic Mellitus (DM) is from the Non Communicable Diseases (NCD), and lots of people are suffering from it. Now days, for developing countries such as India, DM has become a big health issue. The DM is one of the critical diseases which has long term complications associated with it and also follows with various health problems. With the help of technology, it is necessary to build a system that store and analyze the diabetic data and predict possible risks accordingly. Predictive analysis is a method that integrates various data mining techniques, machine learning algorithms and statistics that use current and past data sets to gain insight and predict future risks. In this work machine learning algorithm in Hadoop MapReduce environment are implemented for Pima Indian diabetes data set to find out missing values in it and to discover patterns from it. This work will be able to predict types of diabetes are widespread, related future risks and according to the risk level of patient the type of treatment can be provided.","Diabetes,
Classification algorithms,
Algorithm design and analysis,
Data mining,
Clustering algorithms,
Unsupervised learning"
AccurateML: Information-aggregation-based approximate processing for fast and accurate machine learning on MapReduce,"The growing demands of processing massive datasets have promoted irresistible trends of running machine learning applications on MapReduce. When processing large input data, it is often of greater values to produce fast and accurate enough approximate results than slow exact results. Existing techniques produce approximate results by processing parts of the input data, thus incurring large accuracy losses when using short job execution times, because all the skipped input data potentially contributes to result accuracy. We address this limitation by proposing AccurateML that aggregates information of input data in each map task to create small aggregated data points. These aggregated points enable all map tasks producing initial outputs quickly to save computation times and decrease the outputs' size to reduce communication times. Our approach further identifies the parts of input data most related to result accuracy, thus first using these parts to improve the produced outputs to minimize accuracy losses. We evaluated AccurateML using real machine learning applications and datasets. The results show: (i) it reduces execution times by 30 times with small accuracy losses compared to exact results; (ii) when using the same execution times, it achieves 2.71 times reductions in accuracy losses compared to existing approximate processing techniques.","Sparks,
Correlation,
Clustering algorithms,
Conferences,
Indexes,
Aggregates"
A CRF Based Machine Learning Approach for Biomedical Named Entity Recognition,"The amount of biomedical textual information available in the web becomes more and more. It is very difficult to extract the right information that users are interested in considering the size of documents in the biomedical literatures and databases. It is nearly impossible for human to process all these data and it is even difficult for computers to extract the information since it is not stored in structured format. Identifying the named entities and classifying them can help in extracting the useful information in the unstructured text documents. This paper presents a new method of utilizing biomedical knowledge by both exact matching of disease dictionary and adding semantic concept feature through UMLS semantic type filtering, in order to improve the human disease named entity recognition by machine learning. By engineering the concept semantic type into feature set, we demonstrate the importance of domain knowledge on machine learning based disease NER. The background knowledge enriches the representation of named entity and helps to disambiguate terms in the context thereby improves the overall NER performance.","Proteins,
Protein engineering,
Semantics,
Diseases,
Databases,
Dictionaries"
Analysis of banking data using machine learning,"In today's world large amount of data is generated in every field and banking industry is one of them. This data contains valuable information. Hence, it is very important to store, process, manage and analyze this data to extract knowledge from it. It helps to increase business profit. Banking industry plays very important role in economy of country. Customers are the main asset of the bank. Hence it is necessary to focus on problems faced by the banks. Here, we are working on customer retention and fraud detection. In this work, supervised artificial neural network algorithm is implemented for classification purpose.","Artificial neural networks,
Training,
Testing,
Machine learning algorithms,
Classification algorithms,
Data models,
Banking"
Prediction of User's Purchase Intention Based on Machine Learning,"In recent years, the use of machine learning methods to deal with the problem of user interest prediction has become a hot research direction in the field of electronic commerce. In the present stage, a naive Bayesian algorithm has the advantages of simple implementation and high classification efficiency. However, this method is too dependent on the distribution of samples in the sample space, and has the potential of instability. To this end, the decision tree method is introduced to deal with the problem of interest classification, and the innovative use of Localstorage technology in HTML5 to obtain the required the experimental data. Classification method uses the information entropy of the training data set to build the classification model, through the simple search of the classification model to complete the classification of unknown data items. Both theoretical analysis and experimental results show that the decision tree is used to deal with the problem of prediction of users' interests has obvious advantages in the efficiency and stability.","Decision trees,
Algorithm design and analysis,
Niobium,
Classification algorithms,
Data models,
Probability,
Tools"
Optimise web browsing on heterogeneous mobile platforms: A machine learning based approach,"Web browsing is an activity that billions of mobile users perform on a daily basis. Battery life is a primary concern to many mobile users who often find their phone has died at most inconvenient times. The heterogeneous multi-core architecture is a solution for energy-efficient processing. However, the current mobile web browsers rely on the operating system to exploit the underlying hardware, which has no knowledge of individual web contents and often leads to poor energy efficiency. This paper describes an automatic approach to render mobile web workloads for performance and energy efficiency. It achieves this by developing a machine learning based approach to predict which processor to use to run the web rendering engine and at what frequencies the processors should operate. Our predictor learns offline from a set of training web workloads. The built predictor is then integrated into the browser to predict the optimal processor configuration at runtime, taking into account the web workload characteristics and the optimisation goal: whether it is load time, energy consumption or a trade-off between them. We evaluate our approach on a representative ARM big.LITTLE mobile architecture using the hottest 500 webpages. Our approach achieves 80% of the performance delivered by an ideal predictor. We obtain, on average, 45%, 63.5% and 81% improvement respectively for load time, energy consumption and the energy delay product, when compared to the Linux heterogeneous multi-processing scheduler.","Mobile communication,
Rendering (computer graphics),
Optimization,
Energy consumption,
Browsers,
Measurement,
Feature extraction"
Cost estimation of civil construction projects using machine learning paradigm,"Adequate construction cost estimation is main factor in any type of construction projects. Forecasting cost of construction projects can be considered as difficult task. In this paper, Ordinary Least Square (OLS) method which is type ofa simple linear regression model is proposed to forecast iuture cost of construction projects. Ordinary Least Square method can be used to produce best solution and performs well when the dataset is small. Experiments were performed with 12 years of District Schedule Rates of Pune region from India to find out the accuracy of the model. The results show that proposed model gives 91% to 97% prediction accuracy.","Schedules,
Estimation,
Concrete,
Linear regression,
Predictive models,
Neural networks,
Fuzzy logic"
Analyzing road accident data using machine learning paradigms,"To determine the main factors associated with road traffic accidents is one of main objectives of accident data analysis. Due heterogeneity nature of road accident data makes analysis tricky. To overcome heterogeneity of data partitioning is used. The proposed method uses k-means clustering method as the main task of segmentation of road accident data. Further, association rule mining is applied to discover the situations related with the occurrence of the whole data set and the occurrence of clusters recognized by the k-means clustering algorithm. The combined result of k-means clustering and association rule produces major information.","Road accidents,
Roads,
Clustering algorithms,
Data mining,
Injuries"
Incremental and Decremental Extreme Learning Machine based on Generalized Inverse,"In online sequential applications, a machine learning model needs to have a self-updating ability to handle the situation, that the training set is changing. Conventional incremental extreme learning machine (ELM) and online sequential ELM are usually achieved in two approaches: directly updating the output weight and recursively computing the left pseudo inverse of the hidden layer output matrix. In this paper, we develop a novel solution for incremental and decremental ELM, via recursively updating and downdating the generalized inverse of the hidden layer output matrix. By preserving the global optimality and best generalization performance, our approach implements node incremental ELM (N-IELM) and sample incremental ELM (S-IELM) in a universal form, and overcomes the problem of self-starting and numerical instability in the conventional online sequential ELM. We also propose sample decremental ELM (S-DELM), which is the first decremental version of ELM. The experiments on regression and classification problems with real-world datasets demonstrate the feasibility and effectiveness of the proposed algorithms with encouraging performances.","Training,
Approximation error,
Machine learning algorithms,
Neurons,
Computational complexity,
Computational modeling,
Classification algorithms"
Joint Machine Learning and Game Theory for Rate Control in High Efficiency Video Coding,"In this paper, a joint machine learning and game theory modeling (MLGT) framework is proposed for inter frame coding tree unit (CTU) level bit allocation and rate control (RC) optimization in high efficiency video coding (HEVC). First, a support vector machine-based multi-classification scheme is proposed to improve the prediction accuracy of CTU-level rate-distortion (R-D) model. The legacy “chicken-and-egg” dilemma in video coding is proposed to be overcome by the learning-based R-D model. Second, a mixed R-D model-based cooperative bargaining game theory is proposed for bit allocation optimization, where the convexity of the mixed R-D model-based utility function is proved, and Nash bargaining solution is achieved by the proposed iterative solution search method. The minimum utility is adjusted by the reference coding distortion and frame-level quantization parameter (QP) change. Finally, intra frame QP and inter frame adaptive bit ratios are adjusted to make inter frames have more bit resources to maintain smooth quality and bit consumption in the bargaining game optimization. Experimental results demonstrate that the proposed MLGT-based RC method can achieve much better R-D performances, quality smoothness, bit rate accuracy, buffer control results, and subjective visual quality than the other state-of-the-art one-pass RC methods, and the achieved R-D performances are very close to the performance limits from the FixedQP method.","Optimization,
Bit rate,
Predictive models,
Game theory,
Video coding,
Encoding,
Image coding"
Implement of a 6-DOF manipulator with machine vision and machine learning algorithms,This paper explores the application of a Machine Vision and Machine Learning Algorithm to a Manipulator with six degrees of freedom (6-DOF). A Kinect sensor were used to extract images from a screen and obtain the relevant target information. Image processing was accomplished using a Scale-invariant feature transform (SIFT) Algorithm to capture image of the target object. The processed visual is rendered on the computer controller and Manipulator Learning is accomplished using a Reinforcement Learning Algorithm. Markov Decision Processes (MDP) were used to train the Manipulator to move to the location of the target object. Experimental results showed the Reinforcement Learning Algorithm proposed in this paper is effective and can be utilized on a 6-DOF Manipulator with reproducible results.,"Manipulators,
Feature extraction,
Learning (artificial intelligence),
Machine learning algorithms,
Machine vision,
Servosystems"
A multi-key compressed sensing and machine learning privacy preserving computing scheme,"Recently there has been a huge interest in secure and private cloud computing. In particular; to perform signal processing and machine learning tasks in the encrypted domain. Homomorphic encryption offers provably secure, asymmetric encryption solution to this problem, however, it comes with a high storage and computation cost. Compressed sensing (CS) and random projection (RP) approaches are much lighter; however, they lack privacy since the encryption uses a symmetric key which is the random projection matrix. A multi-key, compressed sensing encryption approach is proposed in this paper for performing basic generic computations. The computing architecture consists of a User, a Cloud (which stores encrypted data from an Owner), and a Trusted Third Party (TTP) which is responsible for distributing the random CS keys. The TTP also trains two machine learning modules; ML1 and ML2. ML1, used at the cloud, takes as input the multi-key encrypted data and produces an intermediate encrypted result. ML2, available at the user side, decrypts the results. This novel approach is much cheaper than homomorphic encryption in terms of data expansion, storage as well as encryption time. Also, it offers the privacy of the multi-keys. The proposed approach is applied on 2 generic computing tasks; namely, squared Euclidean distance and dot product. The developed approach is tested on the COREL image classification task using the squared Euclidean distance and on an autoregressive (AR) stock prediction task using the dot product.","Encryption,
Training,
Cloud computing,
Privacy,
Euclidean distance"
Machine learning techniques applied to prepaid subscribers: Case study on the telecom industry of Morocco,"In order to survive the fierce competition in today's telecommunication industry, it is mandatory to understand the need of customers who might think to move toward another competitor. Thus, assessing the churn prediction, which becomes a real concern in the telecom industry, is critical in predicting future trends of the industry. In this work, we wanted to determine the best and reliable prediction model in improving the competitiveness of Moroccan telecommunication sector. We focused on prepaid costumers, a group of subscribers that are unreliable, less committed to a provider, and they can end their services without prior notices. We used various machine learning techniques applied to Call Details Records (CDR) to analyze a dataset of 635997 subscribers and monitored their usage behaviors over a period of three months. The Visitor Location Register ID, which describes the locations where calls/SMS/Recharge and Data events were performed was also used to investigate the churning rate per different regions. Among models tested, we found that the Gradient Boosting Classifier (GBC) is the best classifier model with accuracy of 91% and f1 score of 89%. This model fits better the Churn prediction use case.","Telecommunications,
Predictive models,
Industries,
Classification algorithms,
Prediction algorithms,
Feature extraction,
Boosting"
A robust calibration and supervised machine learning reliability framework for digitally-assisted self-healing RFICs,"A robust calibration and supervised machine learning reliability framework has been developed to aid the circuit designer in the design and implementation of reliable digitally-reconfigurable self-healing RFICs. For calibration algorithm performance and reliability validation, we advocate the use of surrogate modeling, a supervised machine learning technique, which offers a significant reduction in the required computational complexity relative to relying solely on the execution of expensive circuit simulations. An RF phase rotator test case is used to show the robustness and utility of the developed self-healing reliability framework.","Calibration,
Integrated circuit modeling,
Computational modeling,
Radiofrequency integrated circuits,
Reliability engineering,
Algorithm design and analysis"
Facial recognition with PCA and machine learning methods,"Facial recognition is a challenging problem in image processing and machine learning areas. Since widespread applications of facial recognition make it a valuable research topic, this work tries to develop some new facial recognition systems that have both high recognition accuracy and fast running speed. Efforts are made to design facial recognition systems by combining different algorithms. Comparisons and evaluations of recognition accuracy and running speed show that PCA + SVM achieves the best recognition result, which is over 95% for certain training data and eigenface sizes. Also, PCA + KNN achieves the balance between recognition accuracy and running speed.","Support vector machines,
Face recognition,
Training data,
Face,
Principal component analysis,
Databases,
Testing"
From User Demand to Software Service: Using Machine Learning to Automate the Requirements Specification Process,"Bridging the gap between informal, imprecise, and vague user requirements descriptions and precise formalized specifications is the main task of requirements engineering. Techniques such as interviews or story telling are used when requirements engineers try to identify a user's needs. The requirements specification process is typically done in a dialogue between users, domain experts, and requirements engineers. In our research, we aim at automating the specification of requirements. The idea is to distinguish between untrained users and trained users, and to exploit domain knowledge learned from previous runs of our system. We let untrained users provide unstructured natural language descriptions, while we allow trained users to provide examples of behavioral descriptions. In both cases, our goal is to synthesize formal requirements models similar to statecharts. From requirements specification processes with trained users, behavioral ontologies are learned which are later used to support the requirements specification process for untrained users. Our research method is original in combining natural language processing and search-based techniques for the synthesis of requirements specifications. Our work is embedded in a larger project that aims at automating the whole software development and deployment process in envisioned future software service markets.","Software,
Unified modeling language,
Requirements engineering,
Ontologies,
Search problems,
Natural languages"
Using Supervised Machine Learning to Automatically Build Relevance Judgments for a Test Collection,"This paper describes a new approach to building the query based relevance sets (qrels) or relevance judgments for a test collection automatically without using any human intervention. The methods we describe use supervised machine learning algorithms, namely the Naïve Bayes classifier and the Support Vector Machine (SVM). We achieve better Kendall's tau and Spearman correlation results between the TREC system ranking using the newly generated qrels and the ranking obtained from using the human-built qrels than previous baselines. We also apply a variation of these approaches by using the doc2vec representation of the documents rather than using the traditional tf-idf representation.","Correlation,
Niobium,
Support vector machines,
Training,
Buildings,
Machine learning algorithms,
Information retrieval"
The application of machine learning in RCS calculation for antenna-radome system,"In this paper, a method based on machine learning and modified physical optics method is proposed for calculating the radar cross section (RCS) of the antenna-radome system. The scattering mechanism of the antenna-random system is estimated by the Multilayer Perceptron (MLP) model and the measured data in the past is analyzed. The back-propagation algorithm is chosen to train the machine learning model. In the modified physical optics method, the FSS layer is equated as an anisotropic medium, the relationship of the frequency response of the FSS layer with the incident angle, polarization and frequency is estimated by the function fitting method to improve current calculation accuracy on the radome surface. Finally the results calculated by the method proposed in this paper are validated by a real antenna-radome system.","Frequency selective surfaces,
Antenna measurements,
Fitting,
Radar antennas,
Physical optics"
Activation-Kernel Extraction through Machine Learning,"Machine Learning flooded many research fields, including Electronic Design Automation (EDA). The availability of algorithms that can solve complex problems through generic rule formulations represent a fresh opportunity to improve existing design paradigms. In this work we investigate the use of machine learning to manipulate logic circuits. More specifically, we envision the use of Classification and Regression Trees (CARTs) as tools for modeling generic Boolean functions through a representative subset of core expressions, what we call the activation kernels. Experiments conducted on a subset of open-source benchmarks demonstrate that CARTs are indeed able to identify the activation kernels that cover whole Boolean functions with a high degree of accuracy (89% on average). In order to quantify other figures of merit, we also provide a physical implementation of such activation kernels. Results show that the obtained circuits are amazingly smaller than standard-cell based circuits synthesized through a classical logic synthesis flow (16X less devices on average).","Kernel,
Training,
Logic functions,
Input variables,
Tools,
Optimization"
A Machine Learning Approach towards Detecting Extreme Adopters in Digital Communities,"In this study we try to identify extreme adopters on a discussion forum using machine learning. An extreme adopter is a user that has adopted a high level of a community-specific jargon and therefore can be seen as a user that has a high degree of identification with the community. The dataset that we consider consists of a Swedish xenophobic discussion forum where we use a machine learning approach to identify extreme adopters using a number of linguistic features that are independent on the dataset and the community. The results indicates that it is possible to separate these extreme adopters from the rest of the discussants on the discussion forum with more than 80% accuracy. Since the linguistic features that we use are highly domain independent, the results indicates that there is a possibility to use this kind of techniques to identify extreme adopters within other communities as well.","Discussion forums,
Support vector machines,
Pragmatics,
Manuals,
Radio frequency,
Electronic mail,
Social network services"
Enhancing PUF reliability by machine learning,"Physical Unclonable Functions (PUFs) are promising security primitives for device authentication and key generation. This paper proposes a two-step methodology to improve the reliability of PUF under noisy conditions. The first step involves acquiring the parameters of PUF models by using machine learning algorithms. The second step then utilizes these obtained parameters to improve the reliability of PUFs by selectively choosing challenge-response pairs (CRPs) for authentication. Two distinct algorithms for improving the reliability of multiplexer (MUX) PUF, i.e., total delay difference thresholding and sensitive hits grouping, are presented. It is important to note that the methodology can be easily applied to other types of PUFs as well. Our experimental results show that the reliability of PUF-based authentication can be significantly improved by the proposed approaches. For example, in one experimental setting, the reliability of an MUX PUF is improved from 89.75% to 94.07% usmg total delay difference thresholding, while 89.30% of generated challenges are stored. As opposed to total delay difference thresholding, sensitive bits grouping possesses higher efficiency, as it can produce reliable CRPs directly. Our experimental results show that the reliability can be improved to 96.91% under the same setting, when we group 12 bits in the challenge vector of a 128-stage MUX PUF.",
On the Protection of Private Information in Machine Learning Systems: Two Recent Approches,"The recent, remarkable growth of machine learning has led to intense interest in the privacy of the data on which machine learning relies, and to new techniques for preserving privacy. However, older ideas about privacy may well remain valid and useful. This note reviews two recent works on privacy in the light of the wisdom of some of the early literature, in particular the principles distilled by Saltzer and Schroeder in the 1970s.","Privacy,
Training data,
Noise measurement,
Training,
Learning systems,
Data privacy,
Data models"
Detecting Stealthy False Data Injection Using Machine Learning in Smart Grid,"Aging power industries, together with the increase in demand from industrial and residential customers, are the main incentive for policy makers to define a road map to the next-generation power system called the smart grid. In the smart grid, the overall monitoring costs will be decreased, but at the same time, the risk of cyber attacks might be increased. Recently, a new type of attacks (called the stealth attack) has been introduced, which cannot be detected by the traditional bad data detection using state estimation. In this paper, we show how normal operations of power networks can be statistically distinguished from the case under stealthy attacks. We propose two machine-learning-based techniques for stealthy attack detection. The first method utilizes supervised learning over labeled data and trains a distributed support vector machine (SVM). The design of the distributed SVM is based on the alternating direction method of multipliers, which offers provable optimality and convergence rate. The second method requires no training data and detects the deviation in measurements. In both methods, principal component analysis is used to reduce the dimensionality of the data to be processed, which leads to lower computation complexities. The results of the proposed detection methods on IEEE standard test systems demonstrate the effectiveness of both schemes.","Smart grids,
Support vector machines,
Principal component analysis,
Transmission line measurements,
State estimation,
Data structures,
Boolean functions"
A Cloud System for Machine Learning Exploiting a Parallel Array DBMS,"Computing machine learning models in the cloud remains a central problem in big data analytics. In this work, we introduce a cloud analytic system exploiting a parallel array DBMS based on a classical shared-nothing architecture. Our approach combines in-DBMS data summarization with mathematical processing in an external program. We study how to summarize a data set in parallel assuming a large number of processing nodes and how to further accelerate it with GPUs. In contrast to most big data analytic systems, we do not use Java, HDFS, MapReduce or Spark: our system is programmed in C++ and C on top of a traditional Unix le system. In our system, models are ef ciently computed using a suite of innovative parallel matrix operators, which compute comprehensive statistical summaries of a large input data set (matrix) in one pass, leaving the remaining mathematically complex computations, with matrices that t in RAM, to R. In order to be competitive with the Hadoop ecosystem (i.e. HDFS and Spark RDDs) we also introduce a parallel load operator for large matrices and an automated, yet exible, cluster con guration in the cloud. Experiments compare our system with Spark, showing orders of magnitude time improvement. A GPU with many cores widens the gap further. In summary, our system is a competitive solution.","Arrays,
Computational modeling,
Loading,
Cloud computing,
Random access memory,
Data models,
Sparks"
Evaluating the effectiveness of a machine learning approach based on response time and reliability for islanding detection of distributed generation,"Conventional relays, such as vector surge relay, frequency relay and rate-of-change-of-frequency relay, are usually employed for islanding detection; however, these conventional relays fail to detect islanding incidents in the presence of small power imbalance inside the islanded system. This study presents an islanding detection approach for synchronous type distributed generation using multiple features extracted from network variables and a support vector machine (SVM) classifier. Features are extracted from a sliding temporal window, whose width is selected so as to achieve the highest detection rate at a fixed false alarm rate. The SVM classifier is trained with linear, polynomial and Gaussian radial basis function kernels, and the parameters of the kernels are tuned to improve the classification performance. The application of the proposed method is illustrated for islanding cases associated with different power imbalance conditions, including small power imbalance conditions associated with the non-detection zone of conventional relays. Furthermore, variation of detection time as a function of power imbalance scenarios, which involve all probable combinations of deficit of active/reactive and excess of active/reactive power imbalance, is assessed in the testing phase. The performance of the proposed approach is evaluated and compared with those of conventional relays in terms of reliability and response time of islanding detection.","distributed power generation,
feature extraction,
learning (artificial intelligence),
pattern classification,
power engineering computing,
power generation reliability,
radial basis function networks,
relay protection,
support vector machines"
Reducing circuit design complexity for neuromorphic machine learning systems based on Non-Volatile Memory arrays,"Machine Learning (ML) is an attractive application of Non-Volatile Memory (NVM) arrays [1,2]. However, achieving speedup over GPUs will require minimal neuron circuit sharing and thus highly area-efficient peripheral circuitry, so that ML reads and writes are massively parallel and time-multiplexing is minimized [2]. This means that neuron hardware offering full `software-equivalent' functionality is impractical. We analyze neuron circuit needs for implementing back-propagation in NVM arrays and introduce approximations to reduce design complexity and area. We discuss the interplay between circuits and NVM devices, such as the need for an occasional RESET step, the number of programming pulses to use, and the stochastic nature of NVM conductance change. In all cases we show that by leveraging the resilience of the algorithm to error, we can use practical circuit approaches yet maintain competitive test accuracies on ML benchmarks.","Neurons,
Training,
Nonvolatile memory,
Software,
Phase change materials,
Biological neural networks,
Complexity theory"
Event-driven random backpropagation: Enabling neuromorphic deep learning machines,"An ongoing challenge in neuromorphic computing is to devise general and computationally efficient models of inference and learning which are compatible with the spatial and temporal constraints of the brain. The gradient descent back-propagation rule is a powerful algorithm that is ubiquitous in deep learning, but it relies on the immediate availability of network-wide information stored with high-precision memory. However, recent work shows that exact backpropagated weights are not essential for learning deep representations. Here, we demonstrate an event-driven random backpropagation (eRBP) rule that uses an error-modulated synaptic plasticity rule for learning deep representations in neuromorphic computing hardware. The rule is very suitable for implementation in neuromorphic hardware using a two-compartment leaky integrate & fire neuron and a membrane-voltage modulated, spike-driven plasticity rule. Our results show that using eRBP, deep representations are rapidly learned without using backpropagated gradients, achieving nearly identical classification accuracies compared to artificial neural network simulations on GPUs, while being robust to neural and synaptic state quantizations during learning.","Neurons,
Biological neural networks,
Neuromorphics,
Hardware,
Training,
Machine learning,
Backpropagation"
Data Mining and Analytics in the Process Industry: the Role of Machine Learning,"Data mining and analytics have played an important role in knowledge discovery and decision making/supports in the process industry over the past several decades. As a computa-tional engine to data mining and analytics, machine learning serves as basic tools for information extraction, data pattern recognition and predictions. From the perspective of machine learning, this paper provides a review on existing data mining and analytics applications in the process industry over the past several decades. The state-of-the-art of data mining and analytics are reviewed through eight unsupervised learning and ten supervised learning algorithms, as well as the application status of semi-supervised learning algorithms. Several perspectives are highlighted and discussed for future researches on data mining and analytics in the process industry.","Data mining,
Industries,
Data models,
Machine learning algorithms,
Analytical models,
Manufacturing,
Predictive models"
Automatically Classifying Functional and Non-functional Requirements Using Supervised Machine Learning,"In this paper, we take up the second RE17 data challenge: the identification of requirements types using the ""Quality attributes (NFR)"" dataset provided. We studied how accurately we can automatically classify requirements as functional (FR) and non-functional (NFR) in the dataset with supervised machine learning. Furthermore, we assessed how accurately we can identify various types of NFRs, in particular usability, security, operational, and performance requirements. We developed and evaluated a supervised machine learning approach employing meta-data, lexical, and syntactical features. We employed under-and over-sampling strategies to handle the imbalanced classes in the dataset and cross-validated the classifiers using precision, recall, and F1 metrics in a series of experiments based on the Support Vector Machine classifier algorithm. We achieve a precision and recall up to ~92% for automatically identifying FRs and NFRs. For the identification of specific NFRs, we achieve the highest precision and recall for security and performance NFRs with ~92% precision and ~90% recall. We discuss the most discriminating features of FRs and NFRs as well as the sampling strategies used with an additional dataset and their impact on the classification accuracy.","Training,
Usability,
Security,
Feature extraction,
Vegetation,
Support vector machines"
MACORD: Online Adaptive Machine Learning Framework for Silent Error Detection,"Future high-performance computing (HPC) systems with ever-increasing resource capacity (such as compute cores, memory and storage) may significantly increase the risks on reliability. Silent data corruptions (SDCs) or silent errors are among the major sources that corrupt HPC execution results. Unlike fail-stop errors, SDCs can be harmful and dangerous in that they cannot be detected by hardware. To remedy this, we propose an online MAchine-learning-based silent data CORruption Detection framework (abbreviated as MACORD) for detecting SDCs in HPC applications. In our study, we comprehensively investigate the prediction ability of a multitude of machine-learning algorithms and enable the detector to automatically select the best-fit algorithms at runtime to adapt to the data dynamics. Because it takes only spatial features (i.e., neighboring data values for each data point in the current time step) into the training data, our learning framework exhibits low memory overhead (less than 1%). Experiments based on real-world scientific applications/benchmarks show that our framework can elevate the detection sensitivity (i.e., recall) up to 99%. Meanwhile the false positive rate is limited to 0.1% in most cases, which is one order of magnitude improvement compared with the latest state-of-the-art spatial technique.","Training,
Algorithm design and analysis,
Heuristic algorithms,
Vegetation,
Prediction algorithms,
Machine learning algorithms,
Mathematical model"
TGE: Machine Learning Based Task Graph Embedding for Large-Scale Topology Mapping,"Task mapping is an important problem in parallel and distributed computing. The goal in task mapping is to find an optimal layout of the processes of an application (or a task) onto a given network topology. We target this problem in the context of staging applications. A staging application consists of two or more parallel applications (also referred to as staging tasks) which run concurrently and exchange data over the course of computation. Task mapping becomes a more challenging problem in staging applications, because not only data is exchanged between the staging tasks, but also the processes of a staging task may exchange data with each other. We propose a novel method, called Task Graph Embedding (TGE), that harnesses the observable graph structures of parallel applications and network topologies. TGE employs a machine learning based algorithm to find the best representation of a graph, called an embedding, onto a space in which the task-to-processor mapping problem can be solved. We evaluate and demonstrate the effectiveness of TGE experimentally with the communication patterns extracted from runs of XGC, a large-scale fusion simulation code, on Titan.","Topology,
Benchmark testing,
Network topology,
Data visualization,
Resource management,
Heuristic algorithms"
Machine Learning in Cognitive Radios,,
Machine Learning Applied to Cognitive Communications,,
Machine Learning for Cognitive Networks: Technology Assessment and Research Challenges,,
DATA ANALYSIS AND MACHINE LEARNING EFFORT IN HEALTHCARE,,
Machine Learning,,
Machine Learning Techniques for Multimedia Analysis,,
Amazon?s Echo Look: Harnessing the Power of Machine Learning or Subtle Exploitation of Human Vulnerability?,"Here we are in 2017, but, at times, it feels as though we are back in the 1950s. Apparently, how women look and present themselves to the world is so crucial that they must sacrifice their privacy, security, and trust to Amazon's algorithms, just to gain societal acceptance for their fashion choices. Or at least, this is how it appears in the Amazon Echo Look video.","Algorithm design and analysis,
Consumer electronics,
Electronic commerce,
Market research,
Computer security,
Ethics,
Companies"
The Trust Value Calculating for Social Network Based on Machine Learning,"In this paper, a social network model is built for the social network information in the social network, and the machine learning method is used to calculate the node trust value. First, the results calculated by the traditional node trust value calculation method and some auxiliary information are used as the training feature of the machine learning, and the measurement whether there is edge between nodes as label information. Second, the node logistic regression model is used as the training model to calculate the node trust value. Then, recommendation algorithm which is analogous to the user collaborative filtering algorithm is used to calculate node trust value. At last, the simulation is used to verify the performance of the improved method, and the results show that the prediction accuracy of node trust value computing by improved algorithm is significantly higher than that of node trust value computing by formula.","Training,
Social network services,
Logistics,
Prediction algorithms,
Indexes,
Machine learning algorithms,
Predictive models"
A fully configurable and scalable neural coprocessor IP for SoC implementations of machine learning applications,"This paper presents a fully configurable and programmable coprocessor IP core to efficiently compute Artificial Neural Networks (ANNs) in heterogeneous System-on-Chips (SoC). There is an increasing interest in moving applications involving streamed data such as those arising in machine-learning systems (machine-vision, speech-recognition, etc.) to highly-integrated low-power embedded devices. In this context efficient memory utilization, which is critical to both performance and energy consumption, requires handling multiple memory resources and different address spaces. The proposed IP core cooperates with a companion interconnect IP that acts as an abstraction layer to allow seamless integration of such heterogeneous resources. Dynamic (re)allocation is supported, so that embedded memory resources (Block RAMs) and external blocks (DDR memory) can be transparently used. The coprocessor itself is a configurable vector-matrix product computer with optional non-linear filtering, thus any single layer and multilayer ANN model can be easily implemented. The applicability of the IPs is demonstrated in a Hyperspectral Image (HSI) real-time classification problem.","Coprocessors,
Computer architecture,
IP networks,
Program processors,
Field programmable gate arrays,
Hardware,
Clocks"
Category Classification of Text Data with Machine Learning Technique for Visualizing Flow of Conversation in Counseling,"The beginner counselors have more likely to continue counseling in their own interest, they have a high tendency to make great use of the closed-ended question in order to confirm the interpretation with the client. While expert counselors are instructing the counseling skill to beginner counselors, we consider that the reaction of a client for a beginner counselor's question is important to visualize in an appropriate method. To respond the request, we have developed a system for visualizing the flow of conversation in counseling. However, the expert counselor as the system user requires to correct the initial classification result manually, and the work burden is large, because the accuracy of the category classification of conversation data is very low in the current system. To improve this problem, we have implemented on the category classification method of text data with SVM (Support Vector Machine) as machine learning technique to visualize the flow of conversation in counseling. In addition, we have compared and evaluated with results of the initial classification method of the current system. As these results, we have shown that the accuracy rate of the classification method with SVM become higher than the results in the current system.","Employee welfare,
Support vector machines,
Data visualization,
Psychology,
Dictionaries,
Data models,
Employment"
Japanese Fingerspelling Recognition Based on Classification Tree and Machine Learning,"Sign language is a very important communication tool for hearing-impaired people and also for the communication between hearing-impaired and non-handicapped people. There are many methods for sign language recognition, some of which are based on Hidden Markov Model (HMM) and others are based on Support Vector Machine (SVM) and so forth. In fact, the most of previous methods recognize fingerspelling using video sequence because some fingerspellings are accompanied by movement. Some methods use Microsoft Kinect or Leap Motion controller to obtain the finger movement. Some fingerspellings, however, are not accompanied by movement and can be recognized with just one snap shot of fingerspelling. Therefore, this paper proposes a recognition method of fingerspelling without movement. The target fingerspellings are 41 characters without movement in Japanese sign language, and the method uses only one picture. Some of fingerspellings are easily recognized and others are not so that the method is based on pattern recognition using classification tree and machine learning with SVM for easily recognized fingerspellings and difficultly recognized ones, respectively. As the result of the experiment, the averaged recognition rate was 86%.","Gesture recognition,
Assistive technology,
Support vector machines,
Feature extraction,
Character recognition,
Hidden Markov models"
Machine learning approach for optimal determination of wave parameter relationships,"Wave parameter relationships have long been determined using methods that give non-standard and often inaccurate results. With increased commercial activity in the marine sector, the importance of accurate wave parameter relationship determination has become increasingly apparent. The outputs of many numerical models and buoy datasets do not include all requisite wave parameters, and a typical approach is to use a constant conversion factor or relationship based on defined spectra such as the Bretschneider or the joint North Sea wave observation project (JONSWAP) spectrum to determine these parameters. Given that relationships between wave parameters vary significantly over both hourly and seasonal and annual timescales, the currently employed methods are lacking, as subtleties are missed by the simpler approach. This paper addresses the determination of wave parameter relationships using a machine learning (ML)-based model, identifying and selecting the optimal method for the conversion of wave parameters (Te, T01) in coastal Irish Waters. This approach is then validated at two sites on the West coast of Ireland. The aim is to highlight the utility of ML in approximating the relationship between wave parameters; using both buoy and modelled data, and mapping the predicted outcomes for a wave energy converter based on a variety of ML and measure correlate predict approaches.","learning (artificial intelligence),
ocean waves,
power engineering computing,
wave power generation"
Understanding the feasibility of machine learning algorithms in a game theoretic environment for dynamic spectrum access,"The key enabling technology in dynamic spectrum access is Cognitive Radio that allows unlicensed secondary users to access the licensed bands without causing any interference to the primary users. In any situation where there are a certain number of secondary networks trying to get an available channel, there arises a game theoretic competition where they want to get the channel for themselves by incurring as minimum cost as possible. The increase in cost is equivalent to the increase in time caused by the need of a search for an available channel. This process could be sped up if the networks had a predictive mechanism to determine the optimal strategy. In this paper, we investigate various predictive algorithms: Linear regression, Support Vector Regression and Elastic Net and compare them with other traditional non-predictive game theoretic mechanisms. We measure the accuracy of these algorithms in terms of time taken to reach the system convergence. We also observe how a self-learning approach can be helpful in maximizing utilities of the players in comparison to traditional game theoretic approaches.","Games,
Switches,
Machine learning algorithms,
Interference,
Prediction algorithms,
Dynamic spectrum access,
Cognitive radio"
Audio Classification Method Based on Machine Learning,"Audio classification has very large theoretical and practical values in both pattern recognition and artificial intelligence. In this paper, we propose a novel audio classification method based on machine learning technique. Firstly, we illustrate the hierarchical structure of audio data, which is made up of four layers: 1) Audio frame, 2) Audio clip, 3) Audio shot, and 4) Audio high level semantic unit. Secondly, three types of audio data feature are extracted to construct feature vector, including 1) Short time energy, 2) Zero crossing rate and 3) Mel-Frequency cepstral coefficients. Thirdly, we discuss how to classify audio data using the SVM classifier with Gaussian kernel. Finally, experimental results demonstrate that the proposed method is able to achieve higher audio classification accuracy.","Transportation,
Big Data,
Smart cities"
Matheuristic with machine-learning-based prediction for software-defined mobile metro-core networks,"In general, humans follow a routine with highly predictable daily movements. For instance, we commute from home to work on a daily basis and visit a selected set of places for commercial and recreational purposes during the nights and weekends. The use of mobile phones increases when commuting via public transportation, during lunch breaks, and at night. Such regular behavior creates predictable spatiotemporal fluctuations of traffic patterns. In this paper, we introduce a matheuristic for dynamic optical routing, which can be implemented as an application into a software-defined mobile carrier network. We use machine learning to predict tidal traffic variations in a mobile metro-core network, which allows us to solve off-line mixed-integer linear programming instances of an optical routing (and wavelength) assignment optimization problem. The optimal results are used to favor near-optimal on-line routing decisions. Results demonstrate the effectiveness of our on-line methodology, with results that match almost perfectly the behavior of a network that performs optical routing reconfiguration with a perfect, oracle-like traffic prediction and the solution of an optimization problem.","Routing,
Mobile communication,
Planning,
Mobile computing,
Prediction algorithms,
Optimization,
Heuristic algorithms"
Machine Learning Techniques for Analyzing Training Behavior in Serious Gaming,"Training time is a costly, scarce resource across domains such as commercial aviation, healthcare, and military operations. In the context of military applications, serious gaming -- the training of warfighters through immersive, real-time environments rather than traditional classroom lectures -- offers benefits to improve training not only in its hands-on development and application of knowledge, but also in data analytics via machine learning. In this paper, we explore an array of machine learning techniques that allow teachers to visualize the degree to which training objectives are reflected in actual play. First, we investigate the concept of discovery: learning how warfighters utilize their training tools and develop military strategies within their training environment. Second, we develop machine learning techniques that could assist teachers by automatically predicting player performance, identifying player disengagement, and recommending personalized lesson plans. These methods could potentially provide teachers with insight to assist them in developing better lesson plans and tailored instruction for each individual student.","Games,
Training,
Tools,
Hidden Markov models,
Measurement,
Missiles"
A Comparison of Distributed Machine Learning Platforms,"The proliferation of big data and big computing boosted the adoption of machine learning across many application domains. Several distributed machine learning platforms emerged recently. We investigate the architectural design of these distributed machine learning platforms, as the design decisions inevitably affect the performance, scalability, and availability of those platforms. We study Spark as a representative dataflow system, PMLS as a parameter- server system, and TensorFlow and MXNet as examples of more advanced dataflow systems. We take a distributed systems perspective, and analyze the communication and control bottlenecks for these approaches. We also consider fault-tolerance and ease-of-development in these platforms. In order to provide a quantitative evaluation, we evaluate the performance of these three systems with basic machine learning tasks: logistic regression, and an image classification example on the MNIST dataset.","Sparks,
Computational modeling,
Servers,
Training,
Computer architecture,
Data models,
Machine learning algorithms"
Machine-Learning Based Threat-Aware System in Software Defined Networks,"Software-Defined Networking (SDN) is an emerging network architecture that decouples the control plane and the data plane to provide unprecedented programmability, automation, and network control. The SDN controller exercises centralized control over network software, and in doing so, it can monitor and respond to malicious traffic for network protection. This paper proposes a threat-aware system based on machine-learning for timely detection and response against network intrusion in SDN. Our proposed system consists of data preprocessing for feature selection, predictive data modeling for machine-learning and anomaly detection, and decision making for intrusion response in SDN. Due to the time-critical nature of SDN, we propose a practical approach utilizing machine-learning techniques to protect against network intrusion and reduce uncertainty in decision-making outcomes. The maliciousness of most uncertain network traffic subsets is evaluated with selected significant feature sets. Our experimental results show that the proposed approach achieves high performance and significantly reduces uncertainty in the decision process with a small number of feature sets. The results help the SDN controller to properly react against known or unknown attacks that cannot be prevented by signature-based network intrusion detection systems.","Intrusion detection,
Data models,
Predictive models,
Routing,
Real-time systems,
Classification algorithms,
Systems architecture"
Machine-Learning Classifiers for Security in Connected Medical Devices,"Medical devices equipped with wireless connectivity and remote monitoring features are increasingly becoming connected to each other, to an outside programmer and even to the Internet. While Internet of Things technology enables health-care professionals to fine tune or modify medical device settings without invasive procedures, this also opens up large attack surfaces and introduces potential security vulnerabilities. Medical device hacks are slowly becoming a reality and it becomes more critical than ever to defend and protect these devices from security attacks. In this paper, we assess the feasibility of using machine learning models to efficiently determine attacks targeted on a medical device. Specifically, we develop feature sets to accurately profile a medical device and observe any deviation from its normal behavior. We test our method using different machine learning algorithms and provide a comparison analysis of the detection results.","Security,
Medical diagnostic imaging,
Logic gates,
Wireless communication,
Communication system security,
Monitoring,
Machine learning algorithms"
Machine learning based prediction of thermal comfort in buildings of equatorial Singapore,"Majority of energy consumption in Singapore buildings is due to air-conditioning, because of its hot and humid weather. Besides attaining a healthy indoor environment, a prior knowledge about the occupant's thermal comfort can be beneficial in reducing energy consumption, as it can save energy which is otherwise spent in extra cooling. This paper proposes a data-driven approach to predict individual thermal comfort level (`cool-discomfort', `comfort', `warm-discomfort') using environmental and human factors as input. Six types of classifiers have been implemented-Support Vector Machine (SVM), Artificial Neural Network (ANN), Logistic Regression (LR), Linear Discriminant Analysis (LDA), K-Nearest Neighbors (KNN), and Classification Trees (CT), on a publicly available database of 817 occupants for air-conditioned and free-running buildings separately. Results show that our approach achieves prediction accuracies of 73.14-81.2%, outperforming the traditional Fanger's PMV (Predicted Mean Vote) model, which has accuracies of only 41.68-65.5%. Age, gender, and outdoor effective temperature, which are not included in the PMV model, are found to be important factors for thermal comfort. The proposed approach also outperforms modified PMV models-the extended PMV model and the adaptive PMV model which attain accuracies of 61.75% and 35.51% respectively.","Buildings,
Adaptation models,
Meteorology,
Temperature,
Atmospheric modeling,
Support vector machines"
Mesh Convolutional Restricted Boltzmann Machines for Unsupervised Learning of Features With Structure Preservation on 3-D Meshes,"Discriminative features of 3-D meshes are significant to many 3-D shape analysis tasks. However, handcrafted descriptors and traditional unsupervised 3-D feature learning methods suffer from several significant weaknesses: 1) the extensive human intervention is involved; 2) the local and global structure information of 3-D meshes cannot be preserved, which is in fact an important source of discriminability; 3) the irregular vertex topology and arbitrary resolution of 3-D meshes do not allow the direct application of the popular deep learning models; 4) the orientation is ambiguous on the mesh surface; and 5) the effect of rigid and nonrigid transformations on 3-D meshes cannot be eliminated. As a remedy, we propose a deep learning model with a novel irregular model structure, called mesh convolutional restricted Boltzmann machines (MCRBMs). MCRBM aims to simultaneously learn structure-preserving local and global features from a novel raw representation, local function energy distribution. In addition, multiple MCRBMs can be stacked into a deeper model, called mesh convolutional deep belief networks (MCDBNs). MCDBN employs a novel local structure preserving convolution (LSPC) strategy to convolve the geometry and the local structure learned by the lower MCRBM to the upper MCRBM. LSPC facilitates resolving the challenging issue of the orientation ambiguity on the mesh surface in MCDBN. Experiments using the proposed MCRBM and MCDBN were conducted on three common aspects: global shape retrieval, partial shape retrieval, and shape correspondence. Results show that the features learned by the proposed methods outperform the other state-of-the-art 3-D shape features.","Shape,
Machine learning,
Solid modeling,
Feature extraction,
Convolution,
Unsupervised learning"
GPU-Accelerated Parallel Hierarchical Extreme Learning Machine on Flink for Big Data,"The extreme learning machine (ELM) has become one of the most important and popular algorithms of machine learning, because of its extremely fast training speed, good generalization, and universal approximation/classification capability. The proposal of hierarchical ELM (H-ELM) extends ELM from single hidden layer feedforward networks to multilayer perceptron, greatly strengthening the applicability of ELM. Generally speaking, during training H-ELM, large-scale datasets (DSTs) are needed. Therefore, how to make use of H-ELM framework in processing big data is worth further exploration. This paper proposes a parallel H-ELM algorithm based on Flink, which is one of the in-memory cluster computing platforms, and graphics processing units (GPUs). Several optimizations are adopted to improve the performance, such as cache-based scheme, reasonable partitioning strategy, memory mapping scheme for mapping specific Java virtual machine objects to buffers. Most importantly, our proposed framework for utilizing GPUs to accelerate Flink for big data is general. This framework can be utilized to accelerate many other variants of ELM and other machine learning algorithms. To the best of our knowledge, it is the first kind of library, which combines in-memory cluster computing with GPUs to parallelize H-ELM. The experimental results have demonstrated that our proposed GPU-accelerated parallel H-ELM named as GPH-ELM can efficiently process large-scale DSTs with good performance of speedup and scalability, leveraging the computing power of both CPUs and GPUs in the cluster.","Approximation algorithms,
Big Data,
Acceleration,
Training,
Libraries,
Machine learning algorithms,
Clustering algorithms"
A Machine Learning Algorithm for Identifying Atopic Dermatitis in Adults from Electronic Health Records,"The current work aims to identify patients with atopic dermatitis for inclusion in genome-wide association studies (GWAS). Here we describe a machine learning-based phenotype algorithm. Using the electronic health record (EHR), we combined coded information with information extracted from encounter notes as features in a lasso logistic regression. Our algorithm achieves high positive predictive value (PPV) and sensitivity, improving on previous algorithms with low sensitivity. These results demonstrate the utility of natural language processing(NLP) and machine learning for EHR-based phenotyping.","Machine learning algorithms,
Prediction algorithms,
Diseases,
Skin,
Feature extraction,
Gold,
Standards"
Semisupervised Incremental Support Vector Machine Learning Based on Neighborhood Kernel Estimation,"Semisupervised scheme has emerged as a popular strategy in the machine learning community due to the expensiveness of getting enough labeled data. In this paper, a semisupervised incremental support vector machine (SE-INC-SVM) algorithm based on neighborhood kernel estimation is proposed. First, kernel regression is constructed to estimate the unlabeled data from the labeled neighbors and its estimation accuracy is discussed from the analogy with tradition RBF neural network. The incremental scheme is derived to improve the learning efficiency and reduce the computing time. Simulations for manual data set and industrial benchmark-penicillin fermentation process demonstrate the effectiveness of the proposed SE-INC-SVM method.","Support vector machines,
Kernel,
Estimation,
Data models,
Interpolation,
Semisupervised learning,
Training"
Detecting Cognitive Distortions Through Machine Learning Text Analytics,"Machine learning and text analytics have proven increasingly useful in a number of health-related applications, particularly in the context of analyzing online data for disease epidemics and warning signs of a variety of mental health issues. We follow in this tradition here, but focus our attention on cognitive distortion, a precursor and symptom of disruptive psychological disorders such as anxiety, anorexia and depression. We collected a number of personal blogs from the Tumblr API, and labeled them based on whether they exhibited distorted thought patterns. We then used LIWC to extract textual features and applied machine learning to the resulting vectors. Our findings show that it is possible to detect cognitive distortions automatically from personal blogs with relatively good accuracy (73.0%) and false negative rate (30.4%).","Distortion,
Logistics,
Psychology,
Blogs,
Social network services,
Pragmatics,
Decision trees"
A machine learning approach to characterizing the effect of asynchronous distributed electrical stimulation on hippocampal neural dynamics in vivo,"Asynchronous distributed microelectrode theta stimulation (ADMETS) of the hippocampus has been shown to reduce seizure frequency in the tetanus toxin rat model of mesial temporal lobe epilepsy suggesting a hypothesis that ADMETS induces a seizure resistant state. Here we present a machine learning approach to characterize the nature of neural state changes induced by distributed stimulation. We applied the stimulation to two animals under sham and ADMETS conditions and used a combination of machine learning techniques on intra-hippocampal recordings of Local Field Potentials (LFPs) to characterize the difference in the neural state between sham and ADMETS. By iteratively fitting a logistic regression with data from the inter-stimulation interval under sham and ADMETS condition we found that the classification performance improves for both animals until 90s post stimulation before leveling out at AUC of 0.64 ± 0.2 and 0.67 ± 0.02 when all inter-stimulation data is included. The models for each animal were re-fit using elastic net regularization to force many of the model coefficients to 0, identifying those that do not optimally contribute to the classifier performance. We found that there is significant variation in the non-zero coefficients between animals (p < 0.01), suggesting that the ADMETS induced state is represented differently between subject. These findings lay the foundation for using machine learning to robustly and quantitatively characterize neural state.","Animals,
Electrodes,
Data models,
Hippocampus,
Logistics,
Biological system modeling,
Biomarkers"
Comparing deep neural network and other machine learning algorithms for stroke prediction in a large-scale population-based electronic medical claims database,"Electronic medical claims (EMCs) can be used to accurately predict the occurrence of a variety of diseases, which can contribute to precise medical interventions. While there is a growing interest in the application of machine learning (ML) techniques to address clinical problems, the use of deep-learning in healthcare have just gained attention recently. Deep learning, such as deep neural network (DNN), has achieved impressive results in the areas of speech recognition, computer vision, and natural language processing in recent years. However, deep learning is often difficult to comprehend due to the complexities in its framework. Furthermore, this method has not yet been demonstrated to achieve a better performance comparing to other conventional ML algorithms in disease prediction tasks using EMCs. In this study, we utilize a large population-based EMC database of around 800,000 patients to compare DNN with three other ML approaches for predicting 5-year stroke occurrence. The result shows that DNN and gradient boosting decision tree (GBDT) can result in similarly high prediction accuracies that are better compared to logistic regression (LR) and support vector machine (SVM) approaches. Meanwhile, DNN achieves optimal results by using lesser amounts of patient data when comparing to GBDT method.","Electromagnetic compatibility,
Databases,
Prediction algorithms,
Support vector machines,
Training,
Diseases,
Time measurement"
Efficient and Rapid Machine Learning Algorithms for Big Data and Dynamic Varying Systems,"With the exponential growth of data and complexity of systems, fast machine learning/artificial intelligence and computational intelligence techniques are highly required. Many conventional computational intelligence techniques face bottlenecks in learning (e.g., intensive human intervention and convergence time) [item 1) in the Appendix]. However, efficient learning algorithms alternatively offer significant benefits including fast learning speed, ease of implementation, and minimal human intervention. The need for efficient and fast implementation of machine learning techniques in big data and dynamic varying systems poses many research challenges. This special issue highlights some latest development in the related areas.","Big Data,
Feature extraction,
Support vector machines,
Cybernetics,
Acceleration,
Kernel,
Delays"
Supervised Machine Learning to Predict Follow-Up Among Adjuvant Endocrine Therapy Patients,"Long-term adjuvant endocrine therapy patients often fail to follow-up with their care providers for the recommended duration of time. We used electronic health record data, tumor registry records, and appointment logs to predict follow-up for an adjuvant endocrine therapy patient cohort. Learning predictors for follow-up may facilitate interventions that improve follow-up rates, and ultimately improve patient care in the adjuvant endocrine therapy patient population.We selected 1455 adjuvant endocrine therapy patients at Vanderbilt University Medical Center, and modeled them as a matrix of medical-related, appointment-related, and demographic related features derived from EHR data. We built and optimized a random forest classifier and neural network to differentiate between patients that follow-up, or fail to follow-up, with their care provider for at least five years. We measured follow-up three different ways: thought appointments with any care providers, appointments with an oncologist, and adjuvant endocrine therapy medication records. Classifiers make predictions at the start of adjuvant endocrine therapy, and additionally use temporal subsets of data to learn the change in accuracy as patient data accrues.Our best model is a random forest classifier combining medical-related, appointment-related, and demographic-related features to achieve an AUC of 0.74. The most predictive features for follow-up in our random forest model are total medication counts, patient age, and median income for zip code. We suggest that reliable prediction for follow-up may be correlated with amount of care received at VUMC (i.e., VUMC primary care).This study achieved moderately accurate prediction for followup in adjuvant endocrine therapy patients from electronic health record data. Predicting follow-up can facilitate interventions for improving follow-up rates and improve patient care for adjuvant endocrine therapy cohorts. This study demonstrates the ability to find opportunities for patient care improvement from EHR data.","Medical diagnostic imaging,
Tumors,
Electronic medical records,
Neural networks,
Drugs,
Breast cancer"
Retinal hemorrhage detection by rule-based and machine learning approach,"Robust detection of hemorrhages (HMs) in color fundus image is important in an automatic diabetic retinopathy grading system. Detection of the hemorrhages that are close to or connected with retinal blood vessels was found to be challenge. However, most methods didn't put research on it, even some of them mentioned this issue. In this paper, we proposed a novel hemorrhage detection method based on rule-based and machine learning methods. We focused on the improvement of detection of the hemorrhages that are close to or connected with retinal blood vessels, besides detecting the independent hemorrhage regions. A preliminary test for detecting HM presence was conducted on the images from two databases. We achieved sensitivity and specificity of 93.3% and 88% as well as 91.9% and 85.6% on the two datasets.","Blood vessels,
Biomedical imaging,
Hemorrhaging,
Image segmentation,
Retina,
Image color analysis,
Optical filters"
Classifying osteosarcoma patients using machine learning approaches,"Metabolomic data analysis presents a unique opportunity to advance our understanding of osteosarcoma, a common bone malignancy for which genomic and proteomic studies have enjoyed limited success. One of the major goals of metabolomic studies is to classify osteosarcoma in early stages, which is required for metastasectomy treatment. In this paper we subject our metabolomic data on osteosarcoma patients collected by the SJTU team to three classification methods: logistic regression, support vector machine (SVM) and random forest (RF). The performances are evaluated and compared using receiver operating characteristic curves. All three classifiers are successful in distinguishing between healthy control and tumor cases, with random forest outperforming the other two for cross-validation in training set (accuracy rate for logistic regression, support vector machine and random forest are 88%, 90% and 97% respectively). Random forest achieved overall accuracy rate of 95% with 0.99 AUC on testing set.",
Detection of needle to nerve contact based on electric bioimpedance and machine learning methods,"In an ongoing project for electrical impedance-based needle guidance we have previously showed in an animal model that intraneural needle positions can be detected with bioimpedance measurement. To enhance the power of this method we in this study have investigated whether an early detection of the needle only touching the nerve also is feasible. Measurement of complex impedance during needle to nerve contact was compared with needle positions in surrounding tissues in a volunteer study on 32 subjects. Classification analysis using Support-Vector Machines demonstrated that discrimination is possible, but that the sensitivity and specificity for the nerve touch algorithm not is at the same level of performance as for intra-neuralintraneural detection.","Needles,
Electrodes,
Phase measurement,
Impedance measurement,
Impedance,
Muscles,
Training"
Catching Zika Fever: Application of Crowdsourcing and Machine Learning for Tracking Health Misinformation on Twitter,"In February 2016, World Health Organization declared the Zika outbreak a Public Health Emergency of International Concern. With developing evidence it can cause birth defects, and the Summer Olympics coming up in the worst affected country, Brazil, the virus caught fire on social media. In this work, we use Zika as a case study in building a tool for tracking the misinformation around health concerns on Twitter. We collect more than 13 million tweets regarding the Zika outbreak and track rumors outlined by the World Health Organization and Snopes fact checking website. The tool pipeline, which incorporates health professionals, crowdsourcing, and machine learning, allows us to capture health-related rumors around the world, as well as clarification campaigns by reputable health organizations. We discover an extremely bursty behavior of rumor-related topics, and show that, once the questionable topic is detected, it is possible to identify rumor-bearing tweets using automated techniques.","Twitter,
Crowdsourcing,
Organizations,
Public healthcare,
Tools,
Vaccines"
Machine learning techniques for taming the complexity of modern hardware design,"The continual quest to improve performance and efficiency for new generations of IBM servers leads to a corresponding increase in system complexity. As hardware complexity increases, i.e., more complicated hardware architectures requiring more design choices, the level of sophistication in automation also increases to manage the design challenges. The number of design choices in modern hardware design calls for intelligent automated techniques to navigate the design space. This paper covers three machine learning-based automation techniques used during the design and lifetime of IBM systems. In particular, we describe applying these techniques to the IBM z13 mainframe. During the presilicon design phase, a software system called synthesis tuning system is employed to optimize the parameters of the synthesis program vital to hardware implementation. During both the presilicon and postsilicon phases of the design, a framework called MicroProbe automatically generates microbenchmarks, i.e., small programs, to determine power, performance, and resilience characteristics of the system. Following system product deployment in customer environments, the Call Home facility monitors and analyzes a wide range of in-field usage metrics to help administrators understand current system behavior and improve future designs. Beyond existing IBM system contributions, this high-level overview paper also describes additional machine learning (and related) techniques in the field of hardware design, along with future directions for such work.","Hardware,
Complexity theory,
Automation,
Microarchitecture,
Servers"
Drowsy Driving Warning System Based on GS1 Standards with Machine Learning,"Drowsy driving is the primary cause of motor vehicle accidents and is a risk factor that leads to the loss of human life, remaining as a challenge for the global automotive industry. Recently, drowsy monitoring system has been actively studied for prediction system based machine learning. However, the challenges of automotive real-time constraints and flexibility should be taken into consideration against a large amount of heterogeneous data from vehicle network and other device. To solve this problem, we propose drowsy monitoring system based machine learning using GS1 standard. First, vehicle motion data is defined and modeled using the GS1 standard language for drowsy predict. Second, we propose an optimal algorithm selection and detail architecture for automotive real-time environments through machine learning algorithms (KNN, Naïve Bayes, Logistic Regression) and deep learning algorithms (RNN-LSTM). Finally, we describe system-wide integration and implementation through the open source hardware Raspberry Pi and the machine learning SW framework. We provide optimal LSTM architecture and implementation that takes into account the real-time environmental conditions and how to improve the readability and usability of the vehicle motion data. We also share the rapid prototyping methodology case of connected car systems without other sensor devices.","Standards,
Monitoring,
Machine learning algorithms,
Real-time systems,
Automobiles,
Automotive engineering"
Adaptive Cost Efficient Framework for Cloud-Based Machine Learning,"Machine learning is an increasingly important form of cognitive computing, making progress in several application areas. Machine learning often involves big data sets and is computationally challenging, requiring efficient use of resources. The use of cloud computing as the platform for machine learning offers advantages of scalability and efficient use of hardware. It may, however, be difficult to provision appropriate cost-effective resources for a machine learning task. Our experiments have shown that there can be radical differences between different datasets and different algorithms on the same dataset. The cloud-based machine learning framework presented here aims to provide multiple levels of efficient use of resources and uses a high-level cost model to deal with overall cost-efficiency with respect to cloud service providers. The cost model allows evaluation of trade-offs and supports the choice of appropriate provider resources based on user-defined criteria. A user may choose to prioritize performance, prioritize cost or specify a cost-performance balance. An Amazon AWS cost model for instances is used to illustrate the practical benefits of using the approach - it is seen that large savings can be made by employing this job-specific monitoring and cost-performance analysis. The method can provide all the information for a comparison across different cloud service providers as well as comparisons across the Amazon AWS offerings.",
A Study of Machine Learning in Healthcare,"In the past few years, there has been significant developments in how machine learning can be used in various industries and research. This paper discusses the potential of utilizing machine learning technologies in healthcare and outlines various industry initiatives using machine learning initiatives in the healthcare sector.","Medical services,
Industries,
Medical diagnostic imaging,
Big Data,
Market research,
Data models"
Application of machine learning method in bridge health monitoring,"Machine learning algorithms have been a typical type of highly efficient method for data processing in these recent decades, and data-driven approaches for bridge health monitoring is particularly useful since a large quantity of sensor data are available. In this paper, a review of most popular applications of machine learning method are presented in order to illustrate their utilities and limitations in the field of bridge health monitoring.","Bridges,
Monitoring,
Support vector machines,
Data processing,
Neural networks,
Structural beams,
Machine learning algorithms"
Comprehensive Predictions of Tourists' Next Visit Location Based on Call Detail Records Using Machine Learning and Deep Learning Methods,"Recent developments in data mining and machine learning have helped to solve many issues in prediction and recommendation. In this project, we run a comprehensive study on individual behavior patterns from call detail records (CDR) data to predict tourists' future stops. Multiple classification algorithms are employed, including Decision Tree, Random Forest, Neural Network, Naïve Bayes and SVM. In addition, a Recurrent Neural Network-Long Short Term Memory (LSTM) that is ordinarily applied to language inference problems is tested. Surprisingly, we find that LSTM provides us with the best prediction (94.8%), while Random Forest/Neural Network give the second best (85%). Our investigation suggests that the memory-dependence property of LSTM architecture gives it great expressive power to model our time-series location data, making it an outstanding classifier.","Support vector machines,
Vegetation,
Poles and towers,
Decision trees,
Machine learning,
Recurrent neural networks,
Urban areas"
A machine learning based method of constructing virtual inertial measurement predictor of human body,"In recent years, using human characteristics to assist exoskeleton robot control is one of the hot spots in the field of robot technology. For the problem that extremity installation of inertial measurement components in human body cannot achieve effective measurement, a method of constructing virtual inertial measurement predictor of human body based on machine learning is studied. The method uses the outputs of the inertial measurement components synchronously installed on the extremities and other parts of the body as the data samples, through recurrent neural network, it realizes the construction of virtual inertial measurement components and their predictors. In order to improve the training effect, the training samples are filtered based on gait phase detection. The simulation based on Anybody and MATLAB shows that, by installing the inertial measurement component near hip joint, the proposed method can effectively simulate and predict the inertial measurement component's output of centroid position of foot and surface of shank.","Recurrent neural networks,
Foot,
Mathematical model,
Legged locomotion,
Extraterrestrial measurements,
Biological system modeling"
A Machine Learning-Based Approach to Detect Web Service Design Defects,"Design defects are symptoms of poor design and implementation solutions adopted by developers during the development of their software systems. While the research community devoted a lot of effort to studying and devising approaches for detecting the traditional design defects in object-oriented (OO) applications, little knowledge and support is available for an emerging category of Web service interface design defects. Indeed, it has been shown that service designers and developers tend to pay little attention to their service interfaces design. Such design defects can be subjectively interpreted and hence detected in different ways. In this paper, we propose a novel approach, named WS3D, using machine learning techniques that combines Support Vector Machine (SVM) and Simulated Annealing (SA) to learn from real world examples of service design defects. WS3D has been empirically evaluated on a benchmark of Web services from 14 different application domains. We compared WS3D with the state-of-theart approaches which rely on traditional declarative techniques to detect service design defects by combining metrics and threshold values. Results show that WS3D outperforms the the compared approaches in terms of accuracy with a precision and recall scores of 91% and 94%, respectively.","Web services,
Support vector machines,
Measurement,
Training,
Software systems,
Computer bugs,
Simulated annealing"
Improving Web Services Design Quality Using Heuristic Search and Machine Learning,"Web services evolve over time to fix bugs or update and add new features. However, the design of the Web service's interface may become more complex when aggregating many unrelated operations in terms of context and functionalities. A possible solution is to refactor the Web services interface into different modules that help the user quickly identifying relevant operations. The most challenging issue when refactoring a Web services interface is the high number of possible modularization solutions. The evaluation of these solutions is subjective and difficult to quantify. This paper introduces the use of a neural network-based evaluation function for the problem of Web services interface modularization. The users evaluate manually the suggested modularization solutions by a Genetic Algorithm (GA) for a number of iterations then an Artificial Neural Network (ANN) uses these training examples to evaluate the proposed Web services design changes for the remaining iterations. We evaluated the efficiency of our approach using a benchmark of 82 Web services from different domains and compared the performance of our technique with several existing Web services modularization studies in terms of generating well-designed Web services interface for users.","Web services,
Measurement,
Genetic algorithms,
Predictive models,
Artificial neural networks,
Ports (Computers),
Training"
Predicting Application Failure in Cloud: A Machine Learning Approach,"Despite employing the architectures designed for high service reliability and availability, cloud computing systems do experience service outages and performance slowdown. In addition to these, large-scale cloud systems experience failures in their hardware and software components which often result in node and application (e.g., jobs and tasks) failures. Therefore, to build a reliable cloud system, it is important to understand and characterize the observed failures. The goal of this work is to identify the key features that correlate to application failures in cloud and present a failure prediction model that can correctly predict the outcome of a task or job before it actually finishes, fails or gets killed. To accomplish this, we perform a failure characterization study of the Google cluster workload trace. Our analysis reveals that, there is a significant consumption of resources due to failed and killed jobs. We further explore the potential for failure prediction in cloud applications so that we can reduce the wastage of resources by better managing the jobs and tasks that ultimately fail or get killed. For this, we propose a prediction method based on a special type of Recurrent NeuralNetwork (RNN) named Long Short-Term Memory Network(LSTM) to identify application failures in cloud. It takes resource usage measurements or performance data for each job and task, and the goal is to predict the termination status (e.g., failed and finished etc.) of them. Our algorithm can predict task failures with 87%accuracy and achieves a true positive rate of 85% and false positive rate of 11%.",
Root Cause Analysis of Network Failures Using Machine Learning and Summarization Techniques,"Root cause analysis includes the methods to identify the sources of errors in a network. Most techniques rely on knowledge models of the system, which are usually built by using network operators' expertise. This presents problems related to knowledge extraction, scalability, and understandability. We propose an offline method based on machine learning techniques for the automatic identification of dependencies between system events, enhanced with summarization, operations on graphs, and visualization that help network operators identify the root causes of errors. We illustrate it with examples from a corporate network.","Machine learning algorithms,
Predictive models,
Knowledge engineering,
Data mining,
Knowledge based systems,
Machine learning"
Runtime Data Layout Scheduling for Machine Learning Dataset,"Machine Learning (ML) approaches are widely-used classification/regression methods for data mining applications. However, the time-consuming training process greatly limits the efficiency of ML approaches. We use the example of SVM (traditional ML algorithm) and DNN (state-of-the-art ML algorithm) to illustrate the idea in this paper. For SVM, a major performance bottleneck of current tools is that they use a unified data storage format because the data formats can have a significant influence on the complexity of storage and computation, memory bandwidth, and the efficiency of parallel processing. To address the problem above, we study the factors influencing the algorithm's performance and conduct auto-tuning to speed up SVM training. DNN training is even slower than SVM. For example, using a 8-core CPUs to train AlexNet model by CIFAR-10 dataset costs 8.2 hours. CIFAR-10 is only 170 MB, which is not efficient for distributed processing. Moreover, due to the algorithm limitation, only a small batch of data can be processed at each iteration. We focus on finding the right algorithmic parameters and using auto-tuning techniques to make the algorithm run faster. For SVM training, our implementation achieves 1.7-16.3× speedup (6.8× on average) against the non-adaptive case (using the worst data format) for various datasets. For DNN training on CIFAR-10 dataset, we reduce the time from 8.2 hours to only roughly 1 minute. We use the benchmark of dollars per speedup to help the users to select the right deep learning hardware.","Support vector machines,
Training,
Prediction algorithms,
Kernel,
Bandwidth,
Sparse matrices,
Parallel processing"
A Machine Learning Approach for Efficient Parallel Simulation of Beam Dynamics on GPUs,"Parallel computing architectures like GPUs have traditionally been used to accelerate applications with dense and highly-structured workloads; however, many important applications in science and engineering are irregular and dynamic in nature, making their effective parallel implementation a daunting task. Numerical simulation of charged particle beam dynamics is one such application where the distribution of work and data in the accurate computation of collective effects at each time step is irregular and exhibits control-flow and memory access patterns that are not readily amenable to GPU's architecture. Algorithms with these properties tend to present both significant branch and memory divergence on GPUs which leads to severe performance bottlenecks.We present a novel cache-aware algorithm that uses machine learning to address this problem. The algorithm presented here uses supervised learning to adaptively model and track irregular access patterns in the computation of collective effects at each time step of the simulation to anticipate the future control-flow and data access patterns. Access pattern forecast are then used to formulate runtime decisions that minimize branch and memory divergence on GPUs, thereby improving the performance of collective effects computation at a future time step based on the observations from earlier time steps. Experimental results on NVIDIA Tesla K40 GPU shows that our approach is effective in maximizing data reuse, ensuring workload balance among parallel threads, and in minimizing both branch and memory divergence. Further, the parallel implementation delivers up to 485 Gflops of double precision performance, which translates to a speedup of up to 2.5X compared to the fastest known GPU implementation.","Computational modeling,
Data models,
Heuristic algorithms,
Graphics processing units,
Predictive models,
Two dimensional displays,
Instruction sets"
Combining a machine learning and optimization for early pre-FEC BER degradation to meet committed QoS,"Monitoring the physical layer is key to detect bit error rate (BER) degradation caused by failures and to identify the cause of the failure and localize the failed elements. Once the failure has been detected, actions can be taken to reduce as much as possible its impact on the network. Commercially available optical equipment are able to correct degraded optical signals by means of Forward Error Correction (FEC) algorithms. A value of pre-FEC BER over a pre-defined threshold would imply a non-error-free post-FEC transmission and, as a result, communication would be disrupted. Therefore, a prompt detection of lightpaths with excessive pre-FEC BER can help to greatly reduce such SLA violations, in particular when supporting vlinks. As a result of the above, it would be desirable to anticipate such degradations and apply re-optimization to re-route those affected demands according to their SLAs in order to reduce the affected traffic after the degradation is detected. Designing algorithms capable of promptly detect distinct BER anomaly patterns would be desirable. The objective would be to anticipate intolerable BER values as much as possible aiming at leaving enough time to plan a re-routing procedure during off-peak hours. In this paper, we propose an effective machine learning-based algorithm to localize and identify the most probable cause of failure impacting a given service, as well as a re-optimization algorithm to re-route affected demands, targeting at reducing SLA violation. Results show that the proposed detection and re-routing algorithms noticeably reduce bandwidth and number of demands affected.","Degradation,
Quality of service,
Bit error rate,
Bandwidth,
Monitoring,
Adaptive optics,
Optical receivers"
Methodological approach in prediction of balance with machine learning applied on fMRI data,"Machine learning is a useful method to assess biomedical data. However, due to the complexity of these data, it is important to rely on a robust methodology. Predictions with machine learning are not only an add-on to statistical analysis but are a real process of discovering and establishing clinical validity of the diagnostic techniques. This paper describes a machine learning approach to predict balance changes in seniors based on functional magnetic resonance imaging (fMRI) data. Functional MRI results in a huge amount data containing some noise. Thus, a noise filtering technique is proposed and subsequent analyses are conducted with various machine learning algorithms. The best prediction of the balance ability based on brain activation patterns was obtained by applying a neural network approach. This resulted in an area under the curve (AUC) of 0.91. Other learning algorithms such as the tree ensemble are less predictive but allow tracking and better understanding of the source of differentiation. The results are based on a real dataset.","Neural networks,
Standards,
Magnetic resonance imaging,
Prediction algorithms,
Force,
Vegetation,
Force measurement"
Machine learning based reduced reference bitstream audiovisual quality prediction models for realtime communications,"Perceived quality prediction models for multimedia services vary greatly depending on the type of the data and on the amount of information related to the original signal used. In this research, we have developed machine learning-based reduced-reference bitstream audiovisual quality prediction models by using the parametric version of the publicly available INRS audiovisual quality dataset. As that original INRS dataset did not contain bitstream information but provided both reference and transmitted videos, we have computed its bitstream version to develop the reduced-reference bitstream models. We have compared the performance of the Decision Trees based ensemble methods, Genetic Programming and Deep Learning models on this bitstream version of the dataset and have also compared these results with the results of the no-reference parametric models on the parametric version of the dataset. Decision Trees based ensemble methods outperformed Deep Learning and Genetic Programming based models when reduced-reference bitstream data was used and outperformed all existing no-reference parametric models that were trained and tested on the parametric version of the dataset. Our studies show that Decision Trees based approaches are well suited for no-reference parametric models as well as for reduced-reference bitstream models.","Videos,
Predictive models,
Mathematical model,
Computational modeling,
Loss measurement,
Machine learning,
Parametric statistics"
A Novel Ozone Profile Shape Retrieval Using Full-Physics Inverse Learning Machine (FP-ILM),"Identifying ozone profile shapes from nadir-viewing satellite sensors is a critical yet challenging task for accurate reconstruction of vertical distributions of ozone relevant to climate change and air quality. Motivated by the need to develop a methodology to fast, reliably, and efficiently exploit ozone distributions and inspired by the success of machine learning, this paper introduces a novel algorithm for estimating ozone profile shapes from satellite ultraviolet absorption spectra. The Full-Physics Inverse Learning Machine (FP-ILM) algorithm successfully characterizes ozone profile shapes using machine learning approaches. Its implementation mainly consists of a clustering process based on a semi-supervised agglomerative algorithm, a classification process based on full-physics radiative transfer simulations and a neural network (NN), and a profile scaling process based on a NN ensemble. The classification model has been trained with synthetic data generated by a forward model in conjunction with “smart sampling,” while the scaling model corresponding to each cluster requires total ozone information. The main innovation of FP-ILM is that, unlike conventional inversion methods, the ozone profile retrieval is formulated as a classification problem, leading to a noteworthy speed-up and accuracy when dealing with applications of satellite data. An outstanding retrieval performance with errors of less than 10% over 100–1 hPa has been obtained for synthetic measurements. Furthermore, the ozone profiles retrieved from the Global Ozone Monitoring Experiment–2 data using FP-ILM and the optimal estimation method reach an encouraging agreement (the differences are less than 6 Dobson Units or within 5%–20%).","Gases,
Shape,
Atmospheric measurements,
Satellites,
Clustering algorithms,
Remote sensing,
Terrestrial atmosphere"
Implicit Smartphone User Authentication with Sensors and Contextual Machine Learning,"Authentication of smartphone users is important because a lot of sensitive data is stored in the smartphone and the smartphone is also used to access various cloud data and services. However, smartphones are easily stolen or co-opted by an attacker. Beyond the initial login, it is highly desirable to re-authenticate end-users who are continuing to access security-critical services and data. Hence, this paper proposes a novel authentication system for implicit, continuous authentication of the smartphone user based on behavioral characteristics, by leveraging the sensors already ubiquitously built into smartphones. We propose novel context-based authentication models to differentiate the legitimate smartphone owner versus other users. We systematically show how to achieve high authentication accuracy with different design alternatives in sensor and feature selection, machine learning techniques, context detection and multiple devices. Our system can achieve excellent authentication performance with 98.1% accuracy with negligible system overhead and less than 2.4% battery consumption.",
Structure and model of the smart house security system using machine learning methods,"Modem security systems consist of typical parts that may eventually be fooled by professional burglars. Therefore, in recent years people need to improve mechanisms of home protection to make homes more safety. Artificial intelligence algorithms help us to develop new techniques for recognition of burglars inside the house.","Face recognition,
Security"
What Would a Graph Look Like in This Layout? A Machine Learning Approach to Large Graph Visualization,"Using different methods for laying out a graph can lead to very different visual appearances, with which the viewer perceives different information. Selecting a “good” layout method is thus important for visualizing a graph. The selection can be highly subjective and dependent on the given task. A common approach to selecting a good layout is to use aesthetic criteria and visual inspection. However, fully calculating various layouts and their associated aesthetic metrics is computationally expensive. In this paper, we present a machine learning approach to large graph visualization based on computing the topological similarity of graphs using graph kernels. For a given graph, our approach can show what the graph would look like in different layouts and estimate their corresponding aesthetic metrics. An important contribution of our work is the development of a new framework to design graph kernels. Our experimental study shows that our estimation calculation is considerably faster than computing the actual layouts and their aesthetic metrics. Also, our graph kernels outperform the state-of-the-art ones in both time and accuracy. In addition, we conducted a user study to demonstrate that the topological similarity computed with our graph kernel matches perceptual similarity assessed by human users.","Kernel,
Layout,
Measurement,
Visualization,
Inspection,
Data visualization,
Support vector machines"
An appropriate model predicting pest/diseases of crops using machine learning algorithms,Wireless Sensor Network is new technology to world and country like India where it can used in Agriculture Sector in India for increasing yield output by providing early prediction of plant diseases and pest. This can be happen by taking raw data from field where WSN network is install and with fitting appropriate machine learning model for this data to get predicted output. This paper gives an idea to how to deploy WSN on field and how Machine learning model is fitted for prediction of pest/diseases using Navie Bayes Kernel Algorithm.,"Agriculture,
Predictive models,
Wireless sensor networks,
Diseases,
Data models,
Soil,
Temperature sensors"
Distributed and in-Situ Machine Learning for Smart-Homes and Buildings: Application to Alarm Sounds Detection,"We consider the implementation of an in-situ machine learning system with the computing model promoted by Qarnot computing. Qarnot introduced an utility computing model in which servers are distributed in homes and offices where they serve as heaters. The Qarnot servers also embed several sensors for temperature, humidity, CO 2 etc. Qarnot offers an adequate platform to develop in-situ workflows for smart-homes problems. To demonstrate this point, we consider a typical problem: the detection of alarm sounds. Our paper introduces a new orchestration system for in-situ workflows, in the Qarnot platform. We also consider a general parallel framework for training alarm sound classifiers and decline an implementation that makes use of our orchestrator. Finally, we evaluate the implemented framework on different aspects including: the accuracy (of the resulting classifiers) and the runtime gain of the parallelization.","Training,
Buildings,
Computational modeling,
Servers,
Heating systems,
Data models,
Feature extraction"
Compound Analytics: Templates for Integrating Graph Algorithms and Machine Learning,"A general analytical framework is described for melding graph-theoretical algorithms and machine learning technologies. A main goal is to extract latent relationships and other forms of knowledge from immense, noisy, and often incomplete data. Several exemplars illustrate the overall process, and highlight critical methodological decision points encountered across a variety of application domains.","Machine learning algorithms,
Tools,
Pipelines,
Measurement,
Osteoarthritis,
Breast cancer"
Machine learning approach for predicting womens health risk,"Nowadays, developing countries face serious health issues such as Sexually Transmitted Infections, unintended pregnancies and other Reproductive Tract Infections. To tackle those health issues, customized awareness programs are necessary. To provide customized education and training to the women in developing and under developed regions, it is necessary to classify the women in those regions into different health risk segments and sub groups within the segment. In this paper, we discuss about a machine learning approach which will classify women into different health risk segments and sub groups based on the information collected from them. The Microsoft Women Health Risk Assessment competition was used to test the effectiveness of our machine learning model. Our approach is considered successful as we ended up in Top 10 in the competition.","Classification algorithms,
Training,
Numerical models,
Communication systems,
Risk management,
Aging,
Logic gates"
Design of Metaheuristic Based on Machine Learning: A Unified Approach,"In this work, a framework based on maximum likelihood estimation and mutual information is proposed to design a metaheuristic. A multilevel decomposition of metaheuristics is proposed that allow to have a unified vision on this optimization approach. Then, a new layer based on machine learning is added to take profit from the evolution of the algorithm to adapt it to the considered problem to alleviate users efforts interested in designing and implementing metaheuristics. In other terms, the goal is to proved an alternative to implement metaheuristics. Surprisingly, when results were compared to those of other classical metaheuristics, in most of cases the proposed approach provided good results.","Mutual information,
Optimization,
Sociology,
Statistics,
Machine learning algorithms,
Algorithm design and analysis,
Measurement"
A Machine Learning Approach to Automated Gait Analysis for the Noldus Catwalk ^{TM}System,"Objective: Gait analysis of animal disease models can provide valuable insights into in vivo compound effects and thus help in preclinical drug development. The purpose of this paper is to establish a computational gait analysis approach for the Noldus Catwalk^{TM} system, in which footprints are automatically captured and stored. Methods: We present a - to our knowledge - first machine learning based approach for the Catwalk^{TM} system, which comprises a step decomposition, definition and extraction of meaningful features, multivariate step sequence alignment, feature selection and training of different classifiers (Gradient Boosting Machine, Random Forest, Elastic Net). Results: Using animal-wise leave-one-out cross-validation we demonstrate that with our method we can reliable separate movement patterns of a putative Parkinson's disease (PD) animal model and several control groups. Furthermore, we show that we can predict the time point after and the type of different brain lesions and can even forecast the brain region, where the intervention was applied. We provide an in-depth analysis of the features involved into our classifiers via statistical techniques for model interpretation. Conclusion: A machine learning method for automated analysis of data from the Noldus Catwalk^{TM} system was established. Significance: Our works shows the ability of machine learning to discriminate pharmacologically relevant animal groups based on their walking behavior in a multivariate manner. Further interesting aspects of the approach include the ability to learn from past experiments, improve with more data arriving and to make predictions for single animals in future studies.","Feature extraction,
Mice,
Analytical models,
Lesions,
Data mining,
Diseases"
Student placement analyzer: A recommendation system using machine learning,"One of the biggest challenges that higher learning institutions face today is to improve the placement performance of students. The placement prediction is more complex when the complexity of educational entities increase. Educational institutes look for more efficient technology that assist better management and support decision making procedures or assist them to set new strategies. One of the effective ways to address the challenges for improving the quality is to provide new knowledge related to the educational processes and entities to the managerial system. With the machine learning techniques the knowledge can be extracted from operational and historical data that resides within the educational organization's databases using. The dataset for system implementation contains information about past data of students. These data are used for training the model for rule identification and for testing the model for classification. This paper presents a recommendation system that predicts the students to have one of the five placement statuses, viz., Dream Company, Core Company, Mass Recruiters, Not Eligible and Not Interested in Placements. This model helps the placement cell within an organization to identify the prospective students and pay attention to and improve their technical as well as interpersonal skills. Furthermore, the students in pre-final and final years of their B. Tech course can also use this system to know their individual placement status that they are most likely to achieve. With this they can put in more hardwork for getting placed in to the companies that belong to higher hierarchies.","Decision trees,
Companies,
Data mining,
Data models,
Logistics,
Predictive models,
Tools"
Agricultural production output prediction using Supervised Machine Learning techniques,"Farmers usually plan the cultivation process based on their previous experiences. Due to the lack of precise knowledge about cultivation, they end up cultivating undesirable crops. To help the farmers take decisions that can make their farming more efficient and profitable, the research tries to establish an intelligent information prediction analysis on farming in Bangladesh. However, this way of farming here is still at the initial stage. The research suggests area based beneficial crop rank before the cultivation process. It indicates the crops that are cost effective for cultivation for a particular area of land. To achieve these results, we are considering six major crops which are Aus rice, Aman rice, Boro rice, Potato, Jute and Wheat. The prediction is based on analyzing a static set of data using Supervised Machine Learning techniques. This static dataset contains previous years' data taken from the Yearbook of Agricultural Statistics and Bangladesh Agricultural Research Council of those crops according to the area. The research has an intent to use Decision Tree Learning-ID3 (Iterative Dichotomiser 3) and K-Nearest Neighbors Regression algorithms.","Agriculture,
Decision trees,
Prediction algorithms,
Classification algorithms,
Data mining,
Algorithm design and analysis,
Vegetation"
Machine Learning approaches on Diagnostic Term Encoding with the ICD for Clinical Documentation,"This work focuses on data mining applied to the clinical documentation domain. Diagnostic Terms (DTs) are used as keywords to retrieve valuable information from Electronic Health Records (EHRs). Indeed, they are encoded manually by experts following the International Classification of Diseases (ICD). The goal of this work is to explore the aid of text mining on DT encoding. From the machine learning (ML) perspective, this is a high-dimensional classification task, as it comprises thousands of codes. This work delves into a robust representation of the instances to improve ML results. The proposed system is able to find the right ICD-code among more than 1,500 possible ICD-codes with 92% precision for the main disease (primary class) and 88% for the main disease together with the non-essential modifiers (fully-specified class). The methodology employed is simple and portable. According to the experts from public hospitals, the system is very useful in particular for documentation and pharmaco-surveillance services. In fact, they reported an accuracy of 91.2% on a small randomly extracted test. Hence, together with this paper, we made the software publicly available in order to help the clinical and research community.","Standards,
Diseases,
Informatics,
Encoding,
Google,
Encyclopedias"
Microscopic Blood Smear Segmentation and Classification Using Deep Contour Aware CNN and Extreme Machine Learning,"Recent advancement in genomics technologies has opened a new realm for early detection of diseases that shows potential to overcome the drawbacks of manual detection technologies. In this work, we have presented efficient contour aware segmentation approach based based on fully conventional network whereas for classification we have used extreme machine learning based on CNN features extracted from each segmented cell. We have evaluated system performance based on segmentation and classification on publicly available dataset. Experiment was conducted on 64000 blood cells and dataset is divided into 80% for training and 20% for testing. Segmentation results are compared with the manual segmentation and found that proposed approach provided with 98.12% and 98.16% for RBC and WBC respectively whereas classification accuracy is shown on publicly available dataset 94.71% and 98.68% for RBC & its abnormalities detection and WBC respectively.","Blood,
Image segmentation,
Feature extraction,
Shape,
Diseases,
Image color analysis,
Microscopy"
Multiobjective fuzzy genetics-based machine learning based on MOEA/D with its modifications,"Various evolutionary multiobjective optimization (EMO) algorithms have been used in the field of evolutionary fuzzy systems (EFS), because EMO algorithms can easily handle multiple objective functions such as the accuracy maximization and complexity minimization for fuzzy system design. Most EMO algorithms used in EFS are Pareto dominance-based algorithms such as NSGA-II, SPEA2, and PAES. There are a few studies where other types of EMO algorithms are used in EFS. In this paper, we apply a multiobjective evolutionary algorithm based on decomposition called MOEA/D to EFS for fuzzy classifier design. MOEA/D is one of the most well-known decomposition-based EMO algorithms. The key idea is to divide a multiobjective optimization problem into a number of single-objective problems using a set of uniformly distributed weight vectors in a scalarizing function. We propose a new scalarizing function called an accuracy-oriented function (AOF) which is specialized for classifier design. We examine the effects of using AOF in MOEA/D on the search ability of our multiobjective fuzzy genetics-based machine learning (GBML). We also examine the synergy effect of MOEA/D with AOF and parallel distributed implementation of fuzzy GBML on the generalization ability.","Training data,
Error analysis,
Algorithm design and analysis,
Classification algorithms,
Computational modeling,
Data models,
Optimization"
Even More Confident Predictions with Deep Machine-Learning,"Confidence measures aim at discriminating unreliable disparities inferred by a stereo vision system from reliable ones. A common and effective strategy adopted by most top-performing approaches consists in combining multiple confidence measures by means of an appropriately trained random-forest classifier. In this paper, we propose a novel approach by training an n-channel convolutional neural network on a set of feature maps, each one encoding the outcome of a single confidence measure. This strategy enables to move the confidence prediction problem from the conventional 1D feature maps domain, adopted by approaches based on random-forests, to a more distinctive 3D domain, going beyond single pixel analysis. This fact, coupled with a deep network appropriately trained on a small subset of images, enables to outperform top-performing approaches based on random-forests.","Feature extraction,
Proposals,
Atmospheric measurements,
Particle measurements,
Training,
Three-dimensional displays,
Reliability"
Machine Learning Model for Event-Based Prognostics in Gas Circulator Condition Monitoring,"Gas circulator (GC) units are an important rotating asset used in the advanced gas-cooled reactor design, facilitating the flow of CO_2
gas through the reactor core. The ongoing maintenance and examination of these machines are important for operators in order to maintain safe and economic generation. GCs experience a dynamic duty cycle with periods of nonsteady state behavior at regular refueling intervals, posing a unique analysis problem for reliability engineers. In line with the increased data volumes and sophistication of available technologies, the investigation of predictive and prognostic measurements has become a central interest in rotating asset condition monitoring. However, many of the state-of-the-art approaches finding success deal with the extrapolation of stationary time series feeds, with little to no consideration of more complex but expected events in the data. In this paper, we demonstrate a novel modeling approach for examining refueling behaviors in GCs, with a focus on estimating their health state from vibration data. A machine learning model was constructed using the operational history of a unit experiencing an eventual inspection-based failure. This new approach to examining GC condition is shown to correspond well with explicit remaining useful life measurements of the case study, improving on the existing rudimentary extrapolation methods often employed in rotating machinery health monitoring.","Monitoring,
Inductors,
Vibrations,
Circulators,
Condition monitoring,
Reliability engineering"
Applying machine learning techniques to recognize arc in vehicle 48 electrical systems,"Implementing higher voltages in vehicles like 48V mild hybrid or full-hybrid enables CO2 reduction and weight savings. However, the increase in the voltage demands an accurate and robust protection system again potential fault conditions. Series arc is one of the fault conditions which needs to be detected and addressed before the benefits of using higher voltages in vehicle can be fully realized. In this paper, an efficient arc detection algorithm is presented based on advanced machine learning algorithms to recognize series arcs in the electrical systems of 48V vehicles. The results demonstrate that the proposed algorithm recognizes the arcs in the signal with accuracy of more than 99% detection. The algorithm only uses measured current and therefore, no additional wiring will be added to the system.","Feature extraction,
Training,
Machine learning algorithms,
Testing,
Algorithm design and analysis,
Frequency-domain analysis,
Prediction algorithms"
A Machine-Learning Based Connectivity Model for Complex Terrain Large-Scale Low-Power Wireless Deployments,"We evaluate the accuracy of a machine-learning-based path loss model trained on 42,157,324 RSSI samples collected over one year from an environmental wireless sensor network using 2.4 GHz radios. The 2218 links in the network span a 2000 km² basin and are deployed in a complex environment, with large variations of terrain attributes and vegetation coverage. Four candidate machine-learning algorithms were evaluated in order to find the one with lowest error: Random Forest, Adaboost, Neural Networks, and K-Neareast-Neighbors. Of the candidate models, Random Forest showed the lowest error. The independent variables used in the model include path distance, canopy coverage, terrain variability, and path angle. We compare the accuracy of this model to several well-known canonical (Free Space, plane earth) and empirical propagation models (Weissberger, ITU-R, COST235). Unlike canonical models, machine-learning algorithms are not problem-specific: they rely on an extensive dataset and a flexible model architecture to make predictions. We show how this model achieves a 37% reduction in the average prediction error compared to the canonical/empirical model with the best performance. The article presents a in-depth discussion on the strengths and limitations of the proposed approach as well as opportunities for further research.","Sensors,
Wireless sensor networks,
Mathematical model,
Predictive models,
Wireless communication,
Vegetation mapping,
Snow"
Predicting Workflow Task Execution Time in the Cloud using A Two-Stage Machine Learning Approach,"Many techniques such as scheduling and resource provisioning rely on performance prediction of workflow tasks for varying input data. However, such estimates are difficult to generate in the cloud. This paper introduces a novel two-stage machine learning approach for predicting workflow task execution times for varying input data in the cloud. In order to achieve high accuracy predictions, our approach relies on parameters reflecting runtime information and two stages of predictions. Empirical results for four real world workflow applications and several commercial cloud providers demonstrate that our approach outperforms existing prediction methods. In our experiments, our approach respectively achieves a best-case and worst-case estimation error of 1.6% and 12.2%, while existing methods achieved errors beyond 20% (for some cases even over 50%) in more than 75% of the evaluated workflow tasks. In addition, we show that the models predicted by our approach for a specific cloud can be ported with low effort to new clouds with low errors by requiring only a small number of executions.","Cloud computing,
Runtime,
Hardware,
Computational modeling,
Predictive models,
Analytical models"
Comparison of machine learning algorithms for soil type classification,"Machine learning algorithm can be applied for automating soil type classification. This paper compares several machine learning algorithms for classifying soil type. Algorithms that involve support vector machine (SVM), neural network, decision tree, and naïve bayesian are proposed and assessed for this classification. Soil dataset is taken from the real data. Simulation is run by using RapidMiner Studio. The performance observed is the accuracy. The result shows that SVM, with the use of linear function kernel, outperforms the others algorithms. The SVM best accuracy is 82.35%.","Support vector machines,
Soil,
Decision trees,
Neural networks,
Machine learning algorithms,
Kernel,
Classification algorithms"
Probabilistic Regularized Extreme Learning Machine for Robust Modeling of Noise Data,"The extreme learning machine (ELM) has been extensively studied in the machine learning field and has been widely implemented due to its simplified algorithm and reduced computational costs. However, it is less effective for modeling data with non-Gaussian noise or data containing outliers. Here, a probabilistic regularized ELM is proposed to improve modeling performance with data containing non-Gaussian noise and/or outliers. While traditional ELM minimizes modeling error by using a worst-case scenario principle, the proposed method constructs a new objective function to minimize both mean and variance of this modeling error. Thus, the proposed method considers the modeling error distribution. A solution method is then developed for this new objective function and the proposed method is further proved to be more robust when compared with traditional ELM, even when subject to noise or outliers. Several experimental cases demonstrate that the proposed method has better modeling performance for problems with non-Gaussian noise or outliers.","Robustness,
Computational modeling,
Linear programming,
Integrated circuit modeling,
Probabilistic logic,
Reactive power"
Distributed Multi-User Computation Offloading for Cloudlet based Mobile Cloud Computing: A Game-Theoretic Machine Learning Approach,"In this paper, we investigate the problem of multi- user computation offloading for cloudlet based mobile cloud computing (MCC) in a multi-channel wireless contention environ- ment. The studied system is fully distributed so that each mobile device user can make the offloading decisions based only on its individual information, and without information exchange. We first formulate this multi-user computation offloading decision making problem as a noncooperative game. After analyzing the structural property of the formulated game, we show that it is an exact potential game, and has at least one pure-strategy Nash Equilibrium Point (NEP). To achieve the NEPs in a fully distributed environment, we propose a fully distributed compu- tation offloading (FDCO) algorithm based on machine learning technology. We then theoretically analyze the performance of the proposed FDCO algorithm in terms of the number of beneficial cloudlet computing mobile devices and the system-wide execution cost. Finally, simulation results validate the effectiveness of our proposed algorithm compared with counterparts.","Mobile handsets,
Cloud computing,
Games,
Wireless communication,
Mobile communication,
Machine learning algorithms,
Mobile applications"
UAV technology and machine learning techniques applied to the yield improvement in precision agriculture,"A model to estimate Nitrogen nutrition level in corn crops (Zea mays) is presented. The model was based on the information provided by multi-spectral cameras in four bands (red, green, blue and near-infrared (808 nm). The model was validated with ground truth information obtained by destructive methods. For training phase, three different fertilization levels of the crops were used (70, 140 y 210 kg · N · ha/sup -1/) with three repetitions in two stages of growing (V10 and earring). Unmanned Aerial Vehicle (UAV) technology was used. UAV quad-copter type flying 70 meters above the crops and machine learning techniques were used for the prediction stage. Results shown that the model can estimate nitrogen levels with 80% of precision with low cost technologies (multi-spectral cameras and UAVs). This proposal aims to optimize the fertilization since it actually is applied uniformly in the crops. The proposed scheme is focused on areas where the nitrogen is insufficient, avoiding the waste and reducing the impact on the environment.","Biological system modeling,
Agriculture,
Unmanned aerial vehicles,
Nitrogen,
Silicon,
Biomedical imaging,
Electronic mail"
Application of Machine Learning for Channel based Message Authentication in Mission Critical Machine Type Communication,"The design of robust wireless communication systems for industrial applications such as closed loop control processes has been considered manifold recently. Additionally, the ongoing advances in the area of connected mobility have similar or even higher requirements regarding system reliability and availability. Beside unfulfilled reliability requirements, the availability of a system can further be reduced, if it is under attack in the sense of violation of information security goals such as data authenticity or integrity. In order to guarantee the safe operation of an application, a system has at least to be able to detect these attacks. Though there are numerous techniques in the sense of conventional cryptography in order to achieve that goal, these are not always suited for the requirements of the applications mentioned due to resource inefficiency. In the present work, we show how the goal of message authenticity based on physical layer security (PHYSEC) can be achieved. The main idea for such techniques is to exploit user specific characteristics of the wireless channel, especially in spatial domain. Additionally, we show the performance of our machine learning based approach and compare it with other existing approaches.",
Dissolved Gas Analysis Interpretation and Intelligent Machine Learning Techniques,,
Design of Reliable SoCs With BIST Hardware and Machine Learning,"In this paper, a novel framework is presented for designing lifetime-reliable SoCs with self-adaptation capability against aging-induced degradation. The proposed flow utilizes the existing logic built-in-self-test (LBIST) hardware, and software implemented machine learning predictor to activate appropriate countermeasures to remedy the wear out in the field. Using an innovative method, we convert ATPG-generated transition delay test patterns into LBIST patterns to activate high-usage critical/near-critical paths in-field, and the corresponding responses are utilized in developing the predictor. A gate-overlap and path-delay-aware algorithm selects the optimum set of patterns. The area and test time overhead for the framework are very low. We implemented our proposed flow on SoC benchmark designs, and the results demonstrated its efficacy.","Aging,
Degradation,
Reliability,
Delays,
Hardware,
Clocks"
Improved variations for Extreme Learning Machine: Space embedded ELM and optimal distribution ELM,"Due to the simplicity of its implementation and the impressive performance, Extreme Learning Machine (ELM) has been widely used in applications of machine learning. However, there are two potential problems in ELM: 1) lack of an efficient method for minimizing error; 2) consideration of little inherent structural information about correlations among output components. To overcome those problems, this paper proposes two improvements of ELM: Optimal Distribution Extreme Learning Machine (OD-ELM) and Space Embedded Extreme Learning Machine (SE-ELM). Based on our recent discovery that the distributions of the input weights and the bias of hidden nodes in ELM play an important role in the performance of ELM, OD-ELM can reduce the training error by the usage of the derivatives of training error w.r.t the distributions. Simulation results tested on the UCI dataset demonstrate and verify that OD-ELM has better generalization performance than traditional ELM. SE-ELM can `embed' the inherent structural information among outputs into the predictor. SE-ELM captures not only the interdependencies between variables, as in a typical ELM, but also those responses, so correlations among both inputs and outputs are considered. Meanwhile, SE-ELM retains some characteristics of ELM, such as the simplicity of implementation and the hidden layer without tuning. We examine the three methods of embedding on HumanEva benchmark, which is a well-known benchmark about 3D human pose reconstruction. As verified by the simulation results, SE-ELM tends to have better generalization performance than classical ELM.","Kernel,
Training,
Simulation,
Optimization,
Electronic mail,
Correlation,
Benchmark testing"
A Machine Learning Approach for Tracking and Predicting Student Performance in Degree Programs,"Accurately predicting students' future performance based on their ongoing academic records is crucial for effectively carrying out necessary pedagogical interventions to ensure students' on-time and satisfactory graduation. Although there is a rich literature on predicting student performance when solving problems or studying for courses using data-driven approaches, predicting student performance in completing degrees (e.g., college programs) is much less studied and faces new challenges: (1) Students differ tremendously in terms of backgrounds and selected courses; (2) courses are not equally informative for making accurate predictions; and (3) students' evolving progress needs to be incorporated into the prediction. In this paper, we develop a novel machine learning method for predicting student performance in degree programs that is able to address these key challenges. The proposed method has two major features. First, a bilayered structure comprising multiple base predictors and a cascade of ensemble predictors is developed for making predictions based on students' evolving performance states. Second, a data-driven approach based on latent factor models and probabilistic matrix factorization is proposed to discover course relevance, which is important for constructing efficient base predictors. Through extensive simulations on an undergraduate student dataset collected over three years at University of California, Los Angeles, we show that the proposed method achieves superior performance to benchmark approaches.","Prediction algorithms,
Signal processing algorithms,
Recommender systems,
Education,
Complexity theory,
Probabilistic logic,
Machine learning algorithms"
Photorealistic Monocular Gaze Redirection Using Machine Learning,"We propose a general approach to the gaze redirection problem in images that utilizes machine learning. The idea is to learn to re-synthesize images by training on pairs of images with known disparities between gaze directions. We show that such learning-based re-synthesis can achieve convincing gaze redirection based on monocular input, and that the learned systems generalize well to people and imaging conditions unseen during training.","Real-time systems,
Training,
Cameras,
Teleconferencing,
Face,
Magnetic heads"
Strong subthreshold current array PUF with 265 challenge-response pairs resilient to machine learning attacks in 130nm CMOS,"This paper presents a strong silicon physically unclonable function (PUF) immune to machine learning (ML) attacks. The PUF, termed the subthreshold current array (SCA) PUF, is composed of a pair of two-dimensional transistor arrays and a low-offset comparator. The fabricated PUF chip allows 265 challenge-response pairs (CRPs) and achieves high reliability with average bit error rate (BER) of 5.8% for temperatures -20 to 80°C and Vdd + 10%. The calibration-based CRPs filtering method effectively improves BER to 2.6% with a 10% loss of CRPs. When subjected to ML attacks, the PUF shows resilience that is 100X higher than known alternatives, with negligible loss in PUF unpredictability.","Transistors,
High definition video,
Current measurement,
Integrated circuit reliability,
Resilience,
Voltage measurement"
Information Dilution: Granule-Based Information Hiding in Table Data - A Case of Lenses Data Set in UCI Machine Learning Repository,"We reconsider information dilution, which was originally proposed in [4]. This adds the noises to the value in table data in order to hide the actual value, namely information dilution is a kind of information hiding in table data. In this paper, we pick up lenses data set φ in UCI machine learning repository, and we dilute this data set φ with preserving obtainable rules.","Nickel,
Lenses,
Generators,
Electronic mail,
Rough sets,
Algorithm design and analysis,
Information systems"
A user-centric machine learning framework for cyber security operations center,"To assure cyber security of an enterprise, typically SIEM (Security Information and Event Management) system is in place to normalize security events from different preventive technologies and flag alerts. Analysts in the security operation center (SOC) investigate the alerts to decide if it is truly malicious or not. However, generally the number of alerts is overwhelming with majority of them being false positive and exceeding the SOC's capacity to handle all alerts. Because of this, potential malicious attacks and compromised hosts may be missed. Machine learning is a viable approach to reduce the false positive rate and improve the productivity of SOC analysts. In this paper, we develop a user-centric machine learning framework for the cyber security operation center in real enterprise environment. We discuss the typical data sources in SOC, their work flow, and how to leverage and process these data sets to build an effective machine learning system. The paper is targeted towards two groups of readers. The first group is data scientists or machine learning researchers who do not have cyber security domain knowledge but want to build machine learning systems for security operations center. The second group of audiences are those cyber security practitioners who have deep knowledge and expertise in cyber security, but do not have machine learning experiences and wish to build one by themselves. Throughout the paper, we use the system we built in the Symantec SOC production environment as an example to demonstrate the complete steps from data collection, label creation, feature engineering, machine learning algorithm selection, model performance evaluations, to risk score generation.","Learning systems,
Data models,
Predictive models,
Computer security,
Machine learning algorithms,
Mathematical model"
Machine learning or discrete choice models for car ownership demand estimation and prediction?,"Discrete choice models are widely used to explain transportation behaviors, including a household's decision to own a car. They show how some distinct choice of human behavior or preference influences a decision. They are also used to project future demand estimates to support policy exploration. This latter use for prediction is indirectly aligned with and conditional to the model's estimation which aims to fit the observed data. In contrast, machine learning models are derived to maximize prediction accuracy through mechanisms such as out-of-sample validation, non-linear structure, and automated covariate selection, albeit at the expense of interpretability and sound behavioral theory. We investigate how machine learning models can outperform discrete choice models for prediction of car ownership using transportation household survey data from Singapore. We compare our household car ownership model (multinomial logit model) against various machine learning models (e.g. Random Forest, Support Vector Machines) by using 2008 data to derive, i.e. estimate models that we then use to predict 2012 ownership. The machine learning models are inferior to the discrete choice model when using discrete choice features. However, after engineering features more appropriate for machine learning they are superior. These results highlight both the cost of applying machine learning models in econometric contexts and an opportunity for improved prediction and better urban policy making through machine learning models with appropriate features.","Biological system modeling,
Predictive models,
Automobiles,
Data models,
Estimation,
Support vector machines"
Wavelet transform and unsupervised machine learning to detect insider threat on cloud file-sharing,"As increasingly more enterprises are deploying cloud file-sharing services, this adds a new channel for potential insider threats to company data and IPs. In this paper, we introduce a two-stage machine learning system to detect anomalies. In the first stage, we project the access logs of cloud file-sharing services onto relationship graphs and use three complementary graph-based unsupervised learning methods: OddBall, PageRank and Local Outlier Factor (LOF) to generate outlier indicators. In the second stage, we ensemble the outlier indicators and introduce the discrete wavelet transform (DWT) method, and propose a procedure to use wavelet coefficients with the Haar wavelet function to identify outliers for insider threat. The proposed system has been deployed in a real business environment, and demonstrated effectiveness by selected case studies.","Discrete wavelet transforms,
Time series analysis,
Learning systems,
Machine learning algorithms,
Wavelet analysis"
Distributed Extreme Learning Machine with Kernels Based on Mapreduce for Spectral-Spatial Classification of Hyperspectral Image,"ELM with kernels and MapReduce have an unparalleled advantage of other similar technologies, which attract widely attention in machine learning and distributed data processing communities respectively. In this paper, we combine the advantage of ELM with kernels and MapReduce, and propose a Distributed Extreme Learning Machine with kernels based on MapReduce framework (DK-ELMM),which makes full use of the parallel computing ability of MapReduce framework and realizes efficient learning of large-scale training data. In particular, we present a spectral-spatial DELMM-based classifier for hyperspectral remote sensing images that integrates the information provided by extended morphological profiles. The proposed spectral-spatial classifier allows different weights for both (spatial and spectral) features outperforming other ELM-based classifiers in terms of accuracy for land cover applications. The accuracy classification results are also better than those obtained by equivalent spectral-spatial SVM-based classifiers.","Hyperspectral imaging,
Kernel,
Training,
Training data,
Image classification,
Distributed databases"
Modeling tactical lane-change behavior for automated vehicles: A supervised machine learning approach,"In recent years, due to growing interest in automated driving, the need for better understanding the humans driving behavior, and particularly the lane changing and car following behavior, has further increased. Despite its great importance, lane changing has not been studied as extensively as longitudinal behavior and remains one of the most challenging driving behavior maneuvers to understand and to predict. Drivers take into account many factors while making a tactical decision, which cannot be precisely represented by the conventional rule-based models. In this paper, we compare the results of different supervised machine learning classifiers to better understand the lane change decision of drivers using the NGSIM database. For this aim, after choosing the relevant features, the ones which contribute the most to the model were chosen with the help of feature importance analysis. Afterward, the training dataset was used to train the model with naive Bayes, support vector machines, logic regression, nearest neighborhoods, decision trees, extra trees and random forest classifiers. The accuracy of predictions for test dataset indicates that extra trees classifier, decision trees and random forest had the best performance in predicting the lane change decisions of human drivers.","Vehicles,
Acceleration,
Logistics,
Decision trees,
Decision making,
Data models,
Feature extraction"
Forecasting Malaysian exchange rate using machine learning techniques based on commodities prices,"This article investigates the dynamic interactions between four commodities prices and the exchange rate for an emerging economy, Malaysia. The literature has identified a series of contradictory claims in the support and against the accurate prediction of the exchange rate. This article provides a new methodology to perform a comparative analysis of the three machine learning techniques, namely: Support Vector Machine, Neural Networks, and RandomForest. The experimental results demonstrate that the RandomForest is comparatively better than Support Vector Machine and Neural Networks, for accuracy and performance. This shows that the fluctuation in the Malaysian exchange rate can be evaluated accurately using RandomForest as compare to other techniques. Furthermore, this paper reveals that Malaysian specific commodities prices-crude oil, palm oil, rubber, and gold, are the strong dynamic parameters that influence Malaysian exchange rate. Hence, these results are beneficial for policy making, investment modeling, and corporate planning.","Exchange rates,
Support vector machines,
Forecasting,
Neural networks,
Oils,
Biological system modeling,
Rubber"
Automation of a robotic cell using machine learning algorithms and internet of things,"In recent years the imagen processing is an envolving issue, allows a great variety of applications. This article describe the process of automation of a conveyor belt using a machine learning algorithms that is able to immediately recognize each type different electronic board. For image processing a webcam is used. The obtained information is sent a free server of a web page by a WiFi module. Is obtained an automatic and intelligent process of low cost of robotic cell.","Robots,
Wireless fidelity,
MATLAB,
Automation,
Machine learning algorithms,
Internet of Things"
Machine translation using deep learning: An overview,"This Paper reveals the information about Deep Neural Network (DNN) and concept of deep learning in field of natural language processing i.e. machine translation. Now day's DNN is playing major role in machine leaning technics. Recursive recurrent neural network (R2NN) is a best technic for machine learning. It is the combination of recurrent neural network and recursive neural network (such as Recursive auto encoder). This paper presents how to train the recurrent neural network for reordering for source to target language by using Semi-supervised learning methods. Word2vec tool is required to generate word vectors of source language and Auto encoder helps us in reconstruction of the vectors for target language in tree structure. Results of word2vec play an important role in word alignment of the input vectors. RNN structure is very complicated and to train the large data file on word2vec is also a time-consuming task. Hence, a powerful hardware support (GPU) is required. GPU improves the system performance by decreasing training time period.","Artificial neural networks,
History,
Binary trees,
Training,
Predictive models,
Recurrent neural networks"
Phishing detection: A recent intelligent machine learning comparison based on models content and features,"In the last decade, numerous fake websites have been developed on the World Wide Web to mimic trusted websites, with the aim of stealing financial assets from users and organizations. This form of online attack is called phishing, and it has cost the online community and the various stakeholders hundreds of million Dollars. Therefore, effective counter measures that can accurately detect phishing are needed. Machine learning (ML) is a popular tool for data analysis and recently has shown promising results in combating phishing when contrasted with classic anti-phishing approaches, including awareness workshops, visualization and legal solutions. This article investigates ML techniques applicability to detect phishing attacks and describes their pros and cons. In particular, different types of ML techniques have been investigated to reveal the suitable options that can serve as anti-phishing tools. More importantly, we experimentally compare large numbers of ML techniques on real phishing datasets and with respect to different metrics. The purpose of the comparison is to reveal the advantages and disadvantages of ML predictive models and to show their actual performance when it comes to phishing attacks. The experimental results show that Covering approach models are more appropriate as anti-phishing solutions, especially for novice users, because of their simple yet effective knowledge bases in addition to their good phishing detection rate.","Predictive models,
Classification algorithms,
Training,
Artificial neural networks,
Tools,
Security,
Decision trees"
ESP: A Machine Learning Approach to Predicting Application Interference,"Independent applications co-scheduled on the same hardware will interfere with one another, affecting performance in complicated ways. Predicting this interference is key to efficiently scheduling applications on shared hardware, but forming accurate predictions is difficult because there are many shared hardware features that could lead to the interference. In this paper we investigate machine learning approaches (specifically, regularization) to understand the relation between those hardware features and application interference. We propose ESP, a highly accurate and fast regularization technique for application interference prediction. To demonstrate this practicality, we implement ESP and integrate it into a scheduler for both single and multi-node Linux/x86 systems and compare the scheduling performance to state-of-the-art heuristics. We find that ESP-based schedulers increase throughput by 1.25-1.8× depending on the scheduling scenario. Additionally, we find that ESP's accurate predictions allow schedulers to avoid catastrophic decisions, which heuristic approaches fundamentally cannot detect.","Interference,
Predictive models,
Mathematical model,
Linear regression,
Hardware,
Computational modeling,
Data models"
Modeling bike availability in a bike-sharing system using machine learning,"This paper models the availability of bikes at San Francisco Bay Area Bike Share stations using machine learning algorithms. Random Forest (RF) and Least-Squares Boosting (LSBoost) were used as univariate regression algorithms, and Partial Least-Squares Regression (PLSR) was applied as a multivariate regression algorithm. The univariate models were used to model the number of available bikes at each station. PLSR was applied to reduce the number of required prediction models and reflect the spatial correlation between stations in the network. Results clearly show that univariate models have lower error predictions than the multivariate model. However, the multivariate model results are reasonable for networks with a relatively large number of spatially correlated stations. Results also show that station neighbors and the prediction horizon time are significant predictors. The most effective prediction horizon time that produced the least prediction error was 15 minutes.","Predictive models,
Radio frequency,
Autoregressive processes,
Prediction algorithms,
Bicycles,
Data models,
Meteorology"
Sentiment analysis of student feedback using machine learning and lexicon based approaches,"This paper presents a combination of machine learning and lexicon-based approaches for sentiment analysis of students feedback. The textual feedback, typically collected towards the end of a semester, provides useful insights into the overall teaching quality and suggests valuable ways for improving teaching methodology. The paper describes a sentiment analysis model trained using TF-IDF and lexicon-based features to analyze the sentiments expressed by students in their textual feedback. A comparative analysis is also conducted between the proposed model and other methods of sentiment analysis. The experimental results suggest that the proposed model performs better than other methods.","Sentiment analysis,
Feature extraction,
Training,
Analytical models,
Algorithm design and analysis,
Dictionaries,
Measurement"
Playback detection using machine learning with spectrogram features approach,"This paper presents 2D image processing approach to playback detection in automatic speaker verification (ASV) systems using spectrograms as speech signal representation. Three feature extraction and classification methods: histograms of oriented gradients (HOG) with support vector machines (SVM), HAAR wavelets with AdaBoost classifier and deep convolutional neural networks (CNN) were compared on different data partitions in respect of speakers or playback devices: for instance with different speakers in training and test subsets. The playback detection systems were trained and tested on two speech datasets S1 and S2 manufactured independently by two different institutions. The test error for both datasets oscillates about the level of 1% for HOG+SVM and even below it for CNN in bigger S1 base. In cross validation scenario in which one base was used for training and second base for the test the results were very poor what suggests that the information relevant for playback detection appeared in each base in different way.","Spectrogram,
Feature extraction,
Support vector machines,
Training,
Histograms,
Speech"
Wheel Defect Detection With Machine Learning,"Wheel defects on railway wagons have been identified as an important source of damage to the railway infrastructure and rolling stock. They also cause noise and vibration emissions that are costly to mitigate. We propose two machine learning methods to automatically detect these wheel defects, based on the wheel vertical force measured by a permanently installed sensor system on the railway network. Our methods automatically learn different types of wheel defects and predict during normal operation if a wheel has a defect or not. The first method is based on novel features for classifying time series data and it is used for classification with a support vector machine. To evaluate the performance of our method we construct multiple data sets for the following defect types: flat spot, shelling, and non-roundness. We outperform classical defect detection methods for flat spots and demonstrate prediction for the other two defect types for the first time. Motivated by the recent success of artificial neural networks for image classification, we train custom artificial neural networks with convolutional layers on 2-D representations of the measurement time series. The neural network approach improves the performance on wheels with flat spots and non-roundness by explicitly modeling the multi sensor structure of the measurement system through multiple instance learning and shift invariant networks.","Wheels,
Rail transportation,
Learning systems,
Force measurement,
Wavelet transforms,
Strain measurement,
Vibrations"
Machine learning approach for breast cancer localization,"Machine learning has been widely used for solving various classification problems in biomedical research field due to its strength in handling massive data set systematically. In this paper, a feasible way of breast cancer localization via machine learning is presented with a preliminary result for 10 cancerous breast tissue samples (600 μm diameter and 8 μm thickness). Using a custom-built microscope-compatible microindentation system, 500 indentation points per sample were indented to a depth of 2 μm at 20 μm interval. Each indentation point is labeled as either `Normal' or `Cancerous' according to the corresponding pathological image that was annotated appropriately by a certified pathologist. We applied the support vector machine (SVM) algorithm, which is one of the supervised machine learning technique to validate the annotations by a pathologist. The tissue elasticity value which is the actual data set for machine learning is locally determined by a non-linear contact model for a spherical tip, using the contact force and the indentation depth information collected during the indentation experiment. With soft-margin SVM which is for not linearly separable data, each tissue sample is tested while the other 9 tissue samples are used for training like 10-fold cross-validation. Classification accuracy for the entire breast tissue samples was obtained as 76.20% ± 9.28%, which shows that this is a promising approach when making allowance for classification with a single parameter.","Breast tissue,
Support vector machines,
Breast cancer,
Elasticity,
Force"
Comparison of expert algorithms with machine learning models for real estate appraisal,Machine learning models require numerous training examples to provide reliable predictions of real estate prices. Expert algorithms could be applied wherever only several training samples are available. The accuracy of two expert algorithms based on the sales comparison approach was experimentally examined using real-world data derived from a cadastral system and registry of real estate transactions. The performance of the algorithms was compared with three data driven regression models for property valuation. Statistical analysis of the obtained results was conducted.,"Machine learning algorithms,
Prediction algorithms,
Algorithm design and analysis,
Computational modeling,
Data models,
Cost accounting,
Predictive models"
Machine learning for space communications service management tasks,"Currently, NASA space communications links are individually scheduled by each mission's operations personnel coordinating with the network service providers. The scheduling of communications services typically takes place many days in advance of when the service is needed. This suffices because there are only several dozen mission platforms, using point-to-point communications, and generally in nominal operating modes. In the future, with potentially many more active flight platforms, more complex relaying or internetworking, and more emphasis on quality of service for different types of data, network service management will increase in difficulty. Scheduling and other service management activities could grow more labor intensive and costly for both mission operations and communication service provider staff. In order to enable scale up the communications services, while reducing human involvement, this paper describes our work applying machine learning techniques to implement intelligent routing that addresses space communications service management challenges. There are precedents for similar problems in terrestrial networking, and the main contribution of this paper is in extending to the unique aspects of space communications. Successful application of machine learning can assist in automation of both current space communications and future space internetworking service management activities, including pre-service planning, provisioning of acquisition data, in-service performance monitoring, real-time service control, and identification of anomalies or other contingency modes. This paper includes description of some relevant problems, existing machine learning approaches to similar problems, and description of initial evaluations using network emulation.","learning (artificial intelligence),
quality of service,
space communication links,
telecommunication computing,
telecommunication network management"
Stacking and rotation-based technique for machine learning classification with data reduction,"The paper focuses on using stacking and rotation-based technique to improve performance and generalization ability of the machine learning classification with data reduction. The aim of data reduction technique is decreasing the quantity of information required to learn a high quality classifiers, especially when the data are huge. The paper shows that merging both stacking and rotation-based ensemble techniques with machine classification based on data reduction may bring additional benefits with respect to the accuracy of the classification process. The finding that has been confirmed by computational experiments. The paper includes the description of the approach and the discussion of the computational experiment results.","Feature extraction,
Prototypes,
Stacking,
Principal component analysis,
Machine learning algorithms,
Clustering algorithms,
Training"
Machine learning for additive manufacturing of electronics,"Quality of electronic products fabricated with additive manufacturing (AM) techniques such as 3D inkjet printing can be assured by adopting pro-active predictive models for process condition monitoring instead of using conventional post-manufacture assessment techniques. This paper details a model-based approach, and associated machine learning algorithms, which can be used to achieve and maintain optimal product quality during production runs and to realise model predictive process control (MPC). The investigated data-driven prognostics based on state-space modelling of the dynamic behaviour of 3D inkjet printing for electronics manufacturing is new and makes it an original contribution. 3D printing of conductive lines for electronic circuits is a main targeted application, and is used to demonstrate and validate the prognostics capability of machine learning models developed from measured process data. The results show that, for moderately non-linear dynamics of the 3D-Printing process, state-space models can inform on the expected process trends (states) and related product quality characteristics even over large prediction horizons. The models can also support the realisation of model predictive process control for optimal target performance.","Temperature measurement,
State-space methods,
Predictive models,
Three-dimensional displays,
Three-dimensional printing,
Consumer electronics"
Robust kernel-based machine learning localization using NLOS TOAs or TDOAs,"A robust kernel-based machine learning localization scheme using time of arrival (TOA) or time difference of arrival (TDOA) in none-line-of-sight (NLOS) environments is proposed. The scheme can provide accurate position estimation while the reference nodes are coarsely and randomly distributed in the area of interests. Moreover, the scheme is insensitive with respect to random TOA synchronization and measurement errors.","Fingerprint recognition,
Kernel,
Surface waves,
Transmitters,
Robustness,
Databases,
Earth"
"Understanding and personalising smart city services using machine learning, The Internet-of-Things and Big Data","This paper explores the potential of Machine Learning (ML) and Artificial Intelligence (AI) to lever Internet of Things (IoT) and Big Data in the development of personalised services in Smart Cities. We do this by studying the performance of four well-known ML classification algorithms (Bayes Network (BN), Naïve Bayesian (NB), J48, and Nearest Neighbour (NN)) in correlating the effects of weather data (especially rainfall and temperature) on short journeys made by cyclists in London. The performance of the algorithms was assessed in terms of accuracy, trustworthy and speed. The data sets were provided by Transport for London (TfL) and the UK MetOffice. We employed a random sample of some 1,800,000 instances, comprising six individual datasets, which we analysed on the WEKA platform. The results revealed that there were a high degree of correlations between weather-based attributes and the Big Data being analysed. Notable observations were that, on average, the decision tree J48 algorithm performed best in terms of accuracy while the kNN IBK algorithm was the fastest to build models. Finally we suggest IoT Smart City applications that may benefit from our work.","Meteorology,
Classification algorithms,
Training,
Smart cities,
Big Data,
Artificial intelligence,
Bayes methods"
Machine learning for large scale manufacturing data with limited information,"Improving the efficiency of the production plants has always been focus of manufacturing industry. Recently the utilization of data analytics tool is dramatically increased since these methods bring new insights into already existing data. Nevertheless, manufacturing industry in general is still reluctant to make the data available to researcher due to privacy issues. One such example is the challenge sponsored by Bosch and run by kaggle.com, where anonymized data was made available to data scientist with very limited description. In this work, we present our solution to the Bosch assembly line performance challenge, specifically in respect to dealing with raw big data without detailed explanation. The data science methods applied were used to successfully predict internal failures along the assembly lines, although no details of the structure and line description was available.","Feature extraction,
Correlation,
Error analysis,
Sorting,
Companies,
Production,
Data science"
Deep Learning of Semi-supervised Process Data with Hierarchical Extreme Learning Machine and Soft Sensor Application,"Data-driven soft sensors have been widely utilized in indus-trial processes to estimate the critical quality variables which are intractable to directly measure online through physical devices. Due to the low sampling-rate of quality variables, most of the soft sensors are developed on small number of labeled samples and the large number of unlabeled process data are discarded. The loss of information greatly limits the improvement of quality prediction accuracy. One of the main issues of data-driven soft sensor is to furthest exploit the information contained in all available process data. This paper proposes a semi-supervised deep learning model for soft sensor development based on the hierarchical extreme learning machine (HELM). Firstly, the deep network struc-ture of auto-encoders (AE) is implemented for unsupervised feature extraction with all the process samples. Then extreme learning machine (ELM) is utilized for regression through appending the quality variable. Meanwhile, the manifold regularization method is introduced for semi-supervised model training. The new method can not only deeply extract the information that the data contains, but learn more from the extra unlabeled samples as well. The proposed semi-supervised HELM method is applied in a High-low Transformer to estimate the CO content, which shows a significant improvement of the prediction accuracy, com-pared to traditional methods.","Feature extraction,
Training,
Machine learning,
Artificial neural networks,
Neurons,
Process control,
Biological neural networks"
Speeding Up Distributed Machine Learning Using Codes,"Codes are widely used in many engineering applications to offer robustness against noise. In large-scale systems there are several types of noise that can affect the performance of distributed machine learning algorithms – straggler nodes, system failures, or communication bottlenecks – but there has been little interaction cutting across codes, machine learning, and distributed systems. In this work, we provide theoretical insights on how coded solutions can achieve significant gains compared to uncoded ones. We focus on two of the most basic building blocks of distributed learning algorithms: matrix multiplication and data shuffling. For matrix multiplication, we use codes to alleviate the effect of stragglers, and show that if the number of homogeneous workers is n, and the runtime of each subtask has an exponential tail, coded computation can speed up distributed matrix multiplication by a factor of log n. For data shuffling, we use codes to reduce communication bottlenecks, exploiting the excess in storage. We show that when a constant fraction α of the data matrix can be cached at each worker, and n is the number of workers, coded shuffling reduces the communication cost by a factor of (α+1/n)γ(n) compared to uncoded shuffling, where γ(n) is the ratio of the cost of unicasting n messages to n users to multicasting a common message (of the same size) to n users. For instance, γ(n)≃n if multicasting a message to n users is as cheap as unicasting a message to one user. We also provide experiment results, corroborating our theoretical gains of the coded algorithms.","Machine learning algorithms,
Multicast communication,
Distributed databases,
Algorithm design and analysis,
Encoding,
Runtime,
Robustness"
A Machine Learning Based ETA Estimator for WiFi Transmissions,"Recent advancements related to Device to Device (D2D) communication make it possible for a transmitting node to dynamically select the interface to be used for data transfers locally, without traversing any network infrastructure. In this scenario, a controller is identified, whose goal is to manage the D2D connection after its establishment. The Software Defined Networking (SDN) paradigm makes it possible to select this controller node via software: a device becomes the master node of a WiFi-Direct network, whereas the remaining units, i.e., the clients, can exchange data with other devices through the master. This paper develops a machine learning based prediction algorithm for the aforementioned scenario, in which multiple elements, while receiving data from the controller, require an accurate on-the-fly estimation of the remaining transmission time, i.e., the Estimated Time of Arrival (ETA). Different machine learning approaches are considered for this task, with the goal of exploiting only the information available at each client, without modifying any standard communication protocol. This information is critical when, for instance, a mobile user needs to decide whether or not to delay a data transfer, based on the load of the network and on the residual time under radio coverage from an access point.","Wireless fidelity,
Device-to-device communication,
Protocols,
Wireless communication,
Software,
Smart phones,
Quality of service"
Machine learning algorithms for impact localization on formed piezo metal composites,"Smart materials are of growing interest in research as well as industry due to its wide range of possible applications. Combined with a mass production enabled manufacturing of these materials, its application is further increased. A current research project addresses the integration of piezo ceramic elements into metal sheets to develop a smart touch-alike input device. The aim is to develop a low-cost embedded processing system that is able to compute the origin of impacts affecting the material surface. In this paper, the usability of machine learning algorithms to be used for adaptive impact detection and localization is evaluated. Data generated by suitable data pre-processing techniques is applied to neural networks, support vector machines and extreme learning machines for evaluation.","Training,
Sensors,
Machine learning algorithms,
Feature extraction,
Signal processing,
Metals,
Hardware"
Machine Learning Framework for the Detection of Mental Stress at Multiple Levels,"Mental stress has become a social issue and could become a cause of functional disability during routine work. In addition, chronic stress could implicate several psychophysiological disorders. For example, stress increases the likelihood of depression, stroke, heart attack, and cardiac arrest. The latest neuroscience reveals that the human brain is the primary target of mental stress, because the perception of the human brain determines a situation that is threatening and stressful. In this context, an objective measure for identifying the levels of stress while considering the human brain could considerably improve the associated harmful effects. Therefore, in this paper, a machine learning (ML) framework involving electroencephalogram (EEG) signal analysis of stressed participants is proposed. In the experimental setting, stress was induced by adopting a well-known experimental paradigm based on the montreal imaging stress task. The induction of stress was validated by the task performance and subjective feedback. The proposed ML framework involved EEG feature extraction, feature selection (receiver operating characteristic curve, t-test and the Bhattacharya distance), classification (logistic regression, support vector machine and naïve Bayes classifiers) and tenfold cross validation. The results showed that the proposed framework produced 94.6% accuracy for two-level identification of stress and 83.4% accuracy for multiple level identification. In conclusion, the proposed EEG-based ML framework has the potential to quantify stress objectively into multiple levels. The proposed method could help in developing a computer-aided diagnostic tool for stress detection.","Stress,
Electroencephalography,
Feature extraction,
Support vector machines,
Cardiac arrest,
Object recognition,
Psychology"
Evaluating machine learning algorithms for applications with humans in the loop,"Applications employing data classification such as smart lighting that involve human factors such as perception lead to non-deterministic input-output relationships where more than one output may be acceptable for a given input. For these so called non-deterministic multiple output classification (nDMOC) problems, the relationship between the input and output may change over time making it difficult for the machine learning (ML) algorithms in a batch setting to make predictions for a given context. In this paper, we describe the nature of nDMOC problems and discuss the Relevance Score (RS) that is suitable in this context as a performance metric. RS determines the extent by which a predicted output is relevant to the user's context and behaviors, taking into account the inconsistencies that come with human (perception) factors. We tailor the RS metric so that it can be used to evaluate ML algorithms in an online setting at run-time. We assess the performance of a number of ML algorithms, using a smart lighting dataset with non-deterministic one-to-many input-output relationships. The results indicate that using RS instead of classification accuracy (CA) is suitable to analyze the performance of conventional ML algorithms applied to the category of nDMOC problems. Instance-based online ML gives the best RS performance. An interesting finding is that the RS keeps increasing with increasing number of samples, even after the CA performance converges.","Measurement,
Prediction algorithms,
Lighting,
Machine learning algorithms,
Electronic mail,
Monitoring,
Mood"
A 3D multi-layer CMOS-RRAM accelerator for multi-layer machine learning,"Fast machine learning is required for future real-time data analytics. This paper introduces a 3D multi-layer CMOS-RRAM accelerator for learning on neural network. Given input of buffered data hold on the layer of a RRAM memory, intensive matrix-vector multiplication can be firstly accelerated on the layer of a digitized RRAM-crossbar. The remaining algorithmic operations such as feature extraction and classifier training can be accelerated on the layer of CMOS ASIC with consideration of parallelism and pipeline. Experiment results have shown that such a 3D accelerator can significantly reduce training time with acceptable accuracy. Compared to 3D-CMOS-ASIC implementation, it can achieve 1.28× smaller area, 2.05× faster runtime and 12.4× energy reduction.","Three-dimensional displays,
Dictionaries,
Flexible printed circuits,
Feedforward neural networks,
Nickel,
Marine vehicles,
Airplanes"
Machine learning for variability aware statistical device design: The case of perpendicular spin-transfer-torque random access memory,"Spin-transfer-torque random access memory (STT RAM) is a promising memory technology due to its scalability, endurance and non-volatility. Addressing the process induced variations during realistic device fabrication process is a challenge, while trying to meet performance specifications, more so with the technology scaling leading to smaller device dimensions. In a simplified picture, the performance parameters of an STT RAM cell such as switching current density for a given pulse length or the switching delay for a given applied current density, depend on a variety of material parameters such as magnetic anisotropy and damping constant of the “free layer” (FL), the information storage layer. Besides material parameters, device dimensions, such as the diameter and the thickness of FL also vary about the target values, due to imperfections during thin-film deposition, lithography and ion-beam etching, among other process steps. To consider process variations in such parameters, Monte-Carlo simulations can be used, where each of the parameters can be, e.g., a random number from a Gaussian distribution about its target value. However, a modest number of 5 parameters and 100 values for each of them would require (102)5 = 1010 device simulations, and would be computationally infeasible for e.g., micromagnetic simulation. A possible route to circumvent such a prohibitively large computational load would be to use a compact model [1, 2]. However, a method like this relies on the so-called macrospin approximation and assumes spins across FL to be parallel to each other at all times during switching and might not be a true representation. Also, for many emerging devices, a compact model still is not available. Machine learning (ML) has been used in the past to model array of resistive random access memory (RRAM) [3]. In this work, we propose an ML driven simulation methodology to take the effect of process variation into account using micromagnetic simulations with reasonable computational effort. We employ support vector regression (SVR), a method used in supervised learning to anticipate the behavior of a system based on previously obtained “training data”, to predict performance of an STT RAM cell. We use STT RAM as a model system, although the proposed scheme should be usable for other devices too.","Random access memory,
Computational modeling,
Switches,
Current density,
Training,
Gaussian distribution,
Probability distribution"
A flexible low-power machine learning accelerator for healthcare applications,"A flexible machine learning accelerator with low power consumption is presented in this paper. Linear classifier, naïve Bayes (NB) classifier and support vector machine (SVM) classifier with three kernel functions (linear, polynomial and radial basis function (RBF)) are realized in this design using the same architecture. A modified coordinate rotation digital computer (CORDIC) unit with expanded convergence range is employed to calculate exponential functions. The supply voltage is decreased to help reduce the power consumption. Re-characterized libraries for TSMC 65 nm LP technology under near-threshold supply voltages are generated for the implementation of the proposed accelerator. At the supply voltage of 0.6 V and the clock frequency of 10 MHz, the accelerator consumes 0.26 μJ for each classification using SVM with RBF function. Using linear classifier, the power consumption is further reduced to 0.41 nJ per classification.","Support vector machines,
Biosensors,
Brain modeling,
Niobium,
Electroencephalography"
Spacecraft autonomy modeled via Markov decision process and associative rule-based machine learning,"Spacecraft on-board autonomy is an important topic in currently developed and future space missions. In this study, we present a robust approach to the optimal policy of autonomous space systems modeled via Markov Decision Process (MDP) from the values assigned to its transition probability matrix. After addressing the curse of dimensionality in solving the formulated MDP problem via Approximate Dynamic Programming, we use an Apriori-based Association Classifier to infer a specific optimal policy. Finally, we also assess the effectiveness of such optimal policy in fulfilling the spacecraft autonomy requirements.",
Machine Learning and IoT for prediction and detection of stress,"According to a cardiac surgeon, it is difficult to predict age from heart rate as it is nonlinear, but we can use a person's heart beat to predict whether that person is fit, unfit and overtrained or not, provided we have that person's age. Based on heart beat we can predict whether a person is in Stress or not. Stress is one of the main factors that are affecting millions of lives. Thus, it is important to inform the person about his unhealthy life style and even alarm him/her before any acute condition occurs. To detect the stress beforehand we have used heart beat rate as one of the parameters. Internet of Things (IoT) along with Machine Learning (ML) is used to alarm the situation when the person is in real risk. ML is used to predict the condition of the patient and IoT is used to communicate the patience about his/her acute stress condition.","Stress,
Servers,
Support vector machines,
Heart beat,
Training,
Logistics"
Novelty detection of rotating machinery using a non-parametric machine learning approach,"A novelty detection algorithm inspired by human audio pattern recognition is conceptualized and experimentally tested. Time-domain data obtained from a microphone is processed by applying a short-time FFT, which returns time-frequency patterns. Such patterns are fed to a probabilistic algorithm, which is designed to detect novel signals and identify windows in the frequency domain where such novelties occur. The probabilistic algorithm presented in this paper uses one-dimensional kernel density estimation for different frequency bins. This process eliminates the need for data dimension reduction algorithms. Experimental datasets containing synthetic and real novelties are used to illustrate and test the novelty detection algorithm. Novelties are clearly detected in all experiments.","Time-frequency analysis,
Machinery,
Detectors,
Time-domain analysis,
Mathematical model,
Algorithm design and analysis,
Data models"
Assessing machine learning algorithms for self-management of asthma,"Control and monitoring of asthma progress is highly important for patient's quality of life and healthcare management. Emerging tools for self-management of various chronic diseases have the potential to support personalized patient guidance. This work presents the design aspects of the myAirCoach decision support system, with focus on the assessment of three machine learning approaches as support tools the first prototype implementation.","Support vector machines,
Monitoring,
Machine learning algorithms,
Decision support systems,
Training,
Classification algorithms,
Tools"
Machine learning for predicting QoE of video streaming in mobile networks,"As video accounts for larger wireless traffic, improving users' quality of experience becomes important for network service providers. In this paper, we apply supervised machine learning technique to predict one objective QoE metric, video starvation, with the users' features, recorded at the beginning of each video session. We show that static users and adaptive streaming users have less starvation events and that mobile users are more difficult to predict their video starvation. In terms of users' features, we show that system parameters such as channel conditions and number of active users are two important features which contribute to better prediction performance. Prediction with the two features can provide sufficient accuracy for static users but not sufficient for mobile users. We also demonstrate that the two information, number of users served in a cell and the number of users experiencing video starvation, provide similar prediction accuracy.","Streaming media,
Throughput,
Bit rate,
Mobile communication,
Correlation"
Adaptive Demodulator Using Machine Learning for Orbital Angular Momentum Shift Keying,"An m
-ary adaptive demodulator based on machine learning for light beams carrying orbital angular momentums (OAMs) over free-space turbulence channels is proposed and demonstrated. Benefiting from natural advantages in the image recognition, convolutional neural network (CNN) is selected to construct the adaptive demodulator. Without extra space light modulators and digital signal processing at the reception, the adaptive demodulator transforms the sequence of intensity patterns of received Laguerre-Gaussian beams carrying different OAM modes into initial signals efficiently. As comparison, K-nearest neighbor (KNN), naive Bayes classifier (NBC), and back-propagation artificial neural network (BP-ANN) are also studied. Furthermore, the demodulating accuracy of 4-, 8-, and 16-ary OAM is investigated with the comprehensive consideration of the atmospheric turbulence, OAM mode spacing, and transmission distance. The simulation results show that the demodulating error rate (DER) of CNN outperforms KNN, NBC, and BP-ANN, especially under stronger turbulence and longer distance. The DER of CNN is ~0.86% for the 1000-m 8-OAM system under strong turbulence, ~30 % less than those of KNN, NBC, and BP-ANN.","Demodulation,
Feature extraction,
Optical fiber communication,
Neurons,
Complexity theory,
Adaptive systems,
Optical vortices"
Detecting mobile botnets through machine learning and system calls analysis,"Botnets have been a serious threat to the Internet security. With the constant sophistication and the resilience of them, a new trend has emerged, shifting botnets from the traditional desktop to the mobile environment. As in the desktop domain, detecting mobile botnets is essential to minimize the threat that they impose. Along the diverse set of strategies applied to detect these botnets, the ones that show the best and most generalized results involve discovering patterns in their anomalous behavior. In the mobile botnet field, one way to detect these patterns is by analyzing the operation parameters of this kind of applications. In this paper, we present an anomaly-based and host-based approach to detect mobile botnets. The proposed approach uses machine learning algorithms to identify anomalous behaviors in statistical features extracted from system calls. Using a self-generated dataset containing 13 families of mobile botnets and legitimate applications, we were able to test the performance of our approach in a close-to-reality scenario. The proposed approach achieved great results, including low false positive rates and high true detection rates.",
Study of Bot detection on Sina-Weibo based on machine learning,"Accompany with the growth of Sina-Weibo users, mendacious Bot users also emerge, which lead to network environment pollution and lower management efficiency. This paper focuses on Sina-Weibo users, extracts the effective features of Bot user through behavior analysis and features study. Then based these features, Bot user identification model is trained by machine learning process and model performance evaluation. The result shows that these extracted features have satisfactory discrimination and the identification model has good performance.","Blogs,
Fans,
Feature extraction,
Twitter,
Decision trees,
Publishing,
Classification algorithms"
Road friction estimation for connected vehicles using supervised machine learning,"In this paper, the problem of road friction prediction from a fleet of connected vehicles is investigated. A framework is proposed to predict the road friction level using both historical friction data from the connected cars and data from weather stations, and comparative results from different methods are presented. The problem is formulated as a classification task where the available data is used to train three machine learning models including logistic regression, support vector machine, and neural networks to predict the friction class (slippery or non-slippery) in the future for specific road segments. In addition to the friction values, which are measured by moving vehicles, additional parameters such as humidity, temperature, and rainfall are used to obtain a set of descriptive feature vectors as input to the classification methods. The proposed prediction models are evaluated for different prediction horizons (0 to 120 minutes in the future) where the evaluation shows that the neural networks method leads to more stable results in different conditions.","Roads,
Friction,
Temperature measurement,
Automobiles,
Time measurement,
Predictive models,
Meteorology"
Detection of debondings with Ground Penetrating Radar using a machine learning method,"In the field of civil engineering, Ground Penetrating Radar (GPR) is the most widely used method of Non-Destructive Testing (NDT). Using supervised learning methods or signal processing methods, it is possible to analyze the sub-surface defects in pavement. In this paper, we propose to use a supervised machine learning method called Support Vector Machines (SVM) to detect the presence of debondings within the pavement. Here, the ground-coupled GPR in quasi mono-static configuration along with SVM is used to detect debondings. The experiments are done on bituminous concrete pavements with various material characteristics. The classification results show the efficiency of the detection process.",
Machine learning inspired energy-efficient hybrid precoding for mmWave massive MIMO systems,"Hybrid precoding is a promising technique for mmWave massive MIMO systems, as it can considerably reduce the number of required radio-frequency (RF) chains without obvious performance loss. However, most of the existing hybrid precoding schemes require a complicated phase shifter network, which still involves high energy consumption. In this paper, we propose an energy-efficient hybrid precoding architecture, where the analog part is realized by a small number of switches and inverters instead of a large number of high-resolution phase shifters. Our analysis proves that the performance gap between the proposed hybrid precoding architecture and the traditional one is small and keeps constant when the number of antennas goes to infinity. Then, inspired by the cross-entropy (CE) optimization developed in machine learning, we propose an adaptive CE (ACE)-based hybrid precoding scheme for this new architecture. It aims to adaptively update the probability distributions of the elements in hybrid precoder by minimizing the CE, which can generate a solution close to the optimal one with a sufficiently high probability. Simulation results verify that our scheme can achieve the near-optimal sum-rate performance and much higher energy efficiency than traditional schemes.","Radio frequency,
Precoding,
Phase shifters,
Energy consumption,
Antenna arrays,
MIMO,
Inverters"
Low-latency routing for fronthaul network: A Monte Carlo machine learning approach,"A fronthaul bridged network has attracted attention as a way of efficiently constructing the centralized radio access network (C-RAN) architecture. If we change the functional split of C-RAN and employ time-division duplex (TDD), the data rate in fronthaul will become variable and the global synchronization of fronthaul streams will occur. This feature results in an increase in the queuing delay in fronthaul bridges among fronthaul flows. This paper proposes a novel low-latency routing scheme designed to satisfy the latency requirements in fronthaul networks with path-control protocols. The proposed scheme formulates the maximum queuing delay by defining competitive links and flows. It selects the set of paths that satisfy the latency requirements with the Markov chain Monte Carlo method using machine learning (MCMC-ML). The initial paths are selected from candidate paths using the learned solutions, and path-reselection is performed with the MCMC method. We confirmed with computer simulations that the proposed scheme can compute routes for all flows that satisfy the delay requirements. We also confirmed that the route computation is accelerated with the learned solutions, even if the flow distribution changes.","Delays,
Routing,
Quality of service,
Bridges,
Propagation delay,
Algorithm design and analysis,
Monte Carlo methods"
Supervised machine learning via Hidden Markov Models for accurate classification of plant stress levels & types based on imaged Chlorophyll fluorescence profiles & their rate of change in time,"It has long been known that Chlorophyll fluorescence (ChlF), a plant response to stressors in time, is a useful tool in detecting plant stress. Accurate and early plant stress detection is imperative in enabling appropriate and timely intervention. One of the major limitations of prior work in ChIF-based plant classification is that, in general, only a few key inflection points of a localized selection of a chlorophyll fluorescence signal are used to calculate single index values for classification. These values yield limited insight into stress level and especially into stressor type. In this paper, we introduce and present a new method for plant stress classification that uses supervised learning, via Hidden Markov Models (HMMs), to build accurate class profiles using global (versus local) ChlF time-varying signal data acquired via video imaging. We show how creating increased-state supervised models can in particular, classify specific stressor types as well as achieve more granularity in stressor level classification. Experimental results are presented to show the value and potential of the proposed supervised method to enable more accurate and specific classification of plant stressor types and stressor levels.","Hidden Markov models,
Stress,
Fluorescence,
Nitrogen,
Transient analysis,
Steady-state,
Indexes"
"A machine learning based biased-sampling approach for planning safe trajectories in complex, dynamic traffic-scenarios","Many variants of the Rapidly-exploring Random Tree (RRT) algorithm use biased-sampling strategies for solving computationally intensive tasks. One of such tasks is the planning of safe trajectories with the simultaneous intervention in both the longitudinal and the lateral dynamics of the vehicle in complex traffic-scenarios with multiple static and dynamic objects. A recently proposed hybrid statistical learning approach uses a 3D convolutional neural network (3D-ConvNet) to predict suitable longitudinal acceleration profiles in combination with an RRT variant called the Augmented CL-RRT algorithm. This algorithm is not effective in complex traffic-scenarios, i.e., traffic scenarios with more than 4 dynamic objects, because of the lack of flexibility and biasing in the longitudinal and the lateral dynamics intervention, respectively. Therefore, an extension to the Augmented CL-RRT algorithm is introduced to improve the longitudinal dynamics intervention with actuator and stable profile constraints and named as the Augmented CL-RRT+ algorithm. A biased-sampling strategy is also proposed based on the predicted longitudinal acceleration and steering wheel angle profiles provided by a trained 3D-ConvNet. Simulations are performed to compare different trajectory planning algorithms based on efficiency and safety. The results show vast improvements in terms of the efficiency without harming the safety.","Acceleration,
Trajectory,
Heuristic algorithms,
Vehicle dynamics,
Planning,
Prediction algorithms,
Roads"
A machine learning approach for personalized autonomous lane change initiation and control,"We study an algorithm that allows a vehicle to autonomously change lanes in a safe but personalized fashion without the driver's explicit initiation (e.g. activating the turn signals). Lane change initiation in autonomous driving is typically based on subjective rules, functions of the positions and relative velocities of surrounding vehicles. This approach is often arbitrary, and not easily adapted to the driving style preferences of an individual driver. Here we propose a data-driven modeling approach to capture the lane change decision behavior of human drivers. We collect data with a test vehicle in typical lane change situations and train classifiers to predict the instant of lane change initiation with respect to the preferences of a particular driver. We integrate this decision logic into a model predictive control (MPC) framework to create a more personalized autonomous lane change experience that satisfies safety and comfort constraints. We show the ability of the decision logic to reproduce and differentiate between two lane changing styles, and demonstrate the safety and effectiveness of the control framework through simulations.","Vehicles,
Trajectory,
Safety,
Support vector machines,
Actuators,
Training,
Mathematical model"
Timber Health Monitoring using piezoelectric sensor and machine learning,"The Timber Health Monitoring System, which enables constant monitoring of wooden buildings by artificial intelligence based analysis of the signals of a piezoelectric sensor attached to a piece of timber, is proposed. Basic verification was carried out by modeling timber damage and performing vibration tests. Analysis of the obtained waveform data using the k-nearest neighbor (k-NN) method and a support vector machine revealed that the proposed system has a strong classification performance. We also tried reducing the data dimensions by using principal component analysis and found that the classification rates barely decreased even if dimensional reduction was adopted. These results are promising for the realization of our proposed system.","Support vector machines,
Kernel,
Vibrations,
Monitoring,
Artificial intelligence,
Principal component analysis,
Buildings"
Performance of Machine Learning Classifiers for Indoor Person Localization With Capacitive Sensors,"Accurate tagless indoor person localization is important for several applications, such as assisted living and health monitoring. Machine learning (ML) classifiers can effectively mitigate sensor data variability and noise due to deployment-specific environmental conditions. In this paper, we use experimental data from a capacitive sensor-based indoor human localization system in a 3 m × 3 m room to comparatively analyze the performance of Weka collection ML classifiers. We compare the localization performance of the algorithms, its variation with the training set size, and the algorithm resource requirements for both training and inferring. The results show a large variance between algorithms, with the best accuracy, precision, and recall exceeding 93% and 0.05 m average localization error.","Capacitive sensors,
Training,
Monitoring,
Machine learning algorithms,
IEEE members,
Permittivity measurement"
Machine Learning-Based Temperature Prediction for Runtime Thermal Management across System Components,"Elevated temperatures limit the peak performance of systems because of frequent interventions by thermal throttling. Non-uniform thermal states across system nodes also cause performance variation within seemingly equivalent nodes leading to significant degradation of overall performance. In this paper we present a framework for creating a lightweight thermal prediction system suitable for run-time management decisions. We pursue two avenues to explore optimized lightweight thermal predictors. First, we use feature selection algorithms to improve the performance of previously designed machine learning methods. Second, we develop alternative methods using neural network and linear regression-based methods to perform a comprehensive comparative study of prediction methods. We show that our optimized models achieve improved performance with better prediction accuracy and lower overhead as compared with the Gaussian process model proposed previously. Specifically we present a reduced version of the Gaussian process model, a neural network-based model, and a linear regression-based model. Using the optimization methods, we are able to reduce the average prediction errors in the Gaussian process from 4.2^{\circ}
C to 2.9^{\circ}
C. We also show that the newly developed models using neural network and Lasso linear regression have average prediction errors of 2.9^{\circ}
C and 3.8^{\circ}
C respectively. The prediction overheads are 0.22 ms, 0.097 ms, and 0.026 ms per prediction for reduced Gaussian process, neural network, and Lasso linear regression models, respectively, compared with 0.57 ms per prediction for the previous Gaussian process model. We have implemented our proposed thermal prediction models on a two–node system configuration to help identify the optimal task placement. The task placement identified by the models reduces the average system temperature by up to 11.9^{\circ}
C without any performance degradation. Furthermore, these models respectively achieve 75%, 82.5%, and 74.17% success rates in correctly pointing to those task placements with better thermal response, compared with 72.5% success for the original model in achieving the same objective. Finally, we extended our analysis to a 16–node system and we were able to train models and execute them in real time to guide task migration and achieve on average 17% reduction in the overall system cooling power.","Predictive models,
Thermal management,
Gaussian processes,
Computational modeling,
Cooling,
Analytical models,
Neural networks"
An application of machine learning approach to fault detection of a synchronous machine,"Accurate fault diagnosis systems should consider both the historical performance and the assessment of the current state of a machine. Manufacturing, installation, operation and maintenance are part of the machine's history and should be taken into account. The paper focuses on experimental procedures to develop a multi-criteria methodology to classify up to ten machine conditions. Using machine learning for signal processing techniques, any deviation from a normal steady state might be categorized as an abnormal behavior and, when demonstrated, a fault. To take advantage of machine learning algorithms, a significant amount of data is needed. To demonstrate the procedure the authors examined a synchronous machine. The authors recorded currents and voltages primarily, in stator and rotor winding, well as rotational speed and electromechanical torque. The collected signals were filtered and pre-processed, and to 5038 features were calculated and transformed into a tidy dataset. The sparse Linear Discriminant Analysis algorithm was used to extract the most important defined features. The results are shown in 3D scatter plots; in which each machine condition is represented. It is then possible to visualize the ability of the model to identify the most discriminant features. The same method can be used for the diagnostic of other types of machine conditions.","Power harmonic filters,
Feature extraction,
Harmonic analysis,
Sensors,
Stator windings,
Rotors,
Synchronous machines"
Anomaly detection in network traffic using extreme learning machine,"Intrusion detection systems are one of the most relevant security features against network attacks. Machine learning methods are used to analyze network traffic parameters for the presence of an attack signs. In this paper, extreme learning machine method is considered for intrusion detection in network traffic. The experimental results lead to the conclusion of practical significance of the proposed approach for attacks detection in network traffic.","Training,
Intrusion detection,
Telecommunication traffic,
Learning systems,
Indexes"
Automated machine learning for Internet of Things,"This paper presents Decanter AI, a new approach to machine learning that uses automated machine learning techniques to resolve the massive data problem in the rapidly industry of the Internet of Things (IoT). This solution is specialized in IoT data and applied to a real-world example of a smart building with over 100 connected sensors and its performance is compared to industry benchmarks.","Prediction algorithms,
Machine learning algorithms,
Clustering algorithms,
Algorithm design and analysis,
Predictive models,
Training"
A Machine Learning Approach to Evaluating Translation Quality,"We explored supervised machine learning (ML) techniques to understand and predict the adequacy and fluency of English-Spanish machine translation. Five experiments were conducted using three classifiers in Weka, an open-source ML tool. We found that the highest performance was achieved by applying a dimensionality reduction approach to the classification task, which included collapsing a numeric scale of quality to two categories: high quality and low quality. Our results showed that the Support Vector Machine classifier performed the best at predicting the adequacy (65.65%) and fluency (65.77%) of the translations. More research is needed to explore the methodologies of applying ML to translation evaluation.","Metadata,
Decision trees,
Support vector machines,
Google,
Libraries,
Pragmatics,
Correlation"
From algorithms to devices: Enabling machine learning through ultra-low-power VLSI mixed-signal array processing,"Machine learning and related statistical signal processing are expected to endow sensor networks with adaptive machine intelligence and greatly facilitate the Internet of Things. As such, architectures embedding adaptive and learning algorithms on-chip are oft-ignored by the design community and present a new set of design trade-offs. This review focuses on efficient implementation of mixed-signal matrix-vector multiplication as a central computational primitive enabling machine learning and statistical signal processing, with specific examples in spatial filtering for adaptive beamforming. We describe adaptive algorithms amenable for efficient implementation with such primitives in the presence of noise and analog variability. We also briefly highlight current trends in high-density integration in emerging memory device technologies and their use in highdimensional adaptive computing.","Gain,
Dynamic range,
Signal processing algorithms,
Parallel processing,
Switches,
Digital signal processing"
Machine learning-based network modeling: An artificial neural network model vs a theoretical inspired model,"Recent trends in networking are proposing the use of Machine Learning (ML) techniques for the control and operation of the network. The application of ML to networking brings several use-cases as well as challenges. In this paper we focus on a rather fundamental problem in networking: estimating the delays of a network. In particular, we aim to assess the performance of models inspired by the existing knowledge of network modeling in comparison to generic adaptive machine learning models. To do so, we present a M/M/1-inspired ML regressor and compare its performance to a neural network.","Delays,
Training,
Adaptation models,
Biological neural networks,
Load modeling,
Cost function"
Hardware for machine learning: Challenges and opportunities,"Machine learning plays a critical role in extracting meaningful information out of the zetabytes of sensor data collected every day. For some applications, the goal is to analyze and understand the data to identify trends (e.g., surveillance, portable/wearable electronics); in other applications, the goal is to take immediate action based the data (e.g., robotics/drones, self-driving cars, smart Internet of Things). For many of these applications, local embedded processing near the sensor is preferred over the cloud due to privacy or latency concerns, or limitations in the communication bandwidth. However, at the sensor there are often stringent constraints on energy consumption and cost in addition to throughput and accuracy requirements. Furthermore, flexibility is often required such that the processing can be adapted for different applications or environments (e.g., update the weights and model in the classifier). In many applications, machine learning often involves transforming the input data into a higher dimensional space, which, along with programmable weights, increases data movement and consequently energy consumption. In this paper, we will discuss how these challenges can be addressed at various levels of hardware design ranging from architecture, hardware-friendly algorithms, mixed-signal circuits, and advanced technologies (including memories and sensors).","Feature extraction,
Robot sensing systems,
Computer vision,
Training,
Hardware,
Cloud computing,
Neural networks"
Teat detection mechanism using machine learning based vision for smart Automatic Milking Systems,"This paper discusses the work in progress of Automatic Milking system with focus on development of a smart vision system for the manipulator of the milking robot. The vision system being developed aims at giving the manipulator the capability for faster and accurate teat detection. In our laboratory environment, we are developing a smart vision system which is trained by machine learning and can detect the cow's udder and the teats for all the pre-milking, milking and post-milking processes with least to none time being spent on cow herd training for the system in the barn environment. This teat detection mechanism aims to reduce the time spent on training of the system in the first few initial milkings and reduce operator responsibilities, to eventually produce a more of plug and play product.","Machine vision,
Cows,
Feature extraction,
Manipulators,
Robot sensing systems,
Training"
Experience of using of machine learning methods to identify the left ventricle region in echocardiographic records,"The possibility of an ultrasound study of the heart (Echocardiography, EchoCG) is widely used in modern cardiology. This non-invasive technology allows studying cardiac activity of the patient by determining the global contractility of the heart muscle. The methods, which used in echocardiography, require performing manual operations from specially trained medical professionals. A number of researchers are working on the problem of automation of this medical technology. The article shows the way of solving the problem of the left ventricle area identification in echocardiography records with machine learning techniques. The task of the left ventricle delineation is reduced to the problem of pixels classification on video frames. A pixel can belong to one of two classes (the background region or the region of the left ventricle). The possibility of solving the task was tested with the following classifiers: decision tree, gaussian naive Bayes, linear discriminant analysis, quadratic discriminant analysis, adaboost classifier, random forest classifier. The assessment of classification results was performed using ROC curves. The best performance was obtained for both algorithms: decision tree classifier (AUC 0.93) and random forest classifier (AUC 0.93).","Ultrasonic imaging,
Wiener filters,
Decision trees,
Classification algorithms,
Maximum likelihood detection,
Nonlinear filters,
Echocardiography"
Machine learning on merging static and dynamic features to identify malicious mobile apps,"The amount of Android system-targeted malware has increased dramatically in recent years, and Android has been the focus of far more malware targeting than other mobile operating systems. In order to reduce the hazards of malware, this paper proposes a malware detection system with static and dynamic app features. In terms of the static features, the permissions, native-permissions, function and priority of an app are extracted as the base of analysis. In terms of the dynamic feature, the app is executed in a sandbox emulator, and then log files are analyzed to identify behaviors that help judgment, such as sending short messages without permission, modifying system files or reading personal data. This system extracts the static and dynamic features of an app, which are then merged before the weights are adjusted appropriately. Finally, Weka is used for training to obtain the detection module. According to the experiment, an unknown malicious act is evaluated using tenfold cross validation; the proposed system achieves a 97.4% accuracy.","Malware,
Smart phones,
Feature extraction,
Training,
Mobile communication,
Google"
Real time degradation identification of UAV using machine learning techniques,"The usages and functionalities of Unmanned Aerial Vehicles (UAV) have grown rapidly during the last years. They are being engaged in many types of missions, ranging from military to agriculture passing by entertainment and rescue or even delivery. Nonetheless, for being able to perform such tasks, UAVs have to navigate safely in an often dynamic and partly unknown environment. This brings many challenges to overcome, some of which can lead to damages or degradations of different body parts. Thus, new tools and methods are required to allow the successful analysis and identification of the different threats that UAVs have to manage during their missions or flights. Various approaches, addressing this domain, have been proposed. However, most of them typically identify the changes in the UAVs behavior rather than the issue. This work presents an approach, which focuses not only on identifying degradations of UAVs during flights, but estimate the source of the failure as well.","Degradation,
Real-time systems,
Data models,
Solid modeling,
Robot sensing systems"
Bioinspired vision-only UAV attitude rate estimation using machine learning,"This paper presents a bioinspired system for attitude rate estimation using visual sensors for aerial vehicles. The sensorial system consists of three small low-resolution cameras (10x8 pixels), and is based on insect ocelli, a set of three simple eyes related to flight stabilization. Most previous approaches inspired by the ocellar system use model-based techniques and consider different assumptions, like known light source direction. Here, a learning approach is employed, using Artificial Neural Networks, in which the system is trained to recover the angular rates in different illumination scenarios with unknown light source direction. We present a study using real data in an indoor setting, in which we evaluate different network architectures and inputs.",
Machine learning based obstacle detection for Automatic Train Pairing,"Short Range wireless devices are becoming more and more popular for ubiquitous sensor and actuator connectivity in industrial communication scenarios. Apart from communication-only scenarios, there are also mission-critical use cases where the distance between the two communicating nodes needs to be determined precisely. Applications such as Automatic Guided Vehicles (AGV's), Automatic Train Pairing additionally require the devices to scan the environment and detect any potential humans/obstacles. Ultra-Wide Band (UWB) has emerged as a promising candidate for Real-Time Ranging and Localization (RTRL) due to advantages such as large channel capacity, better co-existence with legacy systems due to low transmit power, better performance in multipath environments etc. In this paper, we evaluate the performance of a UWB COTS device - TimeDomain P440 which can operate as a ranging radio and a monostatic radar simultaneously. To this end, we evaluate the possibility of using Supervised Learning based estimators for predicting the presence of obstacles by constructing a multiclass hypothesis. Simulation results show that the Ensemble tree based methods are able to calculate the likelihood of obstacle collision with accuracies close to 95%.","Distance measurement,
Labeling,
Radar detection,
Vegetation,
Training,
Wireless communication"
"Adaptive and Energy-Efficient Architectures for Machine Learning: Challenges, Opportunities, and Research Roadmap","Gigantic rates of data production in the era of Big Data, Internet of Thing (IoT)/Internet of Everything (IoE), and Cyber Physical Systems (CSP) pose incessantly escalating demands for massive data processing, storage, and transmission while continuously interacting with the physical world under unpredictable, harsh, and energy-/power-constrained scenarios. Therefore, such systems need to support not only the high performance capabilities at tight power/energy envelop, but also need to be intelligent/cognitive, self-learning, and robust. As a result, a hype in the artificial intelligence research (e.g., deep learning and other machine learning techniques) has surfaced in numerous communities. This paper discusses the challenges and opportunities for building energy-efficient and adaptive architectures for machine learning. In particular, we focus on brain-inspired emerging computing paradigms, such as approximate computing; that can further reduce the energy requirements of the system. First, we guide through an approximate computing based methodology for development of energy-efficient accelerators, specifically for convolutional Deep Neural Networks (DNNs). We show that in-depth analysis of datapaths of a DNN allows better selection of Approximate Computing modules for energy-efficient accelerators. Further, we show that a multi-objective evolutionary algorithm can be used to develop an adaptive machine learning system in hardware. At the end, we summarize the challenges and the associated research roadmap that can aid in developing energy-efficient and adaptable hardware accelerators for machine learning.","Energy efficiency,
Hardware,
Approximate computing,
Biological neural networks,
Training,
Memory management"
Using data mining and machine learning techniques for system design space exploration and automatized optimization,"Recently, the significance of data mining and machine learning have been highlighted in diversified application scenarios. Various data mining and machine learning techniques are often used to analyze the gigantic amount of data to create more commercial values in high-end enterprise systems. However, the advancement of technologies has made data mining and machine learning possible on low-end systems, such as personal computers or embedded systems. While researchers have proposed excellent work on the management de-signs of different components of the system, most of the work are built upon the characteristics of the system, which may change from time to time. This makes it impossible to optimize the system performance with stat-ic, or statically adaptive, system designs. In this work, we propose to embed the supports of data mining and machine learning to the design of operating system, so as to discover a new, automatized way to adaptively optimize the system without using complex algorithms. To validate the proposed ideas, we choose the cache design as a case study, where the replacement of cached contents is automatically controlled by a decision maker. The decision maker then replies on a data miner, which analyzes the data collected by the system monitor. The efficacy of the considered case is verified by a series of experiments, where the results are quite encouraging.","Data mining,
Optimization,
Monitoring,
Measurement,
System performance,
Adaptive systems,
Machine learning algorithms"
Machine Learning for Anomaly Detection and Categorization in Multi-Cloud Environments,"Cloud computing has been widely adopted by application service providers (ASPs) and enterprises to reduce both capital expenditures (CAPEX) and operational expenditures (OPEX). Applications and services previously running on private data centers are now being migrated to private or public clouds. Since most of the ASPs and enterprises have globally distributed user bases, their services need to be distributed across multiple clouds, spread across the globe which can achieve better performance in terms of latency, scalability and load balancing. The shift has eventually led the research community to study multi-cloud environments. However, the widespread acceptance of such environments has been hampered by major security concerns. Firewalls and traditional rule-based security protection techniques are not sufficient to protect user-data in multi-cloud scenarios. Recently, advances in machine learning techniques have attracted the attention of the research community to build intrusion detection systems (IDS) that can detect anomalies in the network traffic. Most of the research works, however, do not differentiate among different types of attacks. This is, in fact, necessary for appropriate countermeasures and defense against attacks. In this paper, we investigate both detecting and categorizing anomalies rather than just detecting, which is a common trend in the contemporary research works. We have used a popular publicly available dataset to build and test learning models for both detection and categorization of different attacks. To be precise, we have used two supervised machine learning techniques, namely linear regression (LR) and random forest (RF). We show that even if detection is perfect, categorization can be less accurate due to similarities between attacks. Our results demonstrate more than 99% detection accuracy and categorization accuracy of 93.6%, with the inability to categorize some attacks. Further, we argue that such categorization can be applied to multi-cloud environments using the same machine learning techniques.","Feature extraction,
Cloud computing,
Radio frequency,
Firewalls (computing),
Complexity theory,
Training"
Machine Learning Based DDoS Attack Detection from Source Side in Cloud,"Denial of service (DOS) attacks are a serious threat to network security. These attacks are often sourced from virtual machines in the cloud, rather than from the attacker's own machine, to achieve anonymity and higher network bandwidth. Past research focused on analyzing traffic on the destination (victim's) side with predefined thresholds. These approaches have significant disadvantages. They are only passive defenses after the attack, they cannot use the outbound statistical features of attacks, and it is hard to trace back to the attacker with these approaches. In this paper, we propose a DOS attack detection system on the source side in the cloud, based on machine learning techniques. This system leverages statistical information from both the cloud server's hypervisor and the virtual machines, to prevent network packages from being sent out to the outside network. We evaluate nine machine learning algorithms and carefully compare their performance. Our experimental results show that more than 99.7% of four kinds of DOS attacks are successfully detected. Our approach does not degrade performance and can be easily extended to broader DOS attacks.","Computer crime,
Cloud computing,
Virtual machining,
Virtual machine monitors,
Servers,
Feature extraction,
Machine learning algorithms"
Hardware Acceleration for Machine Learning,"This paper presents an approach to enhance the performance of machine learning applications based on hardware acceleration. This approach is based on parameterised architectures designed for Convolutional Neural Network (CNN) and Support Vector Machine (SVM), and the associated design flow common to both. This approach is illustrated by two case studies including object detection and satellite data analysis. The potential of the proposed approach is presented.","Support vector machines,
Kernel,
Field programmable gate arrays,
Convolution,
Machine learning algorithms,
Accelerator architectures"
Fault detection and diagnosis for solar-powered Wireless Mesh Networks using machine learning,"The inherent complexity of Wireless Mesh Networks (WMNs) makes management and configuration tasks difficult, specially for fault detection and diagnosis. In addition, manual inspections are extremely costly and require a highly skilled workforce, thus becoming impractical as the problem scales. To address this issue, this paper proposes a solution that makes use of machine learning techniques for automated fault detection and diagnosis (FDD) on solar-powered Wireless Mesh Networks (WMNs). We have used the Knowledge Discovery in Databases (KDD) methodology and a pre-defined dictionary of failures based on our previous experience with the deployment of WMNs. Thereafter, the problem was solved as a pattern classification problem. Several classification algorithms were evaluated, such as Naive Bayes, Support Vector Machine (SVM), Decision Table, k-Nearest Neighbors (k-NN) and C4.5. The SVM presented the best results, achieving a 90.59% overall accuracy during training and over 85% in validation tests.","Batteries,
Monitoring,
Fault detection,
Databases,
Random access memory,
Solar panels,
Temperature sensors"
Applied Machine Learning predictive analytics to SQL Injection Attack detection and prevention,"The back-end database is pivotal to the storage of the massive size of big data Internet exchanges stemming from cloud-hosted web applications to Internet of Things (IoT) smart devices. Structured Query Language (SQL) Injection Attack (SQLIA) remains an intruder's exploit of choice on vulnerable web applications to pilfer confidential data from the database with potentially damaging consequences. The existing solutions of mostly signature approaches were all before the recent challenges of big data mining and at such lacks the functionality and ability to cope with new signatures concealed in web requests. An alternative Machine Learning (ML) predictive analytics provides a functional and scalable mining to big data in detection and prevention of SQLIA. Unfortunately, lack of availability of readymade robust corpus or data set with patterns and historical data items to train a classifier are issues well known in SQLIA research. In this paper, we explore the generation of data set containing extraction from known attack patterns including SQL tokens and symbols present at injection points. Also, as a test case, we build a web application that expects dictionary word list as vector variables to demonstrate massive quantities of learning data. The data set is pre-processed, labelled and feature hashing for supervised learning. The trained classifier to be deployed as a web service that is consumed in a custom dot NET application implementing a web proxy Application Programming Interface (API) to intercept and accurately predict SQLIA in web requests thereby preventing malicious web requests from reaching the protected back-end database. This paper demonstrates a full proof of concept implementation of an ML predictive analytics and deployment of resultant web service that accurately predicts and prevents SQLIA with empirical evaluations presented in Confusion Matrix (CM) and Receiver Operating Curve (ROC).","Structured Query Language,
Big Data,
Support vector machines,
Databases,
Data mining,
Feature extraction,
Data models"
Experimental evaluation of various machine learning regression methods for model identification of autonomous underwater vehicles,"In this work we investigate the identification of a motion model for an autonomous underwater vehicle by applying different machine learning (ML) regression methods. By using the data collected from the robot's on-board navigation sensors, we train the regression models to learn the damping term which is regarded as one of the most uncertain components of the motion model. Four regression techniques are investigated namely, artificial neural networks, support vector machines, kernel ridge regression, and Gaussian processes regression. The performance of the identified models is tested through real experimental scenarios performed with the AUV Leng. The novelty of this work is the identification of an underwater vehicle's motion model, for the first time, through machine learning methods by using the robot's onboard sensory data. Results show that the damping model learned with nonlinear methods yield better estimates than the simplified linear and quadratic model which is identified with least-squares technique.","Damping,
Kernel,
Sensors,
Data models,
Support vector machines,
Navigation,
Underwater vehicles"
Indoor localization at 5GHz using Dynamic machine learning approach (DMLA),"In recent years, Wi-Fi based indoor localization using received signal strength (RSS) gets considerable attention. However, RSS based Wi-Fi localization at 2.4GHz is highly susceptible and unstable. We proposed dynamic machine learning approach (DMLA) at 5GHz to effectively localize Wi-Fi users by means of feed-forward neural network algorithm. The simulation result shows that 90% of tested result has less than 0.4m estimation error, and no result shows greater than 0.47m location estimation errors.","Wireless fidelity,
Estimation error,
Training,
Fingerprint recognition,
Conferences,
Heuristic algorithms"
Machine learning and coresets for automated real-time video segmentation of laparoscopic and robot-assisted surgery,"Context-aware segmentation of laparoscopic and robot assisted surgical video has been shown to improve performance and perioperative workflow efficiency, and can be used for education and time-critical consultation. Modern pressures on productivity preclude manual video analysis, and hospital policies and legacy infrastructure are often prohibitive of recording and storing large amounts of data. In this paper we present a system that automatically generates a video segmentation of laparoscopic and robot-assisted procedures according to their underlying surgical phases using minimal computational resources, and low amounts of training data. Our system uses an SVM and HMM in combination with an augmented feature space that captures the variability of these video streams without requiring analysis of the nonrigid and variable environment. By using the data reduction capabilities of online k-segment coreset algorithms we can efficiently produce results of approximately equal quality, in realtime. We evaluate our system in cross-validation experiments and propose a blueprint for piloting such a system in a real operating room environment with minimal risk factors.","Surgery,
Streaming media,
Instruments,
Laparoscopes,
Robots,
Hidden Markov models,
Visualization"
Machine learning based VP9-to-HEVC video transcoding,"As more and more coding standards are developed for videos with higher resolution and higher frame rates, the conversion between them is in demand. This paper focuses on the development of fast VP9-to-HEVC transcoding algorithm. Since the decoding-and-full-re-encoding process is time consuming, we explore Naïve-Bayes based machine learning algorithm to accelerate VP9-to-HEVC transcoding. The machine learning process can bridge VP9 decoding information and HEVC splitting decision throughout training results, so the HEVC CU searching process can be simplified for faster transcoding. Two new features closely related to VP9, sub-block counting and depth map, are added for VP9-to-HEVC video transcoder. Using coding features and Naive-Bayes algorithm, several models are built for different QPs and block sizes to reduce transcoding time. Experiments show that a maximum time reduction of 53% and an average reduction of 44% can be achieved and the loss of decoded image quality in BD-rate is almost ignorable.","Transcoding,
Machine learning algorithms,
Training,
Feature extraction,
Decoding,
Standards"
Artificial calf weaning strategies and the role of machine learning: A review,"Research in Machine Learning has increased dramatically over the past couple of decades, with applications greatly benefitting industry. These applications vary widely in the fields of space exploration technologies, transportation, robotics, agriculture, animal husbandry, medicine and finance. However, the agricultural and animal husbandry related applications are relatively few when compared to other fields. This paper reviews the past and present calf weaning strategies mainly focussing on the incorporation of Machine Learning techniques in classifying the behavioural responses in cattle and makes an attempt to extend the scope of machine learning classification applications related to calf weaning. This review further discusses the integration of relevant sensors (sensor fusion), deep learning and the improvement of prediction accuracy.","Sensors,
Dairy products,
Cows,
Sociology,
Statistics,
Stress"
Machine learning based QoE prediction in SDN networks,"Our work in this paper presents a prediction of quality of experience based on full reference parametric (SSIM, VQM) and application metrics (resolution, bit rate, frame rate) in SDN networks. First, we used DCR (Degradation Category Rating) as subjective method to build the training model and validation, this method is based on not only the quality of received video but also the original video but all subjective methods are too expensive, don't take place in real time and takes much time for example our method takes three hours to determine the average MOS (Mean Opinion Score). That's why we proposed novel method based on machine learning algorithms to obtain the quality of experience in an objective manner. Previous researches in this field help us to use four algorithms: Decision Tree (DT), Neural Network, K nearest neighbors KNN and Random Forest RF thanks to their efficiency. We have used two metrics recommended by VQEG group to assess the best algorithm: Pearson correlation coefficient r and Root-Mean-Square-Error RMSE. The last part of the paper describes environment based on: Weka to analyze ML algorithms, MSU tool to calculate SSIM and VQM and Mininet for the SDN simulation.","Prediction algorithms,
Machine learning algorithms,
Estimation,
Decision trees,
Neural networks,
Algorithm design and analysis"
Exploiting machine learning strategies and RSSI for localization in wireless sensor networks: A survey,"In recent years, there has been a growing interest in the wireless sensor networks (WSN) for a variety of applications such as the localization and real time positioning. Different approaches based on artificial intelligence are applied to solve common issues in WSN and improve network performance. This paper addresses a survey on machine learning techniques for localization in WSNs using Received Signal Strength Indicator.","Wireless sensor networks,
Training,
Support vector machines,
Fuzzy logic,
Predictive models,
Machine learning algorithms,
Data models"
Machine learning of real-time power systems reliability management response,"In this paper we study how supervised machine learning could be applied to build simplified models of realtime (RT) reliability management response to the realization of uncertainties. The final objective is to import these models into look-ahead operation planning under uncertainties. Our response models predict in particular the real-time reliability management costs and the resulting reliability level of the system. We tested our methodology on the IEEE-RTS96 benchmark. Among the supervised learning algorithms tested, extremely randomized trees, kernel ridge regression and neural networks appear to be the best methods for this application. Furthermore, by using feature “importances” computed by tree-based ensemble methods, we were able to extract the most relevant variables to predict the response of real-time reliability management, and thus obtain a better understanding of the system properties.","Real-time systems,
Reliability,
Management,
Planning,
Uncertainty,
Supervised learning,
Wind forecasting"
Measuring the Effectiveness of Hidden Context Usage by Machine Learning Methods under Conditions of Increased Entropy of Noise,The context can be defined as the data that is needed for solving given problem. In this view the effectiveness of context search and usage is one of the key factors of computational awareness - to solve given problems computational systems need to become aware of the related context. In this paper we concentrate on the estimation of context usage effectiveness of selected machine learning algorithms for data created with injection of hidden context masked by noise of different levels of entropy.,"Entropy,
Training,
Classification algorithms,
Algorithm design and analysis,
Context modeling,
Data models,
Machine learning algorithms"
Machine-Learning-Guided Selectively Unsound Static Analysis,"We present a machine-learning-based technique for selectively applying unsoundness in static analysis. Existing bug-finding static analyzers are unsound in order to be precise and scalable in practice. However, they are uniformly unsound and hence at the risk of missing a large amount of real bugs. By being sound, we can improve the detectability of the analyzer but it often suffers from a large number of false alarms. Our approach aims to strike a balance between these two approaches by selectively allowing unsoundness only when it is likely to reduce false alarms, while retaining true alarms. We use an anomaly-detection technique to learn such harmless unsoundness. We implemented our technique in two static analyzers for full C. One is for a taint analysis for detecting format-string vulnerabilities, and the other is for an interval analysis for buffer-overflow detection. The experimental results show that our approach significantly improves the recall of the original unsound analysis without sacrificing the precision.","Computer bugs,
Libraries,
Benchmark testing,
Tools,
Software engineering,
Scalability,
Support vector machines"
Machine Learning-Based Detection of Open Source License Exceptions,"From a legal perspective, software licenses govern the redistribution, reuse, and modification of software as both source and binary code. Free and Open Source Software (FOSS) licenses vary in the degree to which they are permissive or restrictive in allowing redistribution or modification under licenses different from the original one(s). In certain cases, developers may modify the license by appending to it an exception to specifically allow reuse or modification under a particular condition. These exceptions are an important factor to consider for license compliance analysis since they modify the standard (and widely understood) terms of the original license. In this work, we first perform a large-scale empirical study on the change history of over 51K FOSS systems aimed at quantitatively investigating the prevalence of known license exceptions and identifying new ones. Subsequently, we performed a study on the detection of license exceptions by relying on machine learning. We evaluated the license exception classification with four different supervised learners and sensitivity analysis. Finally, we present a categorization of license exceptions and explain their implications.",Software engineering
Combining Machine Learning and Genetic Algorithms to Solve the Independent Tasks Scheduling Problem,"We propose a new accurate and fast memetic parallel optimization algorithm for the independent tasks scheduling problem. The new technique combines the Virtual Savant (VS) with a parallel genetic algorithm (called PA-CGA). VS is an optimization framework based on machine learning that learns from a reference set of (pseudo-)optimal solutions how to solve the problem, providing accurate results in extremely low run times. We propose in this work the use of VS to generate a highly accurate initial population for the PA-CGA. Results show how initializing the population with VS (we test two versions of VS, differing on its training process) significantly increases the accuracy of the PA-CGA, compared to two other population initialization techniques: random and using a state-of-the-art heuristic.","Sociology,
Statistics,
Genetic algorithms,
Optimization,
Support vector machines,
Memetics,
Processor scheduling"
Gait phase detection from thigh kinematics using machine learning techniques,"Intelligent orthotic devices require accurate detection of gait events for real-time control. For orthoses that control the knee, an ideal system would only locate sensors at the thigh and knee, thereby facilitating sensor and electronics integration with the assistive device. To determine potential gait phase identification approaches, classification was implemented using J-48 Decision Tree, Random Forest, Multi-layer Perceptrons, and Support Vector Machine classifiers, along with 5-fold (5-FCV) and 10-fold cross validation (10-FCV). Knee angle, thigh angular velocity, and thigh acceleration were obtained from 31 able-bodied participants during walking (10 strides each). Strides were segmented into Loading Response, Push-Off, Swing, and Terminal Swing and features were extracted using a 0.1 second sliding window. Gait phase classification was performed with and without the knee angle parameter. J-48 Decision Tree with the knee angle parameter was ranked the best classifier due to its second highest classification accuracy of 97.5% and lowest mean absolute error of 0.014. Results without the knee angle parameter differed by only 0.5% and 0.003. Therefore, an inertial sensor with accelerometer and gyroscope output, located at the thigh, is a viable approach for classifying gait phases for intelligent orthosis control.","Knee,
Radio frequency,
Support vector machines,
Thigh,
Atmospheric measurements,
Particle measurements,
Data models"
When machine learning meets compressive sampling for wideband spectrum sensing,"This paper proposes a novel technique that exploits spectrum occupancy behaviors inherent to wideband spectrum access to enable efficient cooperative wideband spectrum sensing. Our technique requires lesser number of sensing measurements while still recovering spectrum occupancy information accurately. It does so by leveraging compressive sampling theory to exploit the block-like occupancy structure of wideband spectrum access. Our technique is also adaptive in that it accounts for the variability of spectrum occupancy over time. It exploits supervised learning to provide and use accurate realtime estimates of the spectrum occupancy. Using simulations, we show that our proposed technique outperforms existing approaches by making accurate spectrum occupancy decisions with lesser sensing communication and energy overheads.","Sensors,
Wideband,
Supervised learning,
5G mobile communication,
Performance evaluation,
Protocols,
Prediction algorithms"
Multispectral imaging and machine learning for automated cancer diagnosis,"Advancing technologies in the current era paved a lot to break the hurdles in medical diagnostic field. When cancer turned out to be the most common and dangerous disease of the age, novel diagnostic methodologies were introduced to enable early detection and hence save numerous lives. Accomplishment of various automatic and semi-automatic approaches in the diagnosis has proved its sufficient impetus to improve diagnostic speed and accuracy. A wide range of image processing based tools are currently available as a part of automatic cancer detection systems. Different imaging modalities have been utilized for extracting the suspected patient information, where the multispectral imaging has emerged as an efficient means for capturing the entire range of spectral and spatial data. In this paper, we review the current multispectral imaging based methods for automatic diagnosis of major types of cancer and discuss the limitations which are yet to be overcome, so as to improve the existing systems.","Cancer,
Feature extraction,
Biopsy,
Multispectral imaging,
Hyperspectral imaging,
Lungs"
Comparative study of machine learning techniques in sentimental analysis,"Sentimental Analysis is reference to the task of Natural Language Processing to determine whether a text contains subjective information and what information it expresses i.e., whether the attitude behind the text is positive, negative or neutral. This paper focuses on the several machine learning techniques which are used in analyzing the sentiments and in opinion mining. Sentimental analysis with the blend of machine learning could be useful in predicting the product reviews and consumer attitude towards to newly launched product. This paper presents a detail survey of various machine learning techniques and then compared with their accuracy, advantages and limitations of each technique. On comparing we get 85% of accuracy by using supervised machine learning technique which is higher than that of unsupervised learning techniques.","Support vector machines,
Supervised learning,
Bayes methods,
Unsupervised learning,
Decision trees,
Entropy,
Classification algorithms"
Suspension system status detection of maglev train based on machine learning using levitation sensors,"The objective of this paper is to design a new method, based on machine learning, to detect the abnormal status of the suspension system of maglev train by using the real-time signals collected by the levitation sensors. The abnormal status is harmful to the operation of maglev train. It is necessary to detect the abnormity timely. Generally, the operation data of train is recorded online, but it is analyzed by experts off-line, the efficiency of this approach is very low. So, an efficient and accurate method is urgently demanded. There are 120 levitation sensors in a carriage, which measure the levitation states with nearly 40khz sampling frequency. In tradition, the information of sensors is just used as the feedback of suspension controllers. This paper will study how to use these information for detecting the abnormal status of suspension system by using support vertical machine (SVM). Firstly, some reasonable features originate from experience are extracted from levitation sensors. Secondly, considering that the fault rate is very low, resulting in less fault samples. So, the SVM method is introduced. At last, the raised machine learning model is applied in the practical usage on maglev train by using the programming in the Digital signal processor (DSP) controller. By this method, the existing abnormal status of suspension system can be diagnosed in real-time. The experiment results show that this algorithm can achieve the detection accuracy of 91.3%.","Suspensions,
Levitation,
Feature extraction,
Acceleration,
Sensor systems,
Vibrations"
Machine-Learning Based Performance Estimation for Distributed Parallel Applications in Virtualized Heterogeneous Clusters,"In a virtualized heterogeneous cluster, for a distributed parallel application which runs in multiple virtual machines (VMs) concurrently, there are a huge number of possible ways to place its VMs. This paper investigates a performance estimation technique for distributed parallel applications in virtualized heterogeneous clusters. We first analyze the effects of different VM configurations on the performance of various distributed parallel applications. We then present a machine-learning based performance model for a distributed parallel application. Using a heterogeneous cluster with two different types of nodes, we show that our machine-learning based models can estimate the runtimes of distributed parallel applications with modest error rates.","Runtime,
Error analysis,
Estimation,
Interference,
Quality of service,
Correlation,
Measurement"
Machine learning analysis for a flexibility energy approach towards renewable energy integration with dynamic forecasting of electricity balancing power,"One of the most important instruments to be able to provide the needed level of flexibility in the electricity system supporting renewable energy integration are balancing markets. We propose a dynamic approach of balancing procurement using machine learning algorithms. We apply a simulation for a Dynamic Day-Ahead Dimensioning Model to the Austrian delta control area. By using public data on renewables, generation and load we show that dynamic dimensioning and procurement of balancing power enables savings in comparison to static dimensioning and procurement with the same level of security.","Wind forecasting,
Machine learning algorithms,
Predictive models,
Procurement,
Load modeling,
Bayes methods,
Photovoltaic systems"
Generating Routing-Driven Power Distribution Networks With Machine-Learning Technique,"As technology node keeps scaling and design complexity keeps increasing, power distribution networks (PDNs) require more routing resource to meet IR-drop and electro-migration (EM) constraints. This paper presents a design flow to generate a PDN that can result in near-minimal overhead for the routing of the underlying standard cells while satisfying both IR-drop and EM constraints based on a given cell placement. The design flow relies on a machine-learning model to quickly predict the total wire length of global route associated with a given PDN configuration in order to speed up the search process. The experimental results based on various 28 nm industrial block designs have demonstrated the accuracy of the learned model for predicting the routing cost and the effectiveness of the proposed framework for reducing the routing cost of the final PDN.","Routing,
Metals,
Wires,
Buildings,
Standards,
Indexes,
Predictive models"
Computer game as a tool for machine learning education,"The effective education is one of very important and everlasting challenges of human society. With each generation of students, new approaches have to be implemented to keep the process of education prosperous. This paper introduces a small piece to a set of modern tools for education of Informatics and Electrical Engineering. To be more specific, an interactive software for machine learning testing and demonstration is presented in this paper. The software is designed especially to be used as a motivation and a first encounter to these areas of technical studies, while it supports individual efforts of the students. In the paper, the software architecture is described and, in the second half of the paper, some possibilities of software usage in education process are suggested.","Games,
Tools,
Education,
Probes,
Cost function,
Electrical engineering,
Software"
Detecting Time Synchronization Attacks in Cyber-Physical Systems with Machine Learning Techniques,"Recently, researchers found a new type of attacks, called time synchronization attack (TS attack), in cyber-physical systems. Instead of modifying the measurements from the system, this attack only changes the time stamps of the measurements. Studies show that these attacks are realistic and practical. However, existing detection techniques, e.g. bad data detection (BDD) and machine learning methods, may not be able to catch these attacks. In this paper, we develop a ""first difference aware"" machine learning (FDML) classifier to detect this attack. The key concept behind our classifier is to use the feature of ""first difference"", borrowed from economics and statistics. Simulations on IEEE 14-bus system with real data from NYISO have shown that our FDML classifier can effectively detect both TS attacks and other cyber attacks.","Detectors,
Synchronization,
Supervised learning,
Global Positioning System,
Cyber-physical systems,
Training data"
A Survey of Machine Learning Techniques Applied to Self Organizing Cellular Networks,"In this paper, a survey of the literature of the past fifteen years involving Machine Learning (ML) algorithms applied to self organizing cellular networks is performed. In order for future networks to overcome the current limitations and address the issues of current cellular systems, it is clear that more intelligence needs to be deployed, so that a fully autonomous and flexible network can be enabled. This paper focuses on the learning perspective of Self Organizing Networks (SON) solutions and provides, not only an overview of the most common ML techniques encountered in cellular networks, but also manages to classify each paper in terms of its learning solution, while also giving some examples. The authors also classify each paper in terms of its self-organizing use-case and discuss how each proposed solution performed. In addition, a comparison between the most commonly found ML algorithms in terms of certain SON metrics is performed and general guidelines on when to choose each ML algorithm for each SON function are proposed. Lastly, this work also provides future research directions and new paradigms that the use of more robust and intelligent algorithms, together with data gathered by operators, can bring to the cellular networks domain and fully enable the concept of SON in the near future.","Cellular networks,
Optimization,
Mobile computing,
5G mobile communication,
Self-organizing networks,
Monitoring"
Selective Traffic Offloading on the Fly: A Machine Learning Approach,"It has been well recognized that network transmission constitutes a large portion of smartphone energy consumption, mainly because of the tail energy caused by cellular network interface. Traffic offloading has been proposed to reduce energy by letting a smartphone offload network traffic to its neighbors in vicinity via low-power direct connections (e.g., WiFi Direct or Bluetooth). Our experiments conducted in a realistic environment reveal that energy efficiency cannot be improved or even deteriorates without a carefully designed offloading strategy. In this paper, we propose a selective traffic offloading scheme implemented as a smartphone middleware in a software-defined fashion, which consists of a packet classifier and a traffic scheduler. Using a light-weight machine learning approach exploiting unique smartphone context information, the packet classifier identifies packets generated on the fly as offloadable or not with substantially improved efficiency and feasibility on resource limited smartphones compared to traditional approaches. Both testbed and simulation based experiments are conducted and the results show that our proposal always attains the superior performance on a number of comparison metrics.","Energy consumption,
Cellular networks,
Middleware,
Data communication,
Network interfaces,
Wireless fidelity,
Bluetooth"
Machine learning based approaches for medium-thick plate stress analysis feature extraction and product defect prediction,"In order to accurately predict the plate production process defects, and increase the rate of finished products, and improve enterprise profits, on the base of large-scale industrial data accumulated in medium-thick plate production process, this paper proposes using machine learning and data analysis theories and methods to study the data-driven stress and product defect prediction model of plate. After selecting the data features which have significant effect on the stress of the plate, we establish the logistic classification and forecasting model, and use cross-validation to train and validate the model. The experimental results show that the feature extraction and prediction model can accurately predict the stress defect classification of the medium-thick plate production process.","Temperature measurement,
Temperature distribution,
Heating systems,
Force,
Production,
Predictive models,
Force measurement"
A review and analysis of machine learning and statistical approaches for prediction,"India is the third largest natural rubber producing country of the world, next to Thailand and Indonesia, producing about 9 per cent of the global output. Kerala is the largest natural rubber producing state in India. Among the major plantation crops, natural rubber occupies a major role in agriculture income of Kerala state. The frequent up and down in the rubber price affect the daily life of those involved in the rubber production and consumption. Forecasting is the use of historic data to determine the direction of future trends. Accurate price forecasting of rubber will help the producers and consumers to avoid risk or loss in business. The objective of this survey is to do an experimental analysis for identifying a prediction model which can be used for predicting the Indian natural rubber price with better accuracy and minimum error.","Predictive models,
Mathematical model,
Time series analysis,
Support vector machines,
Computational modeling,
Data models,
Analytical models"
Machine Learning Method Applied in Readout System of Superheated Droplet Detector,"Direct readability is one advantage of superheated droplet detectors in neutron dosimetry. Utilizing such a distinct characteristic, an imaging readout system analyzes image of the detector for neutron dose readout. To improve the accuracy and precision of algorithms in the imaging readout system, machine learning algorithms were developed. Deep learning neural network and support vector machine algorithms are applied and compared with generally used Hough transform and curvature analysis methods. The machine learning methods showed a much higher accuracy and better precision in recognizing circular gas bubbles.","Detectors,
Support vector machines,
Imaging,
Algorithm design and analysis,
Image edge detection,
Machine learning algorithms,
Biological neural networks"
Respiratory Artefact Removal in Forced Oscillation Measurements: A Machine Learning Approach,"Goal: Respiratory artefact removal for the forced oscillation technique can be treated as an anomaly detection problem. Manual removal is currently considered the gold standard, but this approach is laborious and subjective. Most existing automated techniques used simple statistics and/or rejected anomalous data points. Unfortunately, simple statistics are insensitive to numerous artefacts, leading to low reproducibility of results. Furthermore, rejecting anomalous data points causes an imbalance between the inspiratory and expiratory contributions. Methods: From a machine learning perspective, such methods are unsupervised and can be considered simple feature extraction. We hypothesize that supervised techniques can be used to find improved features that are more discriminative and more highly correlated with the desired output. Features thus found are then used for anomaly detection by applying quartile thresholding, which rejects complete breaths if one of its features is out of range. The thresholds are determined by both saliency and performance metrics rather than qualitative assumptions as in previous works. Results: Feature ranking indicates that our new landmark features are among the highest scoring candidates regardless of age across saliency criteria. F1-scores, receiver operating characteristic, and variability of the mean resistance metrics show that the proposed scheme outperforms previous simple feature extraction approaches. Our subject-independent detector, 1IQR-SU, demonstrated approval rates of 80.6% for adults and 98% for children, higher than existing methods. Conclusion: Our new features are more relevant. Our removal is objective and comparable to the manual method. Significance: This is a critical work to automate forced oscillation technique quality control.","Feature extraction,
Pediatrics,
Standards,
Measurement,
Oscillators,
Australia,
Lungs"
Cloud computing threats classification model based on the detection feasibility of machine learning algorithms,"Cloud computing became very popular in past few years, and most of the business and home users rely on its services. Because of its wide usage, cloud computing services became a common target of different cyber-attacks executed by insiders and outsiders. Therefore, cloud computing vendors and providers need to implement strong information security protection mechanisms on their cloud infrastructures. One approach that has been taken for successful threat detection that will lead to the successful attack prevention in cloud computing infrastructures is the application of machine learning algorithms. To understand how machine learning algorithms can be applied for cloud computing threat detection, we propose the cloud computing threat classification model based on the feasibility of machine learning algorithms to detect them. In this paper, we addressed three different criteria types, where we considered three types of classification: a) type of learning algorithm, b) input features and c) cloud computing level. Results proposed in this paper can contribute to further studies in the field of cloud threat detection with machine learning algorithms. More specifically, it will help in selecting appropriate input features, or machine learning algorithms, to obtain higher classification accuracy.","Cloud computing,
Machine learning algorithms,
Computational modeling,
Telecommunication traffic,
Computer crime,
Support vector machines"
Machine learning for performance and power modeling/prediction,"Effective design space exploration relies on fast and accurate pre-silicon performance and power models. Simulation is commonly used for understanding architectural tradeoffs, however many emerging workloads cannot even run on many full-system simulators. Even if you manage to run an emerging workload, it may be a tiny part of the workload, because detailed simulators are prohibitively slow. This talk presents some examples of how machine learning can be used to solve some of the problems haunting the performance evaluation field. An application for machine learning is in cross-platform performance and power prediction. If one model is slow to run real-world benchmarks/workloads, is it possible to predict/estimate its performance/power by using runs on another platform? Are there correlations that can be exploited using machine learning to make cross-platform performance and power predictions? A methodology to perform cross-platform performance/power predictions will be presented in this talk. Another application illustrating the use of machine learning to calibrate analytical power estimation models will be discussed. Yet another application for machine learning has been to create max power stressmarks. Manually developing and tuning so called stressmarks is extremely tedious and time-consuming while requiring an intimate understanding of the processor. In our past research, we created a framework that uses machine learning for the automated generation of stressmarks. In this talk, the methodology of the creation of automatic stressmarks will be explained. Experiments on multiple platforms validating the proposed approach will also be described.","Computational modeling,
Performance evaluation,
Benchmark testing,
Computer architecture,
Digital systems,
Space exploration,
Predictive models"
Application of different machine learning techniques in identifying features of protein sequence data,"This study is carried out on application of different machine learning techniques for prediction of features using bioinformatics data such as that of cancer. The prediction process is undertaken based on feature extraction and then on feature selection process. After selecting the relevant features, machine learning techniques like Support vector machine (SVM), Extreme learning machine (ELM) are applied for classification of the right kind of features. The results obtained with these classifiers are then tested using various test datasets like egfr, TP53, cosmic2plus etc, in order to identify cancer-associated mutations and passenger mutations i.e. neutral polymorphism.","Feature extraction,
Protein sequence,
Support vector machines,
Cancer,
Training,
Prediction algorithms,
Amino acids"
Transformation of discriminative single-task classification into generative multi-task classification in machine learning context,"Classification is one of the most popular tasks of machine learning, which has been involved in broad applications in practice, such as decision making, sentiment analysis and pattern recognition. It involves the assignment of a class/label to an instance and is based on the assumption that each instance can only belong to one class. This assumption does not hold, especially for indexing problems (when an item, such as a movie, can belong to more than one category) or for complex items that reflect more than one aspect, e.g. a product review outlining advantages and disadvantages may be at the same time positive and negative. To address this problem, multi-label classification has been increasingly used in recent years, by transforming the data to allow an instance to have more than one label; the nature of learning, however, is the same as traditional learning, i.e. learning to discriminate one class from other classes and the output of a classifier is still single (although the output may contain a set of labels). In this paper we propose a fundamentally different type of classification in which the membership of an instance to all classes(/labels) is judged by a multiple-input-multiple-output classifier through generative multi-task learning. An experimental study is conducted on five UCI data sets to show empirically that an instance can belong to more than one class, by using the theory of fuzzy logic and checking the extent to which an instance belongs to each single class, i.e. the fuzzy membership degree. The paper positions new research directions on multi-task classification in the context of both supervised learning and semi-supervised learning.","Motion pictures,
Supervised learning,
Lenses,
Emotion recognition,
Handwriting recognition,
Knowledge based systems"
Machine learning techniques in the education process of students of economics,"Nowadays, in the period of the digitalization and knowledge economy development, majority of activities result in the increase of data that should be captured. In each area of business, there is an increasing urge to extract the knowledge from data in a timely manner in order to be able to make shifts that ensure a competitive advantage. Thus, the knowledge of methods and techniques of big data processing is currently a tool of a modern economist. Courses oriented to how to use machine learning tools in the economic practice should become a part of a study plan at universities of economics and business. In our paper, we propose the concept of teaching of machine learning techniques using software Weka as a possible solution to integrate this issue into the study plan based on the example of problem-based learning.","Software,
Economics,
Business,
Data mining,
Education,
Databases,
Machine learning algorithms"
The utilisation of machine learning approaches for medical data classification and personal care system mangementfor sickle cell disease,"The expert systems and smart devices played a key role in the development of health care in terms of continuous monitoring of patients treatment and preservation of E-medication system. The basic challenge that patients faced is the fact of difficulty in contacting physicianspecialists. The problem is there was no direct contact with the physician. This paper proposes an intelligent system that can offer self-care and monitoring system that can simulate the patient based on the application installed on his smartphone. The procedure will be whenever a patient sends his information about his blood test and other tests, the expert system will decide whether the situation is critical or not. In non-criticalcondition, the intelligent system will provide the recommendations and treatment directly. Otherwise, it will contact the physician directly to suggest the proper action that the patient should follow. Further expert system will update information regularly with patient information. A machine-learning algorithm was conductto perform the classification process.","Diseases,
Expert systems,
Monitoring,
Machine learning algorithms,
Medical diagnostic imaging,
Communications technology"
Towards real time machine learning based estimation of fracture risk in osteoporosis patients,"Osteoporosis is a skeletal disorder which leads to bone mass loss and to an increased fracture risk. Recently, physics-based models, employing finite element analysis, have shown great promise in being able to non-invasively estimate biomechanical quantities of interest in the context of osteoporosis. However, these models have high computational demand, limiting their clinical adoption. In this manuscript, we present a machine learning-based model for predicting average strain as an alternative to physics-based approaches. The model is trained on a large database of synthetically generated cancellous bone anatomies, where the target values are computed using the physics-based FEA model. The performance of the trained model was assessed by comparing the predictions against physics-based computations on a separate test data set. Correlation between machine learning and physics-based predictions was very good (0.842, p <; 0.001) and no systematic bias was found in Bland-Altman analysis. Compared to the physics based computation, average execution time was reduced by more than 300 times, leading to real-time assessment of average strain. Average execution time went down from 32.1 ± 3.0 seconds for the FE model to around 0.01 ± 0.005 seconds for the machine learning model on a workstation equipped with 3.0 GHz Intel i7 2-core processor.","Bones,
Computational modeling,
Strain,
Solid modeling,
Three-dimensional displays,
Biological system modeling,
Osteoporosis"
Fault Detection and Classification based on Co-Training of Semi-Supervised Machine Learning,"This paper presents a semi-supervised machine learning approach based on co-training of two classifiers for fault classification in both the transmission and the distribution systems with consideration of micro-grids. Unlike previous work in which only labeled data are treated using supervised machine learning approaches, this work uses a semi-supervised machine learning approach to handle both labeled and unlabeled data. In order to extract the hidden features in the current and voltage waveforms, the discrete wavelet transform is applied while the harmony search algorithm is utilized to identify the optimal parameters of the wavelets. The performance of the proposed method was examined on both transmission and distribution test systems in a simulation environment, and also using experimental hardware. The results have shown that the proposed approach provides flexibility and adaptability in dealing with various system conditions/configurations with high accuracy. The results also have demonstrated that the proposed semi-supervised approach can improve the fault classification accuracy compared to that obtained using other machine learning approaches (i.e. supervised and unsupervised) in the case of utilizing unlabeled data to build and train the classifier's model.","Discrete wavelet transforms,
Training,
Data models,
Support vector machines,
Estimation"
Machine learning techniques for classification of diabetes and cardiovascular diseases,"This paper presents the overview of machine learning techniques in classification of diabetes and cardiovascular diseases (CVD) using Artificial Neural Networks (ANNs) and Bayesian Networks (BNs). The comparative analysis was performed on selected papers that are published in the period from 2008 to 2017. The most commonly used type of ANN in selected papers is multilayer feedforward neural network with Levenberg-Marquardt learning algorithm. On the other hand, the most commonly used type of BN is Na'ive Bayesian network which shown the highest accuracy values for classification of diabetes and CVD, 99.51% and 97.92% retrospectively. Moreover, the calculation of mean accuracy of observed networks has shown better results using ANN, which indicates that higher possibility to obtain more accurate results in diabetes and/or CVD classification is when it is applied to ANN.","Diabetes,
Artificial neural networks,
Bayes methods,
Diseases,
Learning (artificial intelligence),
Biological neural networks,
Classification algorithms"
Machine Learning Methods for Solving Complex Ranking and Sorting Issues in Human Resourcing,"Every organization doesn't necessary to have the common point of view of a particular resume while considering for a job description (JD). Keeping the same role in place, while some stress on technical skills, the other give importance to professional experience and domain expertise. Understanding these hiring patterns are becoming important in today's head hunting. The traditional job search engines offers resumes which matches to the input keywords. As the search outcomes from these search engines grows, the problem in selecting the best profile surges. The role of Human Resource (HR) staff becomes more important in understanding these hiring patterns and suggesting the suitable profiles. HR staff proposes these profiles which are ranked manually. The proposed method is to understand the intelligence behind the hiring pattern and apply the machine learning to accommodate the identified intelligence. The proposed method offers the ranking system according to the hiring patterns. Highly trained models along with the traditional search method, predicts the ranking and sorting of resumes with high accuracy and simplifies the job of human resourcing efficiently.","Resumes,
Support vector machines,
Qualifications,
Companies,
Databases,
Communication system security"
Machine learning based side-channel-attack countermeasure with hamming-distance redistribution and its application on advanced encryption standard,"Side channel analysis (SCA) is effective to reveal the key of crypto devices by applying statistical analysis to a number of power traces, thus hardware countermeasure is necessary to protect the crypto circuits. A SCA-resistance methodology by machine learning trained power compensation module is proposed to compensate the probability of hamming distance (HD) of the intermediate data directly, to make it unable to be distinguished from correct and incorrect sub-key, thus providing resistance to SCA. The machine learning algorithm is used to find out the best HD redistribution mapping by using neural dynamic programming. Implemented on an AES-128 encryption algorithm circuit on a Xilinx Spartan-6 FPGA mounted on a SAKURA-G board, experimental SCA results show that it can provide more than 200 × measures to disclosure and still has no sign to reveal the advanced encryption standard (AES) sub-key. In addition, it has low power and area overhead and zero frequency overhead, thus is appropriate for hardware implementation of SCA countermeasure.","cryptography,
field programmable gate arrays,
learning (artificial intelligence),
statistical analysis"
The use of machine learning techniques to classify power transmission line fault types and locations,"Transmission lines are very important component of the electric power system. Therefore it is necessary to predict and detect transmission lines fault types and locations to enhance the power system protection scheme and increase its reliability. This paper investigates the use of four powerful machine learning classifiers to detect and predict fault types and locations over a 750KV, 600km long power transmission line. Bagging, Boosting, radial basis functions and naïve Bayesian classifiers were utilized for locating and detecting faults in a power transmission line. Findings exhibits that using machine learning technique could be feasible for such task and may represent a great opportunity to increase the power system protection and efficiency.","Circuit faults,
Power transmission lines,
Bagging,
Boosting,
Bayes methods,
Radial basis function networks,
Radio frequency"
Diagnosis of liver diseases using machine learning,Liver Diseases account for over 2.4% of Indian deaths per annum. [14] Liver disease is also difficult to diagnose in the early stages owing to subtle symptoms. Often the symptoms become apparent when it is too late. [1] This paper aims to improve diagnosis of liver diseases by exploring 2 methods of identification-patient parameters and genome expression. The paper also discusses the computational algorithms that can be used in the aforementioned methodology and lists demerits. It proposes methods to improve the efficiency of these algorithms.,"Proteins,
DNA,
Liver diseases,
Machine learning algorithms,
Support vector machines,
Diseases"
Safety helmet wearing detection based on image processing and machine learning,"Safety helmet wearing detection is very essential in power substation. This paper proposed a innovative and practical safety helmet wearing detection method based on image processing and machine learning. At first, the ViBe background modelling algorithm is exploited to detect motion object under a view of fix surveillant camera in power substation. After obtaining the motion region of interest, the Histogram of Oriented Gradient (HOG) feature is extracted to describe inner human. And then, based on the result of HOG feature extraction, the Support Vector Machine (SVM) is trained to classify pedestrians. Finally, the safety helmet detection will be implemented by color feature recognition. Compelling experimental results demonstrated the correctness and effectiveness of our proposed method.","Safety,
Support vector machines,
Feature extraction,
Image color analysis,
Substations,
Surveillance,
Histograms"
Millimetric diagnosis: Machine learning based network analysis for mm-wave communication,"Troubleshooting millimeter-wave (mm-wave) wireless networks is complex due to the directionality of the communication. Issues such as deafness, misaligned antennas, or blockage may severely impact network performance, and identifying them is crucial to improve network deployments. To this end, access to lower-layer information is important. However, commercial off-the-shelf mm-wave wireless devices typically do not provide such information. Even if they would, detecting effects such as deafness based on information of a single node that forms part of the network is typically hard. In this paper, we present the design and evaluation of an external sniffing device that can infer the aforementioned performance issues only using narrowband physical layer energy traces. Our sniffer does not need to decode any data, resulting in a simple but effective approach which also preserves privacy and works on encrypted networks. Our key contribution is a machine learning framework which enables automated energy trace analysis while coping with the non-stationarity of the traces. We evaluate its performance in practice using off-the-shelf wireless devices operating in the 60 GHz band. Our results show that the above framework correctly infers physical layer events in virtually all cases, thus providing valuable information to troubleshoot issues in mm-wave networks.","Hidden Markov models,
Physical layer,
Random variables,
Tools,
Performance evaluation,
Deafness,
Antennas"
Text Mining Approach for Product Quality Enhancement: (Improving Product Quality through Machine Learning),"Text mining, also referred to as text data mining, is the process of extracting interesting and non-trivial patterns or knowledge from text documents. It uses algorithms to transform free flow text (unstructured) into data that can be analyzed (structured) by applying Statistical, Machine Learning and Natural Language Processing (NLP) techniques. Text mining is an evolving technology that allows enterprises to understand their customers well, and help them in redefining customer needs. As e-commerce is becoming more and more established, the number of customer reviews and feedback that a product receives has grown rapidly over a period of time. For a popular asset, the number of review comments can be in thousands or even more. This makes it difficult for the manufacturer to read all of them to make an informed decision in improving product quality and support. Again it is difficult for the manufacturer to keep track and to manage all customer opinions. This article attempts to derive some meaningful information from asset reviews which will be used in enhancing asset features from engineering point of view and helps in improving the support quality and customer experience.","Support vector machines,
Text mining,
Market research,
Quality assessment,
Product design,
Natural language processing"
Modelling human behaviour in smart home energy management systems via machine learning techniques,"Mankind is progressing towards automation solutions that operate via Internet of Things in their everyday lives, which has developed over the years and its demanding nature compels the emerging technologies to smoothen the modern day living. Critical issues like increasing domestic energy usage, if addressed via machine learning techniques in current IOT devices will tremendously help humans in unimaginable ways. The concern of price hikes in energy and the adverse effects on the environment due to inefficient energy usage is a subject that concerns a larger segment of the population. Human behavior plays an important role in energy expenditure. Markets shall never cease to flood with electronic gadgets and humans shall never cease to fancy the upcoming ones that majorly contributes to increasing energy consumption. So we propose a framework with load optimization strategy that shall be responsive to realtime pricing which will exclude frequent human intervention. The load will be optimized during the times of high pricing or when the total power consumption of the household exceeds the maximum constraint. However, Optimal Load Management strategy requires price prediction that can be generated using real-time price algorithm and human activity prediction to ensure discrete decision making. This paper proposes an idea to mitigate energy consumption by modeling human behavior via a combination of Hidden Markov Model and Naive Bayes classifier. Also, we try to minimize the errors generated due to motion/occupancy sensors while detecting a human or an animal(pet). Sometimes activation of motion sensors can result in the wrong detection which can further lead to incorrect activity recognition which then can generate errors in optimizing loads. Therefore, we integrate classification algorithms in our framework which can further aid in correct detection and subsequent strategy generation.","Hidden Markov models,
Energy management,
Energy consumption,
Real-time systems,
Smart homes,
Pricing,
Home appliances"
An analysis of machine learning techniques (J48 & AdaBoost)-for classification,"Extraction of relevant Information from data Is a challenging task. Many times an analyst may end up with an erroneous classifier because of huge, redundant, unreliable and noisy data. It may also be due to misinterpretation of results and usage of inappropriate techniques for a specific situation. In our study, we have investigated the two main approaches in data mining which are Decision Tree (J48 algorithm) and Ensemble Learning Technique (AdaBoost). We have also performed a comparative analysis of these two techniques. The tool which we have used is WEKA which is an open source's software. The datasets which we have used are supermarket.arff, labor.arff, soybean.arff, & segment.arff. Here. arff is an abbreviation of Attribute Relation file Format which is the standard format that is accepted by WEKA for the datasets. The sample size of training set is number of attributes and number of instances available in the dataset. Classification models are assessed on the basis of number of class labels in the dataset, accuracy, amount and length of the generated rules, error rate and standard deviation. Based on number of experiments which we have performed, results show that AdaBoost provides better accuracy than Decision Tree (J48 algorithm) when the number of class labels in the dataset are exactly two whereas the Decision Tree generate rules faster than the Adaboost and when number of class labels in the dataset are more than two then J48 algorithm performs better than Adaboost. The results show the tradeoff of using the two approaches for other researchers in finding the best model for a particular problem.","Decision trees,
Data mining,
Predictive models,
Algorithm design and analysis,
Tools,
Cleaning,
Classification algorithms"
Predictive analytics for banking user data using AWS Machine Learning cloud service,"The aim of the project is to develop a Machine Learning model to perform predictive analytics on the banking dataset. The banking data set consists of details about customers like and whether the customer will buy a product provided by the bank or not. The data set is obtained from University of California Irvine Machine Learning Repository. This data set is used to create a binary classification model using Amazon Web Service(AWS) Machine Learning platform. 70 % of the data is used to train the binary classification model and 30 % of the dataset is used to test the model. Depending upon the test result we evaluate the essential parameters like precision, recall, accuracy and false positive rates. These parameters evaluate the efficiency of our model. Once we design our model we test our model using two features in AWS Machine learning. One, using real time prediction where we give real time input data and test our model. Two, we do batch prediction, where we have a set of customer data and we upload our data to evaluate our prediction.","Predictive models,
Data models,
Mathematical model,
Logistics,
Machine learning algorithms,
Numerical models,
Prediction algorithms"
Katie Malone on Machine Learning,Host Edaena Salinas talks with Civis Analytics' Katie Malone about the basics of machine learning and why we'll be seeing it much more frequently. The Web extra at http://www.se-radio.net/2017/03/se-radio-episode-286-katie-malone-intro-to-machine-learning/ is an audio recording of this episode of Software Engineering Radio.,"Motion pictures,
Machine learning,
Software engineering,
Measurement,
Artificial intelligence,
Classification algorithms"
Aspect based feature extraction and sentiment classification of review data sets using Incremental machine learning algorithm,"As there is a exponential growth of social networks and due to large usage of social media, there is a increasing demand for data in the web for the users which leads to recent trends and ideas in the field of research. The users will be eagerly using these data for the future purpose and get information about the opinions of others thus there is a need of automatic summarization of opinion of the web users. Opinion Mining is a process of extracting and analyzing people's opinion regarding the given object and Sentiment Analysis explains about the hidden sentiments in the opinions. A most important challenge in this is to identify the sentiments and aspects and then perform the data classification based on these features and this process is called aspect based feature extraction. Opinion summarization can be done using machine learning such as maximum entropy, naive bayes, Support vector machine (SVM) model and, Random forest technique. The data sources to be taken for the proposed method are from customer review of product. In this paper experiments were conducted to compare the performance of proposed iterative decision tree method against other machine learning algorithms like SVM, Baseline, and Naive Bayes.","Feature extraction,
Sentiment analysis,
Support vector machines,
Machine learning algorithms,
Data mining,
Classification algorithms,
Bayes methods"
Optimal Mass Transport: Signal processing and machine-learning applications,"Transport-based techniques for signal and data analysis have recently received increased interest. Given their ability to provide accurate generative models for signal intensities and other data distributions, they have been used in a variety of applications, including content-based retrieval, cancer detection, image superresolution, and statistical machine learning, to name a few, and they have been shown to produce state-of-the-art results. Moreover, the geometric characteristics of transport-related metrics have inspired new kinds of algorithms for interpreting the meaning of data distributions. Here, we provide a practical overview of the mathematical underpinnings of mass transport-related methods, including numerical implementation, as well as a review, with demonstrations, of several applications. Software accompanying this article is available from [43].","Linear programming,
Data models,
Estimation,
Probability density function,
Transportation,
Analytical models,
Morphology"
Mood Detector - On Using Machine Learning to Identify Moods and Emotions,"This paper presents the design and implementation of the Mood Detector application, which is designed to detect the mood and emotional state of the user by analyzing 3 physical parameters (pulse, skin electro conductivity and temperature) by using a machine learning algorithm which is trained with data from the users. The application has been tested repetitively until the results generated by the learning algorithm have been validated 100%, thus confirming that the machine learning algorithms provides the correct output. The application also integrates a music recommender system, which suggests the user to listen to specific playlists, designed according to his currently detected mood.","Mood,
Temperature sensors,
Detectors,
Machine learning algorithms,
Skin,
Intelligent sensors"
Utilization of Machine Learning method in prediction of UCG data,"Underground coal gasification (UCG) represents a technology, which can mining coal from a difficult to access coal deposits and coal beds with tectonic faults. This technology is also less expensive than conventional coal mining. In UCG can be measurable a lot of process variables with a common measuring devices and techniques but there are a variables that can not be mesured so easily e.g. underground temperature. On the other hand, it is also necessary to know in advance what will be on the output from the process at different input operating parameters. This paper research the possibility of utilization support vector machines (SVMs) in order to UCG data prediction (i.e. syngas calorific value or underground temperature). In machine learning, support vector machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Provided result were obtained with utilization of Matlab and its statistical toolbox. The analysis was performed on data from the experimental laboratory gasifier.","Support vector machines,
Temperature measurement,
Syngas,
Coal gas,
Coal,
Kernel,
Monitoring"
Applying machine learning techniques to mine ventilation control systems,"The purpose of the research is determination of mine ventilation system regulators positions providing required airflow on ventilated directions. Currently regulators positions are set iteratively that causes hunting. It is proposed to use historical data of the system for defining regulators functional dependencies on required airflow values with consideration of temporal variability of a ventilation network. The problem is solved by a regression model based on neural networks. Consequently, a set of model parameters is defined and the control algorithm of the system is modified for using a historical data set.","Ventilation,
Regulators,
Fuel processing industries,
Atmospheric modeling,
Neural networks,
Predictive models"
Smart Sensing of Loads in an Extra Low Voltage DC Pico-grid using Machine Learning Techniques,"Aiming at the rising number of dc appliances and the growing interest in their monitoring systems, this paper describes the injection of intelligence into dc pico-grids that are made up of “dumb” appliances and loads. Due to reality of economic, dc appliances and loads are usually low in cost and lack intelligence and communication features for effective monitoring and management. This paper proposes a smart sensor design for dc pico-grid with the use of a single sensor multiple loads and states detection in monitoring the “dumb” appliances. This eliminates the need to have intelligence and communication features for every appliance. With the smart sensor, several such smart dc pico-grids can be bundled into bigger scale of smart nano-grid or micro-grid. In addition to knowing how much energy or power the pico-grid is using; the smart sensor also provides load disaggregation and state-change detection. The states of the loads can be learnt and detected via the signatures and features obtained from the transient state or the steady state of the entire grid’s current waveform. Computational intelligence techniques, k-Nearest Neighbours, K-Means clustering and other algorithms are used in the system for loads classification and state-change detection. Working together with the software in the smart sensor are hardware implementation of low cost operational amplifiers and logic gates; these hardware help to share the burden on the controller and release resources for the controller to perform more advanced processes. Experimental results are presented to demonstrate the operation of the smart sensor in dc pico-grid.","Steady-state,
Intelligent sensors,
Monitoring,
Transient analysis,
Home appliances,
Energy management"
Online recurrent extreme learning machine and its application to time-series prediction,"Online sequential extreme learning machine (OS-ELM) is an online learning algorithm training single-hidden layer feedforward neural networks (SLFNs), which can learn data one-by-one or chunk-by-chunk with fixed or varying data size. Due to its characteristics of online sequential learning, OS-ELM is popularly used to solve time-series prediction problem, such as stock forecast, weather forecast, passenger count forecast, etc. OS-ELM, however, has two fatal drawbacks: Its input weights cannot be adjusted and it cannot be applied to learn recurrent neural network (RNN). Therefore we propose a modified version of OS-ELM, called online recurrent extreme learning machine (OR-ELM), which is able to adjust input weights and can be applied to learn RNN, by applying ELM-auto-encoder and a normalization method called layer normalization (LN). Proposed method is used to solve a time-series prediction problem on NewYork City passenger count dataset, and the results show that R-ELM outperforms OS-ELM and other online-sequential learning algorithms such as hierarchical temporal memory (HTM) and online long short-term memory (online LSTM).","Prediction algorithms,
Training,
Feature extraction,
Recurrent neural networks,
Machine learning algorithms,
Weather forecasting,
Urban areas"
Parallel Computing for Machine Learning in Social Network Analysis,"Machine learning, especially deep learning, is revolutionizing how many engineering problems are being solved. Three critical ingredients are needed to apply deep machine learning to significant real world problems: i.) large data sets; ii.) software to implement deep learning and; iii.) significant computing cycles. This paper discusses the state of each ingredient with a specific focus on: a.) how deep learning can apply to large-scale social network analysis and; b.) the computing resources required to make such analyses feasible.","Facebook,
Data models,
Graphics processing units,
Machine learning,
Market research,
Neural networks"
Machine learning approaches for the prediction of obesity using publicly available genetic profiles,"This paper presents a novel approach based on the analysis of genetic variants from publicly available genetic profiles and the manually curated database, the National Human Genome Research Institute Catalog. Using data science techniques, genetic variants are identified in the collected participant profiles and then indexed as risk variants in the National Human Genome Research Institute Catalog. Indexed genetic variants or Single Nucleotide Polymorphisms are used as inputs in various machine learning algorithms for the prediction of obesity. Body mass index status of participants is divided into two classes, Normal Class and Risk Class. Dimensionality reduction tasks are performed to generate a set of principal variables - 13 SNPs - for the application of various machine learning methods. The models are evaluated using receiver operator characteristic curves and the area under the curve. Machine learning techniques including gradient boosting, generalized linear model, classification and regression trees, k-nearest neighbours, support vector machines, random forest and multilayer perceptron neural network are comparatively assessed in terms of their ability to identify the most important factors among the initial 6622 variables describing genetic variants, age and gender, to classify a subject into one of the body mass index related classes defined in this study. Our simulation results indicated that support vector machine generated the highest area under the curve value of 90.5%.","Obesity,
Bioinformatics,
Genomics,
Diseases,
Databases,
Support vector machines"
Predicting QoE in cellular networks using machine learning and in-smartphone measurements,"Monitoring the Quality of Experience (QoE) undergone by cellular network customers has become paramount for cellular ISPs, who need to ensure high quality levels to limit customer churn due to quality dissatisfaction. This paper tackles the problem of QoE monitoring, assessment and prediction in cellular networks, relying on end-user device (i.e., smart-phone) QoS passive traffic measurements and QoE crowdsourced feedback. We conceive different QoE assessment models based on supervised machine learning techniques, which are capable to predict the QoE experienced by the end user of popular smartphone apps (e.g., YouTube and Facebook), using as input the passive in-device measurements. Using a rich QoE dataset derived from field trials in operational cellular networks, we benchmark the performance of multiple machine learning based predictors, and construct a decision-tree based model which is capable to predict the per-user overall experience and service acceptability with a success rate of 91% and 98% respectively To the best of our knowledge, this is the first paper using end-user, in-device passive measurements and machine learning models to predict the QoE of smartphone users in operational cellular networks.","cellular radio,
decision trees,
learning (artificial intelligence),
quality of experience,
smart phones,
telecommunication computing,
telecommunication traffic"
Side-channel analysis and machine learning: A practical perspective,"The field of side-channel analysis has made significant progress over time. Side-channel analysis is now used in practice in design companies as well as in test laboratories, and the security of products against side-channel attacks has significantly improved. However, there are still some remaining issues to be solved for side-channel analysis to become more effective. Side-channel analysis consists of two steps, commonly referred to as identification and exploitation. The identification consists of understanding the leakage and building suitable models. The exploitation consists of using the identified leakage models to extract the secret key. In scenarios where the model is poorly known, it can be approximated in a profiling phase. There, machine learning techniques are gaining value. In this paper, we conduct extensive analysis of several machine learning techniques, showing the importance of proper parameter tuning and training. In contrast to what is perceived as common knowledge in unrestricted scenarios, we show that some machine learning techniques can significantly outperform template attacks when properly used. We therefore stress that the traditional worst case security assessment of cryptographic implementations, that mainly includes template attacks, might not be accurate enough. Besides that, we present a new measure called the Data Confusion Factor that can be used to assess how well machine learning techniques will perform on a certain dataset.","Side-channel attacks,
Correlation,
Tuning,
Noise measurement,
Signal to noise ratio"
Deep Learning for Infrared Thermal Image Based Machine Health Monitoring,"The condition of a machine can automatically be identified by creating and classifying features that summarize characteristics of measured signals. Currently, experts, in their respective fields, devise these features based on their knowledge. Hence, the performance and usefulness depends on the expert's knowledge of the underlying physics, or statistics. Furthermore, if new and additional conditions should be detectable, experts have to implement new feature extraction methods. To mitigate the drawbacks of feature engineering, a method from the sub-field of feature learning, i.e. deep learning, more specifically convolutional neural networks, is researched in this article. The objective of this article is to investigate if and how deep learning can be applied to infrared thermal video to automatically determine the condition of the machine. By applying this method on infrared thermal data in two use cases, i.e. machine fault detection and oil level prediction, we show that the proposed system is able to detect many conditions in rotating machinery very accurately (i.e. 95 % and 91.67 % accuracy for the respective use cases) without requiring any detailed knowledge about the underlying physics, and thus having the potential to significantly simplify condition monitoring using complex sensor data. Furthermore, we show that by using the trained neural networks, important regions in the infrared thermal images can be identified related to specific conditions which can potentially lead to new physical insights.","Feature extraction,
Artificial neural networks,
Condition monitoring,
Machine learning algorithms,
Vibrations,
Machine learning"
On the robustness of machine learning based malware detection algorithms,"With the rapid popularity of the Internet, a large amount of new malware is produced every day, while the traditional signature based malware detection algorithm is unable to detect such unseen malware. In recent years, many machine learning based algorithms have been proposed to detect new malware, and several of these algorithms are able to achieve quite good detection performance when supplied with plenty of training data. However, most of these algorithms just focus on how to improve the classification performance, while the robustness is not taken into consideration. This paper performs a detailed analysis on the robustness of four well-known machine learning based malware detection approaches, i.e. the DLL and API feature, the string feature, PE-Miner and the byte level N-Gram feature. We proposed two pretense approaches under which malware is able to pretend to be benign and bypass the detection algorithms. Experimental results show that the performances of these detection algorithms decline greatly under the pretense approaches. The lack of robustness makes these algorithms unable to be used in real world applications. In future works of machine learning based malware detection, researchers have to take the problem of robustness seriously.",
FPGA-Accelerated Dense Linear Machine Learning: A Precision-Convergence Trade-Off,"Stochastic gradient descent (SGD) is a commonly used algorithm for training linear machine learning models. Based on vector algebra, it benefits from the inherent parallelism available in an FPGA. In this paper, we first present a single-precision floating-point SGD implementation on an FPGA that provides similar performance as a 10-core CPU. We then adapt the design to make it capable of processing low-precision data. The low-precision data is obtained from a novel compression scheme-called stochastic quantization, specifically designed for machine learning applications. We test both full-precision and low-precision designs on various regression and classification data sets. We achieve up to an order of magnitude training speedup when using low-precision data compared to a full-precision SGD on the same FPGA and a state-of-the-art multi-core solution, while maintaining the quality of training. We open source the designs presented in this paper.","Field programmable gate arrays,
Quantization (signal),
Training,
Stochastic processes,
Hardware,
Machine learning algorithms,
Data models"
LUNA: Quantifying and Leveraging Uncertainty in Android Malware Analysis through Bayesian Machine Learning,"Android's growing popularity seems to be hindered only by the amount of malware surfacing for this open platform. Machine learning algorithms have been successfully used for detecting the rapidly growing number of malware families appearing on a daily basis. Existing solutions along these lines, however, have a common limitation: they are all based on classical statistical inference and thus ignore the concept of uncertainty invariably involved in any prediction task. In this paper, we show that ignoring this uncertainty leads to incorrect classification of both benign and malicious apps. To reduce these errors, we utilize Bayesian machine learning - an alternative paradigm based on Bayesian statistical inference - which preserves the concept of uncertainty in all steps of calculation. We move from a black-box to a white-box approach to identify the effects different features (such as sensitive resource usage, declared activities, services and intent filters etc.) have on the classification status of an app. We show that incorporating uncertainty in the learning pipeline helps to reduce incorrect decisions, and significantly improves the accuracy of classification. We achieve a false positive rate of 0.2% compared to the previous best of 1%. We present sufficient details to allow the reader to reproduce our results through openly available probabilistic programming tools and to extend our techniques well beyond the boundaries of this paper.","Uncertainty,
Bayes methods,
Androids,
Humanoid robots,
Malware,
Security,
Pipelines"
A compressive multi-kernel method for privacy-preserving machine learning,"As the analytic tools become more powerful, and more data are generated on a daily basis, the issue of data privacy arises. This leads to the study of the design of privacy-preserving machine learning algorithms. Given two objectives, namely, utility maximization and privacy-loss minimization, this work is based on two previously non-intersecting regimes - Compressive Privacy and multi-kernel method. Compressive Privacy is a privacy framework that employs utility-preserving lossy-encoding scheme to protect the privacy of the data, while multi-kernel method is a kernel-based machine learning regime that explores the idea of using multiple kernels for building better predictors. In relation to the neural-network architecture, multi-kernel method can be described as a two-hidden-layered network with its width proportional to the number of kernels. The compressive multi-kernel method proposed consists of two stages - the compression stage and the multi-kernel stage. The compression stage follows the Compressive Privacy paradigm to provide the desired privacy protection. Each kernel matrix is compressed with a lossy projection matrix derived from the Discriminant Component Analysis (DCA). The multikernel stage uses the signal-to-noise ratio (SNR) score of each kernel to non-uniformly combine multiple compressive kernels. The proposed method is evaluated on two mobile-sensing datasets - MHEALTH and HAR - where activity recognition is defined as utility and person identification is defined as privacy. The results show that the compression regime is successful in privacy preservation as the privacy classification accuracies are almost at the random-guess level in all experiments. On the other hand, the novel SNR-based multi-kernel shows utility classification accuracy improvement upon the state-of-the-art in both datasets. These results indicate a promising direction for research in privacy-preserving machine learning.",
Design of an explainable machine learning challenge for video interviews,"This paper reviews and discusses research advances on “explainable machine learning” in computer vision. We focus on a particular area of the “Looking at People” (LAP) thematic domain: first impressions and personality analysis. Our aim is to make the computational intelligence and computer vision communities aware of the importance of developing explanatory mechanisms for computer-assisted decision making applications, such as automating recruitment. Judgments based on personality traits are being made routinely by human resource departments to evaluate the candidates' capacity of social insertion and their potential of career growth. However, inferring personality traits and, in general, the process by which we humans form a first impression of people, is highly subjective and may be biased. Previous studies have demonstrated that learning machines can learn to mimic human decisions. In this paper, we go one step further and formulate the problem of explaining the decisions of the models as a means of identifying what visual aspects are important, understanding how they relate to decisions suggested, and possibly gaining insight into undesirable negative biases. We design a new challenge on explainability of learning machines for first impressions analysis. We describe the setting, scenario, evaluation metrics and preliminary outcomes of the competition. To the best of our knowledge this is the first effort in terms of challenges for explainability in computer vision. In addition our challenge design comprises several other quantitative and qualitative elements of novelty, including a “coopetition” setting, which combines competition and collaboration.","Computational modeling,
Computer vision,
Machine learning,
Interviews,
Predictive models,
Natural language processing,
Analytical models"
Machine learning approaches to predict learning outcomes in Massive open online courses,"With the rapid advancements in technology, Massive Open Online Courses (MOOCs) have become the most popular form of online educational delivery, largely due to the removal of geographical and financial barriers for participants. A large number of learners globally enrol in such courses. Despite the flexible accessibility, results indicate that the completion rate is quite low. Educational Data Mining and Learning Analytics are emerging fields of research that aim to enhance the delivery of education through the application of various statistical and machine learning approaches. An extensive literature survey indicates that no significant research is available within the area of MOOC data analysis, in particular considering the behavioural patterns of users. In this paper, therefore, two sets of features, based on learner behavioural patterns, were compared in terms of their suitability for predicting the course outcome of learners participating in MOOCs. Our Exploratory Data Analysis demonstrates that there is strong correlation between click stream actions and successful learner outcomes. Various Machine Learning algorithms have been applied to enhance the accuracy of classifier models. Simulation results from our investigation have shown that Random Forest achieved viable performance for our prediction problem, obtaining the highest performance of the models tested. Conversely, Linear Discriminant Analysis achieved the lowest relative performance, though represented only a marginal reduction in performance relative to the Random Forest.","Data mining,
Correlation,
Certification,
Feature extraction,
Data analysis,
Support vector machines"
A hybrid algorithm of differential evolution and machine learning for electromagnetic structure optimization,"Various electromagnetic (EM) structures become more complex and often have increasing degrees of design freedom. Classical optimization methods require numerous simulation trials of different parameter combinations, which leads to a low design efficiency. To address this problem, an efficient EM structure optimization algorithm which combines differential evolution (DE) with machine learning technology is proposed in this paper. By partly substituting electromagnetic (EM) solver, Kriging model predicts the responses and uncertainties of each individual after differential evolution. The exploration and exploitation of the searching can be adjusted by the constitution and prescreening of the population before and after evolution. This algorithm is applied to optimize the resonant frequencies of an E-shaped antenna with 6 variables. Comparing with the other optimization methods, the number of EM simulations needed is reduced by about 80%.","Optimization,
Algorithm design and analysis,
Machine learning algorithms,
Electromagnetics,
Sociology,
Statistics,
Genetic algorithms"
Combining Machine-Learning with Invariants Assurance Techniques for Autonomous Systems,"Autonomous Systems are systems situated in some environment and are able of taking decision autonomously. The environment is not precisely known at design-time and it might be full of unforeseeable events that the autonomous system has to deal with at run-time. This brings two main problems to be addressed. One is that the uncertainty of the environment makes it difficult to model all the behaviours that the autonomous system might have at the design-time. A second problem is that, especially for safety-critical systems, maintaining the safety requirements is fundamental despite the system's adaptations. We address such problems by shifting some of the assurance tasks at run-time. We propose a method for delegating part of the decision making to agent-based algorithms using machine learning techniques. We then monitor at run-time that the decisions do not violate the autonomous system's safety-critical requirements and by doing so we also send feedback to the decision-making process so that it can learn. We plan to implement this approach using reinforcement learning for decision making and predictive monitoring for checking at run-time the preservation and/or violation of invariant properties of the system. We also plan to validate it using ROS as software middleware and miniaturized vehicles and real vehiclesas hardware.","Autonomous systems,
Monitoring,
Decision making,
Learning (artificial intelligence),
Software,
Training,
Conferences"
Use of Synthetic Benchmarks for Machine-Learning-Based Performance Auto-Tuning,"We explore the use of synthetic benchmarks for the training phase of machine-learning-based automatic performance tuning. We focus on the problem of predicting if the use of local memory on a GPU is beneficial for caching a single target array in a GPU kernel. We show that the use of only 13 real benchmarks leads to poor prediction accuracy (about to 58%) of the 13 leave-one-out models trained using these benchmarks, even when the model features are sufficiently comprehensive. We define a metric, called the average vicinity density to measure the quality of a training set. We then use it to demonstrate that the poor accuracy of the models built with the real benchmarks is indeed because of the limited size and coverage of the training set. In contrast, the use of 90K properly generated set of synthetic benchmarks leads to significantly better accuracies, up to 87%. These results validate our approach of using synthetic benchmarks for training machine learning models. We describe a synthetic benchmark template for the local memory optimization. We then present two approaches to using this template and a seed set of real benchmarks to generate a large number of synthetic benchmark. We also explore the impact of the number of synthetic benchmarks used in training.","Kernel,
Benchmark testing,
Optimization,
Training,
Graphics processing units,
Predictive models,
Arrays"
A novel machine learning framework for phenotype prediction based on genome-wide DNA methylation data,"DNA methylation (DNAm) is an epigenetic mechanism used by cells to control gene expression, and identification of DNAm biomarkers can assist in early diagnosis of cancer. Identification of these biomarkers can be done using CpG (Cytosine-phosphate guanine) sites, or particular regions in DNA. Previous machine learning methods known as MS-SPCA and EVORA have been used to link DNAm biomarkers to specific stages of cervical cancer using CpG data. In this paper, we propose a novel machine learning framework that yields greater AUC accuracy than the MS-SPCA and EVORA for predicting stages of cervical cancer using CpG data. This framework appears to be promising in regards to the data examined herein as well as for future biological studies.","Cancer,
DNA,
Predictive models,
Principal component analysis,
Data models,
Biomarkers,
Analytical models"
Using Machine Learning for Data Center Cooling Infrastructure Efficiency Prediction,"Power consumption continues to remain a critical aspect for High Performance Computing (HPC) data centers. It becomes even more crucial for Exascale computing since scaling today's fastest system to an Exaflop level would consume more than 168 MW power which is 8 times higher than the 20 MW power consumption goal set, at the time of this publication, by the US Department of Energy. This naturally leads to a necessity for energy efficiency improvement that will encompass the full chain of the power consumers, starting from the data center infrastructure, including cooling overheads and electrical losses, up to compute resource scheduling and application scaling. In this paper a machine learning approach is proposed to model the Coefficient of Performance (COP) of HPC data center's hot water cooling loop. The suggested model is validated on operational data obtained at Leibniz Supercomputing Centre (LRZ). The paper shows how this COP model can help to improve the energy efficiency of modern HPC data centers.","Cooling,
Power demand,
Data models,
Temperature measurement,
Monitoring,
Buildings,
Neural networks"
A Machine Learning Approach for Determining the Validity of Traceability Links,"Traceability Link Recovery (TLR) is a fundamental software maintenance task in which links are established between related software artifacts of different types (e.g., source code, documentation, requirements specifications, etc.) within a system. Existing approaches to TLR often require a human to analyze a long list of potential links and distinguish valid links from invalid ones. Here we present an approach which bypasses this intermediate step and automatically classifies links as valid or invalid using a machine learning approach and features such as text retrieval (TR) rankings and query quality (QQ) metrics. We performed an evaluation on recovering traceability links in three software systems and the results show the potential of our approach, which achieved 95% accuracy on average using both types of features.","Software engineering,
Measurement,
Software systems,
Conferences,
Documentation,
Context"
Cellular computational extreme learning machine network based frequency predictions in a power system,"Frequency is one of the most important characteristics in power system monitoring, control and protection. Frequency variations can be observed with significant changes in operating conditions. High penetration levels of renewable energy pose variability and uncertainty challenges for grid operation. It is essential to have innovative methodologies to take necessary actions to overcome these challenges. Power system frequency prediction provides an insight to better system control and protection. In this paper, a cellular computational extreme learning machine network (CCELMN) based frequency prediction approach is presented. Results are compared with those obtained with independent ELM models and persistence model and shown to outperform.","Phasor measurement units,
Automatic generation control,
Real-time systems,
Power system dynamics,
Time-frequency analysis"
Machine learning of SVM classification utilizing complete binary tree structure for PAM-4/8 optical interconnection,A machine learning method of effective nonlinear decision frame for PAM-N system based on support vector machine (SVM) using complete binary tree (CBT) structure is demonstrated in this work. The simulations results indicate improved performance by the proposed classifier which enhances the power sensitivity by 2-dB and 6-dB at the receiver side in 100-Gbps PAM-4 and PAM-8 systems respectively.,"Support vector machines,
Nonlinear optics,
Optical sensors,
Sensitivity,
Optical receivers,
Optical interconnections"
A hybrid machine learning approach to automatic plant phenotyping for smart agriculture,"Recently, a new ICT approach to agriculture called “Smart Agriculture” has been received great attention to support farmers' decision-making for good final yield on various kinds of field conditions. For this purpose, this paper presents two image sensing methods that enable an automatic observation to capture flowers and seedpods of soybeans in real fields. The developed image sensing methods are considered as sensors in an agricultural cyber-physical system in which big data on the growth status of agricultural plants and environmental information (e.g., weather, temperature, humidity, solar radiation, soil condition, etc.) are analyzed to mine useful rules for appropriate cultivation. The proposed image sensing methods are constructed by combining several image processing and machine learning techniques. The flower detection is realized based on a coarse-to-fine approach where candidate areas of flowers are first detected by SLIC and hue information, and the acceptance of flowers is decided by CNN. In the seedpod detection, candidates of seedpod regions are first detected by the Viola-Jones object detection method, and we also use CNN to make a final decision on the acceptance of detected seedpods. The performance of the proposed image sensing methods is evaluated for a data set of soybean images that were taken from a crowd of soybeans in real agricultural fields in Hokkaido, Japan.","Sensors,
Agriculture,
Image color analysis,
Image segmentation,
Feature extraction,
Cameras,
Plants (biology)"
Accelerating Graph and Machine Learning Workloads Using a Shared Memory Multicore Architecture with Auxiliary Support for In-hardware Explicit Messaging,"Shared Memory stands out as a sine qua non for parallel programming of many commercial and emerging multicore processors. It optimizes patterns of communication that benefit common programming styles. As parallel programming is now mainstream, those common programming styles are challenged with emerging applications that communicate often and involve large amount of data. Such applications include graph analytics and machine learning, and this paper focuses on these domains. We retain the shared memory model and introduce a set of lightweight in-hardware explicit messaging instructions in the instruction set architecture (ISA). A set of auxiliary communication models are proposed that utilize explicit messages to accelerate synchronization primitives, and efficiently move computation towards data. The results on a 256-core simulated multicore demonstrate that the proposed communication models improve performance and dynamic energy by an average of 4× and 42% respectively over traditional shared memory.","Message systems,
System recovery,
Computer architecture,
Synchronization,
Protocols,
Pipelines,
Radiation detectors"
Machine learning models to search relevant genetic signatures in clinical context,"Clinicians are interested in the estimation of robust and relevant genetic signatures from gene sequencing data. Many machine learning approaches have been proposed trying to address well-known issues of this complex task (feature or gene selection, classification or model selection, and prediction assessment). Addressing this problem often requires a deep knowledge of these methods and some of them demand high computational resources that may not be affordable. In this paper, an exhaustive study that includes different types of feature selection methods and classifiers is presented, providing clinicians an useful insight of the most suitable methods for this purpose. Predictions assessment is performed using a bootstrap cross-validation strategy as an honest validation scheme. The results of this study for six benchmark datasets show that filter or embedded methods are preferred, in general, to wrapper methods according to their better statistical significant results, in terms of accuracy, and lower demand for computational resources.","Genetics,
Cancer,
Biological cells,
Computational modeling,
Predictive models,
Genetic algorithms,
Correlation"
A framework for benchmarking machine learning methods using linear models for univariate time series prediction,"Time series prediction has been attracting interest of researchers due to its increasing importance in decision-making activities in many fields of knowledge. The demand for better accuracy in time series prediction furthered the arising of many machine learning time series prediction methods (MLM). Choosing a suitable method for a particular dataset is a challenge and demands established benchmark methods (BM) for performance assessment. Suppose a particular BM is selected, and an experimental comparison is made with a particular MLM. If the latter does not provide better prediction results for the same dataset, this indicates that some improvements are needed for the MLM. Regarding this matter, adopting a well-established, easy to interpret, and tuned BM is desirable. This paper presents a framework for systematic benchmarking some MLM against well-known Linear Methods (LM), namely Polynomial Regression and models in the ARIMA family, used as BM for univariate time series prediction. We implemented such a framework within the R-Package named TSPred. This implementation was evaluated using a wide number of datasets from past prediction competitions. The results show that fittest LM provided by TSPred are adequate BM for univariate time series predictions.","Time series analysis,
Mathematical model,
Autoregressive processes,
Predictive models,
Benchmark testing,
Computational modeling"
Intrusion detection with autoencoder based deep learning machine,"In changing and constantly evolving information age, together with the developments in computer and internet technology, the production, digitization, storage and sharing of information has become much easier than in the past. The sharing of information via computer networks and the Internet has made information security a vital issue for people, institutions and organizations with critical data. Various information security policies have been established in order to protect the critical preserve data and prevent unauthorized access to this data. Intrusion detection systems which is one of the indispensable elements of information security policies, constantly monitor the network and the system to detect possible unauthorized access and infiltrations. So far, many machine learning methods such as artificial neural networks, support vector machines, decision trees have been used in intrusion detection systems. In this study, differently from other studies, autoencoder based deep learning machines are proposed for intrusion detection. KDDcup99 data set containing 22 attack types has been used in the study and a performance with 99.42% of succes rate has been achieved.","Intrusion detection,
Machine learning,
Principal component analysis,
Art,
Tuning,
Computers"
Music genre classification with machine learning techniques,"The aim of this work is to predict the genres of songs by using machine learning techniques. For this purpose, feature extraction is done by using signal processing techniques, then machine learning algorithms are applied with those features to do a multiclass classification for music genres.","Mel frequency cepstral coefficient,
Multiple signal classification,
Signal processing,
Music information retrieval,
Nanoelectromechanical systems,
Metals,
Music"
Malware detection system based on machine learning methods for Android operating systems,"Mobile devices begin to spread increasingly recently to offer a lot of services which personal computers offer. This condition has led to increase in the number of security threats in mobile devices and services. In this paper, it has been made a research on mobile malware and malware detection techniques. Within the scope of the study, a a permission based detection system based on the machine learning methods for Android malware was developed. Developed system is analyzed by using Random Forest, Support Vector Machine and Artificial Neural Networks algorithms.","Androids,
Humanoid robots,
Malware,
Tutorials,
Computers,
Security"
Machine learning for product quality inspection,"Pistachio is widely consumed as food which requires the nut to be cracked before usage. Automating the cracking process requires quality control which can be done visually. A system developed to perform this task provides over 98% accuracy. First, the pistachio is segmented from the background using support vector regression (SVR) or deep convolutional networks. Then a classification method based on deep neural networks (FistikNet) is employed to decide the quality of the crack. A large data set is collected to train and test the system.","Dogs,
Support vector machines,
Fires,
Quality assessment,
Product design,
Inspection,
Process control"
Machine Learning Applied to the Recognition of Cryptographic Algorithms Used for Multimedia Encryption,"This paper presents a study of encrypted multimedia files in order to identify the encryption algorithm. Audio and video files were encoded with distinct cryptographic algorithms and then metadata were extracted from these cryptograms. The algorithm identification is obtained by using data mining techniques. Therefore, the procedure first stage performs the encryption of audio and video files using DES, Blowfish, RSA, and RC4 algorithms. Then, the encrypted files were submitted to the data mining algorithms: J48, FT, PART, Complement Naive Bayes, and Multilayer Perceptron classifiers. The resulting confusion matrices compiled into charts and it was possible to notice that the percentage of identification for each of the algorithms is greater than a probabilistic bid. There are several scenarios where algorithm identification reaches almost full recognition.","Classification algorithms,
Multilayer perceptrons,
Encryption,
Data mining,
IEEE transactions,
Machine learning algorithms"
Genome-wide Analysis of MDR and XDR Tuberculosis from Belarus: Machine-learning Approach,"Emergence of drug-resistant microorganisms has been recognized as a serious threat to public health worldwide. This problem is extensively discussed in the context of tuberculosis treatment. Alterations in pathogen genomes are among the main mechanisms by which microorganisms exhibit drug resistance. Analysis of 144 M. tuberculosis strains of different phenotypes including drug susceptible, MDR and XDR isolated in Belarus was fulfilled in this paper. A wide range of machine learning methods that can discover SNPs related to drug-resistance in the whole bacteria genomes was investigated. Besides single-SNP testing approaches, methods that allow detecting joint effects from interacting SNPs were considered. We proposed a framework for automated selection of the best performing statistical model in terms of recall, precision and accuracy to identify drug resistance-associated mutations. Analysis of whole-genome sequences often leads to situations where the number of treated features exceeds the number of available observations. For this reason, special attention is paid to fair evaluation of the model prediction quality and minimizing the risk of overfitting while estimating the underlying parameters. Results of our experiments aimed at identifying top-scoring resistance mutations to the major first-line and second-line anti-TB drugs are presented.","Drugs,
Genomics,
Immune system,
Strain,
Bioinformatics,
Organisms"
Predicting unlabeled traffic for intrusion detection using semi-supervised machine learning,"Intrusion is one of the most serious problems with network Security, as new types of intrusions are getting much more challenging to detect. Large amount of network traffic has been generated due to the use of internet; most of the generated traffic is in the format which cannot be used directly to arrive at meaningful information. The cleansing and labeling of data each time needs a considerable amount of human effort, and is time consuming. In this paper we show how, Semi supervised machine learning technique can be used in intrusion detection, for both labeled and unlabeled data. In the proposed technique we take a small amount of labeled data to create model and using this model we show how to predict the unlabeled traffic. Machine Learning tool is used for this purpose which uses semi-supervised classifier to build the model. The created model is then integrated in Pentaho which with the help of Weka Scoring provides the expected output. The proposed technique helps the network administrator to take quick decision by classifying the incoming traffic as either malicious or normal and hence efficient detection of intrusion.","Data models,
Predictive models,
Intrusion detection,
Tools,
Labeling,
Supervised learning"
An improved approach for prediction of Parkinson's disease using machine learning techniques,"Parkinson's disease (PD) is one of the major public health problems in the world. It is a well-known fact that around one million people suffer from Parkinson's disease in the United States whereas the number of people suffering from Parkinson's disease worldwide is around 5 millions. Thus, it is important to predict Parkinson's disease in early stages so that early plan for the necessary treatment can be made. People are mostly familiar with the motor symptoms of Parkinson's disease, however an increasing amount of research is being done to predict the Parkinson's disease from non-motor symptoms that precede the motor ones. If early and reliable prediction is possible then a patient can get a proper treatment at the right time. Non-motor symptoms considered are Rapid Eye Movement (REM) sleep Behaviour Disorder (RBD) and olfactory loss. Developing machine learning models that can help us in predicting the disease can play a vital role in early prediction. In this paper we extend a work which used the non-motor features such as RBD and olfactory loss. Along with this the extended work also uses important biomarkers. In this paper we try to model this classifier using different machine learning models that have not been used before. We developed automated diagnostic models using Multilayer Perceptron, BayesNet, Random Forest and Boosted Logistic Regression. It has been observed that Boosted Logistic Regression provides the best performance with an impressive accuracy of 97.159 % and the area under the ROC curve was 98.9%. Thus, it is concluded that this models can be used for early prediction of Parkinson's disease.","Parkinson's disease,
Biomarkers,
Biological system modeling,
Olfactory,
Single photon emission computed tomography"
Membership Inference Attacks Against Machine Learning Models,"We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial ""machine learning as a service"" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.","Training,
Data models,
Predictive models,
Privacy,
Sociology,
Statistics,
Google"
SecureML: A System for Scalable Privacy-Preserving Machine Learning,"Machine learning is widely used in practice to produce predictive models for applications such as image processing, speech and text recognition. These models are more accurate when trained on large amount of data collected from different sources. However, the massive data collection raises privacy concerns. In this paper, we present new and efficient protocols for privacy preserving machine learning for linear regression, logistic regression and neural network training using the stochastic gradient descent method. Our protocols fall in the two-server model where data owners distribute their private data among two non-colluding servers who train various models on the joint data using secure two-party computation (2PC). We develop new techniques to support secure arithmetic operations on shared decimal numbers, and propose MPC-friendly alternatives to non-linear functions such as sigmoid and softmax that are superior to prior work. We implement our system in C++. Our experiments validate that our protocols are several orders of magnitude faster than the state of the art implementations for privacy preserving linear and logistic regressions, and scale to millions of data samples with thousands of features. We also implement the first privacy preserving system for training neural networks.","Training,
Logistics,
Protocols,
Data models,
Privacy,
Linear regression,
Neural networks"
Application of probabilistic modeling and machine learning to the diagnosis of FTTH GPON networks,"This paper presents insights on the promises of probabilistic modeling and machine learning for fault diagnosis in optical access networks. A Bayesian inference engine, called Probabilistic tool for GPON-FTTH Access Network self-DiAgnosis (PANDA), is applied to fault diagnosis of Gigabit capable Passive Optical Networks (GPON). PANDA approach has been assessed on real diagnosis data, showing very satisfactory alignment with an operational rule-based expert system. Furthermore, it provides diagnosis conclusions for all tested cases, even if some monitoring data is missing or incomplete. Finally, an expectation maximization algorithm allows to finely tune the probabilistic model.","Passive optical networks,
Expert systems,
Probabilistic logic,
Bayes methods,
Fault diagnosis,
Optical variables measurement"
Study of machine learning algorithms for special disease prediction using principal of component analysis,"The worldwide study on causes of death due to heart disease/syndrome has been observed that it is the major cause of death. If recent trends are allowed to continue, 23.6 million people will die from heart disease in coming 2030. The healthcare industry collects large amounts of heart disease data which unfortunately are not “mined” to discover hidden information for effective decision making. In this paper, study of PCA has been done which finds the minimum number of attributes required to enhance the precision of various supervised machine learning algorithms. The purpose of this research is to study supervised machine learning algorithms to predict heart disease. Data mining has number of important techniques like categorization, preprocessing. Diabetic is a life threatening disease which prevent in several urbanized as well as emergent countries like India. The data categorization is diabetic patients datasets which is developed by collecting data from hospital repository consists of 1865 instances with dissimilar attributes. The examples in the dataset are two categories of blood tests, urine tests. In this research paper we discuss a variety of algorithm approaches of data mining that have been utilized for diabetic disease prediction. Data mining is a well known practice used by health organizations for classification of diseases such as diabetes and cancer in bioinformatics research.","Data mining,
Diseases,
Classification algorithms,
Prediction algorithms,
Diabetes,
Heart,
Algorithm design and analysis"
Sea ice type classification based on random forest machine learning with Cryosat-2 altimeter data,"Sea ice type is the most sensitive variables in Arctic ice monitoring and its detailed information is essential for ice situation evaluation, climate prediction and vessels navigating. In this study, we analyzed the different sea ice types with the Cryosat-2 (CS-2) SAR mode waveform data. The waveform of CS-2 data was described by a set of parameters: [pulse peakiness (PP), leading-edge width (LeW), trailing-edge width (TeW), stack standard deviation (SSD) and Maximum value of the echo waveform (Max)] and backscatter coefficient (Sigma0). Random forest (RF) classifier was chosen to classify ice type and the classification results were compared with Arctic and Antarctic Research Institute (AARI) operational ice charts. The results show that 85% of the Arctic surface type can be correctly classified from November 2015 to May 2016, 83% of the FYI can be correctly identified which is the domain ice type in Arctic. In comparison with Bayesian and K nearest-neighbor classifiers, the classification accuracy of RF increased by 5% and 3% respectively.","Sea ice,
Arctic,
Radio frequency,
Sea surface,
Spaceborne radar"
A novel machine learning approach to recognize household objects,"This work introduces a novel artificial intelligence approach to household object recognition. The approach used in this work is feature-based and it works toward recognition under a broad range of circumstances. The necessary image processing techniques are applied to recognize the objects. These techniques include removal of shadow that is segmenting the object from its shadow, extraction of shape and texture features from the object images and creation of descriptors that overcome the difficulties of affine transformations up to some extent. By this prior knowledge of descriptors, objects are categorized to their respective classes using “Back Propagation Neural Network” (BPNN). The system reached the expected result using 38 powerful combined features of shape and texture and BPNN. The system gives accuracy of 81%-92% for 10-25 different types of objects.",
A Machine Learning-Empowered System for Long-Term Motion-Tolerant Wearable Monitoring of Blood Pressure and Heart Rate With Ear-ECG/PPG,"In this paper, we propose a fully ear-worn long-term blood pressure (BP) and heart rate (HR) monitor to achieve a higher wearability. Moreover, to enable practical application scenarios, we present a machine learning framework to deal with severe motion artifacts induced by head movements. We suggest situating all electrocardiogram (ECG) and photoplethysmography (PPG) sensors behind two ears to achieve a super wearability, and successfully acquire weak ear-ECG/PPG signals using a semi-customized platform. After introducing head motions toward real-world application scenarios, we apply a support vector machine classifier to learn and identify raw heartbeats from motion artifacts-impacted signals. Furthermore, we propose an unsupervised learning algorithm to automatically filter out residual distorted/faking heartbeats, for ECG-to-PPG pulse transit time (PTT) and HR estimation. Specifically, we introduce a dynamic time warping-based learning approach to quantify distortion conditions of raw heartbeats referring to a high-quality heartbeat pattern, which are then compared with a threshold to perform purification. The heartbeat pattern and the distortion threshold are learned by a K-medoids clustering approach and a histogram triangle method, respectively. Afterward, we perform a comparative analysis on ten PTT or PTT&HR-based BP learning models. Based on an acquired data set, the BP and HR estimation using the proposed algorithm has an error of -1.4±5.2 mmHg and 0.8±2.7 beats/min, respectively, both much lower than the state-of-the-art approaches. These results demonstrate the capability of the proposed machine learning-empowered system in ear-ECG/PPG acquisition and motion-tolerant BP/HR estimation. This proof-of-concept system is expected to illustrate the feasibility of ear-ECG/PPG-based motion-tolerant BP/HR monitoring.","Heart beat,
Biomedical monitoring,
Electrocardiography,
Sensors,
Estimation,
Monitoring"
Coupling a Fast Fourier Transformation With a Machine Learning Ensemble Model to Support Recommendations for Heart Disease Patients in a Telehealth Environment,"Recently, the use of intelligent technologies in clinical decision making in the telehealth environment has begun to play a vital role in improving the quality of patients' lives and helping reduce the costs and workload involved in their daily healthcare. In this paper, an effective medical recommendation system that uses a fast Fourier transformation-coupled machine learning ensemble model is proposed for short-term disease risk prediction to provide chronic heart disease patients with appropriate recommendations about the need to take a medical test or not on the coming day based on analysing their medical data. The input sequence of sliding windows based on the patient's time series data are decomposed by using the fast Fourier transformation in order to extract the frequency information. A bagging-based ensemble model is utilized to predict the patient's condition one day in advance for producing the final recommendation. A combination of three classifiers-artificial neural network, least squares-support vector machine, and naive bayes-are used to construct an ensemble framework. A real-life time series telehealth data collected from chronic heart disease patients are utilized for experimental evaluation. The experimental results show that the proposed system yields a very good recommendation accuracy and offers an effective way to reduce the risk of incorrect recommendations as well as reduce the workload for heart disease patients in conducting body tests every day. The results conclusively ascertain that the proposed system is a promising tool for analyzing time series medical data and providing accurate and reliable recommendations to patients suffering from chronic heart diseases.","Diseases,
Heart,
Time series analysis,
Medical tests,
Support vector machines,
Tools,
Neurons"
Analytical Modeling of Human Choice Complexity in a Mixed Model Assembly Line Using Machine Learning-Based Human in the Loop Simulation,"Despite the recent advances in manufacturing automation, the role of human involvement in manufacturing systems is still regarded as a key factor in maintaining higher adaptability and flexibility. In general, however, modeling of human operators in manufacturing system design still considers human as a physical resource represented in statistical terms. In this paper, we propose a human in the loop (HIL) approach to investigate the operator's choice complexity in a mixed model assembly line. The HIL simulation allows humans to become a core component of the simulation, therefore influencing the outcome in a way that is often impossible to reproduce via traditional simulation methods. At the initial stage, we identify the significant features affecting the choice complexity. The selected features are in turn used to build a regression model, in which human reaction time with regard to different degree of choice complexity serves as a response variable used to train and test the model. The proposed method, along with an illustrative case study, not only serves as a tool to quantitatively assess and predict the impact of choice complexity on operator's effectiveness, but also provides an insight into how complexity can be mitigated without affecting the overall manufacturing throughput.","Complexity theory,
Analytical models,
Adaptation models,
Computational modeling,
Manufacturing systems,
Predictive models"
Design of a novel wireless power system using machine learning techniques for drone applications,"In this paper, the design of a novel wireless power transfer system utilizing drones with machine learning techniques is presented. Research on drones is currently a fast growing field with a great potential in many ubiquitous applications. The wireless power transfer system with the fixed operation frequency at 13.56MHz is applied to an 1-coil receiver on the drone, with an array of transmitter coils on the ground. This work presents an approach where the data is considered “classified” using machine learning techniques, which allows the accurate prediction of the drone's position, thus enhancing the wireless power transfer efficiency.","Drones,
Transmitters,
Switches,
Receivers,
Coils,
Wireless power transfer,
Position measurement"
MAC protocol selection based on machine learning in cognitive radio networks,"The cognitive radio network is intelligent in adapting to the dynamic network environment, and it also provides a new way to solve the application limitation of nowadays MAC protocols. In this paper, an adaptive MAC protocol selection scheme based on machine learning is proposed for cognitive radio networks. This scheme can combine the respective advantages of the competitive protocols and non-competitive protocols according to the network loads, and help the network nodes to select the MAC protocol that best suits the current network circumstance. Simulation results validate our proposal that it always select the appropriate MAC protocol that best suits the network circumstance. It is also proven that, the accuracy of the proposed MAC protocol selection strategy far surpasses that of existing work.","Media Access Protocol,
Throughput,
Time division multiple access,
Cognitive radio,
Network topology"
Machine Learning Based Power Grid Outage Prediction in Response to Extreme Events,"A machine learning based prediction method is proposed in this paper to determine the potential outage of power grid components in response to an imminent hurricane. The decision boundary, which partitions the components' states into two sets of damaged and operational, is obtained via logistic regression by using a second-order function and proper parameter fitting. Two metrics are examined to validate the performance of the obtained decision boundary in efficiently predicting component outages.","Hurricanes,
Power grids,
Logistics,
Predictive models,
Neural networks,
Regression tree analysis,
Wind speed"
A Deep Learning Scheme for Motor Imagery Classification based on Restricted Boltzmann Machines,"Motor imagery classification is an important topic in brain-computer interface (BCI) research that enables the recognition of a subject's intension to, e.g., implement prosthesis control. The brain dynamics of motor imagery are usually measured by electroencephalography (EEG) as nonstationary time series of low signal-to-noise ratio. Although a variety of methods have been previously developed to learn EEG signal features, the deep learning idea has rarely been explored to generate new representation of EEG features and achieve further performance improvement for motor imagery classification. In this study, a novel deep learning scheme based on restricted Boltzmann machine (RBM) is proposed. Specifically, frequency domain representations of EEG signals obtained via fast Fourier transform (FFT) and wavelet package decomposition (WPD) are obtained to train three RBMs. These RBMs are then stacked up with an extra output layer to form a four-layer neural network, which is named the frequential deep belief network (FDBN). The output layer employs the softmax regression to accomplish the classification task. Also, the conjugate gradient method and backpropagation are used to fine tune the FDBN. Extensive and systematic experiments have been performed on public benchmark datasets, and the results show that the performance improvement of FDBN over other selected state-of-the-art methods is statistically significant. Also, several findings that may be of significant interest to the BCI community are presented in this article.","Machine learning,
Electroencephalography,
Biological neural networks,
Frequency-domain analysis,
Feature extraction,
Training,
Benchmark testing"
Predicting of Job Failure in Compute Cloud Based on Online Extreme Learning Machine: A Comparative Study,"Early prediction of job failures and specific disposal steps in advance could significantly improve the efficiency of resource utilization in large-scale data center. The existing machine learning-based prediction methods commonly adopt offline working pattern, which cannot be used for online prediction in practical operations, in which data arrive sequentially. To solve this problem, a new method based on online sequential extreme learning machine (OS-ELM) is proposed in this paper to predict online job termination status. With this method, real-time data are collected according to the sequence of job arriving, the job status could be predicted and the operation model is thus updated based on these data. The method with online incremental learning strategy has fast learning speed and good generalization. Comparative study using Google trace data shows that prediction accuracy of the proposed method is 93% with updating model in 0.01 s. Compared with some state-of-the-art methods, such, as support vector machine (SVM), ELM, and OS-SVM, the method developed in this paper has many advantages, such as less time-consuming in establishing and updating the model, higher prediction accuracy and precision, and better false negative performance.","Data models,
Predictive models,
Computational modeling,
Google,
Real-time systems,
Support vector machines"
Water Desalination Fault Detection Using Machine Learning Approaches: A Comparative Study,"The presence of faulty valves has been studied in literature with various machine learning approaches. The impact of using fault data only to train the system could solve the class imbalance problem in the machine learning approach. The datasets used for fault detection contain many independent variables, where the salient ones were selected using stepwise regression and applied to various machine learning techniques. A significance test for the given regression technique was used to validate the outcome. Machine learning techniques such as decision trees and deep learning are applied to the given data and the results reveal that the decision tree was able to obtain more than 95% accuracy and performed better than other algorithms when considering the trade-off between the processing time and accuracy.","Desalination,
Feature extraction,
Fault diagnosis,
Principal component analysis,
Fault detection,
Valves,
Decision trees"
Crowd-ML: A library for privacy-preserving machine learning on smart devices,"When user-generated data such as audio and video signals are used to train machine learning algorithms, users' privacy must be considered before the learned model is released. In this work, we present an open-source library for privacy-preserving machine learning framework on smart devices. The library allows Android and iOS devices to collectively learn a common classifier/regression model from distributed data with differential privacy, using a variant of minibatch stochastic gradient descent method. The library allows researchers and developers to easily implement and deploy customized tasks that use on-device sensors to collect sensitive data for machine learning.","Servers,
Sensitivity,
Libraries,
Privacy,
Optimization,
Smart devices,
Performance evaluation"
Fast HEVC intra coding algorithm based on machine learning and Laplacian Transparent Composite Model,"Compared with H.264, High Efficient Video Coding (HEVC) improves the coding efficiency by 50% at the price of significant increase in encoding time, due to Rate Distortion Optimization (RDO) on large variations of block sizes and prediction modes. In this paper, a fast intra coding algorithm is proposed to alleviate the high computational complexity of HEVC intra-frame coding. The proposed algorithm is based on machine learning and Laplacian Transparent Composite Model (LPTCM). Features called Summation of Binarized Outlier Coefficient (SBOC) vectors are firstly extracted from original frames by using LPTCM and then fed into online trained Support Vector Machine (SVM). Two SVMs are combined to predict Coding Unit (CU) decisions so that the encoding process can be significantly sped up. Additionally, a performance controller is introduced to ensure the robustness of machine learning models. It is shown by experiments that compared with HM 16.3, the proposed algorithm reduces the encoding time, on average, by 48% with negligible increase in BD-rate.","Encoding,
Support vector machines,
Feature extraction,
Machine learning algorithms,
Video coding,
Prediction algorithms,
Training"
Disease Prediction by Machine Learning Over Big Data From Healthcare Communities,"With big data growth in biomedical and healthcare communities, accurate analysis of medical data benefits early disease detection, patient care, and community services. However, the analysis accuracy is reduced when the quality of medical data is incomplete. Moreover, different regions exhibit unique characteristics of certain regional diseases, which may weaken the prediction of disease outbreaks. In this paper, we streamline machine learning algorithms for effective prediction of chronic disease outbreak in disease-frequent communities. We experiment the modified prediction models over real-life hospital data collected from central China in 2013-2015. To overcome the difficulty of incomplete data, we use a latent factor model to reconstruct the missing data. We experiment on a regional chronic disease of cerebral infarction. We propose a new convolutional neural network (CNN)-based multimodal disease risk prediction algorithm using structured and unstructured data from hospital. To the best of our knowledge, none of the existing work focused on both data types in the area of medical big data analytics. Compared with several typical prediction algorithms, the prediction accuracy of our proposed algorithm reaches 94.8% with a convergence speed, which is faster than that of the CNN-based unimodal disease risk prediction algorithm.","Diseases,
Hospitals,
Prediction algorithms,
Machine learning algorithms,
Big Data,
Data models"
ADHD subgroup discrimination with global connectivity features using hierarchical extreme learning machine: Resting-state FMRI study,"The differential diagnosis among ADHD subtypes is an important research area for the neuroimaging community. We pursue this goal by using machine learning techniques in this study. Selective subjects matched by age and handedness information from publicly available ADHD-200 dataset were used in this study. In addition, this work is based only on the resting-state fMRI images. We calculated the global connectivity maps from the fMRI images and used the average of the connectivity measure of each atlas-based cortical parcellation as a feature for the classifier input. For the classification, we used hierarchical extreme learning machine (H-ELM) classifier. By using the proposed feature extraction method, we achieved a 71.11% (p <; 0.0090) nested cross-validated accuracy and a kappa score of 0.57 in multiclass classification settings.","Feature extraction,
Magnetic resonance imaging,
Neuroimaging,
Correlation,
Time series analysis,
Mental disorders,
Testing"
Assessing the feasibility of estimating axon diameter using diffusion models and machine learning,"Axon diameter estimation has been a focus of the diffusion MRI community for the past decade. The main argument has been that while diffusion models always overestimate the true axon diameter, their estimation still correlates with changes in true value. Until now, this remains more as a discussion point. The aim of this paper is to clarify this hypothesis using a recently acquired cat spinal cord data set, where the diffusion MRI signal of both a multi-shell and Ax-Caliber acquisition have been registered with the underlying histology values. We find that the axon diameter as estimated by signal models and AxCaliber does not correlate with their true sizes for axon diameters smaller than 3 μm. On the other hand, we also train a random forest machine learning algorithm to map signal-based features to histology values of axon diameter and volume fraction. The results show that, in this dataset, this approach leads to a more reliable estimation of physically relevant axon diameters than using sophisticated diffusion models.","Axons,
Diffusion tensor imaging,
Correlation,
Radio frequency,
Estimation,
Spinal cord"
Random matrices meet machine learning: A large dimensional analysis of LS-SVM,"This article proposes a performance analysis of kernel least squares support vector machines (LS-SVMs) based on a random matrix approach, in the regime where both the dimension of data p and their number n grow large at the same rate. Under a two-class Gaussian mixture model for the input data, we prove that the LS-SVM decision function is asymptotically normal with means and covariances shown to depend explicitly on the derivatives of the kernel function. This provides improved understanding along with new insights into the internal workings of SVM-type methods for large datasets.","Kernel,
Support vector machines,
Error analysis,
Covariance matrices,
Random variables,
Performance analysis,
Optimization"
A case study of machine learning hardware: Real-time source separation using Markov Random Fields via sampling-based inference,"We explore sound source separation to isolate human voice from background noise on mobile phones, e.g. talking on your cell phone in an airport. The challenges involved are real-time execution and power constraints. As a solution, we present a novel hardware-based sound source separation implementation capable of real-time streaming performance. The implementation uses a recently introduced Markov Random Field (MRF) inference formulation of foreground/background separation, and targets voice separation on mobile phones with two microphones. We demonstrate a real-time streaming FPGA implementation running at 150 MHz with total of 207 KB RAM. Our implementation achieves a speedup of 20× over a conventional software implementation, achieves an SDR of 6.655 dB with 1.601 ms latency, and exhibits excellent perceived audio quality. A virtual ASIC design shows that this architecture is quite small (less than 10M gates), consumes only 69.977 mW running at 20 MHz (52× less than an ARM Cortex-A9 software reference), and appears amenable to additional optimization for power.","Source separation,
Real-time systems,
Spectrogram,
Hardware,
Microphones,
Markov random fields,
Time-frequency analysis"
Machine learning based non-intrusive quality estimation with an augmented feature set,We present a method that improves the objective quality estimation of a speech utterance. We show that including raw features that are presumably redundant reduces the effect of input noise and improves the performance of linear regressors. To exploit this effect we propose the novel idea to augment the feature set with redundant features. The proposed augmented feature set and the neural network that consists of an auto-encoder and a linear regressor leads to improved prediction accuracy of the single-ended quality assessment approach. Evaluating the system on the ITU-T Supplement 23 database illustrates that the proposed approach outperforms the current state-of-the-art.,"Speech,
Mathematical model,
Quality assessment,
Estimation,
Distortion,
Training data,
Feature extraction"
Unbiased analysis of mouse social behaviour using unsupervised machine learning,"Mouse models are broadly used to study the mechanisms of neuropsychiatric disorders and to test potential treatments. In these models, automation to monitor behavioural differences during social interactions is currently limited. We propose in the present study a new method to conduct automatic behavioural classification, using an original unsupervised machine learning. We applied the proposed method to mice mutated in Shank2, a gene associated with autism spectrum disorders. We validated our results by comparing automatically extracted results to rule-based classifier labelling. We discovered seven behavioural states matching from 80 to 95% previous rule-based classification, and two unsuspected behaviours. Interestingly, we also highlighted genotype-related differences in two behavioural categories, namely locomotion and facing the conspecific.","Integrated circuits,
Feature extraction,
Mice,
Trajectory,
Autism"
Intelligent medical data storage system using machine learning approach,"In recent days healthcare domains need a efficient storage and retrieval systems to provide a effective medical services to the health seekers. But there is a vocabulary gap in understanding the medical terminologies due to ambiguity. So, the existing systems need a intelligent medical storage using some natural language processing. Users post their queries in free text so it will result in complexity for analyzing and providing instant answers. The main aim of this paper is to develop an intelligent medical data warehousing and mining system. This system can be performed by assigning corpus-aware terminologies to the medical records. This system provides efficient answers to the health seekers by extracting the information related to their queries.","Terminology,
Medical diagnostic imaging,
Tag clouds,
Medical services,
Semantics,
Vocabulary"
A fault location technique for HVDC transmission lines using extreme learning machines,"In this study, a new approach is proposed for fault estimation in high voltage direct current transmission lines using discrete wavelet transform and extreme learning machine. Recently, signal processing and intelligent systems have gained importance to ease very different tasks such as fault location and estimation, load estimations, reactive power compensation, the risk of blackouts. Therefore, a fast, accurate and reliable protection algorithms have a major interest in the extended usage of high voltage direct current systems for many areas. In this study, single phase-ground faults on DC lines examined and a new machine learning approach also discussed. The virtual faults obtained from Matlab simulation is utilized in the course of feature extraction of the wavelet transform. Furthermore, for identifying steady state and faulted condition, Shannon entropy and signal's energy values have been calculated by using coefficients of the wavelet transform. After that, the coefficients normalized between [-1, 1]. Finally, the extreme learning machine used to fault estimation and location process.","Discrete wavelet transforms,
Entropy,
HVDC transmission,
Estimation,
Fault currents"
Intelligence Boosting Engine (IBE): A hardware accelerator for processing sensor fusion and machine learning algorithm for a sensor hub SoC,"This paper proposes a hardware accelerator, named IBE (Intelligence Boost Engine), to process both sensor fusion and machine learning algorithms for the Standing-egg SLH200 sensor hub SoC. The IBE is designed to have both efficiency and flexibility to support various emerging applications for future sensor hub SoCs in addition to the sensor fusion and machine learning algorithm (SVM) which are the target applications of the SLH200. With regard to the SLH200 SoC, the IBE was fabricated in the Global Foundry 55nm process, and the performance and power evaluation with IBE have been performed with the evaluation board of the SLH200 SoC. The evaluation results show that the proposed IBE can achieve up to 31.3× faster speed for target kernel operation and 4× faster speed for the target application (SVM polynomial). It reduces the energy consumption up to 75% as well.","Sensor fusion,
Support vector machines,
Algorithm design and analysis,
Machine learning algorithms,
Buffer storage,
Sparse matrices,
Hardware"
Early Prediction of LBW Cases via Minimum Error Rate Classifier: A Statistical Machine Learning Approach,"Low Birth weight (LBW) acts as an indicator of sickness in newborn babies. LBW is closely associated with infant mortality as well as various health outcomes later in life. Various studies show strong correlation between maternal health during pregnancy and the child's birth weight. This manuscript exploits machine learning techniques to gain useful information from health indicators of pregnant women for early detection of potential LBW cases. The forecasting problem has been reformulated as a classification problem between LBW and NOT-LBW classes using the Bayes' minimum error rate classifier rendering LBW detection as a binary machine classification problem. Expectedly, the proposed model achieved accuracy of 96.77%. Indian health care data was used to construct decision rules to be extrapolated to predictive health care in smart cities. A screening tool based on the decision model is developed to assist health care professionals in Obstetrics and Gynecology (OBG). Index Terms-Low Birth weight (LBW), Smart health informatics, Minimum error rate classifier, Predictive analytics, Machine Learning (ML), Feature Ranking.","Pediatrics,
Pregnancy,
Error analysis,
Tools,
Bayes methods,
Training"
Extreme Learning machines and Support Vector Machines models for email spam detection,"Extreme Learning machines (ELM) and Support Vector Machines have become two of the most widely used machine learning techniques for both classification and regression problems of recent. However the comparison of both ELM and SVM for classification and regression problems has often caught the attention of several researchers. In this work, an attempt has been made at investigating how SVM and ELM compared on the unique and important problem of Email spam detection, which is a classification problem. The importance of email in this present age cannot be overemphasized. Hence the need to promptly and accurately detect and isolate unsolicited mails through spam detection system cannot be over emphasized. Empirical results from experiments carried out using very popular dataset indicated that both techniques outperformed the best earlier published techniques on the same popular dataset employed in this study. However, SVM performed better than ELM on comparison scale based on accuracy. But in term of speed of operation, ELM outperformed SVM significantly.",
A fully automated deep packet inspection verification system with machine learning,"Deep Packet Inspection (DPI) technique has become very important for traffic detection and resource management in core networks. DPI systems use unique byte patterns as signatures to detect application traffic. Applications frequently update their version to add new features and/or to bypass firewall/DPI systems. Thus, an accurate DPI system needs to periodically verify existing signatures and update them if required. The manual task of application traffic generation and verification on multiple platforms is very tedious and error-prone. We propose a fully automated DPI verification system with machine learning techniques for periodic DPI signature verification and update. Automated mobile application traffic generation is achieved by open source tools GUITAR and Appium. Signature verification and new signature pattern suggestion from undetected flows are achieved by well-known and custom made machine learning algorithms, thus completing full signature verification and update cycle. Initial test results show that our solution saves lot of man hours and detects signature update in shortest possible time.","Automation,
Mobile communication,
Tools,
Browsers,
Androids,
Humanoid robots,
Mobile applications"
Towards Distributed Machine Learning in Shared Clusters: A Dynamically-Partitioned Approach,"Many cluster management systems (CMSs) have been proposed to share a single cluster with multiple distributed computing systems. However, none of the existing approaches can handle distributed machine learning (ML) workloads given the following criteria: high resource utilization, fair resource allocation and low sharing overhead. To solve this problem, we propose a new CMS named Dorm, incorporating a dynamically-partitioned cluster management mechanism and an utilization-fairness optimizer. Specifically, Dorm uses the container-based virtualization technique to partition a cluster, runs one application per partition, and can dynamically resize each partition at application runtime for resource efficiency and fairness. Each application directly launches its tasks on the assigned partition without petitioning for resources frequently, so Dorm imposes flat sharing overhead. Extensive performance evaluations showed that Dorm could simultaneously increase the resource utilization by a factor of up to 2.32, reduce the fairness loss by a factor of up to 1.52, and speed up popular distributed ML applications by a factor of up to 2.72, compared to existing approaches. Dorm's sharing overhead is less than 5% in most cases.","Resource management,
Containers,
Servers,
Random access memory,
Graphics processing units,
Sparks,
Training"
Fully Homomorphic Encryption for Classification in Machine Learning,"Using fully homomorphic encryption scheme, we construct fully homomorphic encryption scheme FHE4GT that can homomorphically compute an encryption of the greater-than bit that indicates x > x' or not, given two ciphertexts c and c' of x and x', respectively, without knowing the secret key. Then, we construct homomorphic classifier homClassify that can homomorphically classify a given encrypted data without decrypting it, using machine learned parameters.","Encryption,
Servers,
Timing,
Information security,
Machine learning algorithms,
Protocols"
Automated attendance system using machine learning approach,"The conventional method of taking attendance is done manually by the teacher or the administrator which requires considerable amount of time and efforts also involving errors and proxy attendance. As the number of students are increasing day by day, it is a challenging task for universities or colleges to monitor and maintain the record of the students. Automated systems involving use of biometrics like fingerprint and iris recognition are well developed in the recent years however, it is intrusive and cost required for deployment on large scale gets increased substantially. To overcome these issues, biometric feature like facial recognition can be used which involves the phases such as image acquisition, face detection, feature extraction, face classification, face recognition and eventually marking the attendance. The algorithms like Viola-Jones and HOG features along with SVM classifier are used to acquire the desired results. Various real time scenarios need to be considered such as scaling, illumination, occlusions and pose. The problem of redundancy in manual records and keeping attendance is solved by this system. Quantitative analysis is done on the basis of PSNR values.","Face,
Face recognition,
Feature extraction,
Databases,
Support vector machines,
Histograms,
Computer vision"
Review of the Use of AI Techniques in Serious Games: Decision Making and Machine Learning,"The video game market has become an established and ever-growing global industry. The health of the video and computer games industry, together with the variety of genres and technologies available, means that video game concepts and programmes are being applied in numerous different disciplines. One of these is the field known as serious games. The main goal of this paper is to collect all the relevant articles published during the last decade and create a trend analysis about the use of certain artificial intelligence algorithms related to decision making and learning in the field of serious games. A categorization framework was designed and outlined to classify the 129 papers that met the inclusion criteria. The authors made use of this categorization framework for drawing some conclusions regarding the actual use of intelligent serious games. The authors consider that over recent years enough knowledge has been gathered to create new intelligent serious games to consider not only the final aim but also the technologies and techniques used to provide players with a nearly real experience. However, researchers may need to improve their testing methodology for developed serious games, so as to ensure they meet their final purposes.",
"Poster Abstract: A Beverage Intake Tracking System Based on Machine Learning Algorithms, and Ultrasonic and Color Sensors","We present a novel approach for monitoring beverage intake. Our system is composed of an ultrasonic sensor, an RGB color sensor, and machine learning algorithms. The system not only measures beverage volume but also detects beverage types. The sensor unit is lightweight that can be mounted on the lid of any drinking bottle. Our experimental results demonstrate that the proposed approach achieves more than 97% accuracy in beverage type classification. Furthermore, our regression-based volume measurement has a nominal error of 3%.","Liquids,
Acoustics,
Machine learning algorithms,
Volume measurement,
Color,
Monitoring,
Ultrasonic variables measurement"
Accelerator for Sparse Machine Learning,"Sparse matrix by vector multiplication (SpMV) plays a pivotal role in machine learning and data mining. We propose and investigate an SpMV accelerator, specifically designed to accelerate the sparse matrix by sparse vector multiplication (SpMSpV), and to be integrated in a CPU core. We show that our accelerator outperforms a similar solution by 70x while achieving 8x higher power efficiency, which yields an estimated 29x energy reduction for SpMSpV based applications.","Sparse matrices,
Indexes,
Random access memory,
Algorithm design and analysis,
Memory management,
Acceleration"
How to steal a machine learning classifier with deep learning,"This paper presents an exploratory machine learning attack based on deep learning to infer the functionality of an arbitrary classifier by polling it as a black box, and using returned labels to build a functionally equivalent machine. Typically, it is costly and time consuming to build a classifier, because this requires collecting training data (e.g., through crowdsourcing), selecting a suitable machine learning algorithm (through extensive tests and using domain-specific knowledge), and optimizing the underlying hyperparameters (applying a good understanding of the classifier's structure). In addition, all this information is typically proprietary and should be protected. With the proposed black-box attack approach, an adversary can use deep learning to reliably infer the necessary information by using labels previously obtained from the classifier under attack, and build a functionally equivalent machine learning classifier without knowing the type, structure or underlying parameters of the original classifier. Results for a text classification application demonstrate that deep learning can infer Naive Bayes and SVM classifiers with high accuracy and steal their functionalities. This new attack paradigm with deep learning introduces additional security challenges for online machine learning algorithms and raises the need for novel mitigation strategies to counteract the high fidelity inference capability of deep learning.","Machine learning,
Support vector machines,
Training data,
Neurons,
Machine learning algorithms,
Organizations,
Security"
Machine learning for improved diagnosis and prognosis in healthcare,"Machine learning has gained tremendous interest in the last decade fueled by cheaper computing power and inexpensive memory - making it efficient to store, process and analyze growing volumes of data. Enhanced algorithms are being designed and applied on large datasets to help discover hidden insights and correlations amongst data elements not obvious to human. These insights help businesses take better decisions and optimize key indicators of interest. The growing popularity of machine learning also stems from the fact that learning algorithms are agnostic to the domain of application. Classification algorithms, for example, that could be applied to categorize faults in windmill blades can also be used for categorizing TV viewers in a survey. The actual value of machine learning however depends on the ability to adapt and apply these algorithms to solve specific real world problems. In this paper we discuss two such applications for interpreting medical data for automated analysis. Our first case study demonstrates the use of Bayesian Inference, a paradigm of machine learning, for diagnosing Alzheimer's disease based on cognitive test results and demographic data. In the second case study we focus on automated classification of cell images to determine the advancement and severity of breast cancer using artificial neural networks. Although these research are still preliminary, they demonstrate the value of machine learning techniques in providing quick, efficient and automated data analysis. Machine learning offers hope with early diagnosis of diseases, help patients in making informed decisions on treatment options and can help in improving overall quality of their lives.","Alzheimer's disease,
Machine learning algorithms,
Computational modeling,
Medical diagnostic imaging"
Designing and implementing Machine Learning Algorithms for advanced communications using FPGAs,"Communications systems can obtain substantial benefits from increased intelligence. Improvements to communications include increased spectral situational awareness, spectral optimization, and robust operation in dynamic and demanding communications environments. Furthermore, complex communication systems require a high degree of autonomous intelligence to optimize performance under such varying conditions. Machine Learning Algorithms provide a means to increase the intrinsic intelligence of wideband communication systems. This paper considers the use of Machine Learning Algorithms to increase the intelligence of communication systems. Specifically, the focus of this paper is to sense and learn the communication environment in real-time and optimize system parameters to maximize end-to-end performance. Communications systems have existing adaptive capabilities in many subsystems such as equalization. The focus in this paper is top level system intelligence by learning from the environment, and based on the system capabilities determine an optimal mode in the solution space in real-time. Furthermore, the goal of this paper is to consider implementation of Machine Learning Algorithms using FPGAs. Design data for implementing Machine Learning Algorithms using FPGAs is provided in the paper as well as reference circuits for implementation. Finally, an example implementation of a Machine Learning Algorithm for intelligent communications is provided based on implementation in a Xilinx UltraScale FPGA.","Signal processing algorithms,
Algorithm design and analysis,
Machine learning algorithms,
Clustering algorithms,
Field programmable gate arrays,
Communication systems,
Optimization"
Software effort estimation using machine learning techniques,"Effort Estimation is a very important activity for planning and scheduling of software project life cycle in order to deliver the product on time and within budget. Machine learning techniques are proving very useful to accurately predict software effort values. This paper presents a review of various machine-learning techniques using in estimation of software project effort namely Artificial Neural Network, Fuzzy logic, Analogy estimation etc. Machine learning techniques consistently predicting accurate results because of its learning natures form previously completed projects. This paper summarizes that each technique has its own features and behave differently according to environment so no technique can be preferred over each other.","Handheld computers,
Cloud computing,
Data science"
A smart approach to diagnose Heart disease through machine learning and Springleaf Marketing Response,"Machine learning is the science of getting computers to learn from data without being unambiguously programmed. Instigated two supervised classification problem (Heart disease prediction and Springleaf Marketing Response) using machine learning algorithms. In heart disease prediction, the objective is to envisage the status of heart disease which can be 0, 1, 2, 3 and 4. These 5 classes are not cleared by data dictionary but 0 means diameter narrowing is less than 50 percent. In Springleaf marketing response, the information provided is large and anonymous (with 1933 features) to envisage whether the customers likely to retort to direct mail and good candidates for their services in order to improve their service and efforts.","Heart,
Machine learning algorithms,
Diseases,
Prediction algorithms,
Pain,
Boosting,
Mathematical model"
Prediction of Consumer Purchasing in a Grocery Store Using Machine Learning Techniques,"Over the past decades, prediction of costumers' purchase behavior has been significantly considered, and completely recognized as one of the most significant research topics in consumer behavior researches. While we attempt to measure response of purchase intention to the contextual factors such as customers' age, gender and income, product price and sale promotion, most of business models are basing on a linear equation to estimate weight of these factors due to the linear equation is not only intuitive for other academics to compare and replicate but also luminous to explain the results for business practitioners. Nevertheless, comparing with other research fields (e.g. pattern recognition and text classification), the prediction methods for purchase behavior are overconcentration of the linear models, especially linear discriminant analysis and logistic regression analysis. On the other hand, as more and more information and communication technologies (ICT, e.g. POS and sensor) are introduced into retail, marketing and management to collect business data, the volumes of data are increasing in exponential growth. Analysis based on linear models are insufficient to satisfy the requirement of academics and practitioners any more, and machine learning techniques have been increasingly attracted us to conduct them as an alternative approach for knowledge discovery and data mining. With regard to these issues, this paper employs two representative machine learning methods: Bayes classifier and support vector machine (SVM) and investigates the performance of them with the data in the real world.","Support vector machines,
Fish,
Business,
Mathematical model,
RFID tags,
Kernel"
A machine learning approach to radar sea clutter suppression,"The radar detection of small maritime targets requires special attention to the suppression of sea clutter returns that, in certain circumstances, may be difficult to discriminate from target returns. In this paper, the novel use of machine learning techniques is explored to address this familiar problem. A comparison of the results obtained using two machine learning techniques for the suppression of sea clutter is presented. Data for this experiment was gathered using an experimental S-band radar called NetRAD. This radar system was observing a coastal scene and the data collected was classified as either target or clutter using k-Nearest-Neighbour (kNN) and Support Vector Machine (SVM) algorithms. The results are presented as averaged probability of detection and probability of false alarm.","Clutter,
Support vector machines,
Doppler effect,
Machine learning algorithms,
Radar clutter,
Training"
Root cause analysis of software bugs using machine learning techniques,"Root cause analysis (RCA) is a systematic process for identifying “root causes” of problems or events and an approach for responding to them. The factor that caused a problem or defect should be permanently eliminated through process improvement. In the context of Software development process it may be used to refer to a specific module or a category of bug which in turn can be useful for tackling the problem at its root. In this paper we propose a machine learning approach for finding root cause of a newly filed software bugs which in turn would help in the faster and cleaner resolution of software bugs. This proposed approach is evaluated for feasibility study on an open source system eclipse. [7], [6]","Computer bugs,
Supervised learning,
Software systems,
Clustering algorithms,
Prediction algorithms,
Labeling"
Analysis of Various Machine Learning Algorithms for Enhanced Opinion Mining Using Twitter Data Streams,"Twitter right now gets around 190 million tweets(little content based Web posts) a day, in which individuals share their remarks with respect to an extensive variety ofsubjects. An expansive number of tweets incorporate sentiments about items and administrations. Notwithstanding, with Twitter being a moderately new wonder, these tweets are underutilized as a hotspot for assessing client supposition andhave lead specialists to think about the likelihood of their abuseso as to recognize concealed information. Hence, two territories are pulling in more enthusiasm for the examination group, the feeling mining and assessment investigation. We to perform anassessment examination of general's conclusions mined from the well known smaller scale blogging site Twitter. The real accentuation of this paper is set on assessing precision of various machine learning calculations for the errand of twitter notion investigation.","Twitter,
Blogs,
Support vector machines,
Data mining,
Organizations,
Appraisal,
Data models"
Software defect prediction analysis using machine learning algorithms,"Software Quality is the most important aspect of a software. Software Defect Prediction can directly affect quality and has achieved significant popularity in last few years. Defective software modules have a massive impact over software's quality leading to cost overruns, delayed timelines and much higher maintenance costs. In this paper we have analyzed the most popular and widely used Machine Learning algorithms - ANN (Artificial Neural Network), PSO(P article Swarm Optimization), DT (Decision Trees), NB(Naive Bayes) and LC (Linear classifier). The five algorithms were analyzed using KEEL tool and validated using k-fold cross validation technique. Datasets used in this research were obtained from open source NASA Promise dataset repository. Seven datasets were selected for defect prediction analysis. Classification was performed on these 7 datasets and validated using 10 fold cross validation. The results demonstrated the dominance of Linear Classifier over other algorithms in terms of defect prediction accuracy.","Handheld computers,
Cloud computing,
Data science,
Bayes methods,
Neural networks,
Decision trees,
Conferences"
SOA Based Integrated Software to Develop Fault Diagnosis Models Using Machine Learning in Rotating Machinery,"Fault detection and diagnostic software (FDDS) supports technicians and engineers to deal with operational matters, in major cases related to complicated systems and advanced technology that require higher performance expectation. Information and communication technologies play an importantrole for implementing efficient maintenance software, therefore, the development of FDDS is posed as an industrial necessity. In case of industrial rotating machinery, data-driven FDDS using available vibration signals, or other related signals monitored from sensors, is currently viewed as an industrial informatics requirement. This paper proposes the application of a Service Oriented Architecture (SOA) to implement an integrated tool for automatically developing and testing machine learning based fault diagnosis models in rotating machinery. As a result, a generic architecture is obtained which is able to build and implement diagnosis models in similar devices or processes. A condition monitoring software application, using the proposed SOA, was implemented in Java and deployed on a computational environment to test its performance in a experimental test bed, under realistic fault mechanical conditions in a gearbox.","Service-oriented architecture,
Machinery,
Business,
Feature extraction,
Fault diagnosis,
Computer architecture"
Machine learning from experience feedback on accidents in transport,"In addition to identifying safety deficiencies evidenced by accidents, the independent investigative body makes recommendations to eliminate or reduce these deficiencies. Its main purpose is to advance transportation safety by conducting investigations of accidents in rail and other modes of transportation. He must answer several questions: what happened, why did it happen, and what can be done to reduce the risk of it happening again? In this process, one of the difficulties involved is finding abnormal accident scenarios which are capable of generating a specific hazard. This paper proposes an original method based on machine learning to assist investigators experts in their crucial task of analysis and assessment of the safety for railway transport systems in France. This contribution is based on the use of artificial intelligence techniques and has involved the development of several approaches and tools which assist in the modeling, storage and assessment of knowledge about safety. The proposed approach has two objectives, firstly to record and store experience concerning safety analyses, and secondly to assist those involved in the development and assessment of the systems in the demanding task of evaluating safety studies.","Accidents,
Knowledge acquisition,
Hazards,
Knowledge based systems,
Tools,
Context"
Machine learning framework for image classification,"Hereby in this paper, we are interested to extraction methods and classification in case of image classification and recognition application. We expose the performance of training models on varying classifier algorithms on Caltech 101 images categories. For feature extraction functions we evaluate the use of the classical SURF technique against global color feature extraction. The purpose of our work is to guess the best machine learning framework techniques to recognize the stop sign images. The trained model will be integrated into a robotic system in a future work.","Feature extraction,
Training,
Image classification,
Classification algorithms,
Computer vision,
Support vector machines,
Portable computers"
Energy efficient VLSI circuits for machine learning on-chip,"The machine-learning based data analytics to support a cloud intelligence (such as Google's αGo) has already gone beyond the scalability of the present computing technology and architecture. The current deep learning based method is not efficient and requires huge consumption of data and power, which has a long latency running on data servers. With the emergence of autonomous vehicles, unmanned aerial vehicles and robotics, there is a huge demand to only analyze a necessary sensed data with small latency and low power at edge devices. In this talk, we will discuss efficient machine-learning algorithms such as fast least-squares method, binary and tensory convolutional neural network method, with according prototyping accelerator developed in FPGA and CMOS-ASIC chips, which has potential to outperform traditional GPU devices. The mapping on future RRAM device will be also briefly addressed.",
A Distributed Environment Decision Maker Based on Machine Learning Techniques,"There are many computational options nowadays, capable to serve properly a wide range of users. Web Services, Cloud Computing, Internet of Things among others are some of the ways technology can be offered. To scientific research this variety is fundamental. One of the major concerns in distributed environments such as cloud computing is the size of data and how to handle it's transport. In some cases, it is necessary to save information in hard drives and send it to other locations. The experiments in Protein Structure Predictions (PSP) usually results in very large amounts of data that are difficult to move from one place to another. Besides that, those results have different computational costs, processing time and price range. The current state-of-art shows the limitations the researchers have to face when working with local computers and most of their work is focused on improve the PSP algorithms. We understand that the historic runs in such systems are a reliable source to be studied and we propose a application of machine learning techniques that can be used as a base for a decision maker capable of defining machine configurations and a proper set-up, while keeping Quality of Service (QoS). The results of the kNN algorithm show a strategy capable of predicting with good accuracy the outcomes such as processing time and resulting file sizes. This defines the base for the decision maker mechanism.","Cloud computing,
Bioinformatics,
Quality of service,
Tools,
Servers,
Proteins,
Computer architecture"
Detailed routing violation prediction during placement using machine learning,"The complexity of design rules at 22nm and below precludes direct incorporation of detailed routing (DR) rules into a placement algorithm. However, ignoring routability rules during the placement process may result in infeasible designs. The congestion estimated by a global router is conventionally used for routing estimation during placement, but it does not include real detailed routing violations, which adversely affect the routability of a design. Presently, there are no methods that directly aim to predict detailed routing violations. In this paper we propose a machine learning based method to predict the shorts that are a major component of detailed routing violations. The proposed method can be integrated into a placement tool and be used as a guide during the placement process to reduce the number of shorts happening in the detailed routing stage. Empirical results show that our method is successful in predicting 88% of the shorts with only 16% incorrectly predicting shorts in no short violation area.","Routing,
Feature extraction,
Pins,
Integrated circuit modeling,
Training,
Data models,
Predictive models"
Characterization of TLC 3D-NAND Flash Endurance through Machine Learning for LDPC Code Rate Optimization,"The advent of the 3D-NAND Flash memories introduced significant issues in terms of characterization and system-level optimization that can be performed to increase the memory reliability over its lifetime. Indeed, the knobs that a system designer can leverage to this extent are many. In this work we show that the application of machine learning algorithms like data clustering on a large characterization data set of TLC 3D-NAND Flash devices can help the designers in optimizing the countermeasures for improving the memory reliability while reducing their implementation cost.","Flash memories,
Parity check codes,
Reliability,
Clustering algorithms,
Optimization,
Computer architecture,
Error correction codes"
A high accuracy and robust machine learning network for pattern recognition based on binary RRAM devices,"Utilizing the binary RRAM devices, a hardware implemented network based on the modified k-nearest neighbor (KNN) algorithm is proposed for pattern recognition. Regarding to the recognition of the handwritten digits, the hardware network shows brilliant performance with simple training scheme, high tolerance of input noise (up to 40%) and variation (up to 60%), and high recognition accuracy rate (more than 90%). The simulations show that the stable binary rather than multilevel resistance characteristics of RRAM enable the better coupling of device and algorithm for the proposed learning network.","Training,
Neurons,
Resistance,
Handwriting recognition,
Electrical resistance measurement,
Switches"
Utilizing Dictionary Learning and Machine Learning for Blind Quality Assessment of 3-D Images,"In recent years, perceptual objective quality assessment of 3-D images has become an intense focus of research. In this paper, we propose an efficient blind 3-D image quality assessment (IQA) metric that utilizes binocular vision-based dictionary learning (DL) and k-nearest-neighbors (KNN)-based machine learning (ML) to more accurately align with human subjective judgments. More specifically, in the DL stage, histogram representations from the local patterns of simple and complex cells are concatenated to form basic feature vectors. Then, by using a collaborative representation algorithm, the learned binocular quality-aware features of the distorted 3-D image can be efficiently represented by a linear combination of only a few of these basic feature vectors. In the ML stage, we intuitively simulate the complex high-level behaviors of human perceptual activity with KNN-based ML, which transfers the weighted human subjective quality scores from the annotated 3-D images to the query 3-D image. Our results using three standard subject-rated 3-D-IQA databases confirm that the proposed metric consistently aligns with the subjective ratings and outperforms many representative blind metrics.",
Machine Learning With Big Data: Challenges and Approaches,"The Big Data revolution promises to transform how we live, work, and think by enabling process optimization, empowering insight discovery and improving decision making. The realization of this grand potential relies on the ability to extract value from such massive data through data analytics; machine learning is at its core because of its ability to learn from data and provide data driven insights, decisions, and predictions. However, traditional machine learning approaches were developed in a different era, and thus are based upon multiple assumptions, such as the data set fitting entirely into memory, what unfortunately no longer holds true in this new context. These broken assumptions, together with the Big Data characteristics, are creating obstacles for the traditional techniques. Consequently, this paper compiles, summarizes, and organizes machine learning challenges with Big Data. In contrast to other research that discusses challenges, this work highlights the cause-effect relationship by organizing challenges according to Big Data Vs or dimensions that instigated the issue: volume, velocity, variety, or veracity. Moreover, emerging machine learning approaches and techniques are discussed in terms of how they are capable of handling the various challenges with the ultimate objective of helping practitioners select appropriate solutions for their use cases. Finally, a matrix relating the challenges and approaches is presented. Through this process, this paper provides a perspective on the domain, identifies research gaps and opportunities, and provides a strong foundation and encouragement for further research in the field of machine learning with Big Data.","Big Data,
Machine learning algorithms,
Data mining,
Algorithm design and analysis,
Data analysis,
Support vector machines,
Classification algorithms"
Estimation of low frequency electromagnetic values using machine learning,"It is almost impossible to completely eliminate the effects of electromagnetic waves, but it is possible to reduce their effects. The use of machine learning is important to estimate these effects. Machine learning has been accelerating the upward trend over the last few years. It provides great advantages for problems that are difficult for human but easy to solve for machine. As a result, machine prediction is becoming increasingly popular. In this study, low frequency electromagnetic field values were measured in the buildings inside the campus and the output vector was tried to be estimated using machine learning. The results obtained by using linear regression, Gradient Descent (GD) algorithm and ridge regression models for estimation are evaluated.","Frequency measurement,
Prediction algorithms,
Electromagnetic fields,
Pollution measurement,
Linear regression,
Voltage measurement"
Machine learning via multimodal signal processing,"This paper proposes a methodology for recognition of vocal music (Byzantine music) via multi-modal signals processing. A sequence of multi-modal signals is captured from the expert's (teacher) and student's hymns performances, respectively. The machine learning system is trained using the values of particular features which are extracted from the captured multi-modal signals. After the system is being trained then it becomes able to recognize any hymn performance from the corpus. Training and recognition takes place in real time by utilizing machine learning techniques. The evaluation of the system was carried out with the cross - validation statistical method Jackknife, giving promising results.","Hidden Markov models,
Feature extraction,
Speech recognition,
Music,
Training,
Mel frequency cepstral coefficient"
MEMS accelerometers classification using machine-learning methods,MEMS accelerometers classification with similar design and process parameters using machine-learning methods is proposed in the paper. A numerical experiment was performed. In it we investigated the possibilities of various machine-learning methods for solving classification task. The best results by accuracy parameter were obtained by Subspace KNN method.,"Micromechanical devices,
Accelerometers,
Support vector machines,
Production,
Solid modeling,
Correlation"
Application of machine learning methodologies to multiyear forecasts of video subscribers,"The Market Intelligence division of S&P Global provides annual multiyear forecasts of United States subscribers for the video industry, which is comprised of cable, satellite, and telecommunication service providers. The current forecasts leave room for improvement as they are labor-intensive to generate and can be influenced by biases of subject-matter experts. The focus of this project is to explore the application of machine learning methods to these forecasts. We used demographic and subscriber data to assess the accuracy of different models, splitting it into training, validation, and testing sets. These models include support vector regression, artificial neural networks, and tree-based models, namely the traditional random forest and XGBoost. The training and validation sets were used to identify optimal parameters for each model. After identifying these parameters, we compared each model using the held-out testing set. These results indicated that the XGBoost model performed best, with a root mean squared error of 2,712. This initial exploration into machine learning shows promise and indicates it can be used as a tool to enhance S&P's current forecasting techniques. The industry is continuing to change as new technologies create disruption in the marketplace, which necessitates continued research into forecasting within this field.","Industries,
Support vector machines,
Forecasting,
Neural networks,
Predictive models,
Training,
Testing"
Spark acceleration on FPGAs: A use case on machine learning in Pynq,"Spark is one of the most widely used frameworks for data analytics. Spark allows fast development for several applications like machine learning, graph computations, etc. In this paper, we present Spynq: A framework for the efficient deployment of data analytics on embedded systems that are based on the heterogeneous MPSoC FPGA called Pynq. The mapping of Spark on Pynq allows that fast deployment of embedded and cyber-physical systems that are used in edge and fog computing. The proposed platform is evaluated in a typical machine learning application based on logistic regression. The performance evaluation shows that the heterogeneous FPGA-based MPSoC can achieve up to 11× speedup compared to the execution time in the ARM cores and can reduce significantly the development time of embedded and cyber-physical systems on Spark applications.","Hardware,
Sparks,
Logistics,
Libraries,
Field programmable gate arrays,
Program processors,
Cloud computing"
Dynamic power pre-adjustments with machine learning that mitigate EDFA excursions during defragmentation,"We examine EDFA power excursions during three defragmentation methods of flexgrid super-channels. Using a machine learning approach, we demonstrate automatic and dynamic adjustments of pre-EDFA power levels, and show the mitigation of post-EDFA power discrepancy among channels by over 62%.","Correlation,
Optical fiber networks,
Erbium-doped fiber amplifiers,
Wavelength division multiplexing,
Integrated optics,
Testing,
Stimulated emission"
QoT estimation for unestablished lighpaths using machine learning,"We investigate a machine-learning technique that predicts whether the bit-error-rate of unestablished lightpaths meets the required threshold based on traffic volume, desired route and modulation format. The system is trained and tested on synthetic data.","Modulation,
Training,
Estimation,
Signal to noise ratio,
Optical fiber networks,
Bit error rate"
Machine learning to predict person's interest towards visual object by utilizing EEG signal,"This experiment aims to determine the best algorithm to predict person's interest towards visual object. EEG signal (signal captured by Electroencephalograph) is utilized to show if a person is interested. This can be useful, such as for marketers to discover consumer's brain response towards product and identify whether the product is interesting. Learning algorithm classifies subject's brain response into one of two classes: “interested” or “not interested”. Research on EEG signal regarding a person's interest is rare even no one explicitly discuss about it. It is vice versa for person's emotion or attention. That is why the observation of person's interest is done by adapting the handling on EEG signal that is usually used to identify emotion and attention. The handling includes EEG channels selection, learning algorithms selection, and frequency-based waves selection. This experiment, based on the handling, retrieves EEG signal from 12 EEG channels to get theta, alpha, and low beta wave. It involves the participants of 12 males and 20 females, 19- 22 years old. There are several algorithms that are fit and common to train EEG signal: SVM, ANN, KNN, and Naïve Bayes. To get the best algorithm, this experiment compares the performance measure of these algorithms, the configuration of parameter, and the characteristics. The result shows that SVM and ANN can be chosen as the best algorithms. SVM yielded accuracy of 83.65% and F-measure average of 83.70% by using kernel function of radial basis function. Meanwhile, ANN yielded accuracy of 83.36% and F-measure average of 83.30% by using 1 hidden layer, 50 hidden neurons, learning rate of 0.1, and momentum of 0.2. In addition to have optimum performance and slight difference, both algorithms also have several excellences.",
Nonlinear inter-subcarrier intermixing reduction in coherent optical OFDM using fast machine learning equalization,We experimentally demonstrate a Newton support vector machine (N-SVM)-based nonlinear equalizer (NLE) of reduced classifier complexity for 40-Gb/s 16-QAM CO-OFDM. At 2000-km N-SVM extends the launched optical power by 2-dB compared to Volterra-based NLE.,"equalisers,
learning (artificial intelligence),
light coherence,
OFDM modulation,
optical fibre communication,
optical modulation,
quadrature amplitude modulation,
support vector machines,
telecommunication computing"
Petuum: A New Platform for Distributed Machine Learning on Big Data,"What is a systematic way to efficiently apply a wide spectrum of advanced ML programs to industrial scale problems, using Big Models (up to 100 s of billions of parameters) on Big Data (up to terabytes or petabytes)? Modern parallelization strategies employ fine-grained operations and scheduling beyond the classic bulk-synchronous processing paradigm popularized by MapReduce, or even specialized graph-based execution that relies on graph representations of ML programs. The variety of approaches tends to pull systems and algorithms design in different directions, and it remains difficult to find a universal platform applicable to a wide range of ML programs at scale. We propose a general-purpose framework, Petuum, that systematically addresses data- and model-parallel challenges in large-scale ML, by observing that many ML programs are fundamentally optimization-centric and admit error-tolerant, iterative-convergent algorithmic solutions. This presents unique opportunities for an integrative system design, such as bounded-error network synchronization and dynamic scheduling based on ML program structure. We demonstrate the efficacy of these system designs versus well-known implementations of modern ML algorithms, showing that Petuum allows ML programs to run in much less time and at considerably larger model sizes, even on modestly-sized compute clusters.","Data models,
Computational modeling,
Big data,
Servers,
Convergence,
Mathematical model,
Synchronization"
Nonintrusive Load Monitoring Using Wavelet Design and Machine Learning,"This paper presents a new concept based on wavelet design and machine learning applied to nonintrusive load monitoring. The wavelet coefficients of length-6 filter are determined using procrustes analysis and are used to construct new wavelets to match the load signals to be detected, unlike previous work which used previously designed wavelet functions that are special cases of Daubechies filters to suit other nonpower system applications such as communications and image processing. The results of applying the new concept to a test system consisting of four loads have shown that the newly designed wavelet can improve the prediction accuracy compared with that obtained using Daubechies filter of order three while keeping the prominent features of the pattern in the detail levels.","Wavelet transforms,
Feature extraction,
Wavelet analysis,
Indexes,
Transient analysis,
Switches"
Toward Emotion-Aware Computing: A Loop Selection Approach Based on Machine Learning for Speculative Multithreading,"Emotion-aware computing can recognize, interpret, process, and simulate human affects. These programs in this area are compute-intensive applications, so they need to be executed in parallel. Loops usually have regular structures and programs spend significant amounts of time executing them, and thus loops are ideal candidates for exploiting the parallelism of sequential programs. However, it is difficult to decide which set of loops should be parallelized to improve program performance. The existing research is one-size-fits-all strategy and cannot guarantee to select profitable loops to be parallelized. This paper proposes a novel loop selection approach based on machine learning (ML-based) for selecting the profitable loops and paralleling them on multi-core by speculative multithreading (SpMT). It includes establishing sufficient training examples, building and applying prediction model to select profitable loops for speculative parallelization. Using the ML-based loop selection approach, an unseen emotion-aware sequential program can obtain a stable, much higher speedup than the one-size-fits-all approach. On Prophet, which is a generic SpMT processor to evaluate the performance of multithreaded programs, the novel loop selection approach is evaluated and reaches an average speedup of 1.87 on a 4-core processor. Experiment results show that the ML-based approach can obtain a significant increase in speedup, and Olden benchmarks deliver a better performance improvement of 6.70% on a 4-core than the one-size-fits-all approach.","Instruction sets,
Training,
Computers,
Multithreading,
Predictive models,
Benchmark testing"
DREAM: Diabetic Retinopathy Analysis Using Machine Learning,"This paper presents a computer-aided screening system (DREAM) that analyzes fundus images with varying illumination and fields of view, and generates a severity grade for diabetic retinopathy (DR) using machine learning. Classifiers such as the Gaussian Mixture model (GMM), k-nearest neighbor (kNN), support vector machine (SVM), and AdaBoost are analyzed for classifying retinopathy lesions from nonlesions. GMM and kNN classifiers are found to be the best classifiers for bright and red lesion classification, respectively. A main contribution of this paper is the reduction in the number of features used for lesion classification by feature ranking using Adaboost where 30 top features are selected out of 78. A novel two-step hierarchical classification approach is proposed where the nonlesions or false positives are rejected in the first step. In the second step, the bright lesions are classified as hard exudates and cotton wool spots, and the red lesions are classified as hemorrhages and micro-aneurysms. This lesion classification problem deals with unbalanced datasets and SVM or combination classifiers derived from SVM using the Dempster-Shafer theory are found to incur more classification error than the GMM and kNN classifiers due to the data imbalance. The DR severity grading system is tested on 1200 images from the publicly available MESSIDOR dataset. The DREAM system achieves 100% sensitivity, 53.16% specificity, and 0.904 AUC, compared to the best reported 96% sensitivity, 51% specificity, and 0.875 AUC, for classifying images as with or without DR. The feature reduction further reduces the average computation time for DR severity per image from 59.54 to 3.46 s.","Lesions,
Retinopathy,
Diabetes,
Support vector machines,
Feature extraction,
Hemorrhaging,
Image segmentation"
A Distributed Support Vector Machine Learning Over Wireless Sensor Networks,"This paper is about fully-distributed support vector machine (SVM) learning over wireless sensor networks. With the concept of the geometric SVM, we propose to gossip the set of extreme points of the convex hull of local data set with neighboring nodes. It has the advantages of a simple communication mechanism and finite-time convergence to a common global solution. Furthermore, we analyze the scalability with respect to the amount of exchanged information and convergence time, with a specific emphasis on the small-world phenomenon. First, with the proposed naive convex hull algorithm, the message length remains bounded as the number of nodes increases. Second, by utilizing a small-world network, we have an opportunity to drastically improve the convergence performance with only a small increase in power consumption. These properties offer a great advantage when dealing with a large-scale network. Simulation and experimental results support the feasibility and effectiveness of the proposed gossip-based process and the analysis.","Support vector machines,
Training,
Convergence,
Wireless sensor networks,
Quadratic programming,
Kernel,
Scalability"
Machine Learning and Conceptual Reasoning for Inconsistency Detection,"This paper focuses on detecting inconsistencies within text corpora. It is a very interesting area with many applications. Most existing methods deal with this problem using complicated textual analysis, which is known for not being accurate enough. We propose a new methodology that consists of two steps, the first one being a machine learning step that performs multilevel text categorization. The second one applies conceptual reasoning on the predicted categories in order to detect inconsistencies. This paper has been validated on a set of Islamic advisory opinions (also known as fatwas). This domain is gaining a large interest with users continuously checking the authenticity and relevance of such content. The results show that our method is very accurate and can complement existing methods using the linguistic analysis.","Cognition,
Context,
Natural language processing,
Pragmatics,
Ontologies,
Text categorization,
Density measurement"
A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots,"We study the problem of perceiving forest or mountain trails from a single monocular image acquired from the viewpoint of a robot traveling on the trail itself. Previous literature focused on trail segmentation, and used low-level features such as image saliency or appearance contrast; we propose a different approach based on a deep neural network used as a supervised image classifier. By operating on the whole image at once, our system outputs the main direction of the trail compared to the viewing direction. Qualitative and quantitative results computed on a large real-world dataset (which we provide for download) show that our approach outperforms alternatives, and yields an accuracy comparable to the accuracy of humans that are tested on the same image classification task. Preliminary results on using this information for quadrotor control in unseen trails are reported. To the best of our knowledge, this is the first letter that describes an approach to perceive forest trials, which is demonstrated on a quadrotor micro aerial vehicle.","Cameras,
Robot vision systems,
Roads,
Visual perception,
Mobile robots,
Image segmentation"
Multistep Prediction of Physiological Tremor Based on Machine Learning for Robotics Assisted Microsurgery,"For effective tremor compensation in robotics assisted hand-held device, accurate filtering of tremulous motion is necessary. The time-varying unknown phase delay that arises due to both software (filtering) and hardware (sensors) in these robotics instruments adversely affects the device performance. In this paper, moving window-based least squares support vector machines approach is formulated for multistep prediction of tremor to overcome the time-varying delay. This approach relies on the kernel-learning technique and does not require the knowledge of prediction horizon compared to the existing methods that require the delay to be known as a priori. The proposed method is evaluated through simulations and experiments with the tremor data recorded from surgeons and novice subjects. Comparison with the state-of-the-art techniques highlights the suitability and better performance of the proposed method.","Delays,
Training,
Physiology,
Accuracy,
Robots,
Microsurgery"
A Machine Learning Approach to Meter Placement for Power Quality Estimation in Smart Grid,"Due to the high-measuring cost, the monitoring of power quality (PQ) is nontrivial. This paper is aimed at reducing the cost of PQ monitoring in power network. Using a real-world PQ dataset, this paper adopts a learn-from-data approach to obtain a device latent feature model, which captures the device behavior as a PQ transition function. With the latent feature model, the power network could be modeled, in analogy, as a data-driven network, which presents the opportunity to use the well-investigated network monitoring and data estimation algorithms to solve the network quality monitoring problem in power grid. Based on this network model, algorithms are proposed to intelligently place measurement devices on suitable power links to reduce the uncertainty of PQ estimation on unmonitored power links. The meter placement algorithms use entropy-based measurements and Bayesian network models to identify the most suitable power links for PQ meter placement. Evaluation results on various simulated networks including IEEE distribution test feeder system show that the meter placement solution is efficient, and has the potential to significantly reduce the uncertainty of PQ values on unmonitored power links.","Monitoring,
Estimation,
Reliability,
Voltage fluctuations,
Power grids,
Phasor measurement units,
Power quality"
Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine for Effective Clinical Diagnosis and Treatment,"Clinical practice calls for reliable diagnosis and optimized treatment. However, human errors in health care remain a severe issue even in industrialized countries. The application of clinical decision support systems (CDSS) casts light on this problem. However, given the great improvement in CDSS over the past several years, challenges to their wide-scale application are still present, including: 1) decision making of CDSS is complicated by the complexity of the data regarding human physiology and pathology, which could render the whole process more time-consuming by loading big data related to patients; and 2) information incompatibility among different health information systems (HIS) makes CDSS an information island, i.e., additional input work on patient information might be required, which would further increase the burden on clinicians. One popular strategy is the integration of CDSS in HIS to directly read electronic health records (EHRs) for analysis. However, gathering data from EHRs could constitute another problem, because EHR document standards are not unified. In addition, HIS could use different default clinical terminologies to define input data, which could cause additional misinterpretation. Several proposals have been published thus far to allow CDSS access to EHRs via the redefinition of data terminologies according to the standards used by the recipients of the data flow, but they mostly aim at specific versions of CDSS guidelines. This paper views these problems in a different way. Compared with conventional approaches, we suggest more fundamental changes; specifically, uniform and updatable clinical terminology and document syntax should be used by EHRs, HIS, and their integrated CDSS. Facilitated data exchange will increase the overall data loading efficacy, enabling CDSS to read more information for analysis at a given time. Furthermore, a proposed CDSS should be based on self-learning, which dynamically updates a knowledge model according to the data-stream-based upcoming data set. The experiment results show that our system increases the accuracy of the diagnosis and treatment strategy designs.","Medical diagnostic imaging,
Decision trees,
Data mining,
Diseases,
Clinical diagnosis,
Cognition"
Scaling Up Graph-Based Semisupervised Learning via Prototype Vector Machines,"When the amount of labeled data are limited, semisupervised learning can improve the learner's performance by also using the often easily available unlabeled data. In particular, a popular approach requires the learned function to be smooth on the underlying data manifold. By approximating this manifold as a weighted graph, such graph-based techniques can often achieve state-of-the-art performance. However, their high time and space complexities make them less attractive on large data sets. In this paper, we propose to scale up graph-based semisupervised learning using a set of sparse prototypes derived from the data. These prototypes serve as a small set of data representatives, which can be used to approximate the graph-based regularizer and to control model complexity. Consequently, both training and testing become much more efficient. Moreover, when the Gaussian kernel is used to define the graph affinity, a simple and principled method to select the prototypes can be obtained. Experiments on a number of real-world data sets demonstrate encouraging performance and scaling properties of the proposed approach. It also compares favorably with models learned via ℓ1-regularization at the same level of model sparsity. These results demonstrate the efficacy of the proposed approach in producing highly parsimonious and accurate models for semisupervised learning.","Prototypes,
Approximation methods,
Semisupervised learning,
Kernel,
Manifolds,
Training,
Laplace equations"
Combining Pixel- and Object-Based Machine Learning for Identification of Water-Body Types From Urban High-Resolution Remote-Sensing Imagery,"Water is one of the vital components for the ecological environment, which plays an important role in human survival and socioeconomic development. Water resources in urban areas are gradually decreasing due to the rapid urbanization, especially in developing countries. Therefore, the precise extraction and automatic identification of water bodies are of great significance and urgently required for urban planning. It should be noted that although some studies have been reported regarding the water-area extraction, to our knowledge, few papers concern the identification of urban water types (e.g, rivers, lakes, canals, and ponds). In this paper, a novel two-level machine-learning framework is proposed for identifying the water types from urban high-resolution remote-sensing images. The framework consists of two interpretation levels: 1) water bodies are extracted at the pixel level, where the water/shadow/vegetation indexes are considered and 2) water types are further identified at the object level, where a set of geometrical and textural features are used. Both levels employ machine learning for the image interpretation. The proposed framework is validated using the GeoEye-1 and WorldView-2 images, over two mega cities in China, i.e, Wuhan and Shenzhen, respectively. The experimental results show that the proposed method achieved satisfactory accuracies for both water extraction [95.4% (Shenzhen), 96.2% (Wuhan)], and water type classification [94.1% (Shenzhen), 95.9% (Wuhan)] in complex urban areas.","Indexes,
Feature extraction,
Water resources,
Remote sensing,
Rivers,
Vegetation mapping,
Lakes"
Structured Latent Label Consistent Dictionary Learning for Salient Machine Faults Representation-Based Robust Classification,"This paper investigates the salient machine faults representation-based classification issue by dictionary learning. A novel structured latent label consistent dictionary learning (LLC-DL) model is proposed for joint discriminative salient representation and classification. Our LLC-DL deals with the tasks by solving one objective function that aims to minimize the structured reconstruction error, structured discriminative sparse-code error and classification error simultaneously. Also, LLC-DL decomposes given signals into a sparse reconstruction part over structured latent weighted discriminative dictionary, a salient feature extraction part and an error part fitting noise. Specifically, the dictionary is learnt atom by atom, where each dictionary atom is learnt with a latent vector that reduces the disturbance between interclass atoms. The structured coding coefficients are calculated via minimizing the reconstruction error and discriminative sparse code error simultaneously. The salient representations are learnt by embedding signals onto a projection and a robust linear classifier is then trained over the learned salient features directly so that features can be ensured to be optimal for classification, where robust l2,1-norm imposed on the classifier can make the prediction results more accurate. By including a salient feature extraction term, the classification approach of LLC-DL is very efficient, since there is no need to involve an extra time-consuming sparse reconstruction process with the well-trained dictionary for each test signal. Extensive simulations versify the effectiveness of our algorithm.","Dictionaries,
Feature extraction,
Robustness,
Linear programming,
Encoding,
Informatics,
Classification algorithms"
Extreme Learning Machine With Composite Kernels for Hyperspectral Image Classification,"Due to its simple, fast, and good generalization ability, extreme learning machine (ELM) has recently drawn increasing attention in the pattern recognition and machine learning fields. To investigate the performance of ELM on the hyperspectral images (HSIs), this paper proposes two spatial-spectral composite kernel (CK) ELM classification methods. In the proposed CK framework, the single spatial or spectral kernel consists of activation-function-based kernel and general Gaussian kernel, respectively. The proposed methods inherit the advantages of ELM and have an analytic solution to directly implement the multiclass classification. Experimental results on three benchmark hyperspectral datasets demonstrate that the proposed ELM with CK methods outperform the general ELM, SVM, and SVM with CK methods.","Kernel,
Feature extraction,
Support vector machines,
Training,
Educational institutions,
Hyperspectral imaging"
PolyNet: A Polynomial-Based Learning Machine for Universal Approximation,"Currently, there is a need in all disciplines for efficient and powerful machine learning (ML) algorithms for handling offline and real-time nonlinear data. Industrial applications abound from real-time control systems to modeling and simulation of complex systems and processes. Certain ML methods have become popular with researchers and engineers. Such techniques include fuzzy systems (FSs), artificial neural networks (ANNs), radial basis function (RBF) networks, and support vector regression (SVR) machines. Historically, polynomial-based learning machines (PLMs) based on the group method of data handling (GMDH) model have enjoyed usage similar to that of these other methods. However, unwieldy kernel functions in the form of large high-order polynomials, and relatively limited computer speed and capacity, have limited the use of PLMs to comparatively small problems with low dimensionality and simple functional relationships. Thus, true polynomial-based ML solutions have drifted out of vogue for at least two decades. This work attempts to reinvigorate the interest in PLMs by introducing a novel practical implementation called PolyNet. It will be shown that once certain algorithms are applied to the generation, training, and functional operation of PLMs, they can compete on par with or better than methods currently in use.","Polynomials,
Training,
Artificial neural networks,
Kernel,
Approximation methods,
Approximation algorithms,
Algorithm design and analysis"
A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection,"This survey paper describes a focused literature survey of machine learning (ML) and data mining (DM) methods for cyber analytics in support of intrusion detection. Short tutorial descriptions of each ML/DM method are provided. Based on the number of citations or the relevance of an emerging method, papers representing each method were identified, read, and summarized. Because data are so important in ML/DM approaches, some well-known cyber data sets used in ML/DM are described. The complexity of ML/DM algorithms is addressed, discussion of challenges for using ML/DM for cyber security is presented, and some recommendations on when to use a given method are provided.","IP networks,
Protocols,
Measurement,
Ports (Computers),
Data models,
Data mining,
Computer security"
Predicting Purchase Decisions Based on Spatio-Temporal Functional MRI Features Using Machine Learning,"Machine learning algorithms allow us to directly predict brain states based on functional magnetic resonance imaging (fMRI) data. In this study, we demonstrate the application of this framework to neuromarketing by predicting purchase decisions from spatio-temporal fMRI data. A sample of 24 subjects were shown product images and asked to make decisions of whether to buy them or not while undergoing fMRI scanning. Eight brain regions which were significantly activated during decision-making were identified using a general linear model. Time series were extracted from these regions and input into a recursive cluster elimination based support vector machine (RCE-SVM) for predicting purchase decisions. This method iteratively eliminates features which are unimportant until only the most discriminative features giving maximum accuracy are obtained. We were able to predict purchase decisions with 71% accuracy, which is higher than previously reported. In addition, we found that the most discriminative features were in signals from medial and superior frontal cortices. Therefore, this approach provides a reliable framework for using fMRI data to predict purchase-related decision-making as well as infer its neural correlates.","Accuracy,
Support vector machines,
Feature extraction,
Magnetic resonance imaging,
Time series analysis,
Prediction algorithms,
Testing"
Hand Body Language Gesture Recognition Based on Signals From Specialized Glove and Machine Learning Algorithms,"The man-machine interface (MMI) is one of the most exciting areas of contemporary research. To make the MMI as convenient for a human as possible, it is desirable that efficient algorithms for recognizing body language are developed. This paper presents a system for quick and effective recognition of gestures of hand body language, based on data from a specialized glove equipped with ten sensors. In the experiment, 10 people performed 22 hand body language gestures. Each of the 22 gestures was executed 10 times. Collected data were preprocessed in multiple ways and three machine learning algorithms were designed based on classifiers (probabilistic neural network, support vector machine, and k-nearest neighbors algorithm) trained and tested by a tenfold cross-validation technique. The best designed classifiers gained effectiveness of gesture recognition at κ = 98.24% with a very short time of testing, below 1 ms. The experiments confirm that efficient and quick recognition of hand body language is possible.","Sensors,
Thumb,
Feature extraction,
Gesture recognition,
Informatics,
Principal component analysis"
Machine Learning for Predictive Maintenance: A Multiple Classifier Approach,"In this paper, a multiple classifier machine learning (ML) methodology for predictive maintenance (PdM) is presented. PdM is a prominent strategy for dealing with maintenance issues given the increasing need to minimize downtime and associated costs. One of the challenges with PdM is generating the so-called “health factors,” or quantitative indicators, of the status of a system associated with a given maintenance issue, and determining their relationship to operating costs and failure risk. The proposed PdM methodology allows dynamical decision rules to be adopted for maintenance management, and can be used with high-dimensional and censored data problems. This is achieved by training multiple classification modules with different prediction horizons to provide different performance tradeoffs in terms of frequency of unexpected breaks and unexploited lifetime, and then employing this information in an operating cost-based maintenance decision system to minimize expected costs. The effectiveness of the methodology is demonstrated using a simulated example and a benchmark semiconductor manufacturing maintenance problem.","Informatics,
Training,
Manufacturing,
Production,
Availability,
Predictive maintenance"
Paddy-Rice Phenology Classification Based on Machine-Learning Methods Using Multitemporal Co-Polar X-Band SAR Images,"Crop monitoring and phenology estimation based on the satellite systems have become an important research area due to high demand on crops. Satellites with synthetic aperture radar (SAR) sensor are highly preferred on such studies because of not only their day/night and all weather acquisition capabilities but also their ability to detect small morphological changes in monitored target, regarding the wavelength of signals. Besides, thanks to the high temporal resolution of new generation space-based sensors, it has been possible to monitor growth cycle of crops by classification algorithms. This paper focused on building a feasible phenology classification schema for paddy-rice using multitemporal co-polar TerraSAR-X images. Phenology classification was conducted with support vector machines (SVM) with linear and nonlinear kernel, k-nearest neighbors (kNN), and decision trees (DT). The key implementation challenges such as the number of classes, the identification of the boundaries of the classes, and the selection of textural and polarimetric features were deeply analyzed. According to all the evaluations conducted, the classification schema was finalized to be used for obtaining thematic maps for two independent rice-cultivated agricultural areas located in Spain and Turkey. The results of these experiments enable one to draw a conclusion about feasibility of machine learning (ML) algorithms in operational phenology monitoring.","Agriculture,
Synthetic aperture radar,
Monitoring,
Support vector machines,
Backscatter,
Feature extraction,
Labeling"
Crevasse Detection in Ice Sheets Using Ground Penetrating Radar and Machine Learning,"This paper presents methods to automatically classify ground penetrating radar (GPR) images of crevasses on ice sheets. We use a combination of support vector machines (SVMs) and hidden Markov models (HMMs) with down sampling, a preprocessing step that is unbiased and suitable for real-time analysis and detection. We perform modified cross-validation experiments with 129 examples of Greenland GPR imagery from 2012, collected by a lightweight robot towing a GPR. In order to minimize false positives, an HMM classifier is trained to prescreen the data and mark locations in the GPR files to evaluate with an SVM, and we evaluate the classification results with a similar modified cross-validation technique. The combined HMM-SVM method retains all of the correct classifications by the SVM, and reduces the false positive rate to 0.0007. This method also reduces the computational burden in classifying GPR traces because the SVM is evaluated only on select prescreened traces. Our experiments demonstrate the promise, robustness, and reliability of real-time crevasse detection and classification with robotic GPR surveys.","Ground penetrating radar,
Snow,
Ice,
Hidden Markov models,
Machine learning,
Real-time systems,
Support vector machines"
A Machine Learning Framework for Detecting Landslides on Earthen Levees Using Spaceborne SAR Imagery,"Earthen levees have a significant role in protecting large areas of inhabited and cultivated land in the United States from flooding. Failure of the levees can result in loss of life and property. Slough slides are among the problems which can lead to complete levee failure during a high water event. In this paper, we develop a method to detect such slides using X-band synthetic aperture radar (SAR) data. Our proposed methodology includes: radiometric normalization of the TerraSAR image using high-resolution digital elevation map (DEM) data; extraction of features including backscatter and texture features from the levee; a feature selection method based on minimum redundancy maximum relevance (mRMR); and training a support vector machine (SVM) classifier and testing on the area of interest. To validate the proposed methodology, ground-truth data are collected from slides and healthy areas of the levee. The study area is part of the levee system along the lower Mississippi River in the United States. The output classes are healthy and slide areas of the levee. The results show the average classification accuracies of approximately 0.92 and Cohen's kappa measures of 0.85 for both healthy and slide pixels using ten optimal features selected by mRMR with a sigmoid SVM. A comparison of the SVM performance to the maximum likelihood (ML) and back propagation neural network (BPNN) shows that the average accuracy of the SVM is superior to that of the BPNN and ML classifiers.","Levee,
Synthetic aperture radar,
Terrain factors,
Feature extraction,
Support vector machines,
Accuracy,
Remote sensing"
A Combined Prognostic Model Based on Machine Learning for Tidal Current Prediction,"This paper proposes a univariate prognostic approach based on wavelet transform and support vector regression (SVR) to predict the tidal current speed and direction with high accuracy. The proposed model decomposes the tidal current data into some subharmonic components. The details and approximation components are later fed to several SVR models to attend the prediction process. In order to increase the robustness of the model, the idea of combined prediction is used to model each subharmonic signal by several SVRs. The median operator is further used to determine the aggregated forecast tidal current data. Due to the high reliance of SVR model on the kernel function and hyperplane parameters, a new optimization method based on the bat algorithm is used to train the SVR model. The final forecast tidal current data are constructed using an aggregation operator in the output of the SVRs. The accuracy and satisfying performance of the proposed model are examined on the practical tidal data collected from the Bay of Fundy, NS, Canada. The experimental results reveal the high capability and robustness of the proposed hybrid model for the tidal current prediction.","Predictive models,
Discrete wavelet transforms,
Kernel,
Training,
Optimization"
Human interactive machine learning for trust in teams of autonomous robots,"Unmanned systems are increasing in number, while their manning requirements remain the same. To decrease manpower demands, machine learning techniques and autonomy are gaining traction and visibility. One barrier is human perception and understanding of autonomy. Machine learning techniques can result in “black box” algorithms that may yield high fitness, but poor comprehension by operators. However, Interactive Machine Learning (IML), a method to incorporate human input over the course of algorithm development by using neuro-evolutionary machine-learning techniques, may offer a solution. IML is evaluated here for its impact on developing autonomous team behaviors in an area search task. Initial findings show that IML-generated search plans were chosen over plans generated using a non-interactive ML technique, even though the participants trusted them slightly less. Further, participants discriminated each of the two types of plans from each other with a high degree of accuracy, suggesting the IML approach imparts behavioral characteristics into algorithms, making them more recognizable. Together the results lay the foundation for exploring how to team humans successfully with ML behavior.","Automation,
Robots,
Machine learning algorithms,
Training,
Command and control systems,
Computers,
Conferences"
Modeling Scalability of Distributed Machine Learning,Abstract-Present day machine learning is computationallyintensive and processes large amounts of data. It is implementedin a distributed fashion in order to address these scalabilityissues. The work is parallelized across a number of computingnodes. It is usually hard to estimate in advance how manynodes to use for a particular workload. We propose a simpleframework for estimating the scalability of distributed machinelearning algorithms. We measure the scalability by means of thespeedup an algorithm achieves with more nodes. We proposetime complexity models for gradient descent and graphicalmodel inference. We validate the gradient descent model withexperiments on deep learning training and graphical inferenceswith experiments on loopy belief propagation. The proposedframework was used to study the scalability of machine learningalgorithms in Apache Spark,"Computational modeling,
Machine learning algorithms,
Time complexity,
Scalability,
Graphical models,
Machine learning,
Data models"
Breast Cancer Detection Using K-Nearest Neighbor Machine Learning Algorithm,"Breast cancer is very popular between females all over the world. However, detecting this cancer in its first stages helps in saving lives. Radiologists can predict if the mammography images have cancer or not, but they may miss about 15% of them. In this paper, we propose a new method to detect the breast cancer with high accuracy. This method consists of two main parts, in the first part the image processing techniques are used to prepare the mammography images for feature and pattern extraction process. The second part is presented by utilizing the extracted features as an input for a two types of supervised learning models, which are Back Propagation Neural Network (BPNN) model and the Logistic Regression (LR) model. In this paper we examined the accuracy of these models. The results showed that the LR model utilized more features than the BPNN.","Breast cancer,
Biological neural networks,
Machine learning algorithms,
Logistics,
Image processing,
Mathematical model,
Matrix decomposition"
Random projections for scaling machine learning on FPGAs,"Random projections have recently emerged as a powerful technique for large scale dimensionality reduction in machine learning applications. Crucially, the projection can be obtained from sparse probability distributions, enabling hardware implementations with little overhead. In this paper, we describe a Field-Programmable Gate Array (FPGA) implementation alongside a kernel adaptive filter (KAF) that is capable of reducing computational resources by introducing a controlled error term, achieving higher modelling capacity for given hardware resources. Empirical results involving classification, regression and novelty detection show that a 40% net increase in available resources and improvements in prediction accuracy is achievable for projections which halve the input vector length, enabling us to scale-up hardware implementations of KAF learning algorithms by at least a factor of 2. An implementation on a FPGA-based network card allows novelty detection of an 8× 24-bit input vector with latency of 404 ns, this being a 26-fold reduction compared to an Intel Core i5-2400 processor.","Field programmable gate arrays,
Principal component analysis,
Hardware,
Kernel,
Prediction algorithms,
Mathematical model,
Dictionaries"
Noniterative Deep Learning: Incorporating Restricted Boltzmann Machine Into Multilayer Random Weight Neural Networks,"A general deep learning (DL) mechanism for a multiple hidden layer feed-forward neural network contains two parts, i.e., 1) an unsupervised greedy layer-wise training and 2) a supervised fine-tuning which is usually an iterative process. Although this mechanism has been demonstrated in many fields to be able to significantly improve the generalization of neural network, there is no clear evidence to show which one of the two parts plays the essential role for the generalization improvement, resulting in an argument within the DL community. Focusing on this argument, this paper proposes a new DL approach to train multilayer feed-forward neural networks. This approach uses restricted Boltzmann machine (RBM) as the layer-wise training and uses the generalized inverse of a matrix as the supervised fine-tuning. Different from the general deep training mechanism like back-propagation (BP), the proposed approach does not need to iteratively tune the weights, and therefore, has many advantages such as quick training, better generalization, and high understandability, etc. Experimentally, the proposed approach demonstrates an excellent performance in comparison with BP-based DL and the traditional training method for multilayer random weight neural networks. To a great extent, this paper demonstrates that the supervised part plays a more important role than the unsupervised part in DL, which provides some new viewpoints for exploring the essence of DL.","Training,
Neural networks,
Feature extraction,
Nonhomogeneous media,
Probability distribution,
Cybernetics,
Machine learning"
An Integrated Machine Learning Approach for Extrinsic Plagiarism Detection,"Plagiarism detection is gaining increasing importance due to requirements for integrity in education. In this paper, we have developed a new integrated approach for extrinsic plagiarism detection. The proposed approach is based on four well-known models namely Bag of Words (BOW), Latent Semantic Analysis (LSA), Stylometry and Support Vector Machines (SVM). The proposed approach works by capturing usage patterns of the most common words (MCW) from books of 25 authors. Stylistic features for each author were harnessed in the method by adjusting the LSA weighting technique. The adjusted LSA method was trained in a novel manner using the leave-one-out-cross-validation technique and compared with the traditional LSA method. The results have shown that the enhanced weighting method of the adjusted LSA outperforms the traditional LSA method.","Plagiarism,
Semantics,
Feature extraction,
Tools,
Support vector machines,
Education,
Writing"
On the limits of machine learning-based test: A calibrated mixed-signal system case study,"Testing analog, mixed-signal and RF circuits represents the main cost component for testing complex SoCs. A promising solution to alleviate this cost is the machine learning-based test strategy. These test techniques are an indirect test approach that replaces costly specification measurements by simpler signatures. Machine learning algorithms are used to map these signatures to the performance parameters. Although this approach has a number of undoubtable advantages, it also opens new issues that have to be addressed before it can be widely adopted by the industry. In this paper we present a machine learning-based test for a complex mixed-signal system - i.e. a state-of-the-art pipeline ADC-that includes digital calibration. This paper shows how the introduction of digital calibration for the ADC has a serious impact in the proposed test as calibration completely decorrelates signatures from the target specification in the presence of local mismatch.","Calibration,
Machine learning algorithms,
Pipelines,
Performance evaluation,
Table lookup,
Production,
Training"
Adaptive Scheme for Caching YouTube Content in a Cellular Network: Machine Learning Approach,"Content caching at base stations is a promising solution to address the large demands for mobile data services over cellular networks. Content caching is a challenging problem as it requires predicting the future popularity of the content and the operating characteristics of the cellular networks. In this paper, we focus on constructing an algorithm that improves the users' quality of experience (QoE) and reduces network traffic. The algorithm accounts for users' behavior and properties of the cellular network (e.g. cache size, bandwidth, and load). The constructed content and network aware adaptive caching scheme uses an extreme-learning machine neural network to estimate the popularity of content, and mixed-integer linear programming to compute where to place the content and select the physical cache sizes in the network. The proposed caching scheme simultaneously performs efficient cache deployment and content caching. Additionally, a simultaneous perturbation stochastic approximation method is developed to reduce the number of neurons in the extreme-learning machine method while ensuring a sufficient predictive performance is maintained. Using real-world data from YouTube and a NS-3 simulator, we demonstrate how the caching scheme improves the QoE of users and network performance compared with industry standard caching schemes.","Cellular networks,
Adaptive systems,
Base stations,
YouTube,
Delays"
Machine learning enabled power-aware Network-on-Chip design,"Although Network-on-Chips (NoCs) are fast becoming pervasive as the interconnect fabric for multicore architectures and systems-on-chips, they still suffer from excessive static and dynamic power consumption. High dynamic power consumption results from switching and storing data within routers/links while excess static power is consumed when routers and links are not utilized for communication and yet have to be powered up. In this paper, we propose LESSON (Learning Enabled Sleepy Storage Links and Routers in NoCs) to reduce both static and dynamic power consumption by power-gating the links and routers at low network utilization and moving the data storage from within the routers to the links at high network utilization. As the network utilization increases from low-to-high, to accommodate more traffic, we design the same channels to flow traffic in either direction, thereby avoiding complex routing or look-ahead wake-up algorithms. Machine learning algorithms predict when to power-gate the channels and routers and when to increase the channel bandwidths such that power savings are maximized while performance penalty is minimized. Our results show that we can improve total network power consumption when compared to conventional NoC buffer designs by 85.6% and when compared with aggressive NoC buffer designs by 31.7%. Our predictor shows marginal performance penalties and by dynamically changing the direction of the links, we can improve packet latency by 14%.","Power demand,
Logic gates,
Transistors,
Machine learning algorithms,
Bandwidth,
Threshold voltage,
Switches"
Malware detection using machine learning based analysis of virtual memory access patterns,"Malicious software, referred to as malware, continues to grow in sophistication. Past proposals for malware detection have primarily focused on software-based detectors which are vulnerable to being compromised. Thus, recent work has proposed hardware-assisted malware detection. In this paper, we introduce a new framework for hardware-assisted malware detection based on monitoring and classifying memory access patterns using machine learning. This provides for increased automation and coverage through reducing user input on specific malware signatures. The key insight underlying our work is that malware must change control flow and/or data structures, which leaves fingerprints on program memory accesses. Building on this, we propose an online framework for detecting malware that uses machine learning to classify malicious behavior based on virtual memory access patterns. Novel aspects of the framework include techniques for collecting and summarizing per-function/system-call memory access patterns, and a two-level classification architecture. Our experimental evaluation focuses on two important classes of malware (i) kernel rootkits and (ii) memory corruption attacks on user programs. The framework has a detection rate of 99.0% with less than 5% false positives and outperforms previous proposals for hardware-assisted malware detection.","Malware,
Monitoring,
Histograms,
Training,
Kernel,
Load modeling"
Robust Road Marking Detection and Recognition Using Density-Based Grouping and Machine Learning Techniques,"This paper presents a robust approach for road marking detection and recognition from images captured by an embedded camera mounted on a car. Our method is designed to cope with illumination changes, shadows, and harsh meteorological conditions. Furthermore, the algorithm can effectively group complex multi-symbol shapes into an individual road marking. For this purpose, the proposed technique relies on MSER features to obtain candidate regions which are further merged using density-based clustering. Finally, these regions of interest are recognized using machine learning approaches. Worth noting, the algorithm is versatile since it does not utilize any prior information about lane position or road space. The proposed method compares favorably to other existing works through a large number of experiments on an extensive road marking dataset.","Roads,
Robustness,
Feature extraction,
Lighting,
Image edge detection,
Cameras,
Neural networks"
Location detection for navigation using IMUs with a map through coarse-grained machine learning,"Location detection or localization supporting navigation has assumed significant importance in the recent past. In particular, techniques that exploit cheap inertial measurement units (IMU), the gyroscope and the accelerometer, have garnered attention, especially in an embedded computing context. However, these sensors measurements are quite unreliable, and it is widely believed that these sensors by themselves are too noisy for localization with acceptable accuracy. Consequently, several lines of work embody other costly alternatives to lower the impact of accumulated errors associated with IMU based approaches, invariably leading to very high energy costs resulting in lowered battery life. In this paper, we show that IMUs are sufficient by themselves if we augment them with known structural or geographical information about the physical area being explored by the user. By using the map of the region being explored and the fact that humans typically walk in a structured manner, our approach sidesteps the challenges created by noise and concomitant accumulation of error. Specifically, we show that a simple coarse-grained machine learning approach mitigates the effect of the noisy perturbations in the information from our IMUs, provided we have accurate maps. Throughout, we rely on the principle of inexactness in an overarching manner and relax the need for absolute accuracy in return for significant lowering of resource (energy) costs. Notably, our approach is completely independent of any external guidance from sources including GPS, Bluetooth or WiFi support, and is this privacy preserving. Specifically, we show through experimental results that by relying on gyroscope and accelerometer data alone, we can correctly identify the path-segment where the user is walking/running on a known map, as well as the position within the path with an accuracy of 4.3 meters on the average using 0.44 Joules. This is a factor of 27X cheaper in energy lower than the “gold standard” that one could consider based on GPS support which, surprisingly, has an associated error of 8.7 meters on the average.","Estimation,
Global Positioning System,
Gyroscopes,
Sensors,
Noise measurement,
Accelerometers"
Machine learning for run-time energy optimisation in many-core systems,"In recent years, the focus of computing has moved away from performance-centric serial computation to energy-efficient parallel computation. This necessitates run-time optimisation techniques to address the dynamic resource requirements of different applications on many-core architectures. In this paper, we report on intelligent run-time algorithms which have been experimentally validated for managing energy and application performance in many-core embedded system. The algorithms are underpinned by a cross-layer system approach where the hardware, system software and application layers work together to optimise the energy-performance trade-off. Algorithm development is motivated by the biological process of how a human brain (acting as an agent) interacts with the external environment (system) changing their respective states over time. This leads to a pay-off for the action taken, and the agent eventually learns to take the optimal/best decisions in future. In particular, our online approach uses a model-free reinforcement learning algorithm that suitably selects the appropriate voltage-frequency scaling based on workload prediction to meet the applications' performance requirements and achieve energy savings of up to 16% in comparison to state-of-the-art-techniques, when tested on four ARM A15 cores of an ODROID-XU3 platform.","Hardware,
Prediction algorithms,
Minimization,
Software algorithms,
Embedded systems,
Optimization,
Heuristic algorithms"
The promise of machine learning in cybersecurity,"Over the last few years' machine learning has migrated from the laboratory to the forefront of operational systems. Amazon, Google and Facebook use machine learning every day to improve customer experiences, suggested purchases or connect people socially with new applications and facilitate personal connections. Machine learning's powerful capability is also there for cybersecurity. Cybersecurity is positioned to leverage machine learning to improve malware detection, triage events, recognize breaches and alert organizations to security issues. Machine learning can be used to identify advanced targeting and threats such as organization profiling, infrastructure vulnerabilities and potential interdependent vulnerabilities and exploits. Machine learning can significantly change the cybersecurity landscape. Malware by itself can represent as many as 3 million new samples an hour. Traditional malware detection and malware analysis is unable to pace with new attacks and variants. New attacks and sophisticated malware have been able to bypass network and end-point detection to deliver cyber-attacks at alarming rates. New techniques like machine learning must be leveraged to address the growing malware problem. This paper describes how machine learning can be used to detect and highlight advanced malware for cyber defense analysts. The results of our initial research and a discussion of future research to extend machine learning is presented.","Computer security,
Malware,
Organizations,
Artificial neural networks,
Personnel,
Training"
Network Traffic Classification techniques and comparative analysis using Machine Learning algorithms,"Network Traffic Classification is a central topic nowadays in the field of computer science. It is a very essential task for Internet service providers (ISPs) to know which types of network applications flow in a network. Network Traffic Classification is the first step to analyze and identify different types of applications flowing in a network. Through this technique, internet service providers or network operators can manage the overall performance of a network. There are many methods traditional technique to classify internet traffic like Port Based, Pay Load Based and Machine Learning Based technique. The most common technique used these days is Machine Learning (ML) technique. Which is used by many researchers and got very effective accuracy results. In this paper, we discuss network traffic classification techniques step by step and real time internet data set is develop using network traffic capture tool, after that feature extraction tool is use to extract features from the capture traffic and then four machine learning classifiers Support Vector Machine, C4.5 decision tree, Naïve Bays and Bayes Net classifiers are applied. Experimental analysis shows that C4.5 classifiers gives very good accuracy result as compare to other classifies.","Support vector machines,
Payloads,
MATLAB"
Prediction of the admission lines of college entrance examination based on machine learning,"Accurate prediction to college entrance examination(CEE) results is very important for the candidates to fill in the application and the relevant analysis of the CEE. At present, the prediction of CEE scores is based on data statistics, probability model and some weighted combination models. Since generating the model for predicting college admission lines uses too little reference factor, and the error is relatively large, so the reference value is very small. In this paper, machine learning methods are used to carry out the college admission lines of research and prediction. Specially, in this paper Adaboost algorithm is used to study and forecast, which belongs to ensemble learning. Finally, the result of this model is given, which is better than the current prediction method.","Data models,
Predictive models,
Indium phosphide,
III-V semiconductor materials,
Computational modeling"
Performance Evaluation of Machine Learning Based Signal Classification Using Statistical and Multiscale Entropy Features,"In this paper we study the performance of machine learning based signal classification approaches in realistic wireless environments. We focus in particular on impact of interference from modulated signals and influence of realistic wireless channel conditions on classification performance. For this we use both numerical simulations as well as software defined radio based implementations. We also propose to use additional time series statistics originating from complex systems research as features for the classifier, in addition to classical second and higher order statistics previously employed. Our results show that the extended feature set results in robust classification performance in wide variety of channel conditions, and also when significant modulated interference is present in addition to Gaussian noise.","Entropy,
Signal to noise ratio,
Time series analysis,
Support vector machines,
Frequency modulation,
Wireless communication"
A machine learning based application for predicting Global Horizontal Irradiance,The adoption of residential solar energy solutions has become a popular alternative for many families as the need for alternative energy resources begins to increase and this is causing a fluctuation in the estimated electrical energy needs forecasted by energy companies. This is a problem because electric companies base their production amounts on regional estimated energy needs and might easily overproduce or underproduce energy unless they can better estimate the availability of solar energy in each region and create a solution for monitoring its residential adoption across the same region. This study investigates the former by using ensemble machine learning algorithms to build a more accurate general predictive model which can be used predict the amount of usable solar radiation available at any given hour of the day and at any location. This method is a newer approach and is based on the reliability of a given combination of weather features. The established success metric for the study was to meet or exceed a .900 R2 value with the lowest RMSE possible. Four predictive models were created using data from the National Solar Radiation Database which represented various meteorological and solar measurements taken over a given period. Training data was created from one year of normalized observations taken every half hour; testing data was sampled from an additional year of data taken a decade later. Of the four methods studied a Cubist Model Tree performed best with a .959 R2 value and an RMSE of .0667. This new model was then incorporated into a GPS-based mobile application that could be used to predict the amount of usable solar radiation at any given location and at any given time of day. This application is part of a more comprehensive decision support system that addresses the overproduction of electrical energy through the adoption of new technological solutions and a proposed system for monitoring and/or controlling consumer energy consumption.,"Predictive models,
Training,
Data models,
Companies,
Solar energy,
Solar radiation,
Testing"
iCSI: A Cloud Garbage VM Collector for Addressing Inactive VMs with Machine Learning,"According to a recent study, 30% of VMs in private cloud data centers are ""comatose"", in part because there is generally no strong incentive for their human owners to delete them at an appropriate time. These inactive VMs are still scheduled and executed on physical cloud resources, taking valuable access away from productive VMs. In an extreme, cloud infrastructure may deny legitimate requests for new VMs because capacity limits have been hit. It is not sufficient for cloud infrastructure to identify such inactive VMs by monitoring resource utilization (e.g., CPU utilization) - e.g., management processes (e.g. virus-scan, software update) on inactive VMs often consume high CPU and memory resources, and active VMs with lightweight jobs (e.g. text editing) show almost zero resource utilization. To properly detect and address such inactive VMs, we present iCSI: a cloud garbage VM collector to improve resource utilization and cost efficiency of enterprise data centers. iCSI includes three main components, a lightweight data collector, a VM identification model and a recommendation engine. The data collector periodically gathers primitive information from VMs. The identification model infers the purpose of a VM from the data collection and extracts the most relevant features associated with the purpose. The recommendation engine offers proper actions to end users i.e., suspending or resizing VMs. In this prototype phase, iCSI is deployed into multiple data centers in IBM and manages more than 750 production VMs. iCSI achieves 20% better accuracy (90%) in identifying active/inactive VMs compared with state-of-the-art methods. With recommendations to end users, our estimation results show that iCSI can improve internal cost efficiency with 23% and resource utilization more than 45%.",
Comparative study of relevance vector machine with various machine learning techniques used for detecting breast cancer,"Now-a-days breast cancer has become one of the leading cause of cancer death among women. This cancer is caused mostly due to the lifestyle changes, avoiding breast feeding etc. Detecting breast cancer takes long time due to manual diagnosis. Even though there are many diagnostic systems are available still, it takes more time for proper classification. For detecting breast cancer, mostly machine learning techniques are used. This work deals with the comparative study of Relevance vector machine(RVM) which provides Low computational cost while comparing with other machine learning techniques which are used for breast cancer detection. The aim of this work is to compare and explain how RVM is better than other machine learning algorithms for diagnosing breast cancer even the variables are reduced.","cancer,
learning (artificial intelligence),
medical diagnostic computing"
Predicting network attack patterns in SDN using machine learning approach,"An experimental setup of 32 honeypots reported 17M login attempts originating from 112 different countries and over 6000 distinct source IP addresses. Due to decoupled control and data plane, Software Defined Networks (SDN) can handle these increasing number of attacks by blocking those network connections at the switch level. However, the challenge lies in defining the set of rules on the SDN controller to block malicious network connections. Historical network attack data can be used to automatically identify and block the malicious connections. There are a few existing open-source software tools to monitor and limit the number of login attempts per source IP address one-by-one. However, these solutions cannot efficiently act against a chain of attacks that comprises multiple IP addresses used by each attacker. In this paper, we propose using machine learning algorithms, trained on historical network attack data, to identify the potential malicious connections and potential attack destinations. We use four widely-known machine learning algorithms: C4.5, Bayesian Network (BayesNet), Decision Table (DT), and Naive-Bayes to predict the host that will be attacked based on the historical data. Experimental results show that average prediction accuracy of 91.68% is attained using Bayesian Networks.","Machine learning algorithms,
Security,
Bayes methods,
IP networks,
Data models,
Predictive models,
Prediction algorithms"
Machine learning based paraphrase identification system using lexical syntactic features,"During the natural language communication, meaning understanding is the complex task that humans learn from their childhood but to automate this process of meaning understanding for computers has great real world applications. Simple text processing tasks are not enough to uncover the meaning from given unstructured natural language text. Our current research focuses on the issues pertaining to the same. Paraphrase identification is such important task of identifying the meaning similarity between two text segments in natural language understanding system. Proposed a machine learning system uses lexical features and dependency based features for sentence level paraphrase identification. The performance of proposed system is evaluated by conducting experiment on standard Microsoft paraphrase corpus. Moreover, a comparative study of current system with other machine learning based systems on Microsoft paraphrase corpus for paraphrase identification is carried out. The proposed system achieves competitive results compare to other state-of-the art machine learning systems by using simple linguistic features. The system using SVM classifier achieves 81.41% f-score by using simple lexical features only. Voting based classifier scores 80.97% with lexical features. Results with dependency features are highly sensitive to minor syntactic change.","Syntactics,
Support vector machines,
Semantics,
Feature extraction,
Neural networks,
Natural languages,
Pragmatics"
Reintroducing KAPD as a Dataset for Machine Learning and Data Mining Applications,"KACST Arabic Phonetic Database (KAPD) has been in use by researchers for around fifteen years since its initial release. Researches in acoustics and phonetics have benefited from its phonetically rich content. In fact, KAPD has the potential to go further steps with the research community. In this work, KAPD is subject to enhancements and improvements in order to serve as dataset for machine learning and data mining application. This work involves refining and reviewing the already existing metadata of KAPD and adding new material that are necessary for machine learning and data mining applications. The updated phoneme statistics after the corpus upgrade are presented from different perspectives. Data format and time units are made compatible with those of HTK. The paper discusses the potential of KAPD to serve as either a balanced or an imbalanced dataset.",Europe
Data Analytics and Machine Learning for Design-Process-Yield Optimization in Electronic Design Automation and IC semiconductor manufacturing,"In response to the current challenges of end-of-Moore scaling, a systematic analysis of the data information flows in the Design-to-Manufacturing pipeline highlights opportunities for the introduction of (big) data analytics and machine learning solutions. In this paper we review the eco-system components and describe the fundamental data-flows in the IC Design-to-Manufacturing chain, highlighting both the well-established and functioning sub-systems, as well as the critical bottlenecks. A quantitative definition of physical design space coverage is proposed, as the unifying abstraction available for all components of the Design-to-Manufacturing flow, allowing for the construction of a computational framework where Data Analytics and Machine Learning methodologies and tools can be successfully applied. The juxtaposition of Design-Technology-Co-Optimization (DTCO) with the novel paradigm of DFM-as-Search and their necessary integration in the DFM computational toolkit, clearly exemplify how the all the advanced IC nodes (14, 10, 7 and 5nm) definitely require the adoption of a new class of correlation extraction algorithms for heterogeneous data sets.","Data analysis,
Manufacturing,
IP networks,
Layout,
Algorithm design and analysis"
Enhancing Security Attacks Analysis Using Regularized Machine Learning Techniques,"With the increasing threats of security attacks, Machine learning (ML) has become a popular technique to detect those attacks. However, most of the ML approaches are black-box methods and their inner-workings are difficult to understand by human beings. In the case of network security, understanding the dynamics behind the classification model is a crucial element towards creating safe and human-friendly systems. In this article, we investigate the most important features in identifying well-known security attacks by using Support Vector Machines (SVMs) and l1-regularized method with Least Absolute Shrinkage and Selection Operator (LASSO) for robust regression both to binary and multiclass attack classification. SVMs are one of the standards of ML classification techniques that give a reasonably good performance but with some drawbacks in terms of interpretability. On the other hand, LASSO is a regularized regression method often performing comparably well and it has extra compelling advantages of being very easily interpretable. LASSO provides coefficients that contribute how individual features affect the probability of specific security attack classes to occur. Hence, we finally use LASSO in particular for multiclass classification to help us better understand which actual features shared by attacks in a network are the most important ones. To perform our analysis, we use the recent NSL-KDD intrusion detection public dataset where the data are labeled into either anomalous (denial-of-service (DoS), remote-to-local (R2L), user-to-root (U2R) and probe attack classes) or normal. Empirical results of the analysis and computational performance comparison over the competing methods used are also presented and discussed. We believe that the methodology presented in this paper may strengthen a future research in network intrusion detection settings.","Feature extraction,
Computers,
Computer crime,
Intrusion detection,
Probes,
Communication networks"
A random forest based machine learning approach for mild steel defect diagnosis,"Industries today need to stay ahead in competition by servicing and satisfying customer's needs. Quality of the produced products to match as per customer demands is the key goal for a product manufacturing company. A product produced with variation in characteristics, than the anticipated are called as defect. In the mild steel coil manufacturing plants, large amount of data is generated with the help of many sensors deployed to measure different parameters which can be used for defect diagnosis of the coils produced. In case of mild steel coil, deviation of the final cooling temperature of the coil from desired temperature produces defective coils. The paper presents machine learning approach and the methodology for cooling temperature deviation defect diagnosis that consists of four phases namely data structuring, association identification, statistical derivation and classification. We also provide comparative results obtained with various data mining algorithms like decision trees, neural networks, SVM, ensemble techniques (boosting and random forest) in terms of performance parameters and prove that random forest outperforms rest of the techniques by achieving an accuracy of 95%.","Temperature measurement,
Steel,
Strips,
Neural networks,
Support vector machines,
Decision trees,
Vegetation"
Software and machine learning tools for monitoring railway track switch performance,"Trackside data logging hardware is often used in the UK, and increasingly elsewhere in the world, to record and transmit processed condition data from track switching equipment (points) in order to gauge asset health. This paper presents a novel implementation of three tools which can be used together to make the analysis and handling of this data easier. The first of these tools is a statistical classifier which automatically assigns labels to the process data. The classifier is trained using historical data containing examples of events of interest, such as recordings taken when maintenance activity or failures have developed. In practice, the labels are used to pre-filter the data, to bring swift attention to events of interest, and to automatically create categorised datasets which can be used to analyse historical performance. Two different types of classifier are presented: a Gaussian Naïve Bayes classifier and a neural network classifier. The second tool is a simple pattern recognition algorithm which can determine when the different phases of mechanical operation in a single track switch movement occur, for example locking, unlocking, and moving. The final tool is a statistical technique which is used to extract simple features from the data and raise alarms if they indicate poor track switch performance. The effectiveness of these tools is tested using real world data taken from three different railways.","Bayes methods,
belief networks,
condition monitoring,
Gaussian processes,
learning (artificial intelligence),
neural nets,
pattern classification,
railway engineering"
HazeEst: Machine Learning Based Metropolitan Air Pollution Estimation From Fixed and Mobile Sensors,"Metropolitan air pollution is a growing concern in both developing and developed countries. Fixed-station monitors, typically operated by governments, offer accurate but sparse data, and are increasingly being augmented by lower fidelity but denser measurements taken by mobile sensors carried by concerned citizens and researchers. In this paper, we introduce HazeEst-a machine learning model that combines sparse fixed-station data with dense mobile sensor data to estimate the air pollution surface for any given hour on any given day in Sydney. We assess our system using seven regression models and tenfold cross validation. The results show that estimation accuracy of support vector regression (SVR) is similar to decision tree regression and random forest regression, and higher than extreme gradient boosting, multi-layer perceptrons, linear regression, and adaptive boosting regression. The air pollution estimates from our models are validated via field trials, and results show that SVR not only yields high spatial resolution estimates that correspond well with the pollution surface obtained from fixed and mobile sensor monitoring systems, but also indicates boundaries of polluted area better than other regression models. Our results can be visualized using a Web-based application customized for metropolitan Sydney. We believe that the continuous estimates provided by our system can better inform air pollution exposure and its impact on human health.","Sensors,
Air pollution,
Monitoring,
Atmospheric modeling,
Mobile communication,
Data models,
Regression tree analysis"
Tensor Decomposition for Signal Processing and Machine Learning,"Tensors or multiway arrays are functions of three or more indices (i, j, k, . . . )-similar to matrices (two-way arrays), which are functions of two indices (r, c) for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth and depth that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.","Tensile stress,
Signal processing algorithms,
Matrix decomposition,
Signal processing,
Optimization,
Tutorials"
DEVS execution acceleration with machine learning,"Discrete Event System Specification DEVS separates modeling and simulation execution. Simulation execution is done within a runtime environment that is often called a DEVS simulator. This separation creates an opportunity to incorporate smart algorithms in the simulator to optimize simulation execution. In this paper, we propose incorporating some predictive machine learning algorithms into the DEVS simulator that can cut simulation execution times significantly for many simulation applications without compromising the simulation accuracy. In this paper, we introduce a specific learning mechanism that can be embedded into the DEVS simulator to incrementally build a predictive model that learns from past simulations. We further look into issues related to the predictive model selection, its prediction accuracy, its effect on the overall simulation performance, and when to switch between the predictive model and the simulation during an execution.","Computational modeling,
Semiconductor device modeling,
Numerical models,
Predictive models,
Runtime"
Prognostics of damage growth in composite materials using machine learning techniques,"Composite materials have been adopted and become critical in aerospace industry. However, due to the fatigue under continuous loading, the uncertain in structural integrity still remains an unsolved problem. The assessment of structural damage in composite laminates can be achieved by damage location, classification, and quantification. The growth trend of delamination area is one of the most important factors. In order to predict the delamination size efficiently and accurately, this paper proposes a prognostic method based on machine learning techniques. Prediction models, including linear model, support vector machines, and random forests were investigated. An optimal solution was identified by comparing the test results of different models. In this study, the length of the path across delamination area was selected as the objective value to train the models. The path length measurements augmented the training data sets and avoid the overfitting problem for the models. Moreover, the path length can be used to measure the size of delamination area. The interrogation frequency collected on several composite coupons was adopted as an input variable for the predict model. Experimental results demonstrate the effectiveness of the proposed method.","Delamination,
Training,
Predictive models,
Support vector machines,
Vegetation,
Aircraft,
Inspection"
An efficient machine learning approach for the detection of melanoma using dermoscopic images,"Diagnosis of dermoscopic skin lesions due to skin cancer is the most challenging task for the experienced dermatologists. In this context, dermoscopy is the non-invasive useful method for the detection of skin lesions which are not visible to naked human eye. Among different types of skin cancers, malignant melanoma is the most aggressive and deadliest form of skin cancer. Its diagnosis is crucial if not detected in early stage. This paper mainly aims to present an efficient machine learning approach for the detection of melanoma from dermoscopic images. It detects melanomic skin lesions based upon their discriminating properties. In first step of proposed method, different types of color and texture features are extracted from dermoscopic images based on distinguished structures and varying intensities of melanomic lesions. In second step, extracted features are fed to the classifier to classify melanoma out of dermoscopic images. Paper also focuses on the role of color and texture features in the context of detection of melanomas. Proposed method is tested on publicly available PH2 dataset in terms of accuracy, sensitivity, specificity and Area under ROC curve (AUC). It is observed that good results are achieved using extracted features, hence proving the validity of the proposed system.","Image color analysis,
Feature extraction,
Malignant tumors,
Skin,
Lesions,
Image segmentation,
Skin cancer"
Prediction of Stock Market performance by using machine learning techniques,"One decision in Stock Market can make huge impact on an investor's life. The stock market is a complex system and often covered in mystery, it is therefore, very difficult to analyze all the impacting factors before making a decision. In this research, we have tried to design a stock market prediction model which is based on different factors. The model was built to predict performance of KSE-100 index. The prediction model predicts market as positive or negative with the help of different attributes. These factors include price fluctuation of fuel, commodity, foreign exchange, interest rate, general public sentiment, related NEWS and Auto-Regressive Integrated Moving Average (ARIMA) and Simple Moving Average (SMA) predicted values with help of historical data of the market. The techniques used for prediction include four different versions of Artificial Neural Network (ANN) including Single Layer Perceptron (SLP), Multi-layer Perceptron (MLP), Radial Basis Function (RBF) and Deep Belief Network (DBN). Other techniques include Support Vector Machine (SVM), Decision Tree and Naïve Bayes. All these techniques were compared to find the best predicting model. The results showed that MLP performed best and predicted the market with accuracy of 77%. Each factor was studied independently to find out its association with market performance. The change in Petrol prices showed the strongest association with market performance. The results suggested that behavior of market can be predicted using machine learning techniques.","Stock markets,
Predictive models,
Artificial neural networks,
Support vector machines,
Complex systems,
Indexes,
Fluctuations"
Machine Learning for Detecting Pronominal Anaphora Ambiguity in NL Requirements,"Automated or semi-automated analysis of requirements specification documents, expressed in Natural Language (NL), has always been desirable. An important precursor to this goal is the identification and correction of potentially ambiguous requirements statements. Pronominal Anaphora ambiguity is one such type of pragmatic or referential ambiguity in NL requirements, which needs attention. However, identification of such ambiguous requirements statements is a challenging task since the count of such statements is relatively lower. We present a solution to this challenge by considering the task as that of a classification problem to classify ambiguous requirements statements having pronominal anaphora ambiguity from a corpus of potentially ambiguous requirements statements with pronominal anaphora ambiguity. We show how a classifier can be trained in semi-supervised manner to detect such instances of pronominal anaphoric ambiguous requirements statements. Our study indicates a recall of 95% with Bayesian network classification algorithm.",
Effort Estimation for Embedded Software Development Projects by Combining Machine Learning with Classification,"This paper discusses the effect of classification in estimating the amount of effort (in man-days) associated with code development. Estimating the effort requirements for new software projects is especially important. As outliers are harmful to the estimation, they are excluded from many estimation models. However, such outliers can be identified in practice once the projects are completed, and so they should not be excluded during the creation of models and when estimating the required effort. This paper presents classifications for embedded software development projects using an artificial neural network (ANN) and a support vector machine. After defining the classifications, effort estimation models are created for each class using linear regression, an ANN, and a form of support vector regression. Evaluation experiments are carried out to compare the estimation accuracy of the model both with and without the classifications using 10-fold cross-validation. In addition, the Games-Howell test with one-way analysis of variance is performed to consider statistically significant evidence.","Estimation,
Support vector machines,
Data models,
Embedded software,
Analytical models,
Analysis of variance"
Applications of Machine learning to document classification and clustering,"Recently, Machines learning techniques have had great successes in areas such as spam filtering, recommender systems, human speech recognition systems, handwriting recognition, news articles clustering, etc.","Semantics,
Recommender systems,
Speech recognition,
Handwriting recognition"
"Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection","To cope with the increasing variability and sophistication of modern attacks, machine learning has been widely adopted as a statistically-sound tool for malware detection. However, its security against well-crafted attacks has not only been recently questioned, but it has been shown that machine learning exhibits inherent vulnerabilities that can be exploited to evade detection at test time. In other words, machine learning itself can be the weakest link in a security system. In this paper, we rely upon a previously-proposed attack framework to categorize potential attack scenarios against learning-based malware detection tools, by modeling attackers with different skills and capabilities. We then define and implement a set of corresponding evasion attacks to thoroughly assess the security of Drebin, an Android malware detector. The main contribution of this work is the proposal of a simple and scalable secure-learning paradigm that mitigates the impact of evasion attacks, while only slightly worsening the detection rate in the absence of attack. We finally argue that our secure-learning approach can also be readily applied to other malware detection tasks.","Androids,
Humanoid robots,
Malware,
Security,
Feature extraction,
Tools,
Algorithm design and analysis"
Retinal vessel segmentation under pathological conditions using supervised machine learning,"In this paper we present an automated blood vessel segmentation system algorithm for the retinal images under pathological conditions like Diabetic Retinopathy (DR) using matched filters and supervised classification techniques. Matched filter has been extensively used in the enhancement and segmentation of the retinal blood vessels due to the cross sectional similarity of the vessels to the Gaussian profile. However in addition to the vessel edges the non vessel edges also gives a strong response to the matched filter leading to false detection. Based on the structural and spatial differences between the segmented vessels and the non vessels components, we propose a classification technique using machine learning approach to mask out the false detection due to non vessel structures. The proposed method shows an increased accuracy than the state of the art matched filter techniques especially in the case of vessel segmentation from pathologically affected retinal images.","Image segmentation,
Retina,
Databases,
Support vector machines,
Kernel,
Pathology,
Blood vessels"
Recognition of fruits using hybrid features and machine learning,"Recognition of fruits automatically using machine vision is considered as challenging task as fruits exist in various colors, sizes, shapes and textures. Additionally, when images are acquired of them, variation is introduced due to imaging conditions also. In this paper we have recognized nine different classes of fruits. Fruit image dataset are obtained from web as well as certain images are acquired by using mobile phone camera. These images are pre-processed to subtract the background and extract the blob representing fruit. For representing fruits and capturing their visual characteristics, combination of color, shape and texture features are used. These feature dataset is further passed to two different classifiers; multiclass SVM and KNN. The experimental results obtained are used to draw various conclusions. The best accuracy obtained by us in the study is 91.3% with KNN (K=2), classifier whereas with multiclass SVM (one-versus-all), the best accuracy obtained is 86.96%.","Image color analysis,
Shape,
Feature extraction,
Support vector machines,
Training,
Coherence,
Histograms"
Application of machine learning techniques to sentiment analysis,"Today, we live in a `data age'. Due to rapid increase in the amount of user-generated data on social media platforms like Twitter, several opportunities and new open doors have been prompted for organizations that endeavour hard to keep a track on customer reviews and opinions about their products. Twitter is a huge fast emergent micro-blogging social networking platform for users to express their views about politics, products sports etc. These views are useful for businesses, government and individuals. Hence, tweets can be used as a valuable source for mining public's opinion. Sentiment analysis is a process of automatically identifying whether a user-generated text expresses positive, negative or neutral opinion about an entity (i.e. product, people, topic, event etc). The objective of this paper is to give step-by-step detail about the process of sentiment analysis on twitter data using machine learning. This paper also provides details of proposed approach for sentiment analysis. This work proposes a Text analysis framework for twitter data using Apache spark and hence is more flexible, fast and scalable. Naïve Bayes and Decision trees machine learning algorithms are used for sentiment analysis in the proposed framework.","Sentiment analysis,
Feature extraction,
Twitter,
Machine learning algorithms,
Training,
Data mining"
Parallelization of Machine Learning Applied to Call Graphs of Binaries for Malware Detection,"Malicious applications have become increasingly numerous. This demands adaptive, learning-based techniques for constructing malware detection engines, instead of the traditional manual-based strategies. Prior work in learning-based malware detection engines primarily focuses on dynamic trace analysis and byte-level n-grams. Our approach in this paper differs in that we use compiler intermediate representations, i.e., the callgraph representation of binaries. Using graph-based program representations for learning provides structure of the program, which can be used to learn more advanced patterns. We use the Shortest Path Graph Kernel (SPGK) to identify similarities between call graphs extracted from binaries. The output similarity matrix is fed into a Support Vector Machine (SVM) algorithm to construct highly-accurate models to predict whether a binary is malicious or not. However, SPGK is computationally expensive due to the size of the input graphs. Therefore, we evaluate different parallelization methods for CPUs and GPUs to speed up this kernel, allowing us to continuously construct up-to-date models in a timely manner. Our hybrid implementation, which leverages both CPU and GPU, yields the best performance, achieving up to a 14.2x improvement over our already optimized OpenMP version. We compared our generated graph-based models to previously state-of-the-art feature vector 2-gram and 3-gram models on a dataset consisting of over 22,000 binaries. We show that our classification accuracy using graphs is over 19% higher than either n-gram model and gives a false positive rate (FPR) of less than 0.1%. We are also able to consider large call graphs and dataset sizes because of the reduced execution time of our parallelized SPGK implementation.","Malware,
Kernel,
Feature extraction,
Support vector machines,
Machine learning algorithms,
Engines,
Graphics processing units"
Machine learning for embedded devices software analysis via hardware platform emulation,"Nowadays commercial-off-the-shelf (COTS) embedded devices are widely used in many security-critical systems like nuclear stations and traffic control systems. Most of these devices has proprietary hardware and software (frequently called firmware) with little documentation available. Another common feature is the use of “binary blob” firmware, where hardware specific and high level layers can't be easily separated and, therefore are forced to be analyzed together. All these facts make firmware analysis quite a challenging task. In this paper, we'll suggest an approach for the firmware analysis with poorly documented hardware platform emulation. The advantages of this approach are almost full control under firmware state, achieving easy to scale fuzzing and manual bug hunting facilitation. For the purpose of successful realization, an identification of communication between hardware components (e.g. communication between main SoC and Bluetooth SoC) should be done. To address this issue, we suggest the use of machine learning, which, because of its nature, enables construction of algorithms that can learn from and make predictions on data.","Microprogramming,
Hardware,
Emulation,
Information security,
Embedded systems,
Analytical models"
Machine learning models for material selection: Framework for predicting flatwise compressive strength using ANN,The Polyurethane foam cores (PUR) are synthesized by combining reactive chemicals namely Isocyanates and Polyols in appropriate proportions. The paper presents the physical and ANN approach of forming the PUR foam cores with change in chemical compositions. Five varied proportions with five densities of foams were synthesized and tested. The paper reports the physical properties in comparison with flatwise compression strength. It is observed that the physical test data is in absolute compatibility with the ANN results. The regression values are in good agreement with the 50:50 chemical compositions and higher density foams are observed to be brittle and lower density foams were flexible and elastic.,"Artificial neural networks,
Chemicals,
Mechanical factors,
Sandwich structures,
Predictive models,
Metals"
Classification and development of tool for heart diseases (MRI images) using machine learning,Heart diseases are one of the major killers worldwide. Early detection of heart disease such as Global Hypokinesia can reduce this global burden. Computational method has potential to predict disease in early stages automatically and especially helpful in resources limited countries. Computational method to predict global hypokinesia based on confirms cases of global hypokinesia through MRI was developed. Almost all feature extraction method was used on MRI images and model was generated on merged and different images separately. High accuracy of model independent test set justified our approaches and reliability of model. The newly developed was implemented in python and available for open use.,"Feature extraction,
Standards,
Diseases,
Magnetic resonance imaging,
Decision support systems,
Data models,
Heart"
Machine Learning Paradigms for Next-Generation Wireless Networks,"Next-generation wireless networks are expected to support extremely high data rates and radically new applications, which require a new wireless radio technology paradigm. The challenge is that of assisting the radio in intelligent adaptive learning and decision making, so that the diverse requirements of next-generation wireless networks can be satisfied. Machine learning is one of the most promising artificial intelligence tools, conceived to support smart radio terminals. Future smart 5G mobile terminals are expected to autonomously access the most meritorious spectral bands with the aid of sophisticated spectral efficiency learning and inference, in order to control the transmission power, while relying on energy efficiency learning/inference and simultaneously adjusting the transmission protocols with the aid of quality of service learning/inference. Hence we briefly review the rudimentary concepts of machine learning and propose their employment in the compelling applications of 5G networks, including cognitive radios, massive MIMOs, femto/small cells, heterogeneous networks, smart grid, energy harvesting, device-todevice communications, and so on. Our goal is to assist the readers in refining the motivation, problem formulation, and methodology of powerful machine learning algorithms in the context of future networks in order to tap into hitherto unexplored applications and services.","Hidden Markov models,
Machine learning algorithms,
Bayes methods,
5G mobile communication,
MIMO,
Clustering algorithms,
Support vector machines"
Indexing facial attractiveness and well beings using machine learning,"A challenge is indexing the facial beauty by a machine as same evaluated by human beings. A question arises: Can beauty be learnt by machines? Every individual have different concept of facial beauty. Somebody can be attracted by someone but might not be by another person. In recent past, many psychologists, neurologists and other scientists have done tremendous work in this area. This work presents a study on the facial attractiveness in a machine learning context. Various techniques applied on SCUT-FBP facial images dataset for learning the facial attractiveness. From the results, we showed that facial beauty is a universal concept that a machine can learn. A model is designed that learns from the facial images with their attractiveness ratings and produced human like evaluation of attractiveness ratings. We extracted features from images and normalized all the features value. After that we proposed some techniques of machine learning like Support Vector Machine (SVM), k-Nearest Neighbor (KNN), Decision tree and Artificial Neural Network (ANN). An accuracy of 80% was obtained using KNN and 86% was obtained using ANN for multiclass classification. Accuracies of 61% and 62% were reported by SVM using linear kernel and RBF kernel respectively. These accuracies were obtained for multiclass classification. We also evaluated accuracy for binary class to reduce the effect of non-uniformity of data.","Support vector machines,
Image color analysis,
Skin,
Artificial neural networks,
Classification algorithms,
Decision trees,
Kernel"
Dataset Coverage for Testing Machine Learning Computer Programs,"Machine learning programs are non-testable, and thus testing with pseudo oracles is recommended. Although metamorphic testing is effective for testing with pseudo oracles, identifying metamorphic properties has been mostly ad hoc. This paper proposes a systematic method to derive a set of metamorphic properties for machine learning classifiers, support vector machines. The proposal includes a new notion of test coverage for the machine learning programs; this test coverage provides a clear guideline for conducting a series of metamorphic testing.","Software testing,
Support vector machines,
Computers,
Training,
Systematics,
Guidelines"
Interactive intelligent agents with creative minds: Experiments with mobile robots in cooperating tasks by using machine learning,"In this paper, we present an intelligent system where agents can co-ordinate creative tasks through machine learning and cooperation. For machine learning, we used commonly used pattern recognition algorithm - Principal Component Analysis (PCA). Based on recognition, we plan a task that is performed by multiple intelligent agents. In our case, task is to draw a pattern or perform a creative art by agents. The task action is divided into three phases: obtaining a design, composing a mathematical model and and performing the task by agents. In case of agents co-ordination, various feedback techniques using wireless sensors and on-board sensors are used. As for proof of concept (POC), a flower pattern is detected, which is painted on a canvas by using mobile robots. Also, person's identity and mood is detected and then a creative art is performed by mobile robots to improve the mood.",
Anomaly-based NIDS: A review of machine learning methods on malware detection,"The increasing amount of network traffic threat may originates from various sources, that can led to a higher probability for an organization to be exposed to intruder. Security mechanism such as Intrusion Detection System (IDS) is significant to alleviate such issue. Despite the ability of IDS to detect, some of the anomaly traffic may not be effectively detected. As such, it is vital the IDS algorithm to be reliable and can provide high detection accuracy, reducing as much as possible threats from the network. Nonetheless, every security mechanism has its weaknesses that can be exploited by intruders. Many research works exists, that attempts to address the issue using various methods. This paper discusses a hybrid approach to network IDS, which can minimize the malicious traffic in the network by using machine learning. The paper also provides a review of the available methods to further improve Anomaly-based Network Intrusion Detection System.","Sociology,
Statistics,
Monitoring,
Nails,
Filtering algorithms,
Databases,
Classification algorithms"
On-Device Mobile Phone Security Exploits Machine Learning,"The authors present a novel approach to protecting mobile devices from malware that might leak private information or exploit vulnerabilities. The approach, which can also keep devices from connecting to malicious access points, uses learning techniques to statically analyze apps, analyze the behavior of apps at runtime, and monitor the way devices associate with Wi-Fi access points.","Malware,
Mobile handsets,
Runtime,
Feature extraction,
Computer security,
Computer hacking,
Monitoring"
Augmented ontology by handshaking with machine learning,"Artificial intelligence products are already around us and will be emerging dramatically a lot in near future. Artificial intelligence is all about data analysis. When it comes to data analysis, there are two representative techniques: machine learning and semantic technology. They stand on the other side from where to begin analysis. Simply speaking, machine learning is based on the data while semantic technology relies on human domain knowledge (human learning). What if collected data are insufficient to reflect whole phenomenon? This is a limitation of machine learning. What if circumstance changes a lot as time goes by? Manual rule updating by experts is not a good solution in that circumstance. Based on these observations, we investigate two approaches and find a good solution which maximizes the advantages of both techniques and mitigates the limitations of them. This paper suggests a novel integration idea to compensate each technology with the other: that is semantic filtering. This paper includes a toy semantic modelling and a machine learning algorithm implementation to realize the proposed concept, semantic filtering.","data analysis,
learning (artificial intelligence),
ontologies (artificial intelligence),
semantic networks"
Engineering safety in machine learning,"Machine learning algorithms are increasingly influencing our decisions and interacting with us in all parts of our daily lives. Therefore, just like for power plants, highways, and myriad other engineered sociotechnical systems, we must consider the safety of systems involving machine learning. In this paper, we first discuss the definition of safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. Then we examine dimensions, such as the choice of cost function and the appropriateness of minimizing the empirical average training cost, along which certain real-world applications may not be completely amenable to the foundational principle of modern statistical machine learning: empirical risk minimization. In particular, we note an emerging dichotomy of applications: ones in which safety is important and risk minimization is not the complete story (we name these Type A applications), and ones in which safety is not so critical and risk minimization is sufficient (we name these Type B applications). Finally, we discuss how four different strategies for achieving safety in engineering (inherently safe design, safety reserves, safe fail, and procedural safeguards) can be mapped to the machine learning context through interpretability and causality of predictive models, objectives beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software.","Safety,
Uncertainty,
Risk management,
Training,
Learning systems,
Machine learning algorithms,
Sociotechnical systems"
Predictive maintenance applications for machine learning,"Machine Learning provides a complementary approach to maintenance planning by analyzing significant data sets of individual machine performance and environment variables, identifying failure signatures and profiles, and providing an actionable prediction of failure for individual parts.",
LASSO: A feature selection technique in predictive modeling for machine learning,"Feature selection is one of the techniques in machine learning for selecting a subset of relevant features namely variables for the construction of models. The feature selection technique aims at removing the redundant or irrelevant features or features which are strongly correlated in the data without much loss of information. It is broadly used for making the model much easier to interpret and increase generalization by reducing the variance. Regression analysis plays a vital role in statistical modeling and in turn for performing machine learning tasks. The traditional procedures such as Ordinary Least Squares (OLS) regression, Stepwise regression and partial least squares regression are very sensitive to random errors. Many alternatives have been established in the literature during the past few decades such as Ridge regression and LASSO and its variants. This paper explores the features of the popular regression methods, OLS regression, ridge regression and the LASSO regression. The performance of these procedures has been studied in terms of model fitting and prediction accuracy using real data and simulated environment with the help of R package.","Computational modeling,
Predictive models,
Regression analysis,
Input variables,
Data models,
Conferences,
Computer applications"
A Review on Sarcasm Detection from Machine-Learning Perspective,"In this paper, we want to review one of the challenging problems for the opinion mining task, which is sarcasm detection. To be able to do that, many researchers tried to explore such properties in sarcasm like theories of sarcasm, syntactical properties, psycholinguistic of sarcasm, lexical feature, semantic properties, etc. Studies done in the last 15 years not only made progress in semantic features, but also show increasing amount of method of analysis using a machine-learning approach to process data. Because of this reason, this paper will try to explain current mostly used method to detect sarcasm. Lastly, we will present a result of our finding, which might help other researchers to gain a better result in the future.","Twitter,
Semantics,
Supervised learning,
Support vector machines,
Context,
Classification algorithms,
Logistics"
Predicting adherence of patients with HF through machine learning techniques,"Heart failure (HF) is a chronic disease characterised by poor quality of life, recurrent hospitalisation and high mortality. Adherence of patient to treatment suggested by the experts has been proven a significant deterrent of the above-mentioned serious consequences. However, the non-adherence rates are significantly high; a fact that highlights the importance of predicting the adherence of the patient and enabling experts to adjust accordingly patient monitoring and management. The aim of this work is to predict the adherence of patients with HF, through the application of machine learning techniques. Specifically, it aims to classify a patient not only as medication adherent or not, but also as adherent or not in terms of medication, nutrition and physical activity (global adherent). Two classification problems are addressed: (i) if the patient is global adherent or not and (ii) if the patient is medication adherent or not. About 11 classification algorithms are employed and combined with feature selection and resampling techniques. The classifiers are evaluated on a dataset of 90 patients. The patients are characterised as medication and global adherent, based on clinician estimation. The highest detection accuracy is 82 and 91% for the first and the second classification problem, respectively.","patient treatment,
patient monitoring,
learning (artificial intelligence),
cardiology,
diseases"
Future black board using Internet of Things with cognitive computing: Machine learning aspects,"In an era of Digital computing, where people are more concerned in developing products to get there job done soon and also benefit them in all parameters, came Internet Of things to Solve. After Embedded Systems, there was a void left to be filled to solve automation. By the introduction of Internet, everyone started to visualize the things in a different way, Thus the Arrival of Internet of Things and now It is said to be the future for next few years. On the other side it's Cognitive Computing that's making the Big data to the next level, which is a branch Of Artificial Intelligence. In a similar way, The notes given by the teacher each and every day are taken by the students, but there is no forum yet to save those so to use them again, with help of Internet Of Things this could be possible where Sensors talk to the Chalk piece and gets the data to save. By analyzing (Pattern Recognition) the data obtained we can make a mini augmented reality system, where students can ask board the questions so the board answers, at end of the day it's the kids who will be benefited through this. Henceforth we have come to change the way you look at the Blackboard!!!","Internet of Things,
Radiofrequency identification,
Artificial intelligence,
Intelligent sensors,
Hardware"
In-Memory Computation of a Machine-Learning Classifier in a Standard 6T SRAM Array,"This paper presents a machine-learning classifier where computations are performed in a standard 6T SRAM array, which stores the machine-learning model. Peripheral circuits implement mixed-signal weak classifiers via columns of the SRAM, and a training algorithm enables a strong classifier through boosting and also overcomes circuit nonidealities, by combining multiple columns. A prototype 128 × 128 SRAM array, implemented in a 130-nm CMOS process, demonstrates ten-way classification of MNIST images (using image-pixel features downsampled from 28 × 28 = 784 to 9 × 9 = 81, which yields a baseline accuracy of 90%). In SRAM mode (bit-cell read/write), the prototype operates up to 300 MHz, and in classify mode, it operates at 50 MHz, generating a classification every cycle. With accuracy equivalent to a discrete SRAM/digital-MAC system, the system achieves ten-way classification at an energy of 630 pJ per decision, 113× lower than a discrete system with standard training algorithm and 13× lower than a discrete system with the proposed training algorithm.","Training,
Random access memory,
Standards,
Computer architecture,
Boosting,
Computational modeling,
Machine learning algorithms"
Feasibility of Supervised Machine Learning for Cloud Security,"Cloud computing is gaining significant attention, however, security is the biggest hurdle in its wide acceptance. Users of cloud services are under constant fear of data loss, security threats and availability issues. Recently, learning-based methods for security applications are gaining popularity in the literature with the advents in machine learning techniques. However, the major challenge in these methods is obtaining real-time and unbiased datasets. Many datasets are internal and cannot be shared due to privacy issues or may lack certain statistical characteristics. As a result of this, researchers prefer to generate datasets for training and testing purpose in the simulated or closed experimental environments which may lack comprehensiveness. Machine learning models trained with such a single dataset generally result in a semantic gap between results and their application. There is a dearth of research work which demonstrates the effectiveness of these models across multiple datasets obtained in different environments. We argue that it is necessary to test the robustness of the machine learning models, especially in diversified operating conditions, which are prevalent in cloud scenarios. In this work, we use the UNSW dataset to train the supervised machine learning models. We then test these models with ISOT dataset. We present our results and argue that more research in the field of machine learning is still required for its applicability to the cloud security.","Security,
Cloud computing,
Machine learning algorithms,
Training,
Testing,
Support vector machines,
Robustness"
"Big Universe, Big Data: Machine Learning and Image Analysis for Astronomy","Astrophysics and cosmology are rich with data. The advent of wide-area digital cameras on large aperture telescopes has led to ever more ambitious surveys of the sky. Data volumes of entire surveys a decade ago can now be acquired in a single night, and real-time analysis is often desired. Thus, modern astronomy requires big data know-how, in particular, highly efficient machine learning and image analysis algorithms. But scalability isn't the only challenge: astronomy applications touch several current machine learning research questions, such as learning from biased data and dealing with label and measurement noise. The authors argue that this makes astronomy a great domain for computer science research, as it pushes the boundaries of data analysis. They focus here on exemplary results, discuss main challenges, and highlight some recent methodological advancements in machine learning and image analysis triggered by astronomical applications.","Extraterrestrial measurements,
Telescopes,
Image analysis,
Extrasolar planets,
Big data,
Astronomy,
Computer vision,
Machine learning"
A methodological framework using statistical tests for comparing machine learning based models applied to fault diagnosis in rotating machinery,"Selecting an adequate machine learning model, e.g. for feature selection or classification, is a very important task in developing machine learning applications. In order to perform an adequate selection, statistic tests are introduced by several approaches but some of them are hard to reproduce in different case studies due to the lack of a systematic application procedure. This work presents a methodological framework based on statistic tests, either parametric or non-parametric, to compare multiple machine learning models for solving a specific problem. The procedure first aims to detect which feature selection method is the best for each machine learning based model, and then such models are compared using the previous results. A real world problem for fault detection in rotating machinery is studied to illustrate the application of the proposed methodological framework, using the accuracy in classification as the performance measure.","Unified modeling language,
Feature extraction,
Machine learning algorithms,
Algorithm design and analysis,
Pattern recognition,
Gears,
Electronic mail"
Machine learning and systems for the next frontier in formal verification,"Summary form only given. This tutorial covers basics of machine learning, systems and infrastructure considerations for performing machine learning at scale, and applications of machine learning to improve formal verification performance and usability. It starts with blackbox classifier training with gradient descent, and proceeds on to deep network training and simple convolutional neural networks. Next, it discusses how machine learning can be performed at scale, overcoming the performance and throughput limitations of traditional compute and storage systems. Finally, the tutorial describes several ways in which machine learning can be applied for improving formal tools performance and enhancing debug capabilities.","Tutorials,
Training,
Usability,
Neural networks,
Throughput"
Integrating machine learning in embedded sensor systems for Internet-of-Things applications,"Interpreting sensor data in Internet-of-Things applications is a challenging problem particularly in embedded systems. We consider sensor data analytics where machine learning algorithms can be fully implemented on an embedded processor/sensor board. We develop an efficient real-time realization of a Gaussian mixture model (GMM) for execution on the NXP FRDM-K64F embedded sensor board. We demonstrate the design of a customized program and data structure that generates real-time sensor features, and we show details and training/classification results for select IoT applications. The integrated hardware/software system enables real-time data analytics and continuous training and re-training of the machine learning (ML) algorithm. The real-time ML platform can accommodate several applications with lower sensor data traffic.",
Improving Recognition of Antimicrobial Peptides and Target Selectivity through Machine Learning and Genetic Programming,"Growing bacterial resistance to antibiotics is spurring research on utilizing naturally-occurring antimicrobial peptides (AMPs) as templates for novel drug design. While experimentalists mainly focus on systematic point mutations to measure the effect on antibacterial activity, the computational community seeks to understand what determines such activity in a machine learning setting. The latter seeks to identify the biological signals or features that govern activity. In this paper, we advance research in this direction through a novel method that constructs and selects complex sequence-based features which capture information about distal patterns within a peptide. Comparative analysis with state-of-the-art methods in AMP recognition reveals our method is not only among the top performers, but it also provides transparent summarizations of antibacterial activity at the sequence level. Moreover, this paper demonstrates for the first time the capability not only to recognize that a peptide is an AMP or not but also to predict its target selectivity based on models of activity against only Gram-positive, only Gram-negative, or both types of bacteria. The work described in this paper is a step forward in computational research seeking to facilitate AMP design or modification in the wet laboratory.","Peptides,
Microorganisms,
Support vector machines,
Training,
Standards,
IEEE transactions,
Computational biology"
EEG graph analysis for identification of ex-combatants: A machine learning approach,"Emotional processing of ex-combatants is affected by chronic exposure to violent events. For a successful reintegration into society, it is necessary to discriminate their brain responses from civilian people, as a first stage to develop treatment strategies. This paper presents a comparative analysis between a Multilayer Perceptron Neural Network and a Fuzzy C-Means classifier to differentiate ex-combatant subjects from civilian controls using graph-based functional connectivity networks calculated from scalp EEG recordings. EEG data were acquired while participants performed an adaptation of a Dual Valence Task, a well-known experimental task commonly used to analyze emotional processing. We obtained up to 80%of accuracy with both proposed algorithms. These results are comparable with other approaches reported in the literature. Together with conductual tasks, the computarized techniques proposed in this paper provide a decision support system to psychologists for determining the level of exposure to the conflict of the ex-combatants, and consequently a tool for their posterior treatment.","Standards,
Electroencephalography,
Feature extraction,
Neural networks,
Electrodes,
Scalp,
Correlation"
Error Tolerance Analysis of Deep Learning Hardware Using a Restricted Boltzmann Machine Toward Low-Power Memory Implementation,"Remarkable hardware robustness of deep learning (DL) is revealed by error injection analyses performed using a custom hardware model implementing parallelized restricted Boltzmann machines (RBMs). RBMs in deep belief networks demonstrate robustness against memory errors during and after learning. Fine-tuning significantly affects the recovery of accuracy for static errors injected to the structural data of RBMs. The memory error tolerance is observable using our hardware networks with fine-graded memory distribution, resulting in reliable DL hardware with low-voltage driven memory suitable to low-power applications.","Circuit faults,
Hardware,
Field programmable gate arrays,
Computer architecture,
Machine learning,
Robustness,
Data models"
Machine learning for finding bugs: An initial report,"Static program analysis is a technique to analyse code without executing it, and can be used to find bugs in source code. Many open source and commercial tools have been developed in this space over the past 20 years. Scalability and precision are of importance for the deployment of static code analysis tools - numerous false positives and slow runtime both make the tool hard to be used by development, where integration into a nightly build is the standard goal. This requires one to identify a suitable abstraction for the static analysis which is typically a manual process and can be expensive. In this paper we report our findings on using machine learning techniques to detect defects in C programs. We use three offthe- shelf machine learning techniques and use a large corpus of programs available for use in both the training and evaluation of the results. We compare the results produced by the machine learning technique against the Parfait static program analysis tool used internally at Oracle by thousands of developers. While on the surface the initial results were encouraging, further investigation suggests that the machine learning techniques we used are not suitable replacements for static program analysis tools due to low precision of the results. This could be due to a variety of reasons including not using domain knowledge such as the semantics of the programming language and lack of suitable data used in the training process.","Computer bugs,
Training,
Benchmark testing,
Complexity theory,
Feature extraction,
Data models"
Machine learning algorithms for process analytical technology,"Increased globalisation and competition are drivers for process analytical technologies (PAT) that enable seamless process control, greater flexibility and cost efficiency in the process industries. The paper will discuss process modelling and control for industrial applications with an emphasis on solutions enabling the real-time data analytics of sensor measurements that PAT demands. This research aims to introduce an integrated process control approach, embedding novel sensors for monitoring in real time the critical control parameters of key processes in the minerals, ceramics, non-ferrous metals, and chemical process industries. The paper presents a comparison of machine learning algorithms applied to sensor data collected for a polymerisation process. Several machine learning algorithms including Adaptive Neuro-Fuzzy Inference Systems, Neural Networks and Genetic Algorithms were implemented using MATLAB® Software and compared in terms of accuracy (MSE) and robustness in modelling process progression. The results obtained show that machine learning-based approaches produce significantly more accurate and robust process models compared to models developed manually while also being more adaptable to new data. The paper presents perspectives on the potential benefits of machine learning algorithms with a view to their future in the industrial process industry.","Process control,
Neural networks,
Image color analysis,
Genetic algorithms,
Adaptation models,
Machine learning algorithms,
Industries"
Using machine learning to design a flexible LOC counter,"The results of counting the size of programs in terms of Lines-of-Code (LOC) depends on the rules used for counting (i.e. definition of which lines should be counted). In the majority of the measurement tools, the rules are statically coded in the tool and the users of the measurement tools do not know which lines were counted and which were not. The goal of our research is to investigate how to use machine learning to teach a measurement tool which lines should be counted and which should not. Our interest is to identify which parameters of the learning algorithm can be used to classify lines to be counted. Our research is based on the design science research methodology where we construct a measurement tool based on machine learning and evaluate it based on open source programs. As a training set, we use industry professionals to classify which lines should be counted. The results show that classifying the lines as to be counted or not has an average accuracy varying between 0.90 and 0.99 measured as Matthew's Correlation Coefficient and between 95% and nearly 100% measured as the percentage of correctly classified lines. Based on the results we conclude that using machine learning algorithms as the core of modern measurement instruments has a large potential and should be explored further.","Machine learning algorithms,
Radiation detectors,
Instruments,
Prediction algorithms,
Computer languages,
Design methodology,
Training"
Using Azure Machine Learning for Estimating Indoor Locations,"Indoor systems cannot obtain a precise estimate of the location, due to unstable signals. In this paper, we use realistic wireless data from the IEEE International Conference on Data Mining (ICDM) dataset and Azure Machine Learning Studio to perform Bagging (also called bootstrap aggregating). By using the machine leaning technique in the Azure Machine Learning Studio, we can obtain more than 69 percent precision in identifying the correct area among 247 areas with only 505 training data. This result is equivalent to the second place entry in the IEEE ICDM Data Mining Contest. We show that this can achieve a highly accurate location estimation.","Decision trees,
Data mining,
Training data,
Bagging,
Estimation,
Wireless communication,
Conferences"
Named Entity Recognition using Machine learning techniques for Telugu language,"In this paper, we depict hybrid approach, i.e., combination of rule based approach and machine learning techniques, i.e Conditional Random Fields (CRF) for Named Entity Recognition (NER). The main objective of Named Entity Recognition is to categorize all Named Entities (NE) in a document into predefined classes like Person name, Location name, Organization name. This paper first outlines the Named Entity Recognizer using rule based approach. In this approach we prepared Gazette lists for names of persons, locations and organizations, some suffix and prefix features and dictionary consist of 200000 words to recognize the category of names entities. Further, we used Machine learning technique, i.e., CRF in order to improve the accuracy of the system.","Context,
Algorithm design and analysis"
Extracting Conceptual Interoperability Constraints from API Documentation Using Machine Learning,"Successfully using a software web-service/platform API requires satisfying its conceptual interoperability constraints that are stated within its shared documentation. However, manual and unguided analysis of text in API documents is a tedious and time consuming task. In this work, we present our empirical-based methodology of using machine learning techniques for automatically identifying conceptual interoperability constraints from natural language text. We also show some initial promising results of our research.","Documentation,
Support vector machines,
Interoperability,
Niobium,
Natural languages,
Software engineering,
Software"
Machine Learning Algorithms for Natural Language Semantics and Cognitive Computing,"We present elegant machine learning algorithms to efficiently learn natural language semantics (MLANLP), thus enabling much better Natural Language Computing (NLC) and Cognitive Computing (CC). Our algorithms use human brain-like learning approach and achieve very good generalization on natural language (mainly text) data. Existing machine learning algorithms performs well on numerical data and cannot easily learn semantics of natural language. Such algorithms, however, can address well some specific problems of natural language, like Name Entity Recognition where data can be easily represented by numbers and semantics between words (name and entity) are simple. Besides, the generalization capabilities of existing machine learning algorithms are limited, especially for complex data. The generalization capability for learning semantics of natural language should be very good to ensure reliable NLC and CC. Our MLANLP has good generalization capability, and can also derive new semantics and knowledge, very much needed for NLC and CC.","Semantics,
Natural languages,
Machine learning algorithms,
Engines,
Classification algorithms,
Computer science,
Algorithm design and analysis"
Real Life Machine Learning Case on Mobile Advertisement: A Set of Real-Life Machine Learning Problems and Solutions for Mobile Advertisement,"This paper is an output of data science study on a real life problem. The paper starts with the problem definition and a brief introduction to the mobile advertisement for addressing the machine learning problems. Later on, some machine learning solutions are provided for each of the problems, furthermore the success of classical solution methods in the literature is also compared for the real life problems. Some problems addressed are: unbalanced data sets, parameter optimization, time slicing and history optimization and there are also some performance metrics related to the mobile advertisement problem domain. This paper mainly considers the actions generated by users and advertisement providers as a data stream and proposes a well optimized recommender algorithm based on crucial parameters. Different than most of the papers in the literature, this study is an output of a research collaboration with a real life advertisement platform.","Mobile communication,
Machine learning algorithms,
History,
Algorithm design and analysis,
GSM,
Web pages,
Optimization"
Enrichment of Machine Learning Based Activity Classification in Smart Homes Using Ensemble Learning,"Data streams from various Internet-Of-Things (IOT) enabled sensors in smart homes provide an opportunity to develop predictive models to offer actionable insights in form of preventive care to its residence. This becomes particularly relevant for Aging-In-Place (AIP) solutions for the care of the elderly. Over the last decade, diverse stakeholders from practice, industry, education, research, and professional organizations have collaborated to furnish homes with a variety of IOT enabled sensors to record daily activities of individuals. Machine Learning on such streams allows for detection of patterns and prediction of activities which enables preventive care. Behavior patterns that lead to preventive care constitute a series of activities. Accurate labeling of activities is an extremely time-consuming process and the resulting labels are often noisy and error prone. In this paper, we analyze the classification accuracy of various activities within a home using machine learning models. We present that the use of an ensemble model that combines multiple learning models allows to obtain better classification of activities than any of the constituent learning algorithms.","Feature extraction,
Vegetation,
Mathematical model,
Smart homes,
Activity recognition,
Sensor phenomena and characterization"
Preliminary Results of Applying Machine Learning Algorithms to Android Malware Detection,"As the use of mobile devices continues to increase, so does the need for sophisticated malware detection algorithms. The preliminary research presented in this paper focuses on examining permission requests made by Android apps as a means for detecting malware. By using a machine learning algorithm, we are able to differentiate between benign and malicious apps. The model presented achieved a classification accuracy between 75% and 80% for our dataset and the best combination of parameters. Future work will seek to improve the model by expanding the training dataset, taking more features into account, and exploring other machine learning algorithms.","Malware,
Training,
Androids,
Humanoid robots,
Machine learning algorithms,
Smart phones"
Machine Learning Application to Predict the Risk of Coronary Artery Atherosclerosis,"Coronary artery disease is the leading cause of death in the world. In this research, we propose an algorithm based on the machine learning techniques to predict the risk of coronary artery atherosclerosis. A ridge expectation maximization imputation (REMI) technique is proposed to estimate the missing values in the atherosclerosis databases. A conditional likelihood maximization method is used to remove irrelevant attributes and reduce the size of feature space and thus improve the speed of the learning. The STULONG and UCI databases are used to evaluate the proposed algorithm. The performance of heart disease prediction for two classification models is analyzed and compared to previous work. Experimental results show the improved accuracy percentage of risk prediction of our proposed method compared to other works. The effect of missing value imputation on the prediction performance is also evaluated and the proposed REMI approach performs significantly better than conventional techniques.","Databases,
Atherosclerosis,
Arteries,
Support vector machines,
Solid modeling,
Training"
Machine learning based path management for mobile devices over MPTCP,"Recent mobile devices are equipped with multiple network interfaces such as LTE and Wi-Fi. Transport protocols that can transfer data over multiple paths, especially MPTCP (Multipath TCP), allows the devices like smartphones and tablets to exploit both interfaces concurrently. However, in real environments, wireless devices abound and network quality changes frequently. It makes network connection affect the MPTCP performance negatively. In this paper, we propose a novel path management scheme called MPTCP-ML (MPTCP based on Machine Learning) to make MPTCP troubleshoot the problem. It manages path usage among multiple connections based on decision computed by machine learning model. For accurate capturing of path quality, we utilize various quality metrics including signal strength, data rate, TCP throughput, the number of interference APs, and RTT (Round Trip Time). We have implemented MPTCP-ML in Android and conducted experiments for various and dynamic environments. The results show that MPTCP-ML outperforms generic MPTCP, especially for mobile environments.","Wireless fidelity,
Performance evaluation,
Interference,
Throughput,
Mobile communication,
Smart phones"
SimML Framework: Monte Carlo Simulation of Statistical Machine Learning Algorithms for IoT Prognostic Applications,"Advanced statistical machine learning (ML) algorithms are being developed, trained, tuned, optimized, and validated for real-time prognostics for internet-of-things (IoT) applications in the fields of manufacturing, transportation, and utilities. For such applications, we have achieved greatest prognostic success with ML algorithms from a class of pattern recognition known as nonlinear, nonparametric regression. To intercompare candidate ML algorithmics to identify the ""best"" algorithms for IoT prognostic applications, we use three quantitative performance metrics: false alarm probability (FAP), missed alarm probability (MAP), and overhead compute cost (CC) for real-time surveillance. This paper presents a comprehensive framework, SimML, for systematic parametric evaluation of statistical ML algorithmics for IoT prognostic applications. SimML evaluates quantitative FAP, MAP, and CC performance as a parametric function of input signals' degree of cross-correlation, signal-to-noise ratio, number of input signals, sampling rates for the input signals, and number of training vectors selected for training. Output from SimML is provided in the form of 3D response surfaces for the performance metrics that are essential for comparing candidate ML algorithms in precise, quantitative terms.","Internet of Things,
learning (artificial intelligence),
Monte Carlo methods,
real-time systems,
regression analysis"
Data mining in cloud usage data with Matlab's statistics and machine learning toolbox,"This paper focuses on use of Matlab for data mining. There is wide range of data mining software where free or cheaper solutions offer similar possibilities. We wanted to try Matlab for these purposes. Our data consists of parameters, which describes cloud usage at IT company that offers cloud services. We used phases from the CRISP-DM methodology in our work. We built clustering and classification models that use functions of the Statistics and Machine Learning Toolbox. In the conclusion we summarize our outcomes, weather Matlab is appropriate to data analysis based on conducted experiments.","MATLAB,
Data mining,
Data models,
Data analysis,
Companies,
Decision trees,
Databases"
Dynamic Mapping of Road Conditions Using Smartphone Sensors and Machine Learning Techniques,"Road surface conditions can cause serious traffic accidents, often with tragic consequences. Thus, an efficient system for mapping road anomalies can significantly promote the safety of drivers and pedestrians. This paper proposes a novel road anomaly mapping system that is able to detect a wide variety of conditions with high accuracy. The smartphone's accelerometer and GPS sensors are used for detection to minimize infrastructure costs. In addition, to ensure the system is adaptive to different road conditions, pattern recognition techniques are used to automatically calculate the detection threshold. Furthermore, to compensate for GPS inaccuracies, reinforcement learning based on a proposed reward system is used to maximize confidence in the detected anomalies. The reward system is also able to forget anomalies that have been fixed. Moreover, the system is implemented in a distributed way between the smartphone and a cloud server to minimize cellular bandwidth usage, while still retaining the accuracy advantages of a centralized cloud. Live tests have been conducted to evaluate the performance of the system and the results show it is accurate under different driving conditions.","Roads,
Sensors,
Servers,
Accelerometers,
Global Positioning System,
Vehicles,
Acceleration"
Application of Machine Learning algorithms for betterment in education system,"Machine learning is the process which converts the information into intelligent actions. This paper presents a literature review on application of different Machine Learning algorithms on huge amount of data collected by the academic institutes. Predictive analytics using the machine learning algorithms has become a new tool of this modern era, as it assists academic institutions in improving the retention and success rate of students and to get overview of performance before the examination to reduce the risk of failure. The main aim of the paper is to describe the various ways in which the machine learning is used in educational institutes and how institutes can get prediction of students' performance and the important features that are needed to be considered while making prediction for different things. In addition to this the study also compares the prediction given by different machine learning algorithms. Lastly the paper concludes that the prediction of the students' performance can be made more precise and accurate by considering the learning style of students, their motivation and interest, concentration level, family background, personality type, information processing ability and the way they attempt the exams.","Predictive models,
Neural networks,
Prediction algorithms,
Machine learning algorithms,
Decision trees,
Linear regression,
Education"
A Machine Learning Framework for Performance Coverage Analysis of Proxy Applications,"Proxy applications are written to represent subsets of performance behaviors of larger, and more complex applications that often have distribution restrictions. They enable easy evaluation of these behaviors across systems, e.g., for procurement or co-design purposes. However, the intended correlation between the performance behaviors of proxy applications and their parent codes is often based solely on the developer's intuition. In this paper, we present novel machine learning techniques to methodically quantify the coverage of performance behaviors of parent codes by their proxy applications. We have developed a framework, VERITAS, to answer these questions in the context of on-node performance: (a) which hardware resources are covered by a proxy application and how well, and (b) which resources are important, but not covered. We present our techniques in the context of two benchmarks, STREAM and DGEMM, and two production applications, OpenMC and CMTnek, and their respective proxy applications.","Hardware,
Radiation detectors,
Production,
Computer architecture,
Correlation,
Loss measurement"
A machine learning model for radar rainfall estimation based on gauge observations,"Rainfall estimation based on radar measurements has been addressed by using parametric algorithms such as Z-R relation. However, such empirical relations may not be sufficient to capture the space-time variability of precipitation. In this paper, we introduce a DMLP-based machine learning model for rainfall estimation by using multiple layers to capture the complex abstractions of radar reflectivity at different attitude levels. The radar data collected by the Weather Surveillance Radar - 1988 Doppler (WSR-88DP) in Melbourne, Florida (i.e., KMLB radar) are used for demonstration purposes, while the rain gauge data are used for training purposes. The rainfall product derived from the DMLP model is compared against an independent rain gauge dataset, which shows excellent performance of the new machine learning based rainfall model.","Estimation,
Rain,
Doppler radar,
Radar measurements,
Meteorological radar,
Training"
Diagnosis of diabetic retinopathy using machine learning classification algorithm,"Diabetic Retinopathy is human eye disease which causes damage to retina of eye and it may eventually lead to complete blindness. Detection of diabetic retinopathy in early stage is essential to avoid complete blindness. Many physical tests like visual acuity test, pupil dilation, optical coherence tomography can be used to detect diabetic retinopathy but are time consuming and affects patients as well. This review paper focuses on decision about the presence of disease by applying ensemble of machine learning classifying algorithms on features extracted from output of different retinal image processing algorithms, like diameter of optic disk, lesion specific (microaneurysms, exudates), image level (pre-screening, AM/FM, quality assessment). Decision making for predicting the presence of diabetic retinopathy was performed using alternating decision tree, adaBoost, Naive Bayes, Random Forest and SVM.","Diabetes,
Feature extraction,
Retinopathy,
Classification algorithms,
Retina,
Optical imaging,
Support vector machines"
Cardiotocography Analysis Using Conjunction of Machine Learning Algorithms,"Fetal distress is a primary factor for cesarean section in obstetrics and gynecology. If there is a lack of oxygen for the fetus in the uterus, then there is always a risk of deteriorating health which could lead to death as well. Cardiotocography (CTG) the most popular technique to observe fetal health. Fetalheart rate (FHR) is an essential index to identify occurences of fetal distress. This study makes use of the following to evaluate fetal distress: 1) Decision tree (DT) 2) Support Vector Machines (SVM) 3) Random Forest 4) Neural Networks 5) Gradient Boosting We present in this paper a new algorithm which improves the accuracy to 99.25% which is higher than what was obtained inprevious research.","Fetal heart rate,
Decision trees,
Boosting,
Histograms,
Support vector machines,
Neural networks,
Cardiography"
Stock market sentiment analysis based on machine learning,"Opinion mining is used as scrutiny of public opinions. The growth of social network has put onward the views of the general public on a larger scale and in an open manner. The comments, views and opinions act as deciding factors whether these are positive opinion or negative opinion. Guessing about the opinions' polarity is not a good idea, so, an intelligent system need to be introduced to categorize the views. Sentiment analysis thus emerged as a highlighted area in data mining. The opinions are judged on the basis of unsupervised and supervised learning. Supervised learning has unwavering to be superior to unsupervised mode of view verdict. The proposed paper has given a comparative study of naïve Bayes and SVM on the opinions of the reviewers of the stock market. No system has been created for sentiment analysis in the share market. Thus, new field is chosen and worked upon and its result can helps the user to take better decisions in the field of stock market.","Support vector machines,
Taxonomy,
Data mining,
Algorithm design and analysis,
Supervised learning,
Motion pictures,
Next generation networking"
Analysis of post-harvest losses: An Internet of Things and machine learning approach,"Reduction of post-harvest losses is a critical component of food security. World population is increasing at an alarming rate and thus is the food requirement. Due to limited cultivable land, increasing the food production to meet the needs of people, solely, cannot be the solution. In this paper, we have proposed to build an end-to-end system for farmers and warehouse managers to reduce post-harvest losses. It will consist of a notification-suggestion system which will include data about the current status of farm, suggestions about correct harvesting time and diseases that might affect the crop in its cultivation stages. The system will also include a prediction system for warehouse managers which will suggest the correct dispatch sequence of the stocks and also the optimum temperature and humidity at which one or more crops can be transported so as incur minimum storage and transportation loss. Here, for the prediction-analysis and suggestions, various statistical and probabilistic techniques such as classification and regression are used.","Agriculture,
Androids,
Humanoid robots,
Temperature distribution,
Humidity,
Temperature sensors,
Diseases"
A new machine learning approach to select adaptive IMFs of EMD,"An adaptive algorithm for selection of Intrinsic Mode Functions (IMF) of Empirical Mode Decomposition (EMD) is a time demand in the field of signal processing. This paper presents a new model of an effective algorithm for the adaptive selection of IMFs for the EMD. Our proposed model suggests the decomposition of an input signal using EMD, and the resultant IMFs are classified into two categories the relevant noise free IMFs and the irrelevant noise dominant IMFs using a trained Support Vector Machine (SVM). The Pearson Correlation Coefficient (PCC) is used for the supervised training of SVM. Noise dominant IMFs are then de-noised using the Savitzky-Golay filter. The signal is reconstructed using both noise free and de-noised IMFs. Our proposed model makes the selection process of IMFs adaptive and it achieves high Signal to Noise Ratio (SNR) while the Percentage of RMS Difference (PRD) and Max Error values are low. Experimental result attained up to 41.79% SNR value, PRD and Max Error value reduced to 0.814% and 0.081%, respectively compared to other models.","Support vector machines,
Mathematical model,
Signal to noise ratio,
Computational modeling,
Empirical mode decomposition,
Noise measurement"
"Deep Learning for Consumer Devices and Services: Pushing the limits for machine learning, artificial intelligence, and computer vision.","In the last few years, we have witnessed an exponential growth in research activity into the advanced training of convolutional neural networks (CNNs), a field that has become known as deep learning. This has been triggered by a combination of the availability of significantly larger data sets, thanks in part to a corresponding growth in big data, and the arrival of new graphics-processing-unit (GPU)-based hardware that enables these large data sets to be processed in reasonable timescales. Suddenly, a wide variety of long-standing problems in machine learning, artificial intelligence, and computer vision have seen significant improvements, often sufficient to break through long-standing performance barriers. Across multiple fields, these achievements have inspired the development of improved tools and methodologies leading to even broader applicability of deep learning. The new generation of smart assistants, such as Alexa, Hello Google, and others, have their roots and learning algorithms tied to deep learning. In this article, we review the current state of deep learning, explain what it is, why it has managed to improve on the long-standing techniques of conventional neural networks, and, most importantly, how you can get started with adopting deep learning into your own research activities to solve both new and old problems and build better, smarter consumer devices and services.","Training,
Machine learning,
Convolution,
Neural networks,
Big Data,
Deep learning,
Graphics processing unit"
Reusability of the Output of Map-Matching Algorithms Across Space and Time Through Machine Learning,"A map-matching algorithm outputs a vector per GPS point, projecting the moving object on one of the segments of the transportation network. Although developing more sophisticated map-matching algorithms for vehicle and pedestrian navigation systems have been the focus of research in this field, reusability of the historical information already provided by map-matching algorithms has not been addressed yet. In other words, although researchers have been attempting to improve the accuracy of the aforementioned vector to correctly project GPS points on the transportation network, no research has exploited the spatial-temporal pattern in the arrangement of these projection vectors. This pattern, if properly detected, can be used as a rough surrogate for map-matching algorithms, in addition to other applications that require better positional accuracy for moving objects in smart cities. This paper detects and validates the spatial-temporal pattern in projection vectors produced by map-matching algorithms via machine learning. Projection vectors showed a strong spatial-temporal pattern in Chicago, IL, USA, which was captured best via a local nonlinear regressor, K-nearest neighbors, and helped double the positional accuracy of unseen GPS points. While a global nonlinear regressor, multilayer Perceptron was able to slightly improve the positional accuracy of GPS points, the linear least squares had an exacerbating effect on the positional accuracy.","Global Positioning System,
Machine learning algorithms,
Encoding,
Training,
Satellites,
Receivers"
Improved Arabic characters recognition by combining multiple machine learning classifiers,"In this paper, we investigate a range of strategies for combining multiple machine learning techniques for recognizing Arabic characters, where we are faced with imperfect and dimensionally variable input characters. Experimental results show that combined confidence-based backoff strategies can produce more accurate results than each technique produces by itself and even the ones exhibited by the majority voting combination.","Character recognition,
Feature extraction,
Support vector machines,
Discrete cosine transforms,
Optical character recognition software,
Writing,
Training"
A machine learning approach for authorship attribution for Bengali blogs,"In this paper we described an authorship attribution system for Bengali blog texts. We have presented a new Bengali blog corpus of 3000 passages written by three authors. Our study proposes a text classification system, based on lexical features such as character bigrams and trigrams, word n-grams (n = 1, 2, 3) and stop words, using four classifiers. We achieve best results (more than 99%) on the held-out dataset using Multi layered Perceptrons (MLP) amongst the four classifiers, which indicates MLP can produce very good results for big data sets and lexical n-gram based features can be the best features for any authorship attribution system.","Blogs,
Training,
Writing,
Niobium,
Electronic mail,
Standards,
Information technology"
A Machine Learning Based Framework for Verification and Validation of Massive Scale Image Data,"Big data validation and system verification are crucial for ensuring the quality of big data applications. However, a rigorous technique for such tasks is yet to emerge. During the past decade, we have developed a big data system called CMA for investigating the classification of biological cells based on cell morphology that is captured in diffraction images. CMA includes a group of scientific software tools, machine learning algorithms, and a large scale cell image repository. We have also developed a framework for rigorous validation of the massive scale image data and verification of both the software systems and machine learning algorithms. Different machine learning algorithms integrated with image processing techniques were used to automate the selection and validation of the massive scale image data in CMA. An experiment based technique guided by a feature selection algorithm was introduced in the framework to select optimal machine learning features. An iterative metamorphic testing approach is applied for testing the scientific software. Due to the non-testable characteristic of the scientific software, a machine learning approach is introduced for developing test oracles iteratively to ensure the adequacy of the test coverage criteria. Performance of the machine learning algorithms is evaluated with the stratified N-fold cross validation and confusion matrix. We describe the design of the proposed framework with CMA as the case study. The effectiveness of the framework is demonstrated through verifying and validating the data set, software systems and algorithms in CMA.","Big data,
Diffraction,
Morphology,
Software,
Machine learning algorithms,
Three-dimensional displays,
Testing"
Nature and biology inspired approach of classification towards reduction of bias in machine learning,"Machine learning has become a powerful tool in real applications such as decision making, sentiment prediction and ontology engineering. In the form of learning strategies, machine learning can be specialized into two types: supervised learning and unsupervised learning. Classification is a special type of supervised learning task, which can also be referred to as categorical prediction. In other words, classification tasks involve predictions of the values of discrete attributes. Some popular classification algorithms include Naïve Bayes and K Nearest Neighbour. The above type of classification algorithms generally involves voting towards classifying unseen instances. In traditional ways, the voting is made on the basis of any employed statistical heuristics such as probability. In Naïve Bayes, the voting is made through selecting the class with the highest posterior probability on the basis of the values of all independent attributes. In K Nearest Neighbour, majority voting is usually used towards classifying test instances. This kind of voting is considered to be biased, which may lead to overfitting. In order to avoid such overfitting, this paper proposes to employ a nature and biology inspired approach of voting referred to as probabilistic voting towards reduction of bias. An extended experimental study is reported to show how the probabilistic voting can manage to effectively reduce the bias towards improvement of classification accuracy.","Probabilistic logic,
Mathematical model,
Classification algorithms,
Probability,
Biology,
Training data,
Training"
Simulation of marketplace customer satisfaction analysis based on machine learning algorithms,"Twitter can be a source of public opinion data and sentiment. Such data can be used efficiently for marketing or social studies. In this research addresses this issue by measuring net sentiment based on customer satisfaction through customer's sentiment analysis from Twitter data. Sample model is built and extracted from more than 3.000 raw Twitter messages data from March to April 2016 of top marketplace in Indonesia. We compared several algorithms, and the classification schemes. The sentiments are classified and compared using five different algorithms classification. There are, K-Nearest Neighbor, Logistic Regression, Naïve Bayes, Random Forest, and Support Vector Machine. The five machine learning can be applied to the Indonesian-language sentiment analysis. Preprocessing on the stages of tokenization, parsing, and stop word deletion of word frequency counting. Frequency of the word of the document used weighted by TF-IDF. The Random Forest, Support Vector Machine, and Logistic Regression generate better accuracy and stable compared with the Naïve Bayes and K-Nearest Neighbor. The results showed Support Vector Machine has accuracy 81.82% with 1000 sampling dataset and 85.4% with 2000 sampling dataset. This shows that the more the number of training data will improve the accuracy of the system. The Net Sentiment score for marketplace in Indonesia is 73%. This results also showed that customer satisfaction has average Net Promoter Score (NPS) 3.3%.",
Extreme learning machine based on cross entropy,"Extreme Learning Machine (ELM) is an algorithm for training single hidden layer feed-forward neural networks (SLFNs). Because ELM does not need the process of iterative learning, it is extremely faster than traditional learning algorithms such as back propagation algorithm and support vector machine. In ELM, the optimal solution with least squares norm is found by calculating the generalized inverse of hidden output matrix. When the order of hidden output matrix is a high, i.e., the number of hidden layer nodes is many, the over-fitting phenomenon will occur. Aiming at solving the over-fitting problem existing in ELM with many hidden layer nodes, this paper proposes a Cross Entropy based ELM (CE-ELM) in which, the mean square error minimization principle is replaced with the cross entropy minimization principle. The experimental results confirmed that the proposed CE-ELM can sufficiently overcome the drawback of overfitting in ELM with many hidden layer nodes.","Entropy,
Training,
Mathematical model,
Neural networks,
Approximation algorithms,
Minimization,
Probability distribution"
Extend relation identification in scientific papers based on supervised machine learning,"This paper discusses the identification of extend relation in scientific papers based on supervised machine learning. Identification of extend relations is conducted by classifying each sentence in scientific papers into extend category. Extend relation is one type of papers' relations that obtained by using the citation context based approach. Citation context is a set of words or phrases in a sentence collection that citing or discussing other papers. Citation context based approach use the semantic of citation sentence to identify the relationship between scientific papers and can identify more varied relationship than two other approaches i.e. content-based and citation analysis. The recently research in papers' relations used a rule-based approach to identify extend relations. In this paper, supervised learning approached with proposed features set was used to identify extend relations. The learning of classifier model is explored by using Naïve Bayes, Decision Tree, Ibk and Logistic Regression. Experimental results show that the performance of extend sentence classification based on supervised machine learning with proposed features is superior compared to the baseline. Feature selection based on correlation value is also effective to improve the performance of the extend sentence classification.","Context,
Feature extraction,
Data models,
Text categorization,
Supervised learning,
Predictive models,
Training"
"Comparative analysis of machine learning KNN, SVM, and random forests algorithm for facial expression classification","Human depicts their emotions through facial expression or their way of speech. In order to make this process possible for a machine, a training mechanism is needed to give machine the ability to recognize human expression. This paper compare and analyse the performance of three machine learning algorithm to do the task of classifying human facial expression. The total of 23 variables calculated from the distance of facial features are used as the input for the classification process, with the output of seven categories, such as: angry, disgust, fear, happy, neutral, sad, and surprise. Some test cases were made to test the system, in which each test cases has different amount of data, ranging from 165-520 training data. The result for each algorithm is quite satisfying with the accuracy of 75.15% for K-Nearest Neighbor (KNN), 80% for Support Vector Machine (SVM), and 76.97% for Random Forests algorithm, tested using test case with the smallest amount of data. As for the result using the largest amount of data, the accuracy is 98.85% for KNN, 90% for SVM, and 98.85% for Random Forests algorithm. The training data for each test case was also classified using Discriminant Analysis with the result 97.7% accuracy.","Eyebrows,
Support vector machines,
Mouth,
Classification algorithms,
Machine learning algorithms,
Facial features,
Training"
Learning self-awareness in committee machines,"Self-awareness is a kind of ability of recognizing oneself as an individual being different from the environment and other individuals. This paper proposes negative correlation learning with self-awareness in order for each artificial neural network (ANN) in a committee machine to be self-aware in learning so that it could decide by itself to learn more or less. On one hand, when the learning would force itself to be closer to the ensemble, an individual ANN would choose to learn less so that the learning on that direction would be disencouraged. On the other hand, when the learning would help itself to be more different to the ensemble, an individual ANN would let itself to learn more so that the learning on that direction would be encouraged. It is expected that such ANNs being aware of their own behavior and performance can manage trade-offs between goals at run-time. Such self-awareness enables a committee machine to better meet their requirements for predictions on the unknown data. Measurement results have been presented to how self-awareness could support the different behaviors and maintain the performance.",
Risk factors of heart failure for patients classification with extreme learning machine,"Heart failure (HF), the terminal stage of all kinds of cardiovascular disease, has a high level of morbidity and mortality. But the heavy burden of curing and managing HF can be largely reduced by early detection of it. Motivated by this problem, we study methods to determine its risk factors based on extreme learning machine (ELM). Several state-of-the-art data mining algorithms are employed to estimate the performance of various classification of the HF patients. Our data sets are extracted from reality hospital patients' data, which consist of patients' basic demographic, disease and assay information. The results show that ELM will have a better performance larger than 93% if the selected attributes have more strong correlation with the label.","Heart,
Diseases,
Inspection,
Classification algorithms,
Cybernetics,
Hypertension,
Hospitals"
Structured support vector machine learning of conditional random fields,This research aims to improve the capability of semantic segmentation through data perspective. This research proposed a parameterized Conditional Random Fields model and learns the model by using Structured Support Vector Machine (SSVM). The SSVM utilizes Hamming loss function for optimizing 1-slack Margin Rescaling formulation. The joint feature vector is derived from energy potentials. Variation of image size produces some missing values in the joint feature vector. This research shows that a zero padding can resolve the missing values. The experiment result shows that prediction with parameterized CRF yields 75.867% global accuracy (GA) and 22.1410 % averaged class accuracy (CA).,"Labeling,
Image segmentation,
Semantics,
Support vector machines,
Image color analysis,
Optimization,
Inference algorithms"
Detecting jute plant disease using image processing and machine learning,"Detecting stem diseases of plants by image analysis are still in an inchoate state in the research field. This research has been conducted on detecting the stem diseases of jute plants which is one of the most important cash crops in some of the Asian countries. An automated system based on an Android application has been implemented to take pictures of the disease affected stems of jute plants and send them to the dedicated server for assaying. On the server side, the affected portion from the image will be segmented using customized thresholding formula based on hue-based segmentation. The consequential feature values will be extracted from the segmented portion for texture analysis using color co-occurrence methodology. The extracted values will be compared with the sample values stored in the pre-defined database which will lead the disease to be identified and classified using Multi-SVM classifier. At the final step, the classification result along with the necessary control measures will be sent back to the farmer within three seconds through the application on their phone.","Diseases,
Servers,
Image segmentation,
Mobile applications,
Image color analysis,
Feature extraction"
Increasing accuracy of traffic light color detection and recognition using machine learning,"Traffic light detection is an important system because it can alert driver on upcoming traffic light so that he/she can anticipate a head of time. In this paper we described our work on detecting traffic light color using machine learning approach. Using HSV color representation, our approach is to extract features based on an area of X×X pixels. Traffic light color model is then created by applying a learning algorithm on a set of examples of features representing pixels of traffic and non-traffic light colors. The learned model is then used to classify whether an area of pixels contains traffic light color or not. Evaluation of this approach reveals that it significantly improves the detection performance over the one based on value-range color segmentation technique.","Color,
Image color analysis,
Feature extraction,
Mathematical model,
Support vector machines,
Classification algorithms,
Autonomous vehicles"
Supervised machine learning for signals having RRC shaped pulses,"Classification performances of the supervised machine learning techniques such as support vector machines, neural networks and logistic regression are compared for modulation recognition purposes. The simple and robust features are used to distinguish continuous-phase FSK from QAM-PSK signals. Signals having root-raised-cosine shaped pulses are simulated in extreme noisy conditions having joint impurities of block fading, lack of symbol and sampling synchronization, carrier offset, and additive white Gaussian noise. The features are based on sample mean and sample variance of the imaginary part of the product of two consecutive complex signal values.","Support vector machines,
Training,
Artificial neural networks,
Frequency shift keying,
Signal to noise ratio,
Fading channels"
A machine learning analysis of Twitter sentiment to the Sandy Hook shootings,"Gun related violence is a complex issue and accounts for a large proportion of violent incidents. In the research reported in this paper, we set out to investigate the pro-gun and anti-gun sentiments expressed on a social media platform, namely Twitter, in response to the 2012 Sandy Hook Elementary School shooting in Connecticut, USA. Machine learning techniques are applied to classify a data corpus of over 700,000 tweets. The sentiments are captured using a public sentiment score that considers the volume of tweets as well as population. A web-based interactive tool is developed to visualise the sentiments and is available at http://www.gunsontwitter.com. The key findings from this research are: (i) There are elevated rates of both pro-gun and anti-gun sentiments on the day of the shooting. Surprisingly, the pro-gun sentiment remains high for a number of days following the event but the anti-gun sentiment quickly falls to pre-event levels. (ii) There is a different public response from each state, with the highest pro-gun sentiment not coming from those with highest gun ownership levels but rather from California, Texas and New York.","Weapons,
Feature extraction,
Sociology,
Statistics,
Niobium,
Twitter,
Data visualization"
Machine Learning Techniques for Optical Performance Monitoring From Directly Detected PDM-QAM Signals,"Linear signal processing algorithms are effective in dealing with linear transmission channel and linear signal detection, whereas the nonlinear signal processing algorithms, from the machine learning community, are effective in dealing with nonlinear transmission channel and nonlinear signal detection. In this paper, a brief overview of the various machine learning methods and their application in optical communication is presented and discussed. Moreover, supervised machine learning methods, such as neural networks and support vector machine, are experimentally demonstrated for in-band optical signal to noise ratio estimation and modulation format classification, respectively. The proposed methods accurately evaluate optical signals employing up to 64 quadrature amplitude modulation, at 32 Gbd, using only directly detected data.","Optical noise,
Signal to noise ratio,
Neural networks,
Nonlinear optics,
Optical polarization,
Optical modulation"
Eye refractive error classification using machine learning techniques,"Machine learning is a subdivision of Artificial Intelligence (AI) that is concerned with the design and development of intelligent algorithms that enables machines to learn from data without being programmed. Machine learning mainly focus on how to automatically recognize complex patterns among data and make intelligent decisions. In this paper, intelligent machine learning algorithms are used to classify the type of an eye disease based on ophthalmology data collected from patients of Mecca hospital in Sudan. Three machine-learning techniques are used to predict the severity of the eye that occurred during the investigation, which are Naïve Bayesian, SVM, and J48 decision tree. The obtained result showed that J48 classifier outperforms both Naïve Bayesian as well as SVM.","Data mining,
Decision trees,
Classification algorithms,
Support vector machines,
Diseases,
Prediction algorithms,
Computational modeling"
Survey on Software Vulnerability Analysis Method Based on Machine Learning,"With the increasingly rich of vulnerability related data and the extensive application of machine learning methods, software vulnerability analysis methods based on machine learning is becoming an important research area of information security. In this paper, the up-to-date and well-known works in this research area were analyzed deeply. A framework for software vulnerability analysis based on machine learning was proposed. And the existing works were described and compared, the limitations of these works were discussed. The future research directions on software vulnerability analysis based on machine learning were put forward in the end.","Security,
Feature extraction,
Computer bugs,
Information systems,
Learning systems,
Software systems"
Joint network coding and machine learning for error-prone wireless broadcast,"Reliable broadcasting data to multiple receivers over lossy wireless channels is challenging due to the heterogeneity of the wireless link conditions. Automatic Repeat-reQuest (ARQ) based retransmission schemes are bandwidth inefficient due to data duplication at receivers. Network coding (NC) has been shown to be a promising technique for improving network bandwidth efficiency by combining multiple lost data packets for retransmission. However, it is challenging to accurately determine which lost packets should be combined together due to disrupted feedback channels. This paper proposes an adaptive data encoding scheme at the transmitter by joining network coding and machine learning (NCML) for retransmission of lost packets. Our proposed NCML extracts the important features from historical feedback signals received by the transmitter to train a classifier. The constructed classifier is then used to predict states of transmitted data packets at different receivers based on their corrupted feedback signals for effective data mixing. We have conducted extensive simulations to collaborate the efficiency of our proposed approach. The simulation results show that our machine learning algorithm can be trained efficiently and accurately. The simulation results show that on average the proposed NCML can correctly classify 90% of the states of transmitted data packets at different receivers. It achieves significant bandwidth gain compared with the ARQ and NC based schemes in different transmission terrains, power levels, and the distances between the transmitter and receivers.",
Cross-platform machine learning characterization for task allocation in IoT ecosystems,"With the emergence of the Internet of Things (IoT) and Big Data era, many applications are expected to assimilate a large amount of data collected from environment to extract useful information. However, how heterogeneous computing devices of IoT ecosystems can execute the data processing procedures has not been clearly explored. In this paper, we propose a framework which characterizes energy and performance requirements of the data processing applications across heterogeneous devices, from a server in the cloud and a resource-constrained gateway at edge. We focus on diverse machine learning algorithms which are key procedures for handling the large amount of IoT data. We build analytic models which automatically identify the relationship between requirements and data in a statistical way. The proposed framework also considers network communication cost and increasing processing demand. We evaluate the proposed framework on two heterogenous devices, a Raspberry Pi and a commercial Intel server. We show that the identified models can accurately estimate performance and energy requirements with less than error of 4.8% for both platforms. Based on the models, we also evaluate whether the resource-constrained gateway can process the data more efficiently than the server in the cloud. The results present that the less-powerful device can achieve better energy and performance efficiency for more than 50% of machine learning algorithms.","power aware computing,
Big Data,
energy consumption,
Internet of Things,
internetworking,
learning (artificial intelligence),
network servers"
Recognition of human activities using machine learning methods with wearable sensors,"Body activity recognition using wearable sensor technology has drawn more and more attentions over the past few decades. The complexity and variety of body activities makes it difficult to fast, accurately and automatically recognize body activities. To solve this problem, this paper formulates body activity recognition problem as a classification problem using data collected by wearable sensors. And three different machine learning algorithms, support vector machine, hidden markov model and artificial neural network are presented to recognize different body activities. Various numerical experiments on a real-world wearable sensors dataset are designed to verify the effectiveness of these classification algorithms. Finally the results demonstrate that all the three algorithms achieve satisfactory activity recognition performance.","Hidden Markov models,
Support vector machines,
Training,
Artificial neural networks,
Classification algorithms,
Activity recognition,
Wearable sensors"
Using machine learning to predict if a profiled lay rescuer can successfully deliver a shock using a public access automated external defibrillator?,"A public access automated external defibrillator (AED) is a device that is intended to be used by lay rescuers in an event where a member of the public experiences a sudden cardiac arrest due to a severe ventricular arrhythmia. Therefore, it is imperative that the human-machine interface of an AED is optimized in terms of its usability and intuitive design. This study involved the recruitment of362 subjects (lay people) in a shopping mall to undertake the task of using an AED in a simulated environment as facilitated by a `sensorised' manikin and an AED that was developed by HeartSine Technologies. We found that a large proportion (91.44%) of lay people can successfully use an AED in a simulated emergency scenario to deliver a successful shock. We also found that CPR training did not provide greater likelihood for shock success whilst those with AED training did. Exploratory data analysis and machine learning were used to determine if demographics and other variables are potential predictors for delivering a successful shock using an AED. We found that user demographics and educational attainment were not predictive for AED `usage' success, which is reassuring since the objective of the medical industry is to develop AEDs that are intuitive to any member of the public.","Electric shock,
Training,
Decision trees,
Predictive models,
Usability,
Entropy"
An innovative delay based algorithm to boost PUF security against machine learning attacks,"Physical Unclonable Functions (PUFs) are the state-of-the-art topic in hardware oriented security and trust (HOST). PUFs have recently emerged as a promising solution for cyber security related issues including low-cost chip authentication, hacking of digital systems and tampering of physical systems. In this paper, a dynamic area efficient design along with an algorithm, namely, Optimal Time Delay Algorithm (OTPA) are proposed to improve the ROPUF challenge/response pairs (CRPs space). The generated CRPs space using this algorithm exhibit high PUF performance in terms of uniqueness and reliability. Experimental results demonstrate the competence of this innovative algorithm to enhance the CRP space using the probability of combinational statistic formula for improved security of the ROPUF against modeling attacks.","Table lookup,
Delays,
Machine learning algorithms,
Heuristic algorithms,
Silicon,
Security,
Ring oscillators"
A novel approach for classification of normal/abnormal phonocardiogram recordings using temporal signal analysis and machine learning,"This paper discusses a novel approach used for classification of phonocardiogram (PCG) excerpts into normal and abnormal classes as a part of Physionet 2016 challenge [10]. The dataset used for the competition comprises of cardiac abnormalities such as mitral valve prolapse (MVP), benign murmurs, aortic diseases, coronary artery disease, miscellaneous pathological conditions etc. [3], We present the approach used for classification from a general machine learning application standpoint, giving details on feature extraction, type of classifiers used comparing their performances individually and in combination. We propose a technique which leverages previous research on feature extraction with a novel approach to modeling temporal dynamics of the signal using Markov chain analysis [7,9]. These newly introduced Markov features along with other statistical and frequency domain features, trained over an ensemble of artificial neural networks and gradient boosting trees, with bagging, gave us an accuracy of 82% on the validation dataset provided in the competition and was consistent with the test data with the best result of 78%.","Feature extraction,
Markov processes,
Phonocardiography,
Training data,
Heart,
Training,
Frequency-domain analysis"
Using semi-supervised machine learning to address the Big Data problem in DNS networks,"The problem of Big Data in cyber security (i.e., too much network data to analyze) compounds itself every day. Our approach is based on a fundamental characteristic of Big Data: an overwhelming majority of the network traffic in a traditionally secured enterprise (i.e., using defense-in-depth) is non-malicious. Therefore, one way of eliminating the Big Data problem in cyber security is to ignore the overwhelming majority of an enterprise's non-malicious network traffic and focus only on the smaller amounts of suspicious or malicious network traffic. Our approach uses simple clustering along with a dataset enriched with known malicious domains (i.e., anchors) to accurately and quickly filter out the non-suspicious network traffic. Our algorithm has demonstrated the predictive ability to accurately filter out approximately 97% (depending on the algorithm used) of the non-malicious data in millions of Domain Name Service (DNS) queries in minutes and identify the small percentage of unseen suspicious network traffic. We demonstrate that the resulting network traffic can be analyzed with traditional reputation systems, blacklists, or in-house threat tracking sources (we used virustotal.com) to identify harmful domains that are being accessed from within the enterprise network. Specifically, our results show that the method can reduce a dataset of 400k query-answer domains (with complete malicious domain ground truth) down to only 3% containing 99% of all malicious domains. Further, we demonstrate that this capability scales to 10 million query-answer pairs, which it can reduce by 97% in less than an hour.","Clustering algorithms,
Big data,
Telecommunication traffic,
Semisupervised learning,
Computer security,
Approximation algorithms,
Sensitivity"
Machine learning based identification of pathological heart sounds,"Automated interpretation of heart sounds holds great promise in increasing the diagnostic accuracy and consistency of cardiac auscultation and allowing for use in remote, tele-health settings. However, existing algorithms for classification of hearts sounds have been constrained by limited idealized training sets and methodological issues with validation. As part of the 2016 PhysioNet Challenge competition, we present an algorithm for automated heart sound classification sthat uses Hilbert-envelope and wavelet features to attempt to capture the qualities of the heart sounds that physicians are trained to interpret. We perform a two-step classification of heart sounds into poor quality, normal or abnormal with sensitivity of 0.7958 and specificity of 0.7459.","Heart,
Valves,
Classification algorithms,
Feature extraction,
Hidden Markov models,
Training,
Blood"
Machine learning approaches for supporting patient-specific cardiac rehabilitation programs,"Cardiac rehabilitation is a well-recognised non-pharmacological intervention that prevents the recurrence of cardiovascular events. Previous studies investigated the application of data mining techniques for the prediction of the rehabilitation outcome in terms of physical, but fewer reports are focused on using predictive models to support clinicians in the choice of a patient-specific rehabilitative treatment path. Aim of the work was to derive a prediction model for help clinicians in the prescription of the rehabilitation program. We enrolled 129 patients admitted for cardiac rehabilitation after a major cardiovascular event. Data on anthropometric measures, surgical procedure and complications, comorbidities and physical performance scales were collected at admission. The prediction outcome was the rehabilitation program divided in four different paths. Different algorithms were tested to find the best predictive model. Models performance were measured by prediction accuracy. Mean model accuracy was 0.790 (SD 0.118). Best model selected was Lasso regression showing an average classification accuracy on test set of0.935. Data mining techniques have shown to be a reliable tool for support clinicians in the decision of cardiac rehabilitation treatment path.","Predictive models,
Support vector machines,
Heart,
Data mining,
Training,
Medical services,
Kernel"
Machine learning based performance development for diagnosis of breast cancer,"Breast cancer is prevalent among women and develops from breast tissue. Early diagnosis and accurate treatment is vital to increase the rate of survival. Identification of genetic factors with microarray technology can make significant contributions to diagnosis and treatment process. In this study, several machine learning algorithms are used for Diagnosis of Breast Cancer and their classification performances are compared with each other. In addition, the active genes in breast cancer are identified by attribute selection methods and the conducted study show success rate 90,72 % with 139 feature.","Breast cancer,
Support vector machines,
Biomedical informatics,
Machine learning algorithms,
Lenses,
Receivers"
PSN-aware circuit test timing prediction using machine learning,"Excessive power supply noise (PSN) such as IR drop can cause yield loss when testing very large scale integration chips. However, simulation of circuit timing with PSN is not an easy task. In this study, the authors predict circuit timing for all test patterns using three machine learning techniques, neural network (NN), support vector regression (SVR), and least-square boosting (LSBoost). To reduce the huge dimension of raw data, they propose four feature extractions: input/output transition (IOT), flip-flop transition in window (FFTW), switching activity in window (SAW), and terminal FF transition of long paths (PATH). SAW and FFTW are physical-aware features while PATH is a timing-aware feature. Their experimental results on leon3mp benchmark circuit (638 K gates, 2 K test patterns) show that, compared with the simple IOT method, SAW effectively reduced the dimension by up to 472 times, without significant impact on prediction accuracy [correlation coefficient = 0.79]. Their results show that NN has best prediction accuracy and SVR has the least under-prediction. LSBoost uses the least memory. The proposed method is more than six orders of magnitude faster than traditional circuit simulation tools.","VLSI,
circuit simulation,
data reduction,
feature extraction,
integrated circuit noise,
integrated circuit testing,
learning (artificial intelligence),
least squares approximations,
neural nets,
regression analysis,
support vector machines"
Lung cancer detection using Local Energy-based Shape Histogram (LESH) feature extraction and cognitive machine learning techniques,"The novel application of Local Energy-based Shape Histogram (LESH) feature extraction technique was recently proposed for breast cancer diagnosis using mammogram images [22]. This paper extends our original work to apply the LESH technique to detect lung cancer. The JSRT Digital Image Database of chest radiographs is selected for research experimentation. Prior to LESH feature extraction, we enhanced the radiograph images using a contrast limited adaptive histogram equalization (CLAHE) approach. Selected state-of-the-art cognitive machine learning classifiers, namely extreme learning machine (ELM), support vector machine (SVM) and echo state network (ESN) are then applied using the LESH extracted features for efficient diagnosis of correct medical state (existence of benign or malignant cancer) in the x-ray images. Comparative simulation results, evaluated using the classification accuracy performance measure, are further bench-marked against state-of-the-art wavelet based features, and authenticate the distinct capability of our proposed framework for enhancing the diagnosis outcome.",Decision support systems
A review of machine learning techniques using decision tree and support vector machine,"In this paper, the brief survey of data mining classification by using the machine learning techniques is presented. The machine learning techniques like decision tree and support vector machine play the important role in all the applications of artificial intelligence. Decision tree works efficiently with discrete data and SVM is capable of building the nonlinear boundaries among the classes. Both of these techniques have their own set of strengths which makes them suitable in almost all classification tasks.","Decision trees,
Classification algorithms,
Support vector machines,
Machine learning algorithms,
Data mining,
Databases,
Supervised learning"
Prediction of post-operative implanted knee function using machine learning in clinical big data,"Total knee arthroplasty (TKA) is one of the common knee surgeries. Because there are some types of TKA implant, it is hard to select appropriate type of TKA implant for individual patient. For the sake of pre-operative planning, this study presents a novel approach, which predicts post-operative implanted knee function of individuals. It is based on a clinical big data analysis. The big data is composed by a set of pre-operative knee mobility function and post-operative knee function. The method constructs a post-operative knee function prediction model by means of a machine learning approach. It extracts features using principal component analysis, and constructs a mapping function from pre-operative feature space to post-operative feature space. The method was validated by applying to prediction of post-operative anterior-posterior translation in 52 TKA operated knees. Leave-one-out cross validation test revealed the prediction performances with a mean correlation coefficients of 0.79 and a mean root-mean-squared-error of 3.44 mm.","Kinematics,
Knee,
Eigenvalues and eigenfunctions,
Principal component analysis,
Big data,
Feature extraction,
Machine learning algorithms"
Machine Learning for SQL injection prevention on server-side scripting,"SQL injection is the most common web application vulnerability. The vulnerability can be generated unintentionally by software developer during the development phase. To ensure that all secure coding practices are adopted to prevent the vulnerability. The framework of SQL injection prevention using compiler platform and machine learning is proposed. The machine learning part will be described primarily since it is the core of this framework to support SQL injection prediction by conducting 1,100 datasets of vulnerabilities to train machine learning model. The results indicated that decision tree is the best model in term of processing time, highest efficiency in prediction.",
Machine learning methods for the health-indexing and ranking of underground distribution cables and joints,"An aging asset population and a less predictable volatile electricity consumption and production pattern urge DSOs to get insight in the condition of their medium voltage (MV) and low voltage (LV) networks. Because visual inspections of underground networks are impossible and the number of measurements is still very limited, this paper proposes a method to rank underground assets by looking for trends and patterns in historical outages with help of Machine Learning methods. Nine years of outages of MV and LV cables and joints in the network of a large Dutch DSO are analysed. A model is developed that couples each outage to the asset most probable responsible. Twenty-two different datasets are coupled with the asset database, ranging from load estimates of the asset to distance-to-a-railway. Each set could contain data that explains or correlates to some of the outages. Several Machine Learning techniques are benchmarked. The final model, created by the Random Forest algorithm, is applied to rank current assets. It is operational to determine the positioning of an online monitoring system in the DSO's MV network.","underground cables,
asset management,
learning (artificial intelligence),
power distribution faults,
power distribution lines,
power distribution reliability,
power engineering computing"
Algorithms for determining semantic relations of formal concepts by cognitive machine learning based on concept algebra,"It is recognized that the semantic space of knowledge is a hierarchical concept network. This paper presents theories and algorithms of hierarchical concept classification by quantitative semantic relations via machine learning based on concept algebra. The equivalence between formal concepts are analyzed by an Algorithm of Concept Equivalence Analysis (ACEA), which quantitatively determines the semantic similarity of an arbitrary pair of formal concepts. This leads to the development of the Algorithm of Relational Semantic Classification (ARSC) for hierarchically classify any given concept in the semantic space of knowledge. Experiments applying Algorithms ACEA and ARSC on 20 formal concepts are successfully conducted, which encouragingly demonstrate the deep machine understanding of semantic relations and their quantitative weights beyond human perspectives on knowledge learning and natural language processing.","Writing,
Instruments,
Decision support systems,
Ink,
Animals,
Handheld computers,
Lead"
Improving pseudo-code detection in ubiquitous scholarly data using ensemble machine learning,"A significant number of new algorithms constantly emerge ubiquitously as computer science and other computational related disciplines grow in advancement and complexity. A majority of these algorithms are developed by professional researchers who publish their algorithmic advancements in scholarly articles, especially in the form of pseudo-codes. The ability to automatically collect, manage, and index these pseudo-codes could prove to be useful for computer scientists and software developers seeking cutting-edge algorithmic solutions to their problems. In an effort towards automatic retrieval of these pseudo-codes, a machine learning based approach that detects and extracts these pseudo-codes in large scale scholarly documents has recently been proposed. In this paper, we extend the previous findings by investigating possible enhancement on the previously proposed classification methodology using ensemble learning techniques. The results illustrate that Random Forest is by far the most effective ensemble learning method which improves the classification performance by 13% over the best base classifier.","Feature extraction,
Decision trees,
Data mining,
Prediction algorithms,
Machine learning algorithms,
Learning systems,
Algorithm design and analysis"
Machine-learning approach to analysis of driving simulation data,"In our study, we sought to generate rules for cognitive distractions of car drivers using data from a driving simulation environment. We collected drivers' eye-movement and driving data from 18 research participants using a simulator. Each driver drove the same 15-minute course two times. The first drive was normal driving (no-load driving), and the second drive was driving with a mental arithmetic task (load driving), which we defined as cognitive-distraction driving. To generate rules of distraction driving using a machine-learning tool, we transformed the data at constant time intervals to generate qualitative data for learning. Finally, we generated rules using a Support Vector Machine (SVM).","Support vector machines,
Automobiles,
Data models,
Safety,
Visualization,
Acceleration"
Performance analysis of supervised machine learning algorithms for text classification,"The demand of text classification is growing significantly in web searching, data mining, web ranking, recommendation systems and so many other fields of information and technology. This paper illustrates the text classification process on different dataset using some standard supervised machine learning techniques. Text documents can be classified through various kinds of classifiers. Labeled text documents are used to classify the text in supervised classifications. This paper applied these classifiers on different kinds of labeled documents and measures the accuracy of the classifiers. An Artificial Neural Network (ANN) model using Back Propagation Network (BPN) is used with several other models to create an independent platform for labeled and supervised text classification process. An existing benchmark approach is used to analysis the performance of classification using labeled documents. Experimental analysis on real data reveals which model works well in terms of classification accuracy.","Text categorization,
Support vector machines,
Artificial neural networks,
Static VAr compensators,
Measurement,
Training,
Logistics"
An ensemble approach to detect review spam using hybrid machine learning technique,"Online reviews are becoming one of the vital components of e-commerce in recent years as so many people consider having different opinions prior to buying online products or apprehending any online service. Nowadays, in the era of web 2.0, it is completely understandable that people rely on online reviews more than ever while taking a decision. However, guaranteeing the authenticity of these sensitive and valuable information is hardly visible. Due to fulfill some immoral benefits, many people post fake review or fabricated opinion to uphold or devalue a certain product or service which certainly hampers the ingenuousness of the real fact. To detect fake reviews, many methodologies were introduced by harvesting the obvious content features, rating consistency, empirical conditions, helpfulness voting etc. The most of them are supervised models which mostly rely on pseudo fake reviews and the scarcity of good quality large-scale labeled dataset is still a hindrance. In this paper, we introduce an ensemble learning approach which combines two different types of learning methods (active and supervised) by creating a hybrid dataset of both real-life and pseudo reviews. This model holds 3 different filtering phases that is based on KL and JS distance, TF-IDF features and n-gram features of the review content. It achieves phenomenal results while working on almost 3600 reviews from different domains. In the best case, the precision, recall and f-score are above 95% and the accuracy it achieved is slightly above 88%. In the process, about 2000 reviews were manually labeled. After evaluating and comparing the results with other successful methods, it is quite clear that this detecting method is efficient and very promising.","Support vector machines,
Training,
Computers,
Information technology,
Internet,
Writing,
Data models"
Combining a rule-based classifier with ensemble of feature sets and machine learning techniques for sentiment analysis on microblog,"Microblog, especially Twitter, have become an integral part of our daily life, where millions of users sharing their thoughts daily because of its short length characteristics and simple manner of expression. Monitoring and analyzing sentiments from such massive Twitter posts provide enormous opportunities for companies and other organizations to estimate the user acceptance of their products and services. But the ever-growing unstructured and informal user-generated posts in Twitter demands sentiment analysis tools that can automatically infer sentiments from Twitter posts. In this paper, we propose an approach for sentiment analysis on Twitter, where we combine a rule-based classifier with a majority voting based ensemble of supervised classifiers. We introduce a set of rules for the rule-based classifier based on the occurrences of emoticons and sentiment-bearing words. To train the supervised classifiers, we extract a set of features grouped into Twitter specific features, textual features, parts-of-speech (POS) features, lexicon based features, and bag-of-words (BoW) feature. A supervised feature selection method based on the chi-square statistics (χ2) and information gain (IG) is applied to select the best feature combination. We conducted our experiments on Stanford sentiment140 dataset. Experimental results demonstrate the effectiveness of our method over the baseline and known related work.","Feature extraction,
Sentiment analysis,
Support vector machines,
Uniform resource locators,
Training,
Computers,
Information technology"
Design and implementation of user-oriented video streaming service based on machine learning,"We propose a method to determine appropriate quality of service (QoS) dynamically required by users for video streaming services in this paper. In the proposed method, the QoS parameters for the video streaming are determined based on the machine learning algorithm, by using a regression analysis in particular, according to the user requirements, computational/network resources and service provisioning environments. In this paper, we describe the design and implementation of our method. Furthermore, we confirm the feasibility of our proposed method through an experiment of a prototype system.","Quality of service,
Streaming media,
Video recording,
Quality assessment,
Standards,
Training data,
Prototypes"
Recent machine learning based approaches for disease detection and classification of agricultural products,"Disease infection to agricultural products like plants, fruits and vegetables, results in degradation of quality and quantity of agriculture products. This directly affects the financial source of agriculturists and the human health. Hence, detection of diseases in plants, fruits and vegetables crops at early stages of development leads to reduce loss of yield and quality. The traditional approaches for disease detection requires continues monitoring and observation of farms either by farmers or by experts. But, it is very costly and time consuming. In last few years, various researchers have focused into this area to provide optimized solution. Popular methods have utilized machine learning, image processing and classification based approaches to identify and detect the diseases on agricultural products. The existing techniques for disease detection have utilized various image processing methods followed by various classification techniques. This paper presents an overview of existing reported techniques useful in detection of diseases of agricultural products. A comparative study of different methods based on the type of agricultural product, methodology and its efficiency together with the advantages and disadvantages is also included in this paper.","Diseases,
Feature extraction,
Support vector machines,
Image color analysis,
Agricultural products"
Similarity metric induced metrics with application in machine learning and bioinformatics,"Similarity metric and distance metric are widely used in many research areas and applications. In this paper, for a given similarity metric, we will introduce a family of distance metrics of Minkowski type. We will then show general solutions to construct normalized similarity metric and normalized distance metric from a similarity metric and a distance metric. Applying the general solutions to a given non-negative similarity metric and its induced family of distance metrics, we derive general normalized similarity metrics and normalized distance metrics. Finally we briefly discuss some of the applications of our general similarity and distance metric formulations.","Measurement,
Bioinformatics,
Mutual information,
Entropy,
Context,
Computer science,
Electronic mail"
Classification of tea samples using SVM as machine learning component of E-tongue,"This article introduces a new approach for identification of tea sample using pulse voltammetry method in an electronic tongue based instrumentation. The classifier system consists of a principle component (PCA) based feature extraction module followed by support vector machine based discrimination. The PCA score of unknown tea sample is undergone through different pair-wise (binary) classification using SVM for repeated times. For six different categories of tea samples in the present case, unknown sample is examined for fifteen times. The result of classification is six membership grades. Finally these membership grades are analyzed by decision directed acrylic graph method (DDAG) for decision making task about the exact authentication of unknown tea sample belonging to six categories. The proposed method could be equally followed for more than six categories.",
Human gesture classification by brute-force machine learning for exergaming in physiotherapy,"In this paper, a novel approach for human gesture classification on skeletal data is proposed for the application of exergaming in physiotherapy. Unlike existing methods, we propose to use a general classifier like Random Forests to recognize dynamic gestures. The temporal dimension is handled afterwards by majority voting in a sliding window over the consecutive predictions of the classifier. The gestures can have partially similar postures, such that the classifier will decide on the dissimilar postures. This brute-force classification strategy is permitted, because dynamic human gestures show sufficient dissimilar postures. Online continuous human gesture recognition can classify dynamic gestures in an early stage, which is a crucial advantage when controlling a game by automatic gesture recognition. Also, ground truth can be easily obtained, since all postures in a gesture get the same label, without any discretization into consecutive postures. This way, new gestures can be easily added, which is advantageous in adaptive game development. We evaluate our strategy by a leave-one-subject-out cross-validation on a self-captured stealth game gesture dataset and the publicly available Microsoft Research Cambridge-12 Kinect (MSRC-12) dataset. On the first dataset we achieve an excellent accuracy rate of 96.72%. Furthermore, we show that Random Forests perform better than Support Vector Machines. On the second dataset we achieve an accuracy rate of 98.37%, which is on average 3.57% better then existing methods.","Radio frequency,
Games,
Decision trees,
Gesture recognition,
Training,
Skeleton,
Vegetation"
Molecular Descriptors Selection and Machine Learning Approaches in Protein-Ligand Binding Affinity with Applications to Molecular Docking,"In this paper, we propose algorithms for biomolecular docking sites selection problem by various machine learning approaches with selective features reduction. The proposed method can reduce the number of various amino acid features before constructing machine learning prediction models. Given frame boxes with features, the proposed method analyzes the important features by correlation coefficients to LE values. The algorithm ranks these possible candidate locations on the receptor before launching AutoDock. Given a small molecular, namely ligand, it is a time-consuming task to compute the molecular docking against a large, relatively stationary molecule, or receptor. Our methods divide the surface area of receptor to several subspaces and evaluate these subspaces before choosing the promising subspaces to speed up the molecular docking simulation. The method is implemented upon the widely employed automated molecular docking simulation software package, AutoDock. The paper examines three different machine learning prediction models including the support vector machines (LIBSVM), deep neural networks (H2O), and the logistic regression model (AOD). The proposed affinity estimation algorithm, incorporated with a ligand-specific SVM prediction model, achieves about 4 folds faster comparing with original Autodock searching the whole surface of the receptor with similar binding energy score (LE, lowest engery) measurement. Furthermore, the proposed method can be easily parallelized in the implementation. Hadoop MapReduce frameworks are used in our experiments to parallelize the underlying massive computation works corresponding to ligand-receptor pairs examined under the experiment.","Proteins,
Correlation,
Support vector machines,
Computational modeling,
Predictive models,
Machine learning algorithms,
Prediction algorithms"
From measurement to machine learning: Towards analysing cognition,"This article discusses machine learning and BCI efforts of the BBCI team and co-workers with the general focus on analysing cognition. Due to the fact that many different aspects are reviewed, a high overlap to prior own contributions is not only unavoidable but intentional. When analysing cognition, it is often useful to combine information from various modalities (see e.g. Biessmann et al., 2011, Sui et al., 2012). In BCI recently muItimodal fusion concepts have received great attention under the label hybrid BCI (Pfurtscheller et al., 2010, Müller-Putz et al. 2015, Dähne et al. 2015, Fazli et al. 2012, 2015) or as data analysis technique for extracting (non-) linear relations between data (see e.g. Biessmann et al., 2011, Fazli et al., 2015, Dähne et al., 2015, Samek et al. 2016a). They are rooted in the modem machine leaming and signal processing techniques that are now available fot, analysing EEG, for decoding mental states etc. (see Müller et al. 2008, Bünau et al. 2009, Tomioka and Müller, 2010, Blankertz et al., 2008, 2011, 2016, Lemm et al., 2011, for recent reviews and contributions to Machine Learning for BCI, see Samek et al. 2014 for, a review on robust methods). Note that fusing information has also been a very common practice in the sciences and engineering (Waltz and Llinas, 1990). The talk will discuss recent technical work from the BBCI group that are intended to broaden the application spectrum of Brain Computer Interfaces. I will first report on a novel simuItaneous measurement method for NIRS and EEG that is miniaturized, wireless and has low noise properties (von Lühmann et al. 2016); it is well suited for performing future out-of-Iab BCI experiments, In addition, I will expand on technical advances in unsupervised ML-based BCI analysis that allows to use of label proportions and can thus helps to avoid the usual calibration phase (Hübner et al. 2016, cf. also Kindermans et al. 2014). Finally, if time permits, an application of the explanation framework fot, deep neural networks (Baehrens et al. 2010, Bach et al. 2015, Lapuschkin et al. 2016a and 2016b, Samek et al. 2016b, Montavon et al. 2016) to BCI data is given (Sturm et al. 2016). This abstract is based on joint work with Wojciech Samek, Benjamin Blankertz, Gabriel Curio, Michael Tangermann, Siamac Fazli, Vadim Nikulin, Gregoire Montavon, Sebastian Bach/Lapuschkin, Irene Sturm, Pieter-Jan Kindermans, Alex von Lühmann and many other members of the Berlin Brain Computer Interface team, the machine learning groups and many more esteemed collaborators, We greatly acknowledge funding by BMBF, EU, DFG and NRF.","Biological system modeling,
Brain modeling,
Computational modeling,
Neuroscience,
Monitoring,
Biomedical monitoring"
Fine-grained accelerators for sparse machine learning workloads,"Text analytics applications using machine learning techniques have grown in importance with ever increasing amount of data being generated from web-scale applications, social media and digital repositories. Apart from being large in size, these generated data are often unstructured and are heavily sparse in nature. The performance of these applications on current systems is hampered by hard to predict branches and low compute-per-byte ratio. This paper proposes a set of fine-grained accelerators that improve the performance and energy-envelope of these applications by an order of magnitude.","Engines,
Support vector machines,
Data structures,
Sparse matrices,
Kernel,
System-on-chip,
Data models"
Spendthrift: Machine learning based resource and frequency scaling for ambient energy harvesting nonvolatile processors,"Batteryless energy harvesting systems face a twofold challenge in converting incoming energy into forward progress. Not only must such systems contend with inherently weak and fluctuating power sources, but they have very limited temporal windows for capitalizing on transitory periods of above-average power. To maximize forward progress, such systems should aggressively consume energy when it is available, rather than optimizing for peak averagecase efficiency. However, there are multiple ways that a processor can trade between consumption and performance. In this paper, we examine two approaches, frequency scaling and resource scaling, and develop a predictor-driven scheme for dynamically allocating future power budgets between the two techniques. We show that our solution can achieve forward progress equal to 2.08X of the baseline Out-of-Order (OoO) processor with the best static configuration of frequency and resources. The combined technique outperforms either technique in isolation, with frequency-only and resource-only approaches achieving 1.43X and 1.61X forward progress improvements, respectively.","Biological neural networks,
Program processors,
Microarchitecture,
Energy storage,
Energy harvesting,
Computer architecture,
Power demand"
The rise of machine learning for big data analytics,"This paper addresses the rise of machine learning for big data analytics. First, machine learning and several terms related to machine learning are defined and explained in details and these terms include artificial intelligence, data mining, data science, data analytics and knowledge discovery, statistics and Business Intelligence. These definitions will show how these terms are inter-related to each other. Then, the definition of big data is outlined based on three terms: Volume, Velocity and Variety. Implementing a good big data strategy is very crucial in order to guarantee the success of applying machine learning for learning big data. As a result, the trending in Big Data is also illustrated and defined based on the landscape of big data; Infrastructure, Analytics, Applications, Cross-Infrastructures/Analytics, Open Sources, Data Sources and API, Incubators and Schools. This paper also addresses some of the open source facilities that are available for public in order to ensure that large scale of machine learning application can be realized. Finally, in conclusion, the big trend over the last few months in Big Data analytics has been the increasing focus on artificial intelligence to help analyze massive amounts of data and derive predictive insights. AI/machine learning is now precipitating a trend towards the emergence of the application layer of Big Data. The combination of Big Data and AI will drive incredible innovation across pretty much every industry. From that perspective, the Big Data opportunity is probably even bigger than people thought.","data mining,
machine learning,
big data,
data analytics,
artificial intelligence,
data science"
Neuromorphic accelerators: A comparison between neuroscience and machine-learning approaches,"A vast array of devices, ranging from industrial robots to self-driven cars or smartphones, require increasingly sophisticated processing of real-world input data (image, voice, radio, ...). Interestingly, hardware neural network accelerators are emerging again as attractive candidate architectures for such tasks. The neural network algorithms considered come from two, largely separate, domains: machine-learning and neuroscience. These neural networks have very different characteristics, so it is unclear which approach should be favored for hardware implementation. Yet, few studies compare them from a hardware perspective. We implement both types of networks down to the layout, and we compare the relative merit of each approach in terms of energy, speed, area cost, accuracy and functionality. Within the limit of our study (current SNN and machine learning NN algorithms, current best effort at hardware implementation efforts, and workloads used in this study), our analysis helps dispel the notion that hardware neural network accelerators inspired from neuroscience, such as SNN+STDP, are currently a competitive alternative to hardware neural networks accelerators inspired from machine-learning, such as MLP+BP: not only in terms of accuracy, but also in terms of hardware cost for realistic implementations, which is less expected. However, we also outline that SNN+STDP carry potential for reduced hardware cost compared to machine-learning networks at very large scales, if accuracy issues can be controlled (or for applications where they are less important). We also identify the key sources of inaccuracy of SNN+STDP which are less related to the loss of information due to spike coding than to the nature of the STDP learning algorithm. Finally, we outline that for the category of applications which require permanent online learning and moderate accuracy, SNN+STDP hardware accelerators could be a very cost-efficient solution.","Hardware,
Neurons,
Biological neural networks,
Neuroscience,
Encoding,
Handwriting recognition,
Artificial neural networks"
The role of machine learning in botnet detection,Over the past ten to fifteen years botnets have gained the attention of researchers worldwide. A great deal of effort has been given to developing systems that would efficiently and effectively detect the presence of a botnet. This unique problem saw researchers applying machine learning (ML) to solve this problem. In this paper we provide a brief overview the different machine learning (ML) methods and the part they play in botnet detection. The main aim of this paper is to clearly define the role different ML methods play in Botnet detection. A clear understanding of these roles are critical for developing effective and efficient real-time online detection approaches and more robust models.,Servers
Machine learning classification model for Network based Intrusion Detection System,"With an enormous increase in number of mobile users, mobile threats are also growing rapidly. Mobile malwares can lead to several cybersecurity threats i.e. stealing sensitive information, installing backdoors, ransomware attacks and sending premium SMSs etc. Previous studies have shown that due to the sophistication of threats and tailored techniques to avoid detection, not every antivirus system is capable of detecting advance threats. However, an extra layer of security at the network side can protect users from these advanced threats by analyzing the traffic patterns. To detect these threats, this paper proposes and evaluates, a Machine Learning (ML) based model for Network based Intrusion Detection Systems (NIDS). In this research, several supervised ML classifiers were built using data-sets containing labeled instances of network traffic features generated by several malicious and benign applications. The focus of this research is on Android based malwares due to its global share in mobile malware and popularity among users. Based on the evaluation results, the model was able to detect known and unknown threats with the accuracy of up to 99.4%. This ML model can also be integrated with traditional intrusion detection systems in order to detect advanced threats and reduce false positives.","Malware,
Intrusion detection,
Mobile communication,
Smart phones,
IP networks,
Feature extraction"
Code review analysis of software system using machine learning techniques,"Code review is systematic examination of a software system's source code. It is intended to find mistakes overlooked in the initial development phase, improving the overall quality of software and reducing the risk of bugs among other benefits. Reviews are done in various forms such as pair programming, informal walk-through, and formal inspections. Code review has been found to accelerate and streamline the process of software development like very few other practices in software development can. In this paper we propose a machine learning approach for the code reviews in a software system. This would help in faster and a cleaner reviews of the checked in code. The proposed approach is evaluated for feasibility on an open source system eclipse. [1], [2], [3].",
Analysis framework for machine learning experiments based on classifier combination for petrographic images,"The analysis and description of rocks is very well useful in geological industry and also in rock mining. Igneous rock is most abandoned rock in nature. The classification of Igneous rocks in its two types namely Plutonic and Volcanic is a complex and tedious job because of homogeneity between the classes. In geology this classification process has been carried out on the manual basis regularly, requiring high geological expertise. But being done manually can subject to error and leads to unnecessary delay. Hence for automating the process, pattern classification approach is used. In this paper a locally relevant database of igneous rocks involving both the rock types is considered for experimentation and testing. The grain size is considered as a discriminating textural feature for this classification problem followed by identification statistical features for textural discrimination such as Haralick features, and Laws Masks. A Radial basis function support vector machine classifier is used for the classification providing machine learning approach and giving reasonable results. But In Igneous rocks, the image classes are overlapping in the feature space. So the Classification accuracy can be further increased if the hypothesis of the multiple Support vector machine (SVM) image classifiers are combined to give a single hypothesis for the classification of an image. A method is presented for combining different base classifiers i.e. Adaboost technique.","Support vector machines,
Rocks,
Training,
Kernel,
Databases,
Classification algorithms"
Computer aided analysis of cognitive disorder in patients with Parkinsonism using machine learning method with multilevel ROI-based features,"This paper proposes to use multilevel ROI-based features and machine learning method to improve the accuracy of qualitative recognition of mild cognitive disorder in parkinsonism. 77 Parkinson's patients and 32 normal controls with neuropsychological assessments and structural magnetic resonance images from the Parkinson's Progression Markers Initiative dataset are tested. Specifically, the BrainLab software is used to process images and measure volume of gray matter, thickness of the cortex, and surface area of the cortex at each region of interest (ROI). We utilize t-test, support vector machine (SVM), and minimum redundancy and maximum relevance (mRMR) methods conjunctively to select features and get the optimal features and the classifier. The experimental results reveal that our method with multilevel ROI-based features gives significant improvement of the classification performance compared with other methods using single-level ROI-based features (i.e., using only volume of gray matter, thickness of cortex, or surface area of cortex).","Feature extraction,
Support vector machines,
Parkinson's disease,
Dementia,
Learning systems,
Surface morphology"
Automated weather event analysis with machine learning,"Weather forecasting has numerous impacts in our daily life from cultivation to event planning. Previous weather forecasting models used the complicated blend of mathematical instruments which was insufficient in order to get higher classification rate. In contrast, simple analytical models are well-suited for weather forecasting tasks. In this work, we focus on the weather forecasting by means of classifying different weather events such as normal, rain, and fog by applying comprehensible C4.5 learning algorithm on weather and climate features. The C4.5 classifier classifies weather events by building the decision tree using information entropy from the set of training samples. We conducted experiments on LA weather history dataset; from evaluation results, it is revealed that C4.5 classifier classifies weather events with f-score of around 96.1%. This model also indicates that climate features such as rainfall, visibility, temperature, humidity, and wind speed are highly discriminative toward events classification.","Ocean temperature,
History,
Weather forecasting,
Rain,
Atmospheric modeling,
Predictive models"
Machine Learning for Noise Sensor Placement and Full-Chip Voltage Emergency Detection,"Power supply fluctuation can be potential threat to the correct operations of processors, in the form of voltage emergency that happens when supply voltage drops below a certain threshold. Noise sensors (with either analog or digital outputs) can be placed in the nonfunction area of processors to detect voltage emergencies by monitoring the runtime voltage fluctuations. Our work addresses two important problems related to building a sensor-based voltage emergency detection system: 1) offline sensor placement, i.e., where to place the noise sensors so that the number and locations of sensors are optimized in order to strike a balance between design cost and chip reliability and 2) online voltage emergency detection, i.e., how to use these placed sensors to detect voltage emergencies in the hotspot locations. In this paper, we propose integrated solutions to these two problems, respectively, for analog and digital (more specifically, binary) sensor outputs, by exploiting the voltage correlation between the sensor candidate locations and the hotspot locations. For the analog case, we use the Group Lasso and an ordinary least squares approach; for the binary case, we integrate the Group Lasso and the SVM approach. Experimental results show that, our approach can achieve 2.3X-2.7X better voltage emergency detection results on average for analog outputs when compared to the state-of-the-art work; and for the binary case, on average our methodology can achieve up to 21% improvement in prediction accuracy compared to an approach called max-probability-no-prediction.","Support vector machines,
Predictive models,
Electronic mail,
Buildings,
System-on-chip,
Training,
Sun"
"A novel application of machine learning techniques for activity-based load disaggregation in rural off-grid, isolated solar systems","In power systems and electricity markets, accurate monitoring and prediction of electricity demand is important in order to manage real-time load balancing and for distribution and transmission planning. In the context of rural electrification, the uncertainty of both supply - often with 100% renewables - and demand is large and the central reason that solar-based micro-grids and home systems can be expensive. Oversizing battery storage and solar panel size or limiting the electricity available to each user can alleviate this uncertainty but increases system costs. Historical data for solar irradiance is available and allows quantification of the uncertainty associated with supply. However, electricity demand in rural households is not well characterized in developing world contexts. In an ideal scenario, the data should be captured on a per-household level and must be measured on a sufficiently fine time resolution to translate the demand onto individual, user activities. This massive quantity of household demand data requires automatic tools to convert time series power usage data into data on the use of individual appliances. This paper presents a methodology, based on data acquired in individual, isolated solar home systems in Jharkhand, India, on utilizing classification and clustering algorithms to create activity-based models that can be used to conduct load forecasts. Additional statistical data analysis can yield insights on users' power consumption behavior in relation to exogenous variables such as time of day and conditioned on ambient air temperature.","Decision support systems,
Conferences,
Layout,
Voltage measurement,
Home appliances"
An novel spectrum sensing scheme combined with machine learning,"Cognitive radio (CR) network technology is widely used as a approach to the solve the scarce radio spectrum by allowing the unlicensed users to access the licensed spectrum. Since in the CR network, the licensed users are easily be affected by the introduce of the unlicensed user, we have to avoid to bring the interference to the licensed users when the unlicensed users try to transmit the data on the licensed radio spectrum. It is very difficult to solve this problem especially when the licensed users are mobile. The situation becomes even worse when the distribution of the licensed users is unknown. In this paper, in order to improve the throughput under the mobile network, we propose a novel algorithm which combines the random forest to decrease the interference of the unlicensed user to the licensed users, thus, the network throughput can be dramatically improved. The simulation results show that our proposed novel algorithm has good performance in improving the mobile network throughput.","Sensors,
Algorithm design and analysis,
Throughput,
Mobile communication,
Interference,
Data communication,
Mobile computing"
Performance analysis of machine learning and pattern recognition algorithms for Malware classification,"Anti-Malware industry faces the challenge of evaluating huge amount of data for potential malicious contents. This is due to the fact that hackers introduce polymorphism to the existing malicious groups/classes. Effective feature extraction and classification of malware data is necessary to tackle such issues. In this paper, we visualize viruses in an image as they capture minor changes while retaining a global structure. Later, we implement Principal Component Analysis (PCA) method for feature extraction. Based on extracted PCA features, we study the performance of various Artificial Neural Network (ANN) algorithms along with K-Nearest Neighbors (kNN) and Support Vector Machine (SVM) classification techniques for identification of malware data into their respective classes. We use k-fold validation to gauge the effectiveness of our approach. The study makes use of the publicly available Kaggle database provided by Microsoft for the Microsoft Malware Classification Challenge (BIG 2015).","Malware,
Training,
Principal component analysis,
Support vector machines,
Artificial neural networks,
Feature extraction,
Data visualization"
Data preparation step for automated diagnosis based on HRV analysis and machine learning,"This paper describes the data preparation step of a proposed method for automated diagnosis of various diseases based on heart rate variability (HRV) analysis and machine learning. HRV analysis - consisting of time-domain analysis, frequency-domain analysis, and nonlinear analysis - is employed because its resulting parameters are unique for each disease and can be used as the statistical symptoms for each disease, while machine learning techniques are employed to automate the diagnosis process. The input data consist of electrocardiogram (ECG) recordings. The proposed method is divided into three main steps, namely dataset preparation step, machine learning step, and disease classification step. The dataset preparation step aims to prepare the training data for machine learning step from raw ECG signals, and to prepare the test data for disease classification step from raw RRI signals. The machine learning step aims to obtain the classifier model and its performance metric from the prepared dataset. The disease classification step aims to perform disease diagnosis from the prepared dataset and the classifier model. The implementation of data preparation step is subsequently described with satisfactory result.","Heart rate variability,
Electrocardiography,
Rail to rail inputs,
Diseases,
Frequency-domain analysis,
Pregnancy,
Time-domain analysis"
Probability density estimation of photometric redshifts based on machine learning,"Photometric redshifts (photo-z's) provide an alternative way to estimate the distances of large samples of galaxies and are therefore crucial to a large variety of cosmological problems. Among the various methods proposed over the years, supervised machine learning (ML) methods capable to interpolate the knowledge gained by means of spectroscopical data have proven to be very effective. METAPHOR (Machine-learning Estimation Tool for Accurate PHOtometric Redshifts) is a novel method designed to provide a reliable PDF (Probability density Function) of the error distribution of photometric redshifts predicted by ML methods. The method is implemented as a modular workflow, whose internal engine for photo-z estimation makes use of the MLPQNA neural network (Multi Layer Perceptron with Quasi Newton learning rule), with the possibility to easily replace the specific machine learning model chosen to predict photo-z's. After a short description of the software, we present a summary of results on public galaxy data (Sloan Digital Sky Survey - Data Release 9) and a comparison with a completely different method based on Spectral Energy Distribution (SED) template fitting.","Training,
Estimation,
Probability density function,
Photometry,
Radio frequency,
Neural networks,
Knowledge based systems"
Computational intelligence based machine learning methods for rule-based reasoning in computer vision applications,"In robot control, rule discovery for understanding of data is of critical importance. Basically, understanding of data depends upon logical rules, similarity evaluation and graphical methods. The expert system collects training examples separately by exploring an anonymous environment by using machine learning techniques. In dynamic environments, future actions are determined by sequences of perceptions thus encoded as rule base. This paper is focused on demonstrating the extraction and application of logical rules for image understanding, using newly developed Synergistic Fibroblast Optimization (SFO) algorithm with well-known existing artificial learning methods. The SFO algorithm is tested in two modes: Michigan and Pittsburgh approach. Optimal rule discovery is evaluated by describing continuous data and verifying accuracy and error level at optimization phase. In this work, Monk's problem is solved by discovering optimal rules that enhance the generalization and comprehensibility of a robot classification system in classifying the objects from extracted attributes to effectively categorize its domain.","Fibroblasts,
Optimization,
Classification algorithms,
Training,
Machine learning algorithms,
Algorithm design and analysis,
Sociology"
Multi-objective parameter configuration of machine learning algorithms using model-based optimization,"The performance of many machine learning algorithms heavily depends on the setting of their respective hyperparameters. Many different tuning approaches exist, from simple grid or random search approaches to evolutionary algorithms and Bayesian optimization. Often, these algorithms are used to optimize a single performance criterion. But in practical applications, a single criterion may not be sufficient to adequately characterize the behavior of the machine learning method under consideration and the Pareto front of multiple criteria has to be considered. We propose to use model-based multi-objective optimization to efficiently approximate such Pareto fronts.","Optimization,
Machine learning algorithms,
Algorithm design and analysis,
Tuning,
Prediction algorithms,
Numerical models,
Support vector machines"
A fuzzy-based machine learning model for robot prediction of link quality,"With foresight into the state of the wireless channel, a robot can make various optimization decisions with regards to routing packets, planning mobility paths, or switching between diverse radios. However, the process of predicting link quality (LQ) is nontrivial due to the streaming and dynamic nature of radio wave propagation, which is complicated by robot mobility. Due to robot movement, the wireless propagation environment can change considerably in terms of distance, obstacles, noise, and interference. Therefore, LQ must be learned and regularly updated while the robot is online. However, the existing fuzzy-based models for assessing LQ are non-adaptable due to the absence of any learning mechanism. To address this issue, we introduce a fuzzy-based prediction model designed for the efficient online and incremental learning of LQ. The unique approach uses fuzzy logic to infer LQ based on the collective output from a series of offset classifiers and their posterior probabilities. In essence, the proposed model leverages machine learning for extracting the underlying functional relationship between the input and output variables, but deeper inferences are made from the output of the learning algorithms using fuzzy logic. Wireless link data from a real-world robot network was used to compare the model with the traditional linear regression approach. The results show statistically significant improvements in three out of the six real-world indoor and outdoor environments where the robot operated. Additionally, the novel approach offers a number of other benefits, including the flexibility to use fuzzy logic for model tuning, as well as the ability to make implementation efficiencies in terms of parallelization and the conservation of labeling resources.","Robots,
Predictive models,
Wireless communication,
Wireless sensor networks,
Measurement,
Adaptation models,
Fuzzy sets"
Stochastic performance tuning of complex simulation applications using unsupervised machine learning,"Machine learning for complex multi-objective problems (MOP) can substantially speedup the discovery of solutions belonging to Pareto landscapes and improve Pareto front accuracy. Studying convergence speedup of multi-objective search on well-known benchmarks is an important step in the development of algorithms to optimize complex problems such as High Energy Physics particle transport simulations. In this paper we will describe how we perform this optimization via a tuning based on genetic algorithms and machine learning for MOP. One of the approaches described is based on the introduction of a specific multivariate analysis operator that can be used in case of expensive fitness function evaluations, in order to speed-up the convergence of the “black-box” optimization problem.","Genetic algorithms,
Sociology,
Optimization,
Principal component analysis,
Convergence,
Covariance matrices"
Robust channel coding strategies for machine learning data,"Two important recent trends are the proliferation of learning algorithms along with the massive increase of data stored on unreliable storage mediums. These trends impact each other; noisy data can have an undesirable effect on the results provided by learning algorithms. Although traditional tools exist to improve the reliability of data storage devices, these tools operate at a different abstraction level and therefore ignore the data application, leading to an inefficient use of resources. In this paper we propose taking the operation of learning algorithms into account when deciding how to best protect data. Specifically, we examine several learning algorithms that operate on data that is stored on noisy mediums and protected by error-correcting codes with a limited budget of redundancy; we develop a principled way to allocate resources so that the harm on the output of the learning algorithm is minimized.","Encoding,
Machine learning algorithms,
Redundancy,
Maximum likelihood decoding,
Prediction algorithms,
Robustness"
A review on respiratory sound analysis using machine learning,"Auscultation of the respiratory sounds is an inexpensive and effective method for diagnosing cardio-pulmonary disorders using lung sounds from chest and back. Nowadays, high system performances in the management of robust processes that require great attention were increased using the computer-aided analysis methods and the developments of the diagnosis system. Analysis of the respiratory sounds with computer-aided systems allows objective and useful assessments. In this study, a brief description of the abnormal respiratory sounds was presented. The main aims of the study are performing a systematic review about methods and the machine learning algorithms that are used to classify the abnormal respiratory sounds for diagnosis of cardio-pulmonary disorders and evaluating the development of possible methods on respiratory sounds in the future.","Lungs,
Hidden Markov models,
Diseases,
Conferences,
Markov processes,
Biomedical engineering,
Signal processing"
Partitioning streaming parallelism for multi-cores: A machine learning based approach,"Stream based languages are a popular approach to expressing parallelism in modern applications. The efficient mapping of streaming parallelism to multi-core processors is, however, highly dependent on the program and underlying architecture. We address this by developing a portable and automatic compiler-based approach to partitioning streaming programs using machine learning. Our technique predicts the ideal partition structure for a given streaming application using prior knowledge learned off-line. Using the predictor we rapidly search the program space (without executing any code) to generate and select a good partition. We applied this technique to standard StreamIt applications and compared against existing approaches. On a 4-core platform, our approach achieves 60% of the best performance found by iteratively compiling and executing over 3000 different partitions per program. We obtain, on average, a 1.90x speedup over the already tuned partitioning scheme of the StreamIt compiler. When compared against a state-of-the-art analytical, model-based approach, we achieve, on average, a 1.77x performance improvement. By porting our approach to a 8-core platform, we are able to obtain 1.8x improvement over the StreamIt default scheme, demonstrating the portability of our approach.",
Comparative study of machine learning algorithms for activity recognition with data sequence in home-like environment,"Activity recognition is a key problem in multisensor systems. With data collected from different sensors, a multi-sensor system identifies activities performed by the inhabitants. Since an activity always lasts a certain duration, it is beneficial to use data sequence for the desired recognition. In this work, we experiment several machine learning techniques, including Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU) and Meta-Layer Network for solving this problem. We observe that (1) compare with “single-frame” activity recognition, data sequence based classification gives better performance; and (2) directly using data sequence information with a simple “mete layer” network model yields a better performance than memory based deep learning approaches.","Activity recognition,
Training,
Recurrent neural networks,
TV,
Logic gates,
Testing,
Computational modeling"
Interpretable machine learning via convex cardinal shape composition,"For safety reasons, the interpretability of models is important in consequential applications of supervised classification in which predictions are used to support human decision makers. In this paper, we extend cardinal shape composition, a new method developed in the image processing and computer vision literature for image segmentation, to general machine learning problems. Our transformation results in a computationally-tractable ℓ1-regularized hinge loss optimization over a shape dictionary. This approach yields human-interpretable models with an appropriate choice of atomic shapes in the dictionary used to compose decision boundaries.","Shape,
Dictionaries,
Image segmentation,
Fasteners,
Kernel,
Safety,
Bayes methods"
Using machine learning to predict hypertension from a clinical dataset,"Hypertension is an illness that often leads to severe and life threatening diseases such as heart failure, thickening of the heart muscle, coronary artery disease, and other severe conditions if left untreated. An artificial neural network is a powerful machine learning technique that allows prediction of the presence of the disease in susceptible populations while removing the potential for human error. In this paper, we identify the important risk factors based on patients' current health conditions, medical records, and demographics. These factors are then used to predict the presence of hypertension in an individual. These risk factors are also indicative of the probability of a person developing hypertension in the future and can, therefore, be used as an early warning system. We present a neural network model for predicting hypertension with about 82% accuracy. This is good performance given our chosen risk factors as inputs and the large integrated data used for the study. Our network model utilizes very large sample sizes (185,371 patients and 193,656 controls) from the Canadian Primary Care Sentinel Surveillance Network (CPCSSN) data set. Finally, we present a literature study to show the use of these risk factors in other works along with experimental results obtained from our model.","Hypertension,
Mathematical model,
Diseases,
Blood pressure,
Predictive models,
Medical diagnostic imaging,
Neural networks"
Dynamic prediction of drivers' personal routes through machine learning,"Personal route prediction (PRP) has attracted much research interest recently because of its technical challenges and broad applications in intelligent vehicle and transportation systems. Traditional navigation systems generate a route for a given origin and destination based on either shortest or fastest route schemes. In practice, different people may very likely take different routes from the same origin to the same destination. Personal route prediction attempts to predict a driver's route based on the knowledge of driver's preferences. In this paper we present an intelligent personal route prediction system, I_PRP, which is built based upon a knowledge base of personal route preference learned from driver's historical trips. The I_PRP contains an intelligent route prediction algorithm based on the first order Markov chain model to predict a driver's intended route for a given pair of origin and destination, and a dynamic route prediction algorithm that has the capability of predicting driver's new route after the driver departs from the predicted route.","Vehicles,
Prediction algorithms,
Knowledge based systems,
Heuristic algorithms,
Machine learning algorithms,
Global Positioning System,
Roads"
Machine learning paradigms for weed mapping via unmanned aerial vehicles,"This paper presents a novel strategy for weed monitoring, using images taken with unmanned aerial vehicles (UAVs) and concepts of image analysis and machine learning. Weed control in precision agriculture designs site-specific treatments based on the coverage of weeds, where the key is to provide precise weed maps timely. Most traditional remote platforms, e.g. piloted planes or satellites, are, however, not suitable for early weed monitoring, given their low temporal and spatial resolutions, as opposed to he ultra-high spatial resolution of UAVs. The system here proposed makes use of UAV-imagery and is based on: 1) Divide the image, 2) compute and binarise the vegetation indexes, 3) detect crop rows, 4) optimise the parameters and 4) learn a classification model. Since crops are usually organised in rows, the use of a crop row detection algorithm helps to separate properly weed and crop pixels, which is a common handicap given the spectral similitude of both. Several artificial intelligence paradigms are compared in this paper to identify the most suitable strategy for this topic (i.e. unsupervised, supervised and semi-supervised approaches). Our experiments also study the effect of different parameteres: the flight altitude, the sensor and the use of previously trained models at a different height. Our results show that 1) very promising performance can be obtained, even when using very few labelled data and 2) the classification model can be learnt in a subplot of the experimental field at low altitude and then applied to the whole field at a higher height, which simplifies the whole process. These results motivate the use of this strategy to design weed monitoring strategies for early post-emergence weed control.","Agriculture,
Vegetation mapping,
Indexes,
Monitoring,
Spatial resolution,
Remote sensing,
Cameras"
Mining complex hyperspectral ALMA cubes for structure with neural machine learning,"Astronomy is producing the largest “Big Data” sets today and in the near future, with instruments such as the Atacama Large Millimeter and sub-millimeter Array (ALMA), the Large Synoptic Survey Telescope (LSST), and the Square Kilometer Array (SKA). These observations afford a deeper, wider, and more dynamic glimpse into the structure and composition of the universe than ever before. However, in addition to unprecedented volume, the data also exhibit unprecedented complexity, mandating new approaches for extracting and summarizing relevant information. ALMA data, in particular, challenges with very high dimensionality (measurements in a large number of spectral channels) where the dimensions represent both compositional information and velocities, and the high spectral resolution allows detailed interpretation of the kinematic structure of sources such as molecular clouds or protoplanetary disks. Traditional tools like moment maps can no longer fully exploit and visualize the rich information in these data. We present a neural map-based clustering approach that can utilize all spectral channels simultaneously and is capable of finding clusters of widely varying statistical properties, which are expected in these complex data sets. Many clustering methods, including modern graph segmentation algorithms, run into limitations when encountering such data. We demonstrate our tools, collectively named “NeuroScope”, through structure mining from an ALMA image of the protoplanetary disk HD142527. We highlight the advantages for both the emerging details and visualization. In addition, we explore an augmentation of leading graph segmentation algorithms with NeuroScope products, which can lead to efficient full automation of our clustering process for fast distillation of large data sets on-board or in archives.","Prototypes,
Spatial resolution,
Neurons,
Kinematics,
Manifolds,
Lattices"
Coke optimization with machine learning method in sinter plant,"Sintered product which is the largest entry of blast furnace is produced in the sinter plant which is one of the main unit of integrated iron and steel factories. Limestone and dolomite are added to fine ore in the sinter plant, and this mixture is sent to the blast furnace after smelted with coke fuel. The use of coke powder in the correct ratio is very important in terms of maintain thermal equilibrium in sinter plant and controlling raw material costs. In this study, a coke powder ratio prediction model is established with multiple linear regression and support vector machines by using coke analysis information, raw material and process data of İskenderun Iron & Steel Plant (ISDEMIR) sinter plant. Both methods reached 77% accuracy rate in this study that has 70 sample data have been collected for 20 months.","Art,
Blast furnaces,
Iron,
Steel,
Powders,
Raw materials,
Optimization"
Detecting malicious URLs using machine learning techniques,"The World Wide Web supports a wide range of criminal activities such as spam-advertised e-commerce, financial fraud and malware dissemination. Although the precise motivations behind these schemes may differ, the common denominator lies in the fact that unsuspecting users visit their sites. These visits can be driven by email, web search results or links from other web pages. In all cases, however, the user is required to take some action, such as clicking on a desired Uniform Resource Locator (URL). In order to identify these malicious sites, the web security community has developed blacklisting services. These blacklists are in turn constructed by an array of techniques including manual reporting, honeypots, and web crawlers combined with site analysis heuristics. Inevitably, many malicious sites are not blacklisted either because they are too recent or were never or incorrectly evaluated. In this paper, we address the detection of malicious URLs as a binary classification problem and study the performance of several well-known classifiers, namely Naïve Bayes, Support Vector Machines, Multi-Layer Perceptron, Decision Trees, Random Forest and k-Nearest Neighbors. Furthermore, we adopted a public dataset comprising 2.4 million URLs (instances) and 3.2 million features. The numerical simulations have shown that most classification methods achieve acceptable prediction rates without requiring either advanced feature selection techniques or the involvement of a domain expert. In particular, Random Forest and Multi-Layer Perceptron attain the highest accuracy.","Uniform resource locators,
Security,
Decision trees,
Electronic mail,
Vegetation,
Bayes methods,
Support vector machines"
EPOC aware energy expenditure estimation with machine learning,"In 2014, 39 % of adults were overweight, and 13 % were obese. Clearly, knowing exact energy expenditure (EE) is important for sports training and weight control. Furthermore, excess post-exercise oxygen consumption (EPOC) must be included in the total EE. This paper presents a machine learning-based EE estimation approach with EPOC for aerobic exercise using a heart rate sensor. On a dataset acquired from 33 subjects, we apply machine learning algorithms using Weka machine learning toolkit. We could achieve 0.88 correlation and 0.23 kcal/min root mean square error (RMSE) with linear regression. The proposed model could be applied to various wearable devices such as a smartwatch.",
Machine learning algorithm for retinal image analysis,"Diabetic retinopathy is the most general diabetes complication that affects eyes and results in blindness. It's due to impairment of the arteries a veins located in the fundus of eye (retina) that are composed of light sensitive tissues. The aim of this research work is to design an efficient and sensitive tool for Diabetic Retinopathy using the images acquired from portable fundus camera. The screening tool is based on advanced machine learning and computer vision algorithm which includes patch level prediction. In patch level prediction algorithm will localize the diseased region in the Diabetic Retinopathy image like Hard Exudates and Hemorrhage. The patch level classification uses Support Vector Machine (SVM) machine learning classifier model to predict the potential patch of Hard Exudates and Hemorrhage. In this algorithm, the image is broken into regular rectangular patch. The feature for each patch along with the different class label based on the ground truth is computed and passed to strong classifier SVM. The data sets are split into training dataset and testing dataset. The classifier model is built on training dataset and tested against the test dataset. The performance results of rectangular patch level prediction using SVM the average performance for Hard Exudates was Accuracy 96 %, Sensitivity 94%, Specificity 96%. The average performance for Hemorrhage was Accuracy 85 %, Sensitivity 77%, and Specificity 85%.","Diabetes,
Support vector machines,
Hemorrhaging,
Retinopathy,
Feature extraction,
Retina,
Sensitivity"
Principal component selection of machine learning algorithms based on orthogonal transformation by using interactive evolutionary computation,"We propose a method to solve the selection problem of principal components in machine learning algorithms based on orthogonal transformation by using interactive evolutionary computation. One of the addressed subjects for machine learning algorithms based on orthogonal transformation is how to decide the number of principal components, and which of the principal components should be used to reconstruct the original data. In this work, we use the interactive differential evolution algorithm to study these subjects by using real humans' subjective evaluation in an optimization process. An image compression problem using principal component analysis is introduced to study the proposed method. From the evaluation, we do not only solve the selection problem of principal components for machine learning algorithms based on orthogonal transformation, but also can analyse the human aesthetical characteristics on visual perception and feature selection from the designed method and experimental evaluation. We also discuss and analyse potential research subjects and some open topics, which are invited to further investigate.","Principal component analysis,
IEC,
Machine learning algorithms,
Image coding,
Optimization,
Evolutionary computation,
Conferences"
A reinforcement learning approach for dynamic selection of virtual machines in cloud data centres,"In recent years Machine Learning techniques have proven to reduce energy consumption when applied to cloud computing systems. Reinforcement Learning provides a promising solution for the reduction of energy consumption, while maintaining a high quality of service for customers. We present a novel single agent Reinforcement Learning approach for the selection of virtual machines, creating a new energy efficiency practice for data centres. Our dynamic Reinforcement Learning virtual machine selection policy learns to choose the optimal virtual machine to migrate from an over-utilised host. Our experiment results show that a learning agent has the abilities to reduce energy consumption and decrease the number of migrations when compared to a state-of-the-art approach.","Learning (artificial intelligence),
Cloud computing,
Energy consumption,
Virtual machining,
Resource management,
Quality of service,
Servers"
Analysis of machine learning solutions to detect malware in android,"The recent use of mobile devices and increase in connectivity technologies(GSM, GPRS, Bluetooth & WiFi enable us to access abundant services. These services and communication channels are exploited by susceptibilities immensely. Hence, for malware writers, mobile devices became ideal target. Applications installed on smartphones request access to the sensitive information which may lead to security vulnerabilities. Different malwares named as Botnet, Backdoor, Rootkits, Virus, Worms, and Trojans can attack android Operating System (OS). Due to these attacks privacy of the users is compromised. This paper surveys the already proposed security solutions by using machine learning approaches especially focused on supervised, semi supervised and unsupervised approaches. We also analyzed the architecture of these approaches and present the taxonomy of Android OS based security solutions. Our aim is to provide the best approach for malware detection in Android OS.","Malware,
Smart phones,
Feature extraction,
Androids,
Humanoid robots,
Machine learning algorithms,
Computer architecture"
Intelligent Forwarding Strategy Based on Online Machine Learning in Named Data Networking,"The content-oriented model of Named Data Networking (NDN) allows consumers to pay more attention to the targeting data itself instead of the location of where the data is stored. Different from IP, NDN has a unique feature that forwarding plane enables each router to select the next forwarding hop independently without relying on routing. Therefore, forwarding strategies play a significant role for adaptive and efficient data transmission in NDN. Existing forwarding strategies are not smart enough to cope with the complexity of network and diversity of application demands. This paper presents an intelligent forwarding strategy, which integrates online machine learning method into the optimization of interface probabilities during forwarding process. Originally, a probabilistic binary tree structure is proposed to abstract the forwarding process as a path selection process traversing from the root node to the leaf node, which provides theoretical support for machine learning and reduces the complexity of forwarding process. In addition, we improved our strategy to prevent the convergence into limited local optimal solution by adopting the idea of simulated an nealing. Experimental results show that the proposed strategy can reduce time complexity, as well as achieve higher throughput, better load balance and lower packet drop rates in comparison with other existing forwarding strategies. The drop rates are reduced by 60% and 34% respectively in different scenarios compared with BestRoute, a strategy widely used in NDN.","Probabilistic logic,
Binary trees,
Routing,
Time complexity,
Optimization,
IP networks"
Namatad: Inferring occupancy from building sensors using machine learning,"Driven by the need to improve efficiency, modern buildings are instrumented with numerous sensors to monitor utilization and regulate environmental conditions. While these sensor systems serve as valuable tools for managing the comfort and health of occupants, there is an increasing need to expand the deployment of sensors to provide additional insights. Because many of these desired insights have high temporal value, such as occupancy during emergency situations, such insights are needed in real time. However, augmenting buildings with new sensors is often expensive and requires a significant capital investment. In this paper, we propose and describe the real-time, streaming system called Namatad that we developed to infer insights from many sensors typical of Internet of Things (IoT) deployments. We evaluate the effectiveness of this platform by leveraging machine learning to infer new insights from environmental sensors within buildings. We describe how we built the components of our system leveraging several open source, streaming frameworks. We also describe how we ingest and aggregate from building sensors and sensing platforms, route data streams to appropriate models, and make predictions using machine learning techniques. Using our system, we have been able to predict the occupancy of rooms within a building on the University of Washington campus over the last three months, in real time, at accuracies of up to 95%.","sensors,
building management systems,
control engineering computing,
Internet of Things,
learning (artificial intelligence),
public domain software"
An adaptive learning method of Restricted Boltzmann Machine by neuron generation and annihilation algorithm,"Restricted Boltzmann Machine (RBM) is a generative stochastic energy-based model of artificial neural network for unsupervised learning. Recently, RBM is well known to be a pre-training method of Deep Learning. In addition to visible and hidden neurons, the structure of RBM has a number of parameters such as the weights between neurons and the coefficients for them. Therefore, we may meet some difficulties to determine an optimal network structure to analyze big data. In order to evade the problem, we investigated the variance of parameters to find an optimal structure during learning. For the reason, we should check the variance of parameters to cause the fluctuation for energy function in RBM model. In this paper, we propose the adaptive learning method of RBM that can discover an optimal number of hidden neurons according to the training situation by applying the neuron generation and annihilation algorithm. In this method, a new hidden neuron is generated if the energy function is not still converged and the variance of the parameters is large. Moreover, the inactivated hidden neuron will be annihilated if the neuron does not affect the learning situation. The experimental results for some benchmark data sets were discussed in this paper.","Neurons,
Training,
Learning systems,
Convergence,
Biological neural networks,
Machine learning,
Adaptation models"
Software design pattern recognition using machine learning techniques,"Design patterns helpful for software development are the reusable abstract documents which provide acceptable solutions for the recurring design problems. But in the process of reverse engineering, it is often desired to identify as well as recognize design pattern from source code, as it improves maintainability and documentation of the source code. In this study, the process of software design pattern recognition is presented which is based on machine learning techniques. Firstly, a training dataset is developed which is based on software metrics. Subsequently, machine learning algorithms such as Layer Recurrent Neural Network and Decision Tree are applied for patterns detection process. In order to evaluate the proposed study, an open source software i.e., JHotDraw 7.0.6 has been used for the recognition of design patterns.","Pattern recognition,
Software design,
Production facilities,
Decision trees,
Measurement,
Training"
Detecting BGP anomalies using machine learning techniques,"Border Gateway Protocol (BGP) anomalies affect network operations and, hence, their detection is of interest to researchers and practitioners. Various machine learning techniques have been applied for detection of such anomalies. In this paper, we first employ the minimum Redundancy Maximum Relevance (mRMR) feature selection algorithms to extract the most relevant features used for classifying BGP anomalies and then apply the Support Vector Machine (SVM) and Long Short-Term Memory (LSTM) algorithms for data classification. The SVM and LSTM algorithms are compared based on accuracy and F-score. Their performance was improved by choosing balanced data for model training.","Support vector machines,
Feature extraction,
Training,
Mathematical model,
Logic gates,
Hidden Markov models,
Classification algorithms"
Machine learning application for refrigeration showcase fault discrimination,"Open refrigeration showcases are commonly utilized equipment in super markets and convenience stores to maintain the temperature and quality of foods and drinks. Often set in a broader refrigeration arrangement, where a number of showcases and outdoor condensers are connected, it is a system that remains susceptible to fault events, which can lead to financial losses for stores. Therefore, faults and early abnormal behaviors that can lead to future problems should be identified. To classify events as in-control of faulty, samples or patterns for both types of events are needed, however, it is often the case in practical industrial applications where only the in-control type of data is available. This paper assesses the applicability of the machine learning approaches for supervised and unsupervised learning in the task of identifying unusual behavior in real showcase data.","Training,
Classification algorithms,
Clustering algorithms,
Process control,
Algorithm design and analysis,
Fault diagnosis,
Monitoring"
Dynamic gesture recognition using machine learning techniques and factors affecting its accuracy,"Kinect, a motion sensing input device for gaming consoles has been successfully utilized for video games, and rehabilitation of paralyzed patients. We use this device to make learning a fun activity for children. Children learn to draw shapes by moving their hands in front of the Kinect device. We automatically recognize and classify their dynamic hand gestures into predefined shapes, namely; rectangles, triangles, and circles. To decrease over fitting and the cost of generating sample shapes a novel feature engineering approach is also proposed that increases the performance by more than 11%. We used three different machine learning algorithms and successfully classified the shapes with an accuracy of more than 97%.","Shape,
Decision trees,
Gesture recognition,
Machine learning algorithms,
Training data,
Niobium,
Sensors"
"Modeling behavior of Computer Generated Forces with Machine Learning Techniques, the NATO Task Group approach","Commercial/Military-Off-The-Shelf (COTS/MOTS) Computer Generated Forces (CGF) packages are widely used in modeling and simulation for training purposes. Conventional CGF packages often include artificial intelligence (AI) interfaces, but lack behavior generation and other adaptive capabilities. We believe Machine Learning (ML) techniques can be beneficial to the behavior modeling process, yet such techniques seem to be underused and perhaps under-appreciated. This paper aims at bridging the gap between users in academia and the military/industry at a high level when it comes to ML and AI. We address specific requirements and desired capabilities for applying machine learning to CGF behavior modeling applications. The paper is based on the work of the NATO Research Task Group IST-121 RTG-060 Machine Learning Techniques for Autonomous Computer Generated Entities.","Training,
Adaptation models,
Computational modeling,
Computers,
Learning (artificial intelligence),
Conferences"
Design exploration of ASIP architectures for the K-Nearest Neighbor machine-learning algorithm,"Increasingly, machine-learning algorithms are playing an important role in the context of embedded and real-time systems. Applications such as wireless sensor networks, security, and commercial enterprises are increasingly relying on machine-learning algorithms to efficiently make predictive decisions based on the large volumes of data these systems collect. Therefore, there is a need to accelerate the runtime of these algorithms, especially for real-time applications. In this paper, we propose several Application Specific Instruction Processor (ASIP) architectures for the K-Nearest Neighbor (KNN) classification algorithm. Each ASIP is developed using Cadence Tensilica tools and represents a tightly-coupled architecture. Our experimental results, based on several benchmarks, show that proposed ASIPs achieve speedups of 86×-650× over the original software implementation.","Registers,
Training,
Computer architecture,
Software algorithms,
Prediction algorithms,
Benchmark testing,
Algorithm design and analysis"
YouTube QoE Estimation Based on the Analysis of Encrypted Network Traffic Using Machine Learning,"The widespread use of encryption in the delivery of Over-The-Top video streaming services poses challenges for network operators looking to monitor service performance and detect potential customer perceived Quality of Experience (QoE) degradations. While monitoring solutions deployed on client devices provide insight into application layer KPIs (e.g., video quality levels, buffer underruns, stalling duration) which can be further mapped to user QoE, network providers commonly rely primarily on passive traffic monitoring solutions deployed within their network to obtain insight into user perceived degradations and their potential causes. In this paper we present a methodology for the estimation of end users' QoE when watching YouTube videos which is based only on statistical properties of encrypted network traffic. We have developed a system called YouQ which includes tools for monitoring and analysis of application-layer KPIs and corresponding traffic traces, and the subsequent use of this data for the development of machine learning models for QoE estimation based on traffic features. To test this approach, we have collected a dataset of 1060 different YouTube video traces using 39 different bandwidth scenarios. All video traces are annotated with application-layer KPIs and classified into one of three QoE classes. The dataset was used to test various machine learning algorithms, and results showed that up to 84% QoE classification accuracy could be achieved using only features extracted from encrypted traffic.","Streaming media,
YouTube,
Feature extraction,
Monitoring,
Servers,
Androids,
Humanoid robots"
Classification of breast masses using Tactile Imaging System and machine learning algorithms,"In this study, we used Tactile Imaging System (TIS) and machine learning algorithms to classify breast masses in vivo as malignant or benign. When the silicone probe at the front end of TIS is compressed against the breast mass, the indentation profile of this waveguide is captured by a CCD camera. Then TIS algorithm determines the size and stiffness of inclusions based on the acquired tactile images. The size and stiffness results are then used as the input features for breast tumor classification algorithms. We compared three tumor classification algorithms: k-nearest neighbor, support vector machine, and Naïve Bayes, which are known to work well for limited data set. We tested these algorithms on twelve human breast tumors. The results were evaluated using the leave-one-out cross validation technique. Among the three algorithms, k-nearest neighbor classifier performed the best with sensitivity of 86% and specificity of 100%.","Niobium,
Indexes,
Imaging,
Classification algorithms,
Feature extraction,
Cancer,
Sensitivity"
Machine Learning Based Botnet Identification Traffic,"The continued growth of the Internet has resulted in the increasing sophistication of toolkit and methods to conduct computer attacks and intrusions that are easy to use and publicly available to download, such as Zeus botnet toolkit. Botnets are responsible for many cyber-attacks, such as spam, distributed denial-of-service (DDoS), identity theft, and phishing. Most of existence botnet toolkits release updates for new features, development and support. This presents challenges in the detection and prevention of bots. Current botnet detection approaches mostly ineffective as botnets change their Command and Control (C&C) server structures, centralized (e.g., IRC, HTTP), distributed (e.g., P2P), and encryption deterrent. In this paper, based on real world data sets we present our preliminary research on predicting the new bots before they launch their attack. We propose a rich set of features of network traffic using Classification of Network Information Flow Analysis (CONIFA) framework to capture regularities in C&C communication channels and malicious traffic. We present a case study of applying the approach to a popular botnet toolkit, Zeus. The experimental evaluation suggest that it is possible to detect effectively botnets during the botnet C&C communication generated from new updated Zeus botnet toolkit by building the classifier using machine learning from an earlier version and before they launch their attacks using traffic behaviors. Also, show that there is similarity in C&C structures various Botnet toolkit versions and that the network characteristics of botnet C&C traffic is different from legitimate network traffic. Such methods could reduce many different resources needed to identify C&C communication channels and malicious traffic.",
Automatic clustering of eye gaze data for machine learning,"Eye gaze patterns or scanpaths of subjects looking at art while answering questions related to the art have been used to decode those tasks with the use of certain classifiers and machine learning techniques. Some of these techniques require the artwork to be divided into several Areas or Regions of Interest. In this paper, two ways of clustering the static visual stimuli - k-means and the density based clustering algorithm called OPTICS - were used for this purpose. These algorithms were used to cluster the gaze points before classification. The classification success rates were then compared. While it was observed that both k-means and OPTICS gave better success rates than manual clustering, which is itself higher than chance level, OPTICS consistently gave higher success rates than k-means given the right parameter settings. OPTICS also formed clusters that look more intuitive and consistent with the heat map readings than k-means, which formed clusters that look unintuitive and less consistent with the heat map.","Clustering algorithms,
Optics,
Algorithm design and analysis,
Gaze tracking,
Classification algorithms,
Conferences,
Cybernetics"
Machine learning-based clinical decision support system for early diagnosis from real-time physiological data,"This research aims to design a self-organizing decision support system for early diagnosis of key physiological events. The proposed system consists of pre-processing, clustering and diagnostic system, based on self-organizing fuzzy logic modeling. The clustering technique was employed with empirical pattern analysis, particularly when the information available is incomplete or the data model is affected by vagueness, which is mostly the case with medical/clinical data. Clustering module can be viewed as unsupervised learning from a given dataset. This module partitions the patient vital signs to identify the key relationships, patterns and clusters among the medical data. Secondly, it uses self-organizing fuzzy logic modeling for early symptom and event detection. Based on the clustering outcome, when detecting abnormal signs, a high level of agreement was observed between system interpretation and human expert diagnosis of the physiological events and signs.","Decision support systems,
Biomedical monitoring,
Medical diagnostic imaging,
Monitoring,
Fuzzy logic,
Hypertension"
Data analysis to predictive modeling of marine engine performance using machine learning,"Modeling a complex marine engine system is always very challenging and often subjected to model uncertainties and time-varying inputs. A non-model based predictive modeling of marine engine system performance is therefore required. Predictive analytics tools such as Neural Network, Multiple Linear Regression, and Bagged Regression Tree Model are jointly used to model the marine engine system using different filtered input parameters. The unsupervised machine learnings such as Fuzzy C-Means amongst the K-Means Clustering and Self-Organizing Maps are used to reduce the root-mean-square error of the predicted model further by approximately 50%. With exploratory data analysis and applications of the proposed multiple learning schemes on the real-time engine data leads to a more robust and comparative approach to real-time engine model prediction for data engineers.","Engines,
Shafts,
Predictive models,
Training,
Analytical models,
Data models,
Mathematical model"
"Multiple human skeleton recognition in RGB and depth images with graph theory, anatomic refinement of point clouds and machine learning","Computer visual recognition of multiple human poses infers technological benefit in a variety of systems, including security surveillance, medical therapeutics, sports analytics and many more. For this goal the set of detected body parts on color or depth images must be aligned to reconstruct the skeletons of the humans. Here, an algorithm is introduced that models the body part point clouds using principal component analysis to obtain anatomically correct positions of joints, and that assembles the redundant and/or incomplete number of candidate joints with graph theoretical methods using Suurballe's k-shortest disjoint paths algorithm to build the skeletons. The computations were applied to MOCAP database motions rendered in Blender to produce idealized classified point clouds, and to real human depth images classified with decision forests similar to Shotton et al. For MOCAP data, in 68 images showing 3 persons all 204 skeletons were correctly aligned using 4.285 of 4.682 joints with no false assignment. For 33 real human images each showing 3 people, 71 skeletons were correctly detected with 1 false detection and 17 misses, which is promising with respect to non-perfect body part classification in real world.","Skeleton,
Three-dimensional displays,
Neck,
Legged locomotion,
Head,
Buildings,
Algorithm design and analysis"
Cardiovascular risk prediction based on Retinal Vessel Analysis using machine learning,"Cardiovascular risk prediction is a vital aspect of personalized health care. In this study, retinal vascular function is assessed in asymptomatic participants who are classified into risk groups based on Framingham Risk Score. Feature selection, oversampling and state-of-the-art classification methods are applied to provide a sound individual risk prediction based on Retinal Vessel Analysis (RVA) data obtained by non-invasive methods. The results indicate that the RVA based cardiovascular risk prediction models are competitive with well established Framingham and Qrisk based models.","Retinal vessels,
Feature extraction,
Data mining,
Classification algorithms,
Artificial neural networks,
Support vector machines,
Conferences"
Predicting daily mean solar power using machine learning regression techniques,"Daily mean solar irradiance is the most critical parameter in sizing the installation of solar power generation units. The average solar irradiation on a specific location can help predict the amount of electricity that will be generated through solar panels and an accurate forecast can help in calculating the size of the system, return on investment (ROI) and system load measurements. To predict the mean solar irradiation Wh/m2 various regression algorithms have been used in conjunction with various parameters related to solar irradiance. In this paper we present a comparative analysis of forecasting through artificial neural networks (ANN) against the standard regression algorithms. Furthermore, we show that incorporation of azimuth and zenith parameters in the model significantly improves the performance.","Predictive models,
Weather forecasting,
Support vector machines,
Azimuth,
Solar energy,
Prediction algorithms"
Why We are Interested in Machine Learning,,
"Machine Learning, Statistics, and Data Analytics",,
Parallel and Distributed Methods for Constrained Nonconvex Optimization-Part II: Applications in Communications and Machine Learning,"In Part I of this paper, we proposed and analyzed a novel algorithmic framework for the minimization of a nonconvex objective function, subject to nonconvex constraints, based on inner convex approximations. This Part II is devoted to the (nontrivial) application of the framework to the following relevant large-scale problems ranging from communications to machine learning: 1) (generalizations of) the rate profile maximization in MIMO interference broadcast networks; 2) the max-min fair multicast multigroup beamforming problem in a multicell environment; and 3) a general nonconvex constrained bi-criteria formulation for k-sparse variable selection in statistical learning; the two criteria are a nonconvex loss objective function, measuring the fitness of the model to data, and the latter is a nonconvex sparsity-inducing constraint in the general form of difference-of-convex (DC) functions, which allows to accomodate in a unified fashion convex and nonconvex surrogates of the 10 function. The proposed algorithms outperform current state-of-the-art schemes for 1)-3) both theoretically and numerically. For instance, they are the first distributed schemes for the class of problems 1) and 2); and they also lead to subproblems enjoying closed form solutions.","Interference,
Signal processing algorithms,
Linear programming,
Statistical learning,
Approximation algorithms,
Electronic mail,
Covariance matrices"
On the feasibility of an embedded machine learning processor for intrusion detection,"Intrusion detection and prevention systems serve a pivotal role in securing computer networks. Using machine learning for an intrusion detection system is important for stopping new attacks that do not have known signatures. Lowering the barrier to entry for microprocessor-based systems has enabled the use of specialized machine learning coprocessors to improve analysis performance. This paper proposes a machine learning approach on a small, low-powered embedded system that uses network-based features to distinguish between normal and abnormal network traffic. A hardware-based approach using a machine learning coprocessor is compared with a software-based approach. Machine learning processors can improve power consumption and processing speed especially when dealing with dig data sets. Results of the analysis show that the machine learning coprocessor obtains 66.67% classification accuracy. Additional results are presented and discussed.","Decision support systems,
Big data,
Conferences"
Cloud-based machine learning for predictive analytics: Tool wear prediction in milling,"The proliferation of real-time monitoring systems and the advent of Industrial Internet of Things (IIoT) over the past few years necessitates the development of scalable and parallel algorithms that help predict mechanical failures and remaining useful life of a manufacturing system or system components. Classical model-based prognostics require an in-depth physical understanding of the system of interest and oftentimes assume certain stochastic or random processes. To overcome the limitations of model-based methods, data-driven methods such as machine learning have been increasingly applied to prognostics and health management (PHM). While machine learning algorithms are able to build accurate predictive models, large volumes of training data are required. Consequently, machine learning techniques are not computationally efficient for data-driven PHM. The objective of this research is to create a novel approach for machinery prognostics using a cloud-based parallel machine learning algorithm. Specifically, one of the most popular machine learning algorithms (i.e., random forest) is applied to predict tool wear in dry milling operations. In addition, a parallel random forest algorithm is developed using the MapReduce framework and then implemented on the Amazon Elastic Compute Cloud. Experimental results have shown that the random forest algorithm can generate very accurate predictions. Moreover, significant speedup can be achieved by implementing the parallel random forest algorithm.","Machine learning algorithms,
Prediction algorithms,
Decision trees,
Vegetation,
Maintenance engineering,
Training data,
Predictive models"
QED: Groupon's ETL management and curated feature catalog system for machine learning,"In today's technology industry where machine learning has become essential, the effectiveness of algorithms ultimately depends on a robust data pipeline, and fast model prototyping and tuning require easy feature discovery and consumption. Careful management of ETL processes and their produced datasets is key to both model development in the research stage and model execution in the production environment. In this paper we present QED, an ETL management and curated feature catalog system that provides robust, streamlined machine learning pipelines. First, QED promises dynamic, reliable, and timely data delivery to the production pipeline. Its enhanced ETL process persists data from upstream sources in local data stores and ensures their correctness. Second, in contrast to previous systems, QED is capable not only of producing a daily scoring dataset, but also a training dataset with minimized bias by preserving the historical observations of feature values. Third, QED's multiple data store design allows batch process of large datasets as well as fast random access to single records. Finally, its curated feature catalog system enables sharing and reuse of machine learning features. QED serves as the data backend for a variety of machine learning models that provide key insights into the global business, and optimize the daily operations of Groupon.","Feature extraction,
Data mining,
Machine learning algorithms,
Pipelines,
Training,
Metadata"
Efficient Distributed Machine Learning with Trigger Driven Parallel Training,"Distributed machine learning is becoming increasingly popular for large scale data mining on large scale cluster. To mitigate the interference of straggler machines, recent distributed machine learning systems support flexible model consistency, which allows worker using a local stale model to compute model update without waiting for the newest model, while limiting the asynchronous step in a certain bound to guarantee the algorithm correctness. However, bounded asynchronous computing can not tolerate consistent straggler. We explore that the root cause of this problem derives from the worker driven parallel training mechanism in existing systems. To address the straggler problem fundamentally and fully leverage the asynchronous efficiency, we propose a novel trigger driven parallel training mechanism, where model server proactively triggers to collect updates from workers instead of passively receiving them, which can inherently avoid the coordinating issue among workers. Besides, we devise a dynamic load balancing strategy to make the sampling frequency of each data equal. Furthermore, bounded asynchronous computing is introduced to achieve the algorithm efficiency, as well as the convergence guarantee. Finally, we integrate the above techniques into a distributed machine learning system called Squirrel. Squirrel provides simple programming interface and can easily deploy machine learning algorithms on distributed cluster. In comparison with traditional worker driven parallel training mechanism, trigger driven mechanism can improve up to 4x faster convergence speed of machine learning algorithm.","Computational modeling,
Training,
Load modeling,
Servers,
Clustering algorithms,
Data models,
Machine learning algorithms"
"A machine learning approach for student assessment in E-learning using Quinlan's C4.5, Naive Bayes and Random Forest algorithms","Student Assessment on e-learning platforms is a debated subject. The focal emphasis of this research study is to predict fair/transparent student evaluation using machine learning algorithms. A prediction on students' final grade showing whether the student will pass or fail would benefit the student/instructor and act as a guide for future recommendations/evaluations on performance. An in depth study on the assessment techniques for e-learning such as Markov Model, metacognitive perspectives has been conducted. A proposed model for fair/transparent student evaluation/performance has also been presented. Specific parameters have been defined that are then efficaciously tested by applying machine learning algorithms. In this study, classifiers such as Decision Trees-J48, Naive Bayes and Random Forest are used to progress the excellence of student data by initially eradicating noisy data, and consequently getting better prognostic accuracy. The scope of the paper has been set for undergraduate programs. The experimental results endow with set of guidelines to those students who have low grades. Performance testing has also been conducted for verification, accuracy and validity of results.","Hidden Markov models,
Electronic learning,
Data mining,
Machine learning algorithms,
Data models,
Prediction algorithms,
Physiology"
Fall recognition using wearable technologies and machine learning algorithms,"Falls are common and dangerous for the elderly or individuals with decreased independence or functional limitations. Fall recognition is extremely important for fallers, healthcare providers, and society. Immediate fall recognition triggers emergency services and potentially decreases individuals time with injury without care. Acute post-fall intervention works to mitigate life threatening fall consequences, decrease fall risk through rehabilitation, and improve quality of life. Extended from our research on real-time fall risk estimation with the functional reach test and Timed Up and Go test built in mStroke, a real-time and automatic mobile health system for post-stroke recovery and rehabilitation, our investigation here is expanded to include fall recognition by taking advantage of wearable technologies and machine learning algorithms. Up to three wearable sensors are employed to acquire raw motion data related to activities of daily living or falls. Feature selection and classification on the basis of machine learning algorithms are explored for fall recognition. The fall recognition performances are presented to justify their accuracy and reliability. Meanwhile, the effects of sensor placement/location and the feature number on the recognition performance are also discussed in this paper.","Wearable sensors,
Machine learning algorithms,
Real-time systems,
Legged locomotion,
Data analysis,
Sternum,
Robustness"
A machine learning approach to fall detection algorithm using wearable sensor,"Falls are the primary cause of accidents for the elderly in living environment. Falls frequently cause fatal and non-fatal injuries that are associated with a large amount of medical costs. Reduction hazards in living environment and doing exercise for training balance and muscle are the common strategies for fall prevention. But falls cannot be avoided completely; fall detection provides the alarm in time that can decrease the injuries or death caused by no rescue. We propose machine learning-based fall detection algorithm using multi-SVM with linear, quadratic or polynomial kernel function, and k-NN classifier. Eight kinds of falling postures and seven types of daily activities arranged in the experiment are used to explore the performance of the machine learning-based fall detection algorithm. The emulated falls were performed on a soft mat by ten healthy young subjects wearing protectors. The k-nearest neighbor method with 0.1 second window size has the highest accuracy, which is 96.26%. The results show that the proposed machine learning fall detection algorithm can fulfill the requirements of adaptability and flexibility for the individual differences.","Detection algorithms,
Acceleration,
Support vector machines,
Training,
Feature extraction,
Sensitivity,
Wearable sensors"
Using machine learning to identify major shifts in human gut microbiome protein family abundance in disease,"Inflammatory Bowel Disease (IBD) is an autoimmune condition that is observed to be associated with major alterations in the gut microbiome taxonomic composition. Here we classify major changes in microbiome protein family abundances between healthy subjects and IBD patients. We use machine learning to analyze results obtained previously from computing relative abundance of ~10,000 KEGG orthologous protein families in the gut microbiome of a set of healthy individuals and IBD patients. We develop a machine learning pipeline, involving the Kolomogorv-Smirnov test, to identify the 100 most statistically significant entries in the KEGG database. Then we use these 100 as a training set for a Random Forest classifier to determine ~5% the KEGGs which are best at separating disease and healthy states. Lastly, we developed a Natural Language Processing classifier of the KEGG description files to predict KEGG relative over-or under-abundance. As we expand our analysis from 10,000 KEGG protein families to one million proteins identified in the gut microbiome, scalable methods for quickly identifying such anomalies between health and disease states will be increasingly valuable for biological interpretation of sequence data.","Diseases,
Databases,
Proteins,
Bioinformatics,
Genomics,
Training,
Sequential analysis"
Java thread and process performance for parallel machine learning on multicore HPC clusters,"The growing use of Big Data frameworks on large machines highlights the importance of performance issues and the value of High Performance Computing (HPC) technology. This paper looks carefully at three major frameworks Spark, Flink and Message Passing Interface (MPI) both in scaling across nodes and internally over the many cores inside modern nodes. We focus on the special challenges of the Java Virtual Machine (JVM) using an Intel Haswell HPC cluster with 24 cores per node. Two parallel machine learning algorithms, K-Means clustering and Multidimensional Scaling (MDS) are used in our performance studies. We identify three major issues - thread models, affinity patterns, and communication mechanisms - as factors affecting performance by large factors and show how to optimize them so that Java can match the performance of traditional HPC languages like C. Further we suggest approaches that preserve the user interface and elegant dataflow approach of Flink and Spark but modify the runtime so that these Big Data frameworks can achieve excellent performance and realize the goals of HPC-Big Data convergence.","Java,
Sparks,
Instruction sets,
Message systems,
Data models,
Big data,
Parallel machines"
Consensus-Based Parallel Extreme Learning Machine for Indoor Localization,"In the era of Internet of Things, WiFi fingerprinting based indoor positioning system (IPS) has been recognized as the most promising IPS for indoor location-based service. Fingerprinting-based algorithms critically rely on a fingerprint database built from machine learning methods, and extreme machine learning (ELM) is preferred for its fast training speed. However, traditional WiFi based IPS usually requires a central server to collect and process data, which is tremendously vulnerable to server breakdown and communication link failure. To address this issue, we propose Consensus-based Parallel ELM (CPELM) to enhance the robustness by distributing the data on different computation nodes. Specifically, each node keeps updating the corresponding terms in the ELM regression equation as a weighted average of those from neighboring nodes based on the distributed consensus iterative scheme. Upon the agreement of the regression equation within the network, the output weight of ELM can be calculated on some nodes and propagated to other nodes. Extensive simulation with real data has demonstrated that CPELM is able to produce same level of localization accuracy as centralized ELM without incurring additional computational cost, and in the meanwhile provides more robustness to the entire IPS in case of server breakdown and link failures.",
Solar irradiance forecasting by machine learning for solar car races,"Solar car race competitions offer realistic conditions to test and demonstrate the state-of-the-art technologies in multidisciplinary fields. In such races the solar panels mounted on the car produce the energy required to power the vehicle. A simulator runs during the race determines the optimal race speed based on the predicted availability of solar energy and other parameters as well as road conditions. The accuracy of the forecasts, especially the solar irradiance forecasts, has a significant impact on the race strategy. Here we report on the experience of providing irradiance forecasts for two races run by the University of Michigan Solar Car Team at the Bridgestone World Solar Challenge 2015 in Australia and at the American Solar Challenge 2016 from Ohio to South Dakota. The probabilistic forecasts of hourly solar irradiance generated from machine learning algorithms were deployed to optimally decide on the race strategy. This work showcases an example of real time decision making based on insights derived from machine learning utilizing big geospatial data - weather models and measurement data from weather station networks.","Automobiles,
Clouds,
Wind forecasting,
Predictive models,
Atmospheric modeling"
Evaluating machine learning algorithms for anomaly detection in clouds,"Critical services in the field of Network Function Virtualization require elaborate reliability and high availability mechanisms to meet the high service quality requirements. Traditional monitoring systems detect overload situations and outages in order to automatically scale out services or mask faults. However, faults are often preceded by anomalies and subtle misbehaviors of the services, which are overlooked when detecting only outages. We propose to exploit machine learning techniques to detect abnormal behavior of services and hosts by analysing metrics collected from all layers and components of the cloud infrastructure. Various algorithms are able to compute models of a hosts normal behavior that can be used for anomaly detection at runtime. An offline evaluation of data collected from anomaly injection experiments shows that the models are able to achieve very high precision and recall values.","Machine learning algorithms,
Cloud computing,
Monitoring,
Decision trees,
Data collection,
Vegetation,
Measurement"
"Machine learning, linear and Bayesian models for logistic regression in failure detection problems","In this work, we study the use of logistic regression in manufacturing failures detection. As a data set for the analysis, we used the data from Kaggle competition “Bosch Production Line Performance”. We considered the use of machine learning, linear and Bayesian models. For machine learning approach, we analyzed XGBoost tree based classifier to obtain high scored classification. Using the generalized linear model for logistic regression makes it possible to analyze the influence of the factors under study. The Bayesian approach for logistic regression gives the statistical distribution for the parameters of the model. It can be useful in the probabilistic analysis, e.g. risk assessment.","Logistics,
Bayes methods,
Manufacturing,
Boosting,
Correlation coefficient,
Analytical models"
MaSiF: Machine learning guided auto-tuning of Parallel Skeletons,"We present MaSiF, a novel tool to auto-tune parallelization parameters of skeleton parallel programs. It reduces the cost of searching the optimization space using a combination of machine learning and linear dimensionality reduction. To auto-tune a new program, a set of program features is determined statically and used to compute k nearest neighbors from a set of training programs. Previously collected performance data for the nearest neighbors is used to reduce the size of the search space using Principal Components Analysis. This results in a set of eigenvectors that are used to search the reduced space. MaSiF achieves 88% of the performance of the oracle, which searches a random set of 10,000 parameter values. MaSiF searches just 45 points, or 0.45% of the optimization space, to achieve this performance. MaSiF provides an average speedup of 1.18x over parallelization parameters chosen by a human expert.","Skeleton,
Optimization,
Training,
Feature extraction,
Principal component analysis,
Programming,
Tuning"
A Hybrid Machine Learning Model for Range Estimation of Electric Vehicles,"Data-driven solutions to Electric Vehicle (EV) range estimation is attracting attention recently due to the prevalence of Internet of Things (IoT). However, there raise the Big Data problems with the increased volume and number of sensory sources of unstructured data collected from the EV equipped with In-Vehicle Networks. This means that traditional statistical analysis and Machine Learning tools are not suitable to be directly applied to analyse and interpret data. Hence, we aim to develop a Hybrid Machine Learning Model to predict the power consumption of EV trips practically considering multivariate high- dimensional data and meanwhile extract knowledge from the historical trip features for further applications. The proposed Hybrid Model is a modified Self-Organizing Maps (SOM) integrating Regression Trees (RT) to predict the power consumption of EV trips. The experimental results, including both cross-validation and mathematical accuracy measuring criteria, demonstrate that our Hybrid Model could not only provide a better power consumption estimation of EV trips but also reveal the inherent of the EV Big Data.","Neurons,
Power demand,
Training,
Regression tree analysis,
Kernel,
Estimation,
Data models"
Estimating the Number of Receiving Nodes in 802.11 Networks via Machine Learning Techniques,"Nowadays, most mobile devices are equipped with multiple wireless interfaces, causing an emerging research interest in device to device (D2D) communication: the idea behind the D2D paradigm is to exploit the proper interface to directly communicate with another user, without traversing any network infrastructure. A first issue related to this paradigm is the need for a coordinator, called controller, able to decide when activating a D2D connection is appropriate and to manage such connection. In this view, the paradigm of Software Defined Networking (SDN) can be exploited both to handle the data flows among the devices and to interact directly with every device. This work is focused on a scenario where a device is selected by the SDN controller, in order to become the master node of a WiFi-Direct network. The remaining nodes, called clients, can exchange data with other nodes through the master. The objective is to infer, through different Machine Learning approaches, the number of nodes actively involved in receiving data, exploiting only the information available at the client side and without modifying any standard communication protocol. The information about the number of client nodes is crucial when, e.g., a user desires a precise prediction of the transmission estimated time of arrival (ETA) while downloading a file.","IEEE 802.11 Standard,
Device-to-device communication,
Protocols,
Wireless communication,
Software,
Smart phones,
Batteries"
Human identification from brain EEG signals using advanced machine learning method EEG-based biometrics,"EEG-based human recognition is increasingly becoming a popular modality for biometric authentication. Two important features of EEG signals are liveliness and the robustness against falsification. However, a comprehensive study on human authentication using EEG signal is still remains. On the other hand, low-cost wireless EEG recording devices are now growing in the market places. Although these devices have the potential to many applications, researches have yet to be done to find the feasibility of these devices. In this study, we propose a method for human identification using EEG signals obtained from such low-cost devices. EEG signal is first preprocessed to remove noise and artifacts using Bandpass FIR filter. These signals are then divided into disjoint segments. Three feature extraction methods, namely multiscale shape description (MSD), multiscale wavelet packet statistics (WPS) and multiscale wavelet packet energy statistics (WPES) are then applied. These features are finally used to train a supervised error-correcting output code multiclass model (ECOC) using support vector machine (SVM) classifier, which ultimately can recognize humans from test EEG signals. A preliminary experiment with 9 EEG records from 9 subjects shows the true positive rate of 94.44% of the proposed method.",
Supervised Machine Learning-Based Routing for Named Data Networking,"Named Data Networking (NDN) ambitions the rank of Future Internet Architecture in uniquely addressing content items by their name. In NDN, routers forward Interests for content after finding Longest-Prefix Matches (LPM) of content names in their Forwarding Information Base (FIB). However, the scalability of this structure is challenged by the huge global Internet namespace. In this paper, we propose a novel approach to interest forwarding that compresses the FIB data structure into Artificial Neural Networks (ANNs). A bitwise trie splits the namespace and indexes ANNs. ANNs are offline trained by the control plane from the Routing Information Base and matching Interests. Then, they are made available to the data plane for interrogation. We demonstrate that this approach accelerates packet forwarding by several orders of magnitude. Noteworthily, leveraging ANNs as memory and processor for directing packets towards next hops reminds of Asking For Directions to people in the street, incurring similar reliability regards.","Neurons,
Ribs,
Supervised learning,
Training,
Routing,
Internet,
Routing protocols"
Reverse engineering smart card malware using side channel analysis with machine learning techniques,"From inception, side channel leakage has been widely used for the purposes of extracting secret information, such as cryptographic keys, from embedded devices. However, in a few instances it has been utilized for extracting other information about the internal state of a computing device. In this paper, we exploit side channel information to recover large parts of the Sykipot malware program executed on a smart card. We present the first methodology to recover the program code of a smart card malware by evaluating its power consumption only. Besides well-studied methods from side channel analysis, we apply a combination of dimensionality reduction techniques in the form of PCA and LDA models to compress the large amount of data generated while preserving as much variance of the original data as possible. Among feature extraction techniques, PCA and LDA are very common dimensionality reduction algorithms that have successfully been applied in many classification problems like face recognition, character recognition, speech recognition, etc. with the chief objective being to eliminate insignificant data (without losing too much information) during the pre-processing step. In addition to quantifying the potential of the created side channel based disassembler, we highlight its diverse and unique application scenarios.","Smart cards,
Malware,
Principal component analysis,
Reverse engineering,
Covariance matrices,
Data mining,
Algorithm design and analysis"
Security Perspective of Biometric Recognition and Machine Learning Techniques,"Biometric systems may be used to create a remote access model on devices, ensure personal data protection, personalize and facilitate the access security. Biometric systems are generally used to increase the security level in addition to the previous authentication methods and they seen as a good solution. Biometry occupies an important place between the areas of daily life of the machine learning. In this study;the techniques, methods, technologies used in biometric systems are researched, machine learning techniques used biometric applications are investigated for the security perspective, the advantages and disadvantages that these techniques provide are given. The studies in the literature between 2010-2016 years, used algorithms, technologies, metrics, usage areas, the machine learning techniques used for different biometric systems such as face, palm prints, iris, voice, fingerprint recognition are researched and the studies made are evaluated. The level of security provided by the use of biometric systems by developed using machine learning and disadvantages that arise in the use of these systems are stated in detail in the study. Also, impact on people of biometric methods in terms of ease of use, security and usages areas are examined.",
Differentiation and Integration of Machine Learning Feature Vectors,"This paper presents a new approach to the production of feature maps for the improvement of classification in machine learning. The idea is based on a calculus of differentiation and integration of feature vectors, which can be viewed as functions on a metric space or network. Based on this we propose a novel network-based binary machine learning classifier. We illustrate our method using molecular networks alone to distinguish phenotypes, including cancer types and subtypes. We include feature sets derived from disease-specific gene co-expression networks in different cancer data sets using The Cancer Genome Atlas (TCGA) along with other previously published studies. We also illustrate our network-based predictor on another data type, based on infrared spectroscopy of lung cancer tissue.","Cancer,
Gene expression,
Correlation,
Lungs,
Kernel,
Laplace equations,
Proteins"
An Empirical Study on Machine Learning Models for Wind Power Predictions,"Wind power prediction is of great importance in the utilization of renewable wind power. A lot of research has been done attempting to improve the accuracy of wind power predictions and has achieved desirable performance. However, there is no complete performance evaluation of machine learning methods. This paper presents an extensive empirical study of machine learning methods for wind power predictions. Nine various models are considered in this study which also includes the application and evaluation of deep learning techniques. The experimental data consists of seven datasets based on wind farms in Ontario, Canada. The results indicate that SVM, followed by ANN, has the best overall performance, and that k-NN method is suitable for longer ahead predictions. Despite the findings that deep learning fails to give improvement in basic predictions, it shows the potential for more abstract prediction tasks, such as spatial correlation predictions.","Wind power generation,
Support vector machines,
Neurons,
Wind speed,
Training,
Machine learning,
Wind farms"
Toward Parametric Security Analysis of Machine Learning Based Cyber Forensic Biometric Systems,"Machine learning algorithms are widely used in cyber forensic biometric systems to analyze a subject's truthfulness in an interrogation. An analytical method (rather than experimental) to evaluate the security strength of these systems under potential cyber attacks is essential. In this paper, we formalize a theoretical method for analyzing the immunity of a machine learning based cyber forensic system against evidence tampering attack. We apply our theory on brain signal based forensic systems that use neural networks to classify responses from a subject. Attack simulation is run to validate our theoretical analysis results.","Feature extraction,
Forensics,
Brain modeling,
Electroencephalography,
Security,
Machine learning algorithms,
Iron"
Co-MLM: A SSL Algorithm Based on the Minimal Learning Machine,"Semi-supervised learning is a challenging topic in machine learning that has attracted much attention in recent years. The availability of huge volumes of data and the work necessary to label all these data are two of the reasons that can explain this interest. Among the various methods for semi-supervised learning, the co-training framework has become popular due to its simple formulation and promising results. In this work, we propose Co-MLM, a semi-supervised learning algorithm based on a recently supervised method named Minimal Learning Machine (MLM), built upon co-training framework. Experiments on UCI data sets showed that Co-MLM has promising performance in compared to other co-training style algorithms.","Training,
Semisupervised learning,
Standards,
Predictive models,
Support vector machines,
Cost function,
Intelligent systems"
Automatic Algorithm Selection in Computational Software Using Machine Learning,"Computational software programs, such as Maple and Mathematica, heavily rely on superfunctions and meta-algorithms to select the optimal algorithm for a given task. These meta-algorithms may require intensive mathematical proof to formulate, incur large computational overhead, or fail to consistently select the best algorithm. Machine learning demonstrates a promising alternative for automatic algorithm selection by easing the design process and overhead while also attaining high accuracy in selection. In a case study on the resultant superfunction, a trained neural network is able to select the best algorithm out of the four available 86% of the time in Maple and 78% of the time in Mathematica. When used as a replacement for pre-existing meta-algorithms, the neural network brings about a 68% runtime improvement in Maple and 49% improvement in Mathematica. Random forests, k-nearest neighbors, and both linear and RBF kernel SVMs are also compared to the neural network model, the latter of which offers the best performance out of the tested machine learning methods.","Machine learning algorithms,
Algorithm design and analysis,
Software algorithms,
Runtime,
Software,
Heuristic algorithms,
Mathematical model"
Machine Learning for Plant Disease Incidence and Severity Measurements from Leaf Images,"In many fields, superior gains have been obtained by leveraging the computational power of machine learning techniques to solve expert tasks. In this paper we present an application of machine learning to agriculture, solving a particular problem of diagnosis of crop disease based on plant images taken with a smartphone. Two pieces of information are important here, the disease incidence and disease severity. We present a classification system that trains a 5 class classification system to determine the state of disease of a plant. The 5 classes represent a health class and 4 disease classes. We further extend the classification system to classify different severity levels for any of the 4 diseases. Severity levels are assigned classes 1 - 5, 1 being a healthy plant, 5 being a severely diseased plant. We present ways of extracting different features from leaf images and show how different extraction methods result in different performance of the classifier. We finally present the smartphone-based system that uses the classification model learnt to do real-time prediction of the state of health of a farmers garden. This works by the farmer uploading an image of a plant in his garden and obtaining a disease score from a remote server.","Diseases,
Feature extraction,
Agriculture,
Africa,
Image color analysis,
Histograms,
Microorganisms"
System-Level Test Case Prioritization Using Machine Learning,"Regression testing is the common task of retesting software that has been changed or extended (e.g., by new features) during software evolution. As retesting the whole program is not feasible with reasonable time and cost, usually only a subset of all test cases is executed for regression testing, e.g., by executing test cases according to test case prioritization. Although a vast amount of methods for test case prioritization exist, they mostly require access to source code (i.e., white-box). However, in industrial practice, system-level testing is an important task that usually grants no access to source code (i.e., black-box). Hence, for an effective regression testing process, other information has to be employed. In this paper, we introduce a novel technique for test case prioritization for manual system-level regression testing based on supervised machine learning. Our approach considers black-box meta-data, such as test case history, as well as natural language test case descriptions for prioritization. We use the machine learning algorithm SVM Rank to evaluate our approach by means of two subject systems and measure the prioritization quality. Our results imply that our technique improves the failure detection rate significantly compared to a random order. In addition, we are able to outperform a test case order given by a test expert. Moreover, using natural language descriptions improves the failure finding rate.","Testing,
Support vector machines,
Software,
Training data,
Natural languages,
Dictionaries,
Training"
Analysis for Status of the Road Accident Occurance and Determination of the Risk of Accident by Machine Learning in Istanbul,"The traffic has been transformed into the difficult structure in points of designing and managing by the reason of increasing number of vehicle. This situation has discovered road accidents problem, influenced public health and country economy and done the studies on solution of the problem. Large calibrated data agglomerations have increased by the reasons of the technological improvements and data storage with low cost. Arising the need of accession to information from this large calibrated data obtained the corner stone of the data mining. In this study, assignment of the most compatible machine learning classification techniques for road accidents estimation by data mining has been intended.","Road accidents,
Classification algorithms,
Data mining,
Data models,
Algorithm design and analysis,
Machine learning algorithms"
Review on Machine Learning Based Lesion Segmentation Methods from Brain MR Images,"Brain lesions are life threatening diseases. Traditional diagnosis of brain lesions is performed visually by neuro-radiologists. Nowadays, advanced technologies and the progress in magnetic resonance imaging provide computer aided diagnosis using automated methods that can detect and segment abnormal regions from different medical images. Among several techniques, machine learning based methods are flexible and efficient. Therefore, in this paper, we present a review on techniques applied for detection and segmentation of brain lesions from magnetic resonance images with supervised and unsupervised machine learning techniques.","Image segmentation,
Feature extraction,
Lesions,
Probabilistic logic,
Biological neural networks,
Histograms"
User Movement Prediction: The Contribution of Machine Learning Techniques,"Ambient Assisted Living (AAL) aims to increase the time older people or disabled people can live in their home environment by assisting them in performing activities of daily living by the use of intelligent products. Localization and tracking of users in indoor environment are the main components of AAL. Wireless sensor networks is an effective technology to accomplish these services by using Received Signal Strength (RSS) information. This work seeks to investigate the effect of machine learning techniques on the accuracy of user movement prediction. Five base classifiers and two ensemble learning approaches are employed and the results are evaluated in terms of precision recall, and F-measure. A real-life benchmark dataset in the area of AAL is used for evaluation. The results show that J48 is the best performing model compared to the other base-level classifiers. It also shows that Bagged J48 achieves the best performance.","Boosting,
Bagging,
Training,
Wireless sensor networks,
Tracking,
Benchmark testing,
Radar tracking"
Extreme Learning Machines for Datasets with Missing Values Using the Unscented Transform,"The existence of missing data is a common fact in real applications which can significantly affect the data analysis process. In order to overcome this problem, many methods have been proposed in the literature. Extreme Learning Machine (ELM) has become a very popular research topic in machine learning and artificial intelligence areas due to its characteristics such as fast training procedure, good generalization and universal approximation capability. Although ELM has been successfully applied in different domains, its basic formulation cannot handle datasets with missing values properly. This paper presents a variant of the Extreme Learning Machine (ELM) for datasets with missing values. In the proposed method, probability distributions for the missing values are estimated using the expectation maximization (EM) algorithm, assuming that data is normally distributed. The Unscented Transform (UT) is used to estimate the values of the hidden layer outputs, and the weights of the output layer are assigned using the Moore-Penrose Pseudoinverse. Numerical experiments are carried out in order to evaluate the performance of the proposed method in four real world and two synthetic regression datasets. The results show that the proposed method presented a good performance in terms of Average Root-Mean-Squared Error (ARMSE).","Transforms,
Random variables,
Training,
Covariance matrices,
Computational modeling,
Neurons,
Two dimensional displays"
A Hybrid Machine Learning Approach for Planning Safe Trajectories in Complex Traffic-Scenarios,"Planning of safe trajectories with interventions in both lateral and longitudinal dynamics of vehicles has huge potential for increasing the road traffic safety. Main challenges for the development of such algorithms are the consideration of vehicle nonholonomic constraints and the efficiency in terms of implementation, so that algorithms run in real time in a vehicle. The recently introduced Augmented CL-RRT algorithm is an approach that uses analytical models for trajectory planning based on the brute force evaluation of many longitudinal acceleration profiles to find collision-free trajectories. The algorithm considers nonholonomic constraints of the vehicle in complex road traffic scenarios with multiple static and dynamic objects, but it requires a lot of computation time. This work proposes a hybrid machine learning approach for predicting suitable acceleration profiles in critical traffic scenarios, so that only few acceleration profiles are used with the Augmented CL-RRT to find a safe trajectory while reducing the computation time. This is realized using a convolutional neural network variant, introduced as 3D-ConvNet, which learns spatiotemporal features from a sequence of predicted occupancy grids generated from predictions of other road traffic participants. These learned features together with hand-designed features of the EGO vehicle are used to predict acceleration profiles. Simulations are performed to compare the brute force approach with the proposed approach in terms of efficiency and safety. The results show vast improvement in terms of efficiency without harming safety. Additionally, an extension to the Augmented CL-RRT algorithm is introduced for finding a trajectory with low severity of injury, if a collision is already unavoidable.","Trajectory,
Heuristic algorithms,
Acceleration,
Prediction algorithms,
Roads,
Machine learning algorithms,
Vehicle dynamics"
Semantic Clone Detection Using Machine Learning,"If two fragments of source code are identical to each other, they are called code clones. Code clones introduce difficulties in software maintenance and cause bug propagation. In this paper, we present a machine learning framework to automatically detect clones in software, which is able to detect Types-3 and the most complicated kind of clones, Type-4 clones. Previously used traditional features are often weak in detecting the semantic clones The novel aspects of our approach are the extraction of features from abstract syntax trees (AST) and program dependency graphs (PDG), representation of a pair of code fragments as a vector and the use of classification algorithms. The key benefit of this approach is that our approach can find both syntactic and semantic clones extremely well. Our evaluation indicates that using our new AST and PDG features is a viable methodology, since they improve detecting clones on the IJaDataset 2.0.","Cloning,
Feature extraction,
Syntactics,
Semantics,
Machine learning algorithms,
Control systems,
Measurement"
Forecasting PM2.5 Concentration Using Spatio-Temporal Extreme Learning Machine,"In recent years, air quality has become a severe environmental problem in China. Since bad air quality brought significant influences on traffic and people's daily life, how to predict the future air quality precisely and subtly, has been an urgent and important problem. In this paper, a Spatio-Temporal Extreme Learning Machine (STELM) method is proposed for air quality prediction. STELM considers temporal and spatial characteristics of air quality data and related meteorological data, constructs a prediction model based on ELM, and realizes air quality prediction with more than 80% precision. A prototype system is implemented and the experiments on practical air quality data in Beijing validate the effectiveness of our method and system.","Air quality,
Predictive models,
Atmospheric modeling,
Training,
Data models,
Wind forecasting,
Monitoring"
Machine Learning for Optimum CT-Prediction for qPCR,"Introduction of fluorescence-based Real-Time PCR (RT-PCR) is increasingly used to detect multiple pathogens simultaneously and rapidly by gene expression analysis of PCR amplification data. PCR data is analyzed often by setting an arbitrary threshold that intersect the signal curve in its exponential phase if it exists. The point at which the curve crosses the threshold is called Threshold Cycle (CT) for positive samples. On the other, when such cross of threshold does not occur, the sample is identified as negative. This simple and arbitrary however not an elagant definition of CT value sometimes leads to conclusions that are either false positive or negative. Therefore, the purpose of this paper is to present a stable and consistent alternative approach that is based on machine learning for the definition and determination of CT values.","Prediction algorithms,
Real-time systems,
Mathematical model,
Xenon,
Clustering algorithms,
Pathogens,
Logistics"
Hourly Solar Irradiance Forecasting Based on Machine Learning Models,"In recent years, many research studies are conducted into the use of smart meters data for developping decision-making tools including both analytical, forecasting and display purposes. Forecasting energy generation or forecasting energy consumption demand are indeed central problems for urban stakeholders (electricity companies and urban planners). These issues are helpful to allow them ensuring an efficient planning and optimization of energy resources. This paper investigates the problem for forecasting the hourly solar irradiance within a Machine Learning (ML) framework using Similarity method (SIM), Support Vector Machine (SVM) and Neural Network (NN). These approaches rely on a methodology which takes into account the previous hours of the predicting day and also the days having the same number of sunshine hours in the history. The study is conducted on a real data set collected on the Paris suburb of Alfortville. A comparison with two time series approaches namely Naive method and Autoregressive Moving Average Model (ARMA) is performed. This study is the first step towards the development of the hourly solar irradiance forecasting hybrid models.","Autoregressive processes,
Forecasting,
Time series analysis,
Predictive models,
Hidden Markov models,
Weather forecasting,
Numerical models"
Using Machine Learning to Accelerate Data Wrangling,"70% Of the time spent on data analytics is not actually spent on data analytics, but rather, in data wrangling: the process of finding, interpreting, extracting, preparing and recombining the data to be analyzed. For data that is collected as free-form text, the lack of standards or competing standards often results in a variety of formats for expressing the same type of data, making the data wrangling step a tedious and error-prone process. For example, US street addresses may be expressed with a house number, PO Box, rural or military route, and/or a direction - all of which can be abbreviated or spelled out in a variety of ways. In this paper, we present an algorithm that uses machine learning to efficiently and automatically identify categories of attributes, such as geo-spatial, that are present in a data file and we discuss results on a variety of real data sets. Our implementation can be used to automatically prepare data for consumption by other tools and services, such as mapping and visualization tools, and is motivated by and in support of a customizable severe weather alerting service.","Urban areas,
Meteorology,
Data mining,
Standards,
Geospatial analysis,
Data analysis,
Machine learning algorithms"
Stick Must Fall: Using Machine Learning to Predict Human Error in Virtual Balancing Task,"This work presents a new approach to prediction of human control error in unstable systems. We consider virtual inverted pendulum (stick) as a characteristic example of such system. The proposed approach is based on applying classification via machine learning to distinguish between the samples of human control corresponding to successful balancing and critical control errors (resulting in stick fall). To illustrate the approach, we analyze the previously collected data on human balancing of virtual overdamped stick. The obtained results demonstrate that, at least in the considered balancing problem, as much as 73% of human control errors can be successfully predicted in advance (as early as one second before the stick fall).","Time series analysis,
Standards,
Control systems,
Prediction algorithms,
Real-time systems,
Conferences,
Feature extraction"
A novel 2D to 3D video conversion system based on a machine learning approach,"There has been recently a significant increase in the number of available 3D displays and players. Nevertheless, the amount of 3D content has not increased in the same magnitude, creating a gap between 3D offer and demand. To reduce this difference, many algorithms have appeared that perform 2D-to-3D image and video conversion. These algorithms usually require several images from the same scene to perform the conversion. In this paper, an automatic algorithm for estimating the 3D structure of a scene from a single color image is proposed. It is based on the key assumption that color images with similar structure will likely present similar depth structures. The conversion algorithm is split into an offline and an online module to be easily deployable into consumer devices, such as smartphones or TVs. The offline module pre-processes a color+depth image database to speed up the subsequent depth estimation. The online module infers a depth prior from a color query image using the previous database as training data. Then, it is refined through a segmentation-guided filtering. The conversion algorithm has been evaluated in three publicly available databases, and compared with several state-of-theart algorithms to prove its efficiency.","Color,
Three-dimensional displays,
Estimation,
Databases,
Two dimensional displays,
Machine learning algorithms,
Image color analysis"
A Review on Machine Learning and Data Mining Techniques for Residential Energy Smart Management,"In this paper, the different machine learning and data mining approaches used for Residential Energy Smart Management (RESM) will be discussed and classified according to some meaningful criteria. The proposed classification is an attempt to highlight the advantages and limitations of each category. Moreover, we emphasize the complementarity between approaches belonging to different categories and we point out the main challenges that still face RESM.","Home appliances,
Hidden Markov models,
Energy consumption,
Load modeling,
Monitoring,
Data mining,
Support vector machines"
A machine learning based approach for the detection and recognition of Bangla sign language,"Speech impaired people are detached from the mainstream society due to the lacking of proper communication aid. Sign language is the primary means of communication for them which normal people do not understand. In order to facilitate the conversation conversion of sign language to audio is very necessary. This paper aims at conversion of sign language to speech so that disabled people have their own voice to communicate with the general people. In this paper, Hand Gesture recognition is performed using HOG (Histogram of Oriented Gradients) for extraction of features from the gesture image and SVM (Support Vector Machine) as classifier. Finally, predict the gesture image with output text. This output text is converted into audible sound using TTS (Text to Speech) converter.","Gesture recognition,
Support vector machines,
Assistive technology,
Training,
Testing,
Feature extraction,
Databases"
Comparison of machine learning algorithms for breast cancer,"Machine learning algorithms are computer programs that try to predict cancer type based on the past data. The eventual goal of Machine learning algorithms in cancer diagnosis is to have a trained machine learning algorithm that gives the gene expression levels from cancer patient, can accurately predict what type and severity of cancer they have, aiding the doctor in treating it. The existing technology compares three different machine learning algorithms are Decision Tree, Support Vector Machine, Bayesian Belief Network. The main drawback of these algorithms is unusual because the number of features (gene expressions) far exceeds the number of cases (samples taken from patients). Performance efficiency can be achieved by comparing two more algorithms are Random Forest and Naïve Bayes algorithms. Because Random forest and Naïve Bayes are used as feature selection method, Random Forest is used to rank the feature importance and applied for relevant feedback. The requirements are weka tool, Java and Relational Database.","Breast cancer,
Machine learning algorithms,
Prediction algorithms,
Algorithm design and analysis,
Classification algorithms,
Training"
WeChat Text and Picture Messages Service Flow Traffic Classification Using Machine Learning Technique,"Network Traffic Classification carries great importance for both internet service providers (ISPs) and quality of services (QoSs) management. During the last two decades, a lot of machine learning models have been proposed and applied on different types of real time applications to classify their real time traffic and obtain very proficient accuracy results. However, no research has been done on WeChat text and picture messages traffic classification. In this paper, WeChat text and picture messages traffics are classified using two different types of datasets and 4 well-known machine learning algorithms. These two datasets, Harbin Institute of Technology (HIT) and Dorm13, are collected from two different network environments. Having captured the traffic 50 features, they are extracted respectively. Thereafter, well-known four machine learning algorithms C4.5 decision tree, Bayes Net, Naïve Bayes and SVM are used to classify WeChat text and picture messages traffic. Experimental result analysis show that using HIT data set all the applied machine learning classifiers classify WeChat text and picture messages traffic very accurately as compared to Dorm13 dataset. Using HIT dataset, all ML classifier perform very well, but C4.5 and SVM are the ones that give very effective accuracy results of 99.91% and 99.57% respectively as compared to other ML classifiers.","Machine learning algorithms,
Support vector machines,
Training,
Classification algorithms,
Feature extraction,
Decision trees,
Message service"
IR-UWB Radar Sensor for Human Gesture Recognition by Using Machine Learning,"In this paper, we propose a human gesture recognition algorithm using impulse radio ultra-wideband (IR-UWB) radar. The radar signal is transmitted into a three dimensional space, however, the received signal is only expressed in one dimensional. Therefore, it is difficult to classify 3-D gestures by analyzing specific features, such as power, peak value, index of peak value, and other values of received signal. To resolve this problem, a new human gesture recognition algorithm using machine learning is proposed. Two machine learning technics are used in this paper. One is unsupervised learning technic which is used for extracting features from received radar signal is principal component analysis, and the other one is supervised learning which is used for classifying gestures. The features are extracted by using the principal component analysis (PCA) method, then neural network method is used for training and classifying gestures using the extracted features. In training and classifying step, other method can be used, such as supporting vector machine (SVM), however, this method is hard to recognize noise gesture which means untrained gesture. To resolve this problem, we use neural network method in this paper, then in order to classy noise gestures and trained gestures, a noise determining algorithm is used.","Radar,
Feature extraction,
Gesture recognition,
Neural networks,
Training,
Principal component analysis,
Machine learning algorithms"
Machine Learning Based Approaches for Sex Identification in Bioarchaeology,"In this paper we approach from a machine learning perspective the problem of identifying the sex of archaeological remains from anthropometric data, an important problem with in the field of bio archaeology. As the conditions for detecting the sex of a skeleton are not entirely known, machine learning based data mining models are appropriate to address this problem since they are able to capture unobservable patterns in data. These patterns could be relevant for classifying a skeletal remain as male or female. We propose two machine learning models based on artificial neural networks for identifying the sex of human skeletons from bone measurements. The proposed models are experimentally evaluated on case studies generated from two data sets publicly available in the archaeological literature. The obtained results show that the proposed data mining models are effective for detecting the sex of archaeological remains, confirming the potential of our proposal.","Data models,
Bones,
Data mining,
Computational modeling,
Self-organizing feature maps,
Training"
Using Machine Learning to Decide When to Precondition Cylindrical Algebraic Decomposition with Groebner Bases,"Cylindrical Algebraic Decomposition (CAD) is a key tool in computational algebraic geometry, particularly for quantifier elimination over real-closed fields. However, it can be expensive, with worst case complexity doubly exponential in the size of the input. Hence it is important to formulate the problem in the best manner for the CAD algorithm. One possibility is to precondition the input polynomials using Groebner Basis (GB) theory. Previous experiments have shown that while this can often be very beneficial to the CAD algorithm, for some problems it can significantly worsen the CAD performance. In the present paper we investigate whether machine learning, specifically a support vector machine (SVM), may be used to identify those CAD problems which benefit from GB preconditioning. We run experiments with over 1000 problems (many times larger than previous studies) and find that the machine learned choice does better than the human-made heuristic.",
Machine learning approach for predicting end price of online auction,"An online auction is an auction held over the internet. In today's era of the internet, economies have changed the way than they were a few years back. The scope and reach of online auctions have been propelled by internet to the prominent level because online auctions break down and remove the physical limitations of traditional auctions such as geography, presence, time, space, etc. As online auction has become one of the fastest growing modes of online commerce transaction, sellers and buyers have started preferring to go online for purchasing and selling products respectively. As eBay has been the leading online auction marketplace for nearly two decades, the proposed system collected huge auction data from eBay and machine learning algorithms are used to predict end price of auction items. The proposed system is trained with 70% of dataset and 30% of dataset is used for testing. The proposed system used Naive Bayes which gives 99.33% accuracy in classification of whether the item will sell or not and kernel mapping SVM gives 96.3% accuracy for predicting whether an item maximize profit or not.","Support vector machines,
Prediction algorithms,
Internet,
Feature extraction,
Machine learning algorithms,
Decision trees,
Clustering algorithms"
A Novel Method for Tuning Configuration Parameters of Spark Based on Machine Learning,"Apache Spark is an open source distributed data processing platform, which can use distributed memory abstraction to process large volume of data efficiently. With the application of Apache Spark more and more widely, some problems are exposed. One of the most important aspects is the performance problem. Apache Spark has more than 180 configuration parameters, which can be adjusted by users according to their own specific application so as to optimize the performance. Currently these parameters are tuned manually by trial and error, which is ineffective due to the large parameter space and the complex interactions among the parameters. In this paper, in order to make the parameter tuning process of Spark more effective, a novel method for tuning configuration of Spark based on machine learning is proposed, which is composed of binary classification and multi-classification. This method can be used to auto-tune the configuration parameters of Spark. Furthermore, several common machine learning algorithms based on the proposed method are explored, and experimental results show that decision tree model (C5.0) is the best model considering the accuracy and computational efficiency. Finally, the experimental results also show that the performance can get average 36% gain with the proposed method compared with the default configuration of Spark.","Sparks,
Data models,
Tuning,
Predictive models,
Computational modeling,
Big data,
Optimization methods"
Helping HPC Users Specify Job Memory Requirements via Machine Learning,"Resource allocation in High Performance Computing (HPC) settings is still not easy for end-users due to the wide variety of application and environment configuration options. Users have difficulties to estimate the number of processors and amount of memory required by their jobs, select the queue and partition, and estimate when job output will be available to plan for next experiments. Apart from wasting infrastructure resources by making wrong allocation decisions, overall user response time can also be negatively impacted. Techniques that exploit batch scheduler systems to predict waiting time and runtime of user jobs have already been proposed. However, we observed that such techniques are not suitable for predicting job memory usage. In this paper we introduce a tool to help users predict their memory requirements using machine learning. We describe the integration of the tool with a batch scheduler system, discuss how batch scheduler log data can be exploited to generate memory usage predictions through machine learning, and present results of two production systems containing thousands of jobs.",
Energy efficient link adaptation using machine learning techniques for wireless OFDM,"Energy Efficiency in wireless communication is very important due to the slow progress in battery technology with improvement in technology. In this paper, we applied energy efficient link adaptation using Machine learning techniques in Orthogonal Frequency Division Multiplexing (OFDM). We sounding the channel condition periodically and observing the channel parameters. Our aim is to select the optimal mode of the channel which maximizes energy efficiency or throughput of data subject to a given quality of service (QoS) constraint. Simulation results show that the proposed solution achieves significant improvement over existing link adaptation algorithms. Presented work aims on maximizing the throughput and provides orders of magnitude gain in energy efficiency linked to poorly chosen fixed modes when used for energy efficiency maximization purposes.","Energy efficiency,
OFDM,
Throughput,
Wireless communication,
Energy consumption,
Transmitters,
Classification algorithms"
A Combined Analytical Modeling Machine Learning Approach for Performance Prediction of MapReduce Jobs in Cloud Environment,"Nowadays MapReduce and its open source implementation, Apache Hadoop, are the most widespread solutions for handling massive dataset on clusters of commodity hardware. At the expense of a somewhat reduced performance in comparison to HPC technologies, the MapReduce framework provides fault tolerance and automatic parallelization without any efforts by developers. Since in many cases Hadoop is adopted to support business critical activities, it is often important to predict with fair confidence the execution time of submitted jobs, for instance when SLAs are established with end-users. In this work, we propose and validate a hybrid approach exploiting both queuing networks and support vector regression, in order to achieve a good accuracy without too many costly experiments on a real setup. The experimental results show how the proposed approach attains a 21% improvement in accuracy over applying machine learning techniques without any support from analytical models.","Analytical models,
Data models,
Predictive models,
Computational modeling,
Training,
Cloud computing,
Noise measurement"
A machine learning approach to dynamically generate context sensitive applications on top of a monolithic enterprise system,This industrial research paper outlines an outcome of a machine leaning approach to dynamically generate context sensitive Application User Interfaces (Adaptive UI) on top of a monolithic enterprise software product.,"Context,
Software,
Business,
User interfaces,
Engines,
Google,
Roads"
Improving backfilling by using machine learning to predict running times,"The job management system is the HPC middleware responsible for distributing computing power to applications. While such systems generate an ever increasing amount of data, they are characterized by uncertainties on some parameters like the job running times. The question raised in this work is: To what extent is it possible/useful to take into account predictions on the job running times for improving the global scheduling? We present a comprehensive study for answering this question assuming the popular EASY backfilling policy. More precisely, we rely on some classical methods in machine learning and propose new cost functions well-adapted to the problem. Then, we assess our proposed solutions through intensive simulations using several production logs. Finally, we propose a new scheduling algorithm that outperforms the popular EASY backfilling algorithm by 28% considering the average bounded slowdown objective.","Hidden Markov models,
Scheduling algorithms,
Prediction algorithms,
Uncertainty,
Estimation,
Resource management"
Detecting Malicious URLs: A Semi-Supervised Machine Learning System Approach,"As malware industry grows, so does the means of infecting a computer or device evolve. One of the most common infection vector is to use the Internet as an entry point. Not only that this method is easy to use, but due to the fact that URLs come in different forms and shapes, it is really difficult to distinguish a malicious URL from a benign one. Furthermore, every system that tries to classify or detect URLs must work on a real time stream and needs to provide a fast response for every URL that is submitted for analysis (in our context a fast response means less than 300-400 milliseconds/URL). From a malware creator point of view, it is really easy to change such URLs multiple times in one day. As a general observation, malicious URLs tend to have a short life (they appear, serve malicious content for several hours and then they are shut down usually by the ISP where they reside in). This paper aims to present a system that analyzes URLs in network traffic that is also capable of adjusting its detection models to adapt to new malicious content. Every correctly classified URL is reused as part of a new dataset that acts as the backbone for new detection models. The system also uses different clustering techniques in order to identify the lack of features on malicious URLs, thus creating a way to improve detection for this kind of threats.",
A machine learning approach for indirect human presence detection using IOT devices,"This paper describes the construction of a system that uses information from several home automation devices, to detect the presence of a person in the space where the devices are located. The detection however doesn't rely on the information of devices that explicitly detect human presence, like motion detectors or smart cameras. The information used is the one available in the Muzzley system, which is a mobile application that allows the monitoring and control of several types of devices from a single program. The provided information was anonymized at the source. The first step was to extract adequate features for this problem. A labeling step is introduced using a combination of heuristics to assert the likelihood of anyone being home at a given time, based on all information available, including, but not limited to, direct presence detectors. The solution rests mainly on the use of supervised learning algorithms to train models that detect the presence without any information based on direct presence detectors. The model should be able to detect patterns of usage when the owner is at home rather than rely only on direct sensors. Results show that detection in this context is difficult, but we believe these results shed some light on possible paths to improve the system's accuracy.","Detectors,
Intelligent sensors,
Measurement,
Home automation,
Monitoring,
Ambient intelligence"
A Machine-Learning Approach to Automatic Detection of Delimiters in Tabular Data Files,Detection of string and column delimiters is a critical first step in the automated ingestion of files containing tabular data. In this paper we present an algorithm that uses a logistic-regression classifier to evaluate whether a particular choice of delimiters is correct. The delimiter choice that is given the highest score by the classifier is chosen as the one most likely to be correct. The algorithm makes the correct choice over 90% of the time on a test data set of files with a variety of different delimiters.,"Training,
Standards,
Classification algorithms,
Conferences,
Logistics,
Data models,
Automation"
Efficient statistical validation of machine learning systems for autonomous driving,"Today's automotive industry is making a bold move to equip vehicles with intelligent driver assistance features. A modern automobile is now equipped with a powerful computing platform to run multiple machine learning algorithms for environment perception (e.g., pedestrian detection) and motion control (e.g., vehicle stabilization). These machine learning systems must be highly robust with extremely small failure rate in order to ensure safe and reliable driving. In this paper, we propose a novel Subset Sampling (SUS) algorithm to efficiently validate a machine learning system. In particular, a Markov Chain Monte Carlo algorithm based on graph mapping is developed to accurately estimate the rare failure rate with a minimal amount of test data, thereby minimizing the validation cost. Our numerical experiments show that SUS achieves 15.2× runtime speed-up over the conventional brute-force Monte Carlo method.","Learning systems,
Vehicles,
Monte Carlo methods,
Feature extraction,
Machine learning algorithms,
Algorithm design and analysis,
Computational modeling"
Predicting Thread Profiles across Core Types via Machine Learning on Heterogeneous Multiprocessors,"Given that energy consumption has become one of the most important issues in computer systems, Heterogeneous Multiprocessors (HMPs) have been introduced, where large high performing and small power-efficient cores can co-exist on the same platform and share the processing of the workload. Clearly, the concept is the same whether it is multiple processors on a board or a chip multiprocessor with several cores on a chip. With the advent of HMPs, thread scheduling becomes much more challenging, while having to deal with thread to processor-type mapping. In particular, it is important that the operating system is able to understand the workload behavior when a thread is to be migrated to a core of a different type. In this paper, we describe a thread characterization method that explores machine learning techniques to automate and improve the accuracy of predicting thread execution across different processor types. We use hardware performance counters and use machine learning to predict performance when moving a thread to another core type on heterogeneous processors. We show that our characterization scheme achieves higher structural similarity (SSIM) values when predicting performance indicators, such as instructions per cycle and last-level cache misses, commonly used to determine the mapping of threads to processor types at runtime. We also show that support vector regression achieves higher SSIM values when compared to linear regression, and has very low (1%) overhead.",
Exploiting randomness in sketching for efficient hardware implementation of machine learning applications,"Energy-efficient processing of large matrices for big-data applications using hardware acceleration is an intense area of research. Sketching of large matrices into their lower-dimensional representations is an effective strategy. For the first time, this paper develops a highly energy-efficient hardware implementation of a class of sketching methods based on random projections, known as Johnson Lindenstrauss (JL) transform. Crucially, we show how the randomness inherent in the projection matrix can be exploited to create highly efficient fixed-point arithmetic realizations of several machine-learning applications. Specifically, the transform's random matrices have two key properties that allow for significant energy gains in hardware implementations. The first is the smoothing property that allows us to drastically reduce operand bit-width in computation of the JL transform itself. The second is the randomizing property that allows bit-width reduction in subsequent machine-learning applications. Further, we identify a random matrix construction method that exploits the special sparsity structure to result in the most hardware-efficient realization and implement the highly optimized transform on an FPGA. Experimental results on (1) the k-nearest neighbor (KNN) classification and (2) the principal component analysis (PCA) show that with the same bit-width the proposed flow utilizing random projection achieves an up to 7× improvement in both latency and energy. Furthermore, by exploiting the smoothing and randomizing properties we are able to use a 1-bit instead of a 4-bit multiplier within KNN, which results in additional 50% and 6% improvement in area and energy respectively. The proposed I/O streaming strategy along with the hardware-efficient JL algorithm identified by us is able to achieve a 50% runtime reduction, a 17% area reduction in the stage of random projection compared to a standard design.","Transforms,
Hardware,
Smoothing methods,
Principal component analysis,
Algorithm design and analysis,
Sparse matrices,
Signal processing algorithms"
Re-architecting the on-chip memory sub-system of machine-learning accelerator for embedded devices,"The rapid development of deep learning are enabling a plenty of novel applications such as image and speech recognition for embedded systems, robotics or smart wearable devices. However, typical deep learning models like deep convolutional neural networks (CNNs) consume so much on-chip storage and high-throughput compute resources that they cannot be easily handled by mobile or embedded devices with thrifty silicon and power budget. In order to enable large CNN models in mobile or more cutting-edge devices for IoT or cyberphysics applications, we proposed an efficient on-chip memory architecture for CNN inference acceleration, and showed its application to our in-house general-purpose deep learning accelerator. The redesigned on-chip memory subsystem, Memsqueezer, includes an active weight buffer set and data buffer set that embrace specialized compression methods to reduce the footprint of CNN weight and data set respectively. The Memsqueezer buffer can compress the data and weight set according to their distinct features, and it also includes a built-in redundancy detection mechanism that actively scans through the work-set of CNNs to boost their inference performance by eliminating the data redundancy. In our experiment, it is shown that the CNN accelerators with Memsqueezer buffers achieves more than 2× performance improvement and reduces 80% energy consumption on average over the conventional buffer design with the same area budget.","System-on-chip,
Bandwidth,
Machine learning,
Hardware,
Throughput,
Indexes,
Buffer storage"
Machine Learning with Memristors via Thermodynamic RAM,"Thermodynamic RAM (kT-RAM) is a neuromemristive co-processor design based on the theory of AHaH Computing and implemented via CMOS and memristors. The co-processor is a 2-D array of differential memristor pairs (synapses) that can be selectively coupled together (neurons) via the digital bit addressing of the underlying CMOS RAM circuitry. The chip is designed to plug into existing digital computers and be interacted with via a simple instruction set. Anti-Hebbian and Hebbian (AHaH) computing forms the theoretical framework from which a nature-inspired type of computing architecture is built where, unlike von Neumann architectures, memory and processor are physically combined for synaptic operations. Through exploitation of AHaH attractor states, memristor-based circuits converge to attractor basins that represents machine learning solutions such as unsupervised feature learning, supervised classification and anomaly detection. Because kT-RAM eliminates the need to shuttle bits back and forth between memory and processor and can operate at very low voltage levels, it can significantly surpass CPU, GPU, and FPGA performance for synaptic integration and learning operations. Here, we present a memristor technology developed for use in kT-RAM, in particular bi-directional incremental adaptation of conductance via short low-voltage (<1.0 V, <1.0 muS) pulses.",
A machine learning approach to fab-of-origin attestation,"We introduce a machine learning approach for distinguishing between integrated circuits fabricated in a ratified facility and circuits originating from an unknown or undesired source based on parametric measurements. Unlike earlier approaches, which seek to achieve the same objective in a general, design-independent manner, the proposed method leverages the interaction between the idiosyncrasies of the fabrication facility and a specific design, in order to create a customized fab-of-origin membership test for the circuit in question. Effectiveness of the proposed method is demonstrated using two large industrial datasets from a 65nm Texas Instruments RF transceiver manufactured in two different fabrication facilities.","Integrated circuits,
Fabrication,
Semiconductor device measurement,
Density measurement,
Electrical engineering,
Foundries"
Dynamic Line Rating Using Numerical Weather Predictions and Machine Learning: A Case Study,"In this paper, a dynamic line-rating experiment is presented in which four machine-learning algorithms (generalized linear models, multivariate adaptive regression splines, random forests and quantile random forests) are used in conjunction with numerical weather predictions to model and predict the ampacity up to 27 h ahead in two conductor lines located in Northern Ireland. The results are evaluated against reference models and show a significant improvement in performance for point and probabilistic forecasts. The usefulness of probabilistic forecasts in this field is shown through the computation of a safety-margin forecast which can be used to avoid risk situations. With respect to the state of the art, the main contributions of this paper are an in depth look at explanatory variables and their relation to ampacity, the use of machine learning with numerical weather predictions to model ampacity, the development of a probabilistic forecast from standard point forecasts, and a favorable comparison to standard reference models. These results are directly applicable to protect and monitor transmission and distribution infrastructures, especially if renewable energy sources and/or distributed power generation systems are present.","Weather forecasting,
Predictive models,
Conductors,
Correlation,
Forecasting,
Temperature measurement,
Power system dynamics"
Evaluation of rehearsal effects of multimedia content based on EEG using machine learning algorithms,"Rehearsal is a common phenomenon of practicing something to make it more resilient in long-term memory. This paper will present the rehearsal effects based on electroencephalography (EEG) recorded data for multimedia contents. Three frequency based features are used to discriminate the three learning states mentioned as L1, L2 and L3 using machine learning algorithms. From these three learning states, L1 is the first learning state whether L2 and L3 are the rehearsal states of L1. The set of spectral features that are used for analysis are based on the intensity weighted mean frequency (IWMF), its bandwidth (IWBW), and spectral power density (PSD). For the analysis, the three brain waves investigated are the alpha waves, theta waves and delta waves. The results of the study show that the alpha waves produce de-synchronization from rest to learning state as compared to other EEG recorded waves. This de-synchronization lead to mental effort imposed by working memory during a learning task. The Alpha wave shows more accuracy in L1 using SVM classifier that is 85% using PSD features, 86% for IWFM and 78.4% using IBWB feature. The results also mention that L3 produces less classifier accuracy value as compared to the L2 and L1 for each of three extracted features. This indicates that L3 requires less mental effort during learning. The findings proved the rehearsal as a good phenomenon of long-term memorized learning.","Electroencephalography,
Feature extraction,
Frequency-domain analysis,
Multimedia communication,
Discrete wavelet transforms,
Machine learning algorithms,
Bandwidth"
Eye state prediction using ensembled machine learning models,"As electric signals are transmitted between the brain cell s for transferring of data within the brain, capturing of these signals can result in understanding the functionality of brain and other directly linked parts (like eyes, ears, spinal nerves etc) of our body. We can also capture epileptic seizures that are caused by a disruption in the working of brain, by the Electro Encephalogram Test. These electric signals are to be captured by small electrodes placed on human scalp using a standard 10/20 system on an Electro Encephalograph monitor. In this work, we will predict the state of eye (open or closed) by exploring 13 machine learning models on a 15 features dataset of an EEG test. The records of 14 electrodes are used for this prediction. Results are evaluated using 6 different machine learning parameters i.e. Sensitivity, Confusion matrix, Kappa value, Specificity, Accuracy and Receiver Operating Characteristics (ROC) curve. K-fold validation and ensembling of models will be done on best three predictive models pertaining to our dataset.","Brain models,
Object oriented modeling,
Electrodes,
Predictive models,
Data models,
Analytical models"
Machine learning algorithms for document clustering and fraud detection,"Machine Learning plays very important role in processing of large amounts of structured and unstructured data. A set of algorithms can be used to get meaningful insights into the data that are helpful in making effective business decisions. Document clustering is one of the popular machine learning technique used to group unstructured data (text documents) based on its content and further analyze the data to understand the patterns in it. The unstructured data gets transformed into semi-structured data and structured data in stages by using text mining and clustering (k-means) techniques. Classification is another machine learning technique that can be implemented for use cases like ""fraud detection and cross-sell & up-sell opportunity identification"" in banking, financial services and insurance industry. This paper focuses on the implementation of both document clustering algorithm and a set of classification algorithms (Decision Tree, Random Forest and Naïve Bayes), along with appropriate industry use cases. Also, the performance of three classification algorithms will be compared by calculation of ""Confusion Matrix"" which in turn helps us to calculate performance measures such as, ""accuracy"", ""precision"", and ""recall"".","Classification algorithms,
Feature extraction,
Decision trees,
Entropy,
Prediction algorithms,
Insurance,
Machine learning algorithms"
Multiclass mood classification on Twitter using lexicon dictionary and machine learning algorithms,"Sentiment Analysis (SA), an application of Natural Language processing (NLP), has evolved a lot over the past decade. It is also known as opinion mining, mood extraction and emotion analysis. Social Media provides a good platform for people to share their thoughts, feelings and views. Social Media like Twitter is flooded everyday with tweets from users. These tweets can be effectively used for extracting mood of a person. Extraction of mood from texts is a challenging task as it involves understanding the underlying semantic. In this paper a new approach is been proposed that uses lexicon database to assign each word in a text a value called as `Impact Factor'. The Impact Factor is nothing but how a single word is affecting the whole sentence in which it is used. Every word in a sentence has its own Impact Factor and it tries to influence the overall semantic of the sentence. Higher the value of Impact Factor of a word in the sentence, the more influential it is. The approach proposed in this paper makes use of lexicon based approach as well as machine based learning. It uses AFINN lexicon database to assign Impact Factor to words and Support Vector Machine (SVM), k-Nearest Neighbors (KNN) and Naive Bayesian (NB) machine learning algorithms for training and testing the model.","Mood,
Social network services,
Support vector machines,
Semantics,
Feature extraction,
Training,
Sentiment analysis"
Postsilicon Trace Signal Selection Using Machine Learning Techniques,"A key problem in postsilicon validation is to identify a small set of traceable signals that are effective for debug during silicon execution. Structural analysis used by traditional signal selection techniques leads to a poor restoration quality. In contrast, simulation-based selection techniques provide superior restorability but incur significant computation overhead. In this paper, we propose an efficient signal selection technique using machine learning to take advantage of simulation-based signal selection while significantly reducing the simulation overhead. The basic idea is to train a machine learning framework with a few simulation runs and utilize its effective prediction capability (instead of expensive simulation) to identify beneficial trace signals. Specifically, our approach uses: (1) bounded mock simulations to generate training vectors for the machine learning technique and (2) a compound search-space exploration approach to identify the most profitable signals. Experimental results indicate that our approach can improve restorability by up to 143.1% (29.2% on average) while maintaining or improving runtime compared with the state-of-the-art signal selection techniques.","Integrated circuit modeling,
Predictive models,
Training,
Computational modeling,
Silicon,
Machine learning algorithms,
Prediction algorithms"
Large-Scale Cover Song Retrieval System Developed Using Machine Learning Approaches,"Large-scale cover song retrieval systems should be able to calculate song-to-song similarity and accommodate differences in timing, key, and tempo. Simple vector distance measure is not adequately powerful to perform cover song recognition, and expensive solutions such as dynamic time warping do not scale to millions of instances, making cover song retrieval inappropriate for commercial-scale applications. In this work, we used the content-based music features of songs as input and transformed them into vectors by using the 2D Fourier transform approach. Furthermore, we explored different machine learning approaches to reinforce the pattern of these vectors. By projecting the songs into a semantic vector space, we can use the efficient nearest neighbor algorithm to compare the similarity of songs and retrieve the most similar songs from the large-scale database. The proposed system is not only efficient enough to perform scalable content-based music retrieval but can also develop the potential of machine learning approaches, making similar music recognition applications faster and more accurate.","Two dimensional displays,
Fourier transforms,
Principal component analysis,
Multiple signal classification,
Feature extraction,
Machine learning algorithms"
A review on opinionated sentiment analysis based upon machine learning approach,"Sentiment analysis is a rapidly emerging field and has attracted a large community of researchers to analyze an amalgamated form of people expressions shared on the internet. There are several sources like e-commerce sites, social networking sites, micro blogging sites which help in capturing user reactions corresponding to different market products and other important issues in the form of feedback. The opinions extracted from these feedbacks are useful for both individual and organizations to predict user opinionated sentiments and customer preferences. The key emphasis is put on the employing of an accurate method in the experiments to obtain correctness in the results. The main focus of this paper is to review Sentiment Analysis domain by using machine learning approach. The study is carried out by considering different facets like granularity, algorithms, polarity, applications, and data type related to the domain, which is presented and discussed in a simple manner.",
Exploiting distributional semantics to benefit machine learning in automated classification of Chinese clinical text,"Machine learning has been widely employed for the automated classification of clinical text to enhance the utilization of clinical information and benefit clinical applications. However, the conventional approaches for the vector representations of text in machine learning algorithms cannot model the connections between highly similar words and will also lead to high dimensionality. This study suggests a combination of distributional semantics and supervised classification algorithms in order to tackle these problems. Latent Semantic Analysis, Random Indexing and word2vec are adopted for distributional semantic representations to build classifiers using Support Vector Machines, Naïve Bayes and k-Nearest Neighbors. As an initial study, we adopt Chinese diagnostic phrases as the clinical text to be classified and the 3-digit ICD-10 codes as the class labels. The evaluation results demonstrate that distributional semantic representations can better capture the meanings of text and can improve the accuracy of clinical text classification when the data for training and testing come from different sources and share less consistent language use. Consequently, distributional semantics can enhance the extensibility of the classifiers for clinical text classification.","Semantics,
Support vector machines,
Niobium,
Computational modeling,
Sections"
Using the machine learning approach to predict patient survival from high-dimensional survival data,"Survival analysis with high-dimensional data deals with the prediction of patient survival based on their gene expression data and clinical data. A crucial task for the accuracy of survival analysis in this context is to select the features highly correlated with the patient's survival time. Since the information about class labels is hidden, existing feature selection methods in machine learning are not applicable. In contrast to classical statistical methods which address this issue with the Cox score, we propose to tackle this problem by discretizing the survival time of patients into a suitable number of subgroups via silhouettes clustering validity. To cope with patients' censoring, we use “k-nearest neighbor” based on clinical parameters. Feature selection is then accomplished using Fast Correlation-Based Filtering approach from machine learning community. The effectiveness and efficiency of the proposed method are demonstrated through comparisons with classical statistical methods on real-world datasets and simulation datasets.","Cancer,
Computers"
Machine Learning Approach for the Predicting Performance of SpMV on GPU,"Sparse Matrix-Vector Multiplication (SpMV) kernel dominates the computing cost in numerous scientific applications. Many implementations based on different sparse formats were proposed recently for optimizing this kernel on the GPU side. Since the performance of the SpMV varies significantly according to the sparsity characteristics of the input matrix and the hardware features, developing an accurate performance model for this kernel is a challenging task. The traditional approach of building such models by analytical modeling is difficult in practice and requires a thorough understanding of the interaction between the GPU hardware and the sparse code. In this paper, we propose to use a machine learning approach to predict the performance of the SpMV kernel using several sparse formats (COO, CSR, ELL, and HYB) on GPU. We used two popular machine learning algorithms, Support Vector Regression (SVR) and Multilayer Perceptron neural network (MLP). Our experimental results on two different GPUs (Fermi GTX 512 and Maxwell GTX 980 Ti) show that the SVR models deliver the best accuracy with average prediction error ranging between 7% and 14%.","Sparse matrices,
Graphics processing units,
Kernel,
Machine learning algorithms,
Computational modeling,
Arrays,
Adaptation models"
Machine learning approach for classifying the cognitive states of the human brain with functional magnetic resonance imaging (fMRI),"Cognitive state classification is a challenging task. Many studies were reported using different neuroimaging modalities for classification of the cognitive states of the human brain e.g., EEG, fMRI, MEG etc. However, functional MRI seems to be appropriate for these papers as due to its good spatial resolution and localizing the brain activated regions. In this paper, our objective is to identify the different cognitive brain states. For example, classifying the patterns of high and low cognitive loads. We acquired the fMRI data on the healthy participants. First, data is preprocessed to remove the artifacts and motions corrections. Next, regions of interest were extracted from functional brain volumes of the two states. Data reduction is also performed and data were passed to machine learning classifier i.e., support vector machine. The results showed that high and low cognitive loads were successfully classified with good accuracy.","Support vector machines,
Magnetic resonance imaging,
Electroencephalography,
Brain,
Data acquisition,
Spatial resolution,
Principal component analysis"
Machine learning for affective computing and its applications to automated measurement of human facial affect,"Summary form only given. Automated analysis of human affective behaviour has received a significant research attention over the last decade due to its practical importance in areas such as health, human-computer interaction, social robotics, and marketing, to mention but a few. Traditionally, different behavioural cues are first extracted from the sensory inputs (video, speech and/or physiology). Then, machine learning algorithms are designed to analyse these behavioural cues with the aim of automatically predicting target affective states. However, the modelling of human affect (such as emotion expressions or pain levels) is rather challenging due to the many possible sources of variation in target data, including the target subjects (male vs. female, children vs. adults, etc.), their tasks (human-human or human-robot interaction), culture (eastern vs. western), and so on. All of these make the task of automated estimation of human affect highly context-sensitive. In this talk, I will first provide a general overview of recent trends in the field of affective computing. Then, I will illustrate its application to the domain of facial behaviour analysis by focusing on the most recent advances in estimation of human facial behaviour from static images and video data. To this end, I will describe the state-of-the-art machine learning techniques proposed for context-sensitive modelling of human facial expressions of basic emotions, facial action units and their intensity, as well as the clinical measurement of patient's pain levels. Finally, I will outline the main challenges and provide future directions for applying these approaches `in-the-wild', i.e., naturalistic scenarios such as human-robot interaction and in the context of treatment of neuro developmental disorders (such as autism).","Affective computing,
Robot sensing systems,
Pain,
Human-robot interaction,
Estimation,
Area measurement,
Media"
Timed Dataflow: Reducing Communication Overhead for Distributed Machine Learning Systems,"Many distributed machine learning (ML) systems exhibit high communication overhead when dealing with big data sets. Our investigations showed that popular distributed ML systems could spend about an order of magnitude more time on network communication than computation to train ML models containing millions of parameters. Such high communication overhead is mainly caused by two operations: pulling parameters and pushing gradients. In this paper, we propose an approach called Timed Dataflow (TDF) to deal with this problem via reducing network traffic using three techniques: a timed parameter storage system, a hybrid parameter filter and a hybrid gradient filter. In particular, the timed parameter storage technique and the hybrid parameter filter enable servers to discard unchanged parameters during the pull operation, and the hybrid gradient filter allows servers to drop gradients selectively during the push operation. Therefore, TDF could reduce the network traffic and communication time significantly. Extensive performance evaluations in a real testbed showed that TDF could reduce up to 77% and 79% of network traffic for the pull and push operations, respectively. As a result, TDF could speed up model training by a factor of up to 4 without sacrificing much accuracy for some popular ML models, compared to systems not using TDF.","Servers,
Training data,
Training,
Synchronization,
Bandwidth,
Optimization,
Open area test sites"
A study of spatial machine learning for business behavior prediction in location based social networks,"Understanding business behaviors requires acquiring huge amounts of data from diverse field studies. Location Based Social Networks can provide such large amounts of data that can be used in urban analysis to understand business behaviors. Towards more insight for business behavior, a novel analytical prespective that exploits data collected from Location Based Social Networks is introduced to predict business turnouts. Prediction is implemented using machine learning techniques. Spatial regression models are investigated through a comparative study to model the dataset features relationships for business behavior prediction. Geographically Weighted Regression model is found to be the most appropriate in predicting business turnouts of objects provided by Location Based Social Networks. Moreover, a Partitioned Geographically Weighted Regression model is proposed to deal with the data heterogeneity nature pursuing more accurate predictions for the business turnouts. An experimental case study, using data about venues registered in Foursquare is conducted to assess the performance of the proposed methods. The experimental results confirm the best performance by the Geographically Weighted Regression compared to Durbin, Durbin Error, Spatial Lag, Spatial Error, and Spatial Lag X regression models presented in this study. Moreover, the proposed Partitioned Geographically Weighted Regression model experimental results showed better prediction accuracy compared to the classical Geographically Weighted Regression model.","Business,
Predictive models,
Data mining,
Data models,
Facebook,
Data analysis"
Relative entropy normalized Gaussian supervector for speech emotion recognition using kernel extreme learning machine,"Speech emotion recognition is a challenging and significant task. On the one hand, the emotion features need to be robust enough to capture the emotion information, and while on the other, machine learning algorithms need to be insensitive to model the utterance. In this paper, we presented a novel framework of speech emotion recognition to address the two above-mentioned challenges. Relative Entropy based Normalization (REN) was proposed to normalize the supervectors of Gaussian Mixture Model-Universal Background Model (GMM-UBM) as the features to emotions. The Kernel Extreme Learning Machine (KELM) was adopted as the classifier to identify the emotion represented by the normalized supervectors. Experimental results on the EMR_1309 corpus showed the proposed framework outperformed the state-of-the-art i-vector based systems.","Speech,
Entropy,
Speech recognition,
Feature extraction,
Emotion recognition,
Kernel,
Covariance matrices"
Comparative study of machine learning algorithms for breast cancer detection and diagnosis,"Breast cancer is one of the most widespread diseases among women in the UAE and worldwide. Correct and early diagnosis is an extremely important step in rehabilitation and treatment. However, it is not an easy one due to several uncertainties in detection using mammograms. Machine Learning (ML) techniques can be used to develop tools for physicians that can be used as an effective mechanism for early detection and diagnosis of breast cancer which will greatly enhance the survival rate of patients. This paper compares three of the most popular ML techniques commonly used for breast cancer detection and diagnosis, namely Support Vector Machine (SVM), Random Forest (RF) and Bayesian Networks (BN). The Wisconsin original breast cancer data set was used as a training set to evaluate and compare the performance of the three ML classifiers in terms of key parameters such as accuracy, recall, precision and area of ROC. The results obtained in this paper provide an overview of the state of art ML techniques for breast cancer detection.","Support vector machines,
Radio frequency,
Breast cancer,
Training,
Medical diagnostic imaging,
Bayes methods"
A Stackelberg game perspective on the conflict between machine learning and data obfuscation,"Data is the new oil; this refrain is repeated extensively in the age of internet tracking, machine learning, and data analytics. As data collection becomes more personal and pervasive, however, public pressure is mounting for privacy protection. In this atmosphere, developers have created applications to add noise to user attributes visible to tracking algorithms. This creates a strategic interaction between trackers and users when incentives to maintain privacy and improve accuracy are misaligned. In this paper, we conceptualize this conflict through an N + 1-player, augmented Stackelberg game. First a machine learner declares a privacy protection level, and then users respond by choosing their own perturbation amounts. We use the general frameworks of differential privacy and empirical risk minimization to quantify the utility components due to privacy and accuracy, respectively. In equilibrium, each user perturbs her data independently, which leads to a high net loss in accuracy. To remedy this scenario, we show that the learner improves his utility by proactively perturbing the data himself. While other work in this area has studied privacy markets and mechanism design for truthful reporting of user information, we take a different viewpoint by considering both user and learnerperturbation.","risk management,
data analysis,
data protection,
game theory,
learning (artificial intelligence),
minimisation"
SVM based machine learning approach to identify Parkinson's disease using gait analysis,"Parkinson's Disease (PD) is a neuro-degenerative disease which affects a persons mobility. Tremors, rigidity of the muscles and imprecise gait movements are characteristics of this disease. Past attempts have been made to classify Parkinsons disease from healthy subjects but in this work, effort was made to focus on the specific gait characteristics which would help differentiate Parkinsons Disease from other neurological diseases (Amyotrophic lateral sclerosis (ALS) and Huntingtons Disease) as well as healthy controls. A range of statistical feature vector considered here from the Time-series gait data which are then reduced using correlation matrix. These feature vectors are then individually analysed to extract the best 7 feature vectors which are then classified using a Gaussian radial basis function kernel based Support vector machine (SVM) classifier. Results show that the 7 features selected for SVM achieves good overall accuracy of 83.33%, good detection rate for Parkinsons disease of 75% and low false positive results of 16.67%.",
A machine learning based framework for parameter based multi-objective optimisation of a H.265 video CODEC,"All multimedia devices now incorporate video CODECs that comply with international video coding standards such as H.264 / MPEG4-AVC and the new High Efficiency Video Coding Standard (HEVC) otherwise known as H.265. Although the standard CODECs have been designed to include algorithms with optimal efficiency, large number of coding parameters that can be used to fine tune their operation, within known constraints of for e.g., available computational power, bandwidth, energy consumption, etc. With large number of such parameters involved, determining which parameters will play a significant role in providing optimal quality of service within given constraints is a further challenge that needs to be met. We propose a framework that uses machine learning algorithms to model the performance of a video CODEC based on the significant coding parameters. We define objective functions that can be used to model the video quality, CPU time utilisation and bit-rate. We show that these objective functions can be practically utilised in video Encoder designs, in particular in their performance optimisation within given constraints. A Multi-objective Optimisation framework based on Genetic Algorithms is thus proposed to optimise the performance of a video codec. The framework is designed to jointly minimize the complexity, Bit-rate and to maximize the quality of the compressed video stream.","Optimization,
Encoding,
Video codecs,
Streaming media,
Standards,
High efficiency video coding,
Decoding"
Improving the Performance of Secure Cloud Infrastructure with Machine Learning Techniques,"Security is one of the key concerns of the cloud user community. Most important ask from cloud users is to provide quality of services to manage Security end-to-end. The quality of Services (QoS) for securing cloud images were proposed in the earlier papers [2][3]. However, the key concern that still remains is how to balance performance and security. In this paper, a new and an intelligent model of QoS is being proposed which determines the files to be decrypted, without decrypting the entire file list in the secure wallet. Smart QoS is proposed as an extension to the security method proposed and presented in CCEM 2015[5] to improve the performance of Secure Cloud. The Smart QoS is capable of addressing some of the security concerns of cloud user community ensuring security and performance as well. Machine learning techniques have been used to design and develop the Smart Quality of Services. Solutions to ensure end-to-end security in cloud environments were proposed by us in the earlier paper [5]. In this paper, we have extended the security method proposed in [5] to ensure security and performance. Proposed Model is experimented in HPE Helion Cloud on two real time scenarios and results are attached to this paper.","Quality of service,
Cloud computing,
Cryptography,
Standards,
Filtering,
Optimization"
Machine learning based estimation of Ozone using spatio-temporal data from air quality monitoring stations,"In this paper, models are created to predict the levels of ground level Ozone at particular locations based on the cross-correlation and spatial-correlation of different air pollutants whose readings are obtained from several different air quality monitoring stations in Gauteng province, South Africa, including the City of Johannesburg which is on the cusp of being one of the world's megacities and is currently the most polluted city in the country. Datasets spanning several years collected from the monitoring stations and transmitted through the Internet-of-Things are used. Big data analytics and cognitive computing is used to get insights on the data and create models that can estimate levels of Ozone without requiring massive computational power or intense numerical analysis.",
A demonstration of machine learning for explicit functions for cycle time prediction using MES data,"Cycle time prediction represents a challenging problem in complex manufacturing scenarios. This paper demonstrates an approach that uses genetic programming (GP) and effective process time (EPT) to predict cycle time using a discrete event simulation model of a production line, an approach that could be used in complex manufacturing systems, such as a semiconductor fab. These predictive models could be used to support control and planning of manufacturing systems. GP results in a more explicit function for cycle time prediction. The results of the proposed approach show a difference between 1-6% on the demonstrated production line.","Predictive models,
Manufacturing systems,
Production facilities,
Mathematical model,
Semiconductor device modeling,
Measurement"
Classification of primary biliary cirrhosis using hybridization of dimensionality reduction and machine learning methods,"The key functioning of human body depends on liver health. Liver performs numerous metabolic functions that also enable smooth working of other organs. Any form of illness in liver leads to liver diseases. These diseases are of many types out of which the most commonly occurring are hepatitis A, B, C, D and E, primary biliary cirrhosis (PBC), liver fibrosis, liver tumor, alcoholic liver disease, liver cirrhosis, fatty liver disease and autoimmune hepatitis. Presence of these diseases in various forms indicates the significance of accurate and timely diagnosis. This study accordingly aims to classify PBC stages using individual classifiers and hybrid models. Individual methods include linear discriminant analysis (LDA), diagonal linear discriminant analysis (DLDA), euclidean distance based k-nearest neighbors (KNN), and hybrid models include combination of LDA, DLDA and KNN with dimensionality reduction method. Simulations results showed that hybrid frameworks outperform individual classifiers in terms of classification performance. Furthermore, KNN based hybridization achieved a remarkable accuracy of 91.3%.","Training,
Testing,
Diseases,
Liver diseases,
Analytical models,
Artificial neural networks"
Prediction of the efficacy of Wuji Pills by machine learning methods,"Efficacy prediction is an inseparable part of TCM. We firstly analyze the correlation between indicators and efficacy, and max blood-drug concentration(Cmax) is chosen as the target to reflect the efficacy of drugs. Then we apply linear regression(LR), support vector regression(SVR) as well as artificial neural networks(ANNs) to predict the efficacy of Wuji pills. The results of the leave-one-out method show that SVR performs better than other methods for label Cmax, and appears to be a good method for this task. In order to find the relationship between each component of Wuji Pills, several visualization methods are adopted to deal with this problem. The web server of prediction is available at http://data.jindengtai.cn/#/case/drug for public usage.","Visualization,
Web servers,
Biochemistry,
Neural networks,
Optimization,
Support vector machines"
Improved microarray data analysis using feature selection methods with machine learning methods,"Microarray data analysis directly relates with the state of disease through gene expression profile, and is based upon several feature extractions to classification methodologies. This paper focuses on the study of 8 different ways of feature selection preprocess methods from 4 different feature selection methods. They are Minimum Redundancy-Maximum Relevance (mRMR), Max Relevance (MaxRel), Quadratic Programming Feature Selection (QPFS) and Partial Least Squared (PLS) methods. In this study, microarray datasets of colon cancer and leukemia cancer were used for implementing and testing four different classifiers i.e. K-Nearest-Neighbor (KNN), Random Forest (RF), Support Vector Machine (SVM) and Neural Network (NN). The performance was measured by accuracy and AUC (area under the curve) value. The experimental results show that discretization can somehow improve performance of microarray data analysis, and mRMR gives the best performance of microarray data analysis on the colon and leukemia datasets. We also list some results on comparative performance of methods for the specific (data-ratio) number of features.","Support vector machines,
Radio frequency,
Computational modeling,
Artificial neural networks"
An Investigation of Transfer Learning and Traditional Machine Learning Algorithms,"Previous research focusing on the evaluation of transfer learning algorithms has predominantly used real-world datasets to measure an algorithm's performance. A test with a real-world dataset exposes an algorithm to a single instance of distribution difference between the training (source) and test (target) datasets. These previous works have not measured performance over a wide-range of source and target distribution differences. We propose to use a test framework that creates many source and target datasets from a single base dataset, representing a diverse-range of distribution differences. These datasets will be used as a stress test to measure an algorithm's performance. The stress test process will measure and compare different transfer learning algorithms and traditional learning algorithms. The unique contributions of this paper, with respect to transfer learning, are defining a test framework, defining multiple distortion profiles, defining a stress test suite, and the evaluation and comparison of different transfer learning and traditional machine learning algorithms over a wide-range of distributions.","Distortion,
Machine learning algorithms,
Stress,
Testing,
Training,
Training data,
Algorithm design and analysis"
Citizen security using machine learning algorithms through open data,"The following work is an application proposal based on machine learning algorithms for a possible solution for the public safety problem in a South American city. The aim of this application is to reduce the threat risk of the physical integrity of pedestrians by geolocating, in real-time, safer places to walk. In this context for a city, San Isidro, a business district of Lima, has been established as study case. The district has been divided into map sectors and subsectors, so that by using the GPS location service integrated in mobile devices, it is possible to identify areas that have the highest incidence of different types of incidents. This functionality will allow users to choose safer routes by taking into account the information provided for each sector. The data used in this application has been obtained from an Open Data platform managed by the San Isidro municipality. In this application, we have processed the data enabling the easy and friendly access to the information by the end user. The importance of this work is how we have used the machine learning algorithm for incident rates in real and future time, trying to make predictions that can not only provide safe routes to users, but also predict disasters and allow public authorities to act in advance, thus minimizing the impact of future incidents.","Automobiles,
Vehicle crash testing,
Urban areas,
Machine learning algorithms,
Correlation,
Prediction algorithms"
Machine Learning-Based Framework for Resource Management and Modelling for Video Analytic in Cloud-Based Hadoop Environment,"Hadoop framework has recently been adapted for use by the video analytics community for intensive, distributed video processing, storage. However, the challenge is to estimate the required amount of resources to be used in such an environment to fulfil the requirements of a user with requirements constraints. Therefore, it is important to understand how to model the performance of a Hadoop based implementation of video analytic applications in terms of meeting their performance goals. In this paper we propose the use of machine learning approachs in modelling the execution time based on the given resources. The prediction is based on parameters related to typical video analytic applications such as video file characteristics (e, g, resolution, file size, frame rate. etc.), cluster resource consumption,, Hadoop configuration values (reducer slots, tasks). The investigation carried out compares the use of different machine learning classifiers with regard to their best obtainable performance accuracies, show that a decision based model (M5P) outperforms a Linear Regression model, while the Ensemble Classifier, Bagging, out-performs these standard single classifiers. The research conducted bridges an existing research gap in video analytic-related performance predictions, whereby current research focuses on different application types, is largely limited to using standard learning algorithms such as SVM, Linear Regression, Multilayer Perceptron (MLP).",
Code clones detection using machine learning technique: Support vector machine,"Code clones defined as sequence of source code that occur more than once in the same program or across different programs are undesirable as they increase the size of program and creates the problems of redundancy. Fixing of bugs detected in one clone require detection of all clones. Hence, it is imperative to identify and remove all code clones in a program. The focus of previous research work on the code clone detection was to find identical clones, or clones that are identical up to identifiers and literal values. But, detection of similar clones is often important. In the present paper it is proposed to generate the feature sets after parsing the given C program for code fragments and then match their similarity. On the basis of feature sets the classification of algorithm is being performed by using the Support Vector Machine (SVM) as a machine learning tool. The output of the machine tool would be the similarity ratio with which the two C programs are related to each other and also the class in which they would occur. It was observed that the test results of the tool implementation show detection of code clones in the program and its accuracy increases with the increase in number of instances.","Cloning,
Support vector machines,
Machine learning algorithms,
Software,
Automation,
Redundancy,
Syntactics"
Machine learning for predictive modeling in management of operations of EDM equipment product,"To sustain and excel in competitive global market, organizations often bank on high productivity and world class quality. Endeavor of this research is to comprehend and model the manufacturing process of Electrical Discharge Machining (EDM) equipment product in order to increase productivity. Outcome of EDM operation is strongly influenced by various process parameters. The paper presents a framework based on machine learning algorithms to analyze the relationship between input process parameters and EDM response to build a predictive model of EDM operations. Physical experimentations have conducted considering Discharge Current, Pulse Duration, Duty Cycle and Discharge Voltage as independent variables while Material Removal Rate has been used as target variable. Four different machine learning algorithms namely Random Forest, Support Vector Regression, Elastic Net and Bagging have been adopted as applied predictive modeling tools. Results justify the usage of machine learning methods to deal with the research problem. Statistical analysis has been conducted as well for comparative performance analysis. Further correlation based supervised feature selection methodology has been applied to identify the key predictors.","Predictive models,
Kernel,
Mathematical model,
Bagging,
Machining,
Discharges (electric),
Radio frequency"
A Machine-Learning-Driven Sky Model,"Sky illumination is responsible for much of the lighting in a virtual environment. A machine-learning-based approach can compactly represent sky illumination from both existing analytic sky models and from captured environment maps. The proposed approach can approximate the captured lighting at a significantly reduced memory cost and enable smooth transitions of sky lighting to be created from a small set of environment maps captured at discrete times of day. The author's results demonstrate accuracy close to the ground truth for both analytical and capture-based methods. The approach has a low runtime overhead, so it can be used as a generic approach for both offline and real-time applications.","Lighting,
Atmospheric modeling,
Computational modeling,
Illumination ,
Neural networks,
Analytical models,
Virtual environments"
A method to predict diagnostic codes for chronic diseases using machine learning techniques,"Healthcare in simplest form is all about diagnosis and prevention of disease or treatment of any injury by a medical practitioner. It plays an important role in providing quality life for the society. The concern is how to provide better service with less expensive therapeutically equivalent alternatives. Machine Learning techniques (ML) help in achieving this goal. Healthcare has various categories of data like clinical data, claims data, drugs data and hospital data. This paper focuses on clinical and claims data for studying 11 chronic diseases such as kidney disease, osteoporosis, arthritis etc. using the claims data. The correlation between the chronic diseases and the corresponding diagnostic tests is analyzed, by using ML techniques. An effective conclusion on various diagnostics for each chronic disease is made, keeping in mind the clinical relevance.","Diseases,
Heart,
Medical diagnostic imaging,
Diabetes,
Data mining,
Predictive models"
Early detection of plant faults by using machine learning,"To detect fault early is very important for safety. This paper proposes a method to detect plant faults early. For the early detections, to classify whether the signals is an early sign of faults or not in subtle signal difference is necessary. To the classification, this paper uses SVM (Support Vector Machine), which is one of a powerful classification technique of machine learning. Data in abnormal state of plants is obtained a few moments later after a fault occurs, hence in early stage of faults, the data of abnormal state is not available. Only measured data at normal state of plants is available in learning stage of the method. For a classification of such cases where only one side of data, which is from normal state, available, one class SVM is useful. In addition, to classify a very subtle signal as a signal of abnormal state, a high generalization ability is necessary for SVM. To get the high ability, a new kernel function, the generalized Gaussian function is used. To show the effectiveness of the method given in this paper, a simulation example of a water level control experimental plant is given.","Support vector machines,
Fault detection,
Kernel,
Level control,
Temperature measurement,
Electronic mail,
Noise measurement"
A machine learning approach for recognising woody plants on railway trackbeds,"The purpose of this work in progress study was to test the concept of recognising plants using images acquired by image sensors in a controlled noise-free environment. The presence of vegetation on railway trackbeds and embankments presents potential problems. Woody plants (e.g. Scots pine, Norway spruce and birch) often establish themselves on railway trackbeds. This may cause problems because legal herbicides are not effective in controlling them; this is particularly the case for conifers. Thus, if maintenance administrators knew the spatial position of plants along the railway system, it may be feasible to mechanically harvest them. Primary data were collected outdoors comprising around 700 leaves and conifer seedlings from 11 species. These were then photographed in a laboratory environment. In order to classify the species in the acquired image set, a machine learning approach known as Bag-of-Features (BoF) was chosen. Irrespective of the chosen type of feature extraction and classifier, the ability to classify a previously unseen plant correctly was greater than 85%. The maintenance planning of vegetation control could be improved if plants were recognised and localised. It may be feasible to mechanically harvest them (in particular, woody plants). In addition, listed endangered species growing on the trackbeds can be avoided. Both cases are likely to reduce the amount of herbicides, which often is in the interest of public opinion. Bearing in mind that natural objects like plants are often more heterogeneous within their own class rather than outside it, the results do indeed present a stable classification performance, which is a sound prerequisite in order to later take the next step to include a natural background. Where relevant, species can also be listed under the Endangered Species Act.","railways,
feature extraction,
image classification,
learning (artificial intelligence),
mechanical engineering computing"
Context Free Frequently Asked Questions Detection Using Machine Learning Techniques,"FAQs are the lists of common questions and answers on particular topics. Today one can find them in almost all web sites on the internet and they can be a great tool to give information to the users. Questions in FAQs are usually identified by the site administrators on the basis of the questions that are asked by their users. While such questions can respond to required information about a service, topic, or particular subject, they can not easily be distinguished from non-FAQ questions. This paper describes machine learning based parsing and question classification for FAQs. We demonstrate that questions for FAQs can be distinguished from other types of questions. Identification of specific features is the key to obtaining an accurate FAQ classifier. We propose a simple yet effective feature set including bag of words, lexical, syntactical, and semantic features. To evaluate our proposed methods, we gathered a large data set of FAQs in three different contexts, which were labeled by humans from real data. We showed that the SVM and Naive Bayes reach the accuracy of 80.3%, which is an outstanding result for the early stage research on FAQ classification. Experimental results show that the proposed approach can be a practical tool for question answering systems. To evaluate the accuracy of our classifier we have conducted an evaluation process and built the questionnaire. Therefore, we compared our classifier ranked questions with user rates and almost 81% similarity of the question ratings gives some confidence.","Context,
Support vector machines,
Feature extraction,
Internet,
Semantics,
Niobium,
Knowledge discovery"
Deeper and cheaper machine learning [Top Tech 2017],"Last March, Google's computers roundly beat the world-class Go champion Lee Sedol, marking a milestone in artificial intelligence. The winning computer program, created by researchers at Google DeepMind in London, used an artificial neural network that took advantage of what's known as deep learning, a strategy by which neural networks involving many layers of processing are configured in an automated fashion to solve the problem at hand.","Google,
Machine learning,
Hardware,
Computers,
Tensile stress,
Field programmable gate arrays,
Software"
Repairing Intricate Faults in Code Using Machine Learning and Path Exploration,"Debugging remains costly and tedious, especially for code that performs intricate operations that are conceptually complex to reason about. We present MLR, a novel approach for repairing faults in such operations, specifically in the context of complex data structures. Our focus is on faults in conditional statements. Our insight is that an integrated approach based on machine learning and systematic path exploration can provide effective repairs. MLR mines the data-spectra of the passing and failing executions of conditional branches to prune the search space for repair and generate patches that are likely valid beyond the existing test-suite. We apply MLR to repair faults in small but complex data structure subjects to demonstrate its efficacy. Experimental results show that MLR has the potential to repair this fault class more effectively than state-of-the-art repair tools.","Maintenance engineering,
Support vector machines,
Data structures,
Systematics,
Semisupervised learning,
Debugging,
Space exploration"
Auto-Tagging for Massive Online Selection Tests: Machine Learning to the Rescue,"Difficulty Level of a question is relative to that of other questions in a test and also to the test takers, hence manually assigning Difficulty Level tags may not be accurate. There is a need to infer them from historical data pertaining to the performance of students in a test. e-Yantra Robotics Competition (eYRC) is an annual competition having around 5000 teams (20,000 students) registering in the latest edition of the competition, eYRC-2015. All four team members take a test simultaneously and each individual gets questions which are different but have a similar Difficulty Level. A Question Bank containing 1800 unique questions from 3 subjects - Aptitude, Electronics, and C-Programming - is used to generate question sets each having 30 questions. It is a challenge to ensure that each set contains questions of similar Difficulty Levels tagged manually as Easy, Medium or Hard. In this paper, we discuss a learning algorithm called Weighted Clustering that can automatically tag questions by analyzing the performance of students. We used this algorithm to analyze the performance data in eYRC-2014 for 614 questions from the Question Bank, we found that Manual Tagging accuracy was 44%. We retagged questions with Suggested Tags resulting from our analysis and used them again in eYRC-2015. When we applied the algorithm to the performance data in eYRC-2015, we found that the accuracy of tagging had significantly improved to 67%.","Clustering algorithms,
Tagging,
Manuals,
Partitioning algorithms,
Measurement,
Semantics,
Robots"
Semi-supervised machine learning for textual anomaly detection,"Anomaly detection comprises the identification of observations which do not follow the expected patterns of the assumed data set. We attempt to simplify the problem of textual anomaly detection by constructing a Multinomial Naïve Bayes classifier and enhancing it with an augmented Expectation Maximization (EM) algorithm. By doing so, we utilize large amounts of unlabelled data and show how the EM algorithm could increase the accuracy of the Naïve Bayes classifier. The process is applied to a binary classification environment in order to detect anomalies in text.","Mathematical model,
Data models,
Training data,
Probabilistic logic,
Convergence,
Companies,
Text analysis"
A machine learning approach for identification & diagnosing features of Neurodevelopmental disorders using speech and spoken sentences,"Autism Spectrum Disorder is characterised by the defects in communication and social skills. This is the neuro developmental disorder, that is, the disorder in brain and its functioning that affects the emotion, communication, self-control and person's ability to learn by stopping the growth and development of the central nervous system. A number of studies have been made to find the accurate features to diagnose the ASD. This paper presents the review of the robust and accurate feature used for detection of this wide spectrum disease and also reviews the linguistic and acoustic feature for identifying Autism. It also presents the review of a similar disease called Parkinson's disease, which is mainly due to the loss of cells in the various part of the brain, is also reviewed. The purpose of this study is to expose to view the wide range of mechanisms used in the detection of speech related impairments from three similar researches.","Speech,
Feature extraction,
Harmonic analysis,
Diseases,
Autism,
Jitter,
Standards"
Compressive Privacy: From Information\/Estimation Theory to Machine Learning [Lecture Notes],"Most of our daily activities are now moving online in the big data era, with more than 25 billion devices already connected to the Internet, to possibly over a trillion in a decade. However, big data also bears a connotation of &ldquo;big brother&rdquo; when personal information (such as sales transactions) is being ubiquitously collected, stored, and circulated around the Internet, often without the data owner's knowledge. Consequently, a new paradigm known as online privacy or Internet privacy is becoming a major concern regarding the privacy of personal and sensitive data.","Privacy,
Data privacy,
Entropy,
Covariance matrices,
Cloud computing,
Big data"
Domain based sentiment analysis in regional Language-Kannada using machine learning algorithm,"Sentiment analysis (SA) is one of the important fields of Machine learning Language which involves analysis using natural language processing. The main goal of sentiment analysis is to detect and analyze attitude, opinions or sentiments in the text. Sentiment analysis has reach edits popularity by extracting Knowledge from huge amount data present online. The Process of analysis includes selecting features and opinion which is a challenging task in languages other than English. There are very few research works done for determining sentiments in regional languages. This Paper aims on domain based sentiment analysis in Regional language specific to movie susing machine learning algorithm for classification and provide a comparison between analysis using direct Kannada dataset and machine translated English language.","Feature extraction,
Sentiment analysis,
Decision trees,
Motion pictures,
Speech,
Tagging,
Frequency measurement"
JVM characterization framework for workload generated as per machine learning benckmark and spark framework,"Today there are plenty of frameworks to assist the development of Big-data applications. Computation and Storage are two major activities in these applications. Spark framework has replaced Map-Reduce in Hadoop, which is the preferred analytics engine for Big-data applications. Java Virtual Machine (JVM) is used as execution platform irrespective of which framework is used for development. In the production environment it is essential to monitor the health of application to gain better performance. The parameters like memory usage, CPU utilization and frequency of Garbage Collection etc., will help to decide on the health of application. In this paper a framework is proposed to characterize the JVM behavior to monitor the health of application. Workload generated by running Machine Learning algorithms available in Spark Benchmark Suite.","Sparks,
Monitoring,
Java,
Clustering algorithms,
Machine learning algorithms,
Benchmark testing"
Psychology assisted prediction of academic performance using machine learning,"The psychological state of the student has deep influence on their academic performance is being proved by various studies. The paper demonstrates the impact of student's psychology and their learning and study skills in the predicting their academic performance. The experiment was performed on the real time data collected from final year students. The matriculate and pre-university examination scores, five semester scores along with data on the motivation level, information processing ability and other learning and study skills were taken as input to the model to predict the Cumulative Grade Point Average(CGPA) of the sixth semester. Two machine learning algorithms were used to test the impact of students' psychology on prediction which includes Neural Network for numeric prediction of sixth semester CGPA and Decision Tree for classification of failures in sixth semester. The performance of the models were evaluated using the coefficient of correlation R and the Mean Squared Error. The accuracy of the prediction increases about 4 to 6%. The study reveals that level of motivation in student's life and the way they perceive the information and use the available study materials for the examination, all counts in prediction of their examination performance.","Decision trees,
Correlation,
Neural networks,
Psychology,
Predictive models,
Market research,
Data models"
Multilayer machine learning algorithm to classify diabetic type on knee dataset,"Since last decade, the diabetes risks are increasing in children and adults. Various approaches have been proposed for early detection of the diabetes and prevention on it. Some methods use EMG signals for diabetes classification, due to motion artifacts in the EMG signals during acquisition of signal, these approaches are not able to classify the signal efficiently. To overcome this we propose anew method by considering time domain and frequency domain features of the EMG signals and to perform the classification we use neural network. This method is executed using MATLAB tool and simulation study shows the accuracy of proposed approach is 97.05%.","Diabetes,
Electromyography,
Feature extraction,
Biological neural networks,
Muscles,
Insulin,
Conferences"
Applications of machine learning in induction cooking,"In this paper we present a new line of work combining digital signal processing and machine learning algorithms to identify and classify cooking recipients used in domestic induction heating. First, a recipient `signature' is obtained from processing current and voltage waveforms, which includes impedance harmonics and the power factor. Then, the non-linear fitting capabilities of artificial neural networks and other machine learning algorithms allow processing the pot signature. We show two applications of this technique: recipient size estimation and identification of every cooking recipient of a specific user (for instance, for assigning a specific cooking profile to each vessel). Finally, we implement our procedure onto a low-cost electronic circuit such as those included in commercial induction home appliances. This new approach is of interest for developing new applications in the context of automatic cooking.","Harmonic analysis,
Reactive power,
Impedance,
Neurons,
Power system harmonics,
Estimation,
Spectral analysis"
A machine learning enabled network planning tool,"In the coming years, planning future mobile networks will be infinitely more complex than nowadays. Future networks are expected to present multiple Network Management (NM) challenges to operators, such as managing network complexity in terms of densification of scenarios, heterogeneous nodes, applications, Radio Access Technologies (RAT), among others. In this context, the exploitation of past information gathered by the network is highly relevant when planning future deployments. In this paper we present a network planning tool based on Machine Learning (ML). In particular, we propose an approach which allows to predict Quality of Service (QoS) offered to end-users, based on data collected by the Minimization of Drive Tests (MDT) function. As a QoS indicator, we focus on Physical Resource Block (PRB) per Megabit (Mb) in an arbitrary point of the network. Minimizing this metric allows serving users with the same QoS by consuming less resources, and therefore, being more cost-effective. The proposed network planning tool considers a Genetic Algorithm (GA), which tries to reach the operator targets. The network parameters we desire to optimise are set as the input to the algorithm. Then, we predict the QoS of the network by means of ML techniques. By integrating these techniques in a network planning tool, operators would be able to find the most appropriate deployment layout, by minimizing the resources (i.e., the cost) they need to deploy to offer a given QoS in a newly planned deployment.","Quality of service,
Planning,
Training,
Mobile computing,
Mobile communication,
Face,
Optimization"
Missing data handling using machine learning for human activity recognition on mobile device,"Human activity recognition is important technology in mobile computing era because it can be applied to many real-life, human-centric problems such as eldercare and healthcare. Successful research has so far focused on recognizing simple human activities. Currently, the smartphone is equipped with various sensors such as an accelerometer, gyroscope, digital compass, microphone, GPS and camera. The sensors have been used in various areas such as human gesture and activity recognition which is opening a new area of research and significantly impact in daily life. Activity recognition between the personal computer and smartphone is different. A mobile device has limited computational and memory capacity which has a chance that some data are missing when limitation of the mobile device is happening. In this research, some algorithms are tested to perform their ability to handling missing data, they are Bayesian Network, Multilayer Perceptron (MLP), C4.5 and k-Nearest Neighbour (k-NN). Missing data are implemented with increment scaling from 5%-40%. Optimal result based on accuracy mean is obtained by kNN with 89,4752%. Based on class, Bayesian Network obtained mean 992 recognized on Sitting class and kNN obtained mean 1010 recognized on Walking class. Multilayer Perceptron is obtained endurance point with decreasing about 9.9109% from normal experiment without missing data.","Classification algorithms,
Mobile handsets,
Accelerometers,
Gyroscopes,
Multilayer perceptrons"
WSNs Self-Calibration Approach for Smart City Applications Leveraging Incremental Machine Learning Techniques,"The diffusion of the Internet of Things paradigm, in the last few years, has led to the need of deploying and managing large-scale Wireless Sensor Networks (WSNs), composed by a multitude of geographically distributed sensors, like the ones needed for Smart City applications. The traditional way to manage WSNs is not suitable for this type of applications, because manually managing and monitoring every single sensor would be too expensive, time consuming and error prone. Moreover, unattended sensors may suffer of several issues that progressively make their measures unreliable and consequently useless. For this reason, several automatically techniques have been studied and implemented for the detection and correction of measurements from sensors which are affected by errors caused by aging and/or drift. These methods are grouped under the name of self-calibration techniques. This paper presents a distributed system, which combines an incremental machine learning technique with a non-linear Kalman Filter estimator, which allows to automatically re-calibrate sensors leveraging the correlation with measurements made by neighbor sensors. After the description of the used model and the system implementation details, the paper describes also the proof-of-concept prototype that has been built for testing the proposed solution.","Calibration,
Intelligent sensors,
Sensor phenomena and characterization,
Wireless sensor networks,
Estimation,
Kalman filters"
An ensemble of learning machines for quantitative analysis of bronze alloys,"We deal with the determination of the composition of bronze alloys measured through Laser-Induced Breakdown Spectroscopy (LIBS) analysis. The relation between LIBS spectra and bronze alloy composition, represented by means of the concentrations of constituting elements, is modeled by adopting an ensemble of learning machines, fed with different inputs. Then, the combiner computes the final response. The results obtained on the test set show that the ensemble model manages to determine the composition of alloy samples with mean squared error of about 6.53 10-2.","Artificial neural networks,
Chemical elements,
Genetic algorithms,
Training,
Lead,
Surface emitting lasers"
Evolutionary Online Machine Learning from Imbalanced Data,"The discipline of machine learning has raised plenty of well-understood and partially well-studied challenges. Research has been concerned with issues such as incompletely labeled or missing data, dataset imbalances regarding the distributions of the target values, as well as the non-deterministic and unpredictable behavior of non-stationary environments. In this article, one particular challenge will be reviewed and motivated - the challenge of online learning from imbalanced data common in real world environments. It is hypothesized how interpolation between already gained knowledge and a proactive exploration of the input space may lead to beneficial effects when learning from data streams exhibiting imbalances. After the definition of this doctoral study's objectives, a reference evolutionary online machine learning technique is briefly introduced. On this basis, all aspects that will be thoroughly investigated are sketched and finally integrated into a research schedule.","Machine learning algorithms,
Data models,
Training data,
Approximation algorithms,
Training,
Medical services,
Adaptive systems"
A machine learning-based tourist path prediction,"Intelligent recommendations about where to go will be very helpful for personal and commercial travel recommendations. We deal with this kind of recommendations as a prediction problem based on the tourist's historical visiting sequences and supervised machine learning algorithms, namely Random Forests and LambdaMART. We propose a feature set with 56 dimensions from the tourist's historical traveling data. By utilizing the entropy from information theory, all features are ranked. Evaluation results show that when selecting the most important subset of 20 features as our final input of Random Forests, we can get a 4% higher accuracy and a 70% reduction of computation complexity with regard to using the full set of features. A comparison of five different machine learning algorithms, namely Random Forests, LambdaMART, Ranking SVM, ListNet and RankBoost, has been taken on this feature set, results demonstrate that the Random Forests outperforms the other algorithms.","Prediction algorithms,
Urban areas,
Support vector machines,
Global Positioning System,
Flickr,
Predictive models,
Vegetation"
Passengers' choices on airport drop-off service: A decision forecast based on social learning and machine learning,"Airport drop-off service provided by airlines is a chauffeur-driven service (i.e. Uber and DiDi) as an emerging travel choice for travelers. More and more passenger enjoy the drop-off service. In practice, we find an interesting question: if a passenger has ever choice the drop-off service, whether they are willing to recommend this service to other traveler? Although the acknowledgment that social learning is related to travel decision is promoted, quantitative analysis about how social learning shape and impact the decision of passengers is still limited. We study and estimate a diffusion probability between different passengers by proposing a CCM (Co-travel Link Cascade Model) based on a modified EM iterative algorithm. Then, we segment passengers into three types (Influenced, Unchecked and Immune). The three types of passengers are predicated by approaches of IC-like model, Random Forest model and probabilistic model, respectively. In addition, we also design a parallel implementation of our proposed algorithm in the Apache Spark distributed data processing environment. Experimental results on a real aviation data set demonstrate that CCM can efficiently infer the decision of travelers.","Decision support systems,
Airports,
Atmospheric modeling,
Machine learning algorithms,
Algorithm design and analysis,
Sparks"
Machine Learning in Software Defined Networks: Data collection and traffic classification,Software Defined Networks (SDNs) provides a separation between the control plane and the forwarding plane of networks. The software implementation of the control plane and the built in data collection mechanisms of the OpenFlow protocol promise to be excellent tools to implement Machine Learning (ML) network control applications. A first step in that direction is to understand the type of data that can be collected in SDNs and how information can be learned from that data. In this work we describe a simple architecture deployed in an enterprise network that gathers traffic data using the OpenFlow protocol. We present the data-sets that can be obtained and show how several ML techniques can be applied to it for traffic classification. The results indicate that high accuracy classification can be obtained with the data-sets using supervised learning.,"Switches,
Ports (Computers),
Protocols,
Software,
Conferences,
Electronic mail"
Assessment of defect prediction models using machine learning techniques for object-oriented systems,"Software development is an essential field today. The advancement in software systems leads to risk of them being exposed to defects. It is important to predict the defects well in advance in order to help the researchers and developers to build cost effective and reliable software. Defect prediction models extract information about the software from its past releases and predict the occurrence of defects in future releases. A number of Machine Learning (ML) algorithms proposed and used in the literature to efficiently develop defect prediction models. What is required is the comparison of these ML techniques to quantify the advantage in performance of using a particular technique over another. This study scrutinizes and compares the performances of 17 ML techniques on the selected datasets to find the ML technique which gives the best performance for determining defect prone classes in an Object-Oriented(OO) software. Also, the superiority of the best ML technique is statistically evaluated. The result of this study demonstrates the predictive capability of ML techniques and advocates the use of Bagging as the best ML technique for defect prediction.","Measurement,
Software,
Predictive models,
Object oriented modeling,
Decision trees,
Reliability,
Algorithm design and analysis"
Real time driver drowsiness detection using a logistic-regression-based machine learning algorithm,"The number of car accidents due to driver drowsiness is very steep. An automated non-contact system that can detect driver's drowsiness early could be lifesaving. Motivated by this dire need, we propose a novel method that can detect driver's drowsiness at an early stage by computing heart rate variation using advanced logistic regression based machine learning algorithm. Our developed technique has been tested with human subjects and it can detect drowsiness in a minimum amount of time, with an accuracy above 90%.","Electrocardiography,
Sensors,
Frequency-domain analysis,
Hardware,
Vehicles,
Time series analysis,
Adaptive filters"
Anomaly Detection in Electrical Substation Circuits via Unsupervised Machine Learning,"Cyber-physical systems (CPS), such as smart grids, include cyber assets for monitoring, control, and communication in order to maintain safe and efficient operation of a physical process. We propose that CPS intrusion detection systems (CPS IDS) should seek not just to detect attacks in the host audit logs and network traffic (cyber plane), but should consider how attacks are reflected in measurements from diverse devices at multiple locations (physical plane). In electric grids, voltage and current laws induce physical constraints that can be leveraged in distributed agreement algorithms to detect anomalous conditions. This can be done by explicitly coding the physical constraints into a hybrid CPS IDS, making the detector specific to a particular CPS. We present an alternative approach, along with preliminary results, using machine learning to characterize normal, fault, and attack states in a smart distribution substation CPS, using this as a component of a CPS IDS.","Circuit faults,
Substations,
Voltage measurement,
Current measurement,
Training,
Pattern matching"
Distributed Machine Learning with Self-Organizing Mobile Agents for Earthquake Monitoring,"Ubiquitous computing and The Internet-of-Things (IoT) raises rapidly in today's life and is becoming part of self-organizing systems (SoS). A unified and scalable information processing and communication methodology using mobile agents is presented to merge the IoT with Mobile and Cloud environments seamless. A portable and scalable Agent Processing Platform (APP) is an enabling technology that is central for the deployment of Multi-agent Systems (MAS) in strong heterogeneous networks including the Internet. A large-scale distributed heterogeneous seismic sensor and geodetic network used for earthquake analysis is one example, which can be extended by ubiquitous sensing devices like smart phones. To simplify the development and deployment of MAS in the Internet domain agents are directly implemented in JavaScript (JS). The proposed JS Agent Machine (JAM) is an enabling technology. It is capable to execute AgentJS agents in a sandbox environment with full run-time protection, low-resource requirements, and Machine Learning as a service. A simulation of a seismic network and real earthquake data demonstrates the deployment of the JAM platform. Different (mobile) agents perform sensor sensing, aggregation, local learning and prediction, global voting, and the application.",Smart phones
A Study on Machine Learning for Imbalanced Datasets with Answer Validation of Question Answering,"Question Answering is a system that can process and answer a given question. In recent years, an enormous number of studies have been made on question answering, little is known about the effects of imbalanced datasets with answer validation of question answer system. The objective of this paper is to provide a better understanding of the effects of imbalanced datasets model for answer validation in a real world university entrance exam question answering system. In this paper, we proposed a question answer system and provided a comprehensive analysis of imbalanced datasets and balanced datasets model with Answer Validation of Question Answering system using NTCIR-12 QA-Lab2 Japanese university entrance exams English translation development and test datasets. As a result, our system achieved 90% accuracy with imbalanced datasets machine learning model for the NTCIR-12 QA-Lab2 development datasets.","Knowledge discovery,
History,
Systems architecture,
Europe,
Text analysis,
Sensitivity,
XML"
Adaptive nonsymmetrical demodulation based on machine learning to mitigate time-varying impairments,"Future terabit optical communication systems will rely on higher order Quadrature Amplitude Modulation (QAM) formats to improve the spectral efficiency (SE). However, these formats exhibit a higher sensitivity to linear as well as non-linear impairments throughout the optical link [1, 2]. These impairments may be time-varying in nature arising from mechanical disturbance in optical fibers, temperature variations in the components and small variations of the bias control of the IQ optical modulators for example. Unlike optical noise, these degradations may have a probability density function (PDF) that is non-Gaussian and therefore may shift the centroids (mean value) of the received symbols from their ideal constellation positions by different extents, Fig 1a. Thus, employing non-symmetrical decision boundaries by means of machine learning techniques may improve the overall system performance with high computational efficiency in contradistinction to complex linear equalization or computationally heavy non-linear mitigation techniques [2, 3]. Furthermore, an area that has not been well explored is the use of digital techniques at the receiver for tracking and compensating the effects of fast time-varying link parameters. Here, we demonstrate an adaptive machine learning-based nonsymmetrical decision technique relying on clustering to mitigate time-varying impairments such as IQ imbalance, bias drift, and phase noise. We experimentally verified its effectiveness in a 16QAM Nyquist system at 16 Gbaud for back-to-back (B2B) and 250 km links.","Optical noise,
Demodulation,
Signal to noise ratio,
Optical fiber communication,
Receivers,
Clustering algorithms"
A Binary Time Series Model of LTE Scheduling for Machine Learning Prediction,"In today's Third-Generation Partnership Project (3GPP) Long-Term Evolution Advanced (LTE-A) cellular radio networks, battery lifetime is critical for mobile devices. During time intervals of no user data transmit or receive activity, energy for receiving and processing irrelevant control information in a mobile device could be saved. Therefore, we propose a binary time series model at 1 ms transmission time interval (TTI) granularity to predict the control channel information. To assess the predictability of the proposed time series, we apply three well-known machine learning (ML) algorithms combined with a non-intrusive cost-sensitive classification (CSC) scheme. Predictions of the proposed time series model successfully reach false negative rates (FNRs) below 2%.","Time series analysis,
Uplink,
Long Term Evolution,
Downlink,
Mobile handsets,
Prediction algorithms,
Modems"
A Machine Learning Approach to Improve the Accuracy of GPS-Based Map-Matching Algorithms (Invited Paper),"Advanced map-matching algorithms use location and heading of GPS points along with geometrical and topological features of digital road networks to find the road segment on which the vehicle is moving. However, GPS errors sometimes impede map-matching algorithms in finding the correct segment, especially in dense and complicated parts of the network, such as near intersections with acute angles or on close parallel roads. In this paper an artificial neural network (ANN) approach is explored to improve the segment identification accuracy of map-matching algorithms. The proposed ANN is continuously trained by using the horizontal shift imposed on GPS points and once it is trained, it will be used to correct raw GPS points before inputting them into the map-matching algorithm. Integrating the proposed ANN enabled an existing map-matching algorithm to find the correct segments for some of the GPS points where the original map-matching algorithm had failed to do so.","Global Positioning System,
Training,
Roads,
Artificial neural networks,
Receivers,
Vehicles,
Satellites"
New Ensemble Machine Learning Method for Classification and Prediction on Gene Expression Data,"A reliable and precise classification of tumours is essential for successful treatment of cancer. Recent researches have confirmed the utility of ensemble machine learning algorithms for gene expression data analysis. In this paper, a new ensemble machine learning algorithm is proposed for classification and prediction on gene expression data. The algorithm is tested and compared with three popular adopted ensembles, i.e. bagging, boosting and arcing. The results show that the proposed algorithm greatly outperforms existing methods, achieving high accuracy over 12 gene expression datasets",
Machine learning algorithms in context of intrusion detection,"Design of efficient, accurate, and low complexity intrusion detection system is a challenging task. Intrusion detection method is a core of intrusion detection system and it can be either signature based or anomaly based. Although, signature based has high detection rate but it cannot detect novel attacks. Asymmetrically, anomaly based detection method can detect novel attacks but it has high false positive rate. Many machine learning techniques have been developed to cope with this problem. These machine learning algorithms develop a detection model in a training phase. This paper compares different supervised algorithms for the anomaly-based detection technique. The algorithms have been applied on the KDD99 dataset, which is the benchmark dataset used for anomaly-based detection technique. The result shows that not a single algorithm has a high detection rate for each class of KDD99 dataset. The performance measures used in this comparison are true positive rate, false positive rate, and precision.","Machine learning algorithms,
Intrusion detection,
Classification algorithms,
Support vector machines,
Computers,
Decision trees,
Testing"
Stock market prediction using machine learning techniques,"The main objective of this research is to predict the market performance of Karachi Stock Exchange (KSE) on day closing using different machine learning techniques. The prediction model uses different attributes as an input and predicts market as Positive & Negative. The attributes used in the model includes Oil rates, Gold & Silver rates, Interest rate, Foreign Exchange (FEX) rate, NEWS and social media feed. The old statistical techniques including Simple Moving Average (SMA) and Autoregressive Integrated Moving Average (ARIMA) are also used as input. The machine learning techniques including Single Layer Perceptron (SLP), Multi-Layer Perceptron (MLP), Radial Basis Function (RBF) and Support Vector Machine (SVM) are compared. All these attributes are studied separately also. The algorithm MLP performed best as compared to other techniques. The oil rate attribute was found to be most relevant to market performance. The results suggest that performance of KSE-100 index can be predicted with machine learning techniques.","Stock markets,
Neurons,
Support vector machines,
Data models,
Computational modeling,
Computer science,
Predictive models"
Integration of machine learning approach in item bank test system,"Item test bank system plays very important role in auto generating test or exam paper in assessments in schools and universities. A quite number of researchers have proposed some algorithms in generating test paper based on some well-defined attributes such as time, question type, knowledge point, difficulty level and others. It has always been the aim of these researchers to generate high quality test paper with appropriate level of difficulty in test questions. As a result of this, Bloom taxonomy has been adopted to ensure difficulty level of test questions is appropriate. However, there is no evidence that current test items or questions in the item test bank system are classified in accordance to BT using machine learning approach. Manual classifying is tedious and laborious work and inconsistency in classifying items can take place due to different judgement from instructors. A better approach is to use machine learning namely question classifier such as Support Vector Machine to automate the classification of the test items. Despite some research work has been done on using classifiers to classify questions, there is no evidence that this type of work has been integrated into item bank test system. In view of this, this study proposes a change in existing framework of item test bank system by integrating the facility to automate classifying items in accordance to Bloom taxonomy. With all this in place, the automation of classifying questions or test items in accordance to BT with a reasonable accuracy can be achieved.","Generators,
Feature extraction,
Taxonomy,
Support vector machines,
Context,
Electronic learning,
Load modeling"
Tackling the Cloud Adoption Dilemma - A User Centric Concept to Control Cloud Migration Processes by Using Machine Learning Technologies,"Research studies have shown that especially enterprises in European countries are afraid of losing outsourced data or unauthorized access. Despite various existing cloud security mechanisms companies are currently hesitating to adopt cloud resources. This phenomenon is also known as cloud adoption dilemma. We think that data classification is a promising technique that should be considered in the context of cloud security, supporting cloud migration processes. By using classification techniques enterprises are able to control which documents are suited for Cloud Computing and which cloud service providers are sufficient for protecting sensitive documents. In this work we present an efficient concept that involves enterprises' employees and authorities, making it possible to apply powerful security policies in a simple way. We make use of a well-established machine learning algorithm in our developed tool, identifying security levels for different types of documents. Thus, cloud migration processes can become more transparent and enterprises obtain the ability to discuss more openly about adopting innovative cloud services.","Cloud computing,
Companies,
Sensitivity,
Encryption,
Context,
Machine learning algorithms"
A Machine Learning Based Approach for Detecting DRDoS Attacks and Its Performance Evaluation,"DRDoS (Distributed Reflection Denial of Service) attack is a kind of DoS (Denial of Service) attack, in which third-party servers are tricked into sending large amounts of data to the victims. That is, attackers use source address IP spoofing to hide their identity and cause third-parties to send data to the victims as identified by the source address field of the IP packet. This is called reflection because the servers of benign services are tricked into ""reflecting"" attack traffic to the victims. The most typical existing detection methods of such attacks are designed based on known attacks by protocol and are difficult to detect the unknown ones. According to our investigations, one protocol-independent detection method has been existing, which is based on the assumption that a strong linear relationship exists among the abnormal flows from the reflector to the victim. Moreover, the method is assumed that the all packets from reflectors are attack packets when attacked, which is clearly not reasonable. In this study, we found five features are effective for detecting DRDoS attacks, and we proposed a method to detect DRDoS attacks using these features and machine learning algorithms. Its detection performance is experimentally examined and the experimental result indicates that our proposal is of clearly better detection performance.","Servers,
Protocols,
Computer crime,
IP networks,
Bandwidth,
Reflection,
Feature extraction"
Experimenting Machine Learning Techniques to Predict Vulnerabilities,"Software metrics can be used as a indicator of the presence of software vulnerabilities. These metrics have been used with machine learning to predict source code prone to contain vulnerabilities. Although it is not possible to find the exact location of the flaws, the models can show which components require more attention during inspections and testing. Each new technique uses his own evaluation dataset, which many times has limited size and representativeness. In this experience report, we use a large and representative dataset to evaluate several state of the art vulnerability prediction techniques. This dataset was built with information of 2186 vulnerabilities from five widely used open source projects. Results show that the dataset can be used to distinguish which are the best techniques. It is also shown that some of the techniques can predict nearly all of the vulnerabilities present in the dataset, although with very low precisions. Finally, accuracy, precision and recall are not the most effective to characterize the effectiveness of this tools.","Software metrics,
Predictive models,
Kernel,
Decision trees,
Security"
Smartphone wireless gyroscope platform for machine learning classification of hemiplegic patellar tendon reflex pair disparity through a multilayer perceptron neural network,"The patellar tendon enables fundamental insight regarding neurological health status. Clinically observed dysfunction may warrant escalation to more advanced and expensive medical diagnostics. Conventionally clinicians apply an ordinal scale to quantify reflex response characteristics. However the reliability of ordinal scales is a subject of debate, and even highly skilled clinicians have disputed the observation of an asymmetric reflex pair. An alternative is the use of the wireless quantified reflex system, which features an impact pendulum attached to a reflex hammer for providing precisely targeted levels of potential energy with a smartphone (iPhone) equipped with software to function as a wireless gyroscope platform that can email a trial sample as an email attachment by wireless connectivity to the Internet. With notable attributes of the gyroscope signal recordings of the reflex response of a hemiplegic patellar tendon reflex pair observed a feature set is developed for machine learning classification. Using the multilayer perceptron neural network considerable classification accuracy is attained. The research implications reveal the potential of integrating machine learning with a wireless reflex quantification system that applies a smartphone (iPhone) as a wireless gyroscope platform.","Tendons,
Wireless communication,
Gyroscopes,
Accelerometers,
Wireless sensor networks,
Biological neural networks,
Reliability"
Machine Learning Approach for Predicting Wall Shear Distribution for Abdominal Aortic Aneurysm and Carotid Bifurcation Models,"Computer simulations based on the finite element method (FEM) represent powerful tools for modeling blood flow through arteries. However, due to its computational complexity, this approach may be inappropriate when results are needed quickly. In order to reduce computational time, in this paper we proposed an alternative machine learning based approach for calculation of wall shear stress (WSS) distribution, which may play an important role in mechanisms related to initiation and development of atherosclerosis. In order to capture relationships between geometric parameters, blood density, dynamic viscosity and velocity and WSS distribution of geometrically parameterized abdominal aortic aneurysm (AAA) and carotid bifurcation models, we proposed multivariate linear regression (MLR), multilayer perceptron neural network (MLP) and gaussian conditional random fields (GCRF). Results obtained in this paper show that machine learning approaches can successfully predict WSS distribution at different cardiac cycle time points. Even though all proposed methods showed high potential for WSS prediction, GCRF achieved the highest coefficient of determination (0.930 to 0.948 for AAA model and 0.946 to 0.954 for carotid bifurcation model) demonstrating benefits of accounting for spatial correlation. The proposed approach can be used as an alternative method for real time calculation of wall shear stress distribution.","Bifurcation,
Predictive models,
Aneurysm,
Computational modeling,
Blood,
Stress,
Biological system modeling"
A Machine Learning-Based Protocol for Efficient Routing in Opportunistic Networks,"This paper proposes a novel routing protocol for OppNets called MLProph, which uses machine learning (ML) algorithms, namely decision tree and neural networks, to determine the probability of successful deliveries. The ML model is trained by using various factors such as the predictability value inherited from the PROPHET routing scheme, node popularity, node’s power consumption, speed, and location. Simulation results show that MLProph outperforms PROPHET+, a probabilistic-based routing protocol for OppNets, in terms of number of successful deliveries, dropped messages, overhead, and hop count, at the cost of small increases in buffer time and buffer occupancy values.","Routing protocols,
Training,
Routing,
Artificial neural networks,
Training data"
A comparison of GPU execution time prediction using machine learning and analytical modeling,"Today, most high-performance computing (HPC) platforms have heterogeneous hardware resources (CPUs, GPUs, storage, etc.) A Graphics Processing Unit (GPU) is a parallel computing coprocessor specialized in accelerating vector operations. The prediction of application execution times over these devices is a great challenge and is essential for efficient job scheduling. There are different approaches to do this, such as analytical modeling and machine learning techniques. Analytic predictive models are useful, but require manual inclusion of interactions between architecture and software, and may not capture the complex interactions in GPU architectures. Machine learning techniques can learn to capture these interactions without manual intervention, but may require large training sets. In this paper, we compare three different machine learning approaches: linear regression, support vector machines and random forests with a BSP-based analytical model, to predict the execution time of GPU applications. As input to the machine learning algorithms, we use profiling information from 9 different applications executed over 9 different GPUs. We show that machine learning approaches provide reasonable predictions for different cases. Although the predictions were inferior to the analytical model, they required no detailed knowledge of application code, hardware characteristics or explicit modeling. Consequently, whenever a database with profile information is available or can be generated, machine learning techniques can be useful for deploying automated on-line performance prediction for scheduling applications on heterogeneous architectures containing GPUs.","Graphics processing units,
Computational modeling,
Predictive models,
Computer architecture,
Analytical models,
Mathematical model,
Message systems"
Revolutionizing machine learning algorithms using GPUs,"Machine learning is a very powerful domain in the field of computer science. However, a lot of time is consumed in execution and learning from the humongous datasets. In the last decade General Purpose Graphics Processing Units (GPGPU) have revolutionized the computing industry by their capability to process data in parallel and hence speedup the entire process. This paper aims to showcase a generic model which can be used as a reference to convert implementations of serial machine learning algorithms to parallel implementations. Because of varying types of machine learning algorithms, the model was developed with a broad mindset to support most of them. After development, for the purpose of testing the correctness of the model, a few machine learning algorithms of different kinds are experimented with. They were processed step-by-step in accordance with the model and their original serial implementations in C and the resultant parallel implementations in CUDA were compared w.r.t. the computation time. Also, the serial implementations and parallel implementations were mapped to check if they gave the exact same results i.e. whether perfectly correct parallelism was obtained. The results obtained showed that the model when used for parallelization, gave absolutely correct parallel implementations, without any losses due to parallelization. The model gave highly efficient outcomes for non-neural network based supervised and unsupervised learning algorithms with a speedup of 146× achieved in K-NN algorithm and 75× achieved in K-Means algorithm. The model also gave a correct parallel implementation for Backpropagation algorithm, however here the parallel implementation could not execute faster than the serial implementation.","Computational modeling,
Acceleration,
Robots,
Irrigation,
Hardware,
Optimization,
Reliability"
Prediction of frost episodes based in agrometeorological information and machine learning techniques,"Frosts are one of the main risks faced by farmers during the winter and spring seasons. These events can cause significant damage to cultivations and crops. In Chile, these frost generates significant losses in the agricultural production sector, causing crop losses of an entire year and compromising the income of the following year, especially fruit and wine growers. In this work we developed a prediction model based on historical agrometeorological information able to predict efficiently and up to 12 hours earlier than the occurrence of a frost event. With a special focus on the central Chilean zone. Various algorithms and machine learning methods were evaluated, we found that the method “Random Forest” exhibits the best results overall. The results obtained in the frost prediction reach over (90%) of efficiency in most of the evaluated scenarios.","Predictive models,
Electronic mail,
Agriculture,
Springs,
Machine learning algorithms,
Prediction algorithms"
Distributed machine learning based smart-grid energy management with occupant cognition,"It is challenging to process real-time data analysis and prediction for a smart grid in a building with consideration of both occupant profile and energy profile. This paper proposed a distributed and networked machine learning platform on smart gateways based smart grid. It can analyze occupants motion, provide short-term energy forecasting and allocate renewable energy resource. Firstly, occupant profile is captured by real-time indoor positioning system with Wi-Fi data analysis; and the energy profile is extracted by real-time meter system with electricity load data analysis. Then, the 24-hour occupant profile and energy profile are fused with prediction using an online distributed machine learning with real-time data update. Based on the forecasted occupant motion profile and energy consumption profile, solar energy source is allocated on the additional electricity power-grid in order to reduce peak demand on the main electricity power-grid. The whole management flow can be operated on the distributed smart gateway network with limited computation resource but with a supported general machine-learning engine. Experiment results on real-life datasets have shown that the accuracy of the proposed energy prediction can be 14.83% improvement comparing to SVM method. Moreover, the peak load from main electricity power-grid is reduced by 15.20% with 51.94% energy cost saving.","Solar energy,
Smart grids,
Data analysis,
Logic gates,
Real-time systems,
Energy management,
Data mining"
"Vritthi - a theoretical framework for IT recruitment based on machine learning techniques applied over Twitter, LinkedIn, SPOJ and GitHub profiles","In this model, we propose an innovative recruitment system using social networking websites like Twitter and LinkedIn along with code repository hosting website GitHub and competitive coding platforms like SPOJ. It is aimed to develop advanced search engines to automatically sort the job-seekers based on job offer requirements using various data mining and machine learning techniques. Vritthi allows job-seekers to quantify their job preparedness and offer a list of specific areas for them to focus on. We propose the formulation of VPQF (Vritthi Professional Quotient) that involves the use of K-means algorithm to classify users into appropriate clusters and provide them with appropriate suggestions for improvement. Using classic data mining techniques like filtration, classification, clustering, profiling as well as string matching & user profiling, this tool will enable recruiters to effectively select candidates who fit their organization in a hassle-free automated manner.","Twitter,
Computer languages,
LinkedIn,
Data mining,
Recruitment,
Programming profession"
Mobile ascertainment of smoking status through breath: A machine learning approach,"The ability to remotely and automatically detect whether someone has been smoking can develop systems that promote smoking cessation via behavioral management or gamification. This article advances automated methods for ascertaining smoking status via a mobile system comprised of an environmental carbon monoxide (CO) gas sensor paired with a smartphone. We apply several machine learning strategies to the CO time courses derived from our system to predict recent smoking status in a group of smoking (n=11) and non-smoking (n=9) adult women. The study investigates (i) a range of feature types conventionally used in smoke analysis studies, (ii) the contribution of each feature type in overall prediction accuracies and (iii) the possibility of improving the overall predictions by identifying segments of time during which the most informative data is recorded. The analysis is composed of two stages of (i) conventional classification via Logistic Regression (LR) and Support Vector Machine (SVM) and (ii) feature-ensemble and variations of boosting, bagging, and mixture of expertise methods to investigate the effects of feature mixture, learner mixture, and extra training on overall performances of the predictions. The results indicated that features extracted from the middle segments (end exhalation period) of the recordings led to the highest classification success. The results indicated that having higher number of training samples, and stronger mixed learners are influential to the performance across segments while the feature mixture did not improve the performances across segments.","Training,
Support vector machines,
Bagging,
Logistics,
Boosting,
Current measurement,
Mobile communication"
Prediction of weather-induced airline delays based on machine learning algorithms,"The primary goal of the model proposed in this paper is to predict airline delays caused by inclement weather conditions using data mining and supervised machine learning algorithms. US domestic flight data and the weather data from 2005 to 2015 were extracted and used to train the model. To overcome the effects of imbalanced training data, sampling techniques are applied. Decision trees, random forest, the AdaBoost and the k-Nearest-Neighbors were implemented to build models which can predict delays of individual flights. Then, each of the algorithms' prediction accuracy and the receiver operating characteristic (ROC) curve were compared. In the prediction step, flight schedule and weather forecast were gathered and fed into the model. Using those data, the trained model performed a binary classification to predicted whether a scheduled flight will be delayed or on-time.","Meteorology,
Delays,
Atmospheric modeling,
Data models,
Predictive models,
Training,
Schedules"
AntiWare: An automated Android malware detection tool based on machine learning approach and official market metadata,"The prevalence of mobile devices has increased rapidly in recent years. People store valuable data like personal and financial information on those devices. In addition, applying “bring your own device (BYOD)” policy in companies has become popular. Hence, mobile devices are also source of valuable and confidential company information. Accordingly, there is a growing need for malware detection methods and tools to protection mobile devices against attacks targeting them. In this study, an automated feature-based static analysis method is applied to detect malicious mobile applications on Android devices. By utilizing the metadata of applications on the official market and an online free malware scanner, the feasibility of a mobile malware detection model using free public sources and having quite acceptable accuracy rates is shown. As opposed to previous studies considering only the requested permissions as feature set, additional market metadata including but not limited to application category, download number, developer name, and average rating are included in the analysis as the feature set for training supervised classification algorithms. Based on an experimental evaluation of the majority voting of antivirus (AV) engines on the free online AV community, applications in the training set are labeled as malicious or benign. Naïve Bayes classification algorithm is chosen as supervised learning algorithm for the detection task. In addition, as filter-based algorithms, Chi-Square, Information Gain and ReliefF feature selection methods are used for overcoming potential overfitting problems. Finally, a quick prototype for showing the feasibility of the detection model is demonstrated with sample case applications.","Malware,
Androids,
Humanoid robots,
Classification algorithms,
Feature extraction,
Mobile communication,
Metadata"
Fingerprint-based technique for indoor positioning system via machine learning and convex optimization,"Recently, the indoor positioning system (IPS) based on received signal strength (RSS) has received great attention in both industry and academic fields due to the ubiquity of wireless local area networks (WLAN). In general, Wi-Fi signal is suffering from two major limitations: first, the signal suffers from variation overtime. Secondly, the signal distribution recording on some devices can be more complex (multivariate distribution). In this paper, in order to take into account the full distribution of the RSS, we propose multivariate Kullback-Leibler divergence and Jensen-Shannon divergence (JSD) derived from Kullback-Leibler divergence (KLD) to measure the probability distribution among the fingerprint database. The proposed algorithm results showed high accuracy with localization error accuracy less than 1 m.","Fingerprint recognition,
IEEE 802.11 Standard,
Databases,
Kernel,
IP networks,
Wireless LAN,
Antenna arrays"
Automated solar activity prediction: A hybrid computer platform using machine learning and solar imaging for automated prediction of solar flares,"The importance of real-time processing of solar data especially for space weather applications is increasing continuously. In this paper, we present an automated hybrid computer platform for the short-term prediction of significant solar flares using SOHO/Michelson Doppler Imager images. This platform is called the Automated Solar Activity Prediction tool (ASAP). This system integrates image processing and machine learning to deliver these predictions. A machine learning-based system is designed to analyze years of sunspot and flare data to create associations that can be represented using computer-based learning rules. An imaging-based real-time system that provides automated detection, grouping, and then classification of recent sunspots based on the McIntosh classification is also created and integrated within this system. The properties of the sunspot regions are extracted automatically by the imaging system and processed using the machine learning rules to generate the real-time predictions. Several performance measurement criteria are used and the results are provided in this paper. Also, quadratic score is used to compare the prediction results of ASAP with NOAA Space Weather Prediction Center (SWPC) between 1999 and 2002, and it is shown that ASAP generates more accurate predictions compared to SWPC.","Magnetic resonance imaging,
Magnetic flux,
Meteorology,
Feature extraction,
Computers,
Magnetic separation,
Real-time systems"
Smart-MLlib: A High-Performance Machine-Learning Library,"As the popularity of big data analytics has continued to grow, so has the need for accessible and scalable machine-learning implementations. In recent years, Apache Spark's machine-learning library, MLlib, has been used to fulfill this need. Though Spark outperforms Hadoop, it is not clear if it is the best performing underlying middleware to support machine learning implementations. Building on a C++ and MPI based middleware system Smart, we present a machine-learning library prototype (Smart-MLlib). Like MLlib, Smart MLlib allows machine learning implementations to be invoked from a Scala program, and with a very similar API. To test our library's performance, we built four machine-learning applications that are also provided in Spark's MLlib: k-means clustering, linear regression, Gaussian mixture models, and support vector machines. On average, we outperformed Spark's MLlib by over 800%. Our library also scaled better than Spark's MLlib for every application tested. Thus, the new machine-learning library enables higher performance than Spark's MLlib without sacrificing the easy-to-use API.",
Radio frequency interference detection using machine learning,"Radio frequency interference (RFI) has plagued radio astronomy which potentially might be as bad or worse by the time the Square Kilometre Array (SKA) comes up. RFI can be either internal (generated by instruments) or external that originates from intentional or unintentional radio emission generated by man. With the huge amount of data that will be available with up coming radio telescopes, a machine learning technique will be required to detect RFI. In this paper we present the result of applying such machine learning techniques to cross match RFI from the Karoo Array Telescope (KAT-7) data. We found that not all the features selected to characterise RFI are always important. We further investigated 3 machine learning techniques and conclude that the Random forest classifier performs with a 98% Area Under Curve and 91% recall in detecting RFI.","Feature extraction,
Radio astronomy,
Machine learning algorithms,
Radiofrequency interference,
Data mining,
Time series analysis,
Niobium"
Applying machine learning techniques for forecasting flexibility of virtual power plants,"Previous and existing evaluations of available flexibility using small device demand response have typically been done with detailed information of end-user systems. With these large numbers, having lower level information has both privacy and computational limitations. We propose a black box approach to investigating the longevity of aggregated response of a virtual power plant using historic bidding and aggregated behaviour with machine learning techniques. The two supervised machine learning techniques investigated and compared in this paper are, multivariate linear regression and single hidden layer artificial neural network (ANN). Both techniques are used to model a relationship between the aggregator portfolio state and requested ramp power to the longevity of the delivered flexibility. Using validated individual household models, a smart controlled aggregated virtual power plant is simulated. A hierarchical market-based supply-demand matching control mechanism is used to steer the heating devices in the virtual power plant. For both the training and validation set of clusters, a random number of households, between 200 and 2000, is generated with day ahead profile scaled accordingly. Further, a ramp power (power deviation) is assigned at various hours of the day and requested to hold for the remainder of the day. Using only the bidding functions and the requested ramp powers, the ramp longevity is estimated for a number of different cluster setups for both the artificial neural network as well as the multi-variant linear regression. It is found that it is possible to estimate the longevity of flexibility with machine learning. The linear regression algorithm is, on average, able to estimate the longevity with a 15% error. However, there was a significant improvement with the ANN algorithm achieving, on average, a 5.3% error. This is lowered 2.4% when learning for the same virtual power plant. With this information it would be possible to accurately offer residential VPP flexibility for market operations to safely avoid causing further imbalances and financial penalties.","Space heating,
Water heating,
Heat pumps,
Linear regression,
Mathematical model,
Power generation,
Load management"
Developing Novel Machine Learning Algorithms to Improve Sedentary Assessment for Youth Health Enhancement,"Sedentary behavior of youth is an important determinant of health. However, better measures are needed to improve understanding of this relationship and the mechanisms at play, as well as to evaluate health promotion interventions. Wearable accelerometers are considered as the standard for assessing physical activity in research, but do not perform well for assessing posture (i.e., sitting vs. standing), a critical component of sedentary behavior. The machine learning algorithms that we propose for assessing sedentary behavior will allow us to re-examine existing accelerometer data to better understand the association between sedentary time and health in various populations. We collected two datasets, a laboratory-controlled dataset and a free-living dataset. We trained machine learning classifiers separately on each dataset and compared performance across datasets. The classifiers predict five postures: sit, stand, sit-stand, stand-sit, and stand\walk. We compared a manually constructed Hidden Markov model (HMM) with an automated HMM from existing software. The manually constructed HMM gave more F1-Macro score on both datasets.","Hidden Markov models,
Machine learning algorithms,
Accelerometers,
Algorithm design and analysis,
Standards,
Prediction algorithms,
Training"
New feature selection method based on neural network and machine learning,"Feature selection becomes the focus of much research in many areas of applications for which datasets with large number of features are available. Feature selection is a problem of choosing a subset of relevant features to increase the execution speed of the algorithm and the classification accuracy. It also removes inappropriate features to increase the precision and improve the performances. There has been much effort for solving the feature selection problem up to now and many researchers have proposed and developed many feature selection algorithms in this purpose. In this paper, we propose a new feature selection method based on neural network and machine learning. This new algorithm tends to highlight the best features among existing ones: new weighting-based method of the input features is used in the neural network to choose the best features. Performances show that this method selects the best features on simulated data.","Biological neural networks,
Artificial neural networks,
Conferences,
Machine learning algorithms,
Neurons,
Classification algorithms"
Classifying bent radio galaxies from a mixture of point-like/extended images with Machine Learning,"The hypothesis that bent radio sources are supposed to be found in rich, massive galaxy clusters and the avalibility of huge amount of data from radio surveys have fueled our motivation to use Machine Learning (ML) to identify bent radio sources and as such use them as tracers for galaxy clusters. The shapelet analysis allowed us to decompose radio images into 256 features that could be fed into the ML algorithm. Additionally, ideas from the field of neuro-psychology helped us to consider training the machine to identify bent galaxies at different orientations. From our analysis, we found that the Random Forest algorithm was the most effective with an accuracy rate of 92% for a classification of point and extended sources as well as an accuracy of 80% for bent and unbent classification.",
Machine Learning Predictions of Runtime and IO Traffic on High-End Clusters,"We use supervised machine learning algorithms (i.e., Decision Trees, Random Forest, and K-nearest Neighbors) to predict performance characteristics such as runtime and IO traffic of batch jobs on high-end clusters, using only user job scripts as input. We show that decision trees outperform other algorithms and accurately predict the runtime of 73% of jobs within a error tolerance of 10 minutes, which is a 51% improvement over the user requested runtime.","Decision trees,
Runtime,
Training,
Training data,
Measurement,
Machine learning algorithms,
Monitoring"
Comparing deep learning and support vector machines for autonomous waste sorting,"Waste sorting is the process of separating waste into different types. The current trend is to efficiently separate the waste in order to appropriately deal with it. The separation must be done as early as possible in order to reduce the contamination of waste by other materials. The need to automate this process is a significant facilitator for waste companies. This research aims to automate waste sorting by applying machine learning techniques to recognize the type of waste from their images only. Two popular learning algorithms were used: deep learning with convolution neural networks (CNN) and support vector machines (SVM). Each algorithm creates a different classifier that separates waste into 3 main categories: plastic, paper and metal using only 256 × 256 colored png image of the waste. The accuracies of the two classifiers are compared in order to choose the best one and implement it on a raspberry pi 3. The pi controls a mechanical system that guides the waste from its initial position into the corresponding container. However, in this paper we only compare the two machines learning techniques and implement the best model on the pi in order to measure its speed of classification. SVM achieved high classification accuracy 94.8% while CNN achieved only 83%. SVM also showed an exceptional adaptation to different types of wastes. NVIDIA DIGITS was used for training the CNN while Matlab 2016a was used to train the SVM. The SVM model was finally implemented on a Raspberry pi 3 where it produced quick classification, taking on average 0.1s per image.","Neurons,
Support vector machines,
Convolution,
Sorting,
Training,
Biological neural networks,
Computers"
Fall detection using machine learning algorithms,"In this paper, the recognition of and the differentiation between fall activities and activities of daily living (ADL) was performed using the MobiFall dataset. A large database was constructed to train and validate the model. Feature selection methods were implemented to reduce dimensionality. Five different classification algorithms were implemented and evaluated based on their accuracy' sensitivity, and specificity achieved. The k-Nearest Neighbors' algorithm obtained an overall accuracy of 87.5% with a sensitivity of 90.70%, and a specificity of 83.78%.","Feature extraction,
Accelerometers,
Sensors,
Hidden Markov models,
Machine learning algorithms,
Gyroscopes,
Acceleration"
Machine learning techniques for short term stock movements classification for Moroccan stock exchange,"Accurate stock price forecasting is important for investors and traders to make informed trading decision. However, prices have a complex behavior due to their nonlinearity and nonstationarity. In this paper three Machine learning techniques are implemented to predict a very short term (10 minutes ahead) variations of the Moroccan stock market: Random Forest (RF), Gradient Boosted Trees (GBT) and Support Vector Machine (SVM). A selection of technical indicators was used as inputs variables and a feature selection and samples selection steps were performed to improve prediction accuracy and training time. An eight-year period of intraday prices (tick-by-tick data) of Maroc Telecom (IAM) stocks is employed as experimental database to evaluate the performances of the selected models. The experimental results have shown that RF and GBT are superior to SVM for our dataset. Further, the low computational complexity and reduced training time of RF and GBT are suitable for short term forecasting.","Stock markets,
Support vector machines,
Vegetation,
Input variables,
Forecasting,
Hidden Markov models,
Radio frequency"
A Study to a Film Box Office Prediction Model Based on Machine Learning,Nowadays more and more people focus on the application and development of big data in film industry. This paper presents a study to the box office prediction method. We choose valid inputs into model and use neural network algorithm and multiple linear regression model to do training process. Finally we compare and analyze the results between two models. This is an applicability study and has its value in reality.,"Instruments,
Computers"
Non-intrusive load monitoring using wavelet design and co-testing of machine learning classifiers,This work aims to investigate the effect of implementing co-testing in non-intrusive load monitoring. Wavelet design is used to extract features from the switching transients of loads. The features extracted are evaluated on a real test bed using one-against-the-rest and co-testing approaches. The results showing the effectiveness of both approaches in accurately classifying the loads are presented and discussed.,"Wavelet analysis,
Feature extraction,
Transient analysis,
Mathematical model,
Indexes,
Steady-state,
Training"
A Model-based Machine Learning Approach to Probing Autonomic Regulation from Nonstationary Vital-Signs Time Series,"Physiological variables, such as heart rate (HR), blood pressure (BP) and respiration (RESP), are tightly regulated and coupled under healthy conditions, and a break-down in the coupling has been associated with aging and disease. We present an approach that incorporates physiological modeling within a switching linear dynamical systems (SLDS) framework to assess the various functional components of the autonomic regulation through transfer function analysis of nonstationary multivariate time series of vital signs. We validate our proposed SLDS-based transfer function analysis technique in automatically capturing (i) changes in baroreflex gain due to postural changes in a tilt-table study including 10 subjects, and (ii) the effect of aging on the autonomic control using HR/RESP recordings from 40 healthy adults. Next, using HR/BP time series of over 450 adult ICU patients, we show that our technique can be used to reveal coupling changes associated with severe sepsis (AUC=0.74, sensitivity=0.74, specificity=0.60). Our findings indicate that reduced HR/MAP coupling is significantly associated with severe sepsis even after adjusting for clinical interventions (P<=0.001). These results demonstrate the utility of our approach in phenotyping complex vital-sign dynamics, and in providing mechanistic hypotheses in terms of break-down of autoregulatory systems under healthy and disease conditions.","Time series analysis,
Heart rate,
Switches,
Transfer functions,
Couplings,
Physiology,
Oscillators"
Prediction of information propagation in a drone network by using machine learning,"Drones cooperate with each other by transmitting and receiving packets. Therefore, it is important to conjecture the packet transmission rates within the network. However, the conventional methods are not suitable to describe the transmission patterns with satisfactory computing speed and accuracy. In this paper, we demonstrated that machine learning can successfully predict the transmission patterns in drone network. The packet transmission rates of a communication network with twenty drones were simulated, of which results were used to train the linear regression and Support Vector Machine with Quadratic Kernel (SVM-QK). We found out SVM-QK can precisely predict the communication between drones.","Drones,
Mathematical model,
Communication networks,
Training,
Predictive models,
Simulation,
Learning systems"
A Machine Learning Approach for Dynamic Optical Channel Add/Drop Strategies that Minimize EDFA Power Excursions,We demonstrate a machine learning approach to characterize channel dependence of power excursions in mulit-span EDFA networks. This technique can determine accurate recommendations for channel add/drop with minimal excursions and is applicate to different network design.,
Machine Learning for Optical Performance Monitoring from Directly Detected PDM-QAM Signals,Supervised machine learning methods are applied and demonstrated experimentally for inband OSNR estimation and modulation format classification in optical communication systems. The proposed methods accurately evaluate coherent signals up to 64QAM using only intensity information.,
Automated detection and identification of blue and fin whale foraging calls by combining pattern recognition and machine learning techniques,"a novel approach has been developed for detecting and classifying foraging calls of two mysticete species in passive acoustic recordings. This automated detector/classifier applies a computer-vision based technique, a pattern recognition method, to detect the foraging calls and remove ambient noise effects. The detected calls were then classified as blue whale D-calls [1] or fin whale 40-Hz calls [2] using a logistic regression classifier, a machine learning technique. The detector/classifier has been trained using the 2015 Detection, Classification, Localization and Density Estimation (DCLDE 2015, Scripps Institution of Oceanography UCSD [3]) low-frequency annotated set of passive acoustic data, collected in the Southern California Bight, and its out-of-sample performance was estimated by using a cross-validation technique. The DCLDE 2015 scoring tool was used to estimate the detector/classifier performance in a standardized way. The pattern recognition algorithm's out-of-sample performance was scored as 96.68% recall with 92.03 % precision. The machine learning algorithm's out-of-sample prediction accuracy was 95.20%. The result indicated the potential of this detector/classifier on real-time passive acoustic marine mammal monitoring and bioacoustics signal processing.","Whales,
Pattern recognition,
Training,
Machine learning algorithms,
Prediction algorithms,
Testing,
Acoustics"
A comparison of different machine learning algorithms for automatic classification of sonar targets,"A well-known problem with modern anti-submarine warfare sonars with narrow beamwidths and wide frequency bandwidths, is the frequent occurence of false alarms, particularly in littoral environments. This increases the workload of sonar operators and also reduces the usefulness of automatic systems such as autonomous underwater vehicles, since their limited communication abilities hinder them from sharing large amounts of contacts. In this paper, four traditional machine learning algorithms are tested on sonar data with a high amount of false alarms together with synthetic submarine echoes. It is shown that some of the algorithms can outperform simple signal to noise ratio (SNR) thresholding by a significant amount, but that the performance is highly dependent on the parameter values chosen for each algorithm. These parameters are therefore investigated in order to determine their relative significance.","Underwater vehicles,
Machine learning algorithms,
Sonar,
Signal to noise ratio,
Bagging,
Training,
Feature extraction"
Supervised and unsupervised machine learning for side-channel based Trojan detection,"Hardware Trojan (HT) has recently drawn much attention in both industry and academia due to the global outsourcing trend in semiconductor manufacturing, where a malicious logic can be inserted into the security critical ICs at almost any stages. HT severity mainly stems from its low-cost and stealthy nature where the HT only functions at a strict condition to purposely alter the logic or physical behavior for leaking secrets. This fact makes HT detection very challenging in practice. In this paper, we propose a novel HT detection technique based on machine learning approach. The described solution is constructed over one-class SVM and is shown to be more robust compared to the template based detection techniques. An unsupervised approach is also applied in our solution for mitigating the golden model dependencies. To evaluate the solution, a practical HT design was inserted into an AES coprocessor implemented in a Xilinx FPGA. Based on the partial reconfiguration, the HT size can be dynamically changed without altering cipher part, which helps to precisely evaluate the HT influence. The experimental results have shown that our proposed detection technique achieve a high performance accuracy.","Support vector machines,
Training data,
Trojan horses,
Field programmable gate arrays,
Data models,
Manufacturing,
Performance evaluation"
Camera model identification based machine learning approach with high order statistics features,"Source camera identification methods aim at identifying the camera used to capture an image. In this paper we developed a method for digital camera model identification by extracting three sets of features in a machine learning scheme. These features are the co-occurrences matrix, some features related to CFA interpolation arrangement, and conditional probability statistics. These features give high order statistics which supplement and enhance the identification rate. The method is implemented with 14 camera models from Dresden database with multi class SVM classifier. A comparison is performed between our method and a camera fingerprint correlation-based method which only depends on PRNU extraction. The experiments prove the strength of our proposition since it achieves higher accuracy than the correlation-based method.","Cameras,
Feature extraction,
Image color analysis,
Correlation,
Colored noise,
Interpolation,
Discrete cosine transforms"
Machine learning paradigms for speech recognition of an Indian dialect,"Present era is full of speech recognition based services and products. The machine learning paradigms is at the centre stage of speech recognition methodology. Automatic speech recognition (ASR) technology has vastly evolved in recent years including emerging applications in mobile computing, natural user interface, and man-machine assistive technology. In this paper, it's the first time we are presenting ASR designs based on two important machine learning paradigms Artificial Neural Network (ANN) and Support Vector Machine (SVM) for an rare and geographically important Indian dialect `Chhattisgarhi'. The conventional feed-forward ANN and SVM have been applied on the dataset of maximum 50 isolated words of 15 speakers. The performance of these machine learning paradigms is compared with state of art Hidden Markov Model (HMM). The tendency of ASR to be speaker dependent and independent has been extensively investigated with speaker variation experiments. Furthermore the reliability and stability of ASR has been confirmed with numerical validation. The exhaustive review of ASR techniques from the literature along with the ASR systems designed on Indian languages is presented.","Feature extraction,
Hidden Markov models,
Speech,
Mel frequency cepstral coefficient,
Support vector machines,
Speech recognition,
Data mining"
An Android Behavior-Based Malware Detection Method using Machine Learning,"In this paper, we propose An Android Behavior-Based Malware Detection Method using Machine Learning. We improve an Android application sandbox, Droidbox, by inserting a view-identification automatic trigger program which can click mobile applications in the meaningful order. Taking advantage of Droidbox result, we collect the behavior such as network activities, file read/write and permission as the feature data and use different machine learning algorithms to classify malware and evaluate the performance. We use a large number of malware and normal application samples to prove that our method has high accuracy.","Smart phones,
Malware,
Robots,
Mobile communication,
Machine learning algorithms,
Protocols,
Cryptography"
Deployment of localization system in complex environment. Using machine learning methods,"Conference lecture rooms are considered as an indoor environment for research with indoor localization that faces different challenges. In this paper, we design an indoor localization system in such a complex environment focus on machine learning algorithms with a large scale data. In this system, users can get information where a specified people locate in. Our system has two main sections: system design and algorithm approach. We use Bluetooth Low Energy devices to achieve communication and collect the information consist of people's name and corresponding RSSI data. To illustrate the advantages of machine learning algorithms using in this situation, a traditional algorithm will be proposed as a comparison. We also design and implement a Nine-Rectangle-Grid localization system in a practical laboratory environment. Experiments on real-world environment and simulations show our system can live up to locate people with low cost, high accuracy and short computing time.","Receivers,
Machine learning algorithms,
Radio transmitters,
Bluetooth,
Radiofrequency identification,
Data collection,
Support vector machines"
Machine learning classifiers using stochastic logic,"This paper presents novel architectures for machine learning based classifiers using stochastic logic. Two types of classifier architectures are presented. These include: linear support vector machine (SVM) and artificial neural network (ANN). Stochastic computing systems require fewer logic gates and are inherently fault-tolerant. Thus, these structures are well suited for nanoscale CMOS technologies. These architectures are validated using seizure prediction from electroencephalogram (EEG) as an application example. To improve the accuracy of proposed stochastic classifiers, a novel approach based on linear transformation of input data is proposed for EEG signal classification using linear SVM classifiers. Simulation results in terms of the classification accuracy are presented for the proposed stochastic computing and the traditional binary implementations based datasets from one patient. Compared to conventional binary implementation, the accuracy of the proposed stochastic ANN is improved by 5.89%. Synthesis results are also presented for EEG signal classification. Compared to the traditional binary linear SVM, the hardware complexity, power consumption and critical path of the stochastic implementation are reduced by 78%, 74% and 53%, respectively. The hardware complexity, power consumption and critical path of the stochastic ANN classifier are reduced by 92%, 88% and 47%, respectively, compared to the conventional binary implementation.",
Machine learning approach to forecasting urban pollution,"This work addresses the question of how to predict fine particulate matter given a combination of weather conditions. A compilation of several years of meteorological data in the city of Quito, Ecuador, are used to build models using a machine learning approach. The study presents a decision tree algorithm that learns to classify the concentrations of fine aerosols, into two categories (>15μg/m3 vs. <;15μg/m3), from a limited number of parameters such as the level of precipitation and the wind speed and direction. Requiring few rules, the resulting models are able to infer the concentration outcome with significant accuracy. This fundamental research intends to be a preliminary step in the development of a web-based platform and smartphone app to alert the inhabitants of Ecuador's capital about the risk to human health, with potential future application in other urban areas.","Wind speed,
Urban areas,
Pollution,
Air quality,
Atmospheric modeling,
Road transportation"
Machine Learning-Based Elastic Cloud Resource Provisioning in the Solvency II Framework,"The Solvency II Directive (Directive 2009/138/EC) is a European Directive issued in November 2009 and effective from January 2016, which has been enacted by the European Union to regulate the insurance and reinsurance sector through the discipline of risk management. Solvency II requires European insurance companies to conduct consistent evaluation and continuous monitoring of risks-a process which is computationally complex and extremely resource-intensive. To this end, companies are required to equip themselves with adequate IT infrastructures, facing a significant outlay. In this paper we present the design and the development of a Machine Learning-based approach to transparently deploy on a cloud environment the most resource-intensive portion of the Solvency II-related computation. Our proposal targets DISAR(R), a Solvency II-oriented system initially designed to work on a grid of conventional computers. We show how our solution allows to reduce the overall expenses associated with the computation, without hampering the privacy of the companies' data (making it suitable for conventional public cloud environments), and allowing to meet the strict temporal requirements required by the Directive. Additionally, the system is organized as a self-optimizing loop, which allows to use information gathered from actual (useful) computations, thus requiring a shorter training phase. We present an experimental study conducted on Amazon EC2 to assess the validity and the efficiency of our proposal.","Monte Carlo methods,
Companies,
Insurance,
Computational modeling,
Cloud computing,
Proposals,
Contracts"
Study of annealing induced nanoscale morphology change in organic solar cells with machine learning,"Nanoscale morphology of the active layer in organic solar cells (OSCs) plays a significant role in affecting the overall performance of OSCs. In this work, by using the domain size to describe the morphology change with different annealing temperatures, the influence of annealing to nanoscale morphology in OSCs is intensively studied. Also, machine learning is utilized in investigating the relation between domain size and the annealing temperature. The realization of mapping the domain sizes to their annealing temperatures is significant in bridging the simulation work of OSCs to the actual fabrication process, indicating that the optimized domain sizes obtained through simulation could now be converted to actual annealing temperature in device fabrication.","Annealing,
Photovoltaic cells,
Morphology,
Fabrication,
Temperature measurement,
Nanoscale devices,
Excitons"
Machine learning approach for distinction of ADHD and OSA,"The purpose of this study is to find an efficient way to discriminate between Attention-deficit/ hyperactivity disorder (ADHD) and Obstructive sleep apnea (OSA). The study collected 217 children (aged 6-12 years) data between 2011 and 2015, who were divided into three groups, ADHD, OSA and a combination of ADHD and OSA. Each group based on the doctor's determination, using the DSM-IV diagnostic standards. The data included four questionnaires as follow: CBCL, DBRS, OSA-18 and CSHQ. In order to speed up the whole process of clinical diagnosis classification, we train and test three machine learning models to find the best way to help clinical doctor to diagnosis. The study results indicate that in all of subscale items, there were 17 item show significantly difference among three subgroups, especially in the CBCL. Our results also show that CART model has better computational efficiency than CHAID and Neural Network model for subgroups classification.","Pediatrics,
Sleep apnea,
Distributed Bragg reflectors,
Neural networks,
Computational modeling"
A two-tier network based intrusion detection system architecture using machine learning approach,"Intrusion detection systems are systems that can detect any kind of malicious attacks, corrupted data or any kind of intrusion that can pose threat to our systems. In our paper, we would like to present a novel approach to build a network based intrusion detection system using machine learning approach. We have proposed a two-tier architecture to detect intrusions on network level. Network behaviour can be classified as misuse detection and anomaly detection. As our analysis depends on the network behaviour, we have considered data packets of TCP/IP as our input data. After, pre-processing the data by parameter filtering, we build a autonomous model on training set using hierarchical agglomerative clustering. Further, data gets classified as regular traffic pattern or intrusions using KNN classification. This reduces cost-overheads. Misuse detection is conducted using MLP algorithm. Anomaly detection is conducted using Reinforcement algorithm where network agents learn from the environment and take decisions accordingly. The TP rate of our architecture is 0.99 and false positive rate is 0.01. Thus, our architecture provides a high level of security by providing high TP and low false positive rate. And, it also analyzes the usual network patterns and learns incrementally (to build autonomous system) to separate normal data and threats.","Intrusion detection,
Classification algorithms,
Training,
Clustering algorithms,
Computer architecture,
Data models"
"Majorization-Minimization Algorithms in Signal Processing, Communications, and Machine Learning","This paper gives an overview of the majorization-minimization (MM) algorithmic framework, which can provide guidance in deriving problem-driven algorithms with low computational cost. A general introduction of MM is presented, including a description of the basic principle and its convergence results. The extensions, acceleration schemes, and connection to other algorithmic frameworks are also covered. To bridge the gap between theory and practice, upperbounds for a large number of basic functions, derived based on the Taylor expansion, convexity, and special inequalities, are provided as ingredients for constructing surrogate functions. With the pre-requisites established, the way of applying MM to solving specific problems is elaborated by a wide range of applications in signal processing, communications, and machine learning.","Signal processing algorithms,
Convergence,
Minimization,
Optimization,
Linear programming,
Taylor series,
Estimation"
Towards automatic performance optimization of networks using machine learning,"A key principle for optimizing network performances is to understand the relationship between network topology, configuration parameters, and their influence on the behavior of network protocols. While an attractive approach to this problem is to use formal models of protocols in combination with mathematical optimization, such methods are often limited by either poor scalability or approximations of the models, leading to weak usability for realistic use-cases. In order to overcome those drawbacks, an emerging trend has been to use machine learning based techniques, which offer accurate predictions of the performance of network protocols. Our contribution in this paper is a general methodology which combines the statistical analysis of realistic data with an optimization algorithm in order to plan and optimize network topologies. One benefit of our approach is that it does not require prior knowledge on the studied topology and protocol. As an application example of our method, we address the challenge of placing virtual machines in a given topology with the general goal of optimizing the average bandwidth of TCP flows. In our numerical evaluation, our approach results in an overall average increase of the performance of TCP flows by more than 50%.","Network topology,
Bandwidth,
Topology,
Optimization,
Protocols,
Mathematical model,
Analytical models"
QoE prediction model for IPTV based on machine learning,"IPTV is a new multimedia service over the Internet. In order to improve the quality of IPTV service and the user's satisfaction, telecom operators are interested in studying and improving user's Quality of Experience (QoE). In this paper, we study the relationship between the viewing records from IPTV set-top box and the user's QoE based on IPTV service. Firstly, data processing is performed. After the procedure, the important attributes influencing QoE are selected. Then we propose a new attribute called user's viewing custom from the user's point of view. We also create a mapping between viewing time ratio and user's QoE for subjective video quality evaluation. In addition, we improve the CART algorithm with the idea of weighted mean. Experimental results show that the proposed methods can indeed improve the prediction accuracy of QoE model when comparing with original method.","IPTV,
Predictive models,
Prediction algorithms,
Classification algorithms,
Quality assessment,
Regression tree analysis,
Video recording"
Comparative risk analysis on prediction of Diabetes Mellitus using machine learning approach,This study proposes on the prediction and classification of Diabetes Mellitus using Artificial Neural Network (ANN) and hybrid Adaptive Neuro-Fuzzy Inference System (ANFIS). The network was trained by using the data of 100 individuals with mean age of 42 years with an equal proportion of male and female. The performance of each approach is further discussed on the basis of accuracy and validates accurate prediction. Our research reveals that the ANFIS approach is more acceptable as compared to the ANN approach on the basis of accuracy.,
A machine learning approach for fast future grid small-signal stability scanning,"In this paper, we propose a novel fast scanning approach to perform small signal stability study of future power systems with high penetration of renewable generation. Stability assessment is an important component of power system planning. Due to generation technology diversity and inherent intermittent availability of most renewable sources in future grids, the conventional method to conduct stability analysis based on choosing a limited number of worst case operating points becomes infeasible. One way to capture the stability profile of a future grid scenario is to scan a large number of possible operating conditions. However, to achieve fast scanning and make the time consuming numerical study computationally affordable, the simulation burden has to be reduced. To that end, we propose a fast scanning approach based on feature selection and weighted clustering to reduce simulation burden when conducting stability scanning over a long period of time. We propose a novel algorithm based on conventional Relief-F and K-means techniques. Simulation results show the effectiveness of the proposed approach.","Power system stability,
Stability analysis,
Clustering algorithms,
Analytical models,
Numerical stability,
Computational modeling,
Power system planning"
Identifying malicious web domains using machine learning techniques with online credibility and performance data,"Malicious web domains represent a big threat to web users' privacy and security. With so much freely available data on the Internet about web domains' popularity and performance, this study investigated the performance of well-known machine learning techniques used in conjunction with this type of online data to identify malicious web domains. Two datasets consisting of malware and phishing domains were collected to build and evaluate the machine learning classifiers. Five single classifiers and four ensemble classifiers were applied to distinguish malicious domains from benign ones. In addition, a binary particle swarm optimisation (BPSO) based feature selection method was used to improve the performance of single classifiers. Experimental results show that, based on the web domains' popularity and performance data features, the examined machine learning techniques can accurately identify malicious domains in different ways. Furthermore, the BPSO-based feature selection procedure is shown to be an effective way to improve the performance of classifiers.","Feature extraction,
Web pages,
Malware,
Computational modeling,
Electronic mail,
Information filters"
On the use of inertial sensors and machine learning for automatic recognition of fainting and epileptic seizure,"This paper depicts a machine learning method for fainting and epileptic seizures automatic recognition. We evaluated five machine learning techniques in order to find out which classification method maximizes the accuracy level and, at the same time, minimizes the computational complexity since the experimental environment has very limited computational resources (processing power). We prototype such method in a wearable device, taking into account F-Score and Accuracy metrics. The experimental evaluation shows that there are no significant difference between KNN, PART, and C4.5. However, KNN has high computational cost when compared to PART and C4.5. PART has low computational cost when compared to C4.5 since it identified less rules.","Accelerometers,
Sensors,
Machine learning algorithms,
Support vector machines,
Training,
Epilepsy,
Decision trees"
"Combining cloud computing, machine learning and heuristic optimization for investment opportunities forecasting","Prediction of stock market is a challenging task that has attracted researchers in various fields including the computational intelligence and finance. Since stock market data sets are intrinsically large, nonlinear and time-varying, it is extremely difficult to design models for forecasting the future directions with an acceptable accuracy. In this paper, an integrative and intelligent machine learning framework is proposed through combining cloud computing, machine learning and heuristic optimization. Essentially, the Support Vector Machine (SVM) method is extended with the Grid Search (GS) or Chemical Reaction Optimization (CRO) as a heuristic optimization method together with Principal Component Analysis (PCA) and Feature Noise Filter (FNF) to construct quantitative investment forecasting models for efficient executions on cloud computing platforms. To demonstrate the effectiveness of the proposed framework, the Hang Seng Index and some major stocks listed on the Hong Kong Exchange are predicted using the constructed models on a daily basis. The empirical results clearly indicate that the proposed integrative approach is promising and gives impressive performance in terms of the prediction accuracy.","Support vector machines,
Optimization,
Stock markets,
Artificial neural networks,
Training,
Computational modeling,
Predictive models"
Machine learning techniques for short-term load forecasting,"Selection of an adequate tool for accurate short-term load forecasting task is becoming more important for electric utilities. Machine learning techniques are proving useful for short-term electricity load forecasting. In this paper we evaluate performance of several machine learning algorithms applied to electricity load datasets. We evaluated performance of SMOreg, and Additive regression algorithms for load forecasting using electricity consumption datasets. We also performed an Artificial Neural Networks (ANN) analysis on short-term load forecasting.","Load forecasting,
Artificial neural networks,
Load modeling,
Predictive models,
Additives,
Time series analysis,
Forecasting"
Digital disease detection: Application of machine learning in community health informatics,"Health informatics is a new research area which is interdisciplinary amongst information science, computer science and healthcare. The concept of health informatics is to develop a new way to manipulate healthcare data from various resources and devices by optimizing the method of data acquisition, data storage, data processing, and data visualization. Community health informatics can be described as the systematic application of information and computer science to obtain valuable data for solving health problems and providing it to health policy makers. The challenge of community health informatics is to maximize the efficiency and efficacy of big data analysis. This discussion paper aims to present the various applications of machine learning and software engineering approaches that implemented in digital disease detection.","Predictive models,
Forecasting,
Surveillance,
Informatics,
Support vector machines"
The monitoring system of Business support system with emergency prediction based on machine learning approach,"When Business support systems (BSS) suffer poor performance, the system administrator has to spot the problem of BSS as soon as possible. The monitoring system is a great help to quickly gain insight of the system from all sides. By the monitoring system, the administrator can evaluate various metrics on the different fields in one single arranged page and understands the whole picture of current system status from brief reports. Although this kind of problem solving flow does well in certain circumstance, it may still be too late for some critical BSS. To further improve the speed of trouble-shooting, administrator sets the thresholds to each performance metric. Those thresholds are set based on knowledge and experience of administrator. If the performance metric collected from the system is over the set threshold, monitoring system will send alerts to administrator to inform current BSS status, and he can check the system status in advance before the situation get worse. This kind of traditional approach finds the problem about ten minutes before the emergency break out. In our work, we leverage a machine learning approach to determine emergency earlier. The machine learning model we developed can predict the healthy status of the system before the emergency an hour with average 14 points error.","Measurement,
Monitoring,
Servers,
Prediction algorithms,
Data visualization,
Machine learning algorithms,
Data collection"
On the use of machine learning in microphone array beamforming for far-field sound source localization,This paper presents a weighted minimum variance distortionless response (WMVDR) algorithm for far-field sound source localization in a noisy environment. The broadband beam-forming is computed in the frequency-domain by calculating the response power on each frequency bin and by fusing the narrowband components. A machine learning method based on a support vector machine (SVM) is used for selecting only the narrowband components that positively contribute to the broadband fusion. We investigate the direction of arrival (DOA) estimation problem using a uniform linear array (ULA). The skewness measure of response power function is used as input feature for the supervised SVM learning. Simulations demonstrate the effectiveness of the WMVDR in an outdoor noisy environment.,"Support vector machines,
Narrowband,
Training,
Direction-of-arrival estimation,
Signal to noise ratio,
Broadband communication,
Sensors"
Classification of SSH Attacks Using Machine Learning Algorithms,"SSH Attacks are of various types: SSH port scanning, SSH Brute-force attacks, Attacks using compromised SSH server. Attacks using a compromised server could be DoS attacks, Phishing attacks, E- mail spamming and so on. This paper questions whether the attacks from a compromised SSH server be segregated from other attacks using the network flows. In this work, we categorize SSH attacks into two types. The first category consists of all attack activities after a successful compromise of an SSH server. We name it as ""severe"" attacks. The second type includes all attacks leading to a successful compromise. It consists of SSH port scanning, SSH Brute-force attack, and compromised SSH server with no activities. The second category is named as ""not-so-severe"" attacks. We employ Machine Learning algorithms, namely, Naive Bayes learner, Logistic Regression, J48 decision tree, and Support Vector Machine to classify these attacks. Suitable features were selected based on domain knowledge, literature survey, and feature selection technique to evaluate the performance of machine learning algorithms using the metrics accuracy, sensitivity, precision, and F-score.","Machine learning algorithms,
Servers,
Ports (Computers),
Protocols,
Feature extraction,
Decision trees,
Support vector machines"
ANCFIS-ELM: A machine learning algorithm based on complex fuzzy sets,"The Adaptive Neuro-Complex Fuzzy Inferential System was the first neuro-fuzzy system employing complex fuzzy sets and rule interference. It was shown to be both accurate and parsimonious in time series forecasting. The main disadvantage of this system is its slow learning algorithm. One possible approach to speeding up this neuro-fuzzy system is to apply concepts from the Extreme Learning Machine family of architectures; specifically, we will randomly select the parameters of a “pool” of complex fuzzy sets, and then train the neural network by incrementally updating the parameters of a linear output function. We evaluate this new architecture on four software reliability growth datasets (a particular instance of time series forecasting).",
Better Protection of SS7 Networks with Machine Learning,"Deregulation and migration to IP have made SS7 vulnerable to serious attacks such as location tracking of subscribers, interception of calls and SMS, fraud, and denial of services. Unfortunately, current protection measures such as firewalls, filters, and blacklists are not able to provide adequate protection of SS7. In this paper, a method for detection of SS7 attacks using machine learning is proposed. The paper clarifies the vulnerabilities of SS7 networks and explains how machine learning techniques can help improve SS7 security. A proof- of- concept SS7 protection system using machine learning is also described thoroughly.","Security,
Mobile communication,
Companies,
Communication networks,
IP networks,
Current measurement"
Uncertain training data set conceptual reduction: A machine learning perspective,"Knowledge discovery from data is a challenging problem that has significant importance in many different fields such as biology, economics and social sciences. Real-world data is incomplete and ambiguous; moreover, its rapid increase in size complicates the analysis process. Therefore, data reduction techniques that consider data uncertainty are highly required. In this paper, our objective is to conceptually reduce uncertain data without losing information. Two reduction methods are proposed that are mainly rooted in formal concept analysis theory. The first method is targeting approximate data reduction; it uses the result of Baixeries et al. for detecting functional dependencies by transforming an instance of a database into an approximate formal context. The second method is based on fuzzy data reduction that employs the algorithm of Elloumi et al. in fuzzy data reduction using Lukasiewicz logic. These reduction methods have been compared to three other machine learning based reduction algorithms through a classification case study of breast cancer data. Classification accuracy, root mean square error and reduced data size have been reported to show that reduced training sets using our methods result in very accurate classifiers with minimal data size. Moreover, the reduced data has the advantage of decreasing communication time and memory space.","Context,
Machine learning algorithms,
Formal concept analysis,
Birds,
Uncertainty,
Databases,
Algorithm design and analysis"
WeChat Text Messages Service Flow Traffic Classification Using Machine Learning Technique,"In this era of information technology, network traffic classification is a very important and hot topic from the perspective of network security and management due to substantial use of dynamic applications. Numerous research models have been proposed in network traffic classification to classify different types of applications and achieve significant accuracy results. However, no work has been done to classify WeChat messages flow traffic. WeChat is a free instant messaging application. Hence, it is very important to classify WeChat text messages traffic. In this paper, we classify WeChat messages flows traffic using two different data sets, which are first captured using Wireshark tool from two different locations network environments, Harbin Institute of Technology Lab and Jinyuan Hotel and then 50 features are extracted from captured traffic. After that four machine learning algorithms SVM, C4.5, Bayes Net and nalve Byes are applied to classify the WeChat text messages traffic. Experimental results show that all classifiers give very high accuracy results using two different data sets. Using Jinyuan data set SVM and C4.5 decision tree algorithm give 100% accuracy result as compared to Bayes Net and Naive Bayes algorithm and using Harbin Institute of Technology Lab data set all classifiers give 99.7% high accuracy results.","Machine learning algorithms,
Support vector machines,
Training,
Classification algorithms,
Feature extraction,
Software,
Telecommunication traffic"
Automatic detection of subsurface defects in composite materials using thermography and unsupervised machine learning,"This paper presents a complete framework aimed to nondestructive inspection of composite materials. Starting from the acquisition, performed with lock-in thermography, the method flows through a set of consecutive blocks of data processing: input enhancement, feature extraction, classification and defect detection. Experimental results prove the capability of the presented methodology to detect the presence of defects underneath the surface of a calibrated specimen made of Glass Fiber Reinforced Polymer (GFRP). Results are also compared with those obtained by other techniques, based on different features and unsupervised learning methods. The comparison further proves that the proposed methodology is able to reduce the number of false positives, while ensuring the exact detection of subsurface defects.","Feature extraction,
Composite materials,
Heat transfer,
Robustness,
Inspection,
Data processing,
Histograms"
Diagnostic visualization for non-expert machine learning practitioners: A design study,"As machine learning (ML) becomes increasingly popular, developers without deep experience in ML - who we will refer to as ML practitioners - are facing the need to diagnose problems with ML models. Yet successful diagnosis requires high-level expertise that practitioners lack. As in many complex data-oriented domains, visualization could help. This two-phase study explored the design of visualizations to aid ML diagnosis. In phase 1, twelve ML practitioners were asked to diagnose a model using ten state-of-the-art visualizations; seven design themes were identified. In phase 2, several design themes were embodied in an interactive visualization. The visualization was used to engage practitioners in a participatory design exercise that explored how they would carry out multi-step diagnosis using the visualization. Our findings provide design implications for tools that better support ML diagnosis by non-expert practitioners.","Data models,
Predictive models,
Data visualization,
Probes,
Software,
Visualization,
Prediction algorithms"
Multiobjective fuzzy genetics-based machine learning with a reject option,"Classifier design for a classification problem with M classes can be viewed as finding an optimal partition of its pattern space into M disjoint subspaces. However, this is not always a good strategy especially when training patterns from different classes are heavily overlapping in the pattern space. A simple but practically useful idea is the use of a reject option. In this case, the pattern space is partitioned into (M+1) disjoint subspace where the classification of new patterns is rejected in the (M+1)th subspace. In this paper, we discuss the design of fuzzy rule-based classifiers with a reject option. The rejection subspace is specified by a threshold value for the difference of a kind of matching degrees between the best matching class and the second best matching class. The important research question is how to specify the threshold value. We examine the following two approaches: One is manual specification after designing a fuzzy rule-based classifier, and the other is simultaneous multiobjective optimization of a threshold value and a fuzzy rule-based classifier. In the latter approach, we use three objectives: maximization of the correct classification, and minimization of the rejection and the complexity of the classifier.","Complexity theory,
Optimization,
Minimization,
Training data,
Training,
Error analysis,
Fuzzy systems"
B-CHIRP: A machine learning tool to assist education of novice birders and signal processing programmers,"This paper reports on development of a bird call recognition application and signal processing coding framework referred to as the “Bird Call Heuristic-based Identification and Recognition Program” (B-CHIRP for short). The aim of this project was development of an application that serves two purposes: provides a system to assist novice birders in the identification of birds based on their sounds, and a second objective of providing a coding application framework appropriate for use teaching signal processing techniques with the added twist of bringing in a complex real-world application problem and an environmentally conscious flavor to promote nature appreciation with a systems thinking approach to engineering education. This paper focuses on the developmental aspects of the B-CHIRP framework, its performance and bird identification prediction accuracy tested using a selection of indigenous South African birds. The dimension of using the framework in an educational perspective is limited to reflections on anticipated influences and potential stumbling blocks of using the framework in a signal processing teaching contexts.",
Accelerating machine learning with Non-Volatile Memory: Exploring device and circuit tradeoffs,"Large arrays of the same nonvolatile memories (NVM) being developed for Storage-Class Memory (SCM) - such as Phase Change Memory (PCM) and Resistance RAM (ReRAM) - can also be used in non-Von Neumann neuromorphic computational schemes, with device conductance serving as synaptic “weight.” This allows the all-important multiply-accumulate operation within these algorithms to be performed efficiently at the weight data.","Training,
Nonvolatile memory,
Phase change materials,
Neurons,
Performance evaluation,
Phase change memory,
Neural networks"
Machine Learning-Based Antenna Selection in Wireless Communications,"This letter is the first attempt to conflate a machine learning technique with wireless communications. Through interpreting the antenna selection (AS) in wireless communications (i.e., an optimization-driven decision) to multiclass-classification learning (i.e., data-driven prediction), and through comparing the learning-based AS using k -nearest neighbors and support vector machine algorithms with conventional optimization-driven AS methods in terms of communications performance, computational complexity, and feedback overhead, we provide insight into the potential of fusion of machine learning and wireless communications.","Antennas,
Training,
Support vector machines,
Wireless communication,
Bit error rate,
Indexes,
MIMO"
Supervised and unsupervised machine learning for improved identification of intrauterine growth restriction types,"This paper concerns automated identification of intrauterine growth restriction (IUGR) types by use of machine learning methods. The research presents a comparison of supervised and unsupervised learning covering single and hybrid classification, as well as clustering. Supervised learning techniques included bagging with Naïve Bayes, k-nearest neighbours (kNN), C4.5 and SMO as base classifiers, random forest as a variant of bagging with a decision tree as a base classifier, boosting with Naïve Bayes, SMO, kNN and C4.5 as base classifiers, and voting by all single classifiers using majority as a combination rule, as well as five single classification strategies: kNN, C4.5, Naïve Bayes, random tree and sequential minimal optimization algorithm for training support vector machines. Unsupervised learning encompassed k-means and expectation-maximization algorithms. The major conclusion drawn from the study was that hybrid classifiers have demonstrated their potential ability to identify more accurately symmetrical and asymmetrical types of IUGR, whereas the unsupervised learning techniques produced the worst results.","Boosting,
Bagging,
Training,
Pediatrics,
Algorithm design and analysis,
Biomedical imaging,
Unsupervised learning"
UAV degradation identification for pilot notification using machine learning techniques,"Unmanned Aerial Vehicles are currently investigated as an important sub-domain of robotics, a fast growing and truly multidisciplinary research field. UAVs are increasingly deployed in real-world settings for missions in dangerous environments or in environments which are challenging to access. Combined with autonomous flying capabilities, many new possibilities, but also challenges, open up. To overcome the challenge of early identification of degradation, machine learning based on flight features is a promising direction. Existing approaches build classifiers that consider their features to be correlated. This prevents a fine-grained detection of degradation for the different hardware components. This work presents an approach where the data is considered uncorrelated and, using machine learning techniques, allows the precise identification of UAV's damages.","Time series analysis,
Degradation,
Sensors,
Machine learning algorithms,
Robots,
Heuristic algorithms,
Unmanned aerial vehicles"
Inverse kinematics learning for redundant robot manipulators with blending of support vector regression machines,"Redundant robot manipulator is a kind of robot arm having more degrees-of-freedom (DOF) than required for a given task. Due to the extra DOF, it can be used to accomplish many complicated tasks, such as dexterous manipulation, obstacle avoidance, singularity avoidance, collision free, etc. However, modeling the inverse kinematics of such kind of robot manipulator remains challenging due to its property of null space motion. In this paper, support vector regression (SVR) is implemented to solve the inverse kinematics problem of redundant robotic manipulators. To further improve the prediction accuracy of SVR, a special machine learning technique called blending is used in this work. The proposed approach is verified in MATLAB with a seven DOF Mitsubishi PA-10 robot and the simulation results have proved its high accuracy and effectiveness.","Manipulators,
Support vector machines,
Kinematics,
Collision avoidance,
Service robots,
Planning"
Early warning system for seismic events in Coal Mines using machine learning,This document describes an approach to the prob- lem of predicting dangerous seismic events in active coal mines up to 8 hours in advance. It was developed as a part of the AAIA'16 Data Mining Challenge: Predicting Dangerous Seismic Events in Active Coal Mines. The solutions presented consist of ensembles of various predictive models trained on different sets of features. The best one achieved a winning score of 0.939 AUC.,"Geology,
Geologic measurements"
One-class extreme learning machines for gas turbine combustor anomaly detection,"Gas turbine combustor anomaly detection plays a critical role in reducing operation and maintenance costs in power plant operations. One-class classification, to learn a model that properly describes the normal samples, is one of many anomaly detection approaches that have shown promising performance in real-world applications. Extreme learning machines (ELMs), a recent developed machine learning technique, have outperformed SVMs and other machine learning methods for many machine learning problems. ELMs have been originally used for binary or multiclass classification and regression as well. Using ELM for one-class classification has emerged very recently, but has not been widely explored for real world applications. In this paper, we adopt ELMs to a new application domain - industrial machine condition monitoring. More specifically, we apply one-class ELMs for more accurate anomaly detection of gas turbine combustors. We evaluate different one-class ELMs and compare them against other one-class classifiers. Using a real-world gas turbine combustor anomaly detection as the case study, we demonstrate that one-class ELMs can be more effective than other one-class classification algorithms in early detecting gas turbine combustor anomalies.","Wind turbines,
Training,
Maintenance engineering,
Neurons,
Kernel,
Temperature measurement"
Machine learning based malware classification for Android applications using multimodal image representations,"The popularity of smartphones usage especially Android mobile platform has increased to 80% of share in global smartphone operating systems market, as a result of which it is on the top in the attacker's target list. The fact of having more private data and low security assurance letting the attacker to write several malware programs in different ways for smartphone, and the possibility of obfuscating the malware detection applications through different coding techniques is giving more energy to attacker. Several approaches have been proposed to detect malwares through code analysis which are now severely facing the problem of code obfuscation and high computation requirement. We propose a machine learning based method to detect android malware by analyzing the visual representation of binary formatted apk file into Grayscale, RGB, CMYK and HSL. GIST feature from malware and benign image dataset were extracted and used to train machine learning algorithms. Initial experimental results are encouraging and computationally effective. Among machine learning algorithms Random Forest have achieved highest accuracy of 91% for grayscale image, which can be further improved by tuning the various parameters.","Malware,
Image color analysis,
Feature extraction,
Androids,
Humanoid robots,
Gray-scale,
Machine learning algorithms"
Evaluation of machine learning methods to predict knee loading from the movement of body segments,"Abnormal joint moments during gait are validated predictors of knee pain in osteoarthritis. Calculation of moments necessitates measurement of forces and moment arms about joints during walking. Dynamically changing moment arms can be calculated from motion trackers either optically or with wireless inertia sensing units, but the measurement of forces is more problematic. Either the patient has to walk over a force platform or a force sensing device has to be built into the sole of the shoes. One possible means of registering abnormal joint moments without the restrictions due to force measurements is to predict moments from the movement of body segments using advanced machine learning techniques. To test the viability of this approach, we aimed to predict the frontal plane internal knee abduction moment form 3D Euler angles of the ankle, knee, hip and pelvis during a single gait cycle of 31 patients with alkaptonuria. Four machine-learning algorithms were used in our experiment to predict moments namely: Decision Tree, Random Forest, Linear Regression and Multilayer Perceptron neural network. Based on performance measures of prediction (R2, root mean squared error and area under the recall curve), the random forest algorithm performed best but this was also the slowest by a factor of 10. Considering both performance and speed, the Multilayer Perceptron neural network method was superior with R2, root mean square of error, area under the recall curve and required training time of 0.8616, 0.0743, 0.874 and 730 ms, respectively.","Knee,
Correlation,
Hip,
Training,
Pelvis,
Machine learning algorithms,
Prediction algorithms"
Fast training of convolutional neural network classifiers through extreme learning machines,"This paper presents a fast algorithmic method to train convolutional neural network (CNN) classifiers through extreme learning which has been verified on popular datasets on classification and pedestrian detection. CNN has been one of the best classifiers for images and object recognition. However, the Backpropagation (BP) algorithm, mostly used for training CNN, suffers from slow learning, local minimum, and poor generalization. To solve these problems, a novel architecture called CNN-ELM has been proposed here. Its core architecture is based on a local image (local receptive field) version of the ELM (Extreme Learning Machine) adopting random feature learning. Using MATLAB 2015a, classification experiments using the raw image data as input, show a comparable or mostly better classification performance compared to the BP trained CNN, with its training speed up to 200 times faster on MNIST, NORB, and CIFAR-10 datasets. Pedestrian detection experiments using INRIA datasets also exhibits much faster training than the BP trained CNN without sacrificing detection performance.","Training,
Convolution,
Kernel,
Training data,
Neural networks,
Object recognition,
Machine learning"
Measuring sleep quality from EEG with machine learning approaches,"This study aims at measuring last-night sleep quality from electroencephalography (EEG). We design a sleep experiment to collect waking EEG signals from eight subjects under three different sleep conditions: 8 hours sleep, 6 hours sleep, and 4 hours sleep. We utilize three machine learning approaches, k-Nearest Neighbor (kNN), support vector machine (SVM), and discriminative graph regularized extreme learning machine (GELM), to classify extracted EEG features of power spectral density (PSD). The accuracies of these three classifiers without feature selection are 36.68%, 48.28%, 62.16%, respectively. By using minimal-redundancy-maximal-relevance (MRMR) algorithm and the brain topography, the classification accuracy of GELM with 9 features is improved largely and increased to 83.57% in average. To investigate critical frequency bands for measuring sleep quality, we examine the features of each band and observe their energy changing. The experimental results indicate that Gamma band is more relevant to measuring sleep quality.","Sleep,
Electroencephalography,
Feature extraction,
Current measurement,
Electrodes,
Frequency conversion,
Fourier transforms"
Mapping land cover with hyperspectral and multispectral satellites using machine learning and Spectral Mixture Analysis,"The goal of this study was compare hyperspectral and multispectral imagery for mapping broad land-cover classes at the spatial scale of a satellite image. The study area was the San Francisco Bay Area and was roughly the size of a Landsat scene (30,000 km2). The Random Forests machine learning and Multiple-Endmember Spectral Mixture Analysis (MESMA) classifiers were compared to predictor variables composed of simulated HyspIRI hyperspectral images, simulated Landsat 8 and Sentinel-2 multispectral images, and real Landsat 8 images. The Random Forests machine learning classifier consistently outperformed MESMA and there were significant improvements in overall accuracy with multi-temporal (spring, summer, fall) over summer-only images for all sensors tested. Hyperspectral reflectance data had no difference to less accuracy relative to comparable multispectral datasets. However, HyspIRI hyperspectral metrics that targeted key spectral features, related to chemical and structural properties, yielded significantly improved accuracy over both real and simulated multispectral datasets.","Satellites,
Hyperspectral imaging,
Reflectivity,
Earth,
Measurement"
Machine learning for quality prediction in abrasion-resistant material manufacturing process,"Quality monitoring and prediction plays a key role in improving product quality and achieving automated quality control in manufacturing processes such as the abrasion-resistant material manufacturing process. Traditional methods that rely on the use of first-principle models are difficult to formulate due to the increasing complexity and high dimensionality of manufacturing processes. Data-driven machine learning methods offer an efficient way to learn models for quality prediction, in which the meaningful process information can be learned directly from large amounts of measured process data at different stages. In this paper, based on data collected throughout an abrasion-resistant material manufacturing process, product quality prediction of burned balls is achieved with the use of the Support Vector Machine classification algorithm.","Manufacturing processes,
Support vector machines,
Kernel,
Monitoring,
Classification algorithms,
Prediction algorithms"
Predicting the sentiment of SaaS online reviews using supervised machine learning techniques,"There has been a dramatic increase in the sharing of opinions and information across different web platforms and social media, especially online product reviews. Cloud web portals, such as getApp.com, were designed to amalgamate cloud service information and to also examine how consumers evaluate their experience of using cloud computing products. The current literature shows the growing importance of online users' reviews, hence this study focuses on investigating consumers' feedback on Software-as-a- Service (SaaS) products by developing models to predict reviewers' attitudes. The goal of this paper is to develop prediction models to predict the sentiment of SaaS consumers' reviews (positive or negative). This research proposes five models that are based on five algorithms, the Support Vector Machine algorithm, Naive Bayes algorithm, Naive Bayes (Kernel) algorithm, k-nearest neighbors algorithm, and the decision tree algorithm to predict the attitude of SaaS reviews. The prediction accuracy of the space vector algorithm (5-fold cross-validation) is 92.37% which suggests that this algorithm is able to better determine the sentiment of online reviews compared with the other models. The results of this study provide valuable insight into online SaaS reviews and will assist in the design of SaaS review websites.",
Is sentiment analysis an art or a science? Impact of lexical richness in training corpus on machine learning,"Social Media is exploding with data - that can help you derive an optimal marketing strategy in the internet world, engage with your audience on the fly, and protect your reputation from smearing campaigns if it is processed and analyzed in a timely fashion. Digital marketing analysts and data scientists rely on social media analytics tools to deduce customer sentiment from countless opinions and reviews. While numerous attempts have been made to improve their accuracy in the past, yet we know surprisingly little about how accurate their results are. We present an unbiased study of users' tweets and the methods that leverage the available tools & technologies for opinion mining. Our prime focus is on improving the consistency of text classifiers used for linguistic analysis. We also measure the impact of lexical richness in the sample data on the trained algorithm. This paper attempts to improve the reliability of sentiment classification process by the creation of a custom vote classifier using natural language processing techniques and various machine learning algorithms.","Natural language processing,
Motion pictures,
Data mining,
Blogs,
Twitter,
Informatics"
Ultra-Lightweight Malware Detection of Android Using 2-Level Machine Learning,"As Android becoming the most popular smart phone operating system, malicious applications running on the Android platform appears very frequently and poses the major threat to the security of Android. Considering the resources of smart phone are severely limited, a stable, simple and quick malware detection method for Android is indispensable. In this paper, we propose an ultra-lightweight malware detection method which is able to detect unknown malicious Android applications with limited resources. Firstly, a few features are extracted and divided into three sets for every application. Then, these three feature sets are embedded in the corresponding joint vector spaces and we can get apps's feature vectors. After that, feature vectors of every vector space are classified using a machine learning algorithm. Finally, the three classification results are considered as a group and embedded in a new space and classified again. We evaluate our detection with 3427 malicious samples and 1550 benign applications. Experimental results show that our detection approach has a stable performance that the detection accuracy (true-positive rate) is always higher than 98% and the detection procedure costs only 30ms per sample.","Feature extraction,
Malware,
Androids,
Humanoid robots,
Smart phones,
Support vector machines,
Machine learning algorithms"
An empirical semi-supervised machine learning approach on extracting and ranking document level multi-word product names using improved C-value approach,"In recent years, the volume of data submissions (E-Commerce data) in online on products, service, and organizations is increasing exponentially. This online data is abundantly unstructured; extracting knowledge from that huge volume of data is a non-trivial task. In recent years, extracting product names become a very popular approach and also one of the important methods in sentiment analysis. This product name extraction is very useful in E-commerce, because it helps in identifying people interest on products, generation of reviews' metadata and identification of product attributes, etc. The existing approaches in product name extraction are capable of extracting single word product names. However, the product names can be a sequence of words, which is also called multi-word product names that cannot be obtained automatically by the existing methods. In this paper, a combined approach of semi-supervised machine learning and improved C-value approach is proposed to discover the multi-word product names, ranking those product names and identifying a dominant product in review documents.",
Machine-learning based detection of corresponding interest points in optical and SAR images,"One of the major problems of keypoint-based alignment of SAR and optical images is that keypoint operators react to very different object structures in both image types. This leads to a small mutual overlap in the corresponding sets of keypoints. This paper proposes to cast the task of keypoint detection as a classification problem. A machine-learning based classifier is trained to predict whether a SAR image pixel corresponds to a keypoint in the optical image or not. Experimental results indicate, that the mutual overlap of keypoints can be doubled by the proposed approach.","Optical imaging,
Adaptive optics,
Synthetic aperture radar,
Optical polarization,
Optical detectors,
Training"
Effort estimation of web-based applications using machine learning techniques,"Effort estimation techniques play a crucial role in planning of the development of web-based applications. Web-based software projects, considered in the present-day scenario are different from conventional object oriented projects, and hence the task of effort estimation is a complex one. It is observed that the literature do not provide a guidance to the analysts to use a particular model as being the most suitable one, for effort estimation of web-based applications. A number of models like IFPUG Function Point Model, NESMA, MARK-II, etc. are being considered for web effort estimation purpose. The efficiency of these models can be improved by employing certain intelligent techniques on them. Keeping in mind the end goal to enhance the efficiency of evaluating the effort required to develop web-based application, certain machine learning techniques such as Stochastic Gradient Boosting and Support Vector Regression Kernels are considered in this study for effort estimation of web-based applications using IFPUG Function Point approach. The ISBSG dataset, Release 12 has been considered in this study for obtaining the IFPUG Function Point data. The performance effort estimation models based on various machine learning techniques is assessed with the help of certain metrics, in order to examine them critically.","Informatics,
Conferences"
Exploring machine learning methods for the Star/Galaxy Separation Problem,"For recent or planned deep astronomical surveys, it is important to tell stars and galaxies apart, a task known as Star/Galaxy Separation Problem (SGSP). At faint magnitudes, the separation between pointy and extended sources is fuzzy, which makes SGSP a hard task. This problem is even harder for large surveys like Dark Energy Survey (DES) and, in a near future, the Large Synoptic Survey Telescope (LSST) due to their large data volume. Hence, the search for classification methods that are both accurate and efficient is highly relevant. In this work, we present a comparative analysis of several machine learning methods targeted at solving the SGSP at faint magnitudes. In order to train the classification models, the COSMOS survey was used. We use machine learning methods as distinct as artificial neural networks, k nearest-neighbor, Support Vector Machines, Random Forests and Naive Bayes. The exploratory process was modeled as data centric workflow. The workflow was implemented on top of Hadoop framework and was used to find the best parameter values for each classification method we considered, of which neural networks and random forest present superior performance.","Neural networks,
Learning systems,
Support vector machines,
Telescopes,
Training,
Extraterrestrial measurements,
Spatial resolution"
Extreme Learning Machines for approximating nonlinear dimensionality reduction mappings: Application to Haptic handwritten signatures,"The abundance of computing and mobile devices makes the problem of user identification and verification an essential requirement for many applications. Haptics devices include the sense of touch in the form of kinesthetic and tactile feedback which provide additional features within handwritten signatures. However, they generate high dimensional data and dimensionality reduction techniques become useful for data mining, machine learning and visualization. Nonlinear transformations have been used for this, but in present day scenarios (Big Data, the Internet of Things, massive data streams, etc.) the computation becomes more complex, time consuming or impractical. Moreover, the relationships between the features of the original and the target spaces are more difficult to uncover. Extreme Learning Machines (ELM) are used for approximating nonlinear manifold learning methods in two ways: as a functional representation for implicit methods, and as simpler surrogate models for explicit mapping techniques. In the context of Haptic handwritten signatures, five implicit and explicit nonlinear transformation methods are investigated. In all cases it was found that ELM approximations to the mappings obtained with the original methods exhibit very good behavior and can be used either as functional representations for the implicit methods or as simpler surrogate models for explicit techniques.","Haptic interfaces,
Aerospace electronics,
Data visualization,
Manifolds,
Neural networks,
Electronic mail,
Mobile handsets"
Heterogeneous extreme learning machines,"The developments in communication, sensor and computing technologies are generating information at increasing rates and the nature of the data is becoming highly heterogeneous. Accordingly, the objects under study are described by collections of variables of very different kinds (e.g. numeric, non-numeric, images, signals, videos, documents, etc.) with different degrees of imprecision and incompleteness. Many data mining and machine learning methods do not handle heterogeneity well, requiring variables of the same type, information completeness (or imputation), also assuming no imprecision. Extreme learning machines (ELM) are very interesting computational algorithms because of their structural simplicity, their good performance and their speed. Accordingly, extending their scope by making them capable of processing heterogeneous information may increase their attractiveness as a modeling tool for addressing complex problems. ELMs are discussed in the context of heterogeneous data and approaches to build ELMs capable of performing classification and regression tasks in such cases are presented. Their performance is illustrated with real world examples of classification and regression involving heterogeneous information with scalar data described by nominal, ordinal, interval, ratio, and fuzzy variables as well as with entire empirical probability distributions as predictor variables.","Artificial neural networks,
Manganese"
Combining Crop Proportion Phenology Index models with machine learning algorithms for estimating winter wheat areas,"Monitoring crop areas is a key issue in remote sensing studies. A Crop Proportion Phenology Index (CPPI) model has previously been developed for estimation of winter wheat areas. Here we test the CPPI model in different areas using remote sensing data for varied kernel functions, including linear regression (LR), Artificial Neural Network (ANN), and Support Vector Regression (SVR). The differences of the model performances among different kernel functions were found to be small for areas with simple planting structure. For areas where multiple crop types have similar phenology cycles, the non-linear model of ANN was found to perform the best. This study indicates that the CPPI model can be applied to map winter wheat distribution in areas with complex planting structures, thus it holds promises for estimating fractional areas of winter wheat areas over large geographic areas.","Agriculture,
Remote sensing,
Kernel,
Time series analysis,
MODIS,
Machine learning algorithms,
Artificial neural networks"
Study on correction of daily precipitation data of the Qinghai-Tibetan plateau with machine learning models,"The daily precipitation datasets of the Qinghai-Tibetan plateau (QTP) are mainly assimilated from remote sensing products and in-situ observations. The accuracy of those datasets needs further improvement with environmental and meteorological factors. This paper selected the related environmental and meteorological factors as input; k-Nearest Neighbor (KNN), Multivariate Adaptive Regression Splines (MARS), Support Vector Machine (SVM), Multinomial Log-linear Models (MLM) and Artificial Neural Networks (ANN) as correction models; 112 upscaled daily precipitation observations from the standard meteorological stations as ground truth to correct the commonly used ITPCAS and CMORPH daily precipitation of the QTP. Results show that the KNN model has the highest correction accuracy. The distribution of the corrected ITPCAS precipitation is nearer to the spatial pattern of the precipitation over the QTP than the corrected CMORPH precipitation. The correction accuracy is influenced by the precipitation distribution pattern of the original dataset.","Support vector machines,
Mars,
Data models,
Adaptation models,
Graphical models,
Distribution functions,
Rain"
Machine learning techniques for intrusion detection on public dataset,"The development of computer based systems expands the usage of computer based application in human life. It can be observed that illegal activities such as unauthorized data access, data theft, data modification and various other intrusion activities are rapidly growing during last decade. Hence, deployment and continuous improvement of Intrusion Detection Systems (IDS) are of paramount importance. Training, testing and evaluation of IDS with real network traffic is significant challenge, so most of IDS evaluation is based on intrusion datasets. Therefore, analysis of intrusion datasets are of paramount importance. In this paper, we evaluated Aegean Wi-Fi Intrusion Dataset (AWID) with different machine learning techniques. Feature reduction techniques such as Information Gain (IG) and Chi-Squared statistics (CH) were applied to evaluate dataset performance with feature reduction. Results of experiments show that feature reduction can lead to better analysis in terms of accuracy, processing time and complexity. It was observed that, the maximum increment of classification accuracy with feature reduction from 110 to 41 is 2.4%.","Intrusion detection,
Vegetation,
IEEE 802.11 Standard,
Training,
Testing,
Computers,
Labeling"
Research on Crime Degree of Internet Speech Based on Machine Learning and Dictionary,"Intelligent security technology provides important clues and basis for case detection, however, the traditional intelligent security technology can not alarm a crime's happening in advance. By researching the relationship between criminal psychology and speech feature, we proposed a crime degree theory of internet speech which can alarm a crime's happening in advance by taking advantage of the internet speech. The theory which was based on criminal psychology, using multiple analytical methods such as machine learning and emotional dictionary, establishing the mathematical model and theoretical framework, gave the preliminary implementation method of the real system. Experimental result has shown that the pre-alarming system based on our theory has good pre-alarming capability against crime.","Speech,
Internet,
Dictionaries,
Psychology,
Speech recognition,
Bayes methods,
Semantics"
Learning Boltzmann machine with EM-like method,"We propose an expectation-maximization-like(EM-like) method to train Boltzmann machine with unconstrained connectivity. It adopts Monte Carlo approximation in the E-step, and replaces the intractable likelihood objective with efficiently computed objectives or directly approximates the gradient of likelihood objective in the M-step. The EM-like method is a modification of alternating minimization. We prove that EM-like method will be the exactly same with contrastive divergence in restricted Boltzmann machine if the M-step of this method adopts special approximation. We also propose a new measure to assess the performance of Boltzmann machine as generative models of data, and its computational complexity is O(Rmn). Finally, we demonstrate the performance of EM-like method using numerical experiments.","Minimization,
Monte Carlo methods,
Approximation algorithms,
Training,
Data models,
Estimation,
Computational modeling"
Comparative analysis of features based machine learning approaches for phishing detection,"Machine learning based anti-phishing techniques are based on various features extracted from different sources. These features differentiate a phishing website from a legitimate one. Features are taken from various sources like URL, page content, search engine, digital certificate, website traffic, etc, of a website to detect it as a phishing or non-phishing. The websites are declared as phishing sites if the heuristic design of the websites matches with the predefined rules. The accuracy of the anti-phishing solution depends on features set, training data and machine learning algorithm. This paper presents a comprehensive analysis of Phishing attacks, their exploitation, some of the recent machine learning based approaches for phishing detection and their comparative study. It provides a better understanding of the phishing problem, current solution space in machine learning domain, and scope of future research to deal with Phishing attacks efficiently using machine learning based approaches.","Uniform resource locators,
Feature extraction,
Machine learning algorithms,
Data mining,
Support vector machines,
Electronic mail,
Online banking"
A review of supervised machine learning algorithms,"Supervised machine learning is the construction of algorithms that are able to produce general patterns and hypotheses by using externally supplied instances to predict the fate of future instances. Supervised machine learning classification algorithms aim at categorizing data from prior information. Classification is carried out very frequently in data science problems. Various successful techniques have been proposed to solve such problems viz. Rule-based techniques, Logic-based techniques, Instance-based techniques, stochastic techniques. This paper discusses the efficacy of supervised machine learning algorithms in terms of the accuracy, speed of learning, complexity and risk of over fitting measures. The main objective of this paper is to provide a general comparison with state of art machine learning algorithms.","Decision support systems,
Classification algorithms,
Handheld computers,
Machine learning algorithms,
Artificial neural networks,
Support vector machines,
Roads"
Comparative analysis of machine learning algorithms in OCR,"The purpose of this research is to implement different machine learning algorithms in optical character recognition. The algorithms used the pixel density of image of handwritten digits as an input. The algorithms when implemented produced the value of labels of each handwritten digit. The value of labels generated, was then matched with the actual value of labels of the MNIST handwritten digits to determine the accuracy of an algorithm. Machine learning algorithms that have been used for this research are Naïve Bayes, Naïve Bayes with Laplace Smoothing, Sequential Minimal Optimization, C4.5 decision trees and Logistic Regression. The accuracy for each of the algorithm was calculated and Logistic regression was found out to be the most accurate of them all for handwritten digits.","Logistics,
Algorithm design and analysis,
Classification algorithms,
Machine learning algorithms,
Optical character recognition software,
Training,
Decision trees"
Machine learning techniques for effective text analysis of social network E-health data,"The World Wide Web has evolved very drastically and the recent advent in social networking and media rich websites has necessitated analysis of the social networks and opinions expressed by various users in media like blogs, tweets, and website pages alike. While a lot of previous researches have been done on product / CRM domain, relatively few research has been done on the network analysis of social media with a focus on extraction and opinion of e-Health data i.e. data expressed in various blogs or website pages by users regarding various aspects of their health. While marketing and medical companies can leverage this data to augment customer reach and thus further their profits, sentiment analysis and network analysis on the data can help various organizations understand health patterns, address people's concerns or predict outbreak patterns in case of contagious diseases. This paper outlines the machine learning techniques which are helpful in the analysis of medical domain data from Social networks. In this paper we compare the existing techniques of machine learning, discuss the advantages and challenges encompassing the perspectives involving the use of text mining methods for applications in E-health and medicine. We evaluate medical domain data classification efficiency using various metrics like ROC, AUC implemented via R language packages. We need to infer which machine learning technique is more relevant for processing data from social networks having medical terms in it.","Decision support systems,
Handheld computers"
Comparison of Different Machine Learning Approaches to Predict Small for Gestational Age Infants,"Diagnosing infants who are small for gestational age (SGA) at early stages could help physicians to introduce interventions for SGA infants earlier. Machine learning (ML) is envisioned as a tool to identify SGA infants. However, ML has not been widely studied in this field. To develop effective SGA prediction models, we conducted four groups of experiments that considered basic ML methods, imbalanced data, feature selection and the time characteristics of variables, respectively. Infants with SGA data collected from 2010 to 2013 with gestational weeks between 24 and 42 were detected. Support vector machine (SVM), random forest (RF), logistic regression (LR) and Sparse LR models were trained on 10-fold cross validation. Precision and the area under the curve (AUC) of the receiver operator characteristic curve were evaluated. For each group, the performance of SVM and Sparse LR was similarly well. LR without any sparsity penalties performed worst, possibly caused by the overfitting problem. With the combination of handling imbalanced data and feature selection, the RF ensemble classifier performed best, which even obtained the highest AUC value (0.8547) with the help of expert knowledge. In other cases, RF performed worse than Sparse LR and SVM, possibly because of fully grown trees.","Support vector machines,
Pediatrics,
Predictive models,
Training,
Radio frequency,
Vegetation,
Big data"
Construction of gazetteers from geo big data using machine learning technique on Hadoop,"Most gazetteers have been built and maintained for the purpose of visualizing geographical location on the Geographical Information system (GIS) client. The advent of big data allows us to construct gazetteers by directly mining rich volunteered information from the web. In this, we propose a technique for extracting location based spatial information from the web documents and media services like flickr, twitter, facebook for construction of gazetteers. To achieve this, we need to search the web for existing data pertaining location. A web crawler (Google search engine) generates the web pages based on the location keyword given by the user and maintaining the index of the web pages and the proposed system passes it to the Hadoop environment. For further simplification, the name node transfers the index group of web pages to different data nodes for extraction of spatial information from the dynamic web documents that we gather using machine learning process. Each data node is then utilized for the generation of a common template. The common template allows the extraction of location based spatial information from the dynamic web documents and media services. Resultant information from the data node is further merged using map reduce algorithms and the Hadoop Distributed File System (HDFS) is produced which is then converted to Geo-Java Script Object Notation (JSON) format, thus aiding in the task of visualizing the extracted information on the GIS client.","Web pages,
Spatial databases,
Data mining,
Data visualization,
Crawlers,
Hospitals,
File systems"
Comparison of machine learning algorithms for classification of Penaeid prawn species,"Classification of Penaeid prawn species is an important research problem in the area of aquaculture. In literature, many ML algorithms have been proposed for classification. In this paper, performance and usability of penaied prawn species are compared using K-Nearest Neighbourhood (KNN) algorithm, Support Vector Machines (SVM) and Back Propagation Neural Networks (BPNN). For experimental evaluation a dataset containing 100 samples of each species are classified. Also the classification accuracy of each species is analyzed using the above mentioned three classifiers. Experimental results indicate that the SVM out performs KNN classifier and ANN classifiers and may potentially fill gap for the current use or for future.","Handheld computers,
Decision support systems,
Support vector machines"
Centralized and Localized Data Congestion Control Strategy for Vehicular Ad Hoc Networks Using a Machine Learning Clustering Algorithm,"In an urban environment, intersections are critical locations in terms of road crashes and number of killed or injured people. Vehicular ad hoc networks (VANETs) can help reduce the traffic collisions at intersections by sending warning messages to the vehicles. However, the performance of VANETs should be enhanced to guarantee delivery of the messages, particularly safety messages to the destination. Data congestion control is an efficient way to decrease packet loss and delay and increase the reliability of VANETs. In this paper, a centralized and localized data congestion control strategy is proposed to control data congestion using roadside units (RSUs) at intersections. The proposed strategy consists of three units for detecting congestion, clustering messages, and controlling data congestion. In this strategy, the channel usage level is measured to detect data congestion in the channels. The messages are gathered, filtered, and then clustered by machine learning algorithms. K-means algorithm clusters the messages based on message size, validity of messages, and type of messages. The data congestion control unit determines appropriate values of transmission range and rate, contention window size, and arbitration interframe spacing for each cluster. Finally, RSUs at the intersections send the determined communication parameters to the vehicles stopped before the red traffic lights to reduce communication collisions. Simulation results show that the proposed strategy significantly improves the delay, throughput, and packet loss ratio in comparison with other congestion control strategies using the proposed congestion control strategy.","Vehicles,
Delays,
Roads,
Safety,
Reliability,
Quality of service,
Machine learning algorithms"
Performance of Machine Learning Algorithms for Class-Imbalanced Process Fault Detection Problems,"In recent years, the semiconductor manufacturing industry has recognized class imbalance as a major impediment to the development of high-performance fault detection (FD) models. Class imbalance refers to skews in class distribution in which normal wafer samples are considerably more abundant than fault samples. In such a situation, standard machine learning algorithms create FD models with classification boundaries that are biased toward majority-class data, resulting in high type II error rates. In this paper, we compare the performance of machine learning algorithms for class-imbalanced FD problems. We evaluate the performance of three sampling-based algorithms, four ensemble algorithms, four instance-based algorithms, and two support vector machine algorithms. Two experiments were conducted to compare algorithm performance using etching process data and chemical vapor deposition process data. Different data scenarios were considered by setting the imbalance ratio to three levels. The results of the experiments indicated that the instance-based algorithms presented excellent performance even when the imbalance ratio increased.","Support vector machines,
Machine learning algorithms,
Boosting,
Semiconductor device modeling,
Fault detection,
Performance evaluation"
A Behavioral Biometrics Based and Machine Learning Aided Framework for Academic Integrity in E-Assessment,"The common approaches to academic integrity in the e-learning environment are resource-intensive and require technology and/or dedicated invigilation staff to monitor assessment activities. These approaches are observational and often external to the learning spaces where the majority of the instructional content resides. Authentic assessments such as discussions, projects and portfolios may not always undergo the same scrutiny as high-stakes examinations and therefore differ in the level of identity and authorship assurance they provide. In this paper, we propose an integrated approach to enhancing academic integrity of e-assessments. The approach is based on behavioral biometrics and aided by machine-learning techniques. It provides continuous identity and authorship assurance throughout the learning activities within the existing learning space. It can be applied to measure the degree of learner collaboration with peers and interaction with the course content, concurrently verifying learner identity and validating authorship of the academic artifacts. We present the preliminary results and discuss future directions.","Biometrics (access control),
Plagiarism,
Authentication,
Collaboration,
Monitoring,
Education,
Feature extraction"
Quantum Machine Learning Based on Minimizing Kronecker-Reed-Muller Forms and Grover Search Algorithm with Hybrid Oracles,"This paper formulates the generic Machine Learning (ML) problem into finding the simplest spectral transform form (i.e. one having as many zero coefficients as possible) for an (in)complete binary function. The classical binary logic synthesis problem can be modeled to minimize a single output Boolean function with a two-level structure consisting of an exclusive-OR (EXOR) of ANDs of literals. The innovative approach in this paper is to build and simulate an accelerator that reduces learning to find the exact minimum expression of all 3n Kronecker Reed Muller (KRO) forms of a Boolean function with n input variables. This is in contrast to the previously studied quantum algorithm for the Fixed Polarity Reed-Muller forms (FPRM) which only selects from 2n possible forms. The algorithm, based on repeated application of a ternary Grover's Quantum Search algorithm, was simulated to find the minimum KRO form using a hybrid ternary/binary quantum oracle. This hybrid quantum system was simulated in Matlab and proved to be correct. The method can be also used as a future Quantum EDA Tool for exact minimization of AND/EXOR circuits, including reversible and quantum circuits.","Quantum computing,
Transforms,
Logic gates,
Machine learning algorithms,
Boolean functions,
Quantum mechanics,
Computers"
Prognosis of NBTI aging using a machine learning scheme,"Circuit aging is an important failure mechanism in nanoscale designs and is a growing concern for the reliability of future systems. Aging results in circuit performance degradation over time and the ultimate circuit failure. Among aging mechanisms, Negative-Bias Temperature Instability (NBTI) is the main limiting factor of circuits lifetime. Estimating the effect of aging-related degradation, before it actually occurs, is crucial for developing aging prevention/mitigations actions to avoid circuit failures. In this paper, we propose a general-purpose IC aging prognosis approach by considering a comprehensive set of IC operating conditions including workload, usage time and operating temperature. In addition, our model considers process variation by using a calibration technique applied at the time of manufacturing. Experimental results confirms that our model is able to accurately predict the NBTI-related path delay degradation under various operating conditions. The proposed model is robust to process variations.","Aging,
Delays,
Integrated circuit modeling,
Prognostics and health management,
Negative bias temperature instability,
Thermal variables control,
Degradation"
Investigating combinations of machine learning and classification techniques in a game environment,"An open world turn based monster battle game was developed in Java using the popular LibGDX game framework applying multiple machine learning algorithms for its mechanics consisting of an ID3 decision tree, perceptron, naïve Bayes classifier and A* pathfinding in an attempt to imitate `machine intelligence'. A tiled map was used as the game area containing multiple AI agents with different personalities that change depending on the difficulty level chosen. The aim of the game focuses on the player defeating each `intelligent machine' non-player character's (NPC) upon interaction with each other, when player and enemy NPC sprites meet a battle screen appears to allow the player and enemy to engage in a turn-based battle with their monsters. When a battle is lost the player loses a life, otherwise they can approach and engage other enemy agents to battle on the map, and thus the game is called `Battle Monsters'.","Games,
Training,
Training data,
Classification algorithms,
Artificial intelligence,
Neurons,
Decision trees"
A Machine Learning-Based Approach to Estimate the CPU-Burst Time for Processes in the Computational Grids,"The implementation of CPU-Scheduling algorithms such as Shortest-Job-First (SJF) and Shortest Remaining Time First (SRTF) is relying on knowing the length of the CPU-bursts for processes in the ready queue. There are several methods to predict the length of the CPU-bursts, such as exponential averaging method, however these methods may not give an accurate or reliable predicted values. In this paper, we will propose a Machine Learning (ML) based approach to estimate the length of the CPU-bursts for processes. The proposed approach aims to select the most significant attributes of the process using feature selection techniques and then predicts the CPU-burst for the process in the grid. ML techniques such as Support Vector Machine (SVM) and K-Nearest Neighbors (K-NN), Artificial Neural Networks (ANN) and Decision Trees (DT) are used to test and evaluate the proposed approach using a grid workload dataset named ""GWA-T-4 Auver Grid"". The experimental results show that there is a strength linear relationship between the process attributes and the burst CPU time. Moreover, K-NN performs better in nearly all approaches in terms of CC and RAE. Furthermore, applying attribute selection techniques improves the performance in terms of space, time and estimation.","Scheduling algorithms,
Computational modeling,
Single machine scheduling,
Testing,
Artificial intelligence"
Evaluation of machine learning algorithms for image quality assessment,"In this article, we apply different machine learning (ML) techniques for building objective models, that permit to automatically assess the image quality in agreement with human visual perception. The six ML methods proposed are discriminant analysis, k-nearest neighbors, artificial neural network, non-linear regression, decision tree and fuzzy logic. Both the stability and the robustness of designed models are evaluated by using Monte-Carlo cross-validation approach (MCCV). The simulation results demonstrate that fuzzy logic model provides the best prediction accuracy.","Image quality,
Predictive models,
Measurement,
Correlation,
Fuzzy logic,
Training,
Neural networks"
Hardware Trojans classification for gate-level netlists based on machine learning,"Recently, we face a serious risk that malicious third-party vendors can very easily insert hardware Trojans into their IC products but it is very difficult to analyze huge and complex ICs. In this paper, we propose a hardware-Trojan classification method to identify hardware-Trojan infected nets (or Trojan nets) using a support vector machine (SVM). Firstly, we extract the five hardware-Trojan features in each net in a netlist. Secondly, since we cannot effectively give the simple and fixed threshold values to them to detect hardware Trojans, we represent them to be a five-dimensional vector and learn them by using SVM. Finally, we can successfully classify a set of all the nets in an unknown netlist into Trojan ones and normal ones based on the learned SVM classifier. We have applied our SVM-based hardware-Trojan classification method to Trust-HUB benchmarks and the results demonstrate that our method can much increase the true positive rate compared to the existing state-of-the-art results in most of the cases. In some cases, our method can achieve the true positive rate of 100%, which shows that all the Trojan nets in a netlist are completely detected by our method.","Trojan horses,
Feature extraction,
Hardware,
Logic gates,
Support vector machines,
Integrated circuits,
Benchmark testing"
Data-driven machine learning approach for predicting volumetric moisture content of concrete using resistance sensor measurements,"In sewerage industry, hydrogen sulphide induced corrosion of reinforced concretes is a global problem. To achieve a comprehensive knowledge of the propagation of concrete corrosion, it is vital to monitor the critical factors such as moisture. In this context, this paper investigates the use of resistance measuring and processing for estimating the concrete moisture content. The behavior of concrete moisture with resistance and surface temperature are studied and the effects of pH concentration on concrete are analyzed. Gaussian Process regression modeling is carried out to predict volumetric moisture content of concrete, where the results from experimental data are used to train the prediction model.","Concrete,
Moisture,
Temperature measurement,
Resistance,
Corrosion,
Surface treatment,
Temperature sensors"
Machine Learning with Sensitivity Analysis to Determine Key Factors Contributing to Energy Consumption in Cloud Data Centers,"Machine learning (ML) approach to modeling and predicting real-world dynamic system behaviours has received widespread research interest. While ML capability in approximating any nonlinear or complex system is promising, it is often a black-box approach, which lacks the physical meanings of the actual system structure and its parameters, as well as their impacts on the system. This paper establishes a model to provide explanation on how system parameters affect its output(s), as such knowledge would lead to potential useful, interesting and novel information. The paper builds on our previous work in ML, and also combines an evolutionary artificial neural networks with sensitivity analysis to extract and validate key factors affecting the cloud data center energy performance. This provides an opportunity for software analysts to design and develop energy-aware applications and for Hadoop administrator to optimize the Hadoop infrastructure by having Big Data partitioned in bigger chunks and shortening the time to complete MapReduce jobs.","Artificial neural networks,
Cloud computing,
Servers,
Temperature sensors,
Computational modeling,
Biological cells,
Data models"
Recent machine learning advancements in sensor-based mobility analysis: Deep learning for Parkinson's disease assessment,"The development of wearable sensors has opened the door for long-term assessment of movement disorders. However, there is still a need for developing methods suitable to monitor motor symptoms in and outside the clinic. The purpose of this paper was to investigate deep learning as a method for this monitoring. Deep learning recently broke records in speech and image classification, but it has not been fully investigated as a potential approach to analyze wearable sensor data. We collected data from ten patients with idiopathic Parkinson's disease using inertial measurement units. Several motor tasks were expert-labeled and used for classification. We specifically focused on the detection of bradykinesia. For this, we compared standard machine learning pipelines with deep learning based on convolutional neural networks. Our results showed that deep learning outperformed other state-of-the-art machine learning algorithms by at least 4.6 % in terms of classification rate. We contribute a discussion of the advantages and disadvantages of deep learning for sensor-based movement assessment and conclude that deep learning is a promising method for this field.","Machine learning,
Standards,
Pipelines,
Feature extraction,
Convolution,
Biomedical monitoring,
Training"
Implementation of a smartphone as a wireless gyroscope platform for quantifying reduced arm swing in hemiplegie gait with machine learning classification by multilayer perceptron neural network,"Natural gait consists of synchronous and rhythmic patterns for both the lower and upper limb. People with hemiplegia can experience reduced arm swing, which can negatively impact the quality of gait. Wearable and wireless sensors, such as through a smartphone, have demonstrated the ability to quantify various features of gait. With a software application the smartphone (iPhone) can function as a wireless gyroscope platform capable of conveying a gyroscope signal recording as an email attachment by wireless connectivity to the Internet. The gyroscope signal recordings of the affected hemiplegic arm with reduced arm swing arm and the unaffected arm are post-processed into a feature set for machine learning. Using a multilayer perceptron neural network a considerable degree of classification accuracy is attained to distinguish between the affected hemiplegic arm with reduced arm swing arm and the unaffected arm.","Gyroscopes,
Wireless communication,
Wireless sensor networks,
Multilayer perceptrons,
Biological neural networks,
Sensors,
Software"
Fall detection algorithms for real-world falls harvested from lumbar sensors in the elderly population: A machine learning approach,"Automatic fall detection will promote independent living and reduce the consequences of falls in the elderly by ensuring people can confidently live safely at home for linger. In laboratory studies inertial sensor technology has been shown capable of distinguishing falls from normal activities. However less than 7% of fall-detection algorithm studies have used fall data recorded from elderly people in real life. The FARSEEING project has compiled a database of real life falls from elderly people, to gain new knowledge about fall events and to develop fall detection algorithms to combat the problems associated with falls. We have extracted 12 different kinematic, temporal and kinetic related features from a data-set of 89 real-world falls and 368 activities of daily living. Using the extracted features we applied machine learning techniques and produced a selection of algorithms based on different feature combinations. The best algorithm employs 10 different features and produced a sensitivity of 0.88 and a specificity of 0.87 in classifying falls correctly. This algorithm can be used distinguish real-world falls from normal activities of daily living in a sensor consisting of a tri-axial accelerometer and tri-axial gyroscope located at L5.","Acceleration,
Sensors,
Feature extraction,
Machine learning algorithms,
Accelerometers,
Gyroscopes,
Angular velocity"
A wearable computing platform for developing cloud-based machine learning models for health monitoring applications,"Wearable sensors have the potential to enable clinical-grade ambulatory health monitoring outside the clinic. Technological advances have enabled development of devices that can measure vital signs with great precision and significant progress has been made towards extracting clinically meaningful information from these devices in research studies. However, translating measurement accuracies achieved in the controlled settings such as the lab and clinic to unconstrained environments such as the home remains a challenge. In this paper, we present a novel wearable computing platform for unobtrusive collection of labeled datasets and a new paradigm for continuous development, deployment and evaluation of machine learning models to ensure robust model performance as we transition from the lab to home. Using this system, we train activity classification models across two studies and track changes in model performance as we go from constrained to unconstrained settings.","Legged locomotion,
Data models,
Training,
Testing,
Pipelines,
Feature extraction,
Biological system modeling"
A machine learning framework for auto classification of imaging system exams in hospital setting for utilization optimization,"In clinical environment, Interventional X-Ray (IXR) system is used on various anatomies and for various types of the procedures. It is important to classify correctly each exam of IXR system into respective procedures and/or assign to correct anatomy. This classification enhances productivity of the system in terms of better scheduling of the Cath lab, also provides means to perform device usage/revenue forecast of the system by hospital management and focus on targeted treatment planning for a disease/anatomy. Although it may appear classification of each exam into respective procedure/anatomy a simple task. However, in real-life hospital settings, it is well-known that same system settings are used to perform different types of procedures. Though, such usage leads to under-utilization of the system. In this work, a method is developed to classify exams into respective anatomical type by applying machine-learning techniques (SVM, KNN and decision trees) on log information of the systems. The classification result is promising with accuracy of greater than 90%.",
Identification of promising research directions using machine learning aided medical literature analysis,"The rapidly expanding corpus of medical research literature presents major challenges in the understanding of previous work, the extraction of maximum information from collected data, and the identification of promising research directions. We present a case for the use of advanced machine learning techniques as an aide in this task and introduce a novel methodology that is shown to be capable of extracting meaningful information from large longitudinal corpora, and of tracking complex temporal changes within it.","Vocabulary,
Probability distribution,
Data mining,
Bayes methods,
Data models,
Mixture models,
Evolution (biology)"
A machine learning based approach for identifying traumatic brain injury patients for whom a head CT scan can be avoided,"Head CT scan is more often used to evaluate patients with suspected traumatic brain injury (TBI). However, the use of head CT scans in evaluating TBI is costly with low value endeavor. In this paper, we propose a new algorithm and a set of features to help clinicians determine which patients evaluated for TBI need a head CT scan using cost sensitive random forest (CSRF) classifier. We show that random forest (RF) and CSRF are useful methods for identifying patients likely to have a positive head CT scan. The proposed algorithm has superior diagnostic accuracy in comparison to the Canadian head CT algorithm, which is currently the most accurate and widely used algorithm for determining which TBI patients need a head CT scan. In the highest sensitivity (i.e. 100%), our method outperforms the Canadian rule in terms of specificity, accuracy and area under ROC curve using cost sensitive classifier. Clinical implementation of this algorithm can help decrease financial costs associated with Emergency Department evaluations for traumatic brain injury, while decreasing patient exposure to avoidable ionizing radiation.","Computed tomography,
Head,
Magnetic heads,
Brain injuries,
Radio frequency,
Sensitivity"
A hybrid rule and machine learning based generic alerting platform for smart environments,"Existing smart environment based alert solutions have adopted a relatively complex and tailored approach to supporting individuals. These solutions have involved sensor based monitoring, activity recognition and assistance provisioning. Traditionally they have suffered from a number of issues, rooted in scalability and performance, associated with complex activity recognition processes. This paper introduces a generic approach to realizing an alerting platform within a smart environment. The core concept of this approach is presented and placed within the context of related work. A description of the approach is provided, followed by an evaluation. This evaluation shows the approach offers reasonable accuracy, future work will increase accuracy.","Monitoring,
Temperature sensors,
Temperature measurement,
Computer vision"
Efficient compressive sensing of ECG segments based on machine learning for QRS-based arrhythmia detection,"A novel method for efficient telemonitoring of arrhythmia based on using QRS complexes is proposed. Two features, namely, sum of absolute differences (SAD) and maximum of absolute differences (MAD) are efficiently computed for each ECG segment in the bio-sensor. The computed features can be transmitted from the bio-sensor using wireless channel, and they can be used in the receiver for determining the absence of QRS complex in the segment. By avoiding computationally expensive signal reconstruction for the ECG segments without QRS complex, it is shown, using simulation results, that computation time can be reduced by approximately 7.4% for long-term telemonitoring of QRS-based arrhythmia. Detection of the absence of QRS complex can be carried out in around 7 milliseconds in a standard laptop computer with 2.2GHz processor and 8GB RAM.","Electrocardiography,
Support vector machines,
Signal reconstruction,
Compressed sensing,
Biomedical measurement,
Feature extraction,
Memory management"
Machine learning anomaly detection in large systems,"We have a need for methods to efficiently determine the health of a system. Diagnostics and prognostics determine system heath through analysis of data from sensors. Anomalies in the data can help us determine if there is a failure or a pending failure. There are common statistical methods to detect anomalies in individual measurements. For systems with many measurements, the anomalies may occur as specific combinations of values. Large systems have various associated states and modes which define the valid measurements. The amount of data to analyze grows very quickly as the system becomes more complex. In recent years techniques have been developed to address large data analysis. Machine Learning encompasses a broad selection of tools to optimize a statistical model of the data. These tools include supervised learning techniques, such as linear regression and logistic regression, in which training data exists to tune the model. Unsupervised learning, such as clustering, is used to explore data which does not have a defined output label associated with inputs data. Standard approaches to training supervised learning systems require a large sample of positive and negative outcome data. Some uses of machine learning involve data where there are very few cases of negative outcomes. There are machine learning algorithms defined as Anomaly Detection which are designed to deal with this type of data. Simple algorithms include Gaussian Distribution Analysis, which assumes independence in distributions of data. Large Systems with anomalies defined in the dependent combinations of data require either a manual creation of combinations of independent variables, or Multivariate Gaussian Distribution Analysis, which does not scale well for large systems. A further complication is the mixture of linear and discrete data. Neural Networks are a type of learning system which has been applied to each of the individual needs addressed above. This paper describes an approach to anomaly detection using neural networks for the specific problems in large systems to efficiently determine system health.","Sensors,
Correlation,
Data models,
Supervised learning,
Training data,
Neural networks,
Performance evaluation"
A machine learning approach for predicting bank credit worthiness,"Machine learning is an emerging technique for building analytic models for machines to ""learn"" from data and be able to do predictive analysis. The ability of machines to ""learn"" and do predictive analysis is very important in this era of big data and it has a wide range of application areas. For instance, banks and financial institutions are sometimes faced with the challenge of what risk factors to consider when advancing credit/loans to customers. For several features/attributes of the customers are normally taken into consideration, but most of these features have little predictive effect on the credit worthiness or otherwise of the customer. Furthermore, a robust and effective automated bank credit risk score that can aid in the prediction of customer credit worthiness very accurately is still a major challenge facing many banks. In this paper, we examine a real bank credit data and conduct several machine learning algorithms on the data for comparative analysis and to choose which algorithms are the best fit for learning bank credit data. The algorithms gave over 80% accuracy in prediction. Furthermore, the most important features that determine whether a customer will default or otherwise in paying his/her credit the next month are extracted from a total of 23 features. We then applied these most important features on some selected machine learning algorithms and compare their predictive accuracy with the other algorithms that used all the 23 features. The results show no significant di erence, signifying that these features can accurately determine the credit worthiness of the customers. Finally, we formulate a predictive model using the most important features to predict the credit worthiness of a given customer.",
Features selection for building an early diagnosis machine learning model for Parkinson's disease,"In this work, different approaches were evaluated to optimize building machine learning classification models for the early diagnosis of the Parkinson disease. The goal was to sort the medical measurements and select the most relevant parameters to build a faster and more accurate model using feature selection techniques. Decreasing the number of features to build a model could lead to more efficient machine learning algorithm and help doctors to focus on what are the most important measurements to take into account. For feature selection we compared the Filter and Wrapper techniques. Then we selected a good machine learning algorithm to detect which technique could help us by calculate the crossover scores for each technique. This research is based on a dataset which was created by Athanasius Tsanas and Max Little of the University of Oxford, in collaboration with 10 medical centers in the US and Intel Corporation. This target of these medical measurements is to find the Unified Parkinson's disease rating scale (UPDRS) which is the most commonly used scale for clinical studies of Parkinson's disease.","Parkinson's disease,
Biomedical measurement,
Machine learning algorithms,
Support vector machines,
Medical diagnostic imaging,
Filtering algorithms"
Dermatological disease detection using image processing and machine learning,"Dermatological diseases are the most prevalent diseases worldwide. Despite being common, its diagnosis is extremely difficult and requires extensive experience in the domain. In this research paper, we provide an approach to detect various kinds of these diseases. We use a dual stage approach which effectively combines Computer Vision and Machine Learning on clinically evaluated histopathological attributes to accurately identify the disease. In the first stage, the image of the skin disease is subject to various kinds of pre-processing techniques followed by feature extraction. The second stage involves the use of Machine learning algorithms to identify diseases based on the histopathological attributes observed on analysing of the skin. Upon training and testing for the six diseases, the system produced an accuracy of up to 95 percent.","Feature extraction,
Diseases,
Skin,
Image color analysis,
Computer vision,
Artificial neural networks,
Computational modeling"
Hunting Killer Tasks for Cloud System through Machine Learning: A Google Cluster Case Study,"Motivated by frequent failures in cloud computing systems, we analyze failure frequency and failure continuity of tasks from the Google cloud cluster, and find what we call killer tasks that suffer from frequent failures and repeated rescheduling. Killer tasks cause unnecessary resource wasting and significant increase of scheduling workloads, which can be a big concern in cloud systems. We aim to recognize killer tasks at the very early stage of their occurrence so that they can be addressed proactively instead of being rescheduled repeatedly, so as to promote reliability and save resources. To recognize killer tasks from a large amount of tasks in real time is really challenging. In this paper, we first investigate characteristics and behavior patterns of killer tasks and then develop two machine learning based methods, K-HUNTER and C-HUNTER, for online recognition of killer tasks. The empirical results show that our approach performs at 97% of precision in recognizing killer tasks with an 89% timing advance and 88% of resource saving for the cloud system on average.","Cloud computing,
Google,
Time series analysis,
Predictive models,
Processor scheduling,
Pattern recognition,
Servers"
Machine learning techniques to build geometrical transformations for object matching a review,"Image matching or object matching is one of the cutting edge research fields in machine learning or computer vision domain. Whereas aim of image matching techniques is to build geometrical transformations over source image and target image, videos, real time moving object to extract similarity measure. Several research methods devised for image matching but efficiency of techniques is bounded with various parameters such as image rotation, speed, blurriness, quality etc., these parameters are important while understanding and devising robust image matching techniques. Study and analysis of image matching parameters is highly important while learning and understanding, predicting performance when time is a limiting factor for implementation. Several approaches have been presented to achieve efficiency over real time object matching. Now in this paper we have presented fundamentals of object matching based on geometrical transformation to match object. Comprehensive review of existing methods with analysis of image matching parameters is presented to determine the limitations of existing methods. This review also addresses comparative study of existing image matching techniques to generalize criteria for design of robust technique.","Feature extraction,
Image matching,
Computer vision,
Image edge detection,
Image registration,
Image color analysis,
Communication systems"
Proposed retinal abnormality detection and classification approach: Computer aided detection for diabetic retinopathy by machine learning approaches,"Diabetes occurs when the pancreas fails to secrete enough insulin, slowly affecting the retina of the human eye, leading to diabetic retinopathy. The blood vessels in the retina get altered and have abnormality. Exudates are secreted, micro-aneurysms and haemorrhages occur in the retina. The appearance of these features represents the degree of severity of the disease. Early detection of diabetic retinopathy plays a major role in the success of such disease treatment. The main challenge is to extract exudates which are similar in colour property and size of the optic disk, and then micro-aneurysms are similar in colour and proximity with blood vessels. The main objective of the paper is to develop a computer aided detection system to find the abnormality of retinal imaging and detects the presence of abnormality features from retinal fundus images. There is few existing research works have been undergone by applying machine learning techniques, but existing approaches have not achieved a good accuracy of detection and they have not yielded successful performance in different datasets. The proposed methodology is to enhance the image and filter the noise, detect blood vessel and identify the optic disc, extract the exudates and micro aneurysms, extract the features and classify different stages of diabetic retinopathy into mild, moderate, severe non-proliferative diabetic retinopathy (NPDR) and proliferative Diabetic retinopathy (PDR) by using proposed machine learning methods. The expected output of proposed work in this paper will be a preliminary design and pilot prototype development.","Retina,
Diabetes,
Retinopathy,
Biomedical imaging,
Blood vessels,
Feature extraction"
Data Integration Using Machine Learning,"Today, enterprise integration and cross-enterprise collaboration is becoming evermore important. The Internet of things, digitization and globalization are pushing continuous growth in the integration market. However, setting up integration systems today is still largely a manual endeavor. Most probably, future integration will need to leverage more automation in order to keep up with demand. This paper presents a first version of a system that uses tools from artificial intelligence and machine learning to ease the integration of information systems, aiming to automate parts of it. Three models are presented and evaluated for precision and recall using data from real, past, integration projects. The results show that it is possible to obtain F0.5 scores in the order of 80% for models trained on a particular kind of data, and in the order of 60%-70% for less specific models trained on a several kinds of data. Such models would be valuable enablers for integration brokers to keep up with demand, and obtain a competitive advantage. Future work includes fusing the results from the different models, and enabling continuous learning from an operational production system.","Data models,
Computational modeling,
Training data,
Biological system modeling,
Semantics,
Computer science,
Data integration"
Training restricted Boltzmann Machine with dynamic learning rate,"Restricted Boltzmann Machine (RBM) has been successfully applied to many different machine learning and pattern recognition problems. Usually, fixed learning rate (FLR) is used for training RBM. However, the reconstruction error (RCERR) with FLR may not be declined each iteration, which will result in a slow convergence speed. In this paper, we propose a method to dynamically choose the learning rate by reducing RCERR properly. The experiments on MNIST database and Caltech 101 Silhouettes database show the RBMs trained with dynamic learning rate (DLR) are better than that trained with FLR in classification accuracy and stability. It indicates DLR may be more suitable for training RBM.","Training,
Databases,
Neural networks,
Feature extraction,
Heuristic algorithms,
Bars,
Training data"
Symbolic execution of complex program driven by machine learning based constraint solving,"Symbolic execution is a widely-used program analysis technique. It collects and solves path conditions to guide the program traversing. However, due to the limitation of the current constraint solvers, it is difficult to apply symbolic execution on programs with complex path conditions, like nonlinear constraints and function calls. In this paper, we propose a new symbolic execution tool MLB to handle such problem. Instead of relying on the classical constraint solving, in MLB, the feasibility problems of the path conditions are transformed into optimization problems, by minimizing some dissatisfaction degree. The optimization problems are then handled by the underlying optimization solver through machine learning guided sampling and validation. MLB is implemented on the basis of Symbolic PathFinder and encodes not only the simple linear path conditions, but also nonlinear arithmetic operations, and even black-box function calls of library methods, into symbolic path conditions. Experiment results show that MLB can achieve much better coverage on complex real-world programs.","Optimization,
Libraries,
Engines,
Machine learning algorithms,
Java,
Transforms,
Algorithm design and analysis"
Machine learning application in MOOCs: Dropout prediction,"Massive Open Online Course(MOOC) is undergoing explosive growth recently, both the number of MOOC platforms and courses are increasing dramatically during these years. One of the major concerns in MOOC is high dropout rate, we study dropout prediction in MOOCs, using student's learning activities data in a period of time to measure how likely students would drop out in next couple of days. We collect 39 courses data from XuetangX platform, which is based on the open source Edx platform. Using supervised classification approach in the machine learning field, we achieve 89% accuracy in dropout prediction task with gradient boosting decision tree model. We describe details in drop out prediction framework, including data extraction from Edx platform, data preprocessing, feature engineering and performance test on several supervised classification models.","Predictive models,
Data mining,
Feature extraction,
Education,
Data preprocessing,
Data models,
Context"
"Linear, machine learning and probabilistic approaches for time series analysis","In this paper we study different approaches for time series modeling. The forecasting approaches using linear models, ARIMA algorithm, XGBoost machine learning algorithm are described. Results of different model combinations are shown. For probabilistic modeling the approaches using copulas and Bayesian inference are considered.","Time series analysis,
Forecasting,
Bayes methods,
Predictive models,
Linear regression,
Boosting,
Probabilistic logic"
Improving DRAM Fault Characterization through Machine Learning,"As high-performance computing systems continue to grow in scale and complexity, the study of faults and errors is critical to the design of future systems and mitigation schemes. Fault modes in system DRAM are a frequently-investigated key aspect of memory reliability. While current schemes require offline analysis for proper classification, current state-of-the-art mitigation techniques require accurate online prediction for optimal performance. In this work, we explore the predictive performance of an online machine learning-based approach in classifying DRAM fault modes from two leadership-class supercomputing facilities. Our results compare the predictive performance of this online approach with the current rule-based approach based on expert knowledge, finding a 12% predictive performance improvement. We also investigate the universality of our classifiers by evaluating predictive performance using training data from disparate computing systems to achieve a 7% improvement in predictive performance. Our work provides a critical analysis of this online learning technique and can benefit system designers to help inform best practices for dealing with reliability on future systems.","Random access memory,
Machine learning algorithms,
Supercomputers,
Prediction algorithms,
Reliability engineering,
Error correction codes"
Machine Learning-Based Image Classification for Wireless Camera Sensor Networks,"Wireless sensor networks, with their capability to capture physical phenomena at micro-scale, have changed how we collect and analyze data from the real world. Especially, using low-power cameras we can design interesting applications that provide us with previously difficult-to-capture information from the real-world. While cameras hold privacy threats in human-residing environments, they can be actively used in natural world analyzing applications. However, the disadvantage of using cameras is that the samples themselves (e.g., images) have large sizes, forcing the system to exchange more data, and in turn, decreasing energy efficiency. Given that camera-based sensor networks are usually deployed in remote locations, the lifetime of individual devices become a major concern. In this work, we target to utilize machine-learning algorithms to characterize the context of images captured from a real-world deployments. Specifically, using images from the James Reserve bird nest deployment [1], we utilize and optimize machine learning algorithms to operate on embedded low-power, resource-limited platforms. Using both a systematic and algorithmic approach, our proposed systems architecture and algorithms hold the potential to reduce the amount of data to be transmitted by as much a eight-fold.","Cameras,
Birds,
Machine learning algorithms,
Systems architecture,
Servers,
Histograms,
Wireless sensor networks"
InfluenceRank: A machine learning approach to measure influence of Twitter users,"We devise a system for measuring influence of Twitter users, which we call InfluenceRank, based on certain features extracted from their Twitter profiles and tweets authored over the duration of two months. As in the real world, influence of a user on social media may be judged by the engagement they drive through the content they publish. For a tweet, engagement can be most obviously measured by the number of retweets (RTs), favourites and replies it gets. Our system comprises of a regression based machine learning approach with InfluenceRank as the predictor variable against the set of our proposed features. The regression model has shown reasonable accuracy despite being fit on limited data.","Twitter,
Feature extraction,
Measurement,
Predictive models,
Data models,
Market research"
Machine learning techniques with probability vector for cooperative spectrum sensing in cognitive radio networks,"We study cooperative spectrum sensing in cognitive radio networks (CRN) using machine learning techniques in this paper. A low-dimensional probability vector is proposed as the feature vector for machine learning based classification, instead of the N-dimensional energy vector in a CRN with a single primary user (PU) and N secondary users (SUs). This proposed method down-converts a high-dimensional feature vector to a constant two-dimensional feature vector for machine learning techniques while keeping the same spectrum sensing performance if not better. Due to its lower dimension, the probability vector based classification is capable of having a smaller training duration and a shorter classification time for testing vectors.","Sensors,
Support vector machines,
Training,
Testing,
Clustering algorithms,
Cognitive radio,
Machine learning algorithms"
A Machine Learning approach for classification of sentence polarity,"Opinion Mining is the process used to determine the attitude/opinion/emotion expressed by a person about a particular topic. Analyzing opinions is an integral part for making decisions. In the era of web, if a person wants to buy a product, he will look into the reviews and comments given by the experienced users in web. But it seems to be a tedious task to read the entire reviews available in the web. Hence people are interested in checking whether the review recommends to buy a product or not. If lot of reviews recommends to buy the product, user reach at a conclusion to buy the product, otherwise not to buy the product. In this study, Machine Learning approach is applied to the TripAdvisor dataset in order to develop an efficient review classification. For this work to be carried out, style markers are applied to each of the reviews. In the next stage, significant style markers are recognized with the help of some suitable feature selection method. Thus the reviews can be identified by developing a classifier using the style markers that help to characterize nature of reviews as positive or negative.","Feature extraction,
Data mining,
Signal processing,
Sentiment analysis,
Tagging,
Machine learning algorithms,
Syntactics"
A memristor network with coupled oscillator and crossbar towards L2-norm based machine learning,"This paper introduces a memristor-network based accelerator for L2-norm based machine learning. A coupled-memristor-oscillator network is developed for a L2-norm calculation; and a binary-memristor-crossbar network is developed to accelerate matrix-vector multiplication. As such, one can map gradient-descent (of L2-norm) based on-line machine learning on the proposed memristor-network that is composed of coupled-oscillator (to sample L2-norm) and binary-crossbar (to digitize L2-norm). Experiment results have shown that such a memristor-network based accelerator can achieve significant power reduction and runtime speed-up for both training and testing compared to the conventional CMOS-CPU based implementation.","Memristors,
Oscillators,
Training,
Resistance,
Neurons,
Data analysis"
Analysis of Spectrum Occupancy Using Machine Learning Algorithms,"In this paper, we analyze the spectrum occupancy in cognitive radio networks (CRNs) using different machine learning techniques. Both supervised techniques [naive Bayesian classifier (NBC), decision trees (DT), support vector machine (SVM), linear regression (LR)] and unsupervised algorithms [hidden Markov model (HMM)] are studied to find the best technique with the highest classification accuracy (CA). A detailed comparison of the supervised and unsupervised algorithms in terms of the computational time and the CA is performed. The classified occupancy status is further utilized to evaluate the blocking probability of secondary user for future time slots, which can be used by system designers to define spectrum-allocation and spectrum-sharing policies. Numerical results show that SVM is the best algorithm among all the supervised and unsupervised classifiers. Based on this, we proposed a new SVM algorithm by combining it with a firefly algorithm (FFA), which is shown to outperform all the other algorithms.","Hidden Markov models,
Support vector machines,
Time-frequency analysis,
Silicon,
Machine learning algorithms,
Accuracy"
LBP-HF features and machine learning applied for automated monitoring of insulators for overhead power distribution lines,"With ever-increasing awareness on quality and reliable power distribution, the research in the area of automation of distribution system has great relevance from the practical point of view. Electric power utilities throughout the world are more and more adopting computer aided control, monitoring and management of electric power distribution system to offer improved services to the consumers of electricity. The purpose of on-line condition monitoring of cables or any electrical equipment is to predict possible failures before they actually occur. With phenomenal growth of distribution network even to remote areas, the traditional methods of inspecting the lines by foot-patrolling and pole-climbing to check them in close proximity do not seem to be viable. Since the damaged insulators of the distribution system affects the performance of distribution system significantly in terms of reduction in voltage, aerial patrolling has been adopted in developed countries for the purpose of insulator monitoring. The development of an efficient and alternative method for insulator condition monitoring uses image processing and machine learning techniques and is found to be a sustainable method. This work covers automatic defect detection and classification of insulator systems of electric power lines using vision-based techniques.","Insulators,
Support vector machines,
Feature extraction,
Histograms,
Monitoring,
Condition monitoring,
Discrete Fourier transforms"
Detection and classification of diseases of Grape plant using opposite colour Local Binary Pattern feature and machine learning for automated Decision Support System,"Plant diseases cause major economic and production losses as well as curtailment in both quantity and quality of agricultural production. Now a day's, for supervising large field of crops there is been increased demand for plant leaf disease detection system. The critical issue here is to monitor the health of the plants and detection of the respective diseases. Studies show that most of the plant disease can be diagnosed from the properties of the leaf. Thus leaf based disease analysis for plants is an exciting new domain. The technique proposed for identification of plant disease through the leaf texture analysis and pattern recognition. In this work we focus on Grapes plant leaf disease detection system. The system takes a single leaf of a plant as an input and segmentation is performed after background removal. The segmented leaf image is then analyzed through high pass filter to detect the diseased part of the leaf. The segmented leaf texture is retrieved using unique fractal based texture feature. Fractal based features are locally invariant in nature and therefore provides a good texture model. The texture of every independent disease will be different. The extracted texture pattern is then classified using multiclass SVM. The work classifies focus on major diseases commonly observed in Grapes plant which are downy mildew & black rot. The proposed approach avails advice of agricultural experts easily to farmers with the accuracy of 96.6%.","Diseases,
Feature extraction,
Support vector machines,
Image color analysis,
Agriculture,
Pipelines,
Decision support systems"
Applied Difference Techniques of Machine Learning Algorithm and Web-Based Management System for Sickle Cell Disease,"Machine learning algorithm and web-based application systems have played a major role in improving the healthcare organisation in terms of continuous tele-monitoring therapy and maintaining telemedicine management systems. Currently, no intelligent system has been used in terms of managing sickle cell disease. However, this paper presents a system that facilitates a shift from manual methods to automated approach that can offer fast data collection with a reduced error rate. The system will be able to examine patient data and provide a suitable amount of Hydroxycarbamide drugs/liquid for each patient. By using a web-based system, we tend to improve patient welfare and mitigate patient illness before it gets worse over time, particularly with elderly people. The system will forward any critical concerns from the patient, it generates an automatic message to the medical doctors based on web-based platform in order to assist them with optimal decision-making. The initial case study that has been addressed in this project is how to make predictions for sickle cell disease for the amount of dose based on different architectures of machine learning in order to obtain high accuracy and performance. The most significant key for making predictions of the amount of medication is to enable healthcare organisation to provide accurate therapy recommendations based on previous data. The results using ANN showed that the proposed network produces significant improvements using the different evaluation methods. In our experiments, The MLP algorithm produced the best result in terms of the lowest error rates compare with other techniques. The Mean Square Error, Root Mean Square Error, Mean Absolute Error, and Mean Absolute Percentage Error achieved 17887.55, 133.74, 90.20 and 0.1345, respectively.","Medical diagnostic imaging,
Diseases,
Machine learning algorithms,
Monitoring,
Computer architecture,
Artificial neural networks"
Improvements of Classification Accuracy of Film Defects by Using GPU-accelerated Image Processing and Machine Learning Frameworks,"Research on image classification for natural images are quite actively worked on and recent achievements using deep learning techniques are tremendous. On the other hand, image classification techniques of defects in industrial products are mostly kept secret, partly because defective images contain very sensitive information about the products and the confidential manufacturing technologies. With the help of a leading company in a visual inspection of film defects, we investigated the effectivity of using machine learning techniques for classification of defect images. We also made use of GPU to accelerate both image processing to assist detection of defects and machine learning. We propose the combination of deep neural networks with random forest classifier for image classification of film defects, which performed better than using either of the two techniques alone.","Feature extraction,
Films,
Inspection,
Image classification,
Machine learning,
Graphics processing units,
Visualization"
Spectral anomaly detection with machine learning for wilderness search and rescue,"In wilderness search and rescue missions, unmanned aerial vehicles (UAVs) may be deployed to collect high-resolution imagery which is later reviewed by a first responder. The volume of images and the altitude from which they are taken makes manually identifying potential items of interest, like clothing or other man-made material, a difficult task. For this reason, we created a program that automatically detects unusually-colored objects in aerial imagery in order to assist responders in locating signs of missing persons. The program uses the Reed-Xiaoli (RX) spectral anomaly detection algorithm to determine which pixels in an image are anomalous and then generates an ""anomaly map"" where brighter pixels signify greater abnormality. While the RX algorithm has previously been proposed for search and rescue missions, up until now it has not been evaluated in a high-fidelity setting with real responders and real equipment. We tested the program on 150 aerial images taken over the Blanco River area in Hays County, Texas after the May 2015 flooding and demonstrated the results at a workshop on flooding hosted by Texas A&M's Center for Emergency Informatics. Early feedback from responders suggests that RX spectral anomaly detection is a valuable tool for quickly locating atypically-colored objects in images taken with UAVs for wilderness search and rescue.","Hyperspectral imaging,
Floods,
Unmanned aerial vehicles,
Image color analysis,
Detectors,
Search problems"
IoT and distributed machine learning powered optimal state recommender solution,"Recommender systems add significant benefits to E-commerce in terms of sale conversion, revenues, customer experience, loyalty and lifetime value. But the recommendations from these systems do not change on inputs beyond user and item profile and transaction data. There have been some attempts in the past to optimize on more varied data in recommenders, example of which is the location based recommenders. But location is just one dimension of the state that a user could have shared with GPS/GLONASS/BaiDeu sensor available in most Smartphones. With an upcoming era of Smart-wears and pervasive IoTs, there are a lot many other dimensions of a user state which can be utilized to optimize upon the concept of Optimal State Recommender Solutions. This paper suggests upgrading from conventional recommendations that are based on user/ item preferences alone with systems that provide the best recommendation at the most optimal state when the user is most receptive to accept the recommendation, the “optimal state recommendation solution” and proposes solutions and architectures to overcome the challenges of dealing with real time, distributed machine learning on IoT scale data in implementing this solution. The paper leverages some of the advance distributed machine learning algorithms like variants of Distributed Kalman Filters, Distributed Alternating Least Square Recommenders, Distributed Mini-Batch Stochastic Gradient Descent(SGD) based Classifiers, and highly scalable distributed computation and machine learning platforms like Apache Spark, (Apache) Spark MLlib, Spark Streaming, Python/PySpark, R/SparkR, Apache Kafka in an high performance, distributed, fault tolerant architecture. The solution also aspires to be compliant with upcoming IoT standards and architectures like IEEE P2413 to provide a standard solution for such problems beyond the current scope of this paper.","Sparks,
Machine learning algorithms,
Internet of things,
Kalman filters,
Band-pass filters,
Real-time systems,
Distributed databases"
A machine learning approach to edge type inference in Internet AS graphs,"The Internet AS topology can be represented by AS graphs where nodes represent ASes and edges represent business relationships between ASes. AS relationship can be broadly classified into two types: provider-to-customer (p2c) and peer-to-peer (p2p). In this paper we present a machine learning approach to edge type inference in AS graphs. Given an AS graph derived from publicly available data source, we use the Gentle AdaBoost machine learning algorithm to train a classifier that classifies the edge types (p2c and p2p) based on a set of node features. We use our method to train classifiers for three AS graphs derived from different data sources-a BGP graph, a traceroute graph, and an IRR graph. The three classifiers achieve 93.97%-97.73% accuracy when validated against ground truth and achieve 81.76%-95.66% accuracy when validated against CAIDA's AS relationship inference dataset. We merge the three individual graphs to obtain a combined graph and propose a method to compute edge types in the combined graph. We analyze the characteristics of the three individual graphs and the combined graph and show that combining the three individual graphs gives us a significantly more complete view of both the p2p and p2c ecosystems in the Internet.","Peer-to-peer computing,
Internet,
Topology,
Machine learning algorithms,
Routing,
Training,
Data mining"
Side-channel attacks and machine learning approach,"Most modern devices and cryptoalgorithms are vulnerable to a new class of attack called side-channel attack. It analyses physical parameters of the system in order to get secret key. Most spread techniques are simple and differential power attacks with combination of statistical tools. Few studies cover using machine learning methods for pre-processing and key classification tasks. In this paper, we investigate applicability of machine learning methods and their characteristic. Following theoretical results, we examine power traces of AES encryption with Support Vector Machines algorithm and decision trees and provide roadmap for further research.","Support vector machines,
Side-channel attacks,
Kernel,
Principal component analysis,
Decision trees,
Power demand"
Homogenizing social networking with smart education by means of machine learning and Hadoop: A case study,"In today's age of ever increasing use of internet, there are around 74% active internet users out of which 60% users contribute to social networking and most of them are students from the age group 16-30 [1]. If this young generation is targeted specifically towards educational activities keeping the same social networking environment in the background would create interest in students for educational activities and also yield productive results. Using Big Data analytics, machine learning and recommender system on the student data and activity would provide them with useful information and suggestions which would help them gain knowledge and make proper decisions to make their future in right direction. This can be implemented by creating a social-cum-educational portal with recommender systems, also data can be generated and displayed on the same place after analysis through recommenders. There is large amount of social, educational information generated on a rapid basis on the web which can be analysed and used for the betterment of the students and also the analysed information can be provided to the students based on their interests. Specific information to specific student can be provided. Use of such technology can reduce the gap between students and the information which can lead to their inherent development and success! However, most of the existing Social Recommender systems do not have good scalabilities which are unable to process huge volumes of data. Aiming to this problem we can design a social recommender system based on Hadoop and its parallel computing platform.",
Machine learning approach to automated correction of ETgX documents,"The problem is the automatic synthesis of formal correcting rules for LATEX documents. Each document is represented as a syntax tree. Tree node mappings of initial documents to edited documents form the training set, which is used to generate the rules. Rules with a simple structure, which implement removal, insertion or replacing operations of single node and use linear sequence of nodes to select a position are synthesized primarily. The constructed rules are grouped based on the positions of applicability and quality. The rules that use tree-like structure of nodes to select the position are studied. The changes in the quality of the rules during the sequential increase of the training document set are analyzed.","Syntactics,
Vegetation,
Training,
Particle separators,
Pattern matching,
Physics,
Publishing"
Text categorization with machine learning and hierarchical structures,"Text categorization with machine learning algorithms usually assumes to have flat set of categories. Such classifiers are very domain specific and not reusable for some other generic text classifications. It is very possible that a hierarchically structured set of categories might have a higher impact on the way classifiers are used and built. As presented in this document, the list of most common approaches for text categorization disregards hierarchy. The main reason is because measured performance of hierarchical trials has not showed a significant difference. Nerveless, it is encouraged to perform additional research in text categorization with hierarchical structures.","Text categorization,
Support vector machines,
Classification algorithms,
Machine learning algorithms,
Expert systems,
Training data,
Algorithm design and analysis"
Machine Learning In Incremental Sheet Forming,"Within this article, the authors propose a new methodology to increase the geometric accuracy in the robot-based, incremental sheet forming process ROBOFORMING. This process addresses the production of sheet metal components in small lot sizes and prototypes. In ROBOFORMING, two cooperating industrial robots are applied for the kinematic-based forming of sheet metal workpieces. Hereby, workpiece-dependent tooling and dies are omitted. This offers very high flexibility for the geometrical design of the sheet metal workpieces. One of the major drawbacks of incremental sheet forming processes is the low geometrical accuracy, which limits the widespread industrial application of these. Responding to these constraints, the authors propose the application of machine learning techniques to increase the geometric accuracy in incremental sheet forming processes. In this context, they present a learning approach which applies reinforcement learning as a flexible and promising solution.",
Experimental Robot Inverse Dynamics Identification Using Classical and Machine Learning Techniques,This paper shows the experimental identification of the inverse dynamics model of a KUKA iiwa lightweight robot. We use experimental data from optimal identification experiments to evaluate and compare two different identification approaches: a classical method using a parametrized robot dynamical model and a machine learning method. Both methods accurately estimate the dynamics model and this paper will discuss the pros and cons of each method.,
Using machine learning and SAR data for the upscaling of large scale modelled soil moisture in the Alps,Knowledge of the spatial and temporal distribution of soil moisture is important for many geoscience disciplines. Currently available remote sensing soil moisture products are not able to fully represent the heterogeneous patterns in mountain areas. In this paper we present a machine learning based approach for the upscaling of coarse scale modelled soil moisture based on Synthetic Aperture Radar data. For this purpose a statistical model was trained using the Gradient Tree Boosting approach. Results show that with the trained model it is possible to reproduce the reference data and increase spatial detailed of mapped soil moisture content.,
A novel hidden danger prediction method in cloud-based intelligent industrial production management using timeliness managing extreme learning machine,"To prevent possible accidents, the study of data-driven analytics to predict hidden dangers in cloud service-based intelligent industrial production management has been the subject of increasing interest recently. A machine learning algorithm that uses timeliness managing extreme learning machine is utilized in this article to achieve the above prediction. Compared with traditional learning algorithms, extreme learning machine (ELM) exhibits high performance because of its unique feature of a high generalization capability at a fast learning speed. Timeliness managing ELM is proposed by incorporating timeliness management scheme into ELM. When using the timeliness managing ELM scheme to predict hidden dangers, newly incremental data could be added prior to the historical data to maximize the contribution of the newly incremental training data, because the incremental data may be able to contribute reasonable weights to represent the current production situation according to practical analysis of accidents in some industrial productions. Experimental results from a coal mine show that the use of timeliness managing ELM can improve the prediction accuracy of hidden dangers with better stability compared with other similar machine learning methods.","Production,
Cloud computing,
Neurons,
Artificial neural networks,
Data models,
Machine learning algorithms,
Prediction algorithms"
Evaluation of Free Answer Comment Using Machine Learning by Word Evaluation,"There is a call for an increase to education quality in response to FD (Faculty Development) activities becoming mandatory due to a revision of the University Establishment Standards in 2008. A survey on lectures was carried out as part of FD activities and used for improving lectures based on lecture evaluation and satisfaction from students. However, it is difficult to give an objective evaluation of this text data since these subjects of evaluation are wide-ranging, such as expectations towards lectures or opinions on teachers. An aggregation of the opinions gained through surveys also has its limits for manual assessment because of the heavy artificial costs and time costs required. Therefore, it is difficult to evaluate all comments given in a survey. For this reason there has been extensive research done in recent years on categorizing evaluation texts and documents through evaluation expression of words and machine learning. There has been related research estimating the sentiment polarity of the entirety of the writing using both positives and negatives that appear within, as well as research that extracts elements of evaluation information and measures its sentiment polarity. Machine learning for writing categorization has also achieved a high level of accuracy, with research existing which uses the Naive Bayes Classifier and a Support Vector Machine. This report uses actual open ended responses to surveys to describe a method for re-evaluating a comment by estimating the evaluation of words within comments from a comment evaluation done through machine learning, as well as a method that uses a dictionary of emotional words to attach a polarity value to vocabulary and estimate the evaluation of the body of a comment from that value. Finally, it describes the results of teacher and lecture evaluations from each comment evaluation.","Estimation,
Dictionaries,
Education,
Informatics,
Electronic mail,
Supervised learning,
Standards"
Modeling the condition of lithium ion batteries using the extreme learning machine,"Recent years have seen increased interest in the use of off-grid solutions for electrification of rural areas. Off-grid electrification (such as solar home systems and micro-grids) are particularly applicable to the rural African context, where little infrastructure exists and in many regions grid extension is prohibitively expensive. To be economically viable, these systems must maximize the power delivered while ensuring the health of energy storage devices. Batteries in particular are a key weakness and typically the first major component to fail. In this paper we present an improved and simplified method for simulating the state of charge (SoC) and state of health (SoH) of lithium-ion batteries. SoC and SoH are predicted using the Extreme Learning Machine (ELM) algorithm. ELM is a state of the art single layer, feed-forward neural network that is characterized by its good generalized performance and fast learning speed. Real-life battery data from the NASA-AMES dataset provides the benchmark for evaluation of the ELM model.","Hafnium,
Africa,
Conferences"
VENCE: A new machine learning method enhanced by ontological knowledge to extract summaries,"Obtaining extractive summaries by using functions induced from a training set continue to be a great challenge in the domain of the automatic text summary. This paper presents the VENCE method based on this approach and improves the quality of the abduced functions, using semantic relations of the words (attributes) of the training set that are fetched from a ontology to be inserted in this set. The choice of this training set is reinforced with the optimization of the space of attributes by means of statistical techniques, as well as with the introduction of the Jaccard index, calculated from considering a manual summary that is extracted from the corpus of the chosen documents. The VENCE method is explained in details as well as the different experiments conducted to propose an optimal process. Its application to a text document corpus highlighted its efficiency. The results obtained are very satisfactory for the assessment of discriminating power of the abduced classification function as well as for the quality of summaries produced.","Training,
Machine learning algorithms,
Indexes,
Ontologies,
Classification algorithms,
Computers,
Optimization"
Improving detection rate using misuse detection and machine learning,"Network security is the provision made in an underlying computer network or rules made by the administrator to protect the network and its resources from unauthorized access. Network Security is becoming a crucial issue for all the firms and companies and with the increase in knowledge of intruders and hackers they have made many prosperous attempts to bring down web services and high-profile company networks. Misuse detection detects intrusions by matching the network traffic with a database of stored signatures and anomaly detection looks for behavior deviating from normal or common behavior for detecting intrusions. The primary objective of this paper is to combine both these techniques. The KDD dataset is used for this purpose. Finally, the data is processed by classification algorithms to obtain the results. The results show a high percentage of correct classification and accuracy. Experimental evaluation shows that the combined approach of Machine learning and misuse detection gives better performance.","Testing,
Intrusion detection,
Training,
Support vector machines,
Communication networks,
Classification algorithms"
Teeth periapical lesion prediction using machine learning techniques,"Teeth Periapical lesion is used to be diagnosed by dentists according to patient's x-ray. But most of the time there were a problematic issue to reach a definitive diagnosis. It takes too much time, case and chief complaint history needed, many tests and tools are needed and sometimes taking too many radiographs is required. Even though, before starting the treatment sometimes reaching definitive diagnosis is difficult. Therefore, the objective of this research is to predict whether the patient has teeth periapical lesion or not and its type using machine learning techniques. The proposed system consists of four main steps: Data collection, image preprocessing using median and average filters for removing noise and Histogram equalization for image enhancement, feature extraction using two dimensional discrete wavelet transform algorithm, and finally machine learning (classification) using Feed Forward Neural Networks and K-Nearest Neighbor Classifier. It has been concluded from the results that the K-Nearest Neighbor Classifier performs better than Feed Forward Neural Network on our real database.","Dentistry,
Histograms,
Digital filters,
Filtering algorithms,
Discrete wavelet transforms,
Teeth,
Lesions"
Performance Analysis of Ensemble Supervised Machine Learning Algorithms for Missing Value Imputation,"In this era of cloud computing, web services based solutions are gaining popularity. The applications running on distributed environment seek new parameters for them to perform efficiently to satisfy end user's requirements. Finding these parameters for increasing efficiency has become a talk of researchers now days. Non functional performance of a web service is described through User dependent QoS properties. These QoS parameters are generally described in WS-Policy in Service Level Agreement (SLA). Usually in web service QoS datasets, web service QoS values are missing, which makes missing value imputations an important job while working with cloud web services. In the current work we compared the prediction accuracy of two groups of supervised machine learning ensembles based Meta learners: bagging and additive regression (boosting) with a fusion of the seven base learners in both. Random forest is found to be better performing in both Meta learners: bagging and boosting than other learning algorithms.","Quality of service,
Bagging,
Boosting,
Throughput,
Cloud computing,
Additives"
A service oriented QoS architecture targeting the smart grid world & machine learning aspects,"Dynamic selection of services and by extension of service providers are vital in today's liberalized market of energy. On the other hand it is equally important for Service Providers to spot the one QoS Module that offers the best QoS level in a given cost. Type of service, response time, throughput, availability and cost, consist a basic set of attributes that should be taken into consideration when building a concrete Grid network. In the proposed QoS architecture Prosumers request services based on the aforementioned set of attributes. The Prosumer requests the service through the QoS Module. It is then the QoS Module that seeks the Service Provider that best fits the needs of the client.","Quality of service,
Databases,
Service-oriented architecture,
Smart grids,
Contracts,
Time factors,
Computer architecture"
A Framework for QoS-aware Traffic Classification Using Semi-supervised Machine Learning in SDNs,"In this paper, a QoS-aware traffic classification framework for software defined networks is proposed. Instead of identifying specific applications in most of the previous work of traffic classification, our approach classifies the network traffic into different classes according to the QoS requirements, which provide the crucial information to enable the fine-grained and QoS-aware traffic engineering. The proposed framework is fully located in the network controller so that the real-time, adaptive, and accurate traffic classification can be realized by exploiting the superior computation capacity, the global visibility, and the inherent programmability of the network controller. More specifically, the proposed framework jointly exploits deep packet inspection (DPI) and semi-supervised machine learning so that accurate traffic classification can be realized, while requiring minimal communications between the network controller and the SDN switches. Based on the real Internet data set, the simulation results show the proposed classification framework can provide good performance in terms of classification accuracy and communication costs.","Quality of service,
Real-time systems,
Engines,
Training,
Control systems,
Classification algorithms,
Support vector machines"
Machine learning based job status prediction in scientific clusters,"Large high-performance computing systems are built with increasing number of components with more CPU cores, more memory, and more storage space. At the same time, scientific applications have been growing in complexity. Together, they are leading to more frequent unsuccessful job statuses on HPC systems. From measured job statuses, 23.4% of CPU time was spent to the unsuccessful jobs. We set out to study whether these unsuccessful job statuses could be anticipated from known job characteristics. To explore this possibility, we have developed a job status prediction method for the execution of jobs on scientific clusters. The Random Forests algorithm was applied to extract and characterize the patterns of unsuccessful job statuses. Experimental results show that our method can predict the unsuccessful job statuses from the monitored ongoing job executions in 99.8% the cases with 83.6% recall and 94.8% precision. This prediction accuracy can be sufficiently high that it can be used to mitigation procedures of predicted failures.","Decision trees,
Prediction methods,
Hardware,
Reliability,
Software,
Complexity theory,
Prediction algorithms"
Part-of-speech tagging based on dictionary and statistical machine learning,"Part-of-speech tagging is the basis of Natural Language Processing, and is widely used in information retrieval, text processing and machine translation fields. The traditional statistical machine learning methods of POS tagging rely on the high quality training data, but obtaining the training data is very time-consuming. The methods of POS tagging based on dictionaries ignore the context information, which lead to lower performance. This paper proposed a POS tagging approach which combines methods based on dictionaries and traditional statistical machine learning. The experimental results show that the approach not only can solve the problem that the training data are insufficient in statistical methods, but also can improve the performance of the methods based on dictionaries. The People's Daily corpus in January 1998 is used as testing data, and the accurate rate of POS tagging achieves 95.80%. For the ambiguity word POS tagging, the accuracy achieves 88%.",Decision support systems
A quick view on current techniques and machine learning algorithms for big data analytics,"Big-data is an excellent source of knowledge and information from our systems and clients, but dealing with such amount of data requires automation, and this brings us to data mining and machine learning techniques. In the ICT sector, as in many other sectors of research and industry, platforms and tools are being served and developed in order to help professionals to treat their data and learn from it automatically; most of those platforms coming from big companies like Google or Microsoft, or from incubators at the Apache Foundation. This brief review explains the basics of machine learning with some ICT examples, and enumerates some (but not all) of the most used tools for analyzing and modelling big-data.","Data mining,
Clustering algorithms,
Data models,
Prediction algorithms,
Libraries,
Machine learning algorithms,
Classification algorithms"
An alternative technique for populating Thai tourism ontology from texts based on machine learning,"This paper proposes an alternative technique to perform ontology population by using natural language processing and machine learning techniques. This study conceptually considers the population task as classifying terms into ontological subcategories. The proposed technique adopts the recognition method named Conditional Random Fields (CRFs) to identify boundary of instances and define types of subconcepts to generate relationships between instance-of and related concept. Also, the lexico-syntactic pattern is used to identify the relationships between instances. The experiments are conducted on Thai language documents in the tourism domain. The experimental results showed that the instances extraction step provided 77.62% and 70.87% of precision and recall measures, respectively, and relationships extraction step yielded 82.67% and 72.61% of recall measures.","Ontologies,
Sociology,
Statistics,
Feature extraction,
Natural language processing,
Cultural differences,
Dictionaries"
Clinical text analysis using machine learning methods,"SemEval (Semantic Evaluation) is an annual workshop where attendees participate in a series of evaluations (competitions) of computational semantic analysis for natural language processing (NLP). The series evaluations include 10-20 tasks each year. In this paper we present our entry to the SemEval-2014 Task 7 on the Analysis of Clinical Text evaluation. The main aim of this task is to analyze large amounts of clinical data and to find the mentions of clinical disorders. This task consists of two sub tasks: a) named entity recognition i.e, identifying disorder concepts that belong to Unified Medical Language System (UMLS) semantic group; and b) normalization, i.e, mapping mentions of disorders to UMLS Concept Unique Identifier (CUI). In this paper, we present a supervised machine learning system for prediction of disorder named entities based on the conditional random field model. The data set provided by the Task 7 organizer was used to evaluate our model.","Unified modeling language,
Semantics,
Natural language processing,
Feature extraction,
Testing,
Text analysis,
Training data"
Corporate risk estimation by combining machine learning technique and risk measure,"A precise measure of corporates' operating performance play critical role in it achieving sustainable development during turbulent financial markets, because operating performance is a suitable reflection of corporate management, which has been widely recognized as the main cause of financial troubles. However, most proportion of previous works take return on assets or return on investment as a proxy for operating performance assessment, but both assessing measures merely consider one input and one output features, making them not appropriate for describing the whole facets of a corporate's operating situation. These limitations suggest that there is a need for additional analytical model for effective and useful prediction of corporate operating performance ranking. Thus, this study proposes a reliable and sophisticated prediction architecture that incorporates risk metrics, dimensionality reduction technique, data envelopment analysis, and artificial intelligence technique for corporate operating performance forecasting. The experimental results show that the proposed architecture can reduce unnecessary information, satisfactorily forecast the corporate operating performance ranking, and yield directions for properly allocating limited financial resource on reliable objects. The introduced architecture is a promising alternative for predicting corporate operating performance ranking, it can assist in both internal and external decision makers.","Support vector machines,
Predictive models,
Reliability,
Sensitivity,
Data envelopment analysis,
Forecasting,
Neural networks"
Machine learning based adaptive flow classification for optically interconnected data centers,We optimize flow placement for a hybrid network implementing an adaptive neural network classifier. We predict elephant flows with high accuracy on anonymized university network traffic. We also demonstrate the capability to perform highly complex actions at 40 Gbps using less than 5% of co-processor capacity. This shows that it is possible to implement intelligent actions such as a neural network in a data center using fully programmable NICs without handicapping the server CPU.,"Neural networks,
Mice,
Image motion analysis,
Computer vision,
Classification algorithms,
Optical fiber networks,
Computer networks"
Machine Learning,"In machine learning, a computer first learns to perform a task by studying a training set of examples. The computer then performs the same task with data it hasn't encountered before. This article presents a brief overview of machine-learning technologies, with a concrete case study from code analysis.","Machine learning,
Software engineering,
Classification,
Complexity theory,
Neural networks,
Artificial neural networks,
Clustering,
Supervised learning"
Purely Structural Protein Scoring Functions Using Support Vector Machine and Ensemble Learning,"The function of a protein is determined by its structure, which creates a need for efficient methods of protein structure determination to advance scientific and medical research. Because current experimental structure determination methods carry a high price tag, computational predictions are highly desirable. Given a protein sequence, computational methods produce numerous 3D structures known as decoys. However, selection of the best quality decoys is challenging as the end users can handle only a few ones. Therefore, scoring functions are central to decoy selection. They combine measurable features into a single number indicator of decoy quality. Unfortunately, current scoring functions do not consistently select the best decoys. Machine learning techniques offer great potential to improve decoy scoring. This paper presents two machine-learning based scoring functions to predict the quality of proteins structures, i.e., the similarity between the predicted structure and the experimental one without knowing the latter. We use different metrics to compare these scoring functions against three state-of-the-art scores. This is a first attempt at comparing different scoring functions using the same non-redundant dataset for training and testing and the same features. The results show that adding informative features may be more significant than the method used.","Proteins,
Training,
Support vector machines,
Amino acids,
Electronic mail,
Feature extraction,
Machine learning algorithms"
Comparison of Selected Machine Learning Algorithms for Sub-Pixel Imperviousness Change Assessment,"The paper presents the comparison of nine machine learning algorithms for sub-pixel impervious surface area change assessment. Predictive models were tuned and trained using the caret package in R environment. Their performance was analyzed based on both cross-validation results and results obtained for validation dataset. A paired t-test was used to determine if the differences between model accuracies are statistically significant. In case of imperviousness mapping for individual time points the regression trees based models outperformed other ones both for cross-validation on calibration dataset and for validation dataset. The Cubist algorithm seems to be the best performed one. The best assessment method for ISA change cannot be unambiguously pointed out. Random Forest gave the lowest RMS errors, random kNN was the best one according to MAE measure and support vector machines with radial basis kernel gave the highest mean value of the R2.","Remote sensing,
Satellites,
Land surface,
Earth,
Vegetation mapping,
Machine learning algorithms,
Predictive models"
A novel screen content fast transcoding framework based on statistical study and machine learning,"In this paper, a novel screen content transcoding framework is presented to efficiently bridge the state-of-art High Efficiency Video Coding (HEVC) standard and its incoming screen content coding (SCC) extension currently pending finalization. The proposed scheme is implemented as an Intra-coding “pre-processing” module on top of official SCC test model software (SCM). Both Coding Unit (CU) statistical features (such as CU color quantity, CU pixel variance, CU edge directionality distribution, etc.) and decoded video side information (such as CU partitions, modes, residual, etc.) are jointly analyzed. Accordingly, fast CU mode decisions and CU partitions bypass / termination heuristics are designed. Compared with SCM-4.0 official release, the proposed fast transcoding scheme can achieve an average of 48% re-encoding complexity reduction over JCT-VC screen content testing sequences with less than 2.14% marginal BD-Rate increase under SCC common testing conditions for All-Intra (AI) configuration.","Decision support systems,
Radio frequency"
Improving Classification Accuracy of a Machine Learning Approach for FPGA Timing Closure,"We can use Cloud Computing and Machine Learning to help deliver timing closure of FPGA designs using InTime [2], [3]. This approach requires no modification to the input RTL and relies exclusively on manipulating the CAD tool parameters that drive the optimization heuristics. By running multiple combinations of the parameters in parallel, we learn from results and identify which parameters caused an improvement in the final results. By systematically building a classification model and training it with the results of the parallel CAD runs, we can build an accurate estimation flow for helping identify which parameters are more likely to improve the timing. In this paper, we consider strategies for improving the predictive accuracy of our classifier models to help guide the CAD run towards timing convergence. With ensemble learning we are able to increase average AUC score from 0.74 to 0.79, which could also translate into 2.7× savings in machine learning effort.","Design automation,
Timing,
Field programmable gate arrays,
Benchmark testing,
Solid modeling,
Machine learning algorithms,
Prediction algorithms"
Resource Frequency Prediction in Healthcare: Machine Learning Approach,"Determining the minimal amount of resources needed to ensure minimal number of bottlenecks in the patient flow not only promotes patient satisfaction but also provides financial benefits to hospitals. The increase of data gathering by healthcare facilities in the last years have brought new opportunities to apply machine learning techniques to tackle this problem. This work makes use of data gathered from the Oulu University Hospital in Finland between 2011 and 2014 to study the effectiveness of machine learning techniques to predict resources usage. This work investigates the problem of resource frequency prediction and compares the performance of Nearest Neighbours and Random Forest. The application of data clustering as a preprocessing step is also explored as a way to improve the prediction accuracy of resources whose behavior change over time. The results indicate that 1) highly frequented resources can be predicted with higher accuracy than the lowly frequented resources, 2) the Random Forest have similar performance to the Nearest Neighbours although Random Forest performs better, 3) clustering improves the performance of the Nearest Neighbours but not of Random Forest, and 4) if averages are used to determine the resource frequency then cluster averages yields higher accuracy than all data averages.","Hospitals,
Time series analysis,
Vegetation,
Measurement,
Forecasting,
Optimization"
Solar radiation forecast with machine learning,"Renewable energy forecasting becomes increasingly important as the contribution of solar/wind power production to the electrical power grid constantly increases. Significant improvement in forecasting accuracy has been demonstrated by developing more sophisticated solar irradiance forecasting models using statistics and/or numerical weather predictions. In this presentation, we report the development of a machine-learning based multi-model blending approach for statistically combing multiple meteorological models to improve the accuracy of solar power forecasting. The system leverages upon multiple existing physical models for prediction including numerous atmospheric and cloud prediction models based on satellite imagery as well as numerical weather prediction (NWP) products.","Weather forecasting,
Predictive models,
Atmospheric modeling,
Biological system modeling,
Numerical models,
Computational modeling"
Provenance Constraints and Attributes Definition in OWL Ontology to Support Machine Learning,The Semantic Web considered an intelligent web is an effective way to retrieve data. Though the semantic data is returned by the semantic web it may lack trust and cannot be suitable for consumption by man or machine without the evaluation of provenance. The integration of Provenance in the Trust layer of Semantic web not only allows for intelligent knowledge retrieval but also leads to retrieving trustworthy data. This can be efficiently achieved if we bring to use the various layers of the Provenance Stack thereby creating valid provenance instances. This paper explores the need for information trustworthiness in OWL Ontology and has deliberated the rules to incorporate trustworthiness so as to develop valid provenance instances by means of the constraints provided in the PROV-CONSTRAINTS layer. The resultant Ontology can thus be used by machines for learning & making semantic inferences.,"Ontologies,
Semantics,
Data models,
OWL,
Vocabulary,
Service-oriented architecture"
Identifying Rare Diseases from Behavioural Data: A Machine Learning Approach,"Rare diseases are hard to identify and diagnose. Our goal is to use self-reported behavioural data to distinguish people with rare diseases from people with more common chronic illnesses. To this effect, we adapt a state of the art machine learning algorithm to make this classification. We find that using this method, and an appropriate set of questions, we can accurately identify people with rare diseases.","Diseases,
Sociology,
Statistics,
Drugs,
Medical diagnostic imaging,
Internet"
Perform-ML: Performance optimized machine learning by platform and content aware customization,"We propose Perform-ML, the first Machine Learning (ML) framework for analysis of massive and dense data which customizes the algorithm to the underlying platform for the purpose of achieving optimized resource efficiency. Perform-ML creates a performance model quantifying the computational cost of iterative analysis algorithms on a pertinent platform in terms of FLOPs, communication, and memory, which characterize runtime, storage, and energy. The core of Perform-ML is a novel parametric data projection algorithm, called Elastic Dictionary (ExD), that enables versatile and sparse representations of the data which can help in minimizing performance cost. We show that Perform-ML can achieve the optimal performance objective, according to our cost model, by platform aware tuning of the ExD parameters. An accompanying API ensures automated applicability of Perform-ML to various algorithms, datasets, and platforms. Proof-of-concept evaluations of massive and dense data on different platforms demonstrate more than an order of magnitude improvements in performance compared to the state of the art, within guaranteed user-defined error bounds.","storage management,
application program interfaces,
learning (artificial intelligence),
optimisation"
A Novel Approach for Logo Recognition System Using Machine Learning Algorithm SVM,"In different applications like Complex document image processing, Advertisement and Intelligent transportation logo recognition is an important issue. Logo Recognition is an essential sub process although there are many approaches to study logos in these fields. In this paper a robust method for recognition of a logo is proposed, which involves K-nearest neighbors distance classifier and Support Vector Machine classifier to evaluate the similarity between images under test and trained images. For test images eight set of logo image with a rotation angle of 0°, 45°, 90°, 135°, 180°, 225°, 270°, and 315° are considered. A Dual Tree Complex Wavelet Transform features were used for determining features. Final result is obtained by measuring the similarity obtained from the feature vectors of the trained image and image under test. Total of 31 classes of logo images of different organizations are considered for experimental results. An accuracy of 87.49% is obtained using KNN classifier and 92.33% from SVM classifier.","Support vector machines,
Feature extraction,
Training,
Image recognition,
Wavelet transforms,
Testing"
Data Classification Using Feature Selection and kNN Machine Learning Approach,"The k Nearest Neighbour (kNN) method is one of the most popular algorithm in clustering and data classification. The kNN algorithm founds to be performed very efficient in the experiments on different dataset. In this paper, we focus on the classification problem. The algorithm is experienced over Leukemia dataset. Initially three feature selection algorithm Consistency Based Feature Selection (CBFS), Fuzzy Preference Based Rough Set (FPRS) and Kernelized Fuzzy Rough Set (KFRS) is applied on the dataset and then kNN is applied as a classifier onto the dataset. The results of our experiment demonstrates that CBFS algorithm generally perform better than other two KFRS and FPRS algorithm respectively.","Classification algorithms,
Training,
Cancer,
Prediction algorithms,
Algorithm design and analysis,
Kernel,
Rough sets"
"Nonlinearity Mitigation Using a Machine Learning Detector Based on
k
-Nearest Neighbors","A powerful machine learning detector based on the k-nearest neighbors (KNN) algorithm is proposed to overcome system impairments. The zero-dispersion link (ZDL), dispersion managed link (DML), and dispersion unmanaged link (DUL) are considered. Meanwhile, an improved algorithm, the distance-weight KNN, is introduced, which outperforms the conventional maximum likelihood-post compensation approach. The numerical results show that KNN is feasible for overcoming various impairments, especially for non-Gaussian symmetric noise, such as laser phase noise and nonlinear phase noise in the ZDL or DML.","Training data,
Testing,
Optical fiber dispersion,
Optical fiber amplifiers,
Machine learning algorithms,
Phase noise,
Digital signal processing"
Developing machine learning-based models to estimate time to failure for PHM,"The core of PHM (Prognostic and Health Monitoring) technology is prognostics which is able to estimate time to failure (TTF) for the monitored components or systems using the built-in predictive models. However the development of predictive models for TTF estimation remains a challenge. To address this issue, we proposed to develop machine learning-based models for TTF estimation by using the techniques from machine learning and data mining. In the past decade, we have been working on the development of machine learning-based models for estimating TTF and applied the developed technology to various real-world applications such as train wheel prognostics, and aircraft engine prognostics. In this paper, we report two kinds of machine learning-based models for estimating TTF, including multistage classification, on-demand regression. The multistage classification improves the TTF estimation over one stage classification by dividing the time window into more small narrow time windows. A case study, APU prognostics, demonstrates the usefulness of the developed methods. The results from the case study show that the machine learning-based modeling method is an effective and feasible way to develop predictive models to estimate TTF for PHM.",
Online sequential extreme learning machines for fault detection,"In this paper, we propose the application of a new fault detection approach with a sequential updating function under new operating conditions or natural evolving degradation processes. The proposed approach is based on Online Sequential Extreme Learning Machines (OS-ELM). OS-ELM have the advantages of a strong learning ability, very fast training, and online learning. The concept of applying OS-ELM to fault detection is demonstrated on an artificial case study. We expect that OS-ELM can contribute to improve the fault detection and also the associated initiation of maintenance activities for engineering components working in an evolving environment such as electric components, bearings, gears, alternators, shafts and pumps, in which the monitored signals are not only significantly affected by working load and surrounding environment but may also experience some modifications due to a slow aging and degradation process.","Training,
Fault detection,
Signal reconstruction,
Mathematical model,
Degradation,
Machine learning algorithms,
Data models"
Improving software quality using machine learning,"Software is an entity that keeps on progressing and endures continuous changes, in order to boost its functionality and maintain its effectiveness. During the development of software, even with advanced planning, well documentation and proper process control, are problems that are countered. These defects influence the quality of software in one way or the other which may result into failure. Therefore, in today's neck to neck competition, it is our requirement to control and minimize these defects in software engineering. Software prediction models are typically used to map the patterns of classes of software that are prone to change. This paper highlights the significant analysis in the area's subject to learn and stimulate the association between the metric specifying the object orientation & the concept of change proneness. This would often lead us to rigorous testing so as to find all kinds of possibilities in the data set. We have two views to be addressed: (1) Parameters quantification that affects the quality, functionality and productivity of the software. (2) Machine learning technologies are used for predicting software Here, the focus of the research paper is to equate and compare all of learning methods corresponding to performance parameter with its statistical method & methodology which would often results enhanced. Data points are the basis for prediction of models.","Software,
Predictive models,
Testing,
Androids,
Humanoid robots,
Software measurement"
Diagnosing wind turbine faults using machine learning techniques applied to operational data,"Unscheduled or reactive maintenance on wind turbines due to component failures incurs significant downtime and, in turn, loss of revenue. To this end, it is important to be able to perform maintenance before it's needed. By continuously monitoring turbine health, it is possible to detect incipient faults and schedule maintenance as needed, negating the need for unnecessary periodic checks. To date, a strong effort has been applied to developing Condition monitoring systems (CMSs) which rely on retrofitting expensive vibration or oil analysis sensors to the turbine. Instead, by performing complex analysis of existing data from the turbine's Supervisory Control and Data Acquisition (SCADA) system, valuable insights into turbine performance can be obtained at a much lower cost. In this paper, data is obtained from the SCADA system of a turbine in the South-East of Ireland. Fault and alarm data is filtered and analysed in conjunction with the power curve to identify periods of nominal and fault operation. Classification techniques are then applied to recognise fault and fault-free operation by taking into account other SCADA data such as temperature, pitch and rotor data. This is then extended to allow prediction and diagnosis in advance of specific faults. Results are provided which show success in predicting some types of faults.","Wind turbines,
Maintenance engineering,
Wind speed,
Monitoring,
Generators,
Sensors,
Blades"
When quantitative trading meets machine learning: A pilot survey,"Quantitative trading strategies are designed to look for relationships between data about an underlying security and its future price and then to generate alpha on a trading desk. Recent years have witnessed the increasing attention from both academic and corporate sectors on enhancing quantitative trading by machine learning techniques due to their excellent predictive powers, with a few successful stories from the markets further boosting optimism for this method of analysis. In this paper, we aim to conduct a comprehensive survey on the pilot study of applying machine learning for quantitative trading. We will review some earlier studies of using NNs and SVMs for stock price prediction. We will also touch some recent studies on designing online learning algorithms based on characteristics of financial time series, e.g., mean reversion of stock price. Another application of machine learning in quantitative trading is called meta-learning algorithm which considers how to assign weights to strategies. We will finally summarize the above research by pointing out promising machine learning techniques for different categories of trading strategies. We will also discuss slightly the potentials of machine learning techniques in helping generate strategies that do not only base on financial market data, like behavioral strategy, event-driven and untraditional index strategy.","Support vector machines,
Predictive models,
Neural networks,
Analytical models,
Mathematical model,
Lead,
Kernel"
Reducing data storage requirements for machine learning algorithms using principle component analysis,"While current computers have shown to be particular useful for arithmetic and logic implementations, their accuracy and efficiency for applications such as e.g. face, object and speech recognition, are not that impressive, especially when compared to what the human brain can do. Machine learning algorithms have been useful, especially for these type of applications, as they operate in a similar way to the human brain, by learning the data provided and storing it for future recognition. Until now, there has been a strong focus on developing the process of data storage and retrieval, merely neglecting the value of the provided information and the amount of data required to store. Hence, currently all information provided is stored, because it is difficult for the machine to decide which information needs to be stored. Consequently, large amounts of data are stored, which then affects the processing of the data. Thus, this paper investigates the opportunity to reduce data storage through the use of differentiation and combine it with an existing similarity detection algorithm. The differentiation isWhile current computers have shown to be particular useful for arithmetic and logic implementations, their accuracy and efficiency for applications such as e.g. face, object and speech recognition, are not that impressive, especially when compared to what the human brain can do. Machine learning algorithms have been useful, especially for these type of applications, as they operate in a similar way to the human brain, by learning the data provided and storing it for future recognition. Until now, there has been a strong focus on developing the process of data storage and retrieval, merely neglecting the value of the provided information and the amount of data required to store. Hence, currently all information provided is stored, because it is difficult for the machine to decide which information needs to be stored. Consequently, large amounts of data are stored, which then affects the processing of the data. Thus, this paper investigates the opportunity to reduce data storage through the use of differentiation and combine it with an existing similarity detection algorithm. The differentiation is achieved through the use of, Principal Component Analysis (PCA), which not only reduces the data storage requirements by about 80%, but also improves the overall detection accuracy around 50 to nearly 80%. achieved through the use of, Principal Component Analysis (PCA), which not only reduces the data storage requirements by about 80%, but also improves the overall detection accuracy around 50 to nearly 80%.","Decision support systems,
Training,
Image reconstruction,
Euclidean distance,
Learning systems,
Principal component analysis,
Databases"
Consideration of manufacturing data to apply machine learning methods for predictive manufacturing,"According to the recent development of internet of things and big data, the serious tries of implementing smart factory have been increased. To realize the smart factory, firstly predictive manufacturing system should be implemented. As a first step of predictive manufacturing, this paper focuses on solving the simple but time consuming and high cost task in the predictive manner. The target problem of this paper is predicting CNC tool wear compensation offset using machine learning methods based on the data. To apply machine learning methods, we should understand the characteristics of the data and find the most suitable method according to the data characteristics. Thus, this paper discusses the characteristics of manufacturing data and compares various cases of applying machine learning methods.","Computer numerical control,
Production facilities,
Learning systems,
Manufacturing systems,
Big data,
Companies"
Speeding up distributed machine learning using codes,"Distributed machine learning algorithms that are widely run on modern large-scale computing platforms face several types of randomness, uncertainty and system “noise.” These include stragglers1, system failures, maintenance outages, and communication bottlenecks. In this work, we view distributed machine learning algorithms through a coding-theoretic lens, and show how codes can equip them with robustness against this system noise. Motivated by their importance and universality, we focus on two of the most basic building blocks of distributed learning algorithms: data shuffling and matrix multiplication. In data shuffling, we use codes to reduce communication bottlenecks: when a constant fraction of the data can be cached at each worker node, and n is the number of workers, coded shuffling reduces the communication cost by up to a factor Θ(n) over uncoded shuffling. For matrix multiplication, we use codes to alleviate the effects of stragglers, also known as the straggler problem. We show that if the number of workers is n, and the runtime of each subtask has an exponential tail, the optimal coded matrix multiplication is Θ(log n) times faster than the uncoded matrix multiplication or the optimal task replication scheme.","Machine learning algorithms,
Algorithm design and analysis,
Distributed databases,
Robustness,
Encoding,
Convergence,
Data models"
On-line machine learning accelerator on digital RRAM-crossbar,"On-line machine learning has become the need for future data analytics. This work will show an ℓ2 norm based hardware solver for on-line machine learning that can significantly reduce training time when compared to the traditional gradient-based solution using backward propagation. We will show that the intensive matrix-vector multiplication in ℓ2 norm solution can be mapped onto a distributed in-memory accelerator using the recent resistive switching random access memory (RRAM) device. A digitized matrix-vector multiplication accelerator will be developed based on the distributed RRAM-crossbar. Such a distributed RRAM-crossbar architecture can utilize the reformulated ℓ2 norm solver with a scalable and energy-efficient solution for real-time training and testing in image recognition. Experiment results have shown that significant speedup can be achieved for matrix-vector multiplication in the ℓ2 norm solver such hat the overall training and testing time can be reduced respectively. In addition, large energy saving can be also achieved when compared to the traditional CMOS-based out-of-memory computing architecture.","Training,
Computer architecture,
Feature extraction,
Resistance,
Testing,
Encoding,
Data analysis"
Integrating performance of web search engine with Machine Learning approach,"Todays diversified user query over web search engine for information retrieval; semantic information for relevant web document on web has been plethora of web search research. A lot many web search engine developed based on semantic meaning like ontolook, swoogle etc., for finding relevant information, which helps to find user based semantic meaning related documents. The concept of semantic similarity or semantic information widely focused in many important fields such as Machine Learning, Artificial Intelligence, Cognitive Science, Natural Language Processing and Web Information Retrieval etc., Traditional web search engines and semantic web search engines relates user keyword with terms, entities, texts, documents which have semantic correlation with user query. Both search engines does not use images within web pages to find more relevant information. Now in this paper we have formulated a web document integrated ranking method based on text semantic information and image based object matching information. This integrated approach presented in this paper does not depend upon semantic information of user query but also consider image appearing within web pages to find more relevant information. Approach proposed in this paper includes finding semantic information using ontology based meaning of user query and feature based object matching over image to find image matching score. In proposed approach combined use of ontology based semantic information and image based object matching score will improve web document ranking.","Semantics,
Feature extraction,
Web search,
Search engines,
Web pages,
Ontologies"
Machine learning methods for surgery cancellation,"Surgery cancellation has a greatly negative impact on both hospital and patient. A commonly used method for improving this problem is analyzing cancellation reasons through statistical analysis, while the improvement is inefficient. Based on surgery data from the hospital information system (HIS) of West China Hospital (WCH), we proposed to use machine learning methods include Random forests (RF), Bagging, Boosting and Bayesian additive regression trees (BART) to predict surgery cancellation. The experimental result shows that the Boosting with Over-sampling performed well in prediction accuracy (Sensitive: 0.5234, Specific: 0.8105, Accuracy: 0.7651 and AUC: 0.6670). And our research also points out the latent causes for surgery cancellation. It's useful for hospital managers.","Radio frequency,
Bayes methods,
History,
Surgery"
Machine learning techniques applied to system characterization and equalization,Linear signal processing algorithms are effective in combating linear fibre channel impairments. We demonstrate the ability of machine learning algorithms to combat nonlinear fibre channel impairments and perform parameter extraction from directly detected signals.,"Optical noise,
Signal to noise ratio,
Modulation,
Phase noise,
Estimation,
Nonlinear optics,
Optical polarization"
Designing adaptive learning support through machine learning techniques,"The use of web 2.0 technologies in web based learning systems has made learning more learner-centered. In a learner centered environment, there is need to provide appropriate support to learners based on individual learner characteristics in order to maximize learning. This requires a Web-based learning system to have an adaptive interface to suit individual learner characteristics in order to accommodate diversity of learner needs and abilities and to maintain an appropriate context for interaction and for achieving personalized learning. The purpose of this paper is to discuss how machine learning techniques can provide adaptive learning support in a Web-based learning system. In this research, two machine learning algorithms namely: Heterogeneous Value Difference Metric (HVDM) and Naive Bayes Classifier (NBC) were used. HVDM was used to determine those learners who were similar to the current learner while NBC was used to estimate the likelihood that the learner would need to use additional materials for the current concept. To demonstrate the concept we used a course in object oriented programming (OOP).","Adaptation models,
Object oriented modeling,
Adaptive systems,
Learning systems,
Machine learning algorithms,
User interfaces,
Navigation"
A comparison between heuristic and machine learning techniques in fall detection using Kinect v2,"In this paper, two algorithms were tested on 11 healthy adults: one based on heuristic and another one on video tagging machine learning methods for automatic fall detection; both utilizing Microsoft Kinect v2. For our heuristic approach, we used skeletal data to detect falls based on a set of instructions and signal filtering methods. For the machine learning approach, we implemented a dataset utilizing the Adaptive Boosting Trigger (AdaBoostTrigger) algorithm via video tagging to enable fall detection. For each approach, each subject on average has performed six true positive and six false positive fall incidents in two different conditions: one with objects partially blocking the sensor's view and one with partial obstructed field of view. The accuracy of each approach has been compared against one another in different conditions. The result showed an average of 95.42 % accuracy in the heuristic approach and 88.33 % in machine learning technique. We conclude that heuristic approach performs more accurately for fall detection when there is a limited number of training dataset available. Nevertheless, as the gesture detection's complexity increases, the need for a machine learning technique is inevitable.","Magnetic heads,
Sensors,
Floors,
Training,
Three-dimensional displays,
Software,
Machine learning algorithms"
Design and application of machine learning algorithm computer in connect6 of computer games system,The paper introduces briefly some of the key structures of the computer games system in connect6. And it mainly provides a further optimization for the current existing computer games system in connect6. The machine learning algorithm can increase the adaptive width adjustment in the MTD (f) algorithm and the self-learning method by TD_BP algorithm in the evaluation function which could strengthen the “thinking” ability on the computer games system in connect6. The advanced and modified algorithm is proved to be practical and applicative by experimentations and tests of computer games system in connect6 provided in this paper.,"Games,
Computers,
Algorithm design and analysis,
Machine learning algorithms,
Heuristic algorithms,
Search engines,
Learning systems"
Probability estimation for Predicted-Occupancy Grids in vehicle safety applications based on machine learning,"This paper presents a method to predict the evolution of a complex traffic scenario with multiple objects. The current state of the scenario is assumed to be known from sensors and the prediction is taking into account various hypotheses about the behavior of traffic participants. This way, the uncertainties regarding the behavior of traffic participants can be modelled in detail. In the first part of this paper a model-based approach is presented to compute Predicted-Occupancy Grids (POG), which are introduced as a grid-based probabilistic representation of the future scenario hypotheses. However, due to the large number of possible trajectories for each traffic participant, the model-based approach comes with a very high computational load. Thus, a machine-learning approach is adopted for the computation of POGs. This work uses a novel grid-based representation of the current state of the traffic scenario and performs the mapping to POGs. This representation consists of augmented cells in an occupancy grid. The adopted machine-learning approach is based on the Random Forest algorithm. Simulations of traffic scenarios are performed to compare the machine-learning with the model-based approach. The results are promising and could enable the real-time computation of POGs for vehicle safety applications. With this detailed modelling of uncertainties, crucial components in vehicle safety systems like criticality estimation and trajectory planning can be improved.","Vehicles,
Computational modeling,
Predictive models,
Vehicle dynamics,
Acceleration,
Vehicle safety,
Uncertainty"
Machine learning approach to automatic bucket loading,"The automation of bucket loading for repetitive tasks of earth-moving operations is desired in several applications at mining sites, quarries and construction sites where larger amounts of gravel and fragmented rock are to be moved. In load and carry cycles the average bucket weight is the dominating performance parameter, while fuel efficiency and loading time also come into play with short loading cycles. This paper presents the analysis of data recorded during loading of different types of gravel piles with a Volvo L110G wheel loader. Regression models of lift and tilt actions are fitted to the behavior of an expert driver for a gravel pile. We present linear regression models for lift and tilt action that explain most of the variance in the recorded data and outline a learning approach for solving the automatic bucket loading problem. A general solution should provide good performance in terms of average bucket weight, cycle time of loading and fuel efficiency for different types of material and pile geometries. We propose that a reinforcement learning approach can be used to further refine models fitted to the behavior of expert drivers, and we briefly discuss the scooping problem in terms of a Markov decision process and possible value functions and policy iteration schemes.","Loading,
Rocks,
Wheels,
Load modeling,
Fuels,
Trajectory,
Pistons"
Machine learning in tracking associations with stereo vision and lidar observations for an autonomous vehicle,"Obstacles detection is used nowdays for a number of road safety applications, increasing the drivers awareness in potential dangerous situations. A reliable and robust obstacles detection continues to be largely investigated and still remains an open challenge, especially for difficult scenarios and in general cases, with loosened constraints and multiple simultaneous use-cases. This work presents an obstacles detection, tracking and fusion algorithm which allows to reconstruct the environment surrounding the vehicle. While the techniques used for the detection are well-known in literature, the improvements introduced by this paper regard the data association and tracking approach of heterogeneous sensors observations. An innovative multi-dimensional structure based on association costs originating from a classifier provides an optimal solution to the association problem with respect to the total association cost. An Unscented Kalman Filter (UKF) managing a variable number of observations, arbitrarily composable, allows to correctly address the combined tracking and fusion challenge. The results, obtained on a public benchmark, show improvements with respect to state of the art systems.","Laser radar,
Sensor fusion,
Radar tracking,
Three-dimensional displays,
Reliability,
Cameras"
Applying Computational Aesthetics to a Video Game Application Using Machine Learning,"The authors have developed a novel approach to evaluating the aesthetic quality of the camera direction in video game scenes rendered in real time while the game is being played. Their goal was to improve the visual aesthetic quality of computer-generated images using a computational aesthetics approach via a regression machine learning model. Considering the challenges and limitations involved, the proposed approach yielded promising prediction performance. The results show that near-real-time aesthetic analysis and visual improvement is possible using a virtual camera director.","Cameras,
Image color analysis,
Feature extraction,
Games,
Training,
Predictive models,
Correlation coefficient"
Cloud-based machine learning for the detection of anonymous web proxies,"The emergence and growth of cloud computing has made a serious impact on the IT industry in recent years with large companies starting to offer powerful, reliable and cost-efficient platforms for businesses to build and reshape their business models. Showing no sign of slowing down, cloud computing capabilities now include machine learning, with facilities for both designing and deploying models. With this capability of machine learning using cloud computing comes the increasing need to be able to classify whether an incoming connection is from a legitimate originating IP address or if it is being sent through an intermediary like a web proxy. Taking inspiration from Intrusion Detection Systems that make use of machine learning capabilities to improve anomaly detection accuracy, this paper proposes that cloud based machine learning can be used in order to detect and classify web proxy usage by capturing packet data and feeding it into a cloud based machine learning web service.","Cloud computing,
Servers,
Intrusion detection,
Companies"
A comprehensive study on machine learning concepts for text mining,"The aim of machine learning is to solve a given problem using past experience or example data. Many machine learning applications are using now-a-days already. More aspiring problems can be handled as more data become accessible. Here. in this context we learn in detail about text mining as a multi-dimensional field which involves the closely linked areas or sections like 1. Retrieving information, 2. Machine learning concepts shortly termed as ML, 3. Statistics, 4. And finally Computational linguistics and specifically to be mentioned, data mining. With the use of sample data or previously gained experience, machine learning is included into computers to enhance or improve a performance decisive factor. In this context we have detailed a model up to some level of constraints, and learning is the processing of a main content to enhance the parameter of the form using the training or sample data or previously gained experience. This may be designed to gain knowledge from the given data, or use the effect for changes in the future, or both. These learning techniques also helps us to make solutions to various bugs which includes vision, speech recognition, and robotics. We take the example of the main analysis of preprocessing of tasks and procedures, then classification, then clustering, information extraction and finally visualization.","Context,
Computers,
Hidden Markov models,
Pragmatics,
Knowledge discovery,
Text mining"
A syllabus on data mining and machine learning with applications to cybersecurity,"Big data analytics are very fruitful for solving problems in cybersecurity. We have analyzed modern trends in intelligent security systems research and practice and worked out a syllabus for a new university course in the area of data mining and machine learning with applications to cybersecurity. The course is for undergraduate and graduate students studying the cybersecurity. The main objective of the course is to provide students with fundamental concepts in data mining (in particular, mining frequent patterns, associations and correlations, classification, cluster analysis, outlier detection), machine learning (including neural networks, support vector machines etc.) and related issues, e.g. the basics of multidimensional statistics. Contrary to the traditional data mining and machine learning courses we illustrate course topics by cases from the area of cybersecurity including botnet detection, intrusion detection, deep packet inspection, fraud monitoring, malware detection, phishing detection, active authentication. We note that our course has great potential for development.","Data mining,
Computer security,
Data analysis,
Big data,
Information security,
Correlation"
Revealing encrypted WebRTC traffic via machine learning tools,"The detection of encrypted real-time traffic, both streaming and conversational, is an increasingly important issue for agencies in charge of lawful interception. Aside from well established technologies used in real-time communication (e.g. Skype, Facetime, Lync etc.) a new one is recently spreading: Web Real-Time Communication (WebRTC), which, with the support of a robust encryption method such as DTLS, offers capabilities for encrypted voice and video without the need of installing a specific application but using a common browser, like Chrome, Firefox or Opera. Encrypted WebRTC traffic cannot be recognized through methods of semantic recognition since it does not exhibit a discernible sequence of information pieces and hence statistical recognition methods are called for. In this paper we propose and evaluate a decision theory based system allowing to recognize encrypted WebRTC traffic by means of an open-source machine learning environment: Weka. Besides, a reasoned comparison among some of the most credited algorithms (J48, Simple Cart, Naïve Bayes, Random Forests) in the field of decision systems has been carried out, indicating the prevalence of Random Forests.","WebRTC,
Cryptography,
Browsers,
Classification algorithms,
Training,
Protocols"
Machine Learning Approach for Cloud NoSQL Databases Performance Modeling,"Cloud computing is a successful, emerging paradigm that supports on-demand services with pay-as-you-go model. With the exponential growth of data, NoSQL databases have been used to manage data in the cloud. In these newly emerging settings, mechanisms to guarantee Quality of Service heavily relies on performance predictability, i.e., the ability to estimate the impact of concurrent query execution on the performance of individual queries in a continuously evolving workload. This paper presents a performance modeling approach for NoSQL databases in terms of performance metrics which is capable of capturing the non-linear effects caused by concurrency and distribution aspects. Experimental results confirm that our performance modeling can accurately predict mean response time measurements under a wide range of workload configurations.","Measurement,
Predictive models,
Database systems,
Cloud computing,
Computational modeling,
Concurrent computing"
A Hindi Question Answering System using Machine Learning approach,"A Question Answering (QA) System is fairly an Information Retrieval(IR) system in which a query is stated to the system and it relocates the correct or closest results to the specific question asked in natural language. It is one of the consequences of Natural Language Interface to Database (NLIDB). The paper discusses the implementation of a Hindi Language QA system developed using Machine Learning approach. The implemented QA system is divided into three phases: Accessing natural language (NL) Query; where the input query is read, preprocessed and get tokenized; next is feature extraction (FE) phase; where specific features vectors are identified from the results of previous phase and finally the Classification phase; where the Naïve Baye's classifier has been used, along with the knowledge base already stored in the system. This paper reflects that the concepts of similarity and classification provide better results than the use of `equals' concept by defining the overall accuracy of finding the relevant answers of the specific questions asked by the user.","Natural languages,
Knowledge discovery,
Knowledge based systems,
Feature extraction,
Computer science,
Keyboards,
Databases"
Identification and classification of relations for Indian languages using machine learning approaches for developing a domain specific ontology,"Information extraction and classification using Natural Language Processing techniques of layered architecture such as pre-processing task, processing of semantic analysis etc., helps in implementing further deeper evaluation techniques for the accuracy of natural language based electronic database. This paper explores relational information extraction of multilingual IndoWordNet database matching with domain specific terms. Further, extracted information are processed through conventional statistical methods, Normalized Web Distance (NWD) similarity method and two other machine learning evaluation techniques such as Support Vector Machine (SVM), Neural Network (NN) to compare with their accuracy. Results of machine leaning based techniques outperform with significant improved accuracy over conventional methods. The objective of using these techniques along with semantic web technology is to initiate a proof of concept for ontology generation by identification and classification of extracted relational information from IndoWordNet. This paper also highlights domain specific challenges and issues in developing relational model of ontology.","Databases,
Ontologies,
Support vector machines,
Semantic Web,
Information retrieval,
Semantics,
Neural networks"
Two Machine Learning Approaches for Short-Term Wind Speed Time-Series Prediction,"The increasing liberalization of European electricity markets, the growing proportion of intermittent renewable energy being fed into the energy grids, and also new challenges in the patterns of energy consumption (such as electric mobility) require flexible and intelligent power grids capable of providing efficient, reliable, economical, and sustainable energy production and distribution. From the supplier side, particularly, the integration of renewable energy sources (e.g., wind and solar) into the grid imposes an engineering and economic challenge because of the limited ability to control and dispatch these energy sources due to their intermittent characteristics. Time-series prediction of wind speed for wind power production is a particularly important and challenging task, wherein prediction intervals (PIs) are preferable results of the prediction, rather than point estimates, because they provide information on the confidence in the prediction. In this paper, two different machine learning approaches to assess PIs of time-series predictions are considered and compared: 1) multilayer perceptron neural networks trained with a multiobjective genetic algorithm and 2) extreme learning machines combined with the nearest neighbors approach. The proposed approaches are applied for short-term wind speed prediction from a real data set of hourly wind speed measurements for the region of Regina in Saskatchewan, Canada. Both approaches demonstrate good prediction precision and provide complementary advantages with respect to different evaluation criteria.","Wind speed,
Prediction algorithms,
Sociology,
Statistics,
Artificial neural networks,
Training,
Neurons"
A practice guide of software aging prediction in a web server based on machine learning,"In the past two decades, software aging has been studied by both academic and industry communities. Many scholars focused on analytical methods or time series to model software aging process. While machine learning has been shown as a very promising technique in application to forecast software state: normal or aging. In this paper, we proposed a method which can give practice guide to forecast software aging using machine learning algorithm. Firstly, we collected data from a running commercial web server and preprocessed these data. Secondly, feature selection algorithm was applied to find a subset of model parameters set. Thirdly, time series model was used to predict values of selected parameters in advance. Fourthly, some machine learning algorithms were used to model software aging process and to predict software aging. Fifthly, we used sensitivity analysis to analyze how heavily outcomes changed following input variables change. In the last, we applied our method to an IIS web server. Through analysis of the experiment results, we find that our proposed method can predict software aging in the early stage of system development life cycle.","Aging,
Predictive models,
Web servers,
Machine learning algorithms,
Software systems,
Prediction algorithms"
A personalized recommender system using Machine Learning based Sentiment Analysis over social data,"Social Media platforms are already an indispensable part of our daily lives. With its constant growth, it has contributed to superfluous, heterogeneous data which can be overwhelming due to its volume and velocity, thus limiting the availability of relevant and required information when a particular query is to be served. Hence, a need for personalized, fine-grained user preference-oriented framework for resolving this problem and also, to enhance user experience is increasingly felt. In this paper, we propose a such a social framework, which extracts user's reviews, comments of restaurants and points of interest such as events and locations, to personalize and rank suggestions based on user preferences. Machine Learning and Sentiment Analysis based techniques are used for further optimizing search query results. This provides the user with quicker and more relevant data, thus avoiding irrelevant data and providing much needed personalization.","Sentiment analysis,
Media,
Support vector machines,
Silicon,
Algorithm design and analysis,
Machine learning algorithms,
Feature extraction"
Cycle-slip-tolerant decision-boundary creation with machine learning,"Optimum symbol-decision boundaries created using machine learning approaches such as support-vector machine (SVM) can enhance system performance in the presence of nonlinear signal distortion. However, they fail if the received training signals for boundary creation include cycle slips induced by laser phase noise. In this paper, we propose a novel decision-boundary generation algorithm that is tolerant to cycle slips. Our proposed scheme groups rotationally symmetric constellation symbols and detects cycle slips by monitoring the phase differences between symbols in the same group. Through numerical analysis, we confirm that our proposed decision-boundary creation technique is immune to cycle slips.","Support vector machines,
Phase noise,
Laser noise,
Kernel,
Training,
Bit error rate,
Training data"
A machine learning algorithm for interference removal from a signal,"Wireless Communication systems involve multiple transmission of signals from one place to another through a transmission medium. These signals may interfere with each other and cause loss of information, thus reducing the efficiency of these systems. This interference needs to be taken care of for developing reliable wireless communication systems. In this paper, we have dealt with this problem by using a two-staged machine learning algorithm over a real time signal for removing multiple interference with a minimal information loss. It is based on the working of the notch filter which is used to remove a single narrow band interference from a signal. The algorithm was simulated over a Binary Phase Shift Keying (BPSK) modulated signal with a number of continuous wave (CW) interference. Based on the results obtained, signal power loss and notch characteristics were analysed for optimal performance of the algorithm.",Interference
Heuristic model to improve Feature Selection based on Machine Learning in Data Mining,"Data Mining and Machine Learning is one of the most popular research areas in computer science that is relevant in today's world of unfathomable data. To keep up with the rising size of data, there arises a need to quickly extract knowledge from data sources to aid data analysis research and improve industry and market needs. Primary Data Mining algorithms like k-means, Apriori, PageRank etc. are used today, but Machine Learning techniques can enhance the same by learning from the complex patterns. This paper focuses on the various existing approaches where Machine Learning algorithms have been used to improve data classification and pattern recognition in Data Mining especially for Feature Selection. It compares and contrasts the existing techniques and finds out the best one among them. Further, the paper proposes a heuristic approach to theoretically overcome most of the limitations in existing algorithms.","Data mining,
Decision trees,
Training,
Machine learning algorithms,
Classification algorithms,
Algorithm design and analysis,
Support vector machines"
A hybrid approach to reducing the false positive rate in unsupervised machine learning intrusion detection,"Network intrusion detection aims to uncover unauthorized access to computer networks. Anomaly intrusion detection uses unsupervised learning to detect attacks based on profiles of normal user behaviors. If the system is being used differently, it triggers an alarm. Current methods of intrusion detection are unable to produce alerts without a high number of false positives. The proposed research will utilize a set of artificial intelligence machine learning methods to decrease the number of false positives in anomalous intrusion detection data. This method combines data clustering using the simple K-means algorithm, feature selection that employs the J48 Decision Tree algorithm, and self organizing maps to effectively reduce false positives using the KDD CUP 99 data set.","Intrusion detection,
Clustering algorithms,
Self-organizing feature maps,
Decision trees,
Algorithm design and analysis,
Training,
Feature extraction"
Machine learning techniques for age at death estimation from long bone lengths,"Estimating age at death of cadavers is an important ability in various subfields of forensic science and bioarchaeology. It can allow investigators to pinpoint someone's identity, more accurately locate an event of interest in time and clarify other societal or legal issues concerning a given skeletal collection. There are two main categories of methods for estimating age at death: biochemical methods - which use various biological or chemical processes to obtain an estimation -, and mathematical methods - which employ the use of data mining tools such as regression in order to estimate age from various numerical features. In this paper, we propose two machine learning approaches for the age estimation problem and prove that they outperform existing mathematical approaches on a number of case studies derived from publicly available data used for this task. Moreover, our methods are more robust and easier to reuse on new data.","Estimation,
Bones,
Neurons,
Support vector machines,
Biological neural networks,
Training"
Drug-target interaction prediction with hubness-aware machine learning,"Prediction of interactions between drugs and pharmacological targets is an important task for which various machine learning techniques have been applied recently. Although hubness-aware machine learning techniques are among the most promising recently developed approaches, they have not been used for the prediction of drug-target interactions before. In this paper, we extend the Bipartite Local Model (BLM), one of the most prominent approaches for drug-target interaction prediction. In particular, we propose to use a hubness-aware regression technique, ECkNN, as local model. Furthermore, we propose to represent drugs and targets in the similarity space. In order to assist reproducibility of our work as well as comparison to published results, we perform experiments on widely used publicly available real-world drug-target interaction datasets. The results show that our approach is competitive and, in many cases, superior to state-of-the-art drug-target prediction techniques.","Drugs,
Predictive models,
Error correction,
Biochemistry,
Training,
Computational intelligence,
Informatics"
Machine learning model for temporal pattern recognition,"Temporal abstraction and data mining are two research fields that have tried to synthesis time oriented data and bring out an understanding on the hidden relationships that may exist between time oriented events. In clinical settings, having the ability to know the hidden relationships on patient data as they unfold could help save a life by aiding in detection of conditions that are not obvious to clinicians and healthcare workers. Understanding the hidden patterns is a huge challenge due to the exponential search space unique to time-series data. In this paper, we propose a temporal pattern recognition model based on dimension reduction and similarity measures thereby maintaining the temporal nature of the raw data.","Data mining,
Time series analysis,
Feature extraction,
Data models,
Monitoring,
Market research"
Machine-Learning Indoor Localization with Access Point Selection and Signal Strength Reconstruction,"Indoor localization technique is a key enabling technology for the future Internet of things (IoT) paradigm. Improving the precision of indoor localization will expand the horizon of indoor IoT applications. In this paper, we propose an enhanced machine-learning indoor localization scheme which incorporates access point (AP) selection and the proposed signal strength reconstruction to enhance robustness in noisy environments. The proposed signal strength reconstruction scheme estimates/reconstructs the received signal strength indicator (RSSI) values of the nonselected APs from those of the selected APs to increase the size of the feature space for enhanced noise robustness. The proposed concept can be applied to various machine-learning frameworks. Simulation results demonstrate improved precision yielded by the proposed method in conjunction with support vector regression (SVR), ensemble SVR, and artificial neural network (ANN) models, as compared to these machine- learning techniques alone.","Received signal strength indicator,
Predictive models,
Internet of things,
Training data,
Training"
A machine learning methods: Outlier detection in WSN,Wireless sensor networks are gaining more and more attention these days. They gave us the chance of collecting data from noisy environment. So it becomes possible to obtain precise and continuous monitoring of different phenomenons. However wireless Sensor Network (WSN) is affected by many anomalies that occur due to software or hardware problems. So various protocols are developed in order to detect and localize faults then distinguish the faulty node from the right one. In this paper we are concentrated on a specific type of faults in WSN which is the outlier. We are focus on the classification of data (outlier and normal) using three different methods of machine learning then we compare between them. These methods are validated using real data obtained from motes deployed in an actual living lab.,
Evaluation of three machine learning algorithms as classifiers of premature ventricular contractions on ECG beats,"According to the World Health Organization, cardiovascular diseases (CVD) are the main cause of death worldwide. An estimated 17.5 million people died from CVD in 2012, representing 31% of all global deaths. The electrocardiogram (ECG) is a central tool for the pre-diagnosis of heart diseases. Many advances on ECG arrhythmia classification have been developed in the last century; however, there is still research to identify malignant waveforms on ECG beats. The premature ventricular complexes (PVC) are known to be associated with malignant ventricular arrhythmias and in sudden cardiac death (SCD) cases. Detecting this kind of arrhythmia has been crucial in clinical applications. In this work, we extracted 80 different features from 108,653 ECG classified beats of the MIT-BIH database in order to classify the Normal, PVC and other kind of ECG beats. We evaluated three classifier algorithms based on Machine Learning with different parameters and we got a total of 14 models. We used the F1 score and we compared predictive values as a measured of classifier evaluations. Results show that we could have a F1 scores near to the unit, showing the models are higher than 93% of performance.","Electrocardiography,
Feature extraction,
Artificial neural networks,
Neurons,
Heart rate variability,
Databases,
Classification algorithms"
A Combined Rule-Based and Machine Learning Audio-Visual Emotion Recognition Approach,"This paper proposes an audio-visual emotion recognition system that uses a mixture of rule-based and machine learning techniques to improve the recognition efficacy in the audio and video paths. The visual path is designed using the Bidirectional Principal Component Analysis (BDPCA) and Least-Square Linear Discriminant Analysis (LSLDA) for dimensionality reduction and class discrimination. The extracted visual features are passed into a newly designed Optimized Kernel-Laplacian Radial Basis Function (OKL-RBF) neural classifier. The audio path is designed using a combination of input prosodic features (pitch, log-energy, zero crossing rates and Teager energy operator) and spectral features (Mel-scale frequency cepstral coefficients). The extracted audio features are passed into an audio feature level fusion module that uses a set of rules to determine the most likely emotion contained in the audio signal. An audio visual fusion module fuses outputs from both paths. The performances of the proposed audio path, visual path, and the final system are evaluated on standard databases. Experiment results and comparisons reveal the good performance of the proposed system.","multimodal system,
Emotion recognition,
audio-visual processing,
rule-based,
machine learning"
Gaze prediction using machine learning for dynamic stereo manipulation in games,"Comfortable, high-quality 3D stereo viewing is becoming a requirement for interactive applications today. Previous research shows that manipulating disparity can alleviate some of the discomfort caused by 3D stereo, but it is best to do this locally, around the object the user is gazing at. The main challenge is thus to develop a gaze predictor in the demanding context of real-time, heavily task-oriented applications such as games. Our key observation is that player actions are highly correlated with the present state of a game, encoded by game variables. Based on this, we train a classifier to learn these correlations using an eye-tracker which provides the ground-truth object being looked at. The classifier is used at runtime to predict object category - and thus gaze - during game play, based on the current state of game variables. We use this prediction to propose a dynamic disparity manipulation method, which provides rich and comfortable depth. We evaluate the quality of our gaze predictor numerically and experimentally, showing that it predicts gaze more accurately than previous approaches. A subjective rating study demonstrates that our localized disparity manipulation is preferred over previous methods.","Games,
Three-dimensional displays,
Real-time systems,
Stereo image processing,
Gaze tracking,
Manipulator dynamics,
Context"
Coarse-grained reconfigurable hardware accelerator of machine learning classifiers,"In this paper a universal, coarse-grained reconfigurable architecture for hardware acceleration of decision trees (DTs), artificial neural networks (ANNs), and support vector machines (SVMs) is proposed. Using proposed architecture, two versions of DTs (Functional DT and Axis-Parallel DT), two versions of SVMs (with polynomial and radial kernels) and two versions of ANNs (Multi Layer Perceptron and Radial Basis), have been implemented in FPGA. Experimental results, based on 18 benchmark datasets from standard UCI Machine Learning Repository Database, indicate that FPGA implementation provides significant improvement (1-3 orders of magnitude) in the average instance classification time, in comparison with software implementations, based on WEKA and R project.","Random access memory,
Artificial neural networks,
Computer architecture,
Hardware,
Support vector machines,
Software,
Data processing"
Daleel: Simplifying cloud instance selection using machine learning,"Decision making in cloud environments is quite challenging due to the diversity in service offerings and pricing models, especially considering that the cloud market is an incredibly fast moving one. In addition, there are no hard and fast rules; each customer has a specific set of constraints (e.g. budget) and application requirements (e.g. minimum computational resources). Machine learning can help address some of the complicated decisions by carrying out customer-specific analytics to determine the most suitable instance type(s) and the most opportune time for starting or migrating instances. We employ machine learning techniques to develop an adaptive deployment policy, providing an optimal match between the customer demands and the available cloud service offerings. We provide an experimental study based on extensive set of job executions over a major public cloud infrastructure.","Cloud computing,
Decision making,
Quality of service,
Planning,
Actuators,
Computer architecture,
Knowledge based systems"
Can machine learning aid in delivering new use cases and scenarios in 5G?,"5G represents the next generation of communication networks and services, and will bring a new set of use cases and scenarios. These in turn will address a new set of challenges from the network and service management perspective, such as network traffic and resource management, big data management and energy efficiency. Consequently, novel techniques and strategies are required to address these challenges in a smarter way. In this paper, we present the limitations of the current network and service management and describe in detail the challenges that 5G is expected to face from a management perspective. The main contribution of this paper is presenting a set of use cases and scenarios of 5G in which machine learning can aid in addressing their management challenges. It is expected that machine learning can provide a higher and more intelligent level of monitoring and management of networks and applications, improve operational efficiencies and facilitate the requirements of the future 5G network.","5G mobile communication,
Degradation,
Resource management,
Conferences,
Network topology,
Context,
Topology"
Machine learning based handover management for improved QoE in LTE,"This paper presents a machine learning based handover management scheme for LTE to improve the Quality of Experience (QoE) of the user in the presence of obstacles. We show that, in this scenario, a state-of-the-art handover algorithm is unable to select the appropriate target cell for handover, since it always selects the target cell with the strongest signal without taking into account the perceived QoE of the user after the handover. In contrast, our scheme learns from past experience how the QoE of the user is affected when the handover was done to a certain eNB. Our performance evaluation shows that the proposed scheme substantially improves the number of completed downloads and the average download time compared to state-of-the-art. Furthermore, its performance is close to an optimal approach in the coverage region affected by an obstacle.","Handover,
Training,
Artificial neural networks,
Computer architecture,
Microprocessors"
Online resource mapping for SDN network hypervisors using machine learning,"The visualization of Software-Defined Networks (SDN) allows multiple tenants to share the same physical infrastructure and to use their own SDN controllers. SDN virtualization is achieved through an SDN network hypervisor that operates between the tenants' controllers and the SDN infrastructure. In order to provide performance guarantees, resource mapping is required for both data plane as well as control plane for each virtual SDN network. In the context of SDN virtualization, the control plane resources include the network hypervisor, which needs to be assigned to guarantee the performance for each tenant. In previous work, the hypervisor resource mapping is based on offline benchmarks that measure the hypervisor resource consumption against the control plane work load, e.g., control plane message rate. These offline benchmarks vary across different hypervisor implementations, e.g., single or multi-threaded, and depend on the capabilities of the deployed hardware platform, e.g., the used CPU. We propose an online approach based on machine learning techniques to determine the mapping of hypervisor resources to the control workload at runtime. This concept is already successfully applied in the context of self-configuring networks. We propose three models to estimate hypervisor resources and compare them for two SDN hypervisor implementations, namely FlowVisor and OpenVirteX. We show through measurements on a real virtualized SDN infrastructure that resource mappings can be learned on runtime with insignificant error margins.",
Modulation Format Identification in Coherent Receivers Using Deep Machine Learning,"We propose a novel technique for modulation format identification (MFI) in digital coherent receivers by applying deep neural network (DNN) based pattern recognition on signals' amplitude histograms obtained after constant modulus algorithm (CMA) equalization. Experimental results for three commonly-used modulation formats demonstrate MFI with an accuracy of 100% over a wide optical signal-to-noise ratio (OSNR) range. The effects of fiber nonlinearity on the performance of MFI technique are also investigated. The proposed technique is non-data-aided (NDA) and avoids any additional hardware on top of standard digital coherent receiver. Therefore, it is ideal for simple and cost-effective MFI in future heterogeneous optical networks.","Modulation,
Histograms,
Receivers,
Feature extraction,
Signal to noise ratio,
Optical noise,
Optical attenuators"
Joint modulation format/bit-rate classification and signal-to-noise ratio estimation in multipath fading channels using deep machine learning,"A novel algorithm for simultaneous modulation format/bit-rate classification and non-data-aided (NDA) signal-to-noise ratio (SNR) estimation in multipath fading channels by applying deep machine learning-based pattern recognition on signals’ asynchronous delay-tap plots (ADTPs) is proposed. The results for three widely-used modulation formats at two different bit-rates demonstrate classification accuracy of 99.8%. In addition, NDA SNR estimation over a wide range of 0−30 dB is shown with mean error of 1 dB. The proposed method requires low-speed, asynchronous sampling of signal and is thus ideal for low-cost multiparameter estimation under real-world channel conditions.","mean square error methods,
fading channels,
multipath channels,
modulation,
signal classification,
learning (artificial intelligence),
signal sampling,
parameter estimation"
FishAPP: A mobile App to detect fish falsification through image processing and machine learning techniques,"Food forgery is one of the most articulated socio-economic concerns, which contributed to increase people awareness on what they eat. Identification of species represents a key aspect to expose commercial frauds implemented by substitution of valuable species with others of lower value. Fish species identification is mainly performed by morphological identification of gross anatomical features of the whole fish. However, the increasing presence on markets of new little-known species makes morphological identification of species difficult. In this paper we present FishAPP, a cloud-based infrastructure for fish species recognition. FishAPP is composed of a mobile application developed for the Android and the iOS mobile operating system enabling the user to shot pictures of a whole fish and submit them for remote analysis and a remote cloud-based processing system that implements a complex image processing pipeline and a neural network machine learning system able to analyze the obtained images and to perform classification into predefined fish classes. Preliminary results obtained from the available dataset provided encouraged results.","Mobile communication,
Servers,
Cloud computing,
Mobile applications,
Feature extraction,
Smart phones"
Machine learning approach for sensors validation and clustering,"Wireless sensor network (WSN) is very important these days. It is used frequently in many applications in the area such as health care, security, home and military. As the sensor nodes in the domain may change over the time due to number of factors such as environment condition, lifetime of the battery (low battery power) and coverage (area of interest), then a sensor network needs periodically to check the validation of sensor in the domain. So, as needed of the sensors validations in the domain have been shown in many WSN schemes. In this paper, the validation sensors in the domain using spectral clustering technique have been proposed that is detecting a bad sensor and deleting it from the domain. Sensors have been indexed by their location using simple model of spectral clustering. Results obtained from simulation indicate that our approach is enhanced network performance in terms of detecting the bad sensor location and replace it that is improved our wireless sensor network.","Sensors,
Clustering algorithms,
Signal processing algorithms,
Laplace equations,
Peer-to-peer computing,
Symmetric matrices,
Wireless sensor networks"
Dimension Reduction With Extreme Learning Machine,"Data may often contain noise or irrelevant information, which negatively affect the generalization capability of machine learning algorithms. The objective of dimension reduction algorithms, such as principal component analysis (PCA), non-negative matrix factorization (NMF), random projection (RP), and auto-encoder (AE), is to reduce the noise or irrelevant information of the data. The features of PCA (eigenvectors) and linear AE are not able to represent data as parts (e.g. nose in a face image). On the other hand, NMF and non-linear AE are maimed by slow learning speed and RP only represents a subspace of original data. This paper introduces a dimension reduction framework which to some extend represents data as parts, has fast learning speed, and learns the between-class scatter subspace. To this end, this paper investigates a linear and non-linear dimension reduction framework referred to as extreme learning machine AE (ELM-AE) and sparse ELM-AE (SELM-AE). In contrast to tied weight AE, the hidden neurons in ELM-AE and SELM-AE need not be tuned, and their parameters (e.g, input weights in additive neurons) are initialized using orthogonal and sparse random weights, respectively. Experimental results on USPS handwritten digit recognition data set, CIFAR-10 object recognition, and NORB object recognition data set show the efficacy of linear and non-linear ELM-AE and SELM-AE in terms of discriminative capability, sparsity, training time, and normalized mean square error.","Principal component analysis,
Machine learning,
Mathematical model,
Support vector machines,
Machine learning algorithms"
Adaptive real-time Trojan detection framework through machine learning,"Hardware Trojans inserted at the time of design or fabrication by untrustworthy design house or foundry, poses important security concerns. With the increase in attacker's resources and capabilities, we can anticipate an unexpected new attack from the attacker at run-time. Therefore, the challenge is not only to reduce hardware overhead of added security feature but also to secure design from new attacks introduced at real-time. In this work, we propose a Real-time Online Learning approach for Securing many-core design. In order to prevent unexpected attacks, many-core provides feed-back to online learning algorithm based on core information and its behavior to incoming data packet. The proposed Online Learning approach updates the model run-time at each data transfer based on feed-back from many-core. For demonstration, Online Machine Learning model is initially trained with two types of (known) attacks and Trojan free router packets and then unexpected attack is introduced later at run-time. The results show that, feedback based Online Machine Learning algorithm has 8% higher overall detection accuracy and an average of 3% higher accuracy for unexpected attacks at each interval of 1000 test records than Supervised Machine Learning algorithms. The proposed feed-back based Trojan detection framework is demonstrated using a custom many-core architecture integrated with “Modified Balanced Winnow” Online Machine Learning algorithm on Xilinx Virtex-7 FPGA. Post place and route implementation results show that, secured many-core architecture requires 4 extra cycles to complete data transfer. The proposed architecture achieves 56% reduction in area and 50% less latency overhead as compared to previous published work [1]. Furthermore, we evaluate our framework for many-core platform by employing seizure detection application as a case study.","Trojan horses,
Hardware,
Machine learning algorithms,
Algorithm design and analysis,
Detection algorithms,
Security,
Support vector machines"
Machine learning resistant strong PUF: Possible or a pipe dream?,"Physically unclonable functions (PUFs) are emerging as hardware primitives for key-generation and light-weight authentication. Strong PUFs represent a variant of PUFs which respond to a user challenge with a response determined by its unique manufacturing process variations. Unfortunately many of the Strong PUFs have been shown to be vulnerable to model building attacks when an attacker has access to challenge and response pairs. In mounting a model building attack, typically machine learning is used to build a software model to forge the PUF. Researchers have long been interested in designing Strong PUFs that are resistant to model building attacks. However, with innovations in application of machine learning, nearly all Strong PUFs presented in the literature have been broken. In this paper, first we present results from a set of experiments designed to show that if certain randomness properties can be met, cascaded structure based Strong PUFs can indeed be made machine learning (ML) attack resistant against known ML attacks. Next we conduct machine learning experiments on an abstract PUF model using Support Vector Machines, Logistic Regression, Bagging, Boosting and Evolutionary techniques to establish criteria for machine learning resistant Strong PUF design. This paper does not suggest how to harvest the process variation, which remains within the purview of a circuit designer; rather it suggests what properties of the building blocks to aim for towards building a machine learning resistant Strong PUF - thus paving the path for a systematic design approach.","Resistance,
Support vector machines,
Integrated circuit modeling,
Bagging,
Boosting,
Machine learning algorithms,
Logistics"
Machine learning techniques for cognitive decision making,"Machine learning algorithms in cognitive computing for decision making can help out how to achieve significant solutions by generalizing a learned model from environmental pattern instances. This technique is frequently practicable and economical where manual rigid rule based abstract programming is not suitable. As more training input patterns are obtainable, better-determined tasks can be attempted. As a result, machine learning is extensively used in cognitive computing and artificial intelligence for handling structured, unstructured and multimedia big data. However, evolving fruitful machine learning cognitive applications involves a considerable extent of concept that is not available in general theories. This paper will analyze primary modules of machine learning approaches and attempt to present friendly real world example of cognitive teacher appraisal. The first section will sightsee the meaning of machine learning, deliberate the cognitive abilities it can generate. The second and third section discusses the practical procedure and issues for solving cognitive problems. The next Five section will define concept of machine learning methods and its application problem domain. The last section shows comparison of machine learning algorithm capability and limitations.","Decision making,
Training,
Machine learning algorithms,
Testing,
Appraisal,
Prototypes"
Privacy preserving extreme learning machine classification model for distributed systems,"Machine learning based classification methods are widely used to analyze large scale datasets in this age of big data. Extreme learning machine (ELM) classification algorithm is a relatively new method based on generalized single-layer feedforward network structure. Traditional ELM learning algorithm implicitly assumes complete access to whole data set. This is a major privacy concern in most of cases. Sharing of private data (i.e. medical records) is prevented because of security concerns. In this research, we proposed an efficient and secure privacy-preserving learning algorithm for ELM classification over data that is vertically partitioned among several parties. The new learning method preserves the privacy on numerical attributes, builds a classification model without sharing private data without disclosing the data of each party to others.","Sonar,
Privacy,
Ionosphere,
Breast cancer,
Data privacy,
Big data"
Cardiotocography signals with artificial neural network and extreme learning machine,"Cardiotocography (CTG) is a monitoring technique that is used routinely during pregnancy and labor to assess fetal well-being. CTG consists of two signals which are fetal heart rate (FHR) and uterine contraction (UC). Twenty-one features representing the characteristic of FHR have been used in this work. The features are obtained from a large dataset consisting of 2126 records in UCI Machine Learning Repository. The prominent features, such as baseline, the number of acceleration and deceleration patterns, and variability recommended by International Federation of Gynecology and Obstetrics (FIGO) have also taken into account during CTG analysis. The features were applied as the input to feedforward neural network (ANN) and Extreme Learning Machine (ELM) to classify FHR patterns in this study. FHR is recently divided into three classes as normal, suspicious and pathological. According to the results of this study, the accuracy of classification of ANN and ELM were obtained as 91.84% and 93.42%, respectively.",
A machine learning based spectrum-sensing algorithm using sample covariance matrix,"In this paper, we propose a machine learning based spectrum sensing method using the sample covariance matrix of the received signal vector from multiple antennas. Before sensing, the cognitive radio (CR) will first apply the unsupervised learning algorithm (e.g., K-means Clustering) to discover primary user's (PU) transmission patterns. Then, the supervised learning algorithm (e.g., Support Vector Machine) is used to train CR to distinguish PU's status. These two learning phases are implemented using the feature vector that is formed by two parameters of the sample covariance matrix. One parameter is the ratio between the maximum eigenvalue and the minimum eigenvalue; the other is the ratio between the absolute sum of all matrix elements and absolute sum of the diagonal elements. The proposed method does not need any information about the signal, channel, and the noise power a priori. Simulations clearly demonstrate the effectiveness of the proposed method.","Sensors,
Covariance matrices,
Antennas,
Support vector machines,
Eigenvalues and eigenfunctions,
Clustering algorithms,
Machine learning algorithms"
File size estimation in JPEG XR standard using machine learning,"Although JPEG XR was developed later than JPEG2000, it has not the ability to compress to a certain size unlike JPEG2000. In this study, a machine learning algorithm is proposed in order to bring this feature to JPEG XR method. The results show that, the file size estimation algorithm gives accurate results under the circumstances of giving specific range to compression ratio.","Transform coding,
Image coding,
Machine learning algorithms,
Discrete cosine transforms,
IEC Standards,
ISO Standards,
Estimation"
Kemy: An AQM generator based on machine learning,"With the explosion of multimedia applications, the network QoS is facing a set of challenges especially in congestion control. Active queue management(AQM), which plays an important role in network congestion control, has been proved necessary for decades. Recently, as the widespread bufferbloat being exposed, AQM has been paid more and more attention. However, traditional manually designed AQMs still exist some problems especially in parameter-tuning and scenarios adaption. Instead of designing a perfect AQM for all scenarios, which is nearly impossible, we try to make the computer generate an AQM for the scenario specified by users. We've developed a program called Kemy based on off-line machine learning technologies. The Kemy-generated AQM is evaluated in various scenarios and achieves the goals of solving bufferbloat problem. Compared to some representative human-designed AQMs, Kemy-generated AQM performs even better in some cases.","Computers,
Bandwidth,
Protocols,
Delays,
Topology,
Quality of service,
Tuning"
Performance analysis of machine learning techniques in intrusion detection,"With computer and Internet to be an indispensable part of our daily lives, the number of Web applications on the Internet has increased rapidly. With the increasing number of Web applications, attacks on the disclosure of data on the internet and the number of varieties has increased. Made over the Web attacks and to detect unauthorized access requests, intrusion detection systems have been used successfully. In this study, In order to develop a more efficient STS, machine learning techniques, Bayesian networks, support vector machines, neural networks, k nearest neighbor algorithm and decision trees examined the success of the STS, the success and process time of the classifier according to the types of attacks have been analyzed. Kddcup99 data sets were used in experimental studies.","Probes,
Intrusion detection,
Internet,
Computers,
Neural networks,
Bayes methods,
Principal component analysis"
Workload management for cloud databases via machine learning,"As elastic IaaS clouds continue to become more cost efficient than on-site datacenters, a wide range of data management applications are migrating to pay-as-you-go cloud computing resources. These diverse applications come with an equally diverse set of performance goals, resource demands, and budget constraints. While existing research has tackled individual tasks such as query placement, scheduling, and resource provisioning to meet these goals and constraints, these techniques fail to provide end-to-end customizable workload management solutions, leading application developers to hand-craft custom heuristics that fit their workload specifications and performance goals. In this vision paper, we argue that workload management challenges can be addressed by leveraging machine learning algorithms. These algorithms can be trained on application-specific properties and performance metrics to automatically learn how to provision resources as well as distribute and schedule the execution of incoming query workloads. Towards this goal, we sketch our vision of WiSeDB, a learning-based service that relies on supervised and reinforcement learning to generate workload management strategies for both static and dynamic workloads.","Decision trees,
Measurement,
Schedules,
Cloud computing,
Feature extraction,
Learning (artificial intelligence),
Supervised learning"
CryptoML: Secure outsourcing of big data machine learning applications,"We present CryptoML, the first practical framework for provably secure and efficient delegation of a wide range of contemporary matrix-based machine learning (ML) applications on massive datasets. In CryptoML a delegating client with memory and computational resource constraints wishes to assign the storage and ML-related computations to the cloud servers, while preserving the privacy of its data. We first suggest the dominant components of delegation performance cost, and create a matrix sketching technique that aims at minimizing the cost by data pre-processing. We then propose a novel interactive delegation protocol based on the provably secure Shamir's secret sharing. The protocol is customized for our new sketching technique to maximize the client's resource efficiency. CryptoML shows a new trade-off between the efficiency of secure delegation and the accuracy of the ML task. Proof of concept evaluations corroborate applicability of CryptoML to datasets with billions of non-zero records.","Cryptography,
Protocols,
Approximation algorithms,
Sparse matrices,
Decision support systems,
Algorithm design and analysis"
Smart city planning by estimating energy efficiency of buildings by extreme learning machine,"Estimation of energy efficiency is one of the major issues in smart city planning. Although, there are some papers about estimation of energy efficiency of the buildings, there is still a requirement of an effective method that can be used in all climatic zones. Therefore, extreme learning method (ELM), which is a training method for single hidden layer neural network, was employed in the dataset that contains the properties of buildings such as shape, area and height and cooling and heating loads were calculated. Achieved results by ELM were compared with the results in the literature and the results obtained by some popular machine learning methods such as artificial neural network, linear regression, and etc. Obtained results by ELM found acceptable.","Buildings,
Energy efficiency,
Cooling,
Heating,
Mathematical model,
Learning systems,
Training"
Predicting and analyzing water quality using Machine Learning: A comprehensive model,"The deteriorating quality of natural water resources like lakes, streams and estuaries, is one of the direst and most worrisome issues faced by humanity. The effects of un-clean water are far-reaching, impacting every aspect of life. Therefore, management of water resources is very crucial in order to optimize the quality of water. The effects of water contamination can be tackled efficiently if data is analyzed and water quality is predicted beforehand. This issue has been addressed in many previous researches, however, more work needs to be done in terms of effectiveness, reliability, accuracy as well as usability of the current water quality management methodologies. The goal of this study is to develop a water quality prediction model with the help of water quality factors using Artificial Neural Network (ANN) and time-series analysis. This research uses the water quality historical data of the year of 2014, with 6-minutes time interval. Data is obtained from the United States Geological Survey (USGS) online resource called National Water Information System (NWIS). For this paper, the data includes the measurements of 4 parameters which affect and influence water quality. For the purpose of evaluating the performance of model, the performance evaluation measures used are Mean-Squared Error (MSE), Root Mean-Squared Error (RMSE) and Regression Analysis. Previous works about Water Quality prediction have also been analyzed and future improvements have been proposed in this paper.",
Student research highlight: Secure and resilient distributed machine learning under adversarial environments,"Machine learning algorithms, such as support vector machines (SVMs), neutral networks, and decision trees (DTs) have been widely used in data processing for estimation and detection. They can be used to classify samples based on a model built from training data. However, under the assumption that training and testing samples come from the same natural distribution, an attacker who can generate or modify training data will lead to misclassification or misestimation. For example, a spam filter will fail to recognize input spam messages after training crafted data provided by attackers [1].","Machine learning,
Security,
Machine learning algorithms,
Game theory,
Support vector machines,
Computers,
Linear programming"
A co-learning system for humans and machines,"Artificial intelligence systems are frequently used to solve various problems in our daily lives. However, these systems require problem-specific big data to facilitate their learning processes. Unfortunately, for unknown environments, there are no previous instances available for learning. To support such learning in unknown environments, we propose a novel hybrid learning system that facilitates collaborative learning between humans and artificial intelligence systems. In this study, we verified that the proposed system accelerated the both human and machine learning by employing a simplified color design task.","Psychology,
Image color analysis"
Automatic detection of neurons in high-content microscope images using machine learning approaches,"The study of neuronal cell morphology and function in relation to neurological disease processes is of high importance for developing suitable drugs and therapies. To accelerate discovery, biological experiments for this purpose are increasingly scaled up using high-content screening, resulting in vast amounts of image data. For the analysis of these data fully automatic methods are needed. The first step in this process is the detection of neuron regions in the high-content images. In this paper we investigate the potential of two machine-learning based detection approaches based on different feature sets and classifiers and we compare their performance to an alternative method based on hysteresis thresholding. The experimental results indicate that with the right feature set and training procedure, machine-learning based methods may yield superior detection performance.","Neurons,
Training,
Detectors,
Microscopy,
Morphology,
Feature extraction,
Drugs"
Gas Source Parameter Estimation Using Machine Learning in WSNs,"This paper introduces an original clusterized framework for the detection and estimation of the parameters of multiple gas sources in wireless sensor networks. The proposed method consists of defining a kernel-based detector that can detect gas releases within the network's clusters using concentration measures collected regularly from the network. Then, we define two kernel-based models that accurately estimate the gas release parameters, such as the sources locations and their release rates, using the collected concentrations.","Estimation,
Wireless sensor networks,
Detectors,
Pollution measurement,
Explosions,
Area measurement"
FASP: A machine learning approach to functional astrocyte phenotyping from time-lapse calcium imaging data,"We propose a machine learning approach to characterize the functional status of astrocytes, the most abundant cells in human brain, based on time-lapse Ca2+ imaging data. The interest in analyzing astrocyte Ca2+ dynamics is evoked by recent discoveries that astrocytes play proactive regulatory roles in neural information processing, and is enabled by recent technical advances in modern microscopy and ultrasensitive genetically encoded Ca2+ indicators. However, current analysis relies on eyeballing the time-lapse imaging data and manually drawing regions of interest, which not only limits the analysis throughput but also at risk to miss important information encoded in the big complex dynamic data. Thus, there is an increased demand to develop sophisticated tools to dissect Ca2+ signaling in astrocytes, which is challenging due to the complex nature of Ca2+ signaling and low signal to noise ratio. We develop Functional AStrocyte Phenotyping (FASP) to automatically detect functionally independent units (FIUs) and extract the corresponding characteristic curves in an integrated way. FASP is data-driven and probabilistically principled, flexibly accounts for complex patterns and accurately controls false discovery rates. We demonstrate the effectiveness of FASP on both synthetic and real data sets.","Correlation,
Imaging,
Probabilistic logic,
Standards,
Algorithm design and analysis,
Testing,
Calcium"
Machine learning classification of complex vasculature structures from in-vivo bone marrow 3D data,"Blood vessels inside the bone marrow (BM) play a vital role in the maintenance of hematopoietic stem cell (HSCs). Investigating the interaction of HSCs relative to vasculature has become the main headline for many recent studies. Advances in microscopy and image analysis using mouse models have allowed detection, identification and automated quantification of HSCs alongside their vascular niche. This resulted in new hypotheses concerning the activation state of HSCs adjacent to different blood vessel types (for example sinusoids vs. arterioles). Identifying the different types of BM vasculature has become critically important, however it still requires the use of complex immunostainings ex vivo or transgenic reporter mouse lines in vivo. To eliminate these requirements and increase the throughput of studies focusing on the HSC niche, we present a machine learning classification approach based on the Decision Tree Classifier to classify different regions of bone marrow vasculature into four distinct classes based on their discriminative features.","Blood vessels,
Biomedical imaging,
Training,
Decision trees,
Bones,
Bifurcation,
Three-dimensional displays"
Credit modelling using hybrid machine learning technique,"Credit evaluation models are the important tools used by banks for the evaluation of loan customers as good or bad. These models are developed as a part of data mining projects using mainly the Classification and Clustering tasks. Their accuracy plays a very significant role as they are the backbones behind the important decisions taken by banks. The accuracy can be improved by using many factors, some of these are the use of good machine learning techniques, balanced input data, and using hybrid techniques in model development. The machine learning and statistical techniques can be combined in various ways for creating the effective hybrid models. In this paper the input data has been balanced to avoid biased model training towards the larger class. Machine learning techniques which have been proved successful in many experiments on financial data are used for this study. The machine learning techniques used are: Naïve Bayes, MLP, RBF, Logistic Regression and C4.5. First single models have been developed using these machine learning techniques and the one with highest accuracy has been found. Then this model was hybridized with others for improving the classification accuracy. The accuracy of all these models was tested on a separate test set that has not been shown to the model while training. A bench marked credit dataset has been utilized for conducting the experiments. The results of the single and hybrid models shows that the MLP outperformed all other individual models while the hybrid model developed by combining the MLP with MLP gave the best results.","Data models,
Logistics,
Training,
Decision trees,
Data mining,
Radial basis function networks,
Predictive models"
Ranking of machine learning algorithms based on the performance in classifying DDoS attacks,"Network Security has become one of the most important factors to consider as the Internet evolves. The most important attack which affects the availability of service is Distributed Denial of Service. The service disruption may cause substantial financial loss as well as damage to the concerned network system. The traffic patterns exhibited by the DDoS affected traffic can be effectively captured by machine learning algorithms. This paper gives an evaluation and ranking of some of the supervised machine learning algorithms with the aim of reducing type I and type II errors, increasing precision and recall while maintaining detection accuracy. The performance evaluation is done using Multi Criteria Decision Aid software called Visual PROMETHEE. This work demonstrates the effectiveness of ensemble based classifiers especially the ensemble algorithm of Adaboost with Random Forest as the base classifier. Publicly available datasets such as DARPA scenario specific dataset, CAIDA DDoS Attack 2007 and CAIDA Conficker are used to evaluate the algorithms.","Computer crime,
Machine learning algorithms,
Feature extraction,
Classification algorithms,
Intrusion detection,
Internet"
"Brain tumor segmentation approaches: Review, analysis and anticipated solutions in machine learning","Brain tumor is one of the most rigorous diseases in the medical science. An effective and efficient analysis is always a key concern for the radiologists in the premature phase of tumor growth. At first sight of the imaging modality like in Magnetic Resonance (MR) imaging, the proper visualization of the tumor cells and its differentiation with its nearby soft tissues is somewhat difficult task. The reason for the above problem is the presence of the low illumination in imaging modalities. One of the solutions of such problem is deal by using machine learning based system diagnosis. In past various segmentation methods had been applied on brain MR imaging system to figure out the proper abnormality region from overall volume of the brain. In this paper a decade survey analysis is presented for all such approaches which are used in machine learning system for tumor segmentation. Further, the paper presents the limitations and advantages of all such approaches in machine learning based diagnosis. At last, the comparative segmentation results are discussed with certain clustering performance measures to analyse the effectiveness of each algorithm.","Tumors,
Image segmentation,
Clustering algorithms,
Partitioning algorithms,
Image edge detection,
Imaging,
Prediction algorithms"
Automatic protocol feature word construction based on machine learning,"Automatic protocol reverse engineering for application protocol is becoming more and more important for many applications such as application protocol analyzer, penetration testing, intrusion prevention and detection. Unfortunately, many techniques for extracting the protocol message format specifications of unknown applications often have some limitations for few priori information or the time-consuming problem. Protocol feature words are byte subsequences within traffic payload that could help distinguish application protocols. In this paper, a new approach is proposed for extracting the protocol message format specifications of unknown applications which is based on the Latent Dirichlet Allocation (LDA) model and Huffman Tree Support Vector Machine (HT-SVM). Firstly, the key words are extracted by utilizing the LDA model, which is a kind of machine learning in document library to extract the theme structure named topic. Secondly, the HT-SVM method is applied to constructing the feature words based on the above process. The proposed approach is implemented and evaluated to infer message format specifications of SMTP binary protocol. Experimental results show that the approach accurately parses and infers SMTP protocol with highly recall rate.","Protocols,
Artificial neural networks,
Support vector machines"
Controlled automatic query expansion based on a new method arisen in machine learning for detection of semantic relationships between terms,"With the proliferation of textual data on the web, efficient access to relevant information to meet the user's needs has become an important problem in the information retrieval tasks. This problem is specially due to the short queries submitted usually by users to an information retrieval system to describe their needs. These systems have to complete the user needs with related terms in order to disambiguate the user query and better meet the user's needs. This paper presents a new method to define semantic relationships between terms of the relevant returned documents for a given query in order to improve the description of the user's needs, by expanding automatically the original query with related terms, and to improve the search results. Some experiments have been performed on the CLEF 2014 collection to show the effectiveness of our method.","Integrated circuits,
Nitrogen,
Manuals,
Indexes"
Rebooting Computers as Learning Machines,"Artificial neural networks could become the technological driver that replaces Moore's law, boosting computers' utlity through a process akin to automatic programming--although physics and computer architecture would also factor in.","Artificial neural networks,
Logic gates,
Computers,
Cancer,
Neural networks,
Computational modeling,
Moore's Law"
Distributed forests for MapReduce-based machine learning,"This paper proposes a novel method for training random forests with big data on MapReduce clusters. Random forests are well suited for parallel distributed systems, since they are composed of multiple decision trees and every decision tree can be independently trained by ensemble learning methods. However, naive implementation of random forests on distributed systems easily overfits the training data, yielding poor classification performances. This is because each cluster node can have access to only a small fraction of the training data. The proposed method tackles this problem by introducing the following three steps. (1) ""Shared forests"" are built in advance on the master node and shared with all the cluster nodes. (2) With the help of transfer learning, the shared forests are adapted to the training data placed on each cluster node. (3) The adapted forests on every cluster node are returned to the master node, and irrelevant trees yielding poor classification performances are removed to form the final forests. Experimental results show that our proposed method for MapReduce clusters can quickly learn random forests without any sacrifice of classification performance.","Training data,
Training,
Decision trees,
Distributed databases,
Vegetation,
Computer architecture,
Learning systems"
Monitoring Land-Cover Changes: A Machine-Learning Perspective,"Monitoring land-cover changes is of prime importance for the effective planning and management of critical, natural and man-made resources. The growing availability of remote sensing data provides ample opportunities for monitoring land-cover changes on a global scale using machine-learning techniques. However, remote sensing data sets exhibit unique domain-specific properties that limit the usefulness of traditional machine-learning methods. This article presents a brief overview of these challenges from the perspective of machine learning and discusses some of the recent advances in machine learning that are relevant for addressing them. These approaches show promise for future research in the detection of land-cover change using machine-learning algorithms.","Remote sensing,
Data models,
Machine learning,
Spatial resolution,
Monitoring,
Training,
Land cover monitoring"
Ground-based image analysis: A tutorial on machine-learning techniques and applications,"Ground-based whole-sky cameras have opened up new opportunities for monitoring the earth's atmosphere. These cameras are an important complement to satellite images by providing geoscientists with cheaper, faster, and more localized data. The images captured by whole-sky imagers (WSI) can have high spatial and temporal resolution, which is an important prerequisite for applications such as solar energy modeling, cloud attenuation analysis, local weather prediction, and more. Extracting the valuable information from the huge amount of image data by detecting and analyzing the various entities in these images is challenging. However, powerful machine-learning techniques have become available to aid with the image analysis. This article provides a detailed explanation of recent developments in these techniques and their applications in ground-based imaging, aiming to bridge the gap between computer vision and remote sensing with the help of illustrative examples. We demonstrate the advantages of using machine-learning techniques in ground-based image analysis via three primary applications: segmentation, classification, and denoising.","Remote sensing,
Feature extraction,
Cameras,
Detectors,
Satellites,
Monitoring,
Spatial resolution,
Tutorials"
Efficient classification of Parkinson's disease using extreme learning machine and hybrid particle swarm optimization,"One of the most well-known problems in machine learning framework is classification of Parkinson's Disease (PD) to patient people and healthy people. Due to the importance of that problem, utilization of a novel learning method is necessary. For this purpose, this paper proposes the utilization of Extreme Learning Machine (ELM) as a type of feed-forward neural network with a single hidden layer to classify the PD patients. However ELM is known as the one of the fast and accurate learning methods, selection of relevant feature elements of PD dataset can be effective on improving the classification performance of ELM. To this end, this paper proposes Hybrid Particle Swarm Optimization (PSO) as the second innovation to efficiently select the relevant feature elements. The main advantage of Hybrid PSO is locally improving of particles in order to jump over the local optimum solution and quickly converging to the global optimal solution. Evaluation of the proposed method on PD dataset proves the superiority of the propose method on the problem of PD classification, in comparison to the other learning methods.","Artificial neural networks,
Particle swarm optimization,
Support vector machines,
Optimization,
Parkinson's disease,
Feature extraction"
The classification of breast cancer with Machine Learning Techniques,"In this study, it is aimed to classify breast cancer data attained from UCI(University of California-Irvine), Machine Learning Laboratory with some Machine Learning Techniques. With this aim, clustering performance of some distance measures in Matlab© has been compared, using breast cancer data. Later without using any pre-processing, some of the machine learning techniques are used for the clustering breast cancer data, using WEKA data mining software©. As a result, it has been seen that distance measures effects the clustering performance nearly 12 percentage and the success of the classification varies from %45 to %79, according to the methods.","Breast cancer,
MATLAB,
Conferences,
Support vector machines,
Expert systems,
Computers"
Prediction of liver fibrosis stages by machine learning model: A decision tree approach,"Using Information systems and strategic tools for medical domains is constantly growing. Automated medical models play an important role in medical decision-making, helping physicians to provide a fast and accurate diagnosis or even prediction. Making use of the knowledge or even in the early stages of knowledge acquisition, different statistical mining and machine learning tools can be used. For instance predicting whether the patient with Hepatitis C virus has also liver fibrosis or not is one of the concerns. In case the prediction result is true, in what stage is the fibrosis. To easily reach to this knowledge without costly diagnostic routine laboratory tests there should be a fully integrated system. Therefore in this study we used machine learning technique model based on decision tree classifier to predict individuals' liver fibrosis degree. Results showed that by using decision tree classifier accuracy is 93.7% which is higher range than what is reported in current researches with similar conditions.","Decision trees,
Liver,
Medical diagnostic imaging,
Biomarkers,
Biochemistry,
Predictive models"
Reducing Power Consumption in Data Center by Predicting Temperature Distribution and Air Conditioner Efficiency with Machine Learning,"To reduce the power consumption in data centers, the coordinated control of the air conditioner and the servers is required. It takes tens of minutes for changes of operational parameters of air conditioners including outlet air temperature and volume to be reflected in the temperature distribution in the whole data center. So, the proactive control of the air conditioners is required according to the prediction temperature distribution corresponding to the load on the servers. In this paper, the temperature distribution and the power efficiency of air conditioner were predicted by using a machine-learning technique, and also we propose a method to follow-up proactive control of the air conditioner under the predicted optimum condition. Consequently, by the follow-up proactive control of the air conditioner and the load of servers, power consumption reduction of 30% at maximum was demonstrated.","Temperature sensors,
Power demand,
Temperature distribution,
Servers,
Data models,
Atmospheric modeling,
Temperature control"
Supervised machine learning for document analysis and prediction,"What if the data gets bigger and bigger? What if handling such huge amount of data started to be critically irritating and need much more attention? These questions became very concerning nowadays. Several organizations and industrial businesses are in need of information system and strategic organizational tool to easily handle huge data and learn the behavior of these data. In this study we proposed a model that is based on Supervised Machine learning to measure, evaluate and learn the similarity of attributes within documents. The documents are in the form of business plan executive summary that consist of several attributes that are used as parameters for evaluation. Results showed that by using similarity learning, attributes within the business plan documents are rated and furthermore the overall documents are ranked showing the effective correlation and association between attributes.","Measurement,
Ontologies,
Organizations,
Mathematical model,
Decision trees"
A PSO-based weighting method to enhance machine learning techniques for cooperative spectrum sensing in CR networks,"Cognitive radio (CR) is a recent technology to tackle the problem of radio spectrum scarcity. Successful spectrum sensing is fundamental in performance of CR networks; hence, a PSO-based weighting method is proposed in order to improve the functionality of machine learning techniques which are used with the aim of detecting the activity of secondary users in cooperative cognitive radio (CCR) networks. Regarding classification methods, three supervised classifiers which are supported vector machines (SVM), K-nearest neighbors (K-NN) and naïve Bayes are used for pattern classification. Since our goal is spectrum sensing in CCR networks, the vector of energy levels in radio channel which is considered as a feature vector is fed into the classifier to determine the availability of the channel. The classifier labels each feature vector as two classes: the ""channel available class"" or the ""channel unavailable class"". In our proposed method, first, the three mentioned classifiers go through a training phase. Next, for new feature vectors, a label is assigned to the feature vector by each classifier and the final decision about the availability of the channel is made by a weighted voting method based on the PSO algorithm in an online fashion. The performance of our technique is measured in terms of the classification error. Also, the comparative results show twofold merit over previous methods since it not only reduces the error rate but also decreases the error of the channel available class.","Sensors,
Support vector machines,
Energy states,
Cognitive radio,
Training,
Cascading style sheets,
Error analysis"
Machine learning approach to recognize subject based sentiment values of reviews,"Due to the increase in the number of people participating online on reviewing travel related entities such as hotels, cities and attractions, there is a rich corpus of textual information available online. However, to make a decision on a certain entity, one has to read many such reviews manually, which is inconvenient. To make sense of the reviews, the essential first step is to understand the semantics that lie therein. This paper discusses a system that uses machine learning based classifiers to label the entities found in text into semantic concepts defined in an ontology. A subject classifier with a precision of 0.785 and a sentiment classifier with a correlation coefficient of 0.9423 was developed providing sufficient accuracy for subject categorization and sentiment evaluation in the proposed system.","Training,
Ontologies,
Sentiment analysis,
Poles and towers,
Context,
Training data,
Support vector machines"
Reducing computational time of closed-loop weather monitoring: A Complex Event Processing and Machine Learning based approach,"Modern weather forecasting models are developed to maximize the accuracy of forecasts by running computationally intensive algorithms with vast volumes of data. Consequently, algorithms take a long time to execute, and it may adversely affect the timeliness of forecast. One solution to this problem is to run the complex weather forecasting models only on the potentially hazardous events, which are pre-identified by a lightweight data filtering algorithm. We propose a Complex Event Processing (CEP) and Machine Learning (ML) based weather monitoring framework using open source resources that can be extended and customized according to the users' requirements. The CEP engine continuously filters out the input weather data stream to identify potentially hazardous weather events, and then generates a rough boundary enclosing all the data points within the Areas of Interest (AOI). Filtered data points are then fed to the machine learner, where the rough boundary gets more refined by clustering it into a set of AOIs. Each cluster is then concurrently processed by complex weather algorithms of the WRF model. This reduces the computational time by ~75%, as resource heavy weather algorithms are executed using a small subset of data that corresponds to only the areas with potentially hazardous weather.","Weather forecasting,
Engines,
Indexes,
Filtering algorithms,
Clustering algorithms,
Monitoring"
Fuel consumption prediction of fleet vehicles using Machine Learning: A comparative study,"Ability to model and predict the fuel consumption is vital in enhancing fuel economy of vehicles and preventing fraudulent activities in fleet management. Fuel consumption of a vehicle depends on several internal factors such as distance, load, vehicle characteristics, and driver behavior, as well as external factors such as road conditions, traffic, and weather. However, not all these factors may be measured or available for the fuel consumption analysis. We consider a case where only a subset of the aforementioned factors is available as a multi-variate time series from a long distance, public bus. Hence, the challenge is to model and/or predict the fuel consumption only with the available data, while still indirectly capturing as much as influences from other internal and external factors. Machine Learning (ML) is suitable in such analysis, as the model can be developed by learning the patterns in data. In this paper, we compare the predictive ability of three ML techniques in predicting the fuel consumption of the bus, given all available parameters as a time series. Based on the analysis, it can be concluded that the random forest technique produces a more accurate prediction compared to both the gradient boosting and neural networks.","Fuels,
Predictive models,
Vehicles,
Data models,
Radio frequency,
Boosting,
Prediction algorithms"
Data linearity using Kernel PCA with Performance Evaluation of Random Forest for training data: A machine learning approach,"In this study, Kernel Principal Component Analysis is applied to understand and visualize non-linear variation patterns by inverse mapping the projected data from a high-dimensional feature space back to the original input space. Performance Evaluation of Random Forest on various data sets has been compared to understand accuracy and various statistical measures of interest.","Vegetation,
Training,
Kernel,
Principal component analysis,
Computational modeling,
Eigenvalues and eigenfunctions,
Computers"
Dot-product engine as computing memory to accelerate machine learning algorithms,"Currently, intense work is underway to develop memristor crossbar arrays for high density, nonvolatile memory applications. However, another capability of memristor crossbars - natural dot-product operation for vectors and matrices - holds even greater potential for next-generation computing, including accelerators, neuromorphic computing, and heterogeneous computing. In this paper, we present a dot-product engine (DPE) based on memristor crossbars optimized for dense matrix computation, which is dominated in most machine learning algorithms. We explored multiple methods to enhance DPE's dot-product computing accuracy. Moreover, instead of training crossbars, we try to directly use existing software-trained weight matrices on DPEs so no heroic effort is needed to innovate learning algorithms for new hardware. Our results show that computations utilizing DPEs can achieve 1000 ~ 10000 times better speed-efficiency product comparing to a state-of-art ASIC [1]. And machine learning algorithm utilizing DPEs can easily achieve software-level accuracy on testing. Both experimental demonstrations and data-calibrated circuit simulations are presented to demonstrate the realistic implementation of a memristor crossbar DPE.","Memristors,
Resistance,
Wires,
Programming,
Machine learning algorithms,
Algorithm design and analysis,
Degradation"
Hotspot detection using machine learning,"As technology nodes continue shrinking, lithography hotspot detection has become a challenging task in the design flow. In this work we present a hybrid technique using pattern matching and machine learning engines for hotspot detection. In the training phase, we propose sampling techniques to correct for the hotspot/non-hotspot imbalance to improve the accuracy of the trained Support Vector Machine (SVM) system. In the detection phase, we have combined topological clustering and a novel pattern encoding technique based on pattern regularity to enhance the predictability of the system. Using the ICCAD 2012 benchmark data, our approach shows an accuracy of 88% in detecting hotspots with hit-to-extra ratio of 0.12 which are better results compared to other published techniques using the same benchmark data.","Layout,
Training,
Support vector machines,
Pattern matching,
Encoding,
Training data,
Kernel"
A machine learning approach to predict episodic memory formation,"Episodic memories constitute the essence of our recollections and are formed by autobiographical experiences and contextual knowledge. Memories are rich and detailed, yet at the same time they can be malleable and inaccurate. The contents that end up being remembered are the result of filtering incoming sensory inputs in the context of previous knowledge. Here we asked whether the quintessentially subjective process of memory construction could be predicted by a supervised machine learning approach based exclusively on content information. We considered audiovisual segments from a movie as a proxy for real-life memory formation and built a quantitative model to explain psychophysics data evaluating recognition memory. The inputs to the model included audiovisual information (e.g. presence of specific characters, objects, voices and sounds), scene information (e.g. location, presence or absence of action) and emotional valence information. The machine-learning model could predict memory formation in single trials both for group averages and individual subjects with an accuracy of up to 80% using solely stimulus content properties. These results provide a quantitative and predictive model that links sensory perception and emotional attributes to memory formation. Furthermore, the results demonstrate that a computational model can make sophisticated inferences about a cognitive process that involves selective filtering and subjective interpretation.","Motion pictures,
Predictive models,
Prediction algorithms,
Computational modeling,
Classification algorithms,
Support vector machines,
Training"
Distributed-neuron-network based machine learning on smart-gateway network towards real-time indoor data analytics,"Indoor data analytics is one typical example of ambient intelligence with behaviour or feature extraction from environmental data. It can be utilized to help improve comfort level in building and room for occupants. To address dynamic ambient change in a large-scaled space, real-time and distributed data analytics is required on sensor (or gateway) network, which however has limited computing resources. This paper proposes a computationally efficient data analytics by distributed-neuron-network (DNN) based machine learning with application for indoor positioning. It is based on one incremental L2-norm based solver for learning collected WiFi-data at each gateway and is further fused for all gateways in the network to determine the location. Experimental results show that with multiple distributed gateways running in parallel, the proposed algorithm can achieve 50x and 38x speedup during data testing and training time respectively with comparable positioning accuracy, when compared to traditional support vector machine (SVM) method.","Logic gates,
Training,
Neurons,
IEEE 802.11 Standard,
Support vector machines,
Training data"
Integrating symbolic and statistical methods for testing intelligent systems: Applications to machine learning and computer vision,"Embedded intelligent systems ranging from tiny implantable biomedical devices to large swarms of autonomous unmanned aerial systems are becoming pervasive in our daily lives. While we depend on the flawless functioning of such intelligent systems, and often take their behavioral correctness and safety for granted, it is notoriously difficult to generate test cases that expose subtle errors in the implementations of machine learning algorithms. Hence, the validation of intelligent systems is usually achieved by studying their behavior on representative data sets, using methods such as cross-validation and bootstrapping. In this paper, we present a new testing methodology for studying the correctness of intelligent systems. Our approach uses symbolic decision procedures coupled with statistical hypothesis testing to validate machine learning algorithms. We show how we have employed our technique to successfully identify subtle bugs (such as bit flips) in implementations of the k-means algorithm. Such errors are not readily detected by standard validation methods such as randomized testing. We also use our algorithm to analyze the robustness of a human detection algorithm built using the OpenCV open-source computer vision library. We show that the human detection implementation can fail to detect humans in perturbed video frames even when the perturbations are so small that the corresponding frames look identical to the naked eye.","Machine learning algorithms,
Intelligent systems,
Testing,
Measurement,
Computer vision,
Probabilistic logic,
Algorithm design and analysis"
Probabilistic Error Models for machine learning kernels implemented on stochastic nanoscale fabrics,"Presented in this paper are probabilistic error models for machine learning kernels implemented on low-SNR circuit fabrics where errors arise due to voltage overscaling (VOS), process variations, or defects. Four different variants of the additive error model are proposed that describe the error probability mass function (PMF): additive over Reals Error Model with independent Bernoulli RVs (REM-i), additive over Reals Error Model with joint Bernoulli random variables (RVs) (REM-j), additive over Galois field Error Model with independent Bernoulli RVs (GEM-i), and additive over Galois field Error Model with joint Bernoulli RVs (GEM-j). Analytical expressions for the error PMF is derived. Kernel level model validation is accomplished by comparing the Jensen-Shannon divergence DJS between the modeled PMF and the PMFs obtained via HDL simulations in a commercial 45nm CMOS process of MAC units used in a 2nd order polynomial support vector machine (SVM) to classify data from the UCI machine learning repository. Results indicate that at the MAC unit level, DJS for the GEM-j models are 1-to-2-orders-of-magnitude lower (better) than the REM models for VOS and process variation errors. However, when considering errors due to defects, DJS for REM-j is between 1-to-2-orders-of-magnitude lower than the others. Performance prediction of the SVM using these models indicate that when compared with Monte Carlo with HDL generated error statistics, probability of detection pdet estimated using GEM-j is within 3% for VOS error when the error rate pη ≤ 80%, and within 5% for process variation error when supply voltage Vdd is between 0.3V and 0.7V. In addition, pdet using REM-j is within 2% for defect errors when the defect rate (the percentage of circuit nets subject to stuck-at-faults) psaf is between 10-3 and 0.2.","Integrated circuit modeling,
Computational modeling,
Kernel,
Additives,
Semiconductor device modeling,
Fabrics,
Error analysis"
Adaptive Threshold Non-Pareto Elimination: Re-thinking machine learning for system level design space exploration on FPGAs,"One major bottleneck of the system level OpenCL-to-FPGA design tools is their extremely time consuming synthesis process (including place and route). The design space for a typical OpenCL application contains thousands of possible designs even when considering a small number of design space parameters. It costs months of compute time to synthesize all these possible designs into end-to-end FPGA implementations. Thus, the brute force design space exploration (DSE) is impractical for these design tools. Machine learning is one solution that identifies the valuable Pareto designs by sampling only a small portion of the entire design space. However, most of the existing machine learning frameworks focus on improving the design objective regression accuracy, which is not necessarily suitable for the FPGA DSE task. To address this issue, we propose a novel strategy - Adaptive Threshold Non-Pareto Elimination (ATNE). Instead of focusing on regression accuracy improvement, ATNE focuses on understanding and estimating the inaccuracy. ATNE provides a Pareto identification threshold that adapts to the estimated inaccuracy of the regressor. This adaptive threshold results in a more efficient DSE. For the same prediction quality, ATNE reduces the synthesis complexity by 1.6 - 2.89× (hundreds of synthesis hours) against the other state of the art frameworks for FPGA DSE. In addition, ATNE is capable of identifying the Pareto designs for certain difficult design spaces which the other existing frameworks are incapable of exploring effectively.","Field programmable gate arrays,
Radio frequency,
Linear programming,
Hardware,
Training,
Complexity theory,
Algorithm design and analysis"
Classifier Performance in Primary Somatosensory Cortex Towards Implementation of a Reinforcement Learning Based Brain Machine Interface,Increasingly accurate control of prosthetic limbs has been made possible by a series of advancements in brain machine interface (BMI) control theory. One promising control technique for future BMI applications is reinforcement learning (RL). RL based BMIs require a reinforcing signal to inform the controller whether or not a given movement was intended by the user. This signal has been shown to exist in cortical structures simultaneously used for BMI control. This work evaluates the ability of several common classifiers to detect impending reward delivery within primary somatosensory (S1) cortex during a grip force match to sample task performed by a nonhuman primate. The accuracy of these classifiers was further evaluated over a range of conditions to identify parameters that provide maximum classification accuracy. S1 cortex was found to provide highly accurate classification of the reinforcement signal across many classifiers and a wide variety of data input parameters. The classification accuracy in S1 cortex between rewarding and non-rewarding trials was apparent when the animal was expecting an impending delivery or an impending withholding of reward following trial completion. The high accuracy of classification in S1 cortex can be used to adapt an RL based BMI towards a user's intent. Real-time implementation of these classifiers in an RL based BMI could be used to adapt control of a prosthesis dynamically to match the intent of its user.,"Learning (artificial intelligence),
Brain-computer interfaces,
Decoding,
Neuroscience,
Biomedical engineering,
Force,
Prosthetics"
"Quality of Service timeseries forecasting for Web Services: A machine learning, Genetic Programming-based approach","Today, many software systems and applications are consisted of various services on the Web (Cloud). When selecting services or performing a service operation, a critical criterion is Quality of Service (QoS). Because the actual value of some dynamic QoS attributes could vary with time, there must be an approach that can accurately forecast future QoS value. In this paper, we propose to use a machine learning technique, i.e., Genetic Programming (GP), for the problem. When performing QoS forecasting, the proposed approach employs GP to evolve out a predictor, and then uses it to obtain future QoS forecasts. To test and understand the forecasting performance (accuracy) of the proposed approach, in our experiments with a real-world QoS dataset, we compare our approach with other existing QoS forecasting methods, and then prove and discuss its outperformance.","Forecasting,
Quality of service,
Biological cells,
Sociology,
Statistics,
Predictive models,
Genetic programming"
A machine learning approach for medication adherence monitoring using body-worn sensors,"One of the most important challenges in chronic disease self-management is medication non-adherence, which has irrevocable outcomes. Although many technologies have been developed for medication adherence monitoring, the reliability and cost-effectiveness of these approaches are not well understood to date. This paper presents a medication adherence monitoring system by user-activity tracking based on wrist-band wearable sensors. We develop machine learning algorithms that track wrist motions in real-time and identify medication intake activities. We propose a novel data analysis pipeline to reliably detect medication adherence by examining single-wrist motions. Our system achieves an accuracy of 78.3% in adherence detection without need for medication pillboxes and with only one sensor worn on either of the wrists. The accuracy of our algorithm is only 7.9% lower than a system with two sensors that track motions of both wrists.","Wrist,
Sensors,
Feature extraction,
Training,
Monitoring,
Tracking,
Decision trees"
Internet traffic classification using machine learning approach: Datasets validation issues,"Internet traffic classification is an area of current research interest. The failure of port and payload based classification motivates researchers to head towards a machine learning (ML) approach. However, training and testing dataset validation has not been formally addressed. This paper discusses the problem of ML dataset validation and highlights three training issues to be considered in ML classification. The first issue is when training and testing datasets collected from same or different network characteristics. The second issue considers training dataset classes whose real online traffic classes are not presented. The third issue is the geographic place where the network traffic is captured. Real Internet traffic datasets collected from a campus network are used to study the traffic features and classification accuracy for each validation training issue. The experimental results demonstrate that there are differences in some traffic features such as inter-arrival time when training and testing data were collected from different networks. Furthermore, the experiment of the second issue shows that the online classifier achieved the highest accuracy (92.22%) when the ML classifier was trained by dataset classes which have the same ratio of the real online traffic. For the geographic capturing level, the results indicate that there is a difference in the traffic statistical features when the capturing level is different.","Training,
Testing,
Internet,
Ports (Computers),
World Wide Web,
Classification algorithms,
Inspection"
Machine learning for BCI: towards analysing cognition,"This article discusses machine learning and BCI with a focus on analysing cognition, a topic that has been extensively covered by the author and co-workers in numerous papers and conference papers. Due to the review character of the presentation, a high overlap with the above-mentioned contributions is unavoidable. When analysing cognition, it is often useful to combine information from various modalities (see e.g. Biessmann et al., 2011, Sui et al., 2012). In BCI recently multimodal fusion concepts have received great attention under the label hybrid BCI (Pfurtscheller et al., 2010, Müller-Putz et al. 2015, Dähne et al. 2015, Fazli et al. 2015) or as data analysis technique for extracting (non-) linear relations between data (see e.g. Biessmann et al., 2010, Biessmann et al., 2011, Fazli et al., 2009, 2011, 2012, Dähne et al., 2013, 2014a,b, 2015, Winkler et al. 2015). They are rooted in the modern machine learning and signal processing techniques that are now available for analysing EEG, for decoding mental states etc. (see Müller et al. 2008, Bünau et al. 2009, Tomioka and Müller, 2010, Blankertz et al., 2008, 2011, Lemm et al., 2011, Porbadnigk et al. 2015 for recent reviews and contributions to Machine Learning for BCI, see Samek et al. 2014 for a review on robust methods). Note that fusing information has also been a very common practice in the sciences and engineering (Waltz and Llinas, 1990). The talk will discuss a number of recent contributions from the BBCI group that have helped to broaden the spectrum of applicability for Brain Computer Interfaces and mental state monitoring in particular and for analysis of neuroimaging data in general. I will introduce a novel reliable method for estimating the Hurst exponent, a quantity that has recently become popular for describing network properties and is being used for diagnostic purposes (cf. Blythe et al. 2014). It is applied to estimate and analyse cognitive properties in neurophysiological data from BCI experiments (Samek et al. 2016). Furthermore if time permits I will discuss a recent attractive application of BCI in the context of video coding (Scholler et al. 2012 and Acqualagna et al 2015).","Electroencephalography,
Neuroimaging,
Cognition,
Brain-computer interfaces,
Neuroscience,
Data analysis,
Robustness"
Securing pervasive systems against adversarial machine learning,"Applications and middleware in pervasive systems frequently rely on machine learning to provide adaptivity and customization that results in a seamless user experience despite operating in a dynamic environment. Machine learning algorithms have been shown to be vulnerable to covert, strategic attacks through the manipulation of training data. Machine learning algorithms in pervasive systems frequently train on data that could be manipulated by a malicious 3rd party. In this paper, we present our ongoing work to develop a security mechanism that is designed to work in the dynamic environments of pervasive computing as opposed to traditional security mechanisms that are designed for static environments. Furthermore, we present our modular testing framework that will be used to rapidly compare our work with other security mechanisms, applications and adversarial models.","Training data,
Machine learning algorithms,
Security,
Training,
Testing,
Pervasive computing,
Mathematical model"
A study on the optimization of the uplink period using machine learning in the future IoT network,"In this paper, we propose uplink period optimization algorithm for the future IoT(Internet of Things) network system. The proposed algorithm focuses on minimizing power consumption of device and mitigating the degradation of accuracy due to a variable uplink period. In particular, by using machine learning, the proposed algorithm selects the optimal period which is expected to have the biggest F(T), accuracy divided by power consumption. Simulation results show that the proposed algorithm outperforms the conventional algorithm.","Uplink,
Machine learning algorithms,
Power demand,
Network servers,
Training,
Simulation,
Classification algorithms"
A comparison of feature selection methods for machine learning based automatic malarial cell recognition in wholeslide images,"This paper aims at investigating the best feature selection method for optimized and automated machine learning based detection of malarial parasite in wholeslide images of peripheral blood smears. We do this by extracting samples from the wholeslide images and performing feature extraction. A host of feature selection methods are used to judge the performance of the Support Vector Machine as a binary classifier. For each feature selection method, the Support Vector Machine is trained using the significant features. We perform cross-validation and grid-search for finding the best SVM parameters. The trained SVM is subsequently used to classify known instances of ""Normal"" and ""Infected"" samples that are taken randomly from the wholeslide image but unknown to the SVM. Confusion matrices are generated for each classification performed. Various performance measures of each classification task are reported. We conclude that based on our experiments, the binary SVM classifier yields a superlative accuracy of 95.5% if the feature-selection is based on Kullback-Leibler distance between the two classes.","Support vector machines,
Training,
Kernel,
Diseases,
Feature extraction,
Robustness,
Image resolution"
Machine learning approach for predicting bumps on road,"In today's days, due to increase in number of vehicles the probability of accidents are also increasing. The user should be aware of the road circumstances for safety purpose. Several methods requires installing dedicated hardware in vehicle which are expensive. so we have designed a Smart-phone based method which uses a Accelerometer and GPS sensors to analyze the road conditions. The designed system is called as Bumps Detection System(BDS) which uses Accelerometer for pothole detection and GPS for plotting the location of potholes on Google Map. Drivers will be informed in advance about count of potholes on road. we have assumed some threshold values on z-axis(Experimentally Derived)while designing the system. To justify these threshold values we have used a machine learning approach. The k means clustering algorithm is applied on the training data to build a model. Random forest classifier is used to evaluate this model on the test data for better prediction.","Roads,
Accelerometers,
Sensors,
Smart phones,
Vehicles,
Machine learning algorithms,
Global Positioning System"
False arrhythmia alarm reduction in the intensive care unit using data fusion and machine learning,"With aim of reducing the incidence of false critical arrhythmia alarms in intensive care units, a novel data fusion and machine learning algorithm is presented in this article. The 2015 PhysioNet/Computing in Cardiology Challenge database was used in this present algorithm, with each grouped as an asystole (AS), extreme bradycardia (EB), extreme tachycardia (ET), ventricular tachycardia (VT) or ventricular flutter/fibrillation (VF) arrhythmia alarm. A 10-second segment before the onset of the alarm was truncated from available signals, namely electrocardiogram (ECG), arterial blood pressure (ABP), and/or photoplethysmogram (PPG). By first assessing signal quality of available signals, a robust estimation of beat-to-beat intervals could then be derived. Features in heart rate variability (HRV) analysis and ECG parameters such as temporal statistical parameters, spectral analysis results, wavelet transformation coefficients, and complexity measurement etc were extracted and formed a vector. After feature selection through genetic algorithm (GA), a support vector machine (SVM) model was applied to conduct the classification of alarms for the specific arrhythmia type. The overall true positive rate (TPR) of classification algorithm is 93%, with the true negative rate (TNR) 94%. According to the method of performance evaluation in the 2015 Challenge, this algorithm achieved a gross score of 84.4.","Electrocardiography,
Genetic algorithms,
Support vector machines,
Feature extraction,
Heart rate variability,
Cardiology,
Classification algorithms"
Comparison of some machine learning and statistical algorithms for classification and prediction of human cancer type,"Use of gene expression profile of an animal under a certain disease gives pre-clinical insights for the potential efficacy of novel drugs. Selection of an animal model, accurately resembling the human disease, profoundly reduces the research cost in resources and time. Here, a statistical procedure based on analysis of variance (ANOVA) defined in [1] is investigated to select the animal model that most accurately mimics the human disease in terms of genome-wide gene expression. Two other commonly used data fitting algorithms in machine learning, logistic regression and artificial neural networks are examined and analyzed for the same data set. Implementing procedure of each of these algorithms is discussed and computational cost and advantage and drawback of each algorithm is scrutinized for prediction of pediatric Medulloblastoma cancer type.","Animals,
Analysis of variance,
Measurement,
Cancer,
Gene expression,
Neural networks,
Logistics"
Grading of mammalian cumulus oocyte complexes using machine learning for in vitro embryo culture,"Visual observation of Cumulus Oocyte Complexes provides only limited information about its functional competence, whereas the molecular evaluations methods are cumbersome or costly. Image analysis of mammalian oocytes can provide attractive alternative to address this challenge. However, it is complex, given the huge number of oocytes under inspection and the subjective nature of the features inspected for identification. Supervised machine learning methods like random forest with annotations from expert biologists can make the analysis task standardized and reduces inter-subject variability. We present a semiautomatic framework for predicting the class an oocyte belongs to, based on multi-object parametric segmentation on the acquired microscopic image followed by a feature based classification using random forests.","Feature extraction,
Image segmentation,
In vitro,
Embryo,
Vegetation,
Visualization,
Decision trees"
Machine learning approach for the identification of diabetes retinopathy and its stages,"The effects of the eye abnormalities are mostly gradual in nature which shows the necessity for an accurate abnormality identification system. Abnormality in retina is one among them. Diabetic Retinopathy (DR) is a disease that causes damage to the retina of human eye, which is caused by complications of diabetes. DR is one of the main causes of vision loss and its prevalence keeps rising. Diabetic Retinopathy, a frequent diabetic retinal disease is caused due to the blood vessels in the retina get changes from its original shape. Diabetic Retinopathy generally affects both the human eyes. Most of the ophthalmologists depend on the visual interpretation for the identification of the types of diseases. But, inaccurate diagnosis will change the course of treatment planning which leads to fatal results. Hence, there is a requirement for a bias free automated system which yields highly accurate results. In this paper, we are classifying the various stages of DR. We first present a summary of diabetic retinopathy and its causes. Then, a literature review of the automatic detection of diabetic retinopathy techniques is presented. Explanation and restrictions of retina databases which are used to test the performance of these detection algorithms are given.","Diabetes,
Retinopathy,
Retina,
Feature extraction,
Diseases,
Image segmentation,
Optical imaging"
A customer classification prediction model based on machine learning techniques,"Most of the service providers and product based companies while launching brand new products, services or releasing new versions of existent products need to campaign to reach at the potential customers. While doing so they target their already existing customers who are the ambassadors of their company. To address the existing customers, they maintain the detailed customer data at all levels as customer maser data [9]. In this paper, we have built a prediction model to identify the customers who would most likely respond to the prospective offerings of the company basing on their past purchasing trends. Experiments have been conducted using the well known classifiers, viz., Naïve Bayes, KNN and SVM to classify a bank customer data. Subsequently, we have compared the effectiveness of these techniques and found out which one produces the maximum accuracy for the existing data set.","Classification algorithms,
Data mining,
Support vector machines,
Prediction algorithms,
Decision trees,
Algorithm design and analysis,
Error analysis"
Smartwatch-based activity recognition: A machine learning approach,"Smartwatches and smartphones contain accelerometers and gyroscopes that sense a user's movements, and can help identify the activity a user is performing. Research into smartphone-based activity recognition has exploded over the past few years, but research into smartwatch-based activity recognition is still in its infancy. In this paper we compare smartwatch and smartphone-based activity recognition, and smartwatches are shown to be capable of identifying specialized hand-based activities, such as eating activities, which cannot be effectively recognized using a smartphone (e.g., smartwatches can identify the ""drinking"" activity with 93.3% accuracy while smartphones achieve an accuracy of only 77.3%). Smartwatch-based activity recognition can form the basis of new biomedical and health applications, including applications that automatically track a user's eating habits.","Sensors,
Data models,
Smart phones,
Gyroscopes,
Accelerometers,
Training data,
Radio frequency"
Botnet Domain Name Detection based on machine learning,"Domain Name System (DNS) is a fundamental component of today's Internet: it provides mappings between domain names used by people and the corresponding IP addresses required by network protocols. However, the open and fundamental characteristics of DNS are recently used by the botnet for the communication between bots and C&C. In this paper, we select six kinds of special features of botnet domain querying traffic based on the deep studies of the DNS log. Then three popular classifiers are adopted in order to pick the malicious domains out from the DNS traffic using those features.","telecommunication traffic,
Internet,
invasive software,
IP networks,
learning (artificial intelligence),
pattern classification,
protocols"
Predicting hardware failure using machine learning,"The Weibull distribution has historically been the Reliability Engineer's best tool for describing the probability of failures over time [1]. While this technique is very accurate at describing failure distributions for large populations of components, it works very poorly at predicting the time until failure of an individual component. The mean time until failure is often used to predict times until failure of individual components, but this value may vary greatly with actual times until failure. With the advent of machine learning techniques, the ability to learn from past behavior in order to predict future behavior makes it possible to predict an individual component's time until failure much more accurately. In this paper, we explore the predictive abilities of a machine learning technique to improve upon our ability to predict individual component times until failure in advance of actual failure. Once failure is predicted, an impending problem can be fixed before it actually occurs. This paper brings to light a machine learning approach for predicting individual component times until failure that we will show is far more accurate than the traditional MTBF approach. The algorithm built was able to monitor the health of 14 hardware samples and notify us of an impending failure well ahead of actual failure, providing adequate time to fix the problem before actual failure occurred.","Training,
Machine learning algorithms,
Support vector machines,
Prediction algorithms,
Classification algorithms,
Hardware,
Data models"
Remaining useful life prognostics using pattern-based machine learning,"This paper presents a prognostic methodology that can be implemented in a condition-based maintenance (CBM) program. The methodology estimates the remaining useful life (RUL) of a system by using a pattern-based machine learning and knowledge discovery approach called Logical Analysis of Data (LAD). The LAD approach is based on the exploration of the monitored system's database, and the extraction of useful information which describe the physics that characterize its degradation. The diagnostic information, which is updated each time the new data is gathered, is combined with a non-parametric reliability estimation method, in order to predict the RUL of a monitored system working under different operating conditions. In this paper, the developed methodology is compared to a known CBM prognostic technique; the Cox proportional hazards model (PHM). The methodology has been tested and validated based on the Friedman statistical test. The results of the test indicate that the proposed methodology provides an accurate RUL prediction.","Training,
Estimation,
Monitoring,
Reliability,
Maintenance engineering,
Data mining,
Knowledge discovery"
The scheduling based on machine learning for heterogeneous CPU/GPU systems,"Efficient use all of the available computing devices is an important issue for heterogeneous computing systems. The ability to choose a CPU or GPU processor for a specific task has a positive impact on the performance of GPGPU-systems. It helps to reduce the total processing time and to achieve the uniform system utilization. In this paper, we propose a scheduler that selects the executing device after prior training, based on the size of the input data. The article also contains the plots and time characteristics that demonstrate improvement in overall execution time, depending on the input data. The program modules were developed in C++ using CUDA libraries.","Graphics processing units,
Processor scheduling,
Runtime,
Dynamic scheduling,
Support vector machines,
Central Processing Unit,
Heuristic algorithms"
TABLA: A unified template-based framework for accelerating statistical machine learning,"A growing number of commercial and enterprise systems increasingly rely on compute-intensive Machine Learning (ML) algorithms. While the demand for these compute-intensive applications is growing, the performance benefits from general-purpose platforms are diminishing. Field Programmable Gate Arrays (FPGAs) provide a promising path forward to accommodate the needs of machine learning algorithms and represent an intermediate point between the efficiency of ASICs and the programmability of general-purpose processors. However, acceleration with FPGAs still requires long development cycles and extensive expertise in hardware design. To tackle this challenge, instead of designing an accelerator for a machine learning algorithm, we present TABLA, a framework that generates accelerators for a class of machine learning algorithms. The key is to identify the commonalities across a wide range of machine learning algorithms and utilize this commonality to provide a high-level abstraction for programmers. TABLA leverages the insight that many learning algorithms can be expressed as a stochastic optimization problem. Therefore, learning becomes solving an optimization problem using stochastic gradient descent that minimizes an objective function over the training data. The gradient descent solver is fixed while the objective function changes for different learning algorithms. TABLA provides a template-based framework to accelerate this class of learning algorithms. Therefore, a developer can specify the learning task by only expressing the gradient of the objective function using our high-level language. Tabla then automatically generates the synthesizable implementation of the accelerator for FPGA realization using a set of hand-optimized templates. We use Tabla to generate accelerators for ten different learning tasks targeted at a Xilinx Zynq FPGA platform. We rigorously compare the benefits of FPGA acceleration to multi-core CPUs (ARM Cortex A15 and Xeon E3) and many-core GPUs (Tegra K1, GTX 650 Ti, and Tesla K40) using real hardware measurements. TABLA-generated accelerators provide 19.4x and 2.9x average speedup over the ARM and Xeon processors, respectively. These accelerators provide 17.57x, 20.2x, and 33.4x higher Performance-per-Watt in comparison to Tegra, GTX 650 Ti and Tesla, respectively. These benefits are achieved while the programmers write less than 50 lines of code.","Machine learning algorithms,
Field programmable gate arrays,
Linear programming,
Stochastic processes,
Algorithm design and analysis,
Hardware,
Data models"
Transactional Memory Scheduling Using Machine Learning Techniques,"Current shared memory multi-core systems require powerful software and hardware techniques to support the performance parallel computation and consistency simultaneously. The use of transactional memory results in significant improvement of performance by avoiding thread synchronization and locks overhead. Also, transactions scheduling apparently influences the performance of transactional memory. In this paper, we study the fairness of transactions' scheduling using Lazy Snapshot Algorithm. The fairness of transactions' scheduling aims to balance between transactions types which are read-only and update transactions. Indeed, we support the fairness of the scheduling procedure by a machine learning technique. The machine learning techniques improve the fairness decisions according to transactions' history. The experiments in this paper show that the throughput of the Lazy Snapshot Algorithm is improved with a machine learning support. Indeed, our experiments show that the learning significantly affects the performance if the durations of update transactions are much longer than read-only ones. We also study several machine learning techniques to investigate the fairness decisions accuracy. In fact, K-Nearest Neighbor machine learning technique shows more accuracy and more suitability, for our problem, than Support Vector Machine Model and Hidden Markov Model.","Hidden Markov models,
Support vector machines,
Training,
Scheduling,
Machine learning algorithms,
Throughput,
History"
Memristive Boltzmann machine: A hardware accelerator for combinatorial optimization and deep learning,"The Boltzmann machine is a massively parallel computational model capable of solving a broad class of combinatorial optimization problems. In recent years, it has been successfully applied to training deep machine learning models on massive datasets. High performance implementations of the Boltzmann machine using GPUs, MPI-based HPC clusters, and FPGAs have been proposed in the literature. Regrettably, the required all-to-all communication among the processing units limits the performance of these efforts. This paper examines a new class of hardware accelerators for large-scale combinatorial optimization and deep learning based on memristive Boltzmann machines. A massively parallel, memory-centric hardware accelerator is proposed based on recently developed resistive RAM (RRAM) technology. The proposed accelerator exploits the electrical properties of RRAm to realize in situ, fine-grained parallel computation within memory arrays, thereby eliminating the need for exchanging data between the memory cells and the computational units. Two classical optimization problems, graph partitioning and boolean satisfiability, and a deep belief network application are mapped onto the proposed hardware. As compared to a multicore system, the proposed accelerator achieves 57x higher performance and 25x lower energy with virtually no loss in the quality of the solution to the optimization problems. The memristive accelerator is also compared against an RRAM based processing-in-memory (PIM) system, with respective performance and energy improvements of 6.89x and 5.2x.","Hardware,
Machine learning,
Computational modeling,
Arrays,
Simulated annealing,
Training"
A Machine Learning Approach for the Integration of miRNA-Target Predictions,"Although several computational methods have been developed for predicting interactions between miRNA and target genes, there are substantial differences in the achieved results. For this reason, machine learning approaches are widely used for integrating the predictions obtained from different tools. In this work we adopt a method, called M3GP, which relies on a genetic programming approach, to classify results from three tools: miRanda, TargetScan, and RNAhybrid. Such algorithm is highly parallelizable and its adoption provides great advantages while handling problems involving big datasets, since it is independent from the implementation and from the architecture on which it is executed. More precisely, we apply this technique for the classification of the achieved miRNA target predictions and we compare its results with those obtained with other classifiers.","Genetic programming,
Electronic mail,
RNA,
Genomics,
Sociology,
Statistics,
Niobium"
Anomaly Detection in IPv4 and IPv6 networks using machine learning,"Anomaly Detection is an important requirement to secure a network against the attackers. Detecting attacks within a network by analysing the behaviour pattern has been a significant field of study for several researchers and application systems in IPv4 as well as IPv6 networks. For precise anomaly detection, it is essential to implement and use an efficient data-mining methodology like machine learning. In this paper, we contemplated an anomaly detection model which uses machine learning algorithms for data mining within a network to detect anomalies present at any time. This proposed model is evaluated against Denial of Service (DOS) attacks in both IPv4 and IPv6 networks while selecting the most common and evident features of IPv6 and IPv4 networks for optimizing the detection. The results also show that the proposed system can detect most of the IPv4 and IPv6 attacks in efficient manner.","Intrusion detection,
Protocols,
Training,
Feature extraction,
Machine learning algorithms,
Telecommunication traffic"
Clustering based feature selection using Extreme Learning Machines for text classification,"The expansion of the dynamic Web increases the digital documents, which has attracted many researchers to work in the field of text classification. It is an important and well studied area of machine learning with a variety of modern applications. A good feature selection is of paramount importance to increase the efficiency of the classifiers working on text data. Choosing the most relevant features out of what can be an incredibly large set of data, is particularly important for accurate text classification. This paper is a motivation in that direction where we propose a new clustering based feature selection technique that reduces the feature size. Traditional k-means clustering technique along with TF-IDF and Wordnet helps us to form a quality and reduced feature vector to train the Extreme Learning Machine (ELM) and Multi-layer ELM (ML-ELM) which have been used as the classifiers for text classification. The experimental work has been carried out on 20-Newsgroups and DMOZ datasets. Results on these two standard datasets demonstrate the efficiency of our approach using ELM and ML-ELM as the classifiers over the state-of-the-art classifiers.","Training,
Clustering algorithms,
Frequency measurement,
Support vector machines,
Neural networks,
Computational modeling,
Computer science"
Machine learning-based spectrum decision algorithms for Wireless Sensor Networks,"Wireless Sensor Networks (WSNs) employ Industrial, Scientific and Medical (ISM) spectrum bands for communication, which are overloaded due to various technologies such as WLANs and other WSNs. Therefore, such networks must employ intelligent methods such as Cognitive Radio (CR) to coexist with other networks. This study investigates the use of supervised Machine Learning (ML) for channel selection in WSNs. The proposed models were analyzed using ML tools and techniques, and the best algorithms were evaluated on real sensor nodes. The experiments show performance improvements on the delivery rate and delivery delay when the proposed cognitive solutions are employed.","Wireless sensor networks,
Interference,
Training,
Prediction algorithms,
Algorithm design and analysis,
IEEE 802.11 Standard,
Hardware"
Machine Learning Methods for Binary and Multiclass Classification of Melanoma Thickness From Dermoscopic Images,"Thickness of the melanoma is the most important factor associated with survival in patients with melanoma. It is most commonly reported as a measurement of depth given in millimeters (mm) and computed by means of pathological examination after a biopsy of the suspected lesion. In order to avoid the use of an invasive method in the estimation of the thickness of melanoma before surgery, we propose a computational image analysis system from dermoscopic images. The proposed feature extraction is based on the clinical findings that correlate certain characteristics present in dermoscopic images and tumor depth. Two supervised classification schemes are proposed: a binary classification in which melanomas are classified into thin or thick, and a three-class scheme (thin, intermediate, and thick). The performance of several nominal classification methods, including a recent interpretable method combining logistic regression with artificial neural networks (Logistic regression using Initial variables and Product Units, LIPU), is compared. For the three-class problem, a set of ordinal classification methods (considering ordering relation between the three classes) is included. For the binary case, LIPU outperforms all the other methods with an accuracy of 77.6%, while, for the second scheme, although LIPU reports the highest overall accuracy, the ordinal classification methods achieve a better balance between the performances of all classes.","Malignant tumors,
Lesions,
Image color analysis,
Feature extraction,
Color,
Surgery"
Run-Time Machine Learning for HEVC/H.265 Fast Partitioning Decision,"A novel fast Coding Tree Unit partitioning for HEVC/H.265 encoder is proposed in this paper. This method relies on run-time trained neural networks for fast Coding Units splitting decisions. Contrasting to state-of-the-art solutions, this method does not require any pre-training and provides a high adaptivity to the dynamic changes in video contents. By an efficient sampling strategy and a multi-thread implementation, the presented technique successfully mitigates the computational overhead inherent to the training process on both the overall processing performance and on the initial encoding delay. The experiments show that the proposed method successfully reduces the HEVC/H.265 encoding time for up to 65% with negligible rate-distortion penalties.","Encoding,
Training,
Artificial neural networks,
Input variables,
Instruction sets,
Video coding"
Driver Distraction Detection Using Semi-Supervised Machine Learning,"Real-time driver distraction detection is the core to many distraction countermeasures and fundamental for constructing a driver-centered driver assistance system. While data-driven methods demonstrate promising detection performance, a particular challenge is how to reduce the considerable cost for collecting labeled data. This paper explored semi-supervised methods for driver distraction detection in real driving conditions to alleviate the cost of labeling training data. Laplacian support vector machine and semi-supervised extreme learning machine were evaluated using eye and head movements to classify two driver states: attentive and cognitively distracted. With the additional unlabeled data, the semi-supervised learning methods improved the detection performance (G-mean) by 0.0245, on average, over all subjects, as compared with the traditional supervised methods. As unlabeled training data can be collected from drivers' naturalistic driving records with little extra resource, semi-supervised methods, which utilize both labeled and unlabeled data, can enhance the efficiency of model development in terms of time and cost.","Vehicles,
Training data,
Semisupervised learning,
Labeling,
Real-time systems,
Position measurement,
Support vector machines"
On the feasibility of machine learning as a tool for automatic security classification: A position paper,"With the proliferation of threats of leakage of sensitive information such as military classified documents, information guards have recently gained increased interest. An information guard is merely a filter than controls the content of the exchanged information between two domains where one of them has a higher confidentiality level than the other one. The main role of an information guard is to block leakage of the sensitive information from the higher confidentiality domain to the lower confidentiality domain. An example of a higher confidentiality domain is a military network while a subcontractor network is an example of a lower confidentiality domain. The common practice is to use an automatic information guard based on predefined list of words that is called ""dirty word list"" in order to decide the security level of a document and consequently release it to the lower confidentially domain or block it. Traditional information guards are configured manually based on the notion of ""Dirty Lists"". The classification logic of traditional information guards uses the occurrence of words from the ""Dirty Lists"". In this paper, we advocate the use of machine learning as a corner stone for building advanced information guards. Machine learning can also be used as a supplement to the decision obtained based on ""Dirty Lists"" classification. Machine learning has hardly been analysed for this problem, and the analysis on topical classification presented here provides new knowledge and a basis for further work within this area. Ten different machine learning algorithms were applied on real life data from a military context. Presented results are promising and demonstrates that machine learning can become a useful tool to assist humans in determining the appropriate security label of an information object.","Labeling,
Information exchange,
Machine learning algorithms,
Digital signatures,
Buildings,
Context"
Hybrid Machine Learning Approaches: A Method to Improve Expected Output of Semi-structured Sequential Data,"This paper proposes an intuitive yet simple machine learning (ML) approach that consist of two generic algorithms augmenting one another to solve problems they are not designed to solve. Since most machine learning algorithms are designed for a particular dataset or task, combining multiple ML algorithms can greatly improve the overall result by either helping tune one another, generalize, or adapt to unknown tasks. In this paper, we attempt to augment the architecture of traditional Artificial Neural Network (ANN) with a state machine acting as a form of short term memory in addition to help divide the work amongst multiple modular ANNs through transitioning from state to state. The result is a larger non-stochastic network that is able to self adjust as it is fed input. We train and test the work on data that is outside either an Artificial Neural Network or a state-machine's normal capability with simplified music notation extracted from midi files. The extracted data are used to simulate inherently sequential data to test the principle. Finally, while we find many large improvements in the augmentation of the ANN's architecture, but discuss further approaches to the system to improve generalization for new data.","Artificial neural networks,
Algorithm design and analysis,
Machine learning algorithms,
Prediction algorithms,
Feature extraction,
Data mining,
Complexity theory"
Enabling antenna design with nano-magnetic materials using machine learning,"A machine learning approach to design with magneto dielectric nano-composite (MDNC) substrate for planar inverted-F antenna (PIFA) is presented. A new mixing rule model has been developed. A database of material properties has been created using several particle radius and volume fraction. A second database built with antenna simulations has been developed to complete the machine learning dataset. It is shown that, starting from particle radius and volume fraction of the nano-magnetic material, it is possible to calculate the antenna parameters like gain, bandwidth, radiation efficiency, resonant frequency, and viceversa with good precision by using machine learning techniques.","Antennas,
Permeability,
Permittivity,
Mathematical model,
Magnetic resonance,
Databases"
A coefficient comparison of weighted similarity extreme learning machine for drug screening,"Machine learning techniques are becoming popular in drug discovery process. It can be used to predict the biological activities of compounds. This paper focuses on virtual screening task. We proposed the Weighted Similarity Extreme Learning Machine algorithm (WELM). It is based on Single Layer Feedforward Neural Network. The algorithm is powerful, iteratively free, and easy to program. In this work, we compared the performance of 17 different types of coefficients with WELM on a well-known dataset in the area of virtual screening named Maximum Unbiased Validation dataset. Moreover, the WELM with different types of coefficients were also compared with the conventional technique-similarity searching. WELM together with Jaccard/Tanimoto were able to achieve the best results on average in most of the activity classes.","Compounds,
Training,
Databases,
Drugs,
Biological information theory,
Support vector machines,
Fingerprint recognition"
Machine learning based acoustic sensing for indoor room localisation using mobile phones,"We present a novel indoor localisation system that used acoustic sensing. We developed the Acoustic Landmark Locator to determine a person's current room location, within a building. Indoor environments tend to have distinct acoustic properties due to physical structure. Hence rooms in a building can have distinctive acoustic signatures. We found that these acoustic signatures can determine the position of a person. We attempted to identify location based on acoustic sensing of the surrounding indoor environment. We developed a mobile phone application that determined a person's location by measuring the acoustic levels of the surrounding environment. We used a machine learning artificial neural network based algorithm to classify the location of the person, within proximity to a landmark or room. We tested the Acoustic Landmark Locator in an indoor environment. Our tests show that the Acoustic Landmark Locator mobile phone app was able to successfully determine the location of the person carrying the mobile phone, in all test areas. It was also found that background noise caused by the presence of people does distort the landmark acoustic profiles but the artificial neural network based classifier was able to reliably determine the person's room location. Further work will involve investigating how other machine learning approaches can be used to better improve position accuracy.","Acoustics,
Mobile handsets,
Sensors,
Neural networks,
Indoor environments,
Acoustic measurements,
Radio frequency"
Human learning and machine learning: Building bridges or integration?,"Summary form only given. At the core of Empirical Modelling is an activity we call `making construals'. A construal is a software artefact that embodies how we think about something, or make sense of something. For example, it might be a visualisation of a car engine with gears and controls that behaves - through interaction - like the physical car. We shall show a construal of MENACE : an early example of a simple machine (made with matchboxes) that learns to improve its own performance at playing noughts and crosses. Some experts in machine learning contrast the `big data' methods of training networks with the use of explanatory models. It is proving difficult, but desirable, to integrate these approaches. We'll suggest why Empirical Modelling might offer some useful insights into this problem.","Mathematical model,
Computer science,
Computational modeling,
Biological system modeling,
History,
Buildings"
Machine learning approach for exploring rock arts through the cloud infrastructure,"This paper is aimed at proposing a machine learning approach to analyze and make sense out of the ancient rock arts by exploring them through cloud infrastructure. The visual language of the rock art is proposed to be interpreted and transformed into the current language of human cognition. The rock arts can be captured as 3D motion pictures; ultrasonically detected images; pictures captured using laser sensors and thermography techniques. Since the countries across the Globe are rich in culture and also diverse in nature, rock arts have been explored and keeping on exploring more in quantity, the rock arts information collected through the above said methods can be represented and processed using cloud infrastructure. Further, using machine learning algorithms in the cloud is proposed, to arrive at definite, meaningful information from rock arts. Through the machine learning approach, the symbols represented by rock arts could be matched with the twenty six English alphabets. The proposed work is the interpretation of the olden rock art scripts and hence to predict the meaning that they wish to convey.","Art,
Rocks,
Cloud computing,
Visualization,
Machine learning algorithms,
Computer architecture,
Three-dimensional displays"
Calculating web service similarity using ontology learning with machine learning,"The Web is a popular, easy and common way to propagate information today and according to the growth of the Web, Web service discovery has become a challenging task. Clustering Web services into similar clusters through calculating the semantic similarity of Web services is one way for overcome this issue. Several methods are used for current similarity calculation process such as knowledge based, information-retrieval based, text mining, ontology based and context-aware based methods. Through this paper, present a method for calculating Web service similarity using both ontology learning and machine learning that uses a support vector machine for similarity calculation in generated ontology instead of edge count base method. Experimental results show that our hybrid approach of combining ontology learning and machine learning works efficiently and give accurate results than previous two approaches.","Web services,
Ontologies,
Semantics,
Context,
Clustering algorithms,
Feature extraction,
Quality of service"
Suspicious electric consumption detection based on multi-profiling using live machine learning,"The transition from today's electricity grid to the so-called smart grid relies heavily on the usage of modern information and communication technology to enable advanced features like two-way communication, an automated control of devices, and automated meter reading. The digital backbone of the smart grid opens the door for advanced collecting, monitoring, and processing of customers' energy consumption data. One promising approach is the automatic detection of suspicious consumption values, e.g., due to physically or digitally manipulated data or damaged devices. However, detecting suspicious values in the amount of meter data is challenging, especially because electric consumption heavily depends on the context. For instance, a customers energy consumption profile may change during vacation or weekends compared to normal working days. In this paper we present an advanced software monitoring and alerting system for suspicious consumption value detection based on live machine learning techniques. Our proposed system continuously learns context-dependent consumption profiles of customers, e.g., daily, weekly, and monthly profiles, classifies them and selects the most appropriate one according to the context, like date and weather. By learning not just one but several profiles per customer and in addition taking context parameters into account, our approach can minimize false alerts (low false positive rate). We evaluate our approach in terms of performance (live detection) and accuracy based on a data set from our partner, Creos Luxembourg S.A., the electricity grid operator in Luxembourg.","Context,
Smart grids,
Monitoring,
Smart meters,
Topology,
Network topology"
Machine learning for inferring phase connectivity in distribution networks,"The connectivity model of a power distribution network can easily become outdated due to system changes occurring in the field. Maintaining and sustaining an accurate connectivity model is a key challenge for distribution utilities worldwide. This work focuses on inferring customer to phase connectivity using machine learning techniques. Using voltage time series measurements collected from customer smart meters as the feature set for training classifiers, we study the performance of supervised, semi-supervised and unsupervised techniques. We report analysis and field validation results based on real smart meter measurements collected from three feeder circuits of a large distribution network in North America.","Voltage measurement,
Smart meters,
Smart grids,
Support vector machines,
Training,
Substations,
Manuals"
From probabilistic computing approach to probabilistic rough set for solving problem related to uncertainty under machine learning,"Box and Tiao suggested about the prior distribution, which according to them is hypothetically representing the knowledge about anonymous constraints prior to the availability of data. It acts as a productive role in Bayesian analysis. Further, allotments of such kind also represent former knowledge or relative ignorance [4]. The chance of occurrence or predictability is defined by the term Probability. During the availability of partial information related to the result, the calculation becomes more challenging. Even the partial results are also not available in some real world scenario. Several literatures are available in this direction. Pawlak's Rough sets, decision algorithms and Bayes Theorem is in the used to analyze the result in same direction. In our paper, we have extended our work where we have thoroughly studied and tried to create a relationship from probabilistic computing and Rough sets. We have further extended our study the importance of decision making by the concept of probabilistic rough set. Generally, the paper presents a kind of survey, where we intend to model a decision based system which can work efficiently under uncertainty.","Uncertainty,
Probabilistic logic,
Clustering algorithms,
Bayes methods,
Rough sets,
Computational modeling,
Computers"
Keynote 3: Interplay between Machine Learning and Artificial Intelligence by Yixin Chen,Provides an abstract of the keynote presentation and a brief professional biography of the presenter. The complete presentation was not made available for publication as part of the conference proceedings.,
Identification of ovarian mass through ultrasound images using machine learning techniques,"Today ovarian cancer is second most perilous cause of cancer deaths in women after breast cancer. In this work, we have developed system which acquires ultrasound images and using image processing and machine learning algorithms accurately classify benign and malignant tumors in ovarian cancer. This technique denoise image using wavelet transform, grey level texture features extracted using GLCM (grey level co-occurrence algorithm), extracted features will be trained through SVM (Support vector machine) and selected non-redundant features selected through Relief-F will be further train and test through SVM for output. Proposed technique was validated by 60 malignant and 60 benign images of patients. On evaluating classifier for 14-texture descriptors give 74% and relief-F gives 82% accuracy. After selecting 6 features from 14 features it will give accuracy 86% and relief-F gives 92% accuracy. Thus, the features are significant for result and preliminary results depict that the proposed technique can be reliable for ovarian tumor classification as this system is fully automated, advantageous and cost-effective too.","Feature extraction,
Cancer,
Wavelet transforms,
Ultrasonic imaging,
Entropy,
Classification algorithms"
Machine learning based parametric image estimation for Analyzer-based phase contrast imaging,"An X-ray beam passing through biological tissue is deflected (i.e., refracted) by a small angle typically <;10 μrad. Analyzer-based phase contrast imaging (ABI) systems are capable of measuring this tinny refraction by sampling the intensity of the beam at different propagation directions. An Analyzer crystal is the key element for this task as it acts as a narrow angular filter. Since refraction effects are highly dependent of the radiation wavelength, X-ray beam must be quasi-monochromatic. Therefore the amount of photons that reach the object and detector is much lower then that in traditional radiography. Using a reasonable exposure time, noisy reconstructions of refraction images are obtained. In this manuscript, we present a machine learning parametric image estimation approach to obtain accurate refraction images from noisy raw data.","X-ray imaging,
Image reconstruction,
Radiography,
Estimation,
Gaussian processes,
Filtering"
Probabilistic layer identification in a multi-layer fast timing detector for time-of-flight PET using machine learning,"This work presents an effective algorithm to identify crystal element positions for the design and operation of practical PET imaging detectors capable of achieving both excellent time resolution required for time-of-flight (ToF) and depth-of-Interaction (DoI) information. The detector unit consists of a dual layer (LYSO:Ce and LSO:Ce,Ca(0.4%)) stack of two 3×3×10 mm3 crystals, 1:1 coupled to SiPM arrays. Features of energy, crossover time metric, and a probability density estimation based unsupervised machine learning approach have been used for identification of in which layer a 511 KeV photon interacts. A global coincidence time resolution of 224 ps and a 91% layer identification accuracy has been achieved.","Detectors,
Photonics,
Crystals,
Image resolution,
Positron emission tomography,
Timing"
Extraction of Definitional Contexts through Machine Learning,"Automatic extraction of definitional contexts has been a problem that deserved to be addressed to in different studies by applications demands in the Natural Language Processing. The first approach to the automatic extraction of these resources has been through specific linguistic patterns, but this approach requires previous extensive linguistic knowledge and a thorough previous work. A model machine learning, on the other hand, reduces the work and, as we believe, can improve the results obtained with only one approach based on linguistic rules. Here experiments for extraction/classification of definitional contexts with naive bayes classifier and SVM are presented. We show that through machine learning approaches we can improve the results of this specific task. The highest result was obtained by the naive bayes classifier with back-off as smoothing.","Context,
Pragmatics,
Support vector machines,
Subspace constraints,
Training,
Context modeling,
Syntactics"
The Role of Machine Learning in Finding Chimeric RNAs,"High-throughput sequencing technology and bioinformatics have identified chimeric RNAs (chRNAs), raising the possibility of chRNAs expressing particularly in diseases can be used as potential biomarkers in both diagnosis and prognosis. The task of discriminating true chRNA from the false ones poses an interesting Machine Learning (ML) challenge. First of all, the sequencing data may contain false reads due to technical artefacts and during the analysis process, bioinformatics tools may generate false positives due to methodological biases. Thus predicting the real signal from the noise can be a hard task. Furthermore, even if we succeed to have a proper set of observations (enough sequencing data) about true chRNAs, chances are that the devised model can not be able to generalize beyond it. Like any other machine learning problem, the first big issue is finding the good data, observations, to build the prediction model. Unfortunately, as far as we were concerned, there is no common benchmark data available for chRNAs. And, the definition of a classification baseline is lacking in the related literature. In this work we are moving towards a benchmark data and a fair comparison analysis unraveling the role of ML techniques in finding chRNAs. We have developed a benchmark pipeline incorporating a mutated genome process and simulated RNA-seq data by Flux Simulator. These sequencing reads were aligned and annotated by CRAC. CRAC offers a new way to analyze the RNA-seq data by integrating genomic location and local coverage, allowing biological predictions in one step. The resulting data were used as a benchmark for our comparison analysis. We have observed that the no free lunch theorem do not hold for ensemble classifiers. Ensemble learning strategies demonstrated to be more robust to this classification problem, providing an average AUC performance of 95% (ACC=94%, Kappa=0.87%).","Bioinformatics,
Genomics,
Sequential analysis,
RNA,
Benchmark testing,
Data models,
Buildings"
Conformal Prediction in Spark: Large-Scale Machine Learning with Confidence,"Increasing size of datasets is challenging for machine learning, and Big Data frameworks, such as Apache Spark, have shown promise for facilitating model building on distributed resources. Conformal prediction is a mathematical framework that allows to assign valid confidence levels to object-specific predictions. This contrasts to current best-practices where the overall confidence level for predictions on unseen objects is estimated based on previous performance, assuming exchangeability. Here we report a Spark-based distributed implementation of conformal prediction, which introduces valid confidence estimation in predictive modeling for Big Data analytics. Experimental results on two large-scale datasets show the validity and the scalabilty of the method, which is freely available as open source.","Training,
Calibration,
Predictive models,
Sparks,
Computational modeling,
Prediction algorithms,
Big data"
Variants of heuristic rule generation from multiple patterns in Michigan-style fuzzy genetics-based machine learning,"In the design of rule-based classifiers, a single rule is often generated from a single pattern in a heuristic manner. Since the generated rule is likely to be over-specialized to the pattern, its conditions are often randomly replaced with don't care. However, the generalized rule with don't care conditions does not always have high classification ability. This is because the replacement is randomly performed without utilizing any information about other patterns. In our previous studies, we proposed an idea of generating a fuzzy classification rule from multiple patterns. In this paper, we propose its six variants. Each variant has a different criterion for choosing multiple patterns from which a single rule is generated. The proposed variants are used to generate fuzzy classification rules in Michigan-style fuzzy genetics-based machine learning. The usefulness of each variant is evaluated as a heuristic fuzzy rule generation method through computational experiments on 20 benchmark data sets.","Glass,
Ionosphere,
Sonar,
Heart,
Vehicles"
Quantifying California current plankton samples with efficient machine learning techniques,"This paper improves on the accuracy of other published machine learning results for quantifying plankton samples. The contributions of this work are: (1) Clarifying the number of expertly labeled images required for machine learning results. (2) Providing guidance as to what algorithms provide the best performance, and how to tune them. (3) Leveraging an ensemble of models to achieve recall rates beyond any single algorithm. (4) Investigating the applicability of abstaining. (5) Using size fractionation to learn more efficiently. (6) Analysis of efficacy of simple geometric features for plankton identification.","Training,
Support vector machines,
Classification algorithms,
Radio frequency,
Machine learning algorithms,
Algorithm design and analysis,
Shape"
Pruning Extreme Learning Machines Using the Successive Projections Algorithm,"Extreme Learning Machine (ELM) is a recently proposed machine learning method with successful applications in many domains. The key strengths of ELM are its simple formulation and the reduced number of hyper-parameters. Among these hyper-parameters, the number of hidden nodes has significant impact on ELM performance since too few/many hidden nodes may lead to underfitting/overfitting. In this work, we propose a pruning strategy for ELM using the Successive Projections Algorithm (SPA) as an approach to automatically find the number of hidden nodes. SPA was originally proposed for variable selection. In this work, it was adapted in order to be used to prune ELMs. The proposed method was compared to the Optimally Pruned Extreme Learning Machine algorithm (OP-ELM), which is considered as a state of the art method. Real world datasets were used to assess the performance of the proposed method for regression and classification problems. The application of the proposed model resulted in much simpler models with similar performance compared to the OP-ELM. For some classification instances, the performance of the proposed method outperformed the OP-ELM method.","Presses,
Projection algorithms,
Learning systems,
Input variables,
Classification algorithms,
Feedforward neural networks,
Predictive models"
"HadoopCL2: Motivating the Design of a Distributed, Heterogeneous Programming System With Machine-Learning Applications","Machine learning (ML) algorithms have garnered increased interest as they demonstrate improved ability to extract meaningful trends from large, diverse, and noisy data sets. While research is advancing the state-of-the-art in ML algorithms, it is difficult to drastically improve the real-world performance of these algorithms. Porting new and existing algorithms from single-node systems to multi-node clusters, or from architecturally homogeneous systems to heterogeneous systems, is a promising optimization technique. However, performing optimized ports is challenging for domain experts who may lack experience in distributed and heterogeneous software development. This work explores how challenges in ML application development on heterogeneous, distributed systems shaped the development of the HadoopCL2 (HCL2) programming system. ML applications guide this work because they exhibit features that make application development difficult: large & diverse datasets, complex algorithms, and the need for domain-specific knowledge. The goal of this work is a general, MapReduce programming system that outperforms existing programming systems. This work evaluates the performance and portability of HCL2 against five ML applications from the Mahout ML framework on two hardware platforms. HCL2 demonstrates speedups of greater than 20x relative to Mahout for three computationally heavy algorithms and maintains minor performance improvements for two I/O bound algorithms.","Programming,
Performance evaluation,
Vectors,
Java,
Kernel,
Object oriented modeling,
Computational modeling"
Structured Machine Learning for Data Analytics and Modeling: Intelligent Security as an Example,"Structured machine learning refers to learning a structured hypothesis from data with rich internal structure. We apply semantics-enabled (semi-)supervised learning for perfect and imperfect domain knowledge to fulfill the vision of structured machine learning for big data analytics and modeling. First, domain knowledge is modeled as RDF(S) ontologies, and SPARQL enables approximate queries for a type-labeled training dataset from ontologies to exploit a feature combination of a machine learning for hypothesis testing. Then, the existing type-labeled instances are used for classifying type-unlabeled new instances with the validation of testing dataset errors. Finally, these newly type-labeled instances are further forwarded to the structured ontologies to empower the ontology and rule learning. The proposed concepts have been tested and verified for intelligent security with the real KDD CUP 1999 datasets.","Big data,
Analytical models,
Data models,
Ontologies,
Intrusion detection,
Machine learning algorithms"
An E-Learning System with Multifacial Emotion Recognition Using Supervised Machine Learning,"E-Learning systems based on Affective computingare popularly used for emotional/behavioral analysis of the users. Emotions expressed by the user is depicted by detecting the facialexpression of the user and accordingly the teaching strategies willbe changed. The present eLearning systems mainly focus on thesingle user face detection. Hence, in this paper, we proposemultiuser face detection based eLearning system using supportvector machine based supervised machine learning technique. Experimental results demonstrate that the proposed systemprovides the accuracy of 89% to 100% w.r.t different datasets(LFW, FDDB, and YFD). Further, to improve the speed ofemotional feature processing, we used GPU along with the CPUand thereby achieve a speedup factor of 2.","Face,
Electronic learning,
Graphics processing units,
Face recognition,
Training,
Affective computing"
Multi-sensor Visual Analytics Supported by Machine-Learning Models,"Machines, such as engines, vehicles, or even aircraft, go through extensive controlled trials during their development. Each machine is typically instrumented with hundreds of sensors that produce voluminous time-series data. Engineers analyze suchdata to improve their understanding of how machines are used in practice, which in turn helps them in taking design decisions. Most often they study operational profiles various sensors fora given day of operation using histograms, or examine time-series from multiple sensors together. However, when confrontedwith data from dozens of sensors, over many years of operation, they are challenged by the large number of histograms toanalyze, and the sheer length of time-series' to explore. Traditional approaches such as hierarchical histograms, time-series semantic zooming etc. often cannot cope with the volume of data encountered in practice. We augment basic data visualizations such as histograms, heat-maps and basic time-series visualizations with machine-learning models that aid in summarizing, querying, searching, and interactively linking visualizations derived fromlarge volumes of multi-sensor data. In this paper we describe our machine-learning augmented approach to visual analytics in thecontext of its actual use in practice for answering questions ofinterest to engineers analyzing large-scale multi-sensor data.","Sensors,
Histograms,
Data visualization,
Visual analytics,
Engines,
Navigation,
Semantics"
Machine Learning Approach to Identify Users Across Their Digital Devices,"This paper discusses methods to identify individual users across their digital devices as part of the ICDM 2015 competition hosted on Kaggle. The competition's data set and prize pool were provided by http://www.drawbrid.ge/ in sponsorship with the ICDM 2015 conference. The methods described in this paper focuses on feature engineering and generic machine learning algorithms like Extreme Gradient Boosting (xgboost), Follow the Reguralized Leader Proximal etc. Machine learning algorithms discussed in this paper can help improve the marketer's ability to identify individual users as they switch between devices and show relevant content/recommendation to users wherever they go.","IP networks,
Boosting,
Companies,
Object recognition,
Measurement,
Prediction algorithms,
Conferences"
Selecting Machine Learning Algorithms Using Regression Models,"In performing data mining, a common task is to search for the most appropriate algorithm(s) to retrieve important information from data. With an increasing number of available data mining techniques, it may be impractical to experiment with many techniques on a specific dataset of interest to find the best algorithm(s). In this paper, we demonstrate the suitability of tree-based multi-variable linear regression in predicting algorithm performance. We take into account prior machine learning experience to construct meta-knowledge for supervised learning. The idea is to use summary knowledge about datasets along with past performance of algorithms on these datasets to build this meta-knowledge. We augment pure statistical summaries with descriptive features and a misclassification cost, and discover that transformed datasets obtained by reducing a high dimensional feature space to a smaller dimension still retain significant characteristic knowledge necessary to predict algorithm performance. Our approach works well for both numerical and nominal data obtained from real world environments.","Prediction algorithms,
Machine learning algorithms,
Measurement,
Training,
Data mining,
Error analysis,
Regression tree analysis"
Feasibility of using machine learning to access control in Squid proxy server,"Fast Internet connectivity and billions of web sites have made World Wide Web an attractive place for people to use the Internet in their day-to-day life. Educational institutes provide the Internet access to students mainly for educational purposes. However, most of the time, students are allowed to access any content on the web. Therefore, the full bandwidth is consumed due to access to non-educational content such as streaming non-educational videos and downloading large image files, etc. Prevention of Internet usage on non-education content is practically difficult due to various reasons. Usually, this is implemented in the proxy server through maintaining a blacklist of URLs. Most of the time, this is a static list of URLs. With the fast growing content on the World Wide Web maintaining a static blacklist is impractical. In this paper, we propose a methodology to generate dynamic blacklist of URLs using machine learning techniques. We experimentally investigate several machine learning algorithms to predict whether the URL in concern is educational or noneducational. The results of the initial experiments show that linear support vector machines can be used to predict the content with 98.9% accuracy.","Manuals,
XML,
Static VAr compensators"
Comparative study of machine learning techniques for pre-processing of network intrusion data,"Machine learning is widely used for network intrusion detection but the data it uses faces problems of large feature set and class imbalance which is inherent in network traffic data. This paper focuses on the performance evaluation of different strategies used for mitigating both the problems. Data used for classification was KDDCUP'99 which is a benchmark data set for intrusion detection and suffers greatly from class imbalance problem. Noise was also added to data to evaluate the performance of classifiers for noisy data. Different combinations of strategies form different scenarios. Four possible scenarios are tested by using different combinations of sampling, feature set reduction and classification. Classifiers are used for evaluating the performance of each scenario. Feature set was reduced to nine features from forty one features. Stratified remove folds and Resampling were applied to remove class imbalance problem. Results have shown that Nearest Neighbor, J48 classifier are best suited for real time detection with pre-processing whereas Gain Ratio is suitable for feature selection.","Intrusion detection,
Feature extraction,
Machine learning algorithms,
Real-time systems,
Data mining,
Classification algorithms,
Benchmark testing"
Temperature Distribution Prediction in Data Centers for Decreasing Power Consumption by Machine Learning,"To decrease the power consumption of data centers, coordinated control of air conditioners and task assignment on servers is crucial. It takes tens of minutes for changes of operational parameters of air conditioners including outlet air temperature and volume to be actually reflected in the temperature distribution in the whole data center. Proactive control of the air conditioners is therefore required according to the predicted temperature distribution, which is highly dependent on the task assignment on the servers. In this paper, we apply a machine learning technique for predicting the temperature distribution in a data center. The temperature predictor employs regression models for describing the temperature distribution as it is predicted to be several minutes in the future, with the model parameters trained using operational data monitored at the target data center. We evaluated the performance of the temperature predictor for an experimental data center, in terms of the accuracy of the regression models and the calculation times for training and prediction. The temperature distribution was predicted with an accuracy of 0.095°C. The calculation times for training and prediction were around 1,000 seconds and 10 seconds, respectively. Furthermore, the power consumption of air conditioners was decreased by roughly 30% through proactive control based on the predicting temperature distribution.","Temperature distribution,
Temperature sensors,
Servers,
Predictive models,
Power demand,
Data models,
Training"
Detection technique for hardware Trojans using machine learning in frequency domain,"Recently, the threat of hardware Trojan has been highlighted. A hardware Trojan is a hardware virus. When predetermined conditions are satisfied, that malicious virus performs subversive activities, such as a system shutdown and the leaking of important information, without the circuit users even being aware of that activity. Therefore, it is important to detect the consumer electronic devices with hardware Trojans from a viewpoint of security. This study proposes a new detection technique for hardware Trojan. The proposed method introduces machine learning for the detection. Experiments using actual devices prove the validity of the proposed method.","Trojan horses,
Hardware,
Power demand,
Large scale integration,
Support vector machines,
Consumer electronics,
Conferences"
"Comparing Tweet Classifications by Authors' Hashtags, Machine Learning, and Human Annotators","Over the last years, many papers have been published about how to use machine learning for classifying postings on microblogging platforms like Twitter, e.g., in order to assist users to reach tweets that interest them. Typically, the automatic classification results are then evaluated against a gold standard classification which consists of either (i) the hashtags of the tweets' authors, or (ii) manual annotations of independent human annotators. In this paper, we show that there are fundamental differences between these two kinds of gold standard classifications, i.e., human annotators are more likely to classify tweets like other human annotators than like the tweets' authors. Furthermore, we discuss how these differences may influence the evaluation of automatic classifications, like they may be achieved by Latent Dirichlet Allocation (LDA). We argue that researchers who conduct machine learning experiments for tweet classification should pay particular attention to the kind of gold standard they use. One may even argue that hashtags are not appropriate as a gold standard for tweet classification.","Twitter,
Tagging,
Gold,
Standards,
Resource management,
Electronic mail,
Probability distribution"
Prediction of Long-Lead Heavy Precipitation Events Aided by Machine Learning,"Long-lead prediction of heavy precipitation events has a significant impact since it can provide an early warning of disasters, like a flood. However, the performance of existed prediction models has been constrained by the high dimensional space and non-linear relationship among variables. In this study, we study the prediction problem from the prospective of machine learning. In our machine-learning framework for forecasting heavy precipitation events, we use global hydro-meteorological variables with spatial and temporal influences as features, and the target weather events that last several days have been formulated as weather clusters. Our study has three phases: 1) identify weather clusters in different sizes, 2) handle the imbalance problem within the data, 3) select the most-relevant features through the large feature space. We plan to evaluate our methods with several real world data sets for predicting the heavy precipitation events.","Predictive models,
Floods,
Conferences,
Wind,
Oceans,
Computer science"
Throughput and Delay Estimator for 2.4GHz WiFi APs: A Machine Learning-Based Approach,"This paper reports our recent result in designing a function for autonomous APs to estimate throughput and delay of its clients in 2.4GHz WiFi channels to support those APs' dynamic channel selection. Our function takes as inputs the traffic volume and strength of signals emitted from nearby interference APs as well as the target AP's traffic volume. By this function, the target AP can estimate throughput and delay of its clients without actually moving to each channel, it is just required to monitor IEEE802.11 MAC frames sent or received by the interference APs. The function is composed of an SVM-based classifier to estimate capacity saturation and a regression function to estimate both throughput and delay in case of saturation in the target channel. The training dataset for the machine learning is created by a highly-precise network simulator. We have conducted over 10,000 simulations to train the model, and evaluated using additional 2,000 simulation results. The result shows that the estimated throughput error is less than 10%.","Channel estimation,
IEEE 802.11 Standard,
Interference,
Throughput,
Delays,
Estimation,
Monitoring"
On Multi-tier Sentiment Analysis Using Supervised Machine Learning,"Document management and Information Retrieval tasks have rapidly increased due to the availability of digital documents anytime, any place. The need for automatic extraction of document information has become prominent in information organization and knowledge discovery. Text Classification is one such solution, where in the natural language text is assigned to one or more predefined categories based on the content. This work focuses on sentiment analysis, also known as opinion mining. It is a way of automatically extracting and analyzing the emotions and opinions, and not facts, of messages and posts. A multi-tier classification architecture is proposed, which consists of major modules such as data cleaning and pre-processing, feature selection, and classifier training that includes a multi-tier prediction model. The architecture and its components are carefully described. Four classifiers (Naïve Bayes, SVM, Random Forest, and SGD) are used in the experiments, which evaluate the performance of the proposed multi-tier architecture by analyzing the sentiments and opinions of 150,000 movie reviews. Results have shown that the multi-tier model is able to significantly improve prediction accuracy over the single-tier model by more than 10%, the improvement is significant when customized dictionary is used. We believe that the proposed multi-tier classification architecture, with the various feature selection techniques described and used, are significant, and are readily applicable to many other areas of sentiment analysis.","Predictive models,
Data models,
Sentiment analysis,
Dictionaries,
Training,
Data mining,
Motion pictures"
Falling Detection System Based on Machine Learning,"As falling is the most important issue that faces elderly people all over the world, this paper proposes a detection system for falling based on Machine Learning (ML). In the proposed system, a dataset of videos containing falling actions has been utilized via dividing each video into many shots that are consequently being converted into gray-level images. Then, for detecting the moving objects in videos, the foreground is firstly detected, then noise and shadow are deleted to detect the moving object. Finally, a number of features, including aspect ratio and falling angle, are extracted and a number of classifiers are being applied in order to detect the occurrence of falling. Experimental results, using 10-fold cross validation, shown that the proposed falling detection approach based on Linear Discriminant Analysis (LDA) classification algorithm has outperformed both support vector machines (SVMs) and Knearest neighbor (KNN) classification algorithms via achieving falling detection with accuracy of 96.59 %.","Videos,
Feature extraction,
Image color analysis,
Mathematical model,
Support vector machines,
Shape,
Electronic mail"
A Machine Learning-Based Approach to Digital Triage,,
A machine learning technique in a multi-agent framework for online outliers detection in Wireless Sensor Networks,"Wireless Sensor Networks enable flexibility, low operational and maintenance costs, as well as scalability in a variety of scenarios. However, in the context of industrial monitoring scenarios the use of Wireless Sensor Networks can compromise the system's performance due to several factors, being one of them the presence of outliers in raw data. In order to improve the overall system's resilience, this paper proposes a distributed hierarchical multi-agent architecture where each agent is responsible for a specific task. This paper deals with online detection and accommodation of outliers in non-stationary time-series by appealing to a machine learning technique. The methodology is based on a Least Squares Support Vector Machine along with a sliding window-based learning algorithm. A modification to this method is considered to improve its performance in transient raw data collected from transmitters over a Wireless Sensor Networks (WSNs). An empirical study based on laboratory test-bed show the feasibility and relevance of incorporating the proposed methodology in the context of monitoring systems over Wireless Sensor Networks.","Monitoring,
Wireless sensor networks,
Kernel,
Symmetric matrices,
Support vector machines,
Context,
Memory"
LTE Connectivity and Vehicular Traffic Prediction Based on Machine Learning Approaches,"The prediction of both, vehicular traffic and communication connectivity are important research topics. In this paper, we propose the usage of innovative machine learning approaches for these objectives. For this purpose, Poisson Dependency Networks (PDNs) are introduced to enhance the prediction quality of vehicular traffic flows. The machine learning model is fitted based on empirical vehicular traffic data. The results show that PDNs enable a significantly better short-term prediction in comparison to a prediction based on the physics of traffic. To combine vehicular traffic with cellular communication networks, a correlation between connectivity indicators and vehicular traffic flow is shown based on measurement results. This relationship is leveraged by means of Poisson regression trees in both directions, and hence, enabling the prediction of both types of network utilization.","Data models,
Roads,
Predictive models,
Detectors,
Communication systems,
Correlation,
Physics"
Performance analysis of machine learning for arbitrary downsizing of pre-encoded HEVC video,"Nowadays, broadcasters deliver ultra-high resolution video to their consumers. This live video is sent to a set-top box for display on a television. However, if one or more users in the home want to view the same video on their personal mobile devices with a lower display resolution and limited processing power, decoding the original ultra-high resolution video would result in stuttering and quickly drain the battery life on these devices. To enable a satisfactory consumer experience, the resolution of the video stream should be adapted to the target mobile device at the set-top box. The aim of this paper is to investigate the performance of different machine learning strategies to arbitrary downsize video pre-encoded with the high efficiency video coding standard (HEVC). These machine learning techniques exploit correlation between input and output coding information to predict the splitting behavior of HEVC coding units. Several machine learning algorithms are optimized. Additionally, both online and offline training strategies are tested. Of the tested algorithms, online-trained random forests achieve the best compression-efficiency with a bit rate increase of 5.4% and an average complexity reduction of 70%1.","Training,
Transcoding,
Streaming media,
Predictive models,
Machine learning algorithms,
Complexity theory"
Machine Learning and Decision Support in Critical Care,"Clinical data management systems typically provide caregiver teams with useful information, derived from large, sometimes highly heterogeneous, data sources that are often changing dynamically. Over the last decade there has been a significant surge in interest in using these data sources, from simply reusing the standard clinical databases for event prediction or decision support, to including dynamic and patient-specific information into clinical monitoring and prediction problems. However, in most cases, commercial clinical databases have been designed to document clinical activity for reporting, liability, and billing reasons, rather than for developing new algorithms. With increasing excitement surrounding “secondary use of medical records” and “Big Data” analytics, it is important to understand the limitations of current databases and what needs to change in order to enter an era of “precision medicine.” This review article covers many of the issues involved in the collection and preprocessing of critical care data. The three challenges in critical care are considered: compartmentalization, corruption, and complexity. A range of applications addressing these issues are covered, including the modernization of static acuity scoring; online patient tracking; personalized prediction and risk assessment; artifact detection; state estimation; and incorporation of multimodal data sources such as genomic and free text data.","Databases,
Complexity theory,
Monitoring,
Interoperability,
Hospitals,
Data privacy"
Study on Case-Based Reasoning-Inspired Approaches to Machine-Learning,"This commentary briefly reviews work on the application of case-based reasoning (CBR) to the design and construction of machine-learning approaches and computer-based teaching systems. The CBR cognitive model is at the core of constructivist learning approaches such as Goal-Based Scenarios and Learning by Design. Case libraries can play roles as intelligent resources while learning and frameworks for articulating one understands. More recently, CBR techniques have been applied to design and construction of simulation-based learning systems and serious games. The main ideas of CBR are explained and pointers to relevant references are provided, both for finished work and on-going research.","Transportation,
Big data,
Smart cities"
Machine Learning Techniques in Storm,"Storm is used to process real-time and bulk data and with the exponential growth of the amount of real-time data, storm has a wide range of usefulness. In order to be better used by companies and projects, Apache adds the technology of machine learning into storm which is called trident-ml. Trident-ml can provides more and more accurate services for us. The paper introduce briefly storm and presents the detail information about trident-ml.","Storms,
Machine learning algorithms,
Real-time systems,
Topology,
Clustering algorithms,
Libraries,
Fasteners"
Integrative Machine Learning augmentation,"In this article, an integrative approach for augmenting the segmentation capabilities of the off-line trained Machine Learning (ML) classifier is presented. The proposed approach augments the ML performance in the graph cut setup. The integration of the prediction capabilities of the classifiers and neighborhood relationship of the pixels result in increase of segmentation performance. The experimental setup includes an evaluation of the Bayesian Network, Multilayer Perceptron, Random Forest and the Histogram approach of Jones and Rehg [1]. The evaluation results based on the color based detection dataset reveal that the proposed integrative approach improves the detection performance compared to using the off-line classifiers alone.","Skin,
Image color analysis,
Histograms,
Image segmentation,
Bayes methods,
Multilayer perceptrons,
Classification algorithms"
A settings tracking and providing scheme for differential protection based on machine learning,"With the extensive use of differential protection in micro-grid, the demand for online obtaining the settings becomes more urgent than the process in the past. This paper proposes a new differential protection scheme for micro-grid where a settings tracking and providing scheme is implemented to acquire the latest enabled protection settings by tracking multiple setting group control block (SGCB) class services. A machine learning technique is implemented to assist classifier in identifying the most relevant electrical features which are required for the fault detection and to establish the best efficient differential protection strategy to micro-grid. A practical case study has successfully verified the adaptability and practicability of the micro-grids protection scheme where statistical classifier will make a decision based on protection settings and differential features.","Relays,
Feature extraction,
Mathematical model,
IEC Standards,
Predictive models,
Switches,
Computational modeling"
Path Planning Efficiency Maximization for Ball-Picking Robot Using Machine Learning Algorithm,"This paper aims to find a ball-picking optimal path and drives the robots to collect all the tennis in the shortest time, according to the optimal path. Thus, the work to be completed for us includes: establish agent model for robot working environment in tennis yard, analyze the advantage and shortcoming of ACO and provide an improved ACO. Our scheme improves the pheromone updating strategy. The global and local updates are integrated to strength the pheromone strength of optimal ant. Then we add the crossover and mutation operation of GA to speed the convergence of algorithm. By the simulation results analysis we find that the improved ACO has stronger optimizing ability and stability, which further improves the performance of ball-picking path.",
Controlled islanding of power networks using machine learning algorithm,"Wide-area blackouts have occurred the last decades due to severe disturbances and cascading failures. Controlled islanding of a power system is a measure for preventing blackout and limiting the effects of disturbances. Intention islanding aims to divide the network into electromechanically stable islands, which should satisfy a number of constraints such as generator coherency, generation-demand balance, system limits, and network dynamic constraints. The controlled islanding addresses the problem of finding which lines should be disconnected to create the islands. In this paper a modified algorithm of SelfOrganizing Map (SOM) in combination with a Particle Swarm Optimization (PSO) algorithm is proposed in order to determine which lines should be disconnected. To evaluate the algorithm performance the method is applied to test networks.","power system reliability,
particle swarm optimisation,
power system control,
power system faults"
Supervised Machine Learning and Bag of Words applied to Polen Grains Classification,This work presents some new results regarding the automation of pollen grains classification using computer vision. A technique based on the Bag of Visual Words and Supervised Machine Learning algorithms is proposed and evaluated. A dataset of pollen grain images taken from 9 different pollen types was created and used in the experiments.,"Visualization,
Support vector machines,
Malignant tumors,
Detectors,
Automation,
Computer vision"
Automatic Recognition of Books Based on Machine Learning,The content-based image recognition is a research focus in the field of computer vision. Machine learning especially deep learning has a great potential in the field of image recognition. This paper adopts the support vector machine algorithm and deep learning method convolutional neural network to recognize books in the digital image library and compares their performance. Experiments show that both methods used in this paper realize a fast and efficient image classification and help improve the intelligence of books retrieval.,"Image recognition,
Training,
Support vector machines,
Libraries,
Image resolution,
Machine learning,
Feature extraction"
Using Analytical Models to Bootstrap Machine Learning Performance Predictors,"Performance modeling is a crucial technique to enable the vision of elastic computing in cloud environments. Conventional approaches to performance modeling rely on two antithetic methodologies: white box modeling, which exploits knowledge on system's internals and capture its dynamics using analytical approaches, and black box techniques, which infer relations among the input and output variables of a system based on the evidences gathered during an initial training phase. In this paper we investigate a technique, which we name Bootstrapping, which aims at reconciling these two methodologies and at compensating the cons of the one with the pros of the other. We analyze the design space of this gray box modeling technique, and identify a number of algorithmic and parametric trade-offs which we evaluate via two realistic case studies, a Key-Value Store and a Total Order Broadcast service.","Training,
Analytical models,
Predictive models,
Cloud computing,
Computational modeling,
Knowledge based systems,
Prediction algorithms"
Machine learning approaches on map reduce for Big Data analytics,"To analyze enormous datasets, collection of algorithms, associated systems and perform necessary processing on massive data structures there is obligation for a novel trend, which is framed by Big Data. Architecture of Big Data varies across compound machines and clusters with unique purpose sub systems. The data produced from several sources requires analysis and organization with meager amounts of time. To potentially speed up the processing, a unified way of machine learning is applied on MapReduce frame work. A broadly applicable programming model MapReduce is applied on different learning algorithms belonging to machine learning family for all business decisions. By using ML algorithms with Hadoop for better storage distribution will improve the time and processing speed. This paper presents parallel implementation of various machine learning algorithms implemented on top of MapReduce model for time and processing efficiency.","Machine learning algorithms,
Clustering algorithms,
Data models,
Big data,
Computational modeling,
Computer architecture,
Algorithm design and analysis"
Heterogeneous Feature Space Based Task Selection Machine for Unsupervised Transfer Learning,"Transfer learning techniques try to transfer knowledge from previous tasks to a new target task with either fewer training data or less training than traditional machine learning techniques. Since transfer learning cares more about relatedness between tasks and their domains, it is useful for handling massive data, which are not labeled, to overcome distribution and feature space gaps, respectively. In this paper, we propose a new task selection algorithm in an unsupervised transfer learning domain, called as Task Selection Machine (TSM). It goes with a key technical problem, i.e., feature mapping for heterogeneous feature spaces. An extended feature method is applied to feature mapping algorithm. Also, TSM training algorithm, which is main contribution for this paper, relies on feature mapping. Meanwhile, the proposed TSM finally meets the unsupervised transfer learning requirements and solves the unsupervised multi-task transfer learning issues conversely.","Training,
Support vector machines,
Feature extraction,
Speech recognition,
Algorithm design and analysis,
Intelligent systems,
Knowledge engineering"
Generic and specific impression estimation of clothing fabric images based on machine learning,"Consumers' psychological feeling or impression is an important factor for product design. The impression estimation becomes an important issue. In this paper, we propose generic and specific impression estimation methods based on machine learning for cloth fabric images. We use a semantic differential (SD) method to measure the user's impression such as bright, warm while they viewing a cloth fabric image. We also extract both global and local features of cloth fabric images such as color and texture using computer vision techniques. Then we use support vector regression to model the mapping functions between the generic impression (or specific impression) and image features. The learned mapping functions are used to estimate the generic or specific impression of cloth fabric images.","Estimation,
Fabrics,
Semantics,
Clothing,
Feature extraction,
Image color analysis,
Correlation"
OpenMP-Based Multi-core Parallel Cooperative PSO with ICS Using Machine Learning for Global Optimization Problem,"Novel parallel cooperative multiple particles swarm optimization algorithm with immune clonal selection (ICS) using machine learning based on multi-core architecture is presented for global optimization problem in this paper, the proposed method named O-PCPSO-ICS. The O-PCPSO-ICS consists of one memory and bottom multiple swarms. In O-PCPSO-ICS, the global best individuals are saved into the antibody memory and promoted by using the improved ICS operator. An opposition-based learning operator is employed to accelerate the convergence speed of Pbests. Furthermore, excellent search information is spread among different subpopulations by a migration scheme. Finally, the proposed method is running on multi-core architecture using open multiprocessing (OpenMP). The numerical simulations validated the O-PCPSO-ICS has a better performance in global search, solution accuracy, and convergence speed. Meanwhile, the computational efficiency of the proposed method is greatly enhanced by parallelization.","Sociology,
Statistics,
Convergence,
Optimization,
Acceleration,
Cloning,
Standards"
Accelerating Support Vector Machine Learning with GPU-Based MapReduce,"With the exploding growth of data, the computational complexity required by learning Support Vector Machine (SVM) lays a heavy burden on real-world applications. To address this issue, parallel computational techniques can be employed such as the Graphics Processing Units (GPUs) and MapReduce model. As it is well known, GPUs are microprocessors on a multi-core architecture which reveal high performance in mass data parallel computing, and MapReduce allows computational tasks to be divided into a plurality of parts, distributed to various computing nodes and combined on a single node. In this paper, we propose a GPU-based MapReduce framework to accelerate SVM learning by jointly utilizing the parallel computing power of GPU and MapReduce. Extensive experimental results have verified the effectiveness and efficiency of the proposed approach.","Graphics processing units,
Support vector machines,
Kernel,
Training,
Optimization,
Acceleration,
Parallel processing"
Detection of De-Authentication DoS Attacks in Wi-Fi Networks: A Machine Learning Approach,"Media Access Layer (MAC) vulnerabilities are the primary reason for the existence of the significant number of Denial of Service (DoS) attacks in 802.11 Wi-Fi networks. In this paper we focus on the de-authentication DoS (Deauth-DoS) attack in Wi-Fi networks. In Deauth-DoS attack an attacker sends a large number of spoofed de-authentication frames to the client (s) resulting in their disconnection. Existing solutions to mitigate Deauth-DoS attack rely on encryption, protocol modifications, 802.11 standard up gradation, software and hardware upgrades which are costly. In this paper we propose a Machine Learning (ML) based Intrusion Detection System (IDS) to detect the Deauth-DoS attack in Wi-Fi network which does not suffer from these drawbacks. To the best of our knowledge ML based techniques have never been used for detection of Deauth-DoS attack. We have used a variety of ML based classifiers for detection of Deauth-DoS attack enabling an administrator to choose among a host of classification algorithms. Experiments performed on in-house test bed shows that the proposed ML based IDS detects Deauth-DoS attack with precision (accuracy) and recall (detection rate) exceeding 96% mark.","IEEE 802.11 Standard,
Computer crime,
Encryption,
Authentication,
Protocols,
Software"
A Novel Ontology and Machine Learning Inspired Hybrid Cardiovascular Decision Support Framework,"Healthcare information management systems (HIMS) have a substantial amount of limitations such as rigidity and nonconformity to complex clinical processes like Electronic Healthcare records and effective utilisation of clinical practice guidelines to help provide effective clinical decision support. The conventional healthcare systems suffer from a general lack of intelligence, they are successful in offering basic patient management capabilities, but they do not offer consistent and holistic decision support capabilities for clinicians working under tight deadlines in a fast paced environment. The conventional healthcare information management systems are designed using branching logic based rigid architectures, which are hard to maintain and upgrade without considerable labour intensive effort. The proposed ontology and machine learning driven hybrid clinical decision support framework comprises of two key components (1) ontology driven clinical risk assessment and recommendation system and (2) machine learning driven prognostic system. The key aim of our research is to utilise information collected through the knowledge based ontology driven clinical risk assessment and recommendation system and non-knowledge based/evidence based machine learning driven prognostic system to deliver a holistic clinical decision support framework in the cardiovascular domain. The ontology driven clinical risk assessment and recommendation system could be used as a triage system for cardiovascular patients as a preventative solution, this could help clinicians prioritise patient referrals after reviewing a snapshot of patient's medical history (collected through an ontology driven intelligent context aware information collection using standardised clinical questionnaires) containing patient demographics information, cardiac risk scores, cardiac chest pain score, medication and recommended lab tests details. The machine learning driven prognostic system is developed using a chest pain clinical case study identified by the consultant cardiologist, Professor Stephen Leslie from Raigmore Hospital in Inverness. The key aim of this clinical case study UK is to provide a clinical decision support mechanism for Raigmore Hospital's Rapid Access Chest Pain Clinic (RACPC) patients by combining evidence, extrapolated through legacy patient data (based on machine learning driven techniques) to facilitate evidence based cardiovascular preventative care. The machine learning driven prognostic system provides cardiac chest pain prognosis through a cardiac chest pain specific prognostic model which is validated through consultant cardiologist from Raigmore Hospital. The cardiac chest pain prognostic model could help clinicians diagnose cardiac chest pain patients efficiently and could also help clinicians reduce load on overly prescribed angiography treatment in a cost effective manner. Additional two clinical case studies in the heart disease and breast cancer domains are considered for the development and clinical validation of the machine learning driven prognostic system. The proposed novel ontology and machine learning driven hybrid clinical decision support framework will also be validated in other application areas.","Ontologies,
Pain,
Risk management,
Diseases,
Hospitals"
Study on implementation of machine learning methods combination for improving attacks detection accuracy on Intrusion Detection System (IDS),"Many computer-based devices are now connected to the internet technology. These devices are widely used to manage critical infrastructure such energy, aviation, mining, banking and transportation. The strategic value of the data and the information transmitted over the Internet infrastructure has a very high economic value. With the increasing value of the data and the information, the higher the threats and attacks on such data and information. Statistical data shows a significant increase in threats to cyber security. The Government is aware of the threats to cyber security and respond to cyber security system that can perform early detection of threats and attacks the internet. The success of a nation's cyber security system depends on the extent to which it is able to produce independently their cyber defense system. Independence is manifested in the form of the ability to process, analyze and create an action to prevent threats or attacks originating from within and outside the country. One of the systems can be developed independently is Intrusion Detection System (IDS) which is very useful for early detection of cyber threats and attacks. The advantages of an IDS is determined by its ability to detect cyber attacks with little false. This study learn how to implement a combination of various methods of machine-learning to the IDS to improve the accuracy in detecting attacks. This study is expected to produce a prototype IDS. This prototype IDS, will be equipped with a combination of machine-learning methods to improve the accuracy in detecting various attacks. The addition of machine-learning feature is expected to identify the specific characteristics of the attacks occurred in the Indonesian Internet network. Novel methods used and techniques in implementation and the national strategic value are becoming the unique value and advantages of this research.","Support vector machines,
Internet,
Artificial neural networks,
Training data,
Ports (Computers),
Engines,
Databases"
Vehicle models identification based on the double updating support vector machine online learning algorithm,"For the traditional online support vector machine classification algorithm based on the kernel function, the weight of the misclassified sample in the learning process of classification remains unchanged, which will inevitably affect the classification accuracy. This paper presents a Double Updating Online Support Vector Machine Learning Algorithm that can update the weight real-timely. According to the change of the training support vector set when the new sample is added, the algorithm can update the weights of misclassified sample and update the existing sample weights at the same time, making the algorithm achieve better classification performance in large-scale data situation. The online support vector machine double update algorithm is applied to the vehicle recognition, the added newly vehicle models classification and recognition can be done beautifully from the experiment, and the experiment proved the validity and feasibility of the algorithm robustness.","Support vector machine classification,
Vehicles,
Classification algorithms,
Training,
Machine learning algorithms,
Learning systems"
Machine Learning based criminal short listing using Modus Operandi features,"One of the most challenging problems faced by crime analysts is identifying sets of crimes committed by the same individual or group. Amount of criminal records piling up daily has made it cumbersome to manually process connections between crimes. These Crime series' possess certain attributes that are characteristic of the criminal(s) involved in them, which are useful in defining their modus operandi (MO). After a careful study in the grave crime category of House breaking and Theft in Sri Lanka, we have identified certain MO attributes which we have used to collect from past crime scene data from police records. Then we have explored whether it is possible to group suspects who have similar MO patterns through a machine learning approach and give a short list for a new crime from the existing data. The evaluation of the research presented an accuracy above 75% which proved that Machine Learning is capable of short listing criminals based on their Modus Operandi features.","Feature extraction,
Weapons,
Encoding,
Unsupervised learning"
An integrated pattern matching and machine learning approach for question classification,"In question answering system, the process of classifying a question to appropriate class and identification of the focus word play key role in determining accurate answer. In this paper, we propose an integrated pattern matching and machine learning approach for higher education domain that focuses on factoid question answering. We have developed a question taxonomy for higher education domain and defined 9 coarse classes and 63 fine classes. We adopted pattern matching for the primary stage of classification and focus word identification and used machine learning approach i.e., Support Vector Machine (SVM) for the secondary classification approach only to those questions whose pattern are not present in question pattern corpus. Our experimental result shows that the accuracy of question classification using integrated approach outperforms the accuracy shown by individual approaches. SVM enhances the classification accuracy while focus word identification is achieved by virtue of pattern matching. The integrated approach shows the accuracy of 92.5% and 87.8% for coarse and fine class respectively and achieved focus word identification up to 83.4%.","Pattern matching,
Support vector machines,
Magnetic heads,
Taxonomy,
Kernel,
Education,
Shape"
Smartphone-Based Tele-Rehabilitation System for Frozen Shoulder Using a Machine Learning Approach,"Frozen shoulder is a very painful condition that affects patients' daily life. Patients with frozen shoulder have to go to a hospital or medical center to get appropriate rehabilitation. Transportation to the hospital raises healthcare costs and the process can be time-consuming. We have developed a tele rehabilitation system which allows patients to perform an at-home exercise. According to our existing system, it is only available for high-end smartphones with multiple sensors that include accelerometer, gyroscope, and magnetic field sensors. In this work, we propose a novel approach using machine learning to estimate the arm angle of rotation using only the accelerometer sensor. Results show that reasonable accuracy can be obtained so that it may be used with lower-end Android smartphone devices that only have an accelerometer available. A web-based interface enables the medical practitioner such as a physiotherapist to monitor and administer an appropriate rehabilitation program for more effective recovery.","Smart phones,
Accelerometers,
Magnetic sensors,
Shoulder,
Data models,
Gyroscopes"
Big Data and Machine Learning for Applied Weather Forecasts: Forecasting Solar Power for Utility Operations,"To blend growing amounts of renewable energy into utility grids requires accurate estimate of the power from those resources for both day ahead planning and real-time operations. This requires predicting the wind and solar resource on those timescales. Accurate prediction of these meteorological variables is a big data problem that requires a multitude of disparate data, multiple models that are each applicable to a specific time frame, and application of computational intelligence techniques to successfully blend all of the model and observational information in real-time and deliver it to the decision-makers at utilities and grid operators. Considering that the capacity of renewable energy continues to grow an additional challenge includes selecting and archiving data for continuous retraining of machine learning algorithms.","Predictive models,
Forecasting,
Data models,
Atmospheric modeling,
Wind forecasting"
Robust blind watermarking technique for color images using Online Sequential Extreme Learning Machine,"In this paper, a robust blind color image watermarking technique using Online Sequential Extreme Learning Machine (OS-ELM) is proposed. Blue channel is utilized and transformed using DWT. Low frequency LL4 subband is used for watermark embedding. A variant of mini-batch machine learning algorithm i.e. OS-ELM is initially tuned with a fixed number of training data used in its initial phase and size of block of data learned by it in each step. The training data to OSELM is constructed by combining the quantized and desired LL4 sub-band coefficients of the DWT domain. A random key decides the starting watermark embedding position of the coefficients. Two binary images are used as watermark. The robustness towards common image processing attacks is enhanced using this process. Experimental results show that the extracted watermarks from watermarked and attacked images are similar to the original watermarks. Computed time spans for embedding and extraction are of the order of milliseconds, which is suitable for developing real time watermarking applications.","Watermarking,
Robustness,
Discrete wavelet transforms,
Training data,
Color,
Image processing,
Visualization"
Weighting technique on multi-timeline for machine learning-based anomaly detection system,"Anomaly detection is one of the crucial issues of network security. Many techniques have been developed for certain application domains, and recent studies show that machine learning technique contains several advantages to detect anomalies in network traffic. One of the issues applying this technique to real network is to understand how the learning algorithm contains more bias on new traffic than old traffic. In this paper, we investigate the dependency of the time period for learning on the performance of anomaly detection in Internet traffic. For this, we introduce a weighting technique that controls influence of recent and past traffic data in an anomaly detection system. Experimental results show that the weighting technique improves detection performance between 2.7-112% for several learning algorithms, such as multivariate normal distribution, knearest neighbor, and one-class support vector machine.","Routing protocols,
Throughput,
Vehicular ad hoc networks,
Delays,
Routing"
Reduced overhead error compensation for energy efficient machine learning kernels,"Low overhead error-resiliency techniques such as RAZOR [1] and algorithmic noise-tolerance (ANT) [2] have proven effective in reducing energy consumption. ANT has been shown to be particularly effective for signal processing and machine learning kernels. In ANT, an explicit estimator block compensates for large magnitude errors in a main block. The estimator represents the overhead in ANT and can be as large as 30%. This paper presents a low overhead ANT technique referred to as ALG-ANT. In ALG-ANT, the estimator is embedded inside the main block via algorithmic reformulation and thus completely eliminates the overhead associated with ANT. However, ALG-ANT is algorithm-specific. This paper demonstrates the ALG-ANT concept in the context of a finite impulse response (FIR) filter kernel and a dot product kernel, both of which are commonly employed in signal processing and machine learning applications. The proposed ALG-ANT FIR filter and dot product kernels are applied to the feature extractor (FE) and SVM classification engine (CE) of an EEG seizure classification system. Simulation results in a commercial 45nm CMOS process show that ALG-ANT can compensate for error rates of up to 0.41 (errors in FE only), and up to 0.19 (errors in FE and CE) and maintain the true positive rate ptp > 0.9 and false positive rate pfp ≤ 0.01. This represents a greater than 3-orders-of-magnitude improvement in error tolerance over the conventional architecture. This error tolerance is employed to reduce energy via the use of voltage overscaling (VOS). ALG-ANT is able to achieve 44.3% energy savings when errors are in FE only, and up to 37.1% savings when errors are in both FE and CE.","Kernel,
Support vector machines,
Error analysis,
Finite impulse response filters,
Iron,
Signal processing algorithms,
Electroencephalography"
A machine learning approach to find association between imaging features and XRF signatures of rocks in underground mines,"This study investigated the applicability of machine learning algorithms to detect the presence of elements in underground mines from rock surface images, which is proposed as a heuristic classification method inspired by the ability of human geologists to make judgments about the location of ore veins by eye. A regression algorithm was investigated to find associations between image features and X-Ray Fluorescence (XRF) signatures indicating elemental content of the surface and near-surface region of the rocks. A set of image processing algorithms was used to extract color distribution, edge orientation statistics, and texture of the rock surfaces. XRF signatures were obtained from the same samples, providing a semi-quantitative measure of element concentration. The process was performed on a set of 20 rock samples. The regression algorithm was then trained to find a mapping between image features and the semi-quantitative element concentrations (corresponding with XRF peaks). Experimental results demonstrate the potential effectiveness of the proposed approach in the context of a specific ore body.","Rocks,
Image color analysis,
Imaging,
Machine learning algorithms,
Image edge detection,
Surface texture,
Histograms"
Optimizing 3D NoC design for energy efficiency: A machine learning approach,"Three-dimensional (3D) Network-on-Chip (NoC) is an emerging technology that has the potential to achieve high performance with low power consumption for multicore chips. However, to fully realize their potential, we need to consider novel 3D NoC architectures. In this paper, inspired by the inherent advantages of small-world (SW) 2D NoCs, we explore the design space of SW network-based 3D NoC architectures. We leverage machine learning to intelligently explore the design space to optimize the placement of both planar and vertical communication links for energy efficiency. We demonstrate that the optimized 3D SW NoC designs perform significantly better than their 3D MESH counterparts. On an average, the 3D SW NoC shows 35% energy-delay-product (EDP) improvement over 3D MESH for the nine PARSEC and SPLASH2 benchmarks considered in this work. The highest performance improvement of 43% was achieved for RADIX. Interestingly, even after reducing the number of vertical links by 50%, the optimized 3D SW NoC performs 25% better than the fully connected 3D MESH, which is a strong indication of the effectiveness of our optimization methodology.","Three-dimensional displays,
Optimization,
Algorithm design and analysis,
Search problems,
Energy consumption,
Space exploration,
Machine learning algorithms"
Dynamic machine learning based matching of nonvolatile processor microarchitecture to harvested energy profile,"Energy harvesting systems without an energy storage device have to efficiently harness the fluctuating and weak power sources to ensure the maximum computational progress. While a simpler processor enables a higher turn-on potential with a weak source, a more powerful processor can utilize more energy that is harvested. Earlier work shows that different complexity levels of nonvolatile microarchitectures provide best fit for different power sources, and even different trails within same power source. In this work, we propose a dynamic nonvolatile microarchitecture by integrating all non-pipelined (NP), N-stage-pipeline (NSP), and Out of Order (OoO) cores together. Neural network machine learning algorithms are also integrated to dynamically adjust the microarchitecture to achieve the maximum forward progress. This integrated solution can achieve forward progress equal to 2.4× of the baseline NP architecture (1.82× of an OoO core).","Microarchitecture,
Computer architecture,
Registers,
Energy storage,
Switches,
Nonvolatile memory"
Memetic Self-Configuring Genetic Programming for Solving Machine Learning Problems,"A hybridization of self-configuring genetic programming algorithms (SelfCGPs) with a local search in the space of trees is fulfilled to improve their performance for symbolic regression problem solving and artificial neural network automated design. The local search is implemented with two neighborhood systems (1-level and 2-level neighborhoods), three strategies of a tree scanning (""full"", ""incomplete"" and ""truncated"") and two ways of a movement between adjacent trees (transition by the first improvement and the steepest descent). The Lamarckian local search is applied on each generation to ten percent of best individuals. The performance of all developed memetic algorithms is estimated on a representative set of test problems of the functions approximation as well as on real-world machine learning problems. It is shown that developed memetic algorithms require comparable amount of computational efforts but outperform the original SelfCGPs both for the symbolic regression and neural network design. The best variant of the local search always uses the steepest descent but different tree scanning strategies, namely, full scanning for the solving of symbolic regression problems and incomplete scanning for the neural network automated design. Additional advantage of the approach proposed is a possibility of the automated features selection.","Algorithm design and analysis,
Neurons,
Search problems,
Reliability,
Memetics,
Artificial neural networks"
Proactive Scalability and Management of Resources in Hybrid Clouds via Machine Learning,"In this paper, we present a novel framework for supporting the management and optimization of application subject to software anomalies and deployed on large scale cloud architectures, composed of different geographically distributed cloud regions. The framework uses machine learning models for predicting failures caused by accumulation of anomalies. It introduces a novel workload balancing approach and a proactive system scale up/scale down technique. We developed a prototype of the framework and present some experiments for validating the applicability of the proposed approaches.","Cloud computing,
Computer architecture,
Predictive models,
Proposals,
Computational modeling,
Computer crashes"
Forecasting Time Series Water Levels on Mekong River Using Machine Learning Models,"Forecasting water levels on Mekong river is an important problem needed to be studied for flood warning. In this paper, we investigate the application to forecasting of daily water levels at Thakhek station on Mekong river using machine learning models such as LASSO, Random Forests and Support Vector Regression (SVR). Experimental results showed that SVR was able to achieve feasible results, the mean absolute error of SVR is 0.486(m) while the acceptable error of a flood forecast model required by the Mekong River Commission is between 0.5(m) and 0.75(m).","Predictive models,
Yttrium,
Forecasting,
Rivers,
Data models,
Radio frequency,
Support vector machines"
Heat event detection in dairy cows with collar sensors: An unsupervised machine learning approach,"The detection of heat (estrus) events in pasture-based dairy cows fitted with on-animal sensors was investigated using an unsupervised learning. Accelerometer data from the cow collar sensors were used in this approach where the aim was to identify increased activity level (restlessness, increased walking for mating) and to find association with recorded heat events. High dimensional time series data from accelerometers were first segmented in windows followed by feature extractions. The extracted features are standard deviation, amplitude, energy and Fast Fourier Transform (FFT). K-means clustering algorithm was then applied across the windows for grouping. The groups were labeled in terms of activity intensities: high, medium and low. An activity index level (AIxL) was derived from the activity intensity labels. We compared the AIxL with recorded heat events and observed significant associations between the increased activities through high AIxL values and the observed heat events.","Heating,
Cows,
Time series analysis,
Sensors,
Accelerometers,
Indexes,
Feature extraction"
Machine learning-based energy management in a hybrid electric vehicle to minimize total operating cost,"This paper investigates the energy management problem in hybrid electric vehicles (HEVs) focusing on the minimization of the operating cost of an HEV, including both fuel and battery replacement cost. More precisely, the paper presents a nested learning framework in which both the optimal actions (which include the gear ratio selection and the use of internal combustion engine versus the electric motor to drive the vehicle) and limits on the range of the state-of-charge of the battery are learned on the fly. The inner-loop learning process is the key to minimization of the fuel usage whereas the outer-loop learning process is critical to minimization of the amortized battery replacement cost. Experimental results demonstrate a maximum of 48% operating cost reduction by the proposed HEV energy management policy.","Batteries,
Hybrid electric vehicles,
Ice,
Energy management,
Fuels,
Propulsion"
Exploring machine learning techniques for identification of cues for robot navigation with a LIDAR scanner,"In this paper, we report on our explorations of machine learning techniques based on backpropagation neural networks and support vector machines in building a cue identifier for mobile robot navigation using a LIDAR scanner. We use synthetic 2D laser data to identify a technique that is most promising for actual implementation in a robot, and then validate the model using realistic data. While we explore data preprocessing applicable to machine learning, we do not apply any specific extraction of features from the raw data; instead, our feature vectors are the raw data. Each LIDAR scan represents a sequence of values for measurements taken from progressive scans (with angles vary from 0° to 180°); i.e., a curve plotting distances as a functions of angles. Such curves are different for each cue, and so can be the basis for identification. We apply varied grades of noise to the ideal scanner measurement to test the capability of the generated models to accommodate for both laser inaccuracy and robot motion. Our results indicate that good models can be built with both back-propagation neural network applying Broyden-Fletcher-Goldfarb-Shannon (BFGS) optimization, and with Support Vector Machines (SVM) assuming that data shaping took place with a [-0.5, 0.5] normalization followed by a principal component analysis (PCA). Furthermore, we show that SVM can create models much faster and more resilient to noise, so that is what we will be using in our further research and can recommend for similar applications.","Training,
Standards,
Mobile robots,
Principal component analysis,
Navigation,
Support vector machines"
Fast CU partition decision using machine learning for screen content compression,"Screen Content Coding (SCC) extension is currently being developed by Joint Collaborative Team on Video Coding (JCT-VC), as the final extension for the latest High-Efficiency Video Coding (HEVC) standard. It employs some new coding tools and algorithms (including palette coding mode, intra block copy mode, adaptive color transform, adaptive motion compensation precision, etc.), and outperforms HEVC by over 40% bitrate reduction on typical screen contents. However, enormous computational complexity is introduced on encoder primarily due to heavy optimization processing, especially rate distortion optimization (RDO) for Coding Unit (CU) partition decision and mode selection. This paper proposes a novel machine learning based approach for fast CU partition decision using features that describe CU statistics and sub-CU homogeneity. The proposed scheme is implemented as a ""preprocessing"" module on top of the Screen Content Coding reference software (SCM-3.0). Compared with SCM-3.0, experimental results show that our scheme can achieve 36.8% complexity reduction on average with only 3.0% BD-rate increase over 11 JCT-VC testing sequences when encoded using ""All Intra"" (AI) configuration.","Image color analysis,
Histograms,
Training,
Channel coding,
Color,
Neural networks"
Feasibility Study of a Machine Learning Approach to Predict Dementia Progression,"We conducted a feasibility study of machine-learning to predict progression of cognitive impairment to Alzheimer's disease (AD) among individuals enrolled in the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our approach uses diverse participant information including genetic, imaging, biomarker, and neuropsychological data to predict transition to dementia in three clinical scenarios: short-term prediction (half or one year) based on a single assessment (simulating a ""new patient"" visit), short-term prediction based on information from two time points (simulating a ""follow up"" visit), and long-term (multiple years) prediction (simulating ongoing follow-up with repeated opportunities for assessment).","Dementia,
Informatics,
Genetics,
Biomedical imaging"
Machine learning techniques for improved data prefetching,"With the advent of teraflop-scale computing on both a single coprocessor and many-core designs, there is tremendous need for techniques to fully utilize the compute power by keeping cores fed with data. Data prefetching has been used as a popular method to hide memory latencies by fetching data proactively before the processor needs the data. Fetching data ahead of time from the memory subsystem into faster caches reduces observable latencies or wait times on the processor end and this improves overall program execution times. We study two types of prefetching techniques that are available on a 61-core Intel Xeon Phi co-processor, namely software (compiler-guided) prefetching and hardware prefetching on a variety of workloads. Using machine learning techniques, we synthesize workload phases and the sequence of phase patterns using raw performance data from hardware counters such as memory bandwidth, miss ratios, prefetches issued, etc. Furthermore, we use performance data from workloads with different impacts and behaviors under various prefetcher settings. Our contribution can help in future prefetching design in the following ways: (1) to identify phases within workloads that have different characteristics and behaviors and help dynamically modify prefetch types and intensities to suit the phase; (2) to manage auto setting of prefetcher knobs without great effort from the user; (3) to influence software and hardware prefetching interaction designs in future processors; and (4) to use valuable insights and performance data in many areas such as power provisioning for the nodes in a large cluster to maximize both energy and performance efficiencies.","Prefetching,
Hardware,
Coprocessors,
Tuning,
Measurement"
Integrating Static and Dynamic Malware Analysis Using Machine Learning,"Malware Analysis and Classification Systems use static and dynamic techniques, in conjunction with machine learning algorithms, to automate the task of identification and classification of malicious codes. Both techniques have weaknesses that allow the use of analysis evasion techniques, hampering the identification of malwares. In this work, we propose the unification of static and dynamic analysis, as a method of collecting data from malware that decreases the chance of success for such evasion techniques. From the data collected in the analysis phase, we use the C5.0 and Random Forest machine learning algorithms, implemented inside the FAMA framework, to perform the identification and classification of malwares into two classes and multiple categories. In our experiments, we showed that the accuracy of the unified analysis achieved an accuracy of 95.75% for the binary classification problem and an accuracy value of 93.02% for the multiple categorization problem. In all experiments, the unified analysis produced better results than those obtained by static and dynamic analyzes isolated.","Malware,
Software,
Machine learning algorithms,
Linux,
Heuristic algorithms,
Information security,
Support vector machines"
Behavior analysis of malware using machine learning,"In today's scenario, cyber security is one of the major concerns in network security and malware pose a serious threat to cyber security. The foremost step to guard the cyber system is to have an in-depth knowledge of the existing malware, various types of malware, methods of detecting and bypassing the adverse effects of malware. In this work, machine learning approach to the fore-going static and dynamic analysis techniques is investigated and reported to discuss the most recent trends in cyber security. The study captures a wide variety of samples from various online sources. The peculiar details about the malware such as file details, signatures, and hosts involved, affected files, registry keys, mutexes, section details, imports, strings and results from different antivirus have been deeply analyzed to conclude origin and functionality of malware. This approach contributes to vital cyber situation awareness by combining different malware discovery techniques, for example, static examination, to alter the session of malware triage for cyber defense and decreases the count of false alarms. Current trends in warfare have been determined.","Malware,
Classification algorithms,
Machine learning algorithms,
Monitoring,
HTML,
Internet"
A Machine Learning Based Approach to de novo Sequencing of Glycans from Tandem Mass Spectrometry Spectrum,"Recently, glycomics has been actively studied and various technologies for glycomics have been rapidly developed. Currently, tandem mass spectrometry (MS/MS) is one of the key experimental tools for identification of structures of oligosaccharides. MS/MS can observe MS/MS peaks of fragmented glycan ions including cross-ring ions resulting from internal cleavages, which provide valuable information to infer glycan structures. Thus, the aim of de novo sequencing of glycans is to find the most probable assignments of observed MS/MS peaks to glycan substructures without databases. However, there are few satisfiable algorithms for glycan de novo sequencing from MS/MS spectra. We present a machine learning based approach to de novo sequencing of glycans from MS/MS spectrum. First, we build a suitable model for the fragmentation of glycans including cross-ring ions, and implement a solver that employs Lagrangian relaxation with a dynamic programming technique. Then, to optimize scores for the algorithm, we introduce a machine learning technique called structured support vector machines that enable us to learn parameters including scores for cross-ring ions from training data, i.e., known glycan mass spectra. Furthermore, we implement additional constraints for core structures of well-known glycan types including N-linked glycans and O-linked glycans. This enables us to predict more accurate glycan structures if the glycan type of given spectra is known. Computational experiments show that our algorithm performs accurate de novo sequencing of glycans. The implementation of our algorithm and the datasets are available at http://glyfon.dna.bio.keio.ac.jp/.","Sequential analysis,
Ions,
Bioinformatics,
Linear programming,
Glycomics ,
Computational biology,
Dynamic programming"
An ensemble of machine learning and anti-learning methods for predicting tumour patient survival rates,"This paper primarily addresses a dataset relating to cellular, chemical and physical conditions of patients gathered at the time they are operated upon to remove colorectal tumours. This data provides a unique insight into the biochemical and immunological status of patients at the point of tumour removal along with information about tumour classification and post-operative survival. The relationship between severity of tumour, based on TNM staging, and survival is still unclear for patients with TNM stage 2 and 3 tumours. We ask whether it is possible to predict survival rate more accurately using a selection of machine learning techniques applied to subsets of data to gain a deeper understanding of the relationships between a patient's biochemical markers and survival. We use a range of feature selection and single classification techniques to predict the 5 year survival rate of TNM stage 2 and 3 patients which initially produces less than ideal results. The performance of each model individually is then compared with subsets of the data where agreement is reached for multiple models. This novel method of selective ensembling demonstrates that significant improvements in model accuracy on an unseen test set can be achieved for patients where agreement between models is achieved. Finally we point at a possible method to identify whether a patients prognosis can be accurately predicted or not.","Cancer,
Support vector machines,
Tumors,
Prediction algorithms,
Immune system,
Biological system modeling,
Data models"
Pruned search: A machine learning based meta-heuristic approach for constrained continuous optimization,"Searching for solutions that optimize a continuous function can be difficult due to the infinite search space, and can be further complicated by the high dimensionality in the number of variables and complexity in the structure of constraints. Both deterministic and stochastic methods have been presented in the literature with a purpose of exploiting the search space and avoiding local optima as much as possible. In this research, we develop a machine learning framework aiming to `prune' the search effort of both types of optimization techniques by developing meta-heuristics, attempting to knowledgeably reordering the search space and reducing the search region. Numerical examples demonstrate that this approach can effectively find the global optimal solutions and significantly reduce the computational time for seven benchmark problems with variable dimensions of 100, 500 and 1000, compared to Genetic Algorithms.","Optimization,
Search problems,
Complexity theory,
Linear programming,
Data mining,
Computational geometry,
Approximation methods"
Efficient autism spectrum disorder prediction with eye movement: A machine learning framework,"We propose an autism spectrum disorder (ASD) prediction system based on machine learning techniques. Our work features the novel development and application of machine learning methods over traditional ASD evaluation protocols. Specifically, we are interested in discovering the latent patterns that possibly indicate the symptom of ASD underneath the observations of eye movement. A group of subjects (either ASD or non-ASD) are shown with a set of aligned human face images, with eye gaze locations on each image recorded sequentially. An image-level feature is then extracted from the recorded eye gaze locations on each face image. Such feature extraction process is expected to capture discriminative eye movement patterns related to ASD. In this work, we propose a variety of feature extraction methods, seeking to evaluate their prediction performance comprehensively. We further propose an ASD prediction framework in which the prediction model is learned on the labeled features. At testing stage, a test subject is also asked to view the face images with eye gaze locations recorded. The learned model predicts the image-level labels and a threshold is set to determine whether the test subject potentially has ASD or not. Despite the inherent difficulty of ASD prediction, experimental results indicates statistical significance of the predicted results, showing promising perspective of this framework.","Face,
Histograms,
Feature extraction,
Support vector machines,
Dictionaries,
Visualization,
Kernel"
Machine learning based detection of compensatory balance responses to lateral perturbation using wearable sensors,"Loss of balance is prevalent in the older population and also in people who have mobility impairment. The primary aim of the present paper is to develop an efficient classifier to automatically distinguish compensatory balance responses (or near-falls) from regular stepping patterns. In this study, 5 young, healthy subjects were perturbed by lateral pushes while walking and the compensatory reactions were recorded by three wearable inertial measurement units (IMUs). Time domain features of these signals were extracted and reduced, using different dimension reduction methods, i.e., PCA, SPCA and KSPCA. The performance of k-nearest neighbor (k-NN) and support vector machines (SVMs) classification methods for detection of compensatory balance responses is investigated. The results of this study advances wearable measurement methods to accurately and reliably monitor gait balance control behavior in at-home settings (unsupervised conditions), over long periods of time (i.e., weeks, months). Building on the current study, subsequent research will examine ambulatory data to identify balance recovery processes for clinical assessment of fall risk.","Acceleration,
Feature extraction,
Principal component analysis,
Support vector machines,
Sternum,
Wearable sensors,
Legged locomotion"
Indoor positioning by distributed machine-learning based data analytics on smart gateway network,"Real-time data analysis on sensor nodes is challenging due to limited computing resources. A changing environment where received signal strength (RSSI) varies with time makes it more complex to update position predictors for real-time indoor positioning. Based on the distributed collection and analytics of RSSI values in a gateway network, a time-efficient workload-based (WL) distributed support vector machine (WL-DSVM) algorithm is introduced in this paper to perform the indoor positioning. Experimental results show that with 5 distributed sensor nodes running in parallel, the proposed WL-DSVM can achieve a performance improvement in run time up to 3.2× with a stable positioning accuracy.","Support vector machines,
Logic gates,
Training,
IEEE 802.11 Standard,
Real-time systems,
Data analysis,
Prediction algorithms"
Automated frequency selection for machine-learning based EH/EW prediction from S-Parameters,"In the field of High Speed SerDes (HSS) channel analysis and design, the most widely accepted metrics for gauging signal integrity are Time Domain (TD) metrics: Bit Error Rate (BER), Eye-Height (EH) and Eye-Width (EW). With increasing bit-rates, TD simulations are getting compute-time intensive especially as the BER criterion is getting lower. Learning based mapping of Frequency Domain (FD) S-Parameter data to EH/EW in TD provides a fast alternative solution for thorough design-space exploration. A key challenge in this mapping procedure is the identification of the optimal frequency points in the S-Parameter data that are used for training the learning network. This paper outlines a methodology to identify the minimal set of critical frequency points using a Fast Correlation Based Feature (FCBF) selection algorithm. This technique is applied for prediction of EH/EW for a PCIe Gen 3 interface and the prediction accuracy is quantified.","Artificial neural networks,
Scattering parameters,
Bit error rate,
Training,
Correlation,
Measurement,
Predictive models"
The failure analysis of extreme learning machine on big data and the counter measure,"Extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) was known for its extremely fast learning speed while maintaining acceptable generalization. Unfortunately, the failure of ELM on big data occurs frequently. The course is, the main computation of ELM focus on the calculation of generalized inverse of hidden layer output matrix, which depends on singular value decomposition (SVD) and has very low efficiency especially on high order matrix. In view of this high calculation complexity directly courses the failure of ELM on big data, normal equation extreme learning machine is proposed, which use the normal equation to reduce the size of the matrix equation and overcome the failure. The experiments on benchmarks show that the new proposed model has better performance than the ELM, so as to have more potential for large scale data learning.","singular value decomposition,
Big Data,
failure analysis,
feedforward neural nets,
learning (artificial intelligence),
matrix algebra"
Automated microscopy and machine learning for expert-level malaria field diagnosis,"The optical microscope is one of the most widely used tools for diagnosing infectious diseases in the developing world. Due to its reliance on trained microscopists, field microscopy often suffers from poor sensitivity, specificity, and reproducibility. The goal of this work, called the Autoscope, is a low-cost automated digital microscope coupled with a set of computer vision and classification algorithms, which can accurately diagnose of a variety of infectious diseases, targeting use-cases in the developing world. Our initial target is malaria, because of the high difficulty of the task and because manual microscopy is currently a central but highly imperfect tool for malaria work in the field. In addition to diagnosis, the algorithm performs species identification and quantitation of parasite load, parameters which are critical in many field applications but which are not effectively determined by rapid diagnostic tests (RDTs). We have built a hardware prototype which can scan approximately 0.1 μL of blood volume in a standard Giemsa-stained thick smear blood slide in approximately 20 minutes. We have also developed a comprehensive machine learning framework, leveraging computer vision and machine learning techniques including support vector machines (SVMs) and convolutional neural networks (CNNs). The Autoscope has undergone successful initial field testing for malaria diagnosis in Thailand.","Microscopy,
Diseases,
Blood,
Image color analysis,
Optical microscopy,
Computer vision,
Support vector machines"
A new technique for restricted Boltzmann machine learning,"Over the last decade, deep belief neural networks have been a hot topic in machine learning. Such networks can perform a deep hierarchical representation of input data. The first layer can extract low-level features, the second layer can extract high-level features and so on. In general, deep belief neural network represents many-layered perceptron and permits to overcome some limitations of conventional multilayer perceptron due to deep architecture. In this work we propose a new training technique called Reconstruction Error-Based Approach (REBA) for deep belief neural network based on restricted Boltzmann machine. In contrast to classical Hinton's training approach, which is based on a linear training rule, the proposed technique is based on a nonlinear learning rule. We demonstrate the performance of REBA technique for the MNIST dataset visualization. The main contribution of this paper is a novel view on the training of a restricted Boltzmann machine.","Training,
Feature extraction,
Mathematical model,
Data visualization,
Biological neural networks,
Mean square error methods"
Machine Learning for Achieving Self-* Properties and Seamless Execution of Applications in the Cloud,"Software anomalies are recognized as a major problem affecting the performance and availability of many computer systems. Accumulation of anomalies of different nature, such as memory leaks and unterminated threads, may lead the system to both fail or work with suboptimal performance levels. This problem particularly affects web servers, where hosted applications are typically intended to continuously run, thus incrementing the probability, therefore the associated effects, of accumulation of anomalies. Given the unpredictability of occurrence of anomalies, continuous system monitoring would be required to detect possible system failures and/or excessive performance degradation in order to timely start some recovering procedure. In this paper, we present a Machine Learning-based framework for proactive management of client-server applications in the cloud. Through optimized Machine Learning models and continually measuring system features, the framework predicts the remaining time to the occurrence of some unexpected event (system failure, service level agreement violation, etc.) of a virtual machine hosting a server instance of the application. The framework is able to manage virtual machines in the presence of different types anomalies and with different anomaly occurrence patterns. We show the effectiveness of the proposed solution by presenting results of a set of experiments we carried out in the context of a real world-inspired scenario.","Servers,
Monitoring,
Cloud computing,
Predictive models,
System performance,
Computer crashes"
Feature selection and recognition of electroencephalogram signals: An extreme learning machine and genetic algorithm-based approach,"The effective recognition approach of the electroencephalogram (EEG) signals can significantly boost the performance and the development of the EEG-based diagnosis and treatment. A new approach which combines the Extreme Learning Machine (ELM) with the Genetic algorithm (GA) is proposed in this paper. In the proposed approach, the ELM is used both as the final classifier and the fitness function for the GA to select the optimal feature subset from the initial features extracted through time-frequency (TF) analysis. The GA is adopted as the complementary input optimization mechanism to improve the performance of the ELM. To testify the performance of the proposed approach, experiments were simulated using the real-world EEG signals of 2003 International BCI Competition dataset. The recognition results have proved the effectiveness of the proposed approach.","time-frequency analysis,
brain-computer interfaces,
electroencephalography,
feature selection,
genetic algorithms,
learning (artificial intelligence),
medical signal processing,
patient diagnosis,
patient treatment"
Assessing ergonomic and postural data for pain and fatigue markers using machine learning techniques,"Ergonomic data obtained from trials with human participants at a number of workstations are evaluated in terms of whether different workstations elicit different fatigue and pain responses. Data is analyzed using a pair of simple machine-learning based classifiers in order to identify activities associated with the workstations that lead to or avoid pain and fatigue. Results indicate that information content sufficient to predict pain and fatigue is present in this data, with evidence of information increase consistent with postures held for a period of time. Additional analysis will be performed to isolate postures associated with fatigue and pain in follow-up work.","Pain,
Fatigue,
Neck,
Workstations,
Hafnium,
Back,
Atmospheric measurements"
Machine learning for the activation of contraflows during hurricane evacuation,"Contraflows are a critical part of an emergency evacuation plan. In most cases, a contraflow lane reversal will double the capacity of key evacuation routes. The Contraflow plan for the evacuation of southeast Louisiana during a hurricane threat uses a typical schedule for the activation of contraflows based on the predicted time of landfall. This work will apply machine learning techniques using real-time traffic data to schedule the activation of contraflows. Optimizing the Contraflow plan should increase the effectiveness of the evacuation plan by increasing the flow of evacuation traffic based on demand and retaining the availability of incoming traffic until contraflow lanes are needed. These techniques could be applied to other locations, including those without an existing evacuation plan.","Hurricanes,
Machine learning algorithms,
Delays,
Algorithm design and analysis,
Supervised learning,
Prediction algorithms,
Classification algorithms"
Towards a Generic Trust Management Framework Using a Machine-Learning-Based Trust Model,"Nowadays, the ever-growing capabilities in computer communication networks have entitled and encouraged developers and researchers to build collaborative applications, systems, and devices. On the one hand with increased collaboration, several advantages have been obtained, but, on the other hand, issues may arise due to untrustworthy interactions. To address these issues, many researchers have studied trust as a computer science concept. Nevertheless, one of the greatest challenges in the trust domain is the lack of a generic trust management framework that will ease and encourage existing collaborative systems to adopt such concepts. In this paper, we propose a generic trust management framework which is capable of processing different trust features as required. We propose a RESTful message exchanging architecture, and a trust model based on the solution of a multi-class classification problem using machine learning techniques, namely Support Vector Machines(SVM).","Context,
Engines,
Support vector machines,
Fuzzy logic,
Monitoring,
Security,
Collaboration"
Machine learning techniques in optical communication,"Techniques from the machine learning community are reviewed and employed for laser characterization, signal detection in the presence of nonlinear phase noise, and nonlinearity mitigation. Bayesian filtering and expectation maximization are employed within nonlinear state-space framework for parameter tracking.","Phase noise,
Nonlinear optics,
Optical noise,
Optical polarization,
Bayes methods,
Optical signal processing,
Optical variables measurement"
Nonlinear decision boundary created by a machine learning-based classifier to mitigate nonlinear phase noise,"A machine learning-based classifier, namely SVM, is introduced to create the nonlinear decision boundary in M-ary PSK-based coherent optical system to mitigate NLPN. The maximum transmission distance and LPRD tolerance are improved by 480 km and 3.3 dBm for 8PSK.","Support vector machines,
Binary phase shift keying,
Bit error rate,
Maximum likelihood estimation,
Optical fibers,
Optical fiber communication"
An incremental learning approach for restricted boltzmann machines,"Determination of model complexity is a challenging issue to solve computer vision problems using restricted boltzmann machines (RBMs). Many algorithms for feature learning depend on cross-validation or empirical methods to optimize the number of features. In this work, we propose an learning algorithm to find the optimal model complexity for the RBMs by incrementing the hidden layer. The proposed algorithm is composed of two processes: 1) determining incrementation necessity of neurons and 2) computing the number of additional features for the increment. Specifically, the proposed algorithm uses a normalized reconstruction error in order to determine incrementation necessity and prevent unnecessary increment for the number of features during training. Our experimental results demonstrated that the proposed algorithm converges to the optimal number of features in a single layer RBMs. In the classification results, our model could outperform the non-incremental RBM.","Neurons,
Complexity theory,
Computational modeling,
Training,
Error analysis,
Training data,
Standards"
Is audio signal processing still useful in the era of machine learning?,"Audio signal processing has long been the obvious approach to problems such as microphone array processing, active noise control, or speech enhancement. Yet, it is increasingly being challenged by black-box machine learning approaches based on, e.g., deep neural networks (DNN), which have already achieved superior results on certain tasks. In this talk, I will try to convince that machine learning approaches shouldn't be disregarded, but that black boxes won't solve these problems either. There is hence an opportunity for signal processing researchers to join forces with machine learning researchers and solve these problems together. I will provide examples of this multi-disciplinary approach for audio source separation and robust automatic speech recognition.","Signal processing,
Speech recognition,
Multiple signal classification,
Speech,
Conferences,
Acoustics,
Laboratories"
Machine learning approach for quality assessment and prediction in large software organizations,"The importance of software in everyday products and services has been on constant rise and so is the complexity of software. In face of this rising complexity and our dependence on software — measuring, maintaining and increasing software quality is of critical importance. Software metrics provide a quantitative means to measure and thus control various attributes of software systems. In the paradigm of machine learning, software quality prediction can be cast as a classification or concept learning problem. In this paper we provide a general framework for applying machine learning approaches for assessment and prediction of software quality in large software organizations. Using ISO 15939 measurement information model we show how different software metrics can be used to build software quality model which can be used for quality assessment and prediction that satisfies the information need of these organizations with respect to quality. We also document how machine learning approaches can be effectively used for such evaluation.","ISO Standards,
Software quality,
IEC Standards,
Organizations,
Software measurement,
Adaptation models"
Improving Power Grid Monitoring Data Quality: An Efficient Machine Learning Framework for Missing Data Prediction,"Big data techniques has been applied to power grid for the evaluation and prediction of grid conditions. However, the raw data quality rarely can meet the requirement of precise data analytics since raw data set usually contains samples with missing data to which the common data mining models are sensitive. Though classic interpolation or neural network methods can been used to fill the gaps of missing data, their predicted data often fail to fit the rules of power grid conditions. This paper presents a machine learning framework (OR_MLF) to improve the prediction accuracy for datasets with missing data points, which mainly combines preprocessing, optimizing support vector machine (OSVM) and refining SVM (RSVM). On top of the OSVM engine, the scheme introduces dedicated data training strategies. First, the original data originating from data generation facilities is preprocessed through standardization. Traditional SVM is then trained to obtain a preliminary prediction model. Next, the optimized SVM predictors are achieved with new training data set, which is extracted based on the preliminary prediction model. Finally, the missing data prediction result depending on OSVM is selectively inputted into the traditional SVM and the refined SVM is lastly accomplished. We test the OR_MLF framework on missing data prediction of power transformers in power grid system. The experimental results show that the predictors based on the proposed framework achieve lower mean square error than traditional ones. Therefore, the framework OR_MLF would be a good candidate to predict the missing data in power grid system.","Support vector machines,
Data models,
Predictive models,
Training,
Data mining,
Feature extraction,
Power grids"
A seizure-detection IC employing machine learning to overcome data-conversion and analog-processing non-idealities,"This paper presents a seizure-detection system wherein the accuracy required of the analog frontend is substantially relaxed. Typically, readout of electroencephalogram (EEG) signals would dominate the energy of such a system, due to the precision (noise, linearity) requirements. The presented system performs data conversion and analog multiplication for EEG feature extraction via simple circuits to demonstrate that feature errors can be overcome by appropriate retraining of a classification model, using a machine-learning algorithm. This precludes the need to design a high-precision frontend. The prototype, in 32nm CMOS, results in features whose RMS error normalized to their ideal values is 1.16 (i.e. errors are larger than ideal values). An ideal implementation of the seizure detector exhibits sensitivity, latency, false alarms of 5/5, 2.0 sec., 8, respectively. The feature errors degrade this to 5/5, 3.6 sec., 443, causing high false alarms; but retraining of the classification model restores this to 5/5, 3.4 sec., 4.",Decision support systems
A monitoring system to prepare machine learning data sets for earthquake prediction based on seismic-acoustic signals,"Estimating the location, time and magnitude of a possible earthquake has been the subject of many studies. Various methods have been tried using many input variables such as temperature changes, seismic movements, weather conditions etc. The relation between recorded seismic-acoustic data and occurring an anomalous seismic processes (ASP) has been proved in articles written by Aliev and et al. [1–4]. But it is difficult to predict the location, time and magnitude of the earthquake by using these data. In this study, it is aimed to prepare a data set/sets for prediction of an earthquake to be used in machine learning algorithms. An Earthquake-Well Signal Monitoring Software has been developed to construct these data sets. This study uses the on-line recordings of robust noise monitoring (RNM) signals of ASP from stations in Azerbaijan. An interface for analyzing the recordings and mapping them with previous earthquakes is designed.","Earthquakes,
Monitoring,
Databases,
Training,
Computer science,
Software,
Robustness"
Simple modifications on heuristic rule generation and rule evaluation in Michigan-style fuzzy genetics-based machine learning,"Fuzzy genetics-based machine learning (FGBML) is one of the representative approaches to obtain a set of fuzzy if-then rules by evolutionary computation. A number of FGBML methods have been proposed so far. Among them, Michigan-style approaches are popular thanks to thier lower computational cost than Pittsburgh approaches. In this study, we introduce two simple modifications for our Michigan-style FGBML. One is related to heuristic rule generation. In the original FGBML, each rule in an initial population is generated from a randomly-selected training pattern in a heuristic manner. The heuristic rule generation also performs during evolution where each rule is generated from a misclassified pattern. As its modification, we propose the use of multiple patterns to generate each fuzzy if-then rule. The other is related to the fitness calculation. In the original FGBML, the fitness of each rule is calculated as the number of correctly classified training patterns, while the number of misclassified patterns is ignored. As its modification, we incorporate a penalty term into the fitness function. Through computational experiments using 20 benchmark data sets, we examine the effects of these two modifications on the search ability of our Michigan-style FGBML.","Training,
Sociology,
Statistics,
Genetics,
Fuzzy sets,
Fuzzy systems,
Probabilistic logic"
Maximizing Hardware Prefetch Effectiveness with Machine Learning,"Modern processors are equipped with multiple hardware prefetchers, each of which targets a distinct level in the memory hierarchy and employs a separate prefetching algorithm. However, different programs require different subsets of these prefetchers to maximize their performance. Turning on all available prefetchers rarely yields the best performance and, in some cases, prefetching even hurts performance. This paper studies the effect of hardware prefetching on multithreaded code and presents a machine-learning technique to predict the optimal combination of prefetchers for a given application. This technique is based on program characterization and utilizes hardware performance events in conjunction with a pruning algorithm to obtain a concise and expressive feature set. The resulting feature set is used in three different learning models. All necessary steps are implemented in a framework that reaches, on average, 96% of the best possible prefetcher speedup. The framework is built from open-source tools, making it easy to extend and port to other architectures.","Prefetching,
Hardware,
Algorithms,
Optimization,
Testing,
Training"
An Improved Machine Learning Scheme for Data-Driven Fault Diagnosis of Power Grid Equipment,"In recent power grid systems, data-driven approach has been taken to grid condition evaluation and classification after successful adoption of big data techniques in internet applications. However, the raw training data from single monitoring system, e.g. dissolved gas analysis (DGA), are rarely sufficient for training in the form of valid instances and the data quality can rarely meet the requirement of precise data analytics since raw data set usually contains samples with noisy data. This paper proposes a machine learning scheme (PCA_IR) to improve the accuracy of fault diagnose, which combines dimension-increment procedure based on association analysis, dimension-reduction procedure based on principal component analysis and back propagation neural network (BPNN). First, the dimension of training data is increased by adding selected data which originates from different source such as production management system (PMS) to the original data obtained by DGA. The added data would also inevitably result in more noise. Thus, we then take advantage of the PCA method to reduce the noise in the training data as well as retaining significant information for classification. Finally, the new training data yielded after PCA procedure is inputted into BPNN for classification. We test the PCA_IR scheme on fault diagnosis of power transformers in power grid system. The experimental results show that the classifiers based on our scheme achieve higher accuracy than traditional ones. Therefore, the scheme PCA_IR would be successfully deployed for fault diagnosis in power grid system.","Fault diagnosis,
Principal component analysis,
Accuracy,
Power grids,
Correlation,
Power transformers,
Correlation coefficient"
Fuzzy Restricted Boltzmann Machine for the Enhancement of Deep Learning,"In recent years, deep learning caves out a research wave in machine learning. With outstanding performance, more and more applications of deep learning in pattern recognition, image recognition, speech recognition, and video processing have been developed. Restricted Boltzmann machine (RBM) plays an important role in current deep learning techniques, as most of existing deep networks are based on or related to it. For regular RBM, the relationships between visible units and hidden units are restricted to be constants. This restriction will certainly downgrade the representation capability of the RBM. To avoid this flaw and enhance deep learning capability, the fuzzy restricted Boltzmann machine (FRBM) and its learning algorithm are proposed in this paper, in which the parameters governing the model are replaced by fuzzy numbers. This way, the original RBM becomes a special case in the FRBM, when there is no fuzziness in the FRBM model. In the process of learning FRBM, the fuzzy free energy function is defuzzified before the probability is defined. The experimental results based on bar-and-stripe benchmark inpainting and MNIST handwritten digits classification problems show that the representation capability of FRBM model is significantly better than the traditional RBM. Additionally, the FRBM also reveals better robustness property compared with RBM when the training data are contaminated by noises.","Approximation methods,
Markov processes,
Optimization,
Robustness,
Probability distribution,
Training,
Linear programming"
Machine Learning and Mass Estimation Methods for Ground-Based Aircraft Climb Prediction,"In this paper, we apply machine learning methods to improve the aircraft climb prediction in the context of ground-based applications. Mass is a key parameter for climb prediction. As it is considered a competitive parameter by many airlines, it is currently not available to ground-based trajectory predictors. Consequently, most predictors today use a reference mass that may be different from the actual aircraft mass. In previous papers, we have introduced a least squares method to estimate the mass from past trajectory points, using the physical model of the aircraft. Another mass estimation method, based on an adaptive mechanism, has also been proposed by Schultz et al. We now introduce a new approach, in which the mass is considered the response variable of a prediction model that is learned from a set of example trajectories. This machine learning approach is compared with the results obtained when using the base of aircraft data (BADA) reference mass or the two state-of-the-art mass estimation methods. In these experiments, nine different aircraft types are considered. When compared with the baseline method (respectively, the mass estimation methods), the Machine Learning approach reduces the RMSE (Root Mean Square Error) on the predicted altitude by at least 58% (resp. 27%) when assuming the speed profile to be known, and by at least 29% (resp. 17%) when using the BADA speed profile except for the aircraft types E145 and F100. For these types, the observed speed profile is far from the BADA speed profile.","Aircraft,
Trajectory,
Predictive models,
Machine learning"
Accelerating common machine learning algorithms through GPGPU symbolic computing,"This paper evaluates the implementation of two well known machine learning algorithms, kernel k-means and logistic regression, using Graphics Processing Units (GPUs). The main goal was to do an implementation that exploited the processing power of GPU while keeping the implementation simple, easy to understand and modify. The paper presents an empirical analysis of the performance of the implementations under different execution scenarios.","Graphics processing units,
Kernel,
Logistics,
Clustering algorithms,
Libraries,
Machine learning algorithms,
Algorithm design and analysis"
Machine learning based multi-physical-model blending for enhancing renewable energy forecast - improvement via situation dependent error correction,"With increasing penetration of solar and wind energy to the total energy supply mix, the pressing need for accurate energy forecasting has become well-recognized. Here we report the development of a machine-learning based model blending approach for statistically combining multiple meteorological models for improving the accuracy of solar/wind power forecast. Importantly, we demonstrate that in addition to parameters to be predicted (such as solar irradiance and power), including additional atmospheric state parameters which collectively define weather situations as machine learning input provides further enhanced accuracy for the blended result. Functional analysis of variance shows that the error of individual model has substantial dependence on the weather situation. The machine-learning approach effectively reduces such situation dependent error thus produces more accurate results compared to conventional multi-model ensemble approaches based on simplistic equally or unequally weighted model averaging. Validation over an extended period of time results show over 30% improvement in solar irradiance/power forecast accuracy compared to forecasts based on the best individual model.","Predictive models,
Atmospheric modeling,
Wind forecasting,
Clouds,
Accuracy,
Numerical models"
A study of power distribution system fault classification with machine learning techniques,"Power system protection includes the process of identifying and correcting faults (failures) before fault currents cause damage to utility equipment or customer property. In distribution systems, where the number of measurements is increasing, there is an opportunity to improve fault classification techniques. This work presents a study in fault classification using machine learning techniques and quarter-cycle fault signatures. Separate voltage- and current-based feature vectors are defined using multi-resolution analysis and input to a two-stage classifier. The classifier was trained and tested on experimental fault data collected in Drexel University's Reconfigurable Distribution Automation and Control (RDAC) software/hardware laboratory. Results show: (1) non-linear, and even non-contiguous decision regions on a “fault plane”, using a phase voltage-based feature, and (2) an accurate classifier for determining the grounding status of multi-phase faults, using a neutral current-based feature.","Circuit faults,
Fault diagnosis,
Support vector machines,
Classification algorithms,
Grounding,
Discrete wavelet transforms"
A comparative study of different machine learning methods for electricity prices forecasting of an electricity market,"Generally, it is difficult to accurately forecast electricity prices because they are unpredictable. Yet, accurate price forecasting is expected to provide crucial information, needed by power producers and consumers to bid strategically, thereby decreasing their risks and increasing their profits in the electricity market. In this paper, two models using artificial neural networks (ANN) and support vector machines (SVM) were developed for electricity price forecasting. In addition, ant colony optimization (ACO) was used to reduce the feature space and give the best attribute subset for ANN model. Using ACO for feature selection significantly reduced the training time for ANN-based electricity price forecasting model while the results were almost as accurate as those from ANN model.","Forecasting,
Support vector machines,
Artificial neural networks,
Electricity supply industry,
Predictive models,
Mathematical model"
Machine-Learning Aided Optimal Customer Decisions for an Interactive Smart Grid,"In this paper, a hierarchical smart grid architecture is presented. The concept of smart home is extended in two aspects: 1) from traditional households with smart devices, such as advanced metering infrastructure, to intelligent entities with instantaneous and distributive decision-making capabilities; and 2) from individual households to general customer units of possibly large scales. We then develop a hidden mode Markov decision process (HM-MDP) model for a customer real-time decision-making problem. This real-time decision-making framework can effectively be integrated with demand response schemes, which are prediction based and therefore inevitably lead to real-time power-load mismatches. With the Baum-Welch algorithm adopted to learn the nonstationary dynamics of the environment, we propose a value iteration (VI)-based exact solution algorithm for the HM-MDP problem. Unlike conventional VI, the concept of parsimonious sets is used to enable a finite representation of the optimal value function. Instead of iterating the value function in each time step, we iterate the representational parsimonious sets by using the incremental pruning algorithm. Although this exact algorithm leads to optimal policies giving maximum rewards for the smart homes, its complexity suffers from the curse of dimensionality. To obtain a low-complexity real-time algorithm that allows adaptively incorporating new observations as the environment changes, we resort to Q-learning-based approximate dynamic programming. Q-learning offers more flexibility in practice because it does not require specific starting and ending points of the scheduling period. Performance analysis of both exact and approximate algorithms, as compared with the other possible alternative decision-making strategies, is presented in simulation results.","Decision making,
Smart grids,
Heuristic algorithms,
Load modeling,
Real-time systems,
Microgrids,
Approximation algorithms"
"Automated change detection in satellite images using machine learning algorithms for Delhi, India","In remote sensing, change detection is used in land use and cover analysis, forest or vegetation assessment and, flood monitoring. Although manual change detection is an option, the time required for it can be prohibitive. It is also highly subjective depending on the expertise of the analyst. Hence, the need for automated methods for such analysis tasks have emerged and thus gives rise to unsupervised machine learning algorithm. This paper analyzes the effectiveness of the three types of unsupervised Machine learning algorithms (MLAs) for change detection to detect the change in some of the dominant classes in an urban area, such as, vegetation, built-up and water bodies. Landsat 5 TM and Landsat 8 OLI imageries have been selected for a part of New Delhi and its nearby area. In this study, three indices namely Normalized Difference Built-up Index (NDBI), Modified Normalized Difference Water Index (MNDWI) and Modified Soil Adjusted Vegetation Index (MSAVI2) have been generated from the Landsat data. Three algorithms, namely, K-Means, FCM and EM have been used, since these represent three different concepts in machine learning category i.e. partition based, fuzzy and probability based respectively. The same have been implemented in MATLAB for identifying different type of land cover over a period of 1998-2011. Considering both the intra- and inter-cluster distances, silhouette coefficients have been used for evaluation of cluster quality. The change is quantified in terms of percentage that depends upon the outcome of clustering and the number of pixel grouped in each class i.e. urban, vegetation and water.","Remote sensing,
Machine learning algorithms,
Vegetation mapping,
Satellites,
Classification algorithms,
Earth,
Clustering algorithms"
Diabetic retinal exudates detection using machine learning techniques,"Diabetic Retinopathy (DR) is an eye filled illness caused by the complication of polygenic disease and that is to be detected accurately for timely treatment. As polygenic disease progresses, the vision of a patient could begin to deteriorate and leads to blindness. In this proposed work, the presence or absence of retinal exudates are detected using machine learning (ML) techniques. To detect the presence of exudates features like Mean, Standard deviation, Centroid and Edge Strength are extracted from Luv color space after segmenting the Retinal image. A total of 100 images were used, out of which 80 images were used for training and 20 images were used for testing. The classification task carried out with classifiers like Naive bayes (NB), Multilayer Perceptron (MLP) and Extreme Learning Machine (ELM). Experimental results shows that the model built using Extreme Learning Machine outperforms other two models and effectively detects the presence of exudates in retinal images.","Diabetes,
Image color analysis,
Image segmentation,
Neurons,
Feature extraction,
Accuracy,
Retina"
Comparison of machine learning algotithms for leaf area index retrieval from time series MODIS data,"Temporally continuous and high quality leaf area index (LAI) products are urgently needed for crop growth monitoring, yield estimation and other research fields. However, most of the methods used to retrieve LAI just use a single phase satellite observational data to estimate LAI. Because of the impact of clouds and aerosols, the LAI products generated by these methods are temporally discontinuous. In this study, performance of three machine learning algorithms for parameter estimation using time series data is evaluated. The three machine learning algorithms are back-propagation neutral network (BPNN), general regression neutral networks (GRNNs) and multivariate adaptive regression splines (MARS). The results show that these machine learning algorithms have a good performance in time series LAI retrieval and GRNNs outperform the other algorithms.","Machine learning algorithms,
MODIS,
Time series analysis,
Mars,
Reflectivity,
Training,
Indexes"
Securing virtual execution environments through machine learning-based intrusion detection,"Virtualization has gained tremendous traction as the go-to computing technology due to many advantages it offers such as server consolidation, increased reliability and availability, and enhanced security through isolation of virtual machines. Within a virtual machine itself, securing workloads against cyber attacks becomes an increasingly critical task. In this paper, we present the application of machine learning and anomaly detection to automatically detect malicious attacks on typical server workloads running on virtual machines. An integral aspect of the work is finding the right set of features that can be used to distinguish normal from malicious activity.","Malware,
Servers,
Feature extraction,
Machine learning algorithms,
Home appliances,
Virtual machining,
Intrusion detection"
Replacing radiative transfer models by surrogate approximations through machine learning,"Physically-based radiative transfer models (RTMs) help in understanding the processes occurring on the Earth's surface and their interactions with vegetation and atmosphere. However, advanced RTMs can take a long computational time, which makes them unfeasible in many real applications. To overcome this problem, it has been proposed to substitute RTMs through so-called emulators. Emulators are statistical models that approximate the functioning of RTMs. They are advantageous in real practice because of the computational efficiency and excellent accuracy and flexibility for extrapolation. We here present an `Emulator toolbox' that enables analyzing three multi-output machine learning regression algorithms (MO-MLRAs) on their ability to approximate an RTM. As a proof of concept, a case study on emulating sun-induced fluorescence (SIF) is presented. The toolbox is foreseen to open new opportunities in the use of advanced RTMs, in which both consistent physical assumptions and data-driven machine learning algorithms live together.","Fluorescence,
Table lookup,
Computational modeling,
Training,
Approximation methods,
Accuracy,
Biological system modeling"
AMSR2 soil moisture downscaling using multisensor products through machine learning approach,"Soil moisture is important to understand the interaction between the land and the atmosphere, and has an influence on hydrological and agricultural processes such as drought and crop yield. In-situ measurements at stations have been used to monitor soil moisture. However, data measured in the field are point-based and difficult to represent spatial distribution of soil moisture. Remote sensing techniques using microwave sensors provide spatially continuous soil moisture. The spatial resolution of remotely sensed soil moisture based on typical passive microwave sensors is coarse (e.g., tens of kilometers), which is inadequate for local or regional scale studies. In this study, AMSR2 soil moisture was downscaled to 1km using MODIS products that are closely related to soil moisture through statistical ordinary least squares (OLS) and random forest (RF) machine learning approaches. RF (r2=0.96, rmse=0.06) outperformed OLS (r2=0.47, rmse=0.16) in modeling soil moisture possibly because RF is much flexible through randomization and adopts an ensemble approach. Both approaches identified T·V (i.e., multiplication between land surface temperature and normalized difference vegetation index) and evapotranspiration. AMSR2 soil moisture produced from the VUA-NASA algorithm appeared overestimated at high elevation areas because the characteristics of ground data for validation and correction used in the algorithm were different from those in our study area. In future study, AMSR2 soil moisture based on the JAXA algorithm will be evaluated with additional input variables including land cover, elevation and precipitation.","Soil moisture,
Radio frequency,
MODIS,
Spatial resolution,
Remote sensing,
Microwave radiometry,
Soil measurements"
Machine learning approach to segment Saccharomyces cerevisiae yeast cells,"In biological studies, Saccharomyces cerevisiae yeast cells are used to study the behaviour of proteins. This is a time consuming and not completely objective process. Hence, image analysis platforms are developed to address these problems and to offer analysis per cell as well. The segmentation algorithms implemented in such platforms can segment the healthy cells, along with artefacts such as debris and dead cells that exist in the cultured medium. The novel idea in this work is to apply a machine learning approach to train the segmentation system in order to classify the healthy cell objects from the other objects. Such approach is based on the analysis of a set of relevant individual cell features extracted from the microscope images of yeast cells. These features include texture measurements and wavelet-based texture measurements, as well as moment invariant features. Those features were introduced to describe the intensity and morphology characteristics in a more sophisticated way. A set of classification systems, data sampling techniques, data normalization schemes and feature selection algorithms were tested and evaluated to build a classification model in order to be used within the segmentation module. The study picks the simple logistic classification model as the best approach to classify our dataset of 1380 cells. This system increases the performance level in our image and data analysis modules, improve the segmentation and consequently the analysis of the measurement results. This leads to a better pattern recognition system as well.","Image segmentation,
Biomedical measurement,
Accuracy,
Logistics,
Vegetation,
Machine learning algorithms"
An automated segmentation of brain MRI for detection of normal tissues using improved machine learning approach,"Due to an increased need for efficient and objective evaluation of large amounts of data, MRI-based medical image analysis is gaining attention in recent times. The goal is to simplify an image into something that is more meaningful and making it easier to analyze. The aim of medical image segmentation in brain MRI is to separate the region of interest from the background after denoising and skull removal. Accurate segmentation of normal and abnormal tissues is still a challenge for researchers. In this paper, we propose a fully automated segmentation of normal tissues viz., white matter (WM), gray matter (GM) and cerebro spinal fluid (CSF) from brain MRI using an improved machine learning approach that uses Neuro-fuzzy as classifier. The segmentation is carried out using gradient method and orthogonal polynomial transform. The performance of our method is assessed with metrics such as false positive rate (FPR), false negative rate (FNR), specificity, sensitivity and accuracy. Also, the entire procedure is developed as a graphical user interface (GUI) which results in automated classification and segmentation.","Image segmentation,
Magnetic resonance imaging,
Biomedical imaging,
Accuracy,
Feature extraction,
Graphical user interfaces,
Communication systems"
Analysis of multimodality brain images using machine learning techniques,"In the recent era, due to the technological growth and requirement, various modern medical imaging equipments are developed with different imaging principles. Analyzing these images manually in different dimensions has been proven critical for physicians, biologists and radiologists to seek answers for diagnosis problems. Presently problems exists at each level of imaging across different imaging modalities/scales, registration, fusion, image analysis, pattern recognition, image mining, visualization, reconstruction and informatics methods. This paper is focused on multimodality brain images and its analysis at different stages of process such as fusion, classification and understanding using machine learning techniques. The importance of fusion is illustrated using the result of classification and the need of understanding technique is introduced for further research.","Single photon emission computed tomography,
Medical diagnostic imaging,
Positron emission tomography,
Magnetic resonance imaging,
Visualization,
Semantics"
Exploiting Intrinsic Variability of Filamentary Resistive Memory for Extreme Learning Machine Architectures,"In this paper, we show for the first time how unavoidable device variability of emerging nonvolatile resistive memory devices can be exploited to design efficient low-power, low-footprint extreme learning machine (ELM) architectures. In particular, we utilize the uncontrollable off-state resistance (Roff/HRS) spreads, of nanoscale filamentary-resistive memory devices, to realize random input weights and random hidden neuron biases; a characteristic requirement of ELM. We propose a novel RRAM-ELM architecture. To validate our approach, experimental data from different filamentary-resistive switching devices (CBRAM, OXRAM) are used for full-network simulations. Learning capability of our RRAM-ELM architecture is illustrated with the help of two real-world applications: 1) diabetes diagnosis test (classification) and 2) SinC curve fitting (regression).","Neuromorphic engineering,
Nanoscale devices,
Memory architecture,
Machine learning,
Random access memory,
Stochastic systems"
An Energy-Efficient Nonvolatile In-Memory Computing Architecture for Extreme Learning Machine by Domain-Wall Nanowire Devices,"The data-oriented applications have introduced increased demands on memory capacity and bandwidth, which raises the need to rethink the architecture of the current computing platforms. The logic-in-memory architecture is highly promising as future logic-memory integration paradigm for high throughput data-driven applications. From memory technology aspect, as one recently introduced nonvolatile memory device, domain-wall nanowire (or race-track) not only shows potential as future power efficient memory, but also computing capacity by its unique physics of spintronics. This paper explores a novel distributed in-memory computing architecture where most logic functions are executed within the memory, which significantly alleviates the bandwidth congestion issue and improves the energy efficiency. The proposed distributed in-memory computing architecture is purely built by domain-wall nanowire, i.e., both memory and logic are implemented by domain-wall nanowire devices. As a case study, neural network-based image resolution enhancement algorithm, called DW-NN, is examined within the proposed architecture. We show that all operations involved in machine learning on neural network can be mapped to a logic-in-memory architecture by nonvolatile domain-wall nanowire. Domain-wall nanowire-based logic is customized for in machine learning within image data storage. As such, both neural network training and processing can be performed locally within the memory. The experimental results show that the domain-wall memory can reduce 92% leakage power and 16% dynamic power compared to main memory implemented by DRAM; and domain-wall logic can reduce 31% both dynamic and 65% leakage power under the similar performance compared to CMOS transistor-based logic. And system throughput in DW-NN is improved by 11.6x and the energy efficiency is improved by 56x when compared to conventional image processing system.","Nonvolatile memory,
Nanoscale devices,
Magnetic tunneling,
Random access memory,
Memory architecture,
Nanowires"
Web Application Vulnerability Prediction Using Hybrid Program Analysis and Machine Learning,"Due to limited time and resources, web software engineers need support in identifying vulnerable code. A practical approach to predicting vulnerable code would enable them to prioritize security auditing efforts. In this paper, we propose using a set of hybrid (static+dynamic) code attributes that characterize input validation and input sanitization code patterns and are expected to be significant indicators of web application vulnerabilities. Because static and dynamic program analyses complement each other, both techniques are used to extract the proposed attributes in an accurate and scalable way. Current vulnerability prediction techniques rely on the availability of data labeled with vulnerability information for training. For many real world applications, past vulnerability data is often not available or at least not complete. Hence, to address both situations where labeled past data is fully available or not, we apply both supervised and semi-supervised learning when building vulnerability predictors based on hybrid code attributes. Given that semi-supervised learning is entirely unexplored in this domain, we describe how to use this learning scheme effectively for vulnerability prediction. We performed empirical case studies on seven open source projects where we built and evaluated supervised and semi-supervised models. When cross validated with fully available labeled data, the supervised models achieve an average of 77 percent recall and 5 percent probability of false alarm for predicting SQL injection, cross site scripting, remote code execution and file inclusion vulnerabilities. With a low amount of labeled data, when compared to the supervised model, the semi-supervised model showed an average improvement of 24 percent higher recall and 3 percent lower probability of false alarm, thus suggesting semi-supervised learning may be a preferable solution for many real world applications where vulnerability data is missing.","Computer security,
Data models,
Semisupervised learning,
HTML,
Servers,
Software protection,
Predictive models"
Personalized learning materials for children with special needs using machine learning,"Children with neurological disorder such as autism need personalized development system for their daily activities. Technology can play a significant role. This paper introduces a research to deliver personalized learning materials for children with special needs based on diverse characteristics of children. There are four parts of the system: i) identifying level of the user by using machine learning algorithm. ii) web mining to generate multimodal learning materials from text story or learning keywords, iii) linking user preferences and IoT enabled sensor data with the result, iv) personalizing contents for users delineated with an intelligent interface. This paper explains personalization of results using machine learning algorithm.","Nonhomogeneous media,
Writing,
Flowcharts,
Accuracy"
Implementation of a smartphone wireless accelerometer platform for establishing deep brain stimulation treatment efficacy of essential tremor with machine learning,"Essential tremor (ET) is a highly prevalent movement disorder. Patients with ET exhibit a complex progressive and disabling tremor, and medical management often fails. Deep brain stimulation (DBS) has been successfully applied to this disorder, however there has been no quantifiable way to measure tremor severity or treatment efficacy in this patient population. The quantified amelioration of kinetic tremor via DBS is herein demonstrated through the application of a smartphone (iPhone) as a wireless accelerometer platform. The recorded acceleration signal can be obtained at a setting of the subject's convenience and conveyed by wireless transmission through the Internet for post-processing anywhere in the world. Further post-processing of the acceleration signal can be classified through a machine learning application, such as the support vector machine. Preliminary application of deep brain stimulation with a smartphone for acquisition of a feature set and machine learning for classification has been successfully applied. The support vector machine achieved 100% classification between deep brain stimulation in `on' and `off' mode based on the recording of an accelerometer signal through a smartphone as a wireless accelerometer platform.","Accelerometers,
Wireless communication,
Acceleration,
Brain stimulation,
Support vector machines,
Diseases,
Wireless sensor networks"
Classification of older adults with/without a fall history using machine learning methods,"Falling is a serious problem in an aged society such that assessment of the risk of falls for individuals is imperative for the research and practice of falls prevention. This paper introduces an application of several machine learning methods for training a classifier which is capable of classifying individual older adults into a high risk group and a low risk group (distinguished by whether or not the members of the group have a recent history of falls). Using a 3D motion capture system, significant gait features related to falls risk are extracted. By training these features, classification hypotheses are obtained based on machine learning techniques (K Nearest-neighbour, Naive Bayes, Logistic Regression, Neural Network, and Support Vector Machine). Training and test accuracies with sensitivity and specificity of each of these techniques are assessed. The feature adjustment and tuning of the machine learning algorithms are discussed. The outcome of the study will benefit the prediction and prevention of falls.","Accuracy,
Support vector machines,
History,
Joints,
Logistics,
Training,
Kinematics"
Implementation of machine learning for classifying prosthesis type through conventional gait analysis,"Current forecasts imply a significant increase in the quantity of lower limb amputations. Synergizing the capabilities of a conventional gait analysis system and machine learning facilitates the capacity to classify disparate types of transtibial prostheses. Automated classification of prosthesis type may eventually advance rehabilitative acuity for selecting an appropriate prosthesis for a given aspect of the rehabilitation process. The presented research utilized a force plate as a conventional gait analysis device to acquire a feature set for two types of prosthesis: passive Solid Ankle Cushioned Heel (SACH) and the iWalk BiOM powered prosthesis. The feature set consists of both temporal and kinetic data with respect to the force plate signal during stance. Intuitively a passive prosthesis and powered prosthesis generate distinctively different force plate recordings. A support vector machine, which is type of machine learning application, achieves 100% classification between a passive prosthesis and powered prosthesis regarding the feature set derived from force plate recordings.","Prosthetics,
Force,
Support vector machines,
Kinetic theory,
Legged locomotion,
Brakes,
Context"
Evaluating team behaviors constructed with human-guided machine learning,"Machine learning games such as NERO incorporate adaptive methods such as neuroevolution as an integral part of the gameplay by allowing the player to train teams of autonomous agents for effective behavior in challenging open-ended tasks. However, rigorously evaluating such human-guided machine learning methods and the resulting teams of agent policies can be challenging and is thus rarely done. This paper presents the results and analysis of a large scale online tournament between participants who evolved team agent behaviors and submitted them to be compared with others. An analysis of the teams submitted for the tournament indicates a complex, non-transitive fitness landscape, multiple successful strategies and training approaches, and performance above hand-constructed and random baselines. The tournament and analysis presented provide a practical way to study and improve human-guided machine learning methods and the resulting NPC team behaviors, potentially leading to better games and better game design tools in the future.","Games,
Training,
Artificial intelligence,
Neural networks,
Correlation,
Stability criteria,
Standards"
On the usefulness of machine learning techniques in collaborative anomaly detection,"Due to the increase in network attacks, anomaly detection has gained importance. In this paper, we present and investigate the idea of institutions cooperating for performing anomaly detection, i.e. institutions jointly analyzing their network traffic, in order to identify malicious attacks, using classification-based machine learning techniques. We compare the results of such a collaborative analysis with a single analysis. Moreover, as institutions might not be willing to share confidential data, we analyze the benefits of a collaborative approach if some parts of the traffic are being anonymized. While, intuitively, having more data at hand should lead to improved detection rates, our results indicate that a federated analysis using standard classification-based methods improves detection rates only slightly. Moreover, when using anonymized data, the obtained detection rates of a joint data analysis further deteriorate such that the analysis of individual traffic is more useful. Thus, our research indicates that the classical classification based machine learning approaches for anomaly detection must be adapted and improved in order to leverage the advantage of having data from various sources.","Accuracy,
Hardware,
Software,
Random access memory,
Virtualization"
Machine learning methods for credibility assessment of interviewees based on posturographic data,"This paper discusses the advantages of using posturographic signals from force plates for non-invasive credibility assessment. The contributions of our work are two fold: first, the proposed method is highly efficient and non invasive. Second, feasibility for creating an autonomous credibility assessment system using machine-learning algorithms is studied. This study employs an interview paradigm that includes subjects responding with truthful and deceptive intent while their center of pressure (COP) signal is being recorded. Classification models utilizing sets of COP features for deceptive responses are derived and best accuracy of 93.5% for test interval is reported.","Sensitivity,
Support vector machines,
Force,
Feature extraction,
Kernel,
Polynomials,
Accuracy"
Slow release drug dissolution profile prediction in pharmaceutical manufacturing: A multivariate and machine learning approach,"Slow release drugs must be manufactured to meet target specifications with respect to dissolution curve profiles. In this paper we consider the problem of identifying the drivers of dissolution curve variability of a drug from historical manufacturing data. Several data sources are considered: raw material parameters, coating data, loss on drying and pellet size statistics. The methodology employed is to develop predictive models using LASSO, a powerful machine learning algorithm for regression with high-dimensional datasets. LASSO provides sparse solutions facilitating the identification of the most important causes of variability in the drug fabrication process. The proposed methodology is illustrated using manufacturing data for a slow release drug.","Data models,
Predictive models,
Computational modeling,
Drugs,
Coatings,
Training,
Analytical models"
Predicting individual thermal comfort using machine learning algorithms,"Human thermal sensation in an environment may be delayed, which may lead to life threatening conditions, such as hypothermia and hyperthermia. This is especially true for senior citizens, as aging alters the thermal perception in humans. We envision a decision support system that predicts human thermal comfort in real-time using various environmental conditions as well psychological and physiological features, and suggest corresponding actions, which can significantly improve overall thermal comfort and health of individuals, especially senior citizens. The key to realize this vision is an accurate thermal comfort model. We propose a novel machine learning based approach to learn an individual's thermal comfort model. This approach identifies the best set of features, and then learns a classifier that takes a feature vector as input and outputs a corresponding thermal sensation class (i.e. “feeling cold”, “neutral” and “feeling warm”). Evaluation using a large-scale publicly available data demonstrates that when using Support Vector Machines (SVM) classifiers, the accuracy of our approach is 76.7%, over two times higher than that of the widely adopted Fanger's model (which only achieves accuracy of 35.4%). In addition, our study indicates that two factors, a person's age and outdoor temperature that are not included in Fanger's model, play an important role in thermal comfort, which is a finding interesting in its own right.","Support vector machines,
Temperature sensors,
Accuracy,
Machine learning algorithms,
Adaptation models"
FPGA based nonlinear Support Vector Machine training using an ensemble learning,"Support Vector Machines (SVMs) are powerful supervised learning methods in machine learning. However, their applicability to large problems has been limited due to the time consuming training stage whose computational cost scales quadratically with the number of examples. In this work, a complete FPGA-based system for nonlinear SVM training using ensemble learning is presented. The proposed framework builds on the FPGA architecture and utilizes a cascaded multi-precision training flow, exploits the heterogeneity within the training problem by tuning the number representation used, and supports ensemble training tuned to each internal memory structure so to address very large datasets. Its performance evaluation shows that the proposed system achieves more than an order of magnitude better results compared to state-of-the-art CPU and GPU-based implementations.","Training,
Support vector machines,
Field programmable gate arrays,
Kernel,
Hardware,
Computer architecture,
Acceleration"
Estimation of Water Demand in Residential Building Using Machine Learning Approach,"This paper shows an estimation model for residential water consumption using machine learning approach in Korea. We verify the diversifying elements constituting apartment buildings as input datasets for the Back- Propagation Neural Network (BPNN), the most novel supervised learning neural network based model in accordance with the empirical water use data. A water use prediction for residential buildings is a complex and nonlinear function of geographic, climatic, and morphological variables of buildings. For the verification purpose, empirical data sets consisting of water usage data retrieved from multiple residential apartment buildings in Korea were analyzed as case studies. The proposed model accurately forecast water uses for each examined residential apartment buildings. The results of the proposed models could offer a reliable water supply to meet the useful needs of customers and the local community while facilitating the efficient consumption of water.","Buildings,
Water resources,
Predictive models,
Water conservation,
Estimation,
Reliability,
Neural networks"
Modelling time-varying delays in networked automation systems with heterogeneous networks using machine learning techniques,"Time-varying delays affect the performance and reliability of networked automation systems (NAS). Recent trend to use wired and wireless networks within NAS induces network delays that vary depending on many factors such as loading, sharing, length of the channel, protocol, and so on. As these factors are inherently time-varying, developing analytical models capturing the effect of all these parameters is complex. This investigation presents a methodology that combines experiments with machine learning techniques to model time-varying delays in networked automation systems integrated with heterogeneous networks. Experiments are conducted on NAS by varying the factors that influence delays and time stamping obtained using Wireshark are used to compute the delay. The data collected on the factors influencing the delays and the corresponding delay values are used to model the delays. In data-mining techniques, the accuracy of the estimates varies with the number of computing elements in the hidden layer and selecting them using trial-and-error approach is cumbersome. The minimum resource allocation network (MRAN) over comes the short-coming as it decides the number of computing elements (neurons) in the hidden layer using error thresholds and pruning strategy. The data collected from the experiment is the input training set to the MRAN. Once trained, the MRAN model gives a functional representation relating the factors affecting delays and the estimated delay for a given network condition. During testing, MRAN estimates are validated using error measurements. Results show that the MRAN delay model can capture delays with good accuracy and can be used a tool to assist design decisions on engineering automation systems with heterogeneous networks. The proposed model gives a framework to model time-varying delays as a function of factors influencing them and can be modified to include any number of parameters. This is a significant benefit against existing models in literature that capture the delays only for particular conditions.","Delays,
Hidden Markov models,
Load modeling,
Data models,
Neurons,
Automation,
Loading"
Segmenting music library for generation of playlist using machine learning,"People like listening music primarily due to the emotion it evokes. Any activity or work that a person performs also generates emotions. Considering the above two statements it can assume that people tend to associate music with certain activity if it induces emotions that are in sync with it. In today's world of infinite storage, the number of songs that a user has is ever increasing. With the increased number of songs, listening music in a way one likes has become difficult. Playlist provide us a way to better organize our music library. These can be created manually or by using various smart playlist creators that generates them by analyzing various components of music files used by the user. In the current scenario the playlist created has a specific number of tracks which are played over and over again, with the only variation occurring being in the order in which songs are played. Also the songs that are not part of any playlist are left redundant, occupying useful memory. Our work focuses on the reducing the number of redundant songs in the music library while creating playlist. We have worked on grouping songs with similar emotional effect together in a segment, and then creating a dynamic playlist every time the user plays music. The results show that as the system is provided with required input, we get a robust playlist for each segment, which consist of diverse mix of songs having similar emotional perception. The playlist created is a non monotonous collection of music which is emotionally supportive to the listener towards the work he is doing.","Libraries,
Music,
Robustness,
Accuracy,
Standards,
Multimedia communication,
Synchronization"
Intelligent data mining and machine learning for mental health diagnosis using genetic algorithm,"Inappropriate diagnosis of mental health illnesses leads to wrong treatment and causes irreversible deterioration in the client's mental health status including hospitalization and/or premature death. About 12 million patients are misdiagnosed annually in US. In this paper, a novel study introduces a semi-automated system that aids in preliminary diagnosis of the psychological disorder patient. This is accomplished based on matching description of a patient's mental health status with the mental illnesses illustrated in DSM-IV-TR, Fourth Edition Text Revision. The study constructs the semi-automated system based on an integration of the technology of genetic algorithm, classification data mining and machine learning. The goal is not to fully automate the classification process of mentally ill individuals, but to ensure that a classifier is aware of all possible mental health illnesses could match patient's symptoms. The classifier/psychological analyst will be able to make an informed, intelligent and appropriate assessment that will lead to an accurate prognosis. The analyst will be the ultimate selector of the diagnosis and treatment plan.","Genetic algorithms,
Algorithm design and analysis,
Biological cells,
Databases,
Sociology,
Statistics,
Data mining"
"Malware detection via API calls, topic models and machine learning","Dissemination of malicious code, also known as malware, poses severe challenges to cyber security. Malware authors embed software in seemingly innocuous executables, unknown to a user. The malware subsequently interacts with security-critical OS resources on the host system or network, in order to destroy their information or to gather sensitive information such as passwords and credit card numbers. Malware authors typically use Application Programming Interface (API) calls to perpetrate these crimes. We present a model that uses text mining and topic modeling to detect malware, based on the types of API call sequences. We evaluated our technique on two publicly available datasets. We observed that Decision Tree and Support Vector Machine yielded significant results. We performed t-test with respect to sensitivity for the two models and found that statistically there is no significant difference between these models. We recommend Decision Tree as it yields `if-then' rules, which could be used as an early warning expert system.","Feature extraction,
Support vector machines,
Trojan horses,
Sensitivity,
Text mining,
Grippers"
Efficient domain module from electronic textbooks using machine learning method,"In modern days technologies supported learning system (TSLS), intelligent tutoring systems (ITS), adaptive hypermedia systems (AHS), learning management systems (LMS), and system are used. Moodle and other educational motivation tools are used in institution. DOM-Sortze is a technique where many educational NLP are implemented. The electronic book are taken as input and from these book thee searching is done. The keywords are searched in the whole e-book Instead of using Moodle, and other educational motivation tools, DOM-Sortze is implemented and being used for the search purpose.","Ontologies,
Learning systems,
Security,
Communication networks,
Portable document format,
Speech"
Energy detection and machine learning for the identification of wireless MAC technologies,"ISM spectrum is becoming increasingly populated with various wireless technologies, rendering it a scarce resource. Consequently, wireless coexistence is increasingly vulnerable to new wireless devices attempting to access the same spectrum. This paper presents a novel method for identifying wireless technologies through the use of simple energy detection techniques. Energy detection is used to measure the channel statistical temporal characteristics including activity and inactivity probability distributions. Features uniquely belonging to specific wireless technologies are extracted from the probability distributions and fed into a machine-learning algorithm to identify the technologies under evaluation. Wireless technology identification enables situational awareness to improve coexistence and reduce interference among the devices. An intelligent wireless device is capable of detecting wireless technologies operating within same vicinity. This can be performed by scanning energy levels without the need for signal demodulation and decoding. In this work, a wireless technology identification algorithm was assessed experimentally. Temporal traffic pattern for 802.11b/g/n homogeneous and heterogeneous networks were measured and used as algorithm input. Identification accuracies of up to 96.83% and 85.9% were achieved for homogeneous and heterogeneous networks, respectively.","Wireless communication,
Wireless sensor networks,
Feature extraction,
Accuracy,
IEEE 802.11n Standard"
Online sequential classification of imbalanced data by combining extreme learning machine and improved SMOTE algorithm,"Presently, the data imbalance problems become more pronounced in the applications of machine learning and pattern recognition. However, many traditional machine learning methods suffer from the imbalanced data which are also collected in online sequential manner. To get fast and efficient classification for this special problem, a new online sequential extreme learning machine method with sequential SMOTE strategy is proposed. The key idea of this method is to reduce the randomness while generating virtual minority samples by means of the distribution characteristic of online sequential data. Utilizing online-sequential extreme learning machine as baseline algorithm, this method contains two stages. In offline stage, principal curve is introduced to model the each class's distribution based on which some virtual samples are generated by synthetic minority over-sampling technique(SMOTE). In online stage, each class's membership is determined according to the projection distance of sample to principal curve. With the help of these memberships, the redundant majority samples as well as unreasonable virtual minority samples are all excluded to lighten the imbalance level in online stage. The proposed method is evaluated on four UCI datasets and the real-world air pollutant forecasting dataset. The experimental results show that, the proposed method outperforms the classical ELM, OS-ELM and SMOTE-based OS-ELM in terms of generalization performance and numerical stability.","Classification algorithms,
Prediction algorithms,
Standards,
Niobium"
Cubic spline as an alternative to methods of machine learning,"Approximation of unknown functions in multiple dimensions is an important topic in many areas of industrial engineering, such as nonlinear control. Currently, approaches such as neural networks or fuzzy systems are used to create highly nonlinear surfaces from data. Here we show the capabilities of a very simple classical numerical method such as cubic spline to compete with state of the art machine learning techniques such as Artificial Neural Networks (ANN), Fuzzy Systems (FS), Support Vector Machine (SVM), and Extreme Learning Machines (ELM). Machine learning techniques have many issues such as choosing rules or building a network architecture that can be avoided. Without randomness in the initialization process, there is no need to run the same problem hundreds of times to get a good result. The proposed methods are tested on a variety of problems pertaining to industrial applications against many popular algorithms. Experimental results show that simple cubic splines are indeed competitive in terms of computation time and approximation accuracy when compared with adaptive methods.","Splines (mathematics),
Support vector machines,
Training,
Neurons,
Approximation methods,
FCC,
Fuzzy systems"
A generalized pruning algorithm for extreme learning machine,"This paper presents a generalized pruning extreme learning machine (GP-ELM) algorithm which can generate a compact single-hidden-layer neural network (SLNN) by automatically pruning the number of hidden nodes iteratively while keep high accuracy. The proposed GP-ELM algorithm initializes a SLNN by using extreme learning algorithm (ELM) algorithm given superfluous number of hidden nodes. The following of GP-ELM algorithm consists of two iterative processes. First, the significance of each hidden nodes is estimated and the insignificant nodes are removed. Secondly, the existing hidden nodes are updated by using ELM algorithm. These two processes continue until the stop condition is satisfied such that a reasonably compact network is achieved. Compared against other state-of-the-art algorithms, GP-ELM algorithm has mainly two improvements. First, a supervised training process is integrated into the pruning process such that significance of each hidden node can be estimated more precisely in the next iteration. Secondly, the pruning threshold is set based on the input data dimension such that the threshold can accommodate any data type. Experimental results have shown that the GP-ELM algorithm can automatically achieve a reasonable compact network structure while keep comparable or much higher accuracy in classification and regression.","Accuracy,
Neural networks,
Training,
Support vector machines,
Diabetes,
Machine learning algorithms,
Cost function"
Network intrusion classification based on extreme learning machine,"Extreme learning machine (ELM) is an efficient learning algorithm which can be easily used with least human intervene. But when ELM is applied as multiclass classifier, the results of some classes are not satisfactory and it's hard to adjust the parameters for these classes without affecting other classes. To overcome these limitations, a novel method is proposed. In proposed approach, binary ELM classifiers for each class are combined into an ensemble classifier using one-to-all strategy. The experiment on NSL-KDD data shows that the proposed approach outperforms ELM multiclass classifier, decision tree, neural network (NN) and support vector machines (SVM).","Training,
Support vector machines,
Decision trees,
Artificial neural networks,
Biological neural networks,
Machine learning algorithms,
Accuracy"
Estimating complex networks centrality via neural networks and machine learning,"Vertex centrality measures are important analysis elements in complex networks and systems. These metrics have high space and time complexity, which is a severe problem in applications that typically involve large networks. To apply such high complexity metrics in large networks we trained and tested off-the-shelf machine learning algorithms on several generated networks using five well-known complex network models. Our main hypothesis is that if one uses low complexity metrics as inputs to train the algorithms, one will achieve good approximations of high complexity measures. Our results show that the regression output of the machine learning algorithms applied in our experiments successfully approximate the real metric values and are a robust alternative in real world applications, in particular in complex and social network analysis.","Robustness,
Optimization"
A Machine Learning-Based Framework for Building Application Failure Prediction Models,"In this paper, we present the Framework for building Failure Prediction Models (F2PM), a Machine Learning-based Framework to build models for predicting the Remaining Time to Failure (RTTF) of applications in the presence of software anomalies. F2PM uses measurements of a number of system features in order to create a knowledge base, which is then used to build prediction models. F2PM is application-independent, i.e. It solely exploits measurements of system-level features. Thus, it can be used in differentiated contexts, without the need for any manual modification or intervention to the running applications. To generate optimized models, F2PM can perform a feature selection to identify, among all the measured system features, which have a major impact in the prediction of the RTTF. This allows to produce different models, which use different set of input features. Generated models can be compared by the user by using a set of metrics produced by F2PM, which are related to the model prediction accuracy, as well as to the model building time. We also present experimental results of a successful application of F2PM, using the standard TPC-W e-commerce benchmark.","Predictive models,
Measurement,
Monitoring,
Training,
Software,
Buildings,
Computer crashes"
Machine Learning Based Auto-Tuning for Enhanced OpenCL Performance Portability,"Heterogeneous computing, which combines devices with different architectures, is rising in popularity, and promises increased performance combined with reduced energy consumption. OpenCL has been proposed as a standard for programing such systems, and offers functional portability. It does, however, suffer from poor performance portability, code tuned for one device must be re-tuned to achieve good performance on another device. In this paper, we use machine learning-based auto-tuning to address this problem. Benchmarks are run on a random subset of the entire tuning parameter configuration space, and the results are used to build an artificial neural network based model. The model can then be used to find interesting parts of the parameter space for further search. We evaluate our method with different benchmarks, on several devices, including an Intel i7 3770 CPU, an Nvidia K40 GPU and an AMD Radeon HD 7970 GPU. Our model achieves a mean relative error as low as 6.1%, and is able to find configurations as little as 1.3% worse than the global minimum.","Performance evaluation,
Tuning,
Benchmark testing,
Graphics processing units,
Neurons,
Kernel,
Predictive models"
Multi-scale local shape analysis and feature selection in machine learning applications,"We introduce a method called multi-scale local shape analysis for extracting features that describe the local structure of points within a dataset. The method uses both geometric and topological features at multiple levels of granularity to capture diverse types of local information for subsequent machine learning algorithms operating on the dataset. Using synthetic and real dataset examples, we demonstrate significant performance improvement of classification algorithms constructed for these datasets with correspondingly augmented features.",Stability analysis
A Novel Approach Towards Context Sensitive Recommendations Based on Machine Learning Methodology,"Contexts and aspects have been distinguished as the significant factors in fabricating recommender systems. Most recommender systems aim at utilizing either non-contextual preferences or contextual preferences distinctly, while very few endeavors have been made to identify the significance of both. Hence an attempt has been made to study the influence of both, users' context dependent and context independent preferences in the single recommender system. In this case, accuracy has always been a challenge. Therefore, there exists a need for such a classification technique which can be commonly applied to both types of preferences that helps enhance the accuracy of the retrieved results. For this purpose, use of a standard Machine learning technique well known as Support Vector Machines was proposed. The idea behind using Support vector machines is to split the data in an optimal way and classify the data precisely to aid prediction purpose. For generating recommendations, these context-dependent preferences are further combined with users' context-independent preferences. Finally this technique is applied on a real-life dataset to demonstrate that our method is proficient in dealing with contextual preferences of users and well classify them to achieve better recommendation accuracy than the relative works.","Support vector machines,
Context,
Recommender systems,
Accuracy,
Data mining,
Training"
Correlation Analysis of Big Data to Support Machine Learning,"The large size and complexity of datasets in Big Data need specialized statistical tools for analysis and we use R for correlation analysis of our data set. This paper explores the correlation analysis through best fit linear regression of quantitative variables with help of the demonstration based on scatter plots and linear regression best fit line. The analysis demonstrated in this paper is scalable to Big Data in any other context where the quantitative variables are clearly delineated. R provides multiple techniques and inferences to statistical analysis of dataset, this paper however explores the correlation between quantitative variable establishing the extent of dependability between them using R functions. The correlation and best fit line functions of R i.e. Cor () and abline(lmout) respectively are significantly explored.","Correlation,
Big data,
Education,
Linear regression,
Data mining,
Data analysis,
Complexity theory"
A comparative study on machine learning algorithms for indoor positioning,"Fingerprinting based positioning is commonly used for indoor positioning. In this method, initially a radio map is created using Received Signal Strength (RSS) values that are measured from predefined reference points. During the positioning, the best match between the observed RSS values and existing RSS values in the radio map is established as the predicted position. In the positioning literature, machine learning algorithms have widespread usage in estimating positions. One of the main problems in indoor positioning systems is to find out appropriate machine learning algorithm. In this paper, selected machine learning algorithms are compared in terms of positioning accuracy and computation time. In the experiments, UJIIndoorLoc indoor positioning database is used. Experimental results reveal that k-Nearest Neighbor (k-NN) algorithm is the most suitable one during the positioning. Additionally, ensemble algorithms such as AdaBoost and Bagging are applied to improve the decision tree classifier performance nearly same as k-NN that is resulted as the best classifier for indoor positioning.","Classification algorithms,
Floors,
Accuracy,
Machine learning algorithms,
Training,
Decision trees"
Mining the impact of object oriented metrics for change prediction using Machine Learning and Search-based techniques,"Change in a software is crucial to incorporate defect correction and continuous evolution of requirements and technology. Thus, development of quality models to predict the change proneness attribute of a software is important to effectively utilize and plan the finite resources during maintenance and testing phase of a software. In the current scenario, a variety of techniques like the statistical techniques, the Machine Learning (ML) techniques and the Search-based techniques (SBT) are available to develop models to predict software quality attributes. In this work, we assess the performance of ten machine learning and search-based techniques using data collected from three open source software. We first develop a change prediction model using one data set and then we perform inter-project validation using two other data sets in order to obtain unbiased and generalized results. The results of the study indicate comparable performance of SBT with other employed statistical and ML techniques. This study also supports inter project validation as we successfully applied the model created using the training data of one project on other similar projects and yield good results.","Software,
Predictive models,
Computational modeling,
Accuracy,
Sensitivity,
Data models"
Machine learning for seizure prediction: A revamped approach,"Occurrence of multiple seizures is a common phenomenon observed in patients with epilepsy: a neurological malfunction that affects approximately 50 million people in the world. Seizure prediction is widely acknowledged as an important problem in the neurological domain, as it holds promise to improve the quality of life for patients with epilepsy. A noticeable number of clinical studies showed evidence of symptoms (patterns) before seizures and thus, there is large research on predicting seizures. There is very little existing literature that systematically illustrates the steps in machine learning for seizure prediction, limited training data and class imbalance are a few challenges. In this paper, we propose a novel way to overcome these challenges. We present the improved results for various classification algorithms. An average of 21.71% improvement in accuracy is attained using our approach.","Feature extraction,
Accuracy,
Electroencephalography,
Classification algorithms,
Epilepsy,
Machine learning algorithms,
Predictive models"
Machine learning approach for detection of cyber-aggressive comments by peers on social media network,The fast growing use of social networking sites among the teens have made them vulnerable to get exposed to bullying. Cyberbullying is the use of computers and mobiles for bullying activities. Comments containing abusive words effect psychology of teens and demoralizes them. In this paper we have devised methods to detect cyberbullying using supervised learning techniques. We present two new hypotheses for feature extraction to detect offensive comments directed towards peers which are perceived more negatively and result in cyberbullying. Our initial experiments show that using features from our hypotheses in addition to traditional feature extraction techniques like TF-IDF and N-gram increases the accuracy of the system.,"Feature extraction,
Dictionaries,
Accuracy,
Standards,
Logistics,
Machine learning algorithms,
Support vector machines"
Sentiment Analysis of Malayalam film review using machine learning techniques,"The unprecedented growth of data in web, social media and the attempt to make the cognitive process using computers make Sentiment Analysis a challenging and interesting research problem. Sentiment Analysis mainly deals with the process of analyzing the sentiments or feelings from someone's expression or piece of information, and also in discovering the cognitive behavior of humans. The usage of computers to get feedback, opinion or remarks about a product, entertainment or political view of the public is very common. This paper demonstrates how Sentiment Analysis can be used in reviewing Malayalam films by using machine learning techniques. It is a hybrid approach comprising of machine learning techniques and rule based approach. This work would help the users to analyze the film criticism and also to assign the rank and popularity of new arrival films. In this work, the system checks the polarity at the sentence level, resulted in an accuracy of 91%.","Sentiment analysis,
Support vector machines,
Context,
Accuracy,
Semantics,
Training,
Motion pictures"
An alternative evaluation of post traumatic stress disorder with machine learning methods,"In the world we live in, people from different professions are at increased risk for depressive symptoms and posttraumatic stress disorder (PTSD) due to hard working or extreme environmental conditions. Accurate diagnosis and determining the causes are very important to solve these kinds of psychological problems. Machine learning (ML) techniques are gaining popularity in neuroscience due to their high diagnostic capability and effective classification ability. In this paper, alternative hybrid systems which allowed us to develop automatic classifiers for finding the Posttraumatic stress disorder (PTSD) patients are proposed and compared. With the proposed system, not only the PTSD individuals are classified by ML techniques such as sequential minimal optimization (SMO), multilayer perceptron (MLP), Naïve Bayes (NB) but also the important indications of patients' trauma are determined by three popular feature selection methods such as chi-square, principal component analysis (PCA) and correlation based-feature selection (CFS). The effectiveness of the proposed system is examined on a real world dataset. Due to obtained results we can estimate the individuals as PTSD or NONPTSD patients with 74-79% accuracy range, further to that instead of 39 features 7 features are remarked as the most critical symptoms for PTSD.","Classification algorithms,
Principal component analysis,
Niobium,
Stress,
Support vector machines,
Algorithm design and analysis,
Medical services"
Twitter sentiment classification using machine learning techniques for stock markets,"Sentiment classification of Twitter data has been successfully applied in finding predictions in a variety of domains. However, using sentiment classification to predict stock market variables is still challenging and ongoing research. The main objective of this study is to compare the overall accuracy of two machine learning techniques (logistic regression and neural network) with respect to providing a positive, negative and neutral sentiment for stock-related tweets. Both classifiers are compared using Bigram term frequency (TF) and Unigram term frequency - inverse document term frequency (TF-IDF) weighting schemes. Classifiers are trained using a dataset that contains 42,000 automatically annotated tweets. The training dataset forms positive, negative and neutral tweets covering four technology-related stocks (Twitter, Google, Facebook, and Tesla) collected using Twitter Search API. Classifiers give the same results in terms of overall accuracy (58%). However, empirical experiments show that using Unigram TF-IDF outperforms TF.","Twitter,
Cloud computing,
Logistics,
Accuracy,
Computational modeling,
Support vector machines,
Sentiment analysis"
Applying Machine Learning Techniques to Transportation Mode Recognition Using Mobile Phone Sensor Data,"This paper adopts different supervised learning methods from the field of machine learning to develop multiclass classifiers that identify the transportation mode, including driving a car, riding a bicycle, riding a bus, walking, and running. Methods that were considered include K-nearest neighbor, support vector machines (SVMs), and tree-based models that comprise a single decision tree, bagging, and random forest (RF) methods. For training and validating purposes, data were obtained from smartphone sensors, including accelerometer, gyroscope, and rotation vector sensors. K-fold cross-validation as well as out-of-bag error was used for model selection and validation purposes. Several features were created from which a subset was identified through the minimum redundancy maximum relevance method. Data obtained from the smartphone sensors were found to provide important information to distinguish between transportation modes. The performance of different methods was evaluated and compared. The RF and SVM methods were found to produce the best performance. Furthermore, an effort was made to develop a new additional feature that entails creating a combination of other features by adopting a simulated annealing algorithm and a random forest method.","Support vector machines,
Transportation,
Data models,
Global Positioning System,
Kernel,
Accelerometers,
Gyroscopes"
A heuristic machine learning-based algorithm for power and thermal management of heterogeneous MPSoCs,"In this work, we propose a power and thermal management algorithm based on machine learning to control the thermal stresses and power consumption of the heterogeneous MPSoCs. The objectives of the proposed algorithm are increasing the performance and decreasing the spatial and temporal temperature gradients along with the thermal cycling under the power and temperature constraints. Our proposed power and thermal management method is based on a heuristic approach to speed up the convergence of the machine learning algorithm which makes it applicable for general purpose processors. Adopting Q-Learning as the machine learning algorithm, the heuristic approach aids to limit the learning space by suggesting the most appropriate actions to the agent in each decision epoch. The heuristic algorithm employs the current and previous states of the machine learning, as well as the amount of the temperature stress and power consumption of each core to determine the appropriate action for each core, independently. The proposed algorithm is evaluated on 4-core, 8-core and 16-core homogeneous and heterogeneous MPSoCs for some benchmarks in the Splash2 benchmark package. The results reveal a faster convergence of machine learning and more thermal stresses reduction.","thermal stresses,
learning (artificial intelligence),
low-power electronics,
system-on-chip"
Unified Programming Model and Software Framework for Big Data Machine Learning and Data Analytics,"In a new era of Big Data, the rapid growth of the applications, such as social media and web-search, requires efficient and scalable machine learning and statistical analytical algorithms. However, there lacks easy-to-use and efficient software frameworks or systems that can support fast development of such big data analytical algorithms. To solve these problems, we propose Octopus, an easy-to-use and efficient analytical system for big data. Octopus allows data analysts conduct complex data analytics for big data with traditional programming languages and methods in an easy and efficient way. To achieve the goal of ease-to-use, we propose a matrix-based unified programming model, which is the core of many data-intensive statistical applications such as numerical analysis and data mining. Further, in order to improve the performance, the Octopus software framework adopts various distributed computing platforms, including Hadoop MapReduce, Spark and MPI. On these computing platforms, we design several parallel matrix computation algorithms, which are suitable for various scenarios. Finally, the features of Octopus are encapsulated into a library with matrix-based APIs and exposed to users as an R package. R is a widely-used statistical programming language and supports diversified data analysis tasks through extension packages. Experimental results show that Octopus achieves efficient performance and near linear scalability.","Sparks,
Scalability,
Big data,
Libraries,
Programming,
Machine learning algorithms,
Distributed databases"
An improved algorithm model based on machine learning,"In the last decades, Reinforcement Learning (RL) algorithm has attracted more and more attention, and become the research focus in the field of machine learning. This paper leads the typical RL algorithm, Q-learning algorithm, into computer game platform (Connect6), and proposes an improved method. We adjust reward parameter according to the shape of Connect6, and optimize the adjustment of evaluation function to achieve the global optimization. Moreover, the optimization of the reward makes the valueless units away from the evaluation, to reduce the interference of valueless units for optimal results and improve the convergence speed, thereby reducing the overall time of self-learning process.",
Network State-Based Algorithm Selection for Power Flow Management Using Machine Learning,"This paper demonstrates that machine learning can be used to create effective algorithm selectors that select between power system control algorithms depending on the state of a network, achieving better performance than always using the same algorithm for every state. Also presented is a novel method for creating algorithm selectors that consider two objectives. The method is used to develop algorithm selectors for power flow management algorithms on versions of the IEEE 14- and 57-bus networks, and a network derived from a real distribution network. The selectors choose from within a diverse set of power flow management algorithms, including those based on constraint satisfaction, optimal power flow, power flow sensitivity factors, and linear programming. The network state-based algorithm selectors offer performance benefits over always using the same power flow management algorithm for every state, in terms of minimizing the number of overloads while also minimizing the curtailment applied to generators.","Machine learning algorithms,
Generators,
Training,
Testing,
Feature extraction,
Prediction algorithms,
Training data"
Intrusion Detection System Using Bagging Ensemble Method of Machine Learning,"Intrusion detection system is widely used to protect and reduce damage to information system. It protects virtual and physical computer networks against threats and vulnerabilities. Presently, machine learning techniques are widely extended to implement effective intrusion detection system. Neural network, statistical models, rule learning, and ensemble methods are some of the kinds of machine learning methods for intrusion detection. Among them, ensemble methods of machine learning are known for good performance in learning process. Investigation of appropriate ensemble method is essential for building effective intrusion detection system. In this paper, a novel intrusion detection technique based on ensemble method of machine learning is proposed. The Bagging method of ensemble with REPTree as base class is used to implement intrusion detection system. The relevant features from NSL_KDD dataset are selected to improve the classification accuracy and reduce the false positive rate. The performance of proposed ensemble method is evaluated in term of classification accuracy, model building time and False Positives. The experimental results show that the Bagging ensemble with REPTree base class exhibits highest classification accuracy. One advantage of using Bagging method is that it takes less time to build the model. The proposed ensemble method provides competitively low false positives compared with other machine learning techniques.","Intrusion detection,
Bagging,
Accuracy,
Classification algorithms,
Hidden Markov models,
Training,
Feature extraction"
Recent advances on kernel fuzzy support vector machine model for supervised learning,"A most fashionable off-the-shelf classifier is the Support Vector Machine. It is a powerful recent proceed of the data mining practitioner. A computing world has a lot to gain of new generation learning system. Statistical learning theory is a latest advances in supervised learning community. It is a feed forward network and binary learning machine with highly elegant properties. It plays a vital role in the reduction of machine learning problem into optimization problem, convex problems, linear programming, smaller quadratic programming, convex analysis, second order cone programming and so on. The SVM has enthused and established by the way of kernel learning algorithm. That maps data into some dot product feature space perform the linear algorithm. This kernel function is implemented by Platt's sequential minimal optimization algorithm used in function estimation that can train efficiently and fast with Polykernel, Normalized poly kernel, Pearson VII function based Universal Kernel (PUK), Radial basis function. In this paper we focus Kernel Fuzzy Support Vector Machine to tune the kernel parameters to gain high performance from the classifier model. The trendy parameter technique k fold cross validation adopted under various kernel function and efficiency is empirically evaluate and observed a significant progress whilst the dataset is not linearly separable.","Kernel,
Support vector machines,
Training,
Data mining,
Accuracy,
Optimization,
Classification algorithms"
Weighted Tanimoto Extreme Learning Machine with Case Study in Drug Discovery,"Machine learning methods are becoming more and more popular in the field of computer-aided drug design. The specific data characteristic, including sparse, binary representation as well as noisy, imbalanced datasets, presents a challenging binary classification problem. Currently, two of the most successful models in such tasks are the Support Vector Machine (SVM) and Random Forest (RF). In this paper, we introduce a Weighted Tanimoto Extreme Learning Machine (T-WELM), an extremely simple and fast method for predicting chemical compound biological activity and possibly other data with discrete, binary representation. We show some theoretical properties of the proposed model including the ability to learn arbitrary sets of examples. Further analysis shows numerous advantages of T-WELM over SVMs, RFs and traditional Extreme Learning Machines (ELM) in this particular task. Experiments performed on 40 large datasets of thousands of chemical compounds show that T-WELMs achieve much better classification results and are at the same time faster in terms of both training time and further classification than both ELM models and other state-of-the-art methods in the field.","Machine learning,
Compounds,
Design automation,
Fingerprint recognition,
Drugs,
Biological system modeling,
Computational modeling"
Executive Roundtable Series: Machine Learning and Cognitive Computing,"Machine learning and cognitive computing are today's newest buzzwords, and considerable hype surrounds them. This article is based on a recent webinar on analytics produced by IT Professional, the Journal of Applied Marketing Analytics, and consultancy Technology Business Research (TBR), along with the Content Wrangler; it was hosted by Earley Information Science (EIS). The goal is to help organizations understand what's practical and what's possible in the fast-growing fields of machine learning and cognitive computing, and how these fields are related to artificial intelligence (AI).","Artificial intelligence,
Cognitive computing,
Machine learning algorithms,
Interviews"
Scalable and parallel machine learning algorithms for statistical data mining - Practice & experience,"Many scientific datasets (e.g. earth sciences, medical sciences, etc.) increase with respect to their volume or in terms of their dimensions due to the ever increasing quality of measurement devices. This contribution will specifically focus on how these datasets can take advantage of new `big data' technologies and frameworks that often are based on parallelization methods. Lessons learned with medical and earth science data applications that require parallel clustering and classification techniques such as support vector machines (SVMs) and density-based spatial clustering of applications with noise (DBSCAN) are a substantial part of the contribution. In addition, selected experiences of related `big data' approaches and concrete mining techniques (e.g. dimensionality reduction, feature selection, and extraction methods) will be addressed too. In order to overcome identified challenges, we outline an architecture framework design that we implement with open available tools in order to enable scalable and parallel machine learning applications in distributed systems.","Machine learning algorithms,
Standards,
Algorithm design and analysis,
Data mining,
Support vector machines,
Concrete,
Clustering algorithms"
A comparison of different machine learning algorithms using single channel EEG signal for classifying human sleep stages,"In recent years, the estimation of human sleep disorders from Electroencephalogram (EEG) signals have played an important role in developing automatic detection of sleep stages. A few methods exist in the market presently towards this aim. However, sleep physicians may not have full assurance and consideration in such methods due to concerns associated with systems accuracy, sensitivity and specificity. This paper presents a novel and efficient technique that can be implemented in a microcontroller device to identify sleep stages in an effort to assist physicians in the diagnosis and treatment of related sleep disorders by enhancing the accuracy of the developed algorithm using a single channel of EEG signals. First, the dataset of EEG signal is filtered and decomposed into delta, theta, alpha, beta and gamma subbands using Butterworth band-pass filters. Second, a set of sample statistical discriminating features are derived from each frequency band. Finally, sleep stages consisting of Wakefulness, Rapid Eye Movement (REM) and Non-Rapid Eye Movement (NREM) are classified using several supervised machine learning classifiers including multi-class Support Vector Machines (SVM), Decision Trees (DT), Neural Networks (NN), K-Nearest Neighbors (KNN) and Naive Bayes (NB). This paper combines REM with Stage 1 NREM due to data similarities. Performance is then compared based on single channel EEG signals that were obtained from 20 healthy subjects. The results show that the proposed technique using DT classifier efficiently achieves high accuracy of 97.30% in differentiating sleeps stages. Also, a comparison of our method with some recent available works in the literature reiterates the high classification accuracy performance.","Sleep,
Electroencephalography,
Accuracy,
Support vector machines,
Feature extraction,
Databases,
Testing"
An efficient unstructured big data analysis method for enhancing performance using machine learning algorithm,"In this modern world, data mining technology holds an essential position in all the major Engineering fields. Handling of Unstructured Big Data is an essential task of this era. At present, making the maximum advantage of parallel processing know-hows and the task of rapid examination of huge data steadily and continuously transmitted or received from various sources is becoming popular or conventional. The big data analytics job is fragmented into smaller jobs and ran over tens, hundreds or thousands of product servers by the parallel processing architecture. This helps in maintaining the data center cost efficient and facilitates easy handling of the enormous work in an efficient way. In this paper, proposed solution takes online consumer purchase. The online system has unrivalled bank of data on online consumer purchasing behavior that can be mined from its 100 million customers accounts. They use customer click-stream data and historical purchase data of all those 100 million customers and each user is shown personalized results on customized web pages. For improving Big Data performance the Machine Learning Method i.e. K-Nearest Neighbour algorithm used to support to take good analysis. Hadoop simulator is used to solve this kind of task.","Big data,
Data mining,
Business,
Industries,
Machine learning algorithms,
Databases,
Computers"
Driving Timing Convergence of FPGA Designs through Machine Learning and Cloud Computing,"Machine learning and cloud computing techniques can help accelerate timing closure for FPGA designs without any modification to original RTL code. RTL is generally frozen closer to system delivery target to avoid injecting new unforeseen bugs or significantly affecting design characteristics. In these circumstances, developers trying to close timing are either at the mercy of random trials through placement seed exploration or through vendor-provided design space exploration tools that run a few compilation trials with changes to the CAD tool options (or parameters). Instead, we propose evaluating multiple CAD runs in parallel on the cloud, supported by a Bayesian learning and classification framework for generating multiple CAD parameter combinations most likFPGA CAD tool parametersely to help attain timing closure. We maintain a database of FPGA CAD tool parameters (input) along with associated variations in timing slack (output)to enable the learning process. A key engineering resource we use here is cheap and abundant parallelism made possible through cloud computing frameworks such as the Google Compute Engine. Across a range of open-source benchmarks, we show that learning helps improve total negative slack (TNS) scores by 10.5× (geomean) when compared to a single baseline run of Quart us 14.1 and by 7× (geomean) when compared to Alter a Quart us 14.1 Design Space Explorer (DSE).","Design automation,
Field programmable gate arrays,
Timing,
Convergence,
Cloud computing,
Databases,
Proposals"
Energy-Efficient Transmission Scheduling in Mobile Phones Using Machine Learning and Participatory Sensing,"Energy efficiency is important for smartphones because they are powered by batteries with limited capacity. Existing work has shown that the tail energy of the third-generation (3G)/fourth-generation (4G) network interface on a mobile device would lead to low energy efficiency. To solve the tail energy minimization problem, some online scheduling algorithms have been proposed, but with a big gap from the offline algorithms that work depending on the knowledge of future transmissions. In this paper, we study the tail energy minimization problem by exploiting the techniques of machine learning and participatory sensing. We design a client-server architecture, in which the training process is conducted in a server, and mobile devices download the constructed predictor from the server to make transmission decisions. A system is developed and deployed on real hardware to evaluate the performance of our proposal. The experimental results show that it can significantly improve the energy efficiency of mobile devices while incurring minimum overhead.","Delays,
Servers,
Training,
Vectors,
Mobile handsets,
Energy consumption,
Sensors"
MobDBTest: A machine learning based system for predicting diabetes risk using mobile devices,"Diabetes mellitus (DM) is reaching possibly epidemic proportions in India. The degree of disease and destruction due to diabetes and its potential complications are enormous, and originated a significant health care burden on both households and society. The concerning factor is that diabetes is now being proven to be linked with a number of complications and to be occurring at a comparatively younger age in the country. In India, the migration of people from rural to urban areas and corresponding modification in lifestyle are all moving the degree of diabetes. Deficiency of knowledge about diabetes causes untimely death among the population at large. Therefore, acquiring a proficiency that should spread awareness about diabetes may affect the people in India. In this work, a mobile/android application based solution to overcome the deficiency of awareness about diabetes has been shown. The application uses novel machine learning techniques to predict diabetes levels for the users. At the same time, the system also provides knowledge about diabetes and some suggestions on the disease. A comparative analysis of four machine learning (ML) algorithms were performed. The Decision Tree (DT) classifier outperforms amongst the 4 ML algorithms. Hence, DT classifier is used to design the machinery for the mobile application for diabetes prediction using real world dataset collected from a reputed hospital in the Chhattisgarh state of India.","Diabetes,
Support vector machines,
Decision trees,
Classification algorithms,
Multilayer perceptrons,
Machine learning algorithms,
Prediction algorithms"
Threat prediction using honeypot and machine learning,"Data is an abstraction which encapsulates information. In today's era businesses are data driven which gives insight to predict the destiny of the business by making predictions but another side of the coin is data also helps in placing the present health of the business under our radar and looking back in our past and answer some important questions: what exactly went wrong in the past?. In this paper we try to look into the architecture of frameworks which can predict threat using Honeypot as the source of data and various machine learning algorithms to make precise prediction using OSSEC as Host Intrusion Detection System [HIDS], SNORT for Network Intrusion Detection System [NIDS] and Honeyd an open source Honeypot.","Conferences,
Ports (Computers),
Intrusion detection,
Operating systems,
Computer hacking,
IP networks,
Market research"
Diabetic Retinopathy using morphological operations and machine learning,"Diabetic Retinopathy that is DR which is a eye disease that affect retina and further later at severe stage it lead to vision loss. Early detection of DR is helpful to improve the screening of patient to prevent further damage. Retinal micro-aneurysms, haemorrhages, exudates and cotton wool spots are kind of major abnormality to find the Non- Proliferative Diabetic Retinopathy (NPDR) and Proliferative Diabetic Retinopathy (PDR). The main objective of our proposed work is to detect retinal micro-aneurysms and exudates for automatic screening of DR using Support Vector Machine (SVM) and KNN classifier. To develop this proposed system, a detection of red and bright lesions in digital fundus photographs is needed. Micro-aneurysms are the first clinical sign of DR and it appear small red dots on retinal fundus images. To detect retinal micro-aneurysms, retinal fundus images are taken from Messidor, DB-rect dataset. After pre-processing, morphological operations are performed to find micro-aneurysms and then features are get extracted such as GLCM and Structural features for classification. In order to classify the normal and DR images, different classes must be represented using relevant and significant features. SVM gives better performance over KNN classifier.","Retina,
Feature extraction,
Image segmentation,
Support vector machines,
Estimation"
Accelerating Machine Learning Kernel in Hadoop Using FPGAs,"Big data applications share inherent characteristics that are fundamentally different from traditional desktop CPU, parallel and web service applications. They rely on deep machine learning and data mining applications. A recent trend for big data analytics is to provide heterogeneous architectures to allow support for hardware specialization to construct the right processing engine for analytics applications. However, these specialized heterogeneous architectures require extensive exploration of design aspects to find the optimal architecture in terms of performance and cost. % Considering the time dedicated to create such specialized architectures, a model that estimates the potential speedup achievable through offloading various parts of the algorithm to specialized hardware would be necessary. This paper analyzes how offloading computational intensive kernels of machine learning algorithms to a heterogeneous CPU+FPGA platform enhances the performance. We use the latest Xilinx Signboards for implementation and result analysis. Furthermore, we perform a comprehensive analysis of communication and computation overheads such as data I/O movements, and calling several standard libraries that can not be offloaded to the accelerator to understand how the speedup of each application will contribute to its overall execution in an end-to-end Hadoop MapReduce environment.","Acceleration,
Kernel,
Computer architecture,
Hardware,
Field programmable gate arrays,
Big data,
Machine learning algorithms"
Scaling Machine Learning for Target Prediction in Drug Discovery using Apache Spark,"In the context of drug discovery, a key problem is the identification of candidate molecules that affect proteins associated with diseases. Inside Janssen Pharmaceutical, the Chemo genomics project aims to derive new candidates from existing experiments through a set of machine learning predictor programs, written in single-node C++. These programs take a long time to run and are inherently parallel, but do not use multiple nodes. We show how we reimplementation the pipeline using Apache Spark, which enabled us to lift the existing programs to a multi-node cluster without making changes to the predictors. We have benchmarked our Spark pipeline against the original, which shows almost linear speedup up to 8 nodes. In addition, our pipeline generates fewer intermediate files while allowing easier check pointing and monitoring.","Pipelines,
Sparks,
Compounds,
Drugs,
Proteins,
Predictive models,
Training data"
Real-time network anomaly detection system using machine learning,"The ability to process, analyze, and evaluate realtime data and to identify their anomaly patterns is in response to realized increasing demands in various networking domains, such as corporations or academic networks. The challenge of developing a scalable, fault-tolerant and resilient monitoring system that can handle data in real-time and at a massive scale is nontrivial. We present a novel framework for real time network traffic anomaly detection using machine learning algorithms. The proposed prototype system uses existing big data processing frameworks such as Apache Hadoop, Apache Kafka, and Apache Storm in conjunction with machine learning techniques and tools. Our approach consists of a system for real-time processing and analysis of the real-time network-flow data collected from the campus-wide network at the University of Missouri-Kansas City. Furthermore, the network anomaly patterns were identified and evaluated using machine learning techniques. We present preliminary results on anomaly detection with the campus network data.","Real-time systems,
IP networks,
Storms,
Fasteners,
Support vector machines,
Ports (Computers),
Accuracy"
Cloud-Based Machine Learning Tools for Enhanced Big Data Applications,"We propose Cloud-based machine learning tools for enhanced Big Data applications, where the main idea is that of predicting the ""next"" workload occurring against the target Cloud infrastructure via an innovative ensemble-based approach that combine the effectiveness of different well-known classifiers in order to enhance the whole accuracy of the final classification, which is very relevant at now in the specific context of Big Data. So-called workload categorization problem plays a critical role towards improving the efficiency and the reliability of Cloud-based big data applications. Implementation-wise, our method proposes deploying Cloud entities that participate to the distributed classification approach on top of virtual machines, which represent classical ""commodity"" settings for Cloud-based big data applications. Preliminary experimental assessment and analysis clearly confirm the benefits deriving from our classification framework.","Hidden Markov models,
Benchmark testing,
Discrete cosine transforms,
Big data,
Virtual machining,
Machine learning algorithms,
Training"
Machine-learning-integrated load scheduling for reduced peak power demand,"Load scheduling over cyclic electrical devices can reduce the peak power demand. In this paper, we propose a machine-learning-integrated load control (MILC) scheme for improved performance and reliability. By dynamic capacity adjustment and interactive load heuristic, MILC tries to reduce the power deviation while keeping the temperature violation ratio and switching counts within an acceptable range. A prototype of the proposed scheme has been implemented and, through experiments using load traces from a real home, we evaluate the performance of MILC. The results show that MILC reduces the peak demand from 4993 W to 4236 W and successfully decreases the power deviation by 12.1% on average.","Switches,
Refrigerators,
Temperature measurement,
Support vector machines,
Performance evaluation,
Dynamic scheduling"
Performance Prediction of Large-Scale 1S1R Resistive Memory Array Using Machine Learning,"A methodology to analyze device-to-circuit characteristics and predict memory array performance is presented. With a five- parameter characterization of the selection device and a compact model of RRAM, we are able to capture the behaviors of reported selection devices and simulate 1S1R cell/array performance with RRAM compact modeling using HSPICE. To predict the performance of the memory array for a variety of selectors, machine-learning algorithms are employed, using device characteristics and circuit simulation results as the training data. The influence of selector parameters on the 1S1R cell and array behavior is investigated and projected to large Gbit arrays. The machine learning methods enable time-efficient and accurate estimates of 1S1R array performance to guide large-scale memory design.","Arrays,
Prediction algorithms,
Integrated circuit modeling,
Machine learning algorithms,
Computational modeling,
Circuit simulation,
Feature extraction"
Comparative study on machine learning techniques in predicting the QoS-values for web-services recommendations,"This is an era of Internet computing and computing as a service on the internet is called cloud computing. Mainly three services like SaaS (applications), PaaS, and IaaS are being accessed through internet on demand, pay as per usage basis. Quality of Service (QoS) is the main issue in internet based computing for service providers and user-dependent as well as user-independent QoS parameters. In the current work we compared different machine learning algorithms for predicting the response time and throughput QoS values using past usage data. Bagging and support vector machines are found to be better performing prediction methods in comparison with other learning algorithms.","Quality of service,
Throughput,
Time factors,
Cloud computing,
Bagging,
Standards"
Quality assessment of adaptive bitrate videos using image metrics and machine learning,"Adaptive bitrate (ABR) streaming is widely used for distribution of videos over the internet. In this work, we investigate how well we can predict the quality of such videos using well-known image metrics, information about the bitrate levels, and a relatively simple machine learning method. Quality assessment of ABR videos is a hard problem, but our initial results are promising. We obtain a Spearman rank order correlation of 0.88 using content-independent cross-validation.","Videos,
Quality assessment,
Bit rate,
Training,
Correlation,
Video recording"
Machine Learning Prediction for 13X Endurance Enhancement in ReRAM SSD System,"The variable behavior of ReRAM memory cells is modeled with machine learning. Two types of prediction are investigated, reset in the next-cycle and cell fail in the long term. A new proposal, Proactive Bit Redundancy, introduces a ML-trained Prediction Engine into the SSD controller, to predict fail cells and replace them proactively - before actual failure- by redundancy. With the Invalid Masking technique, predicted cells are marked in-place within the page, so that no extra address table is needed. Thus, with ninimal overhead, 2.85x bit error rate reduction or 13x endurance improvement is obtained based on a 50nm AlxOy testchip.","Predictive models,
Redundancy,
Error correction codes,
Engines,
Accuracy,
Proposals,
Data models"
Video quality assessment and machine learning: Performance and interpretability,"In this work we compare a simple and a complex Machine Learning (ML) method used for the purpose of Video Quality Assessment (VQA). The simple ML method chosen is the Elastic Net (EN), which is a regularized linear regression model and easier to interpret. The more complex method chosen is Support Vector Regression (SVR), which has gained popularity in VQA research. Additionally, we present an ML-based feature selection method. Also, it is investigated how well the methods perform when tested on videos from other datasets. Our results show that content-independent cross-validation performance on a single dataset can be misleading and that in the case of very limited training and test data, especially in regards to different content as is the case for many video datasets, a simple ML approach is the better choice.","Training,
Correlation,
Support vector machines,
Complexity theory,
Estimation,
Standards,
Quality assessment"
Characterization between child and adult voice using machine learning algorithm,"Speech Feature Detection is a technique employed in speech processing in which different features of speech are used to distinguish between speech in different age groups. This paper implements a new approach for the extraction and classification of the speech features using the Mel-frequency cepstral coefficient, and Support Vector Machine. This paper presents the Mel-frequency cepstral coefficients (MFCC) for extracting the speech features of child and adult voices. Using the support vector machine, we classify the datasets in a child and an adult's speech.","Speech,
Feature extraction,
Mel frequency cepstral coefficient,
Support vector machines,
Speech recognition,
Fourier transforms,
Speech processing"
Underwater robot-object contact perception using machine learning on force/torque sensor feedback,"Autonomous manipulation of objects requires reliable information on robot-object contact state. Underwater environments can adversely affect sensing modalities such as vision, making them unreliable. In this paper we investigate underwater robot-object contact perception between an autonomous underwater vehicle and a T-bar valve using a force/torque sensor and the robot's proprioceptive information. We present an approach in which machine learning is used to learn a classifier for different contact states, namely, a contact aligned with the central axis of the valve, an edge contact and no contact. To distinguish between different contact states, the robot performs an exploratory behavior that produces distinct patterns in the force/torque sensor. The sensor output forms a multidimensional time-series. A probabilistic clustering algorithm is used to analyze the time-series. The algorithm dissects the multidimensional time-series into clusters, producing a one-dimensional sequence of symbols. The symbols are used to train a hidden Markov model, which is subsequently used to predict novel contact conditions. We show that the learned classifier can successfully distinguish the three contact states with an accuracy of 72% ± 12 %.","Robot sensing systems,
Grippers,
Valves,
Force,
Torque,
Hidden Markov models"
Machine Learning Based Session Drop Prediction in LTE Networks and Its SON Aspects,"Abnormal bearer session release (i.e. bearer session drop) in cellular telecommunication networks may seriously impact the quality of experience of mobile users. The latest mobile technologies enable high granularity real-time reporting of all conditions of individual sessions, which gives rise to use data analytics methods to process and monetize this data for network optimization. One such example for analytics is Machine Learning (ML) to predict session drops well before the end of session. In this paper a novel ML method is presented that is able to predict session drops with higher accuracy than using traditional models. The method is applied and tested on live LTE data offline. The high accuracy predictor can be part of a SON function in order to eliminate the session drops or mitigate their effects.","Time series analysis,
Uplink,
Interference,
Support vector machines,
Kernel,
Accuracy,
Signal to noise ratio"
MALMOS: Machine Learning-Based Mobile Offloading Scheduler with Online Training,"This paper proposes and evaluates MALMOS, a novel framework for mobile offloading scheduling based on online machine learning techniques. In contrast to previous works, which rely on application-dependent parameters or predefined static scheduling policies, MALMOS provides an online training mechanism for the machine learning-based runtime scheduler such that it supports a flexible policy that dynamically adapts scheduling decisions based on the observation of previous offloading decisions and their correctness. To demonstrate its practical applicability, we integrated MALMOS with an existing Java-based, offloading-capable code recapturing framework, Partner. Using this integration, we performed quantitative experiments to evaluate the performance and cost for three machine learning algorithms: instance-based learning, perception, and naive Bays, with respect to classifier training time, classification time, and scheduling accuracy. Particularly, we examined the adaptability of MALMOS to various network conditions and computing capabilities of remote resources by comparing the scheduling accuracy with two static scheduling cases: threshold-based and linear equation-based scheduling policies. Our evaluation uses an Android-based prototype for experiments, and considers benchmarks with different computation/communication characteristics, and different computing capabilities of remote resources. The evaluation shows that MALMOS achieves 10.9%~40.5% higher scheduling accuracy than two static scheduling policies.","Training,
Mobile communication,
Runtime,
Processor scheduling,
Accuracy,
Dynamic scheduling"
A machine learning approach to identify DNA replication proteins from sequence-derived features,"DNA replication, a critical step in cell division and proliferation, is a process of producing two identical replicas from one original DNA molecule. Although great advances have been made in DNA replication research, the detailed mechanism of DNA replication is still unresolved. Faithful DNA replication requires the cooperation of many proteins. Failures in DNA replication leave mutations in the genome, which can cause cancers and other diseases. Therefore, accurately identifying these important DNA replication proteins may assist in understanding the molecular mechanisms of DNA replication and drug development. As the experimental methods are expensive and labor intensive, it is highly desired to develop an accurate computational method for identifying DNA replication proteins. In this paper, a machine learning approach to identify DNA replication proteins has been developed using a Naïve Bayes classifier and sequence-derived features. The prediction performance of features extracted from the Reduced Amino Acid Composition (RAAC) and two Pseudo Amino Acid Composition (PseAAC) models is investigated, respectively. Prediction results indicate that the PseAAC (type 2) model yields the best performance. Then, based on the PseAAC (type 2) model, we compare our method with the similarity search method on the independent test dataset. The comparison results reveal that it is feasible to identify DNA replication proteins by machine learning algorithms. The proposed method may provide candidate DNA replication proteins for future experimental verification to assist in understanding the molecular mechanisms of DNA replication and drug development for the treatment of human diseases.","Proteins,
DNA,
Amino acids,
Accuracy,
Feature extraction,
Sensitivity,
Diseases"
A machine learning approach to cognitive radar detection,We consider the requirements of cognitive radar detection in the presence of non-Gaussian clutter. A pair of machine learning approaches based on non-linear transformations of order statistics are examined with the goal of adaptively determining the optimal detection threshold within the low sample support regime. The impact of these algorithms on false alarm rate is also considered. It is demonstrated that the adaptive threshold estimate is effective even when the distribution in question is unknown to the machine learning algorithm.,"Clutter,
Shape,
Detectors,
Covariance matrices,
Radar detection,
Libraries"
Implementation of machine learning applications on a fixed-point DSP,"In this paper, we discuss efficient implementation of machine learning algorithms on DSPs. Specifically, we implement OCR and speech recognition on DSP and show how they can be optimized using fixed point routines. We illustrate the optimal usage of DSP resources like MAC units, shifters and software pipelining through assembly code structuring which massively reduces the MIPS consumed by the processor. We also describe how floating point overheads can be reduced by equivalent fixed point routines for real time implementations. Though the Blackfin-533 DSP is chosen for this illustration, the ideas presented here apply to other fixed point DSPs as well.","Feature extraction,
Digital signal processing,
Hidden Markov models,
Optical character recognition software,
Speech,
Speech recognition,
Mel frequency cepstral coefficient"
Energy Hub optimal sizing in the smart grid; machine learning approach,"The interests in “Energy Hub” (EH) and “Smart Grid” (SG) concepts have been increasing, in recent years. The synergy effect of the coupling between electricity and natural gas grids and utilizing intelligent technologies for communicating, may change energy management in the future. A new solution entitling “Smart Energy Hub” (S. E. Hub) that models a multi-carrier energy system in a SG environment studied in this paper. Moreover, the optimal size of CHP, auxiliary boiler, absorption chiller, and also transformer unit as main elements of a S. E. Hub is determined. Authors proposed a comprehensive cost and benefit analysis to optimize these elements and apply Reinforcement Learning (RL) algorithm for solving the optimization problem. To confirm the proposed method, a residential customer has been investigated as an S. E. Hub in a dynamic electricity pricing market.","Cogeneration,
Learning (artificial intelligence),
Boilers,
Energy management,
Natural gas,
Smart grids,
Absorption"
Estimating solar radiation by machine learning methods,"Solar energy, which is clean and renewable energy source, is a popular subject. The estimation of solar radiation can be done instead of long term measurements. Therefore, the satellite and meteorological values of 53 different locations of Turkey were used for estimations of solar radiation. In this study a hybrid approach was proposed. The train dataset was reduced by employing two times similarity and the reduced dataset was utilized with support vector machine to predict global solar radiation. Additionally, the proposed method was validated by employing neural network, linear regression, k nearest neighbor, extreme learning machine, Gaussian process regression and kernel smooth regression. This study was showed that the machine learning methods can be used instead of long term measurement before investments.","Lead,
Atmospheric modeling,
Data models,
Ground penetrating radar"
Classification with Extreme Learning Machine and ensemble algorithms over randomly partitioned data,"In this age of Big Data, machine learning based data mining methods are extensively used to inspect large scale data sets. Deriving applicable predictive modeling from these type of data sets is a challenging obstacle because of their high complexity. Opportunity with high data availability levels, automated classification of data sets has become a critical and complicated function. In this paper, the power of applying MapReduce based Distributed AdaBoosting of Extreme Learning Machine (ELM) are explored to build reliable predictive bag of classification models. Thus, (i) dataset ensembles are build; (ii) ELM algorithm is used to build weak classification models; and (iii) build a strong classification model from a set of weak classification models. This training model is applied to the publicly available knowledge discovery and data mining datasets.","Skin,
Big data,
Data mining,
Data models,
Predictive models,
Partitioning algorithms,
Reliability"
Machines Learning Culture,"Recent interdisciplinary explorations, integrating computer science, math, the digital humanities, and the arts, point to the utilitarian and expressive capabilities of machine-learning approaches in creating work with diverse appeal. These initiatives include research within the relatively traditional domain of historical art analysis, a growing collection of body-tracking work using machine learning in the background, and a variety of provocative art installations that place algorithmic computing front and center. While these projects tackle their subject at varying levels of scale and depth and in different contexts, each contributes to building the public discourse about the impact, role, and reach of machine learning in our lives.","Motion pictures,
Media,
Art,
Visualization,
Artificial intelligence,
Digital art,
Machine vision"
Determination of the variables affecting the maximal oxygen uptake of cross-country skiers by using machine learning and feature selection algorithms,"Maximal oxygen uptake (VO2max) is one of the most important determinants that directly affects the performance of cross-country skiers during races. In this study, various models have been developed to predict the VO2max of cross-country skiers by combining different machine learning methods with the Relief-F feature selection algorithm. Machine learning methods used in this study include General Regression Neural Network (GRNN), Gene Expression Programming (GEP), Group Method of Data Handling Polynomial Network (GMDH) and Single Decision Tree (SDT). The predictor variables used to develop prediction models are age, gender, weight, height, heart rate (HR), heart rate at lactate threshold (HRLT) and exercise time. By using 10-fold cross-validation on the dataset, the performance of the prediction models has been evaluated by calculating their multiple correlation coefficient (R) and standard error of estimate (SEE). The results show that the GRNN-based model including all predictor variables yields the highest R (0.92) and the lowest SEE (2.98 ml kg-1 min-1).","Support vector machines,
Heart rate,
Neural networks,
Oxygen,
Gene expression,
Programming,
Polynomials"
Development of new non-exercise maximum oxygen uptake models by using different machine learning methods,"Maximal oxygen consumption (VO2max) is the highest amount of oxygen used by the body during intense exercise. In this study, new non-exercise models have been developed by using different machine learning methods for predicting the VO2max values of healthy individuals aged between 18 and 65 years. The models include the non-exercise physiological variables (gender, age, weight and height) and questionnaire data. Cascade Correlation Network (CCN), Group Method of Data Handling (GMDH), Decision Tree Forest (DTF) and Single Decision Tree (SDT) methods have been used for developing the prediction models. The performance of the prediction models has been evaluated by calculating their multiple correlation coefficient (R) and standard error of estimate (SEE). The results show that CCN-based prediction models yield 24.54% on the average lower SEE's than the ones obtained by other methods.","Support vector machines,
Predictive models,
Decision trees,
Mathematical model,
Physiology,
Impedance,
Oxygen"
Prediction of maximum oxygen uptake with different machine learning methods by using submaximal data,"Maximum oxygen uptake (VO2max) is the highest amount of oxygen used by the body during intense exercise and is an important component to determine cardiorespiratory fitness. In this study, models have been developed for predicting VO2max with four different machine learning methods. These methods are Treeboost (TB), Decision Tree Forest (DTF), Gene Expression Programming (GEP) and Single Decision Tree (SDT). The predictor variables used to develop prediction models include gender, age, weight, height, treadmill speed, heart rate and stage. The performance of the prediction models have been evaluated by calculating Standard Error of Estimate (SEE) and Multiple Correlation Coefficient (R) and using 10-fold cross validation. Results show that compared to the SEE's of TB, the maximum percentage decrement rates in SEE's of DTF, GEP and SDT are 8.38%, 12.97% and 23.07%, respectively.","Decision trees,
Predictive models,
Gene expression,
Programming,
Art,
Support vector machines,
Oxygen"
Development of new upper body power prediction models for cross-country skiers by using different machine learning methods,"Upper Body Power (UBP) is one of the most important determinants that directly affects the performance of cross-country skiers during races. In this study, new models have been developed to predict the 10-second UBP (UBP10) and 60-second UBP (UBP60) of cross-country skiers by using different machine learning methods including Cascade Correlation Network (CCN), Radial Basis Function Neural Network (RBF) and Decision Tree Forest (DTF). The predictor variables used to develop prediction models are age, gender, body mass index (BMI), heart rate (HR), maximal oxygen uptake (VO2max) and exercise time. By using 10-fold cross-validation on the dataset, the performance of the prediction models has been evaluated by calculating their multiple correlation coefficient (R) and standard error of estimate (SEE). The results show that the CCN-based model including the predictor variables age, gender, BMI and VO2max yields the lowest SEE both for the prediction of UBP10 and UBP60.","Support vector machines,
Predictive models,
Heart rate,
Radial basis function networks,
Decision trees,
Learning systems"
Kernel-based machine learning using radio-fingerprints for localization in wsns,"This paper introduces an original method for sensors localization in WSNs. Based on radio-location fingerprinting and machine learning, the method consists of defining a model whose inputs and outputs are, respectively, the received signal strength indicators and the sensors locations. To define this model, several kernel-based machine-learning techniques are investigated, such as the ridge regression, support vector regression, and vector-output regularized least squares. The performance of the method is illustrated using both simulated and real data.","Optimization,
Sensors,
Wireless sensor networks,
Kernel,
Databases,
Computational modeling,
Mathematical model"
Comparison of machine learning methods for the sequence labelling applications,"In this study, on artificial data sets, it was compared condition random fields(CRF) and classical machine learning(CML) types. First part of this study, the performances of CRF and CML types were measured on artificial data sets. As the result of studies, CML types, except Naive Bayes, performanced higher than CRF. The success of NR and CRF is high when the outputs consist of one distribution, in other case it stays low. Besides in this study, it was evaluated the effect of education set size on success. The second study was made to test this situation.","Niobium,
Hidden Markov models,
Labeling,
Bagging,
Probabilistic logic,
Data models,
Data mining"
Predicting upper body power of cross-country skiers using machine learning methods combined with feature selection,"Upper body power (UBP) is one of the most important determinants of cross-country ski race performance. In this study, General Regression Neural Networks (GRNN), Radial Basis Function Neural Network (RBF), Decision Tree Forest (DTF) combined with a feature selection algorithm have been used to developed prediction models for estimating 10-second UBP (UBP10) and 60-second UBP (UBP60) of cross-country skiers. By using the Relief-F attribute selection algorithm, the score of each attribute has been calculated. Seven different UBP10 and UBP60 prediction models have been developed by removing the attribute with the lowest score at a time. By using 10-fold cross-validation on the data set, the performance of the prediction models has been evaluated by calculating their multiple correlation coefficients (R) and standard error of estimate (SEE). The results show that gender and VO2max are the most effective variables for prediction of UBP10 and UBP60.","Reliability,
Predictive models"
Machine Learning for Wideband Localization,"Wireless localization has a great importance in a variety of areas including commercial, service, and military positioning and tracking systems. In harsh indoor environments, it is hard to localize an agent with high accuracy due to non-line-of-sight (NLOS) radio blockage or insufficient information from anchors. Therefore, NLOS identification and mitigation are highlighted as an effective way to improve the localization accuracy. In this paper, we develop a robust and efficient algorithm to enhance the accuracy for (ultrawide bandwidth) time-of-arrival localization through identifying and mitigating NLOS signals with relevance vector machine (RVM) techniques. We also propose a new localization algorithm, called the two-step iterative (TSI) algorithm, which converges fast with a finite number of iterations. To enhance the localization accuracy as well as expand the coverage of a localizable area, we continue to exploit the benefits of RVM in both classification and regression for cooperative localization by extending the TSI algorithm to a centralized cooperation case. For self-localization setting, we then develop a distributed cooperative algorithm based on variational Bayesian inference to simplify message representations on factor graphs and reduce communication overheads between agents. In particular, we build a refined version of Gaussian variational message passing to reduce the computational complexity while maintaining the localization accuracy. Finally, we introduce the notion of a stochastic localization network to verify proposed cooperative localization algorithms.","Distance measurement,
Support vector machines,
Accuracy,
Delays,
IEEE 802.15 Standards,
Noise,
Bandwidth"
Categorize the video server in P2P networks based on seasonal and normal popularity videos using machine learning approach,"There is a wide-ranging use of Peer-to-Peer (P2P) computing and applications in majority of the key areas of Engineering and Technology. Devoid of any centralized server, they can share their content since peers are linked with each other. This is the reason why P2P computing gives enhanced communication among peers. It is essential for the video server to maintain the data content link in cache memory so the cache memory sizes will be enlarged to a definite level and also the cache needs to be securely sustained by each and every peers. By utilizing the Machine Learning method, the proposed method centers its concentration on classifying the video server depending on seasonal and non seasonal popularity. Two supervised Machine Learning algorithms are utilized in this paper and are explained as follows. The Case-Based Reasoning algorithm is utilized in order to sort out well-liked videos and the Averaged One-Dependence Estimators (AODE) algorithm is utilized to sort out video server into seasonal and non-seasonal. The first algorithm is based on Retrieve, Reuse, Revise and Retain methods and the latter algorithm sorts out the video server into seasonal and non-seasonal based video servers. The work simulated by Java programming language.","Servers,
Cognition,
Machine learning algorithms,
Classification algorithms,
Streaming media,
Media,
Entertainment industry"
Processing smart plug signals using machine learning,"The automatic identification of appliances through the analysis of their electricity consumption has several purposes in Smart Buildings including better understanding of the energy consumption, appliance maintenance and indirect observation of human activities. Electric signatures are typically acquired with IoT smart plugs integrated or added to wall sockets. We observe an increasing number of research teams working on this topic under the umbrella Intrusive Load Monitoring. This term is used as opposition to Non-Intrusive Load Monitoring that refers to the use of global smart meters. We first present the latest evolutions of the ACS-F database, a collections of signatures that we made available for the scientific community. The database contains different brands and/or models of appliances with up to 450 signatures. Two evaluation protocols are provided with the database to benchmark systems able to recognise appliances from their electric signature. We present in this paper two additional evaluation protocols intended to measure the impact of the analysis window length. Finally, we present our current best results using machine learning approaches on the 4 evaluation protocols.","Home appliances,
Protocols,
Databases,
Accuracy,
Monitoring,
Training,
Hidden Markov models"
Automatic Thread-Level Canvas Analysis: A machine-learning approach to analyzing the canvas of paintings,"Canvas analysis is an important tool in art-historical studies, as it can provide information on whether two paintings were made on canvas that originated from the same bolt. Canvas analysis algorithms analyze radiographs of paintings to identify (ir)regularities in the spacings between the canvas threads. To reduce noise, current state-of-the-art algorithms do this by averaging the signal over a number of threads, which leads to information loss in the final measurements. This article presents an algorithm capable of performing thread-level canvas analysis: the algorithm identifies each of the individual threads in the canvas radiograph and directly measures between-distances and angles of the identified threads. We present two case studies to illustrate the potential merits of our thread-level canvas analysis algorithm, viz. on a small collection of paintings ostensibly by Nicholas Poussin and on a small collection of paintings by Vincent van Gogh.","Painting,
Instruction sets,
Radiography,
Algorithm design and analysis,
Signal processing algorithms,
Art,
Feature extraction,
Information analysis"
Machine learning based biomedical named entity recognition,"The biomedical society makes wide use of text mining technology. Named Entity (NE) extraction is one of the most primary and significant tasks in biomedical information extraction of text mining technology. Named Entity Recognition (NER) involves processing structured and unstructured documents to recognize the definite kinds of entities and categorization of them into some predefined classes. Several Named Entity Recognition systems have been developed for the Biomedical Domain based on the Rule-Based, Dictionary based and Machine Learning based techniques. Implementing the best approach is not possible in all domains. Machine learning based approaches have many advantages than other approaches. In this paper we are proposing a Machine learning based framework for recognizing named entities from biomedical abstracts. For this study we used benchmarked datasets such as GENETAG and JNLPBA.","text analysis,
data mining,
learning (artificial intelligence),
medical computing"
A study on mammography computer aided diagnosis system using machine learning methods,"Early diagnosis is an important aspect of successful treatment for breast cancer. Mammogram is the most reliable imaging technique available. It is a challenging task for radiologists to detect the abnormalities in the mammograms. Computing helps the radiologists in diagnosing the abnormalities in the mammogram. Computer Aided Diagnosis System involves computerized biomedical image analysis to classify the mammography into benign or malign. In a decade of research work number of algorithms had been proposed to classify the image that employ data mining techniques, image processing methods, machine learning methods and pattern recognition. In this paper such algorithms in previous research work is studied and their performance is discussed.","medical image processing,
biological organs,
cancer,
data mining,
image classification,
learning (artificial intelligence),
mammography"
Detection of fraudulent financial reports with machine learning techniques,"This paper describes our efforts to apply various advanced supervised machine learning and natural language processing techniques, including Binomial Logistic Regression, Support Vector Machines, Neural Networks, Ensemble Techniques, and Latent Dirichlet Allocation (LDA), to the problem of detecting fraud in financial reporting documents available from the United States' Security and Exchange Commission EDGAR database. Specifically, we apply LDA to a collection of type 10-K financial reports and to generate document-topic frequency matrix, and then submit these data to a series of advanced classification algorithms. We then apply evaluation metrics, such as Precision, Receiver Operating Characteristic Curve, and Area Under the Curve to evaluate the performance of each algorithm. We conclude that these methods show promise and suggest applying the approach to a larger set of input documents.","Natural language processing,
Support vector machines,
Classification algorithms,
Logistics,
Neural networks,
Accuracy,
Correlation"
Wind farm performance validation through machine learning: Sector-wise Honest Brokers,Recent methods to optimize wind farm performance require new methods to assess and validate wind farm level performance. This paper introduces a machine learning approach based on sector wise honest brokers in order to determine expectation of wind energy and validate performance improvements. The approach treats every turbine in the wind farm as a virtual Metmast. Farm level expectation of power is determined based on machine learning models trained on baseline data with input features reflecting “Honest Brokers”: turbines that experience similar conditions in both a training interval and a testing interval in which we are expecting a change in farm performance. Our approach is able to validate farm level improvements even in the face of farm optimization technologies for controlling wakes that change the wind profile within the farm.,"Wind energy,
Training,
Data models,
Transfer functions"
Domain Adaptation Extreme Learning Machines for Drift Compensation in E-Nose Systems,"This paper addresses an important issue known as sensor drift, which exhibits a nonlinear dynamic property in electronic nose (E-nose), from the viewpoint of machine learning. Traditional methods for drift compensation are laborious and costly owing to the frequent acquisition and labeling process for gas samples' recalibration. Extreme learning machines (ELMs) have been confirmed to be efficient and effective learning techniques for pattern recognition and regression. However, ELMs primarily focus on the supervised, semisupervised, and unsupervised learning problems in single domain (i.e., source domain). To our best knowledge, ELM with cross-domain learning capability has never been studied. This paper proposes a unified framework called domain adaptation extreme learning machine (DAELM), which learns a robust classifier by leveraging a limited number of labeled data from target domain for drift compensation as well as gas recognition in E-nose systems, without losing the computational efficiency and learning ability of traditional ELM. In the unified framework, two algorithms called source DAELM (DAELM-S) and target DAELM (DAELM-T) are proposed in this paper. In order to perceive the differences among ELM, DAELM-S, and DAELM-T, two remarks are provided. Experiments on the popular sensor drift data with multiple batches collected using E-nose system clearly demonstrate that the proposed DAELM significantly outperforms existing drift-compensation methods without cumbersome measures, and also bring new perspectives for ELM.","Training,
Vectors,
Optimization,
Electronic noses,
Target recognition,
Communities,
Pattern recognition"
Developing machine learning tools for long-lead heavy precipitation prediction with multi-sensor data,"A large number of extreme floods were closely related to heavy precipitation which lasted for several days or weeks. Long-lead prediction of extreme precipitation, i.e., prediction of 6-15 days ahead of time, is important for understanding the prognostic forecasting potential of many natural disasters, such as floods. Yet, long-lead flood forecasting is a challenging task due to the cascaded uncertainty with prediction errors from measurements to modeling, which makes the current physics-based numerical simulation models extremely complex and inaccurate. In this paper, we formulate the modeling work as a machine learning problem and introduce a complementary data mining framework for heavy precipitation prediction. Heavy precipitation that may lead to extreme floods is a rare event. Long-lead prediction requires the corresponding feature space to be sampled from extremely high spatio-temporal dimensions. Such a complexity makes long-lead heavy precipitation prediction a high dimensional and imbalanced machine learning problem. In this work, we firstly define the extreme precipitation and non-extreme precipitation clusters and then design the Nearest-Sample Choosing method to handle the imbalanced data sets. We introduce streaming feature selection and subspace learning to extract the most relevant features from high dimensional data. We evaluate the machine learning tools using historical flood data collected in the State of Iowa, the United States and associated hydrometeorological variables from 1948 to 2010.","Floods,
Forecasting,
Training data,
Predictive models,
Data mining,
Classification algorithms,
Accuracy"
Optimizing testing efforts based on change proneness through machine learning techniques,"For any software organization, understanding the software quality is desirable in order to increase user experience of the software. When we talk about security software this factor becomes even more important. This paper aims to develop models for predicting the change proneness for object oriented system. The developed models may be used to predict the change prone classes at early phase of software development. Rigorous testing and allocation of some extra resources to those change prone classes may lead to better quality and it may also reduce our work at the maintenance phase. We apply one statistical and 10 machine learning techniques to predict the models. The results are analyzed from Receiver Operating Characteristics (ROC) analysis using Area under the Curve (AUC) obtained from ROC. Adaboost and Random forest method have shown the best result and hence, based on these results we can claim that quality models have a good relevance with Object Oriented systems.","Object oriented modeling,
Software,
Measurement,
Predictive models,
Unified modeling language,
Security,
Maintenance engineering"
A Machine Learning Approach for Accurate Annotation of Noncoding RNAs,"Searching genomes to locate noncoding RNA genes with known secondary structure is an important problem in bioinformatics. In general, the secondary structure of a searched noncoding RNA is defined with a structure model constructed from the structural alignment of a set of sequences from its family. Computing the optimal alignment between a sequence and a structure model is the core part of an algorithm that can search genomes for noncoding RNAs. In practice, a single structure model may not be sufficient to capture all crucial features important for a noncoding RNA family. In this paper, we develop a novel machine learning approach that can efficiently search genomes for noncoding RNAs with high accuracy. During the search procedure, a sequence segment in the searched genome sequence is processed and a feature vector is extracted to represent it. Based on the feature vector, a classifier is used to determine whether the sequence segment is the searched ncRNA or not. Our testing results show that this approach is able to efficiently capture crucial features of a noncoding RNA family. Compared with existing search tools, it significantly improves the accuracy of genome annotation.","Genomics,
RNA,
Computational modeling,
Hidden Markov models,
Bioinformatics"
Machine learning schemes in augmented reality for features detection,"Augmented Reality (AR) is a relatively old concept technology, which reached the large public very recently. We can use it to enhance our environments, by augmenting the image, the voice and delivering details and annotations about the surrounding space. Augmented reality (AR) is a growing field, with many diverse applications ranging from TV and film production, to industrial maintenance, medicine, education, entertainment and games. This paper presents an improved approach for image augmented-reality, by acting on two axes in the augmented reality process. First, a machine learning step is added to the detection part. Second, the registration of augmented image is processed by using the following techniques: statistical appearance models, and covariance matrices of dense image descriptors. A tuning of the used techniques and algorithms will be done in order to obtain a reliable and real-time image augmentation. We give a detailed description on how we chose the methods, and we compare our approach with other methods used in this domain. Finally, an evaluation of the proposed technique is presented as well as a performance study for a given use case.","Feature extraction,
Image registration,
Augmented reality,
Support vector machines,
Computer vision,
Classification algorithms,
Covariance matrices"
Machine Learning Facilitated Rice Prediction in Bangladesh,"The climate of a region is often determined by its landscape and amount of vegetation present in it. Environment parameters such as rainfall, wind-speed and humidity are highly influenced by these alluvial features. Bangladesh, a country situated on the banks of the Himalaya, does not have a homogeneous topography. Human settlement over the course of centuries has led to pockets of micro-regions. Each of those regions has a different micro climate. An entrepreneur involved in the food industry therefore has to carefully choose regions of land that will give him/her the desirable production. In this study a research initiative has been taken to predict the yield of crops using machine learning models. The models were at first trained on the correlation between past environmental patterns and crop production rate. Then the models are compared to measure their effectiveness on unknown climatic variables.","Agriculture,
Production,
Linear regression,
Regression tree analysis,
Vegetation,
Humidity"
Machine learning and its applications in e-learning systems,"Exploiting the ceaselessly enhancing, online learning frameworks assumes a paramount part for adjusting toward oneself, particularly on account of working individuals. All things considered, learning frameworks don't for the most part adjust to learners' profiles. Learners need to invest a ton of time before arriving at the learning objective that is perfect with their insight foundation. This paper investigates Machine learning and its applications in E-Learning frameworks. Machine learning, we may say, is a sort of artificial intelligence (AI) that gives machines the capacity to learn without being expressly customized. Machine learning concentrates on the advancement of machine projects that can develop themselves and change when presented to new information [8].","Electronic learning,
Adaptation models,
Predictive models,
Organizations"
Machine learning for wear forecasting of naval assets for condition-based maintenance applications,"Economic sustainability of running Naval Propulsion Plants is a key element to cope with, and maintenance costs represent a large slice of total operational expenses: last decades' approaches, based on a repairing-replacing methodology, are being trespassed by more effective approaches, relying on effective continuous monitoring of assets wear. In this framework, Condition-Based Maintenance (CBM) is becoming key thanks to the enhancing capabilities of monitoring the propulsion equipment by exploiting heterogeneous sensors: this enables diagnosis and prognosis of the propulsion system's components and of their potential future failures. The success of CBM is based on the capability of developing effective predictive models, for which purpose state-of-the-art Machine Learning (ML) methods must be developed. Nevertheless, testing the performance of ML models for CBM purposes is not straightforward, mostly due to the lack of publicly available datasets for benchmarking purposes: thus, we present in this work a new dataset, that will be freely distributed to the community working on ML models for CBM, generated from an accurate simulator of a naval vessel Gas Turbine propulsion plant. The latter is then used for benchmarking the effectiveness of two state-of-the-art ML techniques in the considered maritime domain.","Turbines,
Maintenance engineering,
Propulsion,
Predictive models,
Data models,
Numerical models,
Marine vehicles"
Performance evaluation of machine learning techniques for screening of cervical cancer,"This paper presents comparative analysis of various machine learning algorithms in order to evaluate their predictive performance for screening of cervical cancer by characterization and classification of Pap smear images. Papanicolaou smear (also referred to as Pap smear) is a microscopic examination of samples of human cells scraped from the lower, narrow part of the uterus, called cervix. The sample is observed under microscope for any unusual developments indicating any precancerous and potentially precancerous changes. Examining the cell images for abnormalities in the cervix provides grounds for provision of prompt action and thus reducing incidence and deaths from cervical cancer. Pap smear test, if done with a regular screening programs and proper follow-up, can reduce cervical cancer mortality by up to 80% [1]. Authors have applied fifteen different machine learning algorithms under different platforms over two databases and evaluated their screening performances for prognosis of cervical cancer. The results indicate that among all the algorithms implemented, the Ensemble of nested dichotomies (END) is the best predictor and Naïve Bayes was the worst performer.","Cervical cancer,
Classification algorithms,
Machine learning algorithms,
Algorithm design and analysis,
Training,
Databases,
Artificial intelligence"
Predicting source gaze fixation duration: A machine learning approach,"In this paper an attempt has been made to predict the gaze fixation duration at source text words using supervised learning method, namely Support Vector Machine. The machine learning models used in the present work make use of lexical, syntactic and semantic information for predicting the gaze fixation duration. Different features are extracted from the data and models are built by combining the features. Our best set up achieves close to 50% classification accuracy.","Entropy,
Accuracy,
Predictive models,
Correlation coefficient,
Feature extraction,
Syntactics,
Linear regression"
Real time BIG data analytic: Security concern and challenges with Machine Learning algorithm,"With great power of data comes great responsibility! A big data initiative should not only focus on the volume, velocity or variety of the data, but also on the best way to protect it. Security is usually an afterthought, but Elemental provides the right technology framework to get you the deep visibility and multilayer security any big data project requires. Multilevel protection of your data processing nodes means implementing security controls at the application, operating system and network level while keeping a bird's eye on the entire system using actionable intelligence to deter any malicious activity, emerging threats and vulnerabilities. Advances in Machine Learning (ML) provide new challenges and solutions to the security problems encountered in applications, technologies and theories. Machine Learning (ML) techniques have found widespread applications and implementations in security issues. Many ML techniques, approaches, algorithms, methods and tools are extensively used by security experts and researchers to achieve better results and to design robust systems.","Blogs,
Filtering,
Cryptography,
Analytical models,
Big data,
Data models,
High definition video"
Machine Learning and Lexicon Based Methods for Sentiment Classification: A Survey,"Sentiment classification is an important subject in text mining research, which concerns the application of automatic methods for predicting the orientation of sentiment present on text documents, with many applications on a number of areas including recommender and advertising systems, customer intelligence and information retrieval. In this paper, we provide a survey and comparative study of existing techniques for opinion mining including machine learning and lexicon-based approaches, together with evaluation metrics. Also cross-domain and cross-lingual approaches are explored. Experimental results show that supervised machine learning methods, such as SVM and naive Bayes, have higher precision, while lexicon-based methods are also very competitive because they require few effort in human-labeled document and isn't sensitive to the quantity and quality of the training dataset.","Support vector machines,
Sentiment analysis,
Learning systems,
Text categorization,
Accuracy,
Training"
Map based representation of navigation information for robust machines learning,"Much research is being carried out in autonomous driving of vehicles under various disciplines but very few autonomous navigating vehicles are developed till date. This paper presents a novel technique, with a practical example, for the representation of navigational information under real-time considerations. Machine learning algorithms can easily be trained with the data sets developed from the representation and to testify this, an Artificial Neural Network is trained with a represented data set. This technique is independent of driving capabilities of vehicle and provides simple directional instructions for navigation. Using this method, an autonomous driving vehicle can be made to learn to navigate between locations in a known region. Moreover, the usage of ANN makes it completely adaptive and any changes or modifications in the trained region can easily be updated into the knowledge base.","Artificial neural networks,
Navigation,
Vehicles,
Training,
Machine learning algorithms,
Junctions,
Neurons"
Machine Learning Based Cross-Site Scripting Detection in Online Social Network,"Nowadays online social network (OSN) is one of the most popular internet services in the world. It allows us to communicate with others and share knowledge. However, from the security's point of view, OSN is becoming the favorite target for the attackers, and is under a lot of threats such as cross-site scripting (XSS) attacks. In this paper, we present a novel approach using machine learning to do XSS detection in OSN. Firstly, we leverage a new method to capture identified features from web pages and then establish classification models which can be used in XSS detection. Secondly, we propose a novel method to simulate XSS worm spreading and build our webpage database. Finally, we set up experiments to verify the classification models using our test database. Our experiment results demonstrate that our approach is an effective countermeasure to detect the XSS attack.","Feature extraction,
Grippers,
Databases,
HTML,
Security,
Social network services,
Uniform resource locators"
Early detection of VoIP network flows based on sub-flow statistical characteristics of flows using machine learning techniques,"Network traffic classification plays an important role in the areas of network security, network monitoring, QoS and traffic engineering. In this paper, we design a network traffic classifier based on the statistical features extracted from network flows. Instead of deriving the statistical characteristics per flow, our model make use of features extracted from the first few seconds of each flows. The first few seconds of each flow is divided into overlapping time-based windows. This approach enables our classifier to classify each flow early. Attribute selection algorithms Chi-Square, CON and CFS are used to obtain the optimal subset of features. We give a comparative analysis of the result on the said approach based on the classification algorithms (Decision tree (C4.5), Naive Bayes, Bayesian Belief Network and SVM). We also present a single class classifier implementation of C4.5 algorithm. The experimental results show that the proposed method can achieve over 99% accuracy for all testing dataset. Using the proposed method, C4.5 algorithm delivers high speed and accuracy. By taking inference from these offline classifiers, we build an online standalone classifier using C/C++. We used the following applications: Skype, Gtalk and Asterisk.","Feature extraction,
Testing,
Training,
Accuracy,
Ports (Computers),
Media,
Algorithm design and analysis"
Application of Machine Learning Techniques for Amplitude and Phase Noise Characterization,"In this paper, tools from machine learning community, such as Bayesian filtering and expectation maximization parameter estimation, are presented and employed for laser amplitude and phase noise characterization. We show that phase noise estimation based on Bayesian filtering outperforms conventional time-domain approach in the presence of moderate measurement noise. Additionally, carrier synchronization based on Bayesian filtering, in combination with expectation maximization, is demonstrated for the first time experimentally.","Bayes methods,
Phase noise,
Mathematical model,
Kalman filters,
Vectors,
State-space methods"
GPGPU performance and power estimation using machine learning,"Graphics Processing Units (GPUs) have numerous configuration and design options, including core frequency, number of parallel compute units (CUs), and available memory bandwidth. At many stages of the design process, it is important to estimate how application performance and power are impacted by these options. This paper describes a GPU performance and power estimation model that uses machine learning techniques on measurements from real GPU hardware. The model is trained on a collection of applications that are run at numerous different hardware configurations. From the measured performance and power data, the model learns how applications scale as the GPU's configuration is changed. Hardware performance counter values are then gathered when running a new application on a single GPU configuration. These dynamic counter values are fed into a neural network that predicts which scaling curve from the training data best represents this kernel. This scaling curve is then used to estimate the performance and power of the new application at different GPU configurations. Over an 8× range of the number of CUs, a 3.3× range of core frequencies, and a 2.9× range of memory bandwidth, our model's performance and power estimates are accurate to within 15% and 10% of real hardware, respectively. This is comparable to the accuracy of cycle-level simulators. However, after an initial training phase, our model runs as fast as, or faster than the program running natively on real hardware.","Kernel,
Hardware,
Training,
Radiation detectors,
Graphics processing units,
Predictive models,
Engines"
An improved self-localization algorithm for Ad hoc network based on extreme learning machine,"Wireless sensor network (WSN) is a typical application of Ad hoc network in autonomous system (AS). It has attracted considerable attention in the past. Recent years have witnessed a growing interest in the study of localization algorithm for WSN. Self-localization of nodes is one of the key technologies for application of WSN. The localization accuracy is a significant criterion to evaluate the practical utility of localization algorithm. In most of the localization algorithms, increasing the density of anchor nodes is one of the main strategies to improve the localization accuracy. But the number of anchor nodes is always limited due to the hardware limitations, such as energy consumption, cost, and so on. In this paper, a novel machine learning algorithm is developed to improve the accuracy. Specifically, we propose a DV-Hop self-localization algorithm using extreme learning machine (ELM), called DV-Hop-ELM, which achieves the objective by virtually increasing the number of anchor nodes. The ELM-based single-hidden layer feedforward network (SLFN) is firstly designed to find the appropriate sub-anchor nodes. Moreover, the improved DV-Hop localization algorithm utilizes those anchor nodes and sub-anchor nodes to localize remaining sensor nodes. We test the algorithm DV-Hop-ELM to demonstrate its performance. Compared to classic DV-Hop algorithm, the proposed method improves the localization accuracy while reducing the costs of WSN.","Wireless sensor networks,
Ad hoc networks,
Accuracy,
Algorithm design and analysis,
Simulation,
Artificial neural networks,
Feedforward neural networks"
Supervised Machine Learning Approach for Microarray Classification of Malignant Tissues Using Soft Computing Techniques,"In today's highly integrated world, when solutions to problems are cross disciplinary in nature, Soft computing promises to become a powerful means for obtaining solutions to problem quickly, accurately and acceptably. Soft computing refers to a consortium of computational methodologies that has motivated many scientific researchers to contribute their efforts in designing highly powerful intelligent systems. During the early 20th century, the advent of malignant tissues in the human cells came into knowledge of medical researchers. A herculean task of classifying the tumorous cells became a very challenging task in the gamut of information, especially in the field of intelligent systems. In this work, we have applied Artificial Bee Colony (ABC) optimization along with the supervised learning technique i.e. Support vector machine (SVM) to estimate the cost of classification. We have also simulated the cancerous dataset with the implementation of Cat Swarm Optimization (CSO) with SVM. Prior to classification, we have tried our level best to eliminate redundant and unwanted data with the help of Principal Component Analysis (PCA) technique. A great deal of work in predicting feasible as well as global best solutions have been put forward by many innovative and advanced heuristic search strategies.","Cats,
Principal component analysis,
Cancer,
Optimization,
Support vector machine classification,
Computational modeling"
A semantic-based approach for Machine Learning data analysis,"Pervasive applications and services are increasingly based on the intelligent interpretation of data gathered via heterogeneous sensors dipped in the environment. Classical Machine Learning (ML) techniques often do not go beyond a basic classification, lacking a meaningful representation of the detected events. This paper introduces a early proposal for a semantic-enhanced machine learning analysis on data of sensors streams, performing better even on resource-constrained pervasive smart objects. The framework merges an ontology-driven characterization of statistical data distributions with non-standard matchmaking services, enabling a fine-grained event detection by treating the typical classification problem of ML as a resource discovery.","Sensors,
Support vector machine classification"
Constructing Query-Driven Dynamic Machine Learning Model With Application to Protein-Ligand Binding Sites Prediction,"We are facing an era with annotated biological data rapidly and continuously generated. How to effectively incorporate new annotated data into the learning step is crucial for enhancing the performance of a bioinformatics prediction model. Although machine-learning-based methods have been extensively used for dealing with various biological problems, existing approaches usually train static prediction models based on fixed training datasets. The static approaches are found having several disadvantages such as low scalability and impractical when training dataset is huge. In view of this, we propose a dynamic learning framework for constructing query-driven prediction models. The key difference between the proposed framework and the existing approaches is that the training set for the machine learning algorithm of the proposed framework is dynamically generated according to the query input, as opposed to training a general model regardless of queries in traditional static methods. Accordingly, a query-driven predictor based on the smaller set of data specifically selected from the entire annotated base dataset will be applied on the query. The new way for constructing the dynamic model enables us capable of updating the annotated base dataset flexibly and using the most relevant core subset as the training set makes the constructed model having better generalization ability on the query, showing “part could be better than all” phenomenon. According to the new framework, we have implemented a dynamic protein-ligand binding sites predictor called OSML (On-site model for ligand binding sites prediction). Computer experiments on 10 different ligand types of three hierarchically organized levels show that OSML outperforms most existing predictors. The results indicate that the current dynamic framework is a promising future direction for bridging the gap between the rapidly accumulated annotated biological data and the effective machine-learning-based predictors. OSML web server and datasets are freely available at: http://www.csbio.sjtu.edu.cn/bioinf/OSML/ for academic use.","Proteins,
Predictive models,
Data models,
Training,
Feature extraction,
Biological system modeling"
Rule-based and machine learning approach for event sentence extraction in Indonesian online news articles,"With the rapid maturity of internet and web technology over the last decades, the number of Indonesian online news articles is growing rapidly on the web at a pace we never experienced before. In this paper, we introduce a combination of rule-based and machine learning approach to find the sentences that have tropical disease information in them, such as the incidence date and the number of casualty, and we measure its accuracy. Given a set of web pages in tropical disease topic, we first extract the sentences in the pages that match contextual and morphological patterns for a date and number of casualty using a rule-based algorithm. After that, we classify the sentences using Support Vector Machine and collect the sentences that have tropical disease information in them. The results show that the proposed method works well and has good accuracy.","Support vector machines,
Diseases,
Accuracy,
Data mining,
Feature extraction,
Dictionaries,
Kernel"
A machine learning approach for distinguishing hearing perception level using auditory evoked potentials,"An auditory loss is one of the most common disabilities present in newborns and infants in the world. A conventional hearing screening test's applicability is limited as it requires a feedback response from the subject under test. To overcome such problems, the primary focus of this study is to develop an intelligent hearing ability level assessment system using auditory evoked potential signals (AEP). AEP signal is a non-invasive tool that can reflect the stimulated interactions with neurons along the stations of the auditory pathway. The AEP responses of fourteen normal hearing subjects to auditory stimuli (20 dB, 30 dB, 40 dB, 50 dB and 60 dB) were derived from electroencephalogram (EEG) recordings. Higuchi's fractal method is applied to extract the fractal features from the recorded AEP signals. The extracted fractal features were then associated to different hearing perception levels of the subjects. Feed-forward and feedback neural networks are employed to distinguish the different hearing perception levels. The performance of the proposed intelligent hearing ability level assessment found to exceed 85% accuracy. This study indicates that AEP responses to the auditory stimuli to the normal hearing persons can predict the higher order auditory stimuli followed by the lower order auditory stimuli and consequently the state of auditory development of subjects.","Auditory system,
Ear,
Accuracy,
Feature extraction,
Fractals,
Electroencephalography,
Biological neural networks"
Toward Energy-Aware Scheduling Using Machine Learning,,
Memristor Models for Machine Learning,"In the quest for alternatives to traditional complementary metal-oxide-semiconductor, it is being suggested that digital computing efficiency and power can be improved by matching the precision to the application. Many applications do not need the high precision that is being used today. In particular, large gains in area and power efficiency could be achieved by dedicated analog realizations of approximate computing engines. In this work we explore the use of memristor networks for analog approximate computation, based on a machine learning framework called reservoir computing. Most experimental investigations on the dynamics of memristors focus on their nonvolatile behavior. Hence, the volatility that is present in the developed technologies is usually unwanted and is not included in simulation models. In contrast, in reservoir computing, volatility is not only desirable but necessary. Therefore, in this work, we propose two different ways to incorporate it into memristor simulation models. The first is an extension of Strukov’s model, and the second is an equivalent Wiener model approximation. We analyze and compare the dynamical properties of these models and discuss their implications for the memory and the nonlinear processing capacity of memristor networks. Our results indicate that device variability, increasingly causing problems in traditional computer design, is an asset in the context of reservoir computing. We conclude that although both models could lead to useful memristor-based reservoir computing systems, their computational performance will differ. Therefore, experimental modeling research is required for the development of accurate volatile memristor models.",
Machine learning approach to fusion of high and low resolution imagery for improved target classification,"This work utilizes high resolution images in order to improve the classification accuracy on low resolution images. The approach is based on the machine learning paradigm called LUPI - “Learning Using Privileged Information”. In this contribution, the LUPI paradigm is demonstrated on images from the Caltech 101 dataset.","Image resolution,
Support vector machines,
Accuracy,
Training,
Feature extraction,
Data integration,
Machine learning algorithms"
Improving classification using preprocessing and machine learning algorithms on NSL-KDD dataset,"Classification is the category that consists of identification of class labels of records that are typically described by set of features in dataset. The paper describes a system that uses a set of data pre-processing activities which includes Feature Selection and Discretization. Feature selection and dimension reduction are common data mining approaches in large datasets. Here the high data dimensionality of the dataset due to its large feature set poses a significant challenge. In Pre-processing with the help of Feature selection algorithm the various required features are selected, these activities helps to improve the accuracy of the classifier. After this step various classifiers are used such as Naive Bayes, Hidden Naive Bayes and NBTree. The advantage of Hidden Naive Bayes is a data mining model that relaxes the Naive Bayes Method's conditional Independence assumption. Also the next Classifier used is NBTree which induces a hybrid of decision tree classifiers and Naïve Bayes classifiers which significantly improves the accuracy of classifier and decreases the Error rate of the classifier. The output of the proposed method are checked for True positive, True negative, False positive, False negative. Based on these values the Accuracy and error rate of each classifier is computed.","Accuracy,
Error analysis,
Classification algorithms,
Intrusion detection,
Computers,
Data mining,
Training"
SETAP: Software engineering teamwork assessment and prediction using machine learning,"Effective teaching of teamwork skills in local and globally distributed Software Engineering (SE) teams is recognized as an important part of the education of current and future software engineers. Effective methods for assessment and early prediction of learning effectiveness in SE teamwork are not only a critical part of teaching but also of value in industrial training and project management. This paper presents a novel analytical approach to the assessment and, most importantly, the prediction of learning outcomes in SE teamwork based on data from our joint software engineering class concurrently taught at San Francisco State University (SFSU), Florida Atlantic University (FAU) and Fulda University, Germany (Fulda). Our approach focuses on assessment and prediction of SE teamwork in terms of ability of student teams to apply best SE processes and develop SE products. It differs from existing work in the following aspects: a) it develops and uses only objective and quantitative measures of team activity from multiple sources, such as statistics of student time use, software engineering tool use, and instructor observations; b) it leverages powerful machine learning (ML) techniques applied to team activity measurements to identify quantitative and objective factors which can assess and predict learning of software engineering teamwork skills at the team level. In this paper we provide the following contributions: a) we present in detail for the first time the full team activity measurement data set we developed, consisting of over 40 objective and quantitative measures extracted from student teams working on class projects; b) we present a ML framework which applies the Random Forest (RF) algorithm to the team activity measurements and team outcomes, focusing on predicting teams that are likely to fail; c) we describe in detail our now fully implemented and operational data processing pipeline, consisting of data collection methods from multiple sources, ML training database creation, and ML analysis subsystems; and finally d) we present very preliminary results of ML analysis results based on the data from our joint software engineering classes in Fall 2012, and Spring 2013, with the data from 17 student teams. While our ML training database is currently small, it continuously grows. Our preliminary results, verified with two independent accuracy measures, show that RF is able to predict SE Process and SE Product team performance in intuitively explainable manner.","Teamwork,
Databases,
Time measurement,
Educational institutions,
Training,
Software"
Harmful algal blooms prediction with machine learning models in Tolo Harbour,"Machine learning (ML) techniques such as artificial neural network (ANN) and support vector machine (SVM) have been increasingly used to predict harmful algal blooms (HABs). In this paper, we use the biweekly data in Tolo Harbour, Hong Kong, and choose several machine learning methods to develop prediction models of algal blooms. Three different kinds of models are designed based on back-propagation (BP) neural network, generalized regression neural network (GRNN) and support vector machine (SVM) respectively. The experimental results show that the improved BP algorithm and SVM work better than GRNN methods, and the models based on SVM present the best performance in terms of goodness-of-fit measures, but need to be further improved in the running time. We develop these prediction models with different lead time (7-day and 14-day) to study further. The results indicate that the use of biweekly data can simulate the general trend of algal biomass reasonably, but it is not ideally suited for exact predictions. The use of higher frequency data may improve the accuracy of the predictions.","Predictive models,
Support vector machines,
Artificial neural networks,
Biological system modeling,
Lead,
Training,
Testing"
Extraction and categorization of transition information from large volume of texts using patterns and machine learning,"The information on transition of a thing is important for acquiring knowledge on the thing. In this study, we extracted transition information from a large number of sentences using pattern-based methods. We obtained an F-measure of 0.86 for extraction of transition information by this method. In order to improve the results, we then combined the pattern-based methods with supervised machine-learning methods. We obtained F-measures of 0.91 and 0.89 for extraction of transition information by support vector machine and maximum entropy methods, respectively. Thus, we confirmed that the combined approach outperformed the pattern-based method. We also categorized transition information. In the experiments, we obtained an F-measure of 0.6 for transition information categorization for categories containing many events in the training data set. Furthermore, we examined sentences in terms of conceptual changes in their transition information. We classified transition information in various ways. The categories by the classification provide a theoretical basis for future studies on transition information.","Data mining,
Support vector machines,
Manuals,
Robots,
Pattern matching,
Concrete,
Resource description framework"
Order estimation of Japanese paragraphs by supervised machine learning,"In this paper, we propose a method to estimate the order of paragraphs by supervised machine learning. We use a support vector machine (SVM) for supervised machine learning. The estimation of paragraph order is useful for sentence generation and sentence correction. The proposed method obtained a high accuracy (0.86) in the order estimation experiments of the first two paragraphs of an article and achieved the same accuracy as manual estimation. In addition, it obtained a higher accuracy than the baseline methods in the experiments using two paragraphs of an article. We performed feature analysis and we found that adnominals, conjunctions, and dates were effective for the order estimation of the first two paragraphs, and the ratio of new words and the similarity between the preceding paragraphs and an estimated paragraph were effective for the order estimation of all pairs of paragraphs. Moreover, we compared the order estimation of sentences and paragraphs and clarified differences. For the order estimation of the first two paragraphs, paragraph order estimation would be easier than sentence order estimation because paragraphs have more information than sentences. For the order estimation of all pairs of paragraphs, paragraph order estimation would be more difficult than sentence order estimation because a story may conclude in a paragraph.","Estimation,
Accuracy,
Manuals,
Support vector machines,
Training data,
Speech,
Supervised learning"
Machine learning predictions of cancer driver mutations,"A method to predict the activation status of kinase domain mutations in cancer is presented. This method, which makes use of the machine learning technique support vector machines (SVM), has applications to cancer treatment, as well as numerous other diseases that involve kinase misregulation.","Support vector machines,
Accuracy,
Genomics,
Bioinformatics"
Genetic lateral tuning of membership functions as post-processing for hybrid fuzzy genetics-based machine learning,"Genetic fuzzy systems (GFS) have been actively studied in the field of fuzzy classifier design. GFS can generate simple and accurate classifiers with a number of fuzzy if-then rules by evolutionary computation (EC). One general concern is how to discretize numerical attributes into fuzzy partitions. Most GFS use homogeneous fuzzy partitions without considering the class distribution of each attribute. This is the simplest idea, but more accurate classifiers could be obtained by optimizing fuzzy partitions. There are three approaches. One is pre-processing where inhomogeneous fuzzy partitions are specified according to the class distribution before applying EC. Another approach is to simultaneously optimize both a set of fuzzy if-then rules and fuzzy partitions by EC. The other is post-processing where fuzzy partitions used in the obtained classifier are optimized afterward. In this paper, we examine the effect of post-processing where fuzzy partition optimization is applied to the obtained classifier by GFS. In computational experiments, we first use our hybrid fuzzy genetics-based machine learning with homogeneous fuzzy partitions for standard data sets and its parallel distributed implementation for large data sets. Then we apply genetic lateral tuning as post-processing to shift the positions of membership functions according to the pattern distribution.","Genetics,
Tuning,
Distributed databases,
Training,
Sociology,
Statistics,
Standards"
Machine-Learning-Based Hotspot Detection Using Topological Classification and Critical Feature Extraction,"Because of the widening sub-wavelength lithography gap in advanced fabrication technology, lithography hotspot detection has become an essential task in design for manufacturability. Unlike current state-of-the-art works, which unite pattern matching and machine-learning engines, we fully exploit the strengths of machine learning using novel techniques. By combing topological classification and critical feature extraction, our hotspot detection framework achieves very high accuracy. Furthermore, to speed-up the evaluation, we verify only possible layout clips instead of full-layout scanning. We utilize feedback learning and present redundant clip removal to reduce the false alarm. Experimental results show that the proposed framework is very accurate and demonstrates a rapid training convergence. Moreover, our framework outperforms the 2012 CAD contest at International Conference on ComputerAided Design (ICCAD) winner on accuracy and false alarm.","Feature extraction,
Kernel,
Support vector machines,
Training,
Layout,
Accuracy,
Topology"
Machine learning nuclear detonation features,"Nuclear explosion yield estimation equations based on a 3D model of the explosion volume will have a lower uncertainty than radius based estimation. To accurately collect data for a volume model of atmospheric explosions requires building a 3D representation from 2D images. The majority of 3D reconstruction algorithms use the SIFT (scale-invariant feature transform) feature detection algorithm which works best on feature-rich objects with continuous angular collections. These assumptions are different from the archive of nuclear explosions that have only 3 points of view. This paper reduces 300 dimensions derived from an image based on Fourier analysis and five edge detection algorithms to a manageable number to detect hotspots that may be used to correlate videos of different viewpoints for 3D reconstruction. Furthermore, experiments test whether histogram equalization improves detection of these features using four kernel sizes passed over these features. Dimension reduction using principal components analysis (PCA), forward subset selection, ReliefF, and FCBF (Fast Correlation-Based Filter) are combined with a Mahalanobis distance classifiers to find the best combination of dimensions, kernel size, and filtering to detect the hotspots. Results indicate that hotspots can be detected with hit rates of 90% and false alarms i 1%.","Feature extraction,
Kernel,
Principal component analysis,
Explosions,
Three-dimensional displays,
Detectors,
Atmospheric modeling"
Importance of Extreme Learning Machine in the field of Query classification: A novel approach,"The expandable and dynamic web which is a huge repository for information is growing at lightning speed and hence it is hard to find the relevant information from the web. Efficient algorithms reduce the burden of search engines up to a great extent. Query classification is one such aspect and thus a valuable asset for a search engine. Everyday millions of web queries are posted on the web. The main aim of the query classification is to classify web users' queries into a set of predefined categories. Classifying users' queries greatly reduces the number of documents to be searched and hence is a vibrant area of research. In this paper, we propose a new technique to classify queries using Extreme Learning Machines (ELM). ELM is becoming increasingly popular among researchers owing to its fast training speed and ease of implementation. We evaluate our technique on three large datasets and compare with other relevant machine learning algorithms. Results show that proposed technique works well for classifying the queries which demonstrate the accuracy and efficiency of our system.","Accuracy,
Support vector machines,
Search engines,
Training,
Training data,
Vectors,
Kernel"
Predictive Analytics of Sensor Data Using Distributed Machine Learning Techniques,"This work is based on a real-life data-set collected from sensors that monitor drilling processes and equipment in an oil and gas company. The sensor data stream-in at an interval of one second, which is equivalent to 86400 rows of data per day. After studying state-of-the-art Big Data analytics tools including Mahout, RHadoop and Spark, we chose Ox data's H2O for this particular problem because of its fast in-memory processing, strong machine learning engine, and ease of use. Accurate predictive analytics of big sensor data can be used to estimate missed values, or to replace incorrect readings due malfunctioning sensors or broken communication channel. It can also be used to anticipate situations that help in various decision makings, including maintenance planning and operation.","Predictive models,
Water,
Computational modeling,
Analytical models,
Big data,
Machine learning algorithms,
Data models"
Cloud antivirus cost model using machine learning,"An important cloud computing is a new generation of computing and is based on virtualization technology. More and more applications are being deployed in cloud environments. Malware detection or antivirus software has been recently provided as a service in the cloud. A cloud antivirus provider hosts a number of virtual machines each running the same or different antivirus engines on potentially different sets of workloads (files). From the provider's perspective, the problem of optimally allocating physical resources to these virtual machines is crucial to the efficiency of the infrastructure. We propose a search-based optimization approach for solving the resource allocation problem in cloud-based antivirus deployments. An elaborate cost model of the file scanning process in antivirus programs is instrumental to the proposed approach. The general architecture is presented and discussed, and a preliminary experimental investigation into the antivirus cost model is described. The cost model depends on many factors, such as total file size, size of code segment, and count and type of embedded files within the executable. However, not a single parameter of these can be reliably used alone to predict file scanning time. Thus, a machine-learning approach that combines all these parameters as features is used to build a classifier for antivirus file scanning time. The best results we obtained were using the Decision Tree classifier. The highest F-measure value was 0.91, the highest F-measure value using logitboost was 0.87, the highest F-measure value using support vector machine was 0.85 and the highest F-measure value using naïve Bayes was 0.82. We evaluated the accuracy of the classification model versus linear regression model using the Root Mean Square (RMS) measure. We found that the classification model is more accurate than linear regression model, whereas the values average of RMS were 0.988 second and 2.44 second for classification model and linear regression model, respectively.",Decision support systems
A machine learning based approach for gesture recognition from inertial measurements,"The interaction based on gestures has become a prominent approach to interact with electronic devices. In this paper a Machine Learning (ML) based approach to gesture recognition (GR) is illustrated; the proposed tool is freestanding from user, device and device orientation. The tool has been tested on a heterogeneous dataset representative of a typical application of gesture recognition. In the present work two novel ML algorithms based on Sparse Bayesian Learning are tested versus other classification approaches already employed in literature (Support Vector Machine, Relevance Vector Machine, k-Nearest Neighbor, Discriminant Analysis). A second element of novelty is represented by a Principal Component Analysis-based approach, called Pre-PCA, that is shown to enhance gesture recognition with heterogeneous working conditions. Feature extraction techniques are also investigated: a Principal Component Analysis based approach is compared to Frame-Based Description methods.","Support vector machines,
Feature extraction,
Principal component analysis,
Gesture recognition,
Algorithm design and analysis,
Kernel,
Bayes methods"
Automatic Resource Provisioning: A Machine Learning Based Proactive Approach,This paper concerns dynamic provisioning of cloud resources performed by an intermediary enterprise that provides a private cloud (also referred to as a virtual private cloud) for a single client enterprise using resources acquired on demand from a public cloud. A new proactive technique for auto-scaling of resources that changes the number of resources for the private cloud dynamically based on system load is proposed. The technique that supports both on-demand and advance reservation requests uses machine learning to predict future workload based on past workload. Experimental results demonstrate that the proposed technique can effectively lead to a profit for the intermediary enterprise as well as a reduction of cost for the client enterprise.,
Wavelet denoising: Comparative analysis and optimization using machine learning,"Even after a phenomenal progress in the quality of image denoising algorithms over the years, there is yet a vast scope of improving the standard of denoised images. This paper presents a new methodology for denoising by integrating the wavelet denoising technique with regression boosted trees. Based on ensemble learning by regression boosted trees, an optimal threshold value is obtained. Its denoising performance is better than Stein's unbiased risk estimator-linear expansion of thresholds (SURE-LET) method which is an up to date denoising algorithm. We have also compared its performance with the other current state of art wavelet based denoising algorithms like ProbShrink, and BiShrink on the basis of their Peak Signal to Noise Ratio (PSNR). Simulations and experimentation results demonstrate that PSNR of our proposed method outperforms the other methods. Extension to Dual Tree-Complex Wavelet Transform (DT-CWT) is also presented.","Noise reduction,
Discrete wavelet transforms,
PSNR,
Regression tree analysis"
A machine learning approach for dynamic spectrum access radio identification,"Dynamic spectrum access (DSA) technologies offer solutions to the spectral crowding associated with static frequency allocation. Hierarchical DSA networks aim at allowing secondary users to efficiently utilize licensed spectrum, while still protecting primary users and ensuring them first priority to spectrum access. However, these networks are often multi-tiered and the concept of different operating policies for secondary users has arisen. In this study we consider the idea of two operating modes in a stochastically modeled DSA network. Observations from the radio frequency (RF) environment are classified using self organizing maps (SOMs). The discretized observations are then utilized to develop hidden Markov models (HMMs) of each type of radio. These models are developed for a variable number of map sizes and hidden states then sequence matched against unknown radios in order to determine identification performance. The system is shown to perform extremely well for certain combinations of SOM sizes and HMM states.","Hidden Markov models,
Radio frequency,
Training,
Vectors,
Cognitive radio,
Neurons,
Sensors"
Modeling the steel case carburizing quenching process using statistical and machine learning techniques,"Simulation of various manufacturing processes such as heat treatments is rapidly gaining importance in the industry for process optimization, enhancing efficiency and improving product quality. Case carburization followed by quenching is one such significant heat treatment process commonly used in the automotive industry. The equations to be solved for simulation of these processes are non-linear differential equations and require the use of computationally intensive numerical techniques e.g. 3D Finite Element Modelling. Using these models for solving optimization or inverse problems, compounded by the fact that a large number of evaluations need to be carried out becomes computationally expensive. This necessitates a simpler, computationally inexpensive representation of the process, albeit being applicable to a limited range of process parameters and conditions. In this paper, we explore the use of proven statistical techniques such as Linear Regression and machine learning techniques such as Artificial Neural Networks and Genetic Programming to create computationally inexpensive surrogate models of the carburization quenching processes to predict surface hardness and their results are presented.","Mathematical model,
Carbon,
Surface treatment,
Artificial neural networks,
Predictive models,
Data models,
Equations"
A Knowledge Growth and Consolidation Framework for Lifelong Machine Learning Systems,"A more effective vision of machine learning systems entails tools that are able to improve task after task and to reuse the patterns and knowledge that are acquired previously for future tasks. This incremental, long-life view of machine learning goes beyond most of state-of-the-art machine learning techniques that learn throw-away models. In this paper we present a long-life knowledge acquisition, evaluation and consolidation framework that is designed to work with any rule-based machine learning or inductive inference engine and integrate it into a long-life learner. In order to do that we work over the graph of working memory rules and introduce several topological metrics over it from which we derive an oblivion criterion to drop useless rules from working memory and a consolidation process to promote the rules to the knowledge base. We evaluate the framework on a series of tasks in a chess rule learning domain.","Engines,
Measurement,
Knowledge based systems,
Learning systems,
Law,
Optimized production technology"
A Comparison of Supervised Machine Learning Techniques for Predicting Short-Term In-Hospital Length of Stay among Diabetic Patients,"Diabetes is a life-altering medical condition that affects millions of people and results in many hospitalizations per year. Consequently, predicting the length of stay of in-hospital diabetic patients has become increasingly important for staffing and resource planning. Although statistical methods have been used to predict length of stay in hospitalized patients, many powerful machine learning techniques have not yet been explored. In this paper, we compare and discuss the performance of various supervised machine learning algorithms (i.e., Multiple linear regression, support vector machines, multi-task learning, and random forests) for predicting long versus short-term length of stay of hospitalized diabetic patients.","Support vector machines,
Diabetes,
Radio frequency,
Hospitals,
Machine learning algorithms,
Linear regression,
Databases"
Communication requirement for distributed statistical machine learning with application in waveform cognition,"Distributed learning is an effective approach to mitigate the data communications in machine learning when the data is stored in a distributed manner, particularly in the era of big data. In the distributed learning procedure, learners can send intermediate computation results instead of raw data, thus reducing the communication cost. In this paper, the communication requirement for distributed learning is studied in the scenario of multiple data storage nodes having the capability of learning and a fusion center. Lower bounds for communications are derived based on VC-entropy of modeling in the machine learning. Numerical results are provided to show the communication requirement for typical learning problems.","Entropy,
Big data,
Statistical learning,
Distributed databases,
Signal processing,
Cognitive radio,
Data mining"
Machine Learning for Detecting Brute Force Attacks at the Network Level,"The tremendous growth in computer network and Internet usage, combined with the growing number of attacks makes network security a topic of serious concern. One of the most prevalent network attacks that can threaten computers connected to the network is brute force attack. In this work we investigate the use of machine learners for detecting brute force attacks (on the SSH protocol) at the network level. We base our approach on applying machine learning algorithms on a newly generated dataset based upon network flow data collected at the network level. Applying detection at the network level makes the detection approach more scalable. It also provides protection for the hosts who do not have their own protection. The new dataset consists of real-world network data collected from a production network. We use four different classifiers to build brute force attack detection models. The use of different classifiers facilitates a relatively comprehensive study on the effectiveness of machine learners in the detection of brute force attack on the SSH protocol at the network level. Empirical results show that the machine learners were quite successful in detecting the brute force attacks with a high detection rate and low false alarms. We also investigate the effectiveness of using ports as features during the learning process. We provide a detailed analysis of how the models built can change as a result of including or excluding port features.","Force,
Ports (Computers),
Internet,
Feature extraction,
Protocols,
Data models"
Performance comparison of Machine Learning Algorithms for diagnosis of Cardiotocograms with class inequality,The objective of the present paper is to demonstrate the potential of Computational Intelligence in applications pertaining to the automatic identification - categorisation of Cardiotocograms using Machine Learning Algorithms and Artificial Neural Networks whose purpose is to distinguish between healthy or pathological cases leading to mortality during birth or fetal cerebral palsy. Interest is also placed on the performance of the Machine learning algorithms and the comparison of the classifiers' results.,"Accuracy,
Machine learning algorithms,
Classification algorithms,
Fetal heart rate,
Training,
Embryo,
Educational institutions"
Automated Scoring of the Level of Integrative Complexity from Text Using Machine Learning,"Integrative complexity is a construct developed in political psychology and clinical psychology to measure an individual's ability to consider different perspectives on a particular issue and reach a justifiable conclusion after consideration of said perspectives. Integrative complexity (IC) is usually determined from text through manual scoring, which is time-consuming, laborious and expensive. Consequently, there is a demand for automating the scoring, which could significantly reduce the time, expense and cognitive resources spent in the process. Any algorithm that could achieve the above with a reasonable accuracy could assist in the development of intervention systems for reducing the potential for aggression, systems for recruitment processes and even training personnel for improving group disparity in the corporate world. In this study we used machine learning to predict IC levels from text. We achieved over 78% accuracy in a three way classification.","Integrated circuits,
Complexity theory,
Logistics,
Accuracy,
Classification algorithms,
Manuals,
Support vector machines"
Implementation of Machine Learning for Classifying Hemiplegic Gait Disparity through Use of a Force Plate,"The synergy of gait analysis tools with machine learning enables the capacity to classify disparity existing in hemiplegic gait. Hemiplegic gait is characterized by an affected leg and unaffected leg, which can be quantified by the measurement of a force plate. The characteristic features of the force plate recording for gait consist of a two local maxima that represent the braking phase and push off phase of stance and their associated parameters. The quantified features of a hemiplegic pair of affected leg and unaffected leg force plate recordings are intuitively disparate. Logistic regression achieves 100% classification between an affected and unaffected hemiplegic leg pair based on the feature set of the force plate data.","Force,
Legged locomotion,
Medical treatment,
Logistics,
Foot,
Accuracy,
Biomechanics"
Kernel methods and machine learning techniques for man-made object classification in SAR images,The image processing techniques with computer automated object recognization is an emerging area of research in several engineering and biomédical applications. The images created by Synthetic Aperture Radar (SAR) require complex image processing for intelligence extraction. A technique for man made object recognization in SAR created images is presented here. The kernel methods along with machine learning algorithms are investigated in this paper. The kernel methods allow efficient mapping from non-linear to linear feature space and integrate with several existing linear pattern matching techniques. The image's spatial characteristics are used as data for kernel functions. With MATLAB simulation results the kernel based man-made object classification is verified for different sizes of data sets under different conditions.,"Synthetic aperture radar,
Kernel,
Classification algorithms,
Image edge detection,
Educational institutions,
Machine learning algorithms"
Intrusion detection in mobile AdHoc networks using machine learning approach,"Mobile ad hoc networking (MANET) has become a key technology in recent years because of the increased usage of wireless devices and their ability to provide temporary and instant wireless networking in situations like flooding and defense. In spite of their attractive applications, MANET poses high security problems compared to conventional wired and wireless networks due to its unique characteristics such as lack of central coordination, dynamic topology, temporary network life and wireless nature of communication. Attack prevention measures, such as the use of encryption and authentication techniques, have been proposed as a first line of defense to reduce the risk of security problems. However such risks cannot be completely eliminated, there is a strong need of intrusion detection systems (IDS) as a second line of defense for securing MANET. Intrusion detection on MANET is a challenging task due to its unique characteristics such as open medium, dynamic topology, lack of centralized management and highly resource constrained nodes. Conventional intrusion detection system developed for wired networks cannot be directly applied to MANET. It needs to be redesigned to suit the ad hoc technology. The proposed system introduces new architecture that uses machine learning approach to maximize the detection accuracy. Proposed IDS architecture uses the combination of Rough Set Theory (RST) and Support Vector Machine (SVM) to make use of the excellent accuracy of SVM and better performance of RST.","Support vector machines,
Mobile ad hoc networks,
Intrusion detection,
Approximation methods,
Set theory,
Training"
Modeling Semantic Heterogeneity in Dataspace: A Machine Learning Approach,"A data space system facilitates a new way for sharing and integrating the information among the various distributed, autonomous and heterogeneous data sources. To provide the best effort answer of a user query, a data space system needs to resolve the semantic heterogeneity in its core. There are many solutions being proposed to address this problem widely. We are exploring the problem of semantic heterogeneity in a data space system as a part of our PhD work. In this paper, we have addressed the semantic heterogeneity in the context of a data space system, and presented an abstract framework to model the semantic heterogeneity in data space. The proposed model is based on machine learning and ontology approaches. The machine learning technique analyzes the semantically equivalent data items (or entities) in data space, and the ontology conceptualizes the structural entities in a data space. This model resolves the semantic heterogeneity of a data space system, and creates a conceptual model using ""from-data-to-schema"" approach. The proposed approach implicitly creates the domain ontology by finding the most similar concepts comming from different data sources and enriches the performance of the system by finding the semantic relationships among them.","Semantics,
Ontologies,
Data models,
Distributed databases,
Data mining,
Prototypes"
Automatic radiography image orientation using machine learning,"Mobile digital radiography receptors, known as flat panels, apart from numerous advantages create an issue of proper image orientation. Common orientation of anatomical structures in radiography images is vital in reducing pre-diagnostic processing times. Various features and machine learning methods for determining current orientation of an image are examined with the aim of determining appropriate rotation of radiographic hand images. Obtained results are analyzed and further research directions are proposed.","Support vector machines,
Image coding,
Continuous wavelet transforms"
A Machine Learning Approach to Combining Individual Strength and Team Features for Team Recommendation,"In IT strategic outsourcing businesses, it is critical to have competent deal teams design competitive service solutions and swiftly respond to clients' requests for proposals. In this paper we present a general team recommendation framework for finding the best deal teams to pursue such engagement opportunities. Little previous work on team recommendations considers both individual and team-level features at the same time. Our proposed framework can take into account diverse individual and team features, and accommodate various cost or feature functions. We introduce a team quality metric based on a weighted linear combination of these features, the weights of which are learned using a machine learning approach by leveraging historical project outcomes. A combinatorial optimization algorithm is finally applied to search the possible solution space for the approximate best team. We report a preliminary evaluation of our framework by applying it to real-world data from strategic outsourcing businesses at a large IT service company. We also compare our approach with other existing work by using the public DBLP dataset for recommending teams in academic paper authoring.","History,
Feature extraction,
Outsourcing,
Companies,
Collaboration,
Approximation algorithms"
Leveraging Machine Learning Algorithms to Perform Online and Offline Highway Traffic Flow Predictions,"Advanced traffic management systems (ATMS) are heavily depending on traffic flow or equivalent travel time estimation. The main goal of this paper is to accomplish two different algorithms to perform offline and online traffic flow forecasting. A multi-layer perceptron (MLP), which is trained on yearly data, is utilized for mid-term offline predictions. Principal components analysis (PCA) is employed to speed up the training process. This model also serves as a baseline. The stochastic gradient descent deploys online forecasting. Both algorithms predict the flow of a location down a Trunk highway (the target point) using the history of flow of several locations ahead of the target point in Twin Cities Metro area in Minneapolis.","Training,
Principal component analysis,
Estimation,
Prediction algorithms,
History,
Road transportation,
Real-time systems"
Combining Exact and Metaheuristic Techniques for Learning Extended Finite-State Machines from Test Scenarios and Temporal Properties,This paper addresses the problem of learning extended finite-state machines (EFSMs) from user-specified behavior examples (test scenarios) and temporal properties. We show how to combine exact EFSM inference algorithms (that always find a solution if it exists) and metaheuristics to derive an efficient combined EFSM learning algorithm. We also present a new exact EFSM inference algorithm based on Constraint Satisfaction Problem (CSP) solvers. Experimental results are reported showing that the new combined algorithm significantly outperforms a previously used metaheuristic.,Iron
Equipped search results using machine learning from web databases,"Database driven web pages play a vital role in multiple domains like online shopping, e-education systems, cloud computing and other. Such databases are accessible through HTML forms and user interfaces. They return the result pages come from the underlying databases as per the nature of the user query. Such types of databases are termed as Web Databases (WDB). Web databases have been frequently employed to search the products online for retail industry. They can be private to a retailer/concern or publicly used by a number of retailers. Whenever the user queries these databases using keywords, most of the times the user will be deviated by the search results returned. The reason is no relevance exists between the keyword and SRs (Search Results). A typical web page returned from a WDB has multiple Search Result Records (SRRs). An easier way is to group the similar SRRs into one cluster in such a way the user can be more focused on his demand. The key concept of this paper is XML technologies. In this paper, we propose a novel system called CSR (Clustering Search Results) which extracts the data from the XML database and clusters them based on the similarity and finally assigns meaningful label for it. So, the output of the keyword entered will be the clusters containing related data items.","Databases,
XML,
Data mining,
Servers,
Java,
Educational institutions,
Feature extraction"
Overview of machine learning based side-channel analysis methods,"Recent publications have shown that there is a possibility to apply machine learning methods for side-channel analysis, mostly for profiling based attacks. In this paper, we present a brief overview of those methods, and highlight what are the improvements that might be offered. It is shown that, in most cases, the performance of these methods could outperform the classical attacks. Here, we also discuss what could be the other potential applications of the learning algorithms, for example, as feature selection or for construction of leakage model.","Support vector machines,
Machine learning algorithms,
Computer science,
Cryptography,
Unsupervised learning,
Algorithm design and analysis,
Hidden Markov models"
Machine Learning based Approach for Water pollution detection via fish liver microscopic images analysis,"This article presents an automatic classification approach for assessing water quality based on fish liver histopathology. As fish liver is a good bioindicator for detecting water chemical pollution, the proposed approach utilizes fish liver microscopic images in order to detect water pollution. The proposed approach consists of three phases; namely pre-processing, feature extraction, and classification phases. Since color and texture are the most important characteristics of microscopic fish liver images, the proposed system uses colored histogram and Gabor wavelet transform for classifying water quality degree. Also, it implemented Principal Components Analysis (PCA) along with Support Vector Machines (SVMs) algorithms for feature extraction and water quality degree classification, respectively. Collected datasets contain colored JPEG images of 125 images as training dataset and 45 images as testing dataset, respectively. Training dataset is divided into 4 classes representing the different histopathlogical changes and their corresponding water quality degrees. Experimental results showed that the proposed classification approach has obtained water quality classification accuracy of 93.3%, using SVMs linear kernel function with 37 images per class for training.",
Traffic engineering framework with machine learning based meta-layer in software-defined networks,"Software-defined networks is an emerging architecture that separates the control plane and data plane. This paradigm enables flexible network resource allocations for traffic engineering, which aims to gain better network capacity and improved delay and loss performance. As we know, many heuristic algorithms have been developed to solve the dynamic routing problem. Whereas they lead to a high computational time cost, which results in a crucial problem whether such a heuristic approach to this NP-complete problem is of any use in practice. This paper proposes a framework with supervised machine learning based meta-layer to solve the dynamic routing problem in real time. We construct multiple machine learning modules in meta-layer, whose training set is consist of heuristic algorithm's input and its corresponding output. We show that after training process, the meta-layer will give heuristic-like results directly and independently, substituting for the time-consuming heuristic algorithm. We demonstrate, by analysis and simulation, our framework effectively enhance the network performance. Finally, the meta-layer architecture is quite universal and can be extended in numerous ways to accommodate a variety of traffic engineering scenarios in the network.","Heuristic algorithms,
Routing,
Machine learning algorithms,
Training,
Network topology,
Delays,
Topology"
Implementation of a fast coral detector using a supervised machine learning and Gabor Wavelet feature descriptors,"The task of reef restoration is very challenging for volunteer SCUBA divers, if it has to be carried out at deep sea, 200 meters, and low temperatures. This kind of task can be properly performed by an Autonomous Underwater Vehicle (AUV); able to detect the location of reef areas and approach them. The aim of this study is the development of a vision system for coral detections based on supervised machine learning. In order to achieve this, we use a bank of Gabor Wavelet filters to extract texture feature descriptors, we use learning classifiers, from OpenCV library, to discriminate coral from non-coral reef. We compare: running time, accuracy, specificity and sensitivity of nine different learning classifiers. We select Decision Trees algorithm because it shows the fastest and the most accurate performance. For the evaluation of this system, we use a database of 621 images (developed for this purpose), that represents the coral reef located in Belize: 110 for training the classifiers and 511 for testing the coral detector.","Feature extraction,
Image color analysis,
Machine learning algorithms,
Vectors,
Accuracy,
Decision trees,
Support vector machines"
Handling intrusion and DDoS attacks in Software Defined Networks using machine learning techniques,"Software-Defined Networking (SDN) is an emerging concept that intends to replace traditional networks by breaking vertical integration. It does so by separating the control logic of network from the underlying switches and routers, suggesting logical centralization of network control, and allowing to program the network. Although SDN promises more flexible network management, there are numerous security threats accompanied with its deployment. This paper aims at studying SDN accompanied with OpenFlow protocol from the perspective of intrusion and Distributed Denial of Service (DDoS) attacks and suggest machine learning based techniques for mitigation of such attacks.","Artificial neural networks,
Silicon,
Bayes methods,
Support vector machine classification,
Training,
Genetics,
Classification algorithms"
Towards effective feature selection in machine learning-based botnet detection approaches,"Botnets, as one of the most formidable cyber security threats, are becoming more sophisticated and resistant to detection. In spite of specific behaviors each botnet has, there exist adequate similarities inside each botnet that separate its behavior from benign traffic. Several botnet detection systems have been proposed based on these similarities. However, offering a solution for differentiating botnet traffic (even those using same protocol, e.g. IRC) from normal traffic is not trivial. Extraction of features in either host or network level to model a botnet has been one of the most popular methods in botnet detection. A subset of features, usually selected based on some intuitive understanding of botnets, is used by the machine learning algorithms to classify/ cluster botnet traffic. These approaches, tested against two or three botnet traces, have mostly showed satisfactory detection results. Even though, their effectiveness in detection of other botnets or real traffic remains in doubt. Additionally, effectiveness of different combination of features in terms of providing more detection coverage has not been fully studied. In this paper we revisit flow-based features employed in the existing botnet detection studies and evaluate their relative effectiveness. To ensure a proper evaluation we create a dataset containing a diverse set of botnet traces and background traffic.","Feature extraction,
Protocols,
Accuracy,
Ports (Computers),
IP networks,
Peer-to-peer computing,
Security"
An impact analysis: Real time DDoS attack detection and mitigation using machine learning,"Distributed Denial of service (DDoS) attacks is the most devastating attack which tampers the normal functionality of critical services in internet community. DDoS cyber weapon is highly motivated by several aspects including hactivitism, personal revenge, anti-government force, disgruntled employers/customers, ideological and political cause, cyber espionage and so on. IP spoofing is the powerful technique used by attackers to disrupt the availability of services in the internet network by impersonating as a trusted source. Since the spoofed traffic shares the same resources as that of the legitimate one's detection and filtering becomes very essential. The proposed model consists of online monitoring system (OMS), spoofed traffic detection module and interface based rate limiting (IBRL) algorithm. OMS provides DDoS impact measurements in real time by monitoring the degradation in host and network performance metrics. The spoofed traffic detection module incorporates hop count inspection algorithm (HCF) to check the authenticity of incoming packet by means of source IP address and its corresponding hops to destined victim. HCF coupled with support vector machine (SVM) provides 98.99% accuracy with reduced false positive. Followed with, IBRL algorithm restricts the traffic aggregates at victim router when exceeding system limits in order to provide sufficient bandwidth for remaining flows.","IP networks,
Computer crime,
Filtering,
Limiting,
Aggregates,
Measurement,
Support vector machines"
SQL Injection detection using machine learning,"In the present world, the web is the firmest and most common medium of communication and business interchange. Every day, millions of data are loaded through various channels on the web by users and user input can be malicious. Therefore, security becomes a very important aspect of web applications. Since they are easily accessible, they are prone to many vulnerabilities which if neglected can cause harm. The attackers make use of these loopholes to gain unauthorized access by performing various illegal activities. SQL Injection is one such attack which is easy to perform but difficult to detect because of its varied types and channel. This may result in theft, leak of personal data or loss of property. In this paper we have analyzed the existing solutions to the problems such as AMNESIA [1] and SQLrand [3] and their limitations. We have devised a classifier for detection of SQL Injection attacks. The proposed classifier uses combination of Naïve Bayes machine learning algorithm and Role Based Access Control mechanism for detection. The proposed model is tested based on the test cases derived from the three SQLIA attacks: comments, union and tautology.","Feature extraction,
Security,
Classification algorithms,
Machine learning algorithms,
Pattern matching,
Accuracy,
Instruments"
A hybrid wind speed forecasting strategy based on Hilbert-Huang transform and machine learning algorithms,"Precise wind resource assessment is one of the more imminent challenges. In the present work, we develop an adaptive approach to wind speed forecasting. The approach is based on a combination of the efficient apparatus of non-stationary time series of wind speed retrospective data analysis based on the Hilbert-Huang transform and machine learning models. Models that are examined include neural networks, support vector machines, the regression trees approach: random forest and boosting trees. Evaluation results are presented for the Irish power system based on the Atlantic offshore buoy data.","Wind speed,
Forecasting,
Wind forecasting,
Data models,
Predictive models,
Wind power generation"
Application of machine learning for NonHolonomic mobile robot trajectory controlling,"Mobile robots are very interested by researchers over the last few years because of their applications and physical characteristics. The workspace of mobile robots is not always ideal, but typically filled with disturbances (known or unknown) such as uneven surface terrain, natural friction, uncertainties, and parametric changes. In this study, a new approach namely active force control (AFC) scheme integrating artificial neural network (ANN) has been suggested to cope on the disturbances and thus improve the trajectory tracking characteristic of the system. Therefore, a two wheeled mobile robot has been simulated, and ANN technique is explicitly employed for the estimation of the inertia matrix that is needed in the inner feedback control loop of the AFC scheme. The robustness and efficiency of the identified control scheme are studied considering various forms of loading and operating conditions. For the purpose of benchmarking, the AFC scheme performance has been compared to PID controller.","Mobile robots,
Frequency control,
Trajectory,
Wheels,
Artificial neural networks,
Force control"
An efficient inference in meanfield approximation by adaptive manifold filtering (Machine learning & data mining),"A new method for speeding up the approximate maximum posterior marginal (MPM) inference in meanfield approximation of a fully connected graph is introduced. Weight of graph edges is measured by mixture of Gaussian kernels. This fully connected graph is used for segmentation of image data. The bottleneck of the inference in meanfield approximation is where the similar bilateral filtering is needed for updating the marginal in the message passing step. To speed up the inference, the adaptive manifold high dimensional Gaussian filter is used. As its time complexity is 0(ND), it leads to accelerating the marginal update in the message passing step. Its time complexity is linear and relative to the dimension and number of graph nodes. To improve the accuracy of segmentation, instead of the bilateral filter, the non-local mean filter is used. The proposed inference method is more accurate and needs less computations when compared to other existing methods.","Filtering,
Approximation methods,
Manifolds,
Time complexity,
Lattices,
Message passing,
Acceleration"
A framework for Internet data real-time processing: A machine-learning approach,"Nowadays, the Internet Service Providers have to keep track of and in some cases to analyze for legal issues, a great amount of Internet data. Real-time big data processing and analysis introduce new challenges that must be addressed by system engineers. This is because: 1) traditional technologies exploiting databases are not designed to process a huge amount of data in real-time 2) classic machine learning algorithms implemented by widely adopted tools as Weka or R are not designed to perform “on the fly” analysis on streamed data. In this paper the authors propose an architecture that makes the real-time big data processing and analysis possible. The proposed architecture is based on two main components: a stream processing engine called Apache Storm and a framework called Yahoo SAMOA allowing to perform data analysis through distributed streaming machine learning algorithms. Our architecture is tested for Skype traffic recognition within network traffic generated by several Personal Computers in a streamed way. Experimental results have shown the effectiveness of proposed solution.","Machine learning algorithms,
Real-time systems,
Storms,
Computer architecture,
Training,
Databases,
Engines"
Combined passive radiofrequency identification and machine learning technique to recognize human motion,"Moving limbs within an electromagnetic field radiated by an interrogating antenna will generate a modulation of the backscattered field sensed by a receiver. The measured signals may therefore carry raw information about the human motion. Moreover, the proper placement of UHF passive Radiofrequency Identification (RFID) tags over body segments will increase the amount of collected signals. This paper investigate the potentiality of a possible synergy between Electromagnetics and Machine Learning technology at the purpose to recognize and classify, for the first time, the gestures of arms and legs by using only passive transponders. Electromagnetic signals backscattered from the tags during limb motion are collected by a fixed reader antenna and then processed by the Support Vector Machine (SVM) algorithm. Experimental results demonstrated a degree of accuracy in the classification of periodic movements that is fully comparable with that of more complex systems involving active wearable transponders.","Antennas,
Biomedical monitoring,
Passive RFID tags,
Classification algorithms,
Antenna measurements,
Legged locomotion"
A machine learning approach for Twitter spammers detection,"The ever-increasing popularity of Social Networks offers unprecedented opportunities to aggregate people and exchange information, but, at the same time, opens new modalities for cyber-crime perpetrations. The spamming phenomenon, so spread-out in emails, is now affecting microblogs, and exploits specific mechanisms of the messaging process. The paper proposes an inductive-learning method for the detection of Twitter-spammers, and applies a Random-Forest approach to a limited set of features that are extracted from traffic. Experimental results show that the proposed method outperforms existing approaches to this problem.","Twitter,
Feature extraction,
Training,
Vegetation,
Classification algorithms,
Unsolicited electronic mail"
Machine Learning to Data Fusion Approach for Cooperative Spectrum Sensing,"Cooperative spectrum sensing has been shown to be an effective method to improve the detection performance of the licensed user availability by exploiting spatial diversity. However, cooperation among cognitive radio (CR) users may also introduce a variety of overheads due to the extra sensing time, delay, energy, and operations that limit achievable cooperative gain. In responding to this paper, we propose a machine learning based fusion center algorithm that can provide real time per frame training and decision based cooperative spectrum sensing. The new fusion algorithm based on training a machine learning classifier over a set containing some frame energy test statistics along with their corresponding decisions about the presence or absence of the primary user (PU) transmission, so as to predict the decisions for new frames with new energy test statistics. The simulation and numerical results show that the new approach performs the same as the current fusion rule with less sensing time, delay and operations. In this paper we also present a simulation comparison of four supervised machine learning classifiers: K-nearest neighbor (KNN), support vector machine (SVM), Naive Bayes (NB), and Decision Tree (DT) in classifying 1000 testing frames after training these classifiers over a set containing 1000 frames. It shows that KNN and DT classifier outperform the other two classifiers in the accuracy of classifying the new frames.","Training,
Sensors,
Support vector machines,
Decision trees,
Accuracy,
Data integration,
Noise"
Hybrid improved gravitional search algorithm and kernel based extreme learning machine method for classification problems,"In this paper, we hybridize the improved gravitational search algorithm (IGSA) with kernel based extreme learning machine (KELM) method. Based on this, a novel hybrid system IGSA-KELM is proposed to improve the generalization performance for classification problems. In this system, IGSA is designed by combining the search strategy of particle swarm optimization and GSA to effectively reduce the problem of slow convergence rate, moreover, the continuous-value IGSA and binary IGSA are integrated in one algorithm in order to optimize the KELM parameters and feature subset selection simultaneously. This proposed hybrid algorithm is evaluated on several well-known UCI machine learning datasets. The results indicate that the superiority of the proposed model in terms of classification accuracy. Our hybrid method not only can select the most relevant feature subset, but also achieves a high classification accuracy over other similar state-of-the-art classifier systems.","Accuracy,
Decision support systems,
Testing,
Classification algorithms,
Algorithm design and analysis,
Machine learning algorithms,
Training"
Application of Machine Learning to Algorithm Selection for TSP,"The Travelling Salesman Problem (TSP) has been extensively studied in the literature and various solvers are available. However, none of the state-of-the-art solvers for TSP outperforms the others in all problem instances within a given time limit. Therefore, the prediction of the best performing algorithm can save computational resources and optimise the results. In this paper, the TSP is studied in context of automated algorithm selection. Our aim is to identify the relevant features of problem instances and tackle this scenario as a machine learning task. We extend the set of existing features in the literature and propose several novel features to better characterise the problem. The contribution of the new features is statistically analysed and experiments show that adding our new features improves the prediction accuracy. We identified that our features based on kNN graph transformation are especially helpful. To create the training datasets, two state-of-the-art (meta-) heuristic algorithms are systematically evaluated on more than 2000 problems. Overall, we show that our prediction can be substantially more accurate than simple preference of an algorithm with the best performance for a majority of problem instances.","Prediction algorithms,
Machine learning algorithms,
Feature extraction,
Training,
Algorithm design and analysis,
Runtime,
Clustering algorithms"
Area wise high resolution water availability estimation using heterogeneous remote sensing and ensemble machine learning,"In this paper a novel remote sensing data integration framework has been developed using ensemble machine learning to estimate large area wise ground water balance. Heterogeneous spatio-temporal database including `Australian Water Availability Project (AWAP) database', `Australian Digital Elevation data (ADED)', and `NASA MODIS Vegetation Index (VI) data' were processed and integrated. An irrigated farming area (covering 20km × 20km) in Tasmania described by S 42°36 Latitude and E 147°29 Longitude, where weekly data from the period Jan 2007 - Dec 2013 (total 320 weeks) were studied. An ensemble machine learning framework combining Sugano type Adaptive Neuro Fuzzy Inference System (ANFIS), Elman (ENN), Cascade Feed Forward (CFFNN), and Function fitting neural networks (FFNN) were trained with combined training inputs of VI and ADED demographic data against the AWAP based water balance estimations as training targets. Based on the spatial distribution of the training performance, different trained estimators were selected to estimate water balance at various spatial locations purely based on VI and ADED inputs, where no AWAP data were available. A high-resolution (250m) water availability map was created for the whole area on a weekly temporal scale, which could potentially provide accurate irrigation management support over a very large area.","MODIS,
Mathematical model,
Sensors,
Estimation,
Australia,
Availability,
Spatial resolution"
Salad leaf disease detection using machine learning based hyper spectral sensing,"In this paper a novel application of salad leaf disease detection has been developed using a combination of machine learning algorithms and Hyper Spectral sensing. Various field experiments were conducted to acquire different vegetation reflectance spectrum profiles using a portable high resolution ASD FieldSpec4 Spectroradiometer, at a farm located in Richmond, Tasmania, Australia, (-42.36, 147.29), A total of 105 spectral samples were collected through three different experiments with baby salad leaves. In this study, Principal Component Analysis (PCA), Multi-Statistics Feature ranking and Linear Discriminant Analysis (LDA) Classifiers were used to classify disease affected salad leaves from the healthy salad leaves with 84% classification accuracy. This study concluded that the machine learning based approach along with a high resolution hyper Spectroradiometer could potentially provide a novel mechanism to use in the farm for rapid detection of salad leaf disease.","Diseases,
Principal component analysis,
Spectroradiometers,
Sensors,
Soil,
Variable speed drives,
Absorption"
Android Malware Detection Using Parallel Machine Learning Classifiers,"Mobile malware has continued to grow at an alarming rate despite on-going mitigation efforts. This has been much more prevalent on Android due to being an open platform that is rapidly overtaking other competing platforms in the mobile smart devices market. Recently, a new generation of Android malware families has emerged with advanced evasion capabilities which make them much more difficult to detect using conventional methods. This paper proposes and investigates a parallel machine learning based classification approach for early detection of Android malware. Using real malware samples and benign applications, a composite classification model is developed from parallel combination of heterogeneous classifiers. The empirical evaluation of the model under different combination schemes demonstrates its efficacy and potential to improve detection accuracy. More importantly, by utilizing several classifiers with diverse characteristics, their strengths can be harnessed not only for enhanced Android malware detection but also quicker white box analysis by means of the more interpretable constituent classifiers.","Malware,
Androids,
Humanoid robots,
Feature extraction,
Classification algorithms,
Training,
Accuracy"
A Machine Learning Approach to Detection of Core Region of Online Handwritten Bangla Word Samples,"Core region detection of handwritten cursive words is an important step towards their automatic recognition. Several preprocessing operations such as height normalization, slant estimation etc. Are often based on this core region. This is particularly useful for word recognition of major Indian scripts, which have large character sets. The main parts of majority of these characters belong to the core region that is bounded above by a headline and bounded below by an imaginary base line. Only a few such characters or their parts appear either above or below the core region. A few approaches are available in the literature for detection of such a core region of offline handwritten word samples of Latin script. Also, a similar region is often determined for recognition of images of printed Indian scripts. However, none of these approaches have studied detection of core region of an unconstrained online handwritten word. In this article, we propose a novel method for detection of the core region of online handwritten word samples of Bangla, a major Indian script. For this we first perform smoothing on the samples and then segment a stroke into sub strokes. We compute certain novel positional features from each such sub stroke. Using these features, a multilayer perceptron (MLP) is trained by back propagation (BP) algorithm. On the basis of the output of the MLP, we determine the position of both the headline and the baseline. We have tested this approach on a recently developed large database of online unconstrained handwriting Bangla word samples. The proposed approach would also work on similar samples of Devanagari, another major Indian script. Experimental results are encouraging.","Vectors,
Training,
Feature extraction,
Shape,
Databases,
Compounds,
Histograms"
Sentiment Categorization on a Creole Language with Lexicon-Based and Machine Learning Techniques,"We propose polarity detection from colloquial expressions distinctive of a bilingual population. The hybrid language we address it's called ""Jopara"", composed by Spanish and Guaraní, spoken in Paraguay, similar to the ""Louisiana's Creole"" in the United States. We categorize polarity in three classes (positive, negative and neutral) and address this problem by applying both lexicon-based and machine-learning approaches. In this document it's shown the application scenario, the building process of the bilingual lexicon and the attributes preprocessing to create the classifiers' input. The input data is retrieved from Twitter so the expressions are similar to natural language. Finally, results are displayed to compare performance of these techniques when applied on this kind of language. It's shown that classical classifiers have very good performances, with correction rates of over 80% even with small training sets, if their parameters are properly adjusted along with an adequate selection of attributes.","Kernel,
Support vector machine classification,
Training,
Classification algorithms,
Companies,
Sentiment analysis"
Sentiment Classification at the Time of the Tunisian Uprising: Machine Learning Techniques Applied to a New Corpus for Arabic Language,"Sentiment analysis is the field of study that analyzes people's opinions, sentiments, attitudes, and emotions from written language. It is one of the most active research areas in natural language processing and is also widely studied in data mining, web mining, and text mining. In recent years, text mining and sentiment analysis are being in almost every business and social domain which study all human activities and key influencers of our behaviors. Even though there are, at present, several studies related to this theme, most of them focus mainly on English texts. The resources available for opinion mining in other languages, such as Arabic, are still limited. In this paper, we propose a new sentiment analysis system destined to classify users' opinions which is performed with a new corpus for Arabic language gathered from users' posts at the time of the Tunisian revolution. Furthermore, different experiments have been carried out on this corpus, using machine learning algorithms such as Support Vector Machines and Naïve Bayes.","Sentiment analysis,
Facebook,
Testing,
Text mining,
Support vector machines"
Machine learning model for aircraft performances,"This paper presents new idea how trajectory calculations could be improved in order to match real flights better. Exact trajectory calculation is important for future of air traffic control, because it is one of the enablers for safe traffic increase. Methods used to calculate trajectories are based on aircraft types and their performances mainly. However, we believe that there are many other influencing factors which should be taken into account. We collect available data about flights and store them into a multi-dimensional database. Knowledge accumulated in this database is the basis for aircraft performances prediction using machine learning methods. In that way the prediction is not based on aircraft type alone, but also on other attributes like aerodrome of departure, destination and operator. There attributes indirectly imply to procedures, operator's best practices, local airspace characteristics, etc. and enable us to make better predictions of aircraft performances. Predictions in this case are not static but tailored to every particular flight.","Aircraft,
Atmospheric modeling,
Databases,
Trajectory,
Air traffic control,
Radar tracking"
Putting the Scientist in the Loop -- Accelerating Scientific Progress with Interactive Machine Learning,"Technology drives advances in science. Giving scientists access to more powerful tools for collecting and understanding data enables them to both ask and answer new kinds questions that were previously beyond their reach. Of these new tools at their disposal, machine learning offers the opportunity to understand and analyze data at unprecedented scales and levels of detail. The standard machine learning pipeline consists of data labeling, feature extraction, training, and evaluation. However, without expert machine learning knowledge, it is difficult for scientists to optimally construct this pipeline to fully leverage machine learning in their work. Using ecology as a motivating example, we analyze a typical scientist's data collection and processing workflow and highlight many problems facing practitioners when attempting to capitalize on advances in machine learning and pattern recognition. Understanding these shortcomings allows us to outline several novel and underexplored research directions. We end with recommendations to motivate progress in future cross-disciplinary work.","Pipelines,
Data models,
Labeling,
Training,
Educational institutions,
Feature extraction,
Biological system modeling"
Feature Mining for Machine Learning Based Compilation Optimization,"Compilation optimization is critical for software performance. Before a product releases, the most effective algorithm combination should be chosen to minimize the object file size or to maximize the running speed. Compilers like GCC usually have hundreds of optimization algorithms, in which they have complex relationships. Different combinations of algorithms will lead to object files with different performance. Usually developers select the combination manually, but it's unpractical since a combination for one project can't be reused for another one. In order to conquer this difficulty some approaches like iterative search, heuristic search and machine learning based optimization have been proposed. However these methods still need improvements at different aspects like speed and precision. This paper researches machine learning based compilation optimization especially on feature processing which is important for machine learning methods. Program features can be divided into static features and dynamic features. Apart from user defined static features, we design a method to generate lots of static features by template and select best ones from them. Furthermore, we observe that feature value changes during different optimization phases and implement a feature extractor to extract feature values at specific phases and predict optimization plan dynamically. Finally, we implement the prototype on GCC version 4.6 with GCC plug in system and evaluate it with benchmarks. The results show that our system has a 5% average speed up for object file running speed than GCC O3 optimization level.","Feature extraction,
Optimization,
Training,
Vectors,
Predictive models,
Benchmark testing,
Computational modeling"
Features in Identification Approaches for MicroRNA Precursors Based on Machine Learning,"MicroRNAs (miRNAs) are a group of non-coding small RNA of ~ 22 nucleotides in length. They play important roles in gene regulation in animals and plants. The machine learning approach has become an important way to discover miRNAs, which is complement to experimental approaches. Feature selection is the key step of machine learning approaches to discover miRNA precursors. The performance and generalization ability of classifier is affected by the feature set. Features of miRNA precursors used in machine learning approaches were summarized in this review. According to the properties of features to distinguish the miRNA precursors and the non-miRNA precursors, features were categorized into three classes: sequence features, structure features, structure sequence features.","Feature extraction,
Bioinformatics,
Genomics,
RNA,
Sequential analysis,
Support vector machines,
Periodic structures"
A Machine Learning Based Method for Staff Removal,"Staff line removal is an important pre-processing step to convert content of music score images to machine readable formats. Many heuristic algorithms have been proposed for staff removal and recently a competition was organized in the 2013 ICDAR/GREC conference. Music score images are often subject to different deformations and variations, and existing algorithms do not work well for all cases. We investigate the application of a machine learning based method for the staff removal problem. The method consists in learning multiple image operators from training input-output pairs of images and then combining the results of these operators. Each operator is based on local information provided by a neighborhood window, which is usually manually chosen based on the content of the images. We propose a feature selection based approach for automatically defining the windows and also for combining the operators. The performance of the proposed method is superior to several existing methods and is comparable to the best method in the competition.","Training,
Accuracy,
Algorithm design and analysis,
Prototypes,
Machine learning algorithms,
Three-dimensional displays,
Learning systems"
"An Augmented Lagrangian Method for l2,1-Norm Minimization Problems in Machine Learning","In the fields of computer version, text classification and biomedical informatics, it needs to find the joint feature among serval learning tasks. Generally, resent results show that it can be realized by solving a ℓ2,1-norm minimization problem. However, due to the non-smoothness of the norm, solving the resulting optimization problem is always challenging. This thesis designs an augmented Lagrange function method which is used to solve ℓ2,1-norm minimization problem. In this thesis the convergence property of the algorithm is discussed. The numerical experiments indicate that the convergence of this algorithm is easily followed and the algorithm's executing efficiency is very good.","Algorithm design and analysis,
Lagrangian functions,
Convergence,
Machine learning algorithms,
Training,
Minimization,
Joints"
Detection of Tumor Cell Spheroids from Co-cultures Using Phase Contrast Images and Machine Learning Approach,"Automated image analysis is demanded in cell biology and drug development research. The type of microscopy is one of the considerations in the trade-offs between experimental setup, image acquisition speed, molecular labelling, resolution and quality of images. In many cases, phase contrast imaging gets higher weights in this optimization. And it comes at the price of reduced image quality in imaging 3D cell cultures. For such data, the existing state-of-the-art computer vision methods perform poorly in segmenting specific cell type. Low SNR, clutter and occlusions are basic challenges for blind segmentation approaches. In this study we propose an automated method, based on a learning framework, for detecting particular cell type in cluttered 2D phase contrast images of 3D cell cultures that overcomes those challenges. It depends on local features defined over super pixels. The method learns appearance based features, statistical features, textural features and their combinations. Also, the importance of each feature is measured by employing Random Forest classifier. Experiments show that our approach does not depend on training data and the parameters.","Image segmentation,
Feature extraction,
Tumors,
Histograms,
Imaging,
Three-dimensional displays,
Training"
Machine-Learning-Based Identification of Defect Patterns in Semiconductor Wafer Maps: An Overview and Proposal,"Wafers are formed from very thin layers of a semiconductor material, hence, they are highly susceptible to various kinds of defects. The defects are most likely to occur during the lengthy and complex fabrication process, which can include hundreds of steps. Wafer defects are generally caused by machine inaccuracy, chemical stains, physical damages, human mistakes, and atmospheric conditions. The defective chips tend to have several unique spatial patterns across the wafer, namely ring, spot, repetitive and cluster patterns. To locate such defect patterns, wafer maps are used to visualize and ultimately lead to better understanding of what happened during the process failure. To identify the unique patterns of defects and to find the point of manufacturing process that causes such defects accurately, nature-inspired model-free machine-learning techniques have been well accepted. This paper thus reviews the theoretical and experimental literature of such models with a focus on model learnability and efficiency-related issues involving data reduction and transformation techniques, which could be seen as the key model properties to deal with big data applications.","Classification algorithms,
Semiconductor device modeling,
Feature extraction,
Accuracy,
Pattern matching,
Support vector machines,
Vectors"
A combination forecasting model using machine learning and Kalman filter for statistical arbitrage,In this paper we evaluate the combination of Extreme Learning Machine (ELM) and Support Vector Regression (SVR) with a Kalman filter regression model for financial time series forecasting. We also compare the forecast performance with a set of linear regression combination methods. The application of the traditional Kalman Filter for the statistical arbitrage strategy improves the statistical performance of ELM and SVR individual forecasts. The accuracy of the models is statistically tested and an investigation is performed to confirm the impact of the forecasts combination in terms of annualized returns and volatility.,"Kalman filters,
Predictive models,
Training,
Time series analysis,
Forecasting,
Support vector machines,
Mathematical model"
Histogram-based classification of iPSC colony images using machine learning methods,"This paper focuses on induced pluripotent stem cell (iPSC) colony image classification using machine learning methods and different feature sets obtained from the intensity histograms. Intensity histograms are obtained from the whole iPSC colony images and as a baseline for it they are determined only from the iPSC colony area of images. Furthermore, we apply to both of the datasets two simple feature selection methods having altogether four datasets. Altogether, 30 different classification methods are tested and we perform thorough experimental tests. The best accuracy (55%) is obtained for the feature set evaluated from the whole image using Directed Acyclic Graph Support Vector Machines (DAGSVM). DAGSVM is also the best choice when intensity histograms are evaluated only from the iPSC colony area. By this means accuracy of 54% is achieved. The obtained results are promising for further research where, for instance, more sophisticated feature selection and extraction methods and other multi-class extensions of SVM will be examined. However, intensity histograms are not alone adequate for iPSC colony image classification.","Kernel,
Accuracy,
Polynomials,
Support vector machines,
Weight measurement,
Histograms,
Correlation"
Portfolio-Based Selection of Robust Dynamic Loop Scheduling Algorithms Using Machine Learning,"The execution of computationally intensive parallel applications in heterogeneous environments, where the quality and quantity of computing resources available to a single user continuously change, often leads to irregular behavior, in general due to variations of algorithmic and systemic nature. To improve the performance of scientific applications, loop scheduling algorithms are often employed for load balancing of their parallel loops. However, it is a challenge to select the most robust scheduling algorithms for guaranteeing optimized performance of scientific applications on large-scale computing systems that comprise resources which are widely distributed, highly heterogeneous, often shared among multiple users, and have computing availabilities that cannot always be guaranteed or predicted. To address this challenge, in this work we focus on a portfolio-based approach to enable the dynamic selection and use of the most robust dynamic loop scheduling (DLS) algorithm from a portfolio of DLS algorithms, depending on the given application and current system characteristics including workload conditions. Thus, in this paper we provide a solution to the algorithm selection problem and experimentally evaluate its quality. We propose the use of supervised machine learning techniques to build empirical robustness prediction models that are used to predict DLS algorithm's robustness for given scientific application characteristics and system availabilities. Using simulated scientific applications characteristics and system availabilities, along with empirical robustness prediction models, we show that the proposed portfolio-based approach enables the selection of the most robust DLS algorithm that satisfies a user-specified tolerance on the given application's performance obtained in the particular computing system with a certain variable availability. We also show that the portfolio-based approach offers higher guarantees regarding the robust performance of the application using the automatically selected DLS algorithms when compared to the robust performance of the same application using a manually selected DLS algorithm.","Robustness,
Heuristic algorithms,
Prediction algorithms,
Program processors,
Availability,
Predictive models,
Dynamic scheduling"
Predicting GPU Performance from CPU Runs Using Machine Learning,"Graphics processing units (GPUs) can deliver considerable performance gains over general purpose processors. However, GPU performance improvement vary considerably across applications. Porting applications to GPUs by rewriting code with GPU-specific languages requires significant effort. In consequence, it is desirable to predict which applications would benefit most before porting to the GPU. This paper shows that machine learning techniques can build accurate predictive models for GPU acceleration. This study presents an approach which applies supervised learning algorithms to infer predictive models, based on dynamic profile data collected via instrumented runs on general purpose processors. For a set of 18 parallel benchmarks, the results show that a small set of easily-obtainable features can predict the magnitude of GPU speedups on two different high-end GPUs, with accuracies varying between 77% and 90%, depending on the prediction mechanism and scenario. For already-ported applications, similar models can predict the best device to run an application with an effective accuracy of 91%.","Graphics processing units,
Predictive models,
Accuracy,
Support vector machines,
Benchmark testing,
Analytical models,
Training"
Correcting abnormalities in meteorological data by machine learning,"Meteorological data collected from automatic weather stations have played an important role in forecasting and analyzing a large variety of phenomena. However, abnormal values are abundant in meteorological data due to manifold faults in observation systems. In this paper, we attempt to recover abnormal values. We present three estimation models based on machine learning techniques and compare them with traditional estimation methods, interpolations. Unlike the interpolation methods, which use only the target attribute, the proposed models utilize the additional information consisting of the associated attributes of the target station and the relevant data of the neighbor weather stations. Experiments were conducted for 692 locations in South Korea from 2007 to 2012. The results showed that the proposed approaches estimated target values better than the interpolation methods for all weather elements except one and the additional information helped achieve better performance.","Interpolation,
Wind speed,
Decision trees,
Humidity,
Estimation"
Application of statistical machine learning in identifying candidate biomarkers of resistant to anti-cancer drugs in ovarian cancer,"Drug resistance is one of the major challenges in the treatment of ovarian cancer. To facilitate identification of candidate biomarkers of resistant to platinum-based chemotherapy in ovarian cancer, we employed statistical machine learning techniques and integrative genomic data analysis. We used gene expression, somatic mutation and copy number aberration data of platinum sensitive and resistant tumors from the cancer genome atlas. Using regression tree and module network analysis, we identified genes that both contain mutations (copy number aberration and/or point mutation) and their expressions influence groups of their co-regulated genes for resistant and sensitive tumors. Finally, we compared these two gene lists and their associated pathways to extract a short list of genes as potential biomarkers of resistant to platinum-based chemotherapy.","Immune system,
Tumors,
Regulators,
Cancer,
Genomics,
Bioinformatics,
Gene expression"
Machine learning and image processing in astronomy with sparse data sets,"Automated classification systems have allowed for the rapid development of digital large sky surveys. Such systems increase the independence of human intervention in the analysis stage of star and galaxy classification. Artificial neural networks, hierarchical classifiers and ensembles of classifiers have been used as the methods of classification in these systems. This paper investigates the development of an automated classification system for galaxies in astronomical images based on the method of sparse representation. The dependency of classification based on image enhancement by the alpha-rooting, heap-, and paired-transforms is secondarily investigated.","Conferences,
Cybernetics,
Nickel"
Joint layer based deep learning framework for bilingual machine transliteration,"Between the growth of Internet or World Wide Web (WWW) and the emersion of the social networking site like Friendster, Myspace etc., information society started facing exhilarating challenges in language technology applications such as Machine Translation (MT) and Information Retrieval (IR). Nevertheless, there were researchers working in Machine Translation that deal with real time information for over 50 years since the first computer has come along. Merely, the need for translating data has become larger than before as the world was getting together through social media. Especially, translating proper nouns and technical terms has become openly challenging task in Machine Translation. The Machine transliteration was emerged as a part of information retrieval and machine translation projects to translate the Named Entities based on phoneme and grapheme, hence, those are not registered in the dictionary. Many researchers have used approaches such as conventional Graphical models and also adopted other machine translation techniques for Machine Transliteration. Machine Transliteration was always looked as a Machine Learning Problem. In this paper, we presented a new area of Machine Learning approach termed as a Deep Learning for improving the bilingual machine transliteration task for Tamil and English languages with limited corpus. This technique precedes Artificial Intelligence. The system is built on Deep Belief Network (DBN), a generative graphical model, which has been proved to work well with other Machine Learning problem. We have obtained 79.46% accuracy for English to Tamil transliteration task and 78.4 % for Tamil to English transliteration.","Training,
Vectors,
Joints,
Neurons,
Dictionaries,
Support vector machines,
Computers"
Promoting education: A state of the art machine learning framework for feedback and monitoring E-Learning impact,"A serious impediment in E-Learning is that these systems seem to have failed to consider the advantages of the supervision of a teacher. Teachers are able to monitor the progress made by several students, irrespective of their learning abilities and attempt to channel all students towards a common learning goal. E-Learning systems today don't possess a monitoring component. Most approaches customize content to suit differing learning abilities, resulting in different learning goals. However, this study attempts to apply machine learning methods that customizes not the content, but the presentation of the content assuming almost common learning goals - just like how a teacher would modify the content presentation, if some aspects are not clear to students based on their feedback. The primary challenge towards developing such a monitoring system is to decide what aspects of the interaction are to be monitored and how these are to interpreted as feedback with actionable insights - that is, to decide the learning schema, and then apply learning algorithms to gauge the interest or disinterest of the learner in the content presented.","Electronic learning,
Monitoring,
Media,
Learning systems,
Databases,
Real-time systems"
War against mobile malware with cloud computing and machine learning forces,"Today's smart phones are used for wider range of activities. This extended range of functionalities has also seen the infiltration of new security threats. The malicious parties are using highly stealthy techniques to perform the targeted operations, which are hard to detect by the conventional signature and behavior based approaches. Besides, the limited resources of mobile device are inadequate to perform the computationally extensive malware detection tasks and to sustain the device's clean status. In this paper, we propose an effective and resource rich detection system which uses certain distinguishing combinations of permissions and intents used by the apps to identify the malware apps. Different machine learning algorithms are investigated for classification of apps into benign or malware types. To the best of our knowledge, this is the first ever work in which both the permissions and intents have been amalgamated for malware detection using cloud computing paradigm. Effectiveness of our approach is verified by testing the real-world malware and benign apps collected from various sources.","Malware,
Mobile communication,
Smart phones,
Classification algorithms,
Machine learning algorithms"
An automatic flower classification approach using machine learning algorithms,"This work aims to develop an effective flower classification approach using machine learning algorithms. Eight flower categories were analyzed in order to extract their features. Scale Invariant Feature Transform (SIFT) and Segmentation-based Fractal Texture Analysis (SFTA) algorithms are used to extract flower features. The proposed approach consists of three phases namely: segmentation, feature extraction, and classification phases. In segmentation phase, the flower region is segmented to remove the complex background from the images dataset. Then flower image features are extracted. Finally for classification phase, the proposed approach applied Support Vector Machine (SVM) and Random Forests (RF) algorithms to classify different kinds of flowers. An experiment was carried out using the proposed approach on a dataset of 215 flower images. It shows that Support Vector Machine (SVM) based algorithm provides better accuracy compared to the Random Forests (RF) algorithm when using the SIFT as a feature extraction algorithm. While, Random Forests (RF) algorithm provides its better accuracy with SFTA. Moreover, the system is capable of automatically recognize the flower name with a high degree of accuracy.","Image segmentation,
Learning systems,
Image recognition"
Exploiting machine learning algorithms for cognitive radio,"Cognitive radio is an intelligent radio that has the ability to sense and learn from its environment. Basic core of cognitive radio contains a learning engine and it plays an important role in every application of cognitive radio from spectrum sensing to spectrum management. Learning engine implements different learning algorithms. In this paper we discuss various learning algorithms and their application in solving specific problems of cognitive radio. Some of the prominent learning algorithms discussed in this paper are Genetic Algorithm (GA), Artificial Neural Networks (ANN) and Hidden Markov Model (HMM).","Cognitive radio,
Hidden Markov models,
Genetic algorithms,
Sociology,
Statistics,
Bit error rate,
Machine learning algorithms"
Book Recommendation Using Machine Learning Methods Based on Library Loan Records and Bibliographic Information,"We propose a method to recommend books through machine learning modules based on several features, including library loan records. We evaluated the most effective method among ones using (a) a Support Vector Machine (SVM), (b) Random Forest and (c) Adaboost, as well as the most effective combination of relevant features among (1) library loan records, (2) book titles, (3) Nippon Decimal Classification categories, (4) publication year and (5) frequencies at which books were borrowed. We performed an experiment involving 40 subjects who are students at T University. The books that our methods recommended and the loan records that we used were obtained from the T University Library. The results show that books recommended by the SVM based on features (1), (2), (3) and (5) were rated most favorably by the subjects. Our method outperforms preceding ones, such as the method proposed by Tsuji et al. (2013), and is comparable in performance to the recommendation by the website Amazon.co.jp.","Libraries,
Books,
Support vector machines,
Educational institutions,
Association rules,
Learning systems,
Training data"
Machine learning approach for correcting preposition errors using SVD features,"Non-native English writers often make preposition errors in English language. The most commonly occurring preposition errors are preposition replacement, preposition missing and unwanted preposition. So, in this method, a system is developed for finding and handling the English preposition errors in preposition replacement case. The proposed method applies 2-Singular Value Decomposition (SVD2) concept for data decomposition resulting in fast calculation and these features are given for classification using Support Vector Machines (SVM) classifier which obtains an overall accuracy above 90%. Features are retrieved using novel SVD2 based method applied on trigrams which is having a preposition in the middle of the context. A matrix with the left and right vectors of each word in the trigram is computed for applying SVD2 concept and these features are used for supervised classification. Preliminary results show that this novel feature extraction and dimensionality reduction method is the appropriate method for handling preposition errors.","Context,
Vectors,
Support vector machines,
Manuals"
Methodologies and application of machine learning algorithms to classify the performance of high performance cluster components,"High Performance Computing Clusters are designed to host highly parallelized applications, often in excess of thousands of nodes allocated to a job. These jobs, especially those that require a high level of synchronous communication, can be greatly affected by a single poor, or even sub-standard performing component. These components, often referred to as a node, are typically comprised of CPUs, accelerator processors, memory, a communication bus, and so on. Consequently it is important to identify and eliminate these sub-standard performing nodes before a job is scheduled onto them. In this paper we will describe the process used to measure and the methodology used to quantify poor performing nodes or classify suspect performing nodes into groups, or clusters, that can be later used to identify future performance issues. This process is more involved than simply running a scientific calculation across all the nodes, finding one that was “slow”, and labeling it as a bad node. At Los Alamos, this methodology has been used successfully to find problem nodes and has helped characterize the components of other clusters to aid in the proactive elimination of potential problems.","Clustering algorithms,
Graphics processing units,
Bandwidth,
Principal component analysis,
Testing,
Standards"
Machine learning-based techniques for incremental functional diagnosis: A comparative analysis,"Incremental functional diagnosis is the process of iteratively selecting a test, executing it and based on the collected outcome deciding either to execute one more test or to stop the process since a faulty candidate component can be identified. The aim is to minimise the cost and the duration of the diagnosis process. In this paper we compare six engines based on machine learning techniques for driving the diagnosis. The comparison has been carried out under a twofold point of view: on the one hand, we analysed the issues related to the use of the considered techniques for the design of incremental diagnosis engines; on the other hand, we carried out a set of experiments on three synthetic but realistic scenarios to assess accuracy and efficiency.","Engines,
Accuracy,
Artificial neural networks,
Fault diagnosis,
Support vector machines,
Neurons,
Data mining"
"Machine Learning in Wireless Sensor Networks: Algorithms, Strategies, and Applications","Wireless sensor networks (WSNs) monitor dynamic environments that change rapidly over time. This dynamic behavior is either caused by external factors or initiated by the system designers themselves. To adapt to such conditions, sensor networks often adopt machine learning techniques to eliminate the need for unnecessary redesign. Machine learning also inspires many practical solutions that maximize resource utilization and prolong the lifespan of the network. In this paper, we present an extensive literature review over the period 2002-2013 of machine learning methods that were used to address common issues in WSNs. The advantages and disadvantages of each proposed algorithm are evaluated against the corresponding problem. We also provide a comparative guide to aid WSN designers in developing suitable machine learning solutions for their specific application challenges.","Wireless sensor networks,
Routing,
Machine learning algorithms,
Clustering algorithms,
Algorithm design and analysis,
Principal component analysis,
Classification algorithms"
Freight transport prediction using electronic waybills and machine learning,"A waybill is a document that accompanies the freight during transportation. The document contains essential information such as, origin and destination of the freight, involved actors, and the type of freight being transported. We believe, the information from a waybill, when presented in an electronic format, can be utilized for building knowledge about the freight movement. The knowledge may be helpful for decision makers, e.g., freight transport companies and public authorities. In this paper, the results from a study of a Swedish transport company are presented using order data from a customer ordering database, which is, to a larger extent, similar to the information present in paper waybills. We have used the order data for predicting the type of freight moving between a particular origin and destination. Additionally, we have evaluated a number of different machine learning algorithms based on their prediction performances. The evaluation was based on their weighted average true-positive and false-positive rate, weighted average area under the curve, and weighted average recall values. We conclude, from the results, that the data from a waybill, when available in an electronic format, can be used to improve knowledge about freight transport. Additionally, we conclude that among the algorithms IBk, SMO, and LMT, IBk performed better by predicting the highest number of classes with higher weighted average values for true-positive and false-positive, and recall.","Classification algorithms,
Cities and towns,
Prediction algorithms,
Accuracy,
Companies,
Machine learning algorithms,
Databases"
Robust extreme learning machine for regression problems with its application to wifi based indoor positioning system,"We propose two kinds of robust extreme learning machines (RELMs) based on the close-to-mean constraint and the small-residual constraint respectively to solve the problem of noisy measurements in indoor positioning systems (IPSs). We formulate both RELMs as second order cone programming problems. The fact that feature mapping in ELM is known to users is exploited to give the needed information for robust constraints. Real-world indoor localization experimental results show that, the proposed algorithms can not only improve the accuracy and repeatability, but also reduce the deviations and worst case errors of IPSs compared with basic ELM and OPT-ELM based IPSs.","Robustness,
IEEE 802.11 Standards,
Training,
Testing,
Calibration,
Vectors,
Support vector machines"
Detection of epileptic convulsions from accelerometry signals through machine learning approach,"A seizure detection system in the non-clinical environment would enable long-term monitoring and give better insights into the number of seizures and their characteristics. Moreover, an alarm at seizure onset is important for alerting the parents or care-givers so they could comfort the child and optionally give the treatment. Therefore, we developed a patient-independent automatic algorithm for registration and detection of (tonic-)clonic seizures based on four accelerometers attached to the wrists and ankles. The objective is to classify two second epochs as seizure or non-seizure epochs employing supervised learning techniques. Starting from 140 features found in similar publications, a filter method based on mutual information is applied to remove irrelevant and redundant features. A least-squares support vector machine classifier is used to distinguish seizure and non-seizure epochs based on the selected features. For seizures longer than 30 seconds, median sensitivity of 100%, false detection rate of 0.39 h-1 and alarm delay of 15.2 s over all patients are reached.","Support vector machines,
Pediatrics,
Wrist,
Monitoring,
Electroencephalography,
Vectors,
Testing"
Learning Kernels for Support Vector Machines with Polynomial Powers of Sigmoid,"In the pattern recognition research field, Support Vector Machines (SVM) have been an effectiveness tool for classification purposes, being successively employed in many applications. The SVM input data is transformed into a high dimensional space using some kernel functions where linear separation is more likely. However, there are some computational drawbacks associated to SVM. One of them is the computational burden required to find out the more adequate parameters for the kernel mapping considering each non-linearly separable input data space, which reflects the performance of SVM. This paper introduces the Polynomial-Powers of Sigmoid for SVM kernel mapping, and it shows their advantages over well-known kernel functions using real and synthetic datasets.","Kernel,
Support vector machines,
Polynomials,
Training,
Accuracy,
Benchmark testing"
Comparison of machine learning algorithms to predict psychological wellness indices for ubiquitous healthcare system design,"For ubiquitous healthcare service delivery, psychological wellness indices have been developed. A psychological wellness index integrates the survey results that measure stress, depression, anger, and fatigue. The current model is based on a multiple regression method and manually constructs a cause and effect model of the psychological wellness. However, this constructed model depends upon the survey responses. The relationship between these survey responses and psychological wellness indices are not linear due to data imbalance. When any data inconsistency exists, the reliability of the model decreases and eventually cost of maintenance on model revision increases. Also, when new variables or data entries are considered, the entire model should be constructed again. This paper examines the feasibility of machine learning algorithms to predict the psychological wellness indices based on the reconstructed responses. In this paper, four machine learning algorithms including multi-layer perceptron, support vector regression, generalized regression neural network, and k nearest neighbor regression, are compared and the experiment results are presented.","Psychology,
Machine learning algorithms,
Prediction algorithms,
Stress,
Fatigue,
Medical services,
Training data"
Machine learning tool and meta-heuristic based on genetic algorithms for plagiarism detection over mail service,"One of the most modern problems that computer science try to resolve is the plagiarism, in this article we present a new approach for automatic plagiarism detection in world of mail service. Our system is based on the n-gram character for the representation of the texts and tfidf as weighting to calculate the importance of term in the corpus, we use also a combination between the machine learning methods as a way to detect if a document is plagiarized or not, we use pan 09 corpus for the construction and evaluation of the prediction model then we simulate a meta-heuristic method based on genetic algorithms with a variations of parameters to know if it can improve the results. The main objective of our work is to protect intellectual property and improve the efficiency of plagiarism detection system.","Plagiarism,
Electronic mail,
Servers,
Entropy,
Genetic algorithms,
Detectors,
Classification algorithms"
Leveraging machine learning for optimize predictive classification and scheduling E-Health traffic,"Wireless Body Area Network (WBAN) is a special kind of autonomous sensor network evolved to provide wide variety of services. Nowadays WABN becomes an integral component of healthcare management system where a patient needs to be monitors both inside and outside home or hospital. These applications are responsible for gathering and managing heterogeneous data in terms of both for real time and non-real time traffic. Heterogeneous traffic classification plays an important role in various application of WBAN. Due to the ineffectiveness of traditional port-based and payload-based methods, recent work were proposed using machine learning methods to classify flows based on statistical characteristics. In this paper, we evaluate the effectiveness of integral concept of machine learning in terms of binary decision tree and genetic algorithm for classification of heterogeneous traffic flow according to rules. We have also designed an Earliest Deadline based flexible dynamic scheduling algorithm, which has been proven to be an optimal prioritized scheduling for problem like starvation.","Training,
Sensors,
Monitoring,
Genetics,
Springs"
Automated Trading with Machine Learning on Big Data,"Financial markets are now extremely efficient,nevertheless there are still many investment funds that generatealpha systematically beating markets' return benchmarks. Theemergence of big data gave professional traders the newterritory, leverage and evidence and renewed opportunitiesof their profitable exploitation by Machine Learning (ML)models, increasingly taking over the trading floor by 24/7automated trading in response to the continuously fed datastreams. Rapidly increasing data sizes and strictly real-timerequirements of the trading models render large subset ofML methods intractable, overcomplex and impossible to applyin practise. In this work we demonstrate how to efficientlyapproach the problem of automated trading with large portfoliostrategy that continuously consumes streams of data acrossmultiple diverse markets. We demonstrate a simple scalabletrading model that learns to generate profit from multiple intermarketprice predictions and markets' correlation structure.We also introduce the stochastic trade diffusion technique tomaximise trading turnover while reducing strategy's exposureto market impact and construct the efficient risk-mitigatingportfolio that backtests with the strong positive return.",
Identifying moving bodies from CCTV videos using machine learning techniques,"The idea of auto face detection from surveillance cameras and CCTVs is very relevant today. More and more CCTVs and surveillance cameras are being installed everyday. If there is a database of facial data present then the task of recognition boils down to comparison of each and every face detected from the video with every face saved in the database. Now this process involves capturing the faces before hand. This is actually a very tedious job. So the database of images is created (/updated) as and when new faces come into the camera view. The labeling of the faces can be done at leisure (by a human) or not be done at all. The current system once deployed does not need a database of images to start with. It creates its own collection of images, and then tracks the future occurrences of those images. Eigenface, fisherface, LBP histograms and SURF are different algorithms used for face recognition. We have tried all these algorithms.but among these surf shows better result. So the paper uses SURF for comparing image descriptors.","Approximation methods,
Artificial neural networks"
Oak Ridge Biosurveillance Toolkit: Scalable machine learning for public health surveillance,"As the number of data sources for public health surveillance continues to grow both in volume and variety, there is a need to develop data-driven machine learning tools that can automate discovery and aid decision makers in obtaining quantifiable insights on emerging disease spread phenomena. In this talk, we present an overview of scalable machine learning tools that we have been developing as part of advancing this mission. In particular, our machine learning tools can automatically (a) detect multi-scale spatial and temporal break-out patterns of disease occurrence, (b) quantify multi-modal co-occurrence disease patterns to identify local-and national-level “hotspots” and (c) predict how patterns of co-occurrence correspond to `intervention' strategies.","spatiotemporal phenomena,
Big Data,
diseases,
epidemics,
health care,
learning (artificial intelligence),
medical information systems"
Examining the effectiveness of machine learning algorithms for prediction of change prone classes,"Managing change in the early stages of a software development life cycle is an effective strategy for developing a good quality software at low costs. In order to manage change, we use software quality models which can efficiently predict change prone classes and hence guide developers in appropriate distribution of limited resources. This study examines the effectiveness of ten machine learning algorithms for developing such software quality models on three object-oriented software data sets. We also compare the performance of machine learning algorithms with the widely used logistic regression technique and statistically rank various algwith the help of Friedman test.","Measurement,
Prediction algorithms,
Software,
Software algorithms,
Data models,
Predictive models,
Support vector machines"
Breast cancer subtype identification using machine learning techniques,"Breast cancer is the most commonly diagnosed cancer and the second leading cause of death among women worldwide. Accurate diagnosis of the specific subtypes of this disease is vital to ensure that patients are provided with the most effective therapeutic strategies that yield the greatest response. Using the newly proposed ten subtypes of breast cancer, we hypothesize that machine learning techniques offer many benefits for selecting the most informative biomarkers. Unlike existing gene selection approaches, in this study, a hierarchical classification approach is used that selects genes and builds the classifier concurrently. Our results support that this modified approach to gene selection yields a small subset of genes that can predict these ten subtypes with greater than 95% overall accuracy.","Breast cancer,
Accuracy,
Breast tumors,
Support vector machines,
Gene expression"
Chip Health Monitoring Using Machine Learning,"In nanoscale technology nodes, process and runtime variations have emerged as the major sources of timing uncertainties which may ultimately result in circuit failure due to timing violation. Therefore, in-field chip health monitoring is essential to track workload-induced variations at runtime in a per-chip basis. There exist a variety of monitoring circuits to track the delay changes of different on-chip components. However, existing techniques either need to stop normal execution of the chip or introduce a significant overhead unless they are carefully placed for very selective locations. Another challenge is to infer the information regarding the health of every critical paths of the chip with limited information obtained by the monitoring system. We address these challenges in this work using a representative path-selection technique based on machine learning. This technique allows us to measure the delay of a small subset of paths and assess the circuit-level impact of workload for a larger pool of reliability-critical paths.","Delays,
Monitoring,
Runtime,
Logic gates,
Aging,
Vectors"
Machine learning for power system disturbance and cyber-attack discrimination,"Power system disturbances are inherently complex and can be attributed to a wide range of sources, including both natural and man-made events. Currently, the power system operators are heavily relied on to make decisions regarding the causes of experienced disturbances and the appropriate course of action as a response. In the case of cyber-attacks against a power system, human judgment is less certain since there is an overt attempt to disguise the attack and deceive the operators as to the true state of the system. To enable the human decision maker, we explore the viability of machine learning as a means for discriminating types of power system disturbances, and focus specifically on detecting cyber-attacks where deception is a core tenet of the event. We evaluate various machine learning methods as disturbance discriminators and discuss the practical implications for deploying machine learning systems as an enhancement to existing power system architectures.","Learning systems,
Smart grids,
Relays,
Accuracy,
Protocols,
Classification algorithms"
Using a Machine Learning Algorithm to Control an Artificial Hormone System,"The Artificial Hormone System (AHS) is a decentralized software which can be used to allocate tasks in a system of heterogeneous processing elements (PEs). Tasks are allocated according to their suitability for the heterogeneous PEs, the current PE load and task relationships. The AHS also provides properties like self-configuration, self-optimization and self-healing in the context of task allocation. In addition, it is able to guarantee real-time bounds for such self-X-properties. Our contribution in this paper is a machine learning approach for gradually learning the hormone values of different tasks. This is a major advance because expert knowledge is needed to configure the AHS up to now. We present an Observer-/Controller architecture monitoring and controlling the behaviour of the AHS. The user has to provide a simple set of initial rules and the Observer-/Controller is able to generate new rules if needed. The evaluation of our approach is very promising and we show and discuss our evaluation.","Biochemistry,
Radiation detectors,
Computer architecture,
Monitoring,
Observers,
Silicon,
Real-time systems"
Document categorization in multi-agent environment with enhanced machine learning classifier,"Text categorization task have gained the attention of researchers in last 10 years with the increase in web-based contents of documents. For searching a particular document from the web or any large document collection text or document categorization is most useful task. We demand some better system and enhanced machine learning classifiers to accomplish task of document categorization. We designed a multi-agent based system that consists of some software hybrid agents that obtains the category of a document and interact with each other to take final decision about the category and then data is fed to a machine learning classifier in order to enhance the performance. We analyzed the results of the system in form of performance measures such as accuracy, recall, precision and true negative rate. We analyzed the result in two scenarios: one is with decision of agents alone and another is with application of reinforcement clustering technique neural network. We observed that in first scenario the system's accuracy, precision and true negative rate is very good and recall measure is significantly good. After the application of reinforcement clustering technique there is no significant change in system's performance instead the recall is degraded. But still the system is producing good results in both scenarios.","Neural networks,
Text categorization,
Accuracy,
Feature extraction,
Probability,
Equations,
Mathematical model"
Sentiment analysis of twitter data using machine learning approaches and semantic analysis,"The wide spread of World Wide Web has brought a new way of expressing the sentiments of individuals. It is also a medium with a huge amount of information where users can view the opinion of other users that are classified into different sentiment classes and are increasingly growing as a key factor in decision making. This paper contributes to the sentiment analysis for customers' review classification which is helpful to analyze the information in the form of the number of tweets where opinions are highly unstructured and are either positive or negative, or somewhere in between of these two. For this we first pre-processed the dataset, after that extracted the adjective from the dataset that have some meaning which is called feature vector, then selected the feature vector list and thereafter applied machine learning based classification algorithms namely: Naive Bayes, Maximum entropy and SVM along with the Semantic Orientation based WordNet which extracts synonyms and similarity for the content feature. Finally we measured the performance of classifier in terms of recall, precision and accuracy.","Feature extraction,
Semantics,
Support vector machine classification,
Sentiment analysis,
Entropy,
Accuracy"
Weka meets TraceLab: Toward convenient classification: Machine learning for requirements engineering problems: A position paper,"Requirements engineering encompasses many difficult, overarching problems inherent to its subareas of process, elicitation, specification, analysis, and validation. Requirements engineering researchers seek innovative, effective means of addressing these problems. One powerful tool that can be added to the researcher toolkit is that of machine learning. Some researchers have been experimenting with their own implementations of machine learning algorithms or with those available as part of the Weka machine learning software suite. There are some shortcomings to using “one off” solutions. It is the position of the authors that many problems exist in requirements engineering that can be supported by Weka's machine learning algorithms, specifically by classification trees. Further, the authors posit that adoption will be boosted if machine learning is easy to use and is integrated into requirements research tools, such as TraceLab. Toward that end, an initial concept validation of a component in TraceLab is presented that applies the Weka classification trees. The component is demonstrated on two different requirements engineering problems. Finally, insights gained on using the TraceLab Weka component on these two problems are offered.","Classification tree analysis,
Classification algorithms,
Machine learning algorithms,
Educational institutions,
Software,
Supervised learning"
Elicitation of machine learning to human learning from iterative error correcting,"Numerous high performance machine learning algorithms are designed based on human learning, while human learning can also acquire elicitation from machine learning to investigate highly efficient learning process. This paper presents two iteratively error correcting based probabilistic neural networks (PNN) for connecting human learning and machine learning. C-PNN, G-PNN and G-PNN have been used to delete redundancy samples in our learning software based on question bank. In detail, we propose a recommendation approach of learning samples which selects samples according to density of knowledge points through calculating data field of knowledge points covered by problems. The approach also deletes redundant problems in order to deal with the question-sea tactical and remedy the defects of random selecting usually used in human learning.","Accuracy,
Abstracts,
Irrigation"
A Novel approach to predict diabetes mellitus using modified Extreme learning machine,"Data Classification and predictions are one of the prime tasks in Data mining. They continue to play a vital role in the area of computer science and data processing field. Clustering and classifications in Data Mining are used in various domains to give meaning to the available data and give some useful prediction results which can be applied to some of the crucial problem areas of the real world. Diabetes mellitus otherwise known as a slow poison by the medical experts is a major, alarming and gradually becoming a global problem. This paper experimented and used the concept of modified extreme learning machine to identify the patients of being diabetic or non-diabetic basing on some previously given data which in turn helps the medical people to identify whether someone is affected by diabetes or not. It also describes and compares the application of two popular machine learning methods: Back propagation neural network and modified Extreme learning machine which are used as binary classifiers to address the diabetes prediction problem. These two approaches are applied on same type of multi class classification datasets and the work tries to generate some comparative inferences from training and testing results. The datasets which are used in our work is taken from UCI learning repository.","Databases,
Diseases,
Classification algorithms,
Prediction algorithms,
Biology,
Computer architecture,
Artificial neural networks"
Manifold regularization Multiple Kernel Learning machine for classification,"Recently, Multiple Kernel Learning (MKL) is an interesting research area in kernel machine applications and provides better interpretability and adaptability. Previous works have not considered much about the data itself, especially the intrinsic geometry information of data which is possible being beneficial for machine learning. We propose a manifold regularized multiple kernel machines to use the manifold regularization term to explore the inner geometry distribution of data. In fact, there are some real datasets being embedded in low dimensional manifold being undeveloped or hard to be seen. So adding the manifold regularization term to the original MKL is based on the assumption that the data geometrical distribution information may help to get a proper learning machine performance. We use properties of reproducing kernel Hilbert spaces (RKHS), Representer Theorem and Laplacian Graph method to provide theoretical basis for the algorithm. In experiments, classification accuracies of the algorithm and its ability to represent potential low dimensional manifold are given. Testing results suggest that our proposed method is able to yield competent classification accuracy and worth pursuing further research works.","Abstracts,
Manifolds,
Support vector machines,
Sonar,
Breast,
Heart"
Differentiating pancreatic mucinous cystic neoplasms form serous oligocystic adenomas in spectral CT images using machine learning algorithms: A preliminary study,"Pancreatic cancer is one of the most fatal cancers. Distinguishing mucinous cystic neoplasm from serous oligocystic adenoma by using cross-sectional imaging system is very important for patients' prognosis. Gemstone spectral computed tomography (CT) can provide more information as compared with the conventional CT. Machine-learning algorithms have been employed in a great variety of applications. This preliminary study aims to verify the effectiveness of the additional information provided by spectral CT with the use of the state-of-the-art classification algorithm combined with feature-selection methods. Results show that SVM+MI achieves the highest classification accuracy (71.43%). The second highest classification accuracy is obtained by using SVM+LO (63.83%). Features selected by these algorithms are consistent with clinical observations. Top-ranking features include lower viewing energy (around 50 keV) CT values, Iodine-Water concentrations, and Effective-Z.","Support vector machines,
Machine learning algorithms,
Abstracts,
Classification algorithms,
Probabilistic logic,
Rotation measurement,
Computed tomography"
Hybrid fuzzy genetics-based machine learning with entropy-based inhomogeneous interval discretization,"Discretization of continuous attributes is a key issue in classifier design from numerical data. In the machine learning community, continuous attributes are discretized into intervals. An entropy measure is often used to determine the cutting points for interval discretization. In the fuzzy system community, continuous attributes are usually discretized into overlapping fuzzy sets. Learning and optimization techniques are used to adjust the membership function of each fuzzy set. One interesting research issue is a comparison between interval partitions and fuzzy partitions. We address this issue by using an entropy-based interval discretization method in hybrid fuzzy genetics-based machine learning (GBML). Our hybrid fuzzy GBML algorithm is applied to a number of data sets where interval discretization is fuzzified with different fuzzification grades from zero (i.e., interval partitions) to one (i.e., completely fuzzified partitions). Experimental results from various fuzzification grades are compared with each other.","Nonhomogeneous media,
Training,
Accuracy,
Classification algorithms,
Fuzzy sets,
Partitioning algorithms,
Training data"
Active learning with re-sampling for support vector machine in person re-identification,"Person re-identification is defined as to find the same person who re-occurred in a multi-camera surveillance system. A classifier for person re-identification may suffer from the imbalance dataset problem since the number of the targeted images is much less than irrelevant images. In this paper, we proposed over-sampling and under-sampling method for the active learning method for person re-identification. The sampling method is activated when the imbalance level of the training set is higher than a preset value during iteration of the active learning. The effect of the imbalance problem is reduced. Experimental results show the active learning method with the proposed re-sampling method scarifies the true negative rate to achieve higher true positive rate, which is more important in person re-identification.","Support vector machines,
Abstracts,
Surveillance,
Cameras,
Pattern matching,
Image segmentation"
Fourier Neural Network for machine learning,"In this paper, a Fourier Neural Network (FNN), which specializes in regression and classification tasks is introduced, together with its weights initializing and training algorithm. Experiments conducted on various datasets show that the proposed model converges much faster than Multilayer Perceptron (MLP) and has equally good predicting accuracy and generalization ability.","Neurons,
Breast,
Iris,
Training,
Accuracy,
Abstracts,
Manganese"
Supporting binocular visual quality prediction using machine learning,"We present a binocular visual quality prediction model using machine learning (ML). The model includes two steps: training and test phases. To be more specific, we first construct the feature vector from binocular energy response of stereoscopic images with different stimuli of orientations, spatial frequencies and phase shifts, and then use ML to handle the actual mapping of the feature vector into quality scores in training procedure. Finally, quality score is predicted by multiple iterations in test procedure. Experimental results on three publicly available 3D image quality assessment databases demonstrate that, in comparison with the most related existing methods, the proposed technique achieves comparatively consistent performance with subjective assessment.","Three-dimensional displays,
Databases,
Training,
Vectors,
Stereo image processing,
Measurement,
Feature extraction"
A fast and effective Extreme learning machine algorithm without tuning,"Artificial Neural Networks (ANN) is a major machine learning technique inspired by biological neural networks. However, the process of its parameter tuning is usually tedious and time consuming, and thus it becomes a major bottleneck for it being efficiently applied and used by nonexperts. In this paper, a novel ANN algorithm, termed as Automatic Regularized Extreme Learning Machine (AR-ELM), based on a Regularized Extreme Learning Machine (RELM) using ridge regression is proposed. It is a true automatic ANN learning algorithm in the sense that it can automatically identify the appropriate essential system parameter according to the input data without the need of user intervention. Since this method is based on a relatively straightforward formula, it can achieve very fast learning speed. The simulation results shows that the proposed AR-ELM algorithm can achieve comparable results to tedious cross-validation tuned RELM. Furthermore, we also systematically investigate one of the biggest concerns of ELM, its randomness nature, caused by randomly generated parameters.",
Multi-objective optimization of a hybrid model for network traffic classification by combining machine learning techniques,"Considerable effort has been made by researchers in the area of network traffic classification, since the Internet is constantly changing. This characteristic makes the task of traffic identification not a straightforward process. Besides that, encrypted data is being widely used by applications and protocols. There are several methods for classifying network traffic such as known ports and Deep Packet Inspection (DPI), but they are not effective since many applications constantly randomize their ports and the payload could be encrypted. This paper proposes a hybrid model that makes use of a classifier based on computational intelligence, the Extreme Learning Machine (ELM), along with Feature Selection (FS) and Multi-objective Genetic Algorithms (MOGA) to classify computer network traffic without making use of the payload or port information. The proposed model presented good results when evaluated against the UNIBS data set, using four performance metrics: Recall, Precision, Flow Accuracy and Byte Accuracy, with most rates exceeding 90%. Besides that, presented the best features and feature selection algorithm for the given problem along with the best ELM parameters.","Accuracy,
Ports (Computers),
Protocols,
Measurement,
Computational modeling,
Optimization,
Genetic algorithms"
Insights on prediction of patients' response to anti-HIV therapies through machine learning,"We collect data from the HIV Resistance Drug Database and, based on CD4+ and viral load measures, together with RNA sequences of the reverse transcriptase and of the protease of the virus, we design models using machine learning techniques MultiLayer Perception (MLP), Radial Basis Function (RBF), and Support Vector Machine (SVM), to predict the patient's response to anti-HIV treatment. In this work we applied the SMOTE Algorithm to deal with the enormous difference between the number of case and control samples, which was crucial for the accuracy of the models. Our results show that the SVM model proved more accurate than the other two, with a ROC curve area of 0.9398. We observe that, from 1000 patients, there are 646 samples for which the three methods delivered correct predictions. On the other hand, for 69 patients all three models fail. We analyzed the data for those patients more carefully, and we identified codons and properties that are important for a response/non-response result. Among the codons that our models identified, there are several with strong support from the literature and also a few new ones. Our analysis offers numerous insights that can be very useful to the prediction of patients' response to anti-HIV therapies in the future.","Support vector machines,
Predictive models,
Human immunodeficiency virus,
Amino acids,
Training,
Immune system,
Accuracy"
Recognizing slow eye movement for driver fatigue detection with machine learning approach,"Slow eye movement (SEM) regarded as a sign of onset of sleep is very significant for detecting driver fatigue, but its characteristics and detection algorithm have been rarely involved in the study of driver fatigue detection. In this study, some new features were extracted based on wavelet singularity analysis and statistics to detect SEMs. Six subjects participated in this simulated driving experiment, and for each subject, a more than 2 hours electro-oculogram (EOG) session was recorded. Each session was divided into SEM epochs and non-SEM epochs according to the common judgments made by the two of three experts by the visual recognition criteria of SEMs. Regarding the problem of detecting SEMs as an imbalance classification problem, and through the under-sampling and over-sampling methods a 2s horizontal electro-oculogram (HEO) signal could finally be recognized as the category of SEMs or non-SEMs with the classifiers SVM, GELM, and KNN respectively. Results prove that the proposed features was a little better than the wavelet energy features, and through the combination of the wavelet energy features and the new features based on wavelet singularity analysis and statistics, the classification results were improved obviously.","Continuous wavelet transforms,
Feature extraction,
Wavelet analysis,
Sleep,
Fatigue"
Machine-Learning-Based Feature Selection Techniques for Large-Scale Network Intrusion Detection,"Nowadays, we see more and more cyber-attacks on major Internet sites and enterprise networks. Intrusion Detection System (IDS) is a critical component of such infrastructure defense mechanism. IDS monitors and analyzes networks' activities for potential intrusions and security attacks. Machine-learning (ML) models have been well accepted for signature-based IDSs due to their learn ability and flexibility. However, the performance of existing IDSs does not seem to be satisfactory due to the rapid evolution of sophisticated cyber threats in recent decades. Moreover, the volumes of data to be analyzed are beyond the ability of commonly used computer software and hardware tools. They are not only large in scale but fast in/out in terms of velocity. In big data IDS, the one must find an efficient way to reduce the size of data dimensions and volumes. In this paper, we propose novel feature selection methods, namely, RF-FSR (Random Forest-Forward Selection Ranking) and RF-BER (Random Forest-Backward Elimination Ranking). The features selected by the proposed methods were tested and compared with three of the most well-known feature sets in the IDS literature. The experimental results showed that the selected features by the proposed methods effectively improved their detection rate and false-positive rate, achieving 99.8% and 0.001% on well-known KDD-99 dataset, respectively.","Intrusion detection,
Feature extraction,
Computational modeling,
Data models,
Radio frequency,
Big data,
Training"
Computational Diagnosis of Parkinson's Disease Directly from Natural Speech Using Machine Learning Techniques,"The human voice signal carries much information in addition to direct linguistic semantic information. This information can be perceived by computational systems. In this work, we show that early diagnosis of Parkinson's disease is possible solely from the voice signal. This is in contrast to earlier work in which we showed that this can be done using hand-calculated features of the speech (such as formants) as annotated by professional speech therapists. In this paper, we review that work and show that a differential diagnosis can be produced directly from the analog speech signal itself. In addition, differentiation can be made between seven different degrees of progression of the disease (including healthy). Such a system can act as an additional stage (or another building block) in a bigger system of natural speech processing. For example it could be used in automatic speech recognition systems that are used as personal assistants (such as Iphones' Siri, Google Voice), or as natural man-machine interfaces. We also conjecture that such systems can be extended to monitoring and classifying additional neurological diseases and speech pathologies. The methods presented here use a combination of signal processing features and machine learning techniques.","Speech,
Parkinson's disease,
Support vector machines,
Acoustics,
Feature extraction,
Educational institutions"
A machine learning technique for predicting the productivity of practitioners from individually developed software projects,"Context: Productivity management of software developers is a challenge in Information and Communication Technology. Predictions of productivity can be useful to determine corrective actions and to assist managers in evaluating improvement alternatives. Productivity prediction models have been based on statistical regressions, statistical time series, fuzzy logic, and machine learning. Goal: To propose a machine learning model termed general regression neural network (GRNN) for predicting the productivity of software practitioners. Hypothesis: Prediction accuracy of a GRNN is better than a statistical regression model when these two models are applied for predicting productivity of software practitioners who have individually developed their software projects. Method: A sample obtained from 396 software projects developed between the years 2005 and 2011 by 99 practitioners was used for training the models, whereas a sample of 60 projects developed by 15 practitioners in the first months of 2012 was used for testing the models. All projects were developed based upon a disciplined development process within a controlled environment. The accuracy of the GRNN was compared against that of a multiple regression model (MLR). The criteria for evaluating the accuracy of these two models were the Magnitude of Error Relative to the estimate and a t-paired statistical test. Results: Prediction accuracy of an GRNN was statistically better than that of an MLR model at the 99% confidence level. Conclusion: An GRNN could be applied for predicting the productivity of practitioners when New and Changed lines of code, reused code, and programming language experience of practitioners are used as independent variables.","Software,
Productivity,
Predictive models,
Mathematical model,
Accuracy,
Testing,
Neural networks"
Study on machine learning classifications based on OLI images,"Classification for remote sensing images needs to build rules through machine learning. OLI images are useful multi spectral images put into use in 2013. Three kinds of machine learning algorithms were studied for classifying an OLI image in this paper. Samples and 22 features are put in use to test the three kinds of machine learning algorithms. The results are shown as quantitative analysis, visual analysis and feature importance comparison. The results are as follows: In this three machine learning algorithms, using SVM can get the best results, BPNN make the worst results and different classifiers use different features for training and classification.","Support vector machines,
Accuracy,
Kernel,
Machine learning algorithms,
Geometry,
Gray-scale,
Polynomials"
Target Tracking Using Machine Learning and Kalman Filter in Wireless Sensor Networks,"This paper describes an original method for target tracking in wireless sensor networks. The proposed method combines machine learning with a Kalman filter to estimate instantaneous positions of a moving target. The target's accelerations, along with information from the network, are used to obtain an accurate estimation of its position. To this end, radio-fingerprints of received signal strength indicators (RSSIs) are first collected over the surveillance area. The obtained database is then used with machine learning algorithms to compute a model that estimates the position of the target using only RSSI information. This model leads to a first position estimate of the target under investigation. The kernel-based ridge regression and the vector-output regularized least squares are used in the learning process. The Kalman filter is used afterward to combine predictions of the target's positions based on acceleration information with the first estimates, leading to more accurate ones. The performance of the method is studied for different scenarios and a thorough comparison with well-known algorithms is also provided.","Acceleration,
Vectors,
Target tracking,
Sensors,
Covariance matrices,
Mathematical model,
State-space methods"
Improved malicious code classification considering sequence by machine learning,"Classification of malicious code by machine learning gives more flexible and adaptable prediction result than by existing approaches [1]. But the approach just can identify looks-like malicious code instead of real malicious one. In this research, a novel method to reduce the vagueness in the classification by machine learning to consider code sequence.","Vectors,
Accuracy,
Educational institutions,
Support vector machine classification,
Syntactics,
Training"
Computer-aided design of machine learning algorithm: Training fixed-point classifier for on-chip low-power implementation,"In this paper, we propose a novel linear discriminant analysis algorithm, referred to as LDA-FP, to train on-chip classifiers that can be implemented with low-power fixed-point arithmetic with extremely small word length. LDA-FP incorporates the non-idealities (i.e., rounding and overflow) associated with fixed-point arithmetic into the training process so that the resulting classifiers are robust to these non-idealities. Mathematically, LDA-FP is formulated as a mixed integer programming problem that can be efficiently solved by a novel branch-and-bound method proposed in this paper. Our numerical experiments demonstrate that LDA-FP substantially outperforms the conventional approach for the emerging biomedical application of brain computer interface.","Vectors,
Cost function,
Classification algorithms,
Support vector machine classification,
Upper bound,
Fixed-point arithmetic"
A hybrid Machine Learning methodology for imbalanced datasets,"In the Machine Learning systems several imbalanced data sets exhibit skewed class distributions in which most cases are allocated to a class and far fewer cases to a smaller one. A classifier induced from an imbalanced data set has usually a low error rate for the majority class and an unacceptable error rate for the minority class. In this paper a synoptic review of the various related methodologies is given, a new ensemble methodology is introduced and an experimental study with other ensembles is presented. The proposed method that combines the power of OverBagging and Rotation Forest algorithms improves the identification of a difficult small class, while keeping the classification ability of the other class in an acceptable accuracy level.","Classification algorithms,
Training,
Accuracy,
Principal component analysis,
Decision trees,
Learning systems,
Bagging"
A comparative study of one-class classifiers in machine learning problems with extreme class imbalance,"Classification problems with class imbalance occur when prior probabilities for the data classes differ significantly. The use of one-class classifiers is one of the main approaches to solving such problems. We conduct a comparative study of one-class classification algorithms in classification problems with extreme class imbalance. Emphasis is placed on evaluation of the classificatory accuracy of a one-class classifier based on the Real Valued Negative Selection Algorithm (RVNSA) from Artificial Immune Systems theory, as there are no previous studies focusing on it. Its performance is compared to the performance of 14 alternative classification algorithms which are considered as state of the art in one-class classification problems.","Immune system,
Robustness,
Presses,
Software engineering"
Detection of false sharing using machine learning,"False sharing is a major class of performance bugs in parallel applications. Detecting false sharing is difficult as it does not change the program semantics. We introduce an efficient and effective approach for detecting false sharing based on machine learning. We develop a set of mini-programs in which false sharing can be turned on and off. We then run the mini-programs both with and without false sharing, collect a set of hardware performance event counts and use the collected data to train a classifier. We can use the trained classifier to analyze data from arbitrary programs for detection of false sharing. Experiments with the PARSEC and Phoenix benchmarks show that our approach is indeed effective. We detect published false sharing regions in the benchmarks with zero false positives. Our performance penalty is less than 2%. Thus, we believe that this is an effective and practical method for detecting false sharing.","Instruction sets,
Training,
Hardware,
Vectors,
Accuracy,
Training data,
Arrays"
Decision support in attribute selection with machine learning approach,"This paper proposes a method to simultaneously select the most relevant single nucleotide polymorphisms (SNPs) markers - the attributes - for the characterization of any measurable phenotype described by a continuous variable using support vector regression (SVR) with Pearson VII Universal Kernel (PUK). The proposed study is multiattribute towards considering several markers simultaneously to explain the phenotype and is based jointly on a statistical tools, machine learning and computational intelligence.","Kernel,
Dairy products,
Standards,
Support vector machines,
Genetics,
Genetic algorithms,
Accuracy"
Astrophysical applications of machine learning at scale and under duress,"Summary form only given: The universe is teeming with change on timescales from billions of years to milliseconds. A major goal of modern synoptic imaging surveys is to categorize this change over the entire sky to infer the diverse physical origins of variability. However, event discovery is only the beginning in the quest to extract the deepest insights: expensive follow-up resources (telescopes and people) are required, often in a time constrained environment. Viewing discovery and scientific insight through a resource-maximization lens, I discuss how machine learning is being applied to some modern astrophysics challenges. Here, the surfacing of parallelized feature engineering and machine learning into production-quality (scalable and fault tolerant) frameworks is the frontier for our field.","learning (artificial intelligence),
astronomy computing"
Machine-Learning-Based Coadaptive Calibration for Brain-Computer Interfaces,"Brain-computer interfaces (BCIs) allow users to control a computer application by brain activity as acquired (e.g., by EEG). In our classic machine learning approach to BCIs, the participants undertake a calibration measurement without feedback to acquire data to train the BCI system. After the training, the user can control a BCI and improve the operation through some type of feedback. However, not all BCI users are able to perform sufficiently well during feedback operation. In fact, a nonnegligible portion of participants (estimated 15%–30%) cannot control the system (a BCI illiteracy problem, generic to all motor-imagery-based BCIs). We hypothesize that one main difficulty for a BCI user is the transition from offline calibration to online feedback. In this work, we investigate adaptive machine learning methods to eliminate offline calibration and analyze the performance of 11 volunteers in a BCI based on the modulation of sensorimotor rhythms. We present an adaptation scheme that individually guides the user. It starts with a subject-independent classifier that evolves to a subject-optimized state-of-the-art classifier within one session while the user interacts continuously. These initial runs use supervised techniques for robust coadaptive learning of user and machine. Subsequent runs use unsupervised adaptation to track the features’ drift during the session and provide an unbiased measure of BCI performance. Using this approach, without any offline calibration, six users, including one novice, obtained good performance after 3 to 6 minutes of adaptation. More important, this novel guided learning also allows participants with BCI illiteracy to gain significant control with the BCI in less than 60 minutes. In addition, one volunteer without sensorimotor idle rhythm peak at the beginning of the BCI experiment developed it during the course of the session and used voluntary modulation of its amplitude to control the feedback application.",
Resolution of overlapping fluorescence spectra using the kernel learning machine,"A novel method based on combination of highly sensitive spectrofluorimetry and flexible the machine learning techniques was proposed for the simultaneous spectrofluorimetric determination of α-naphthol, β-naphthylamine and carbazole with overlapping peaks. This method addresses multivariate calibration based on the least square support vector machines (LS-SVM) regression to provide a powerful model for machine learning and data mining. The LS-SVM technique has the advantages to offer the capability of learning a high dimensional feature with fewer training data, and to decrease the computational complexity by only requiring to solve a set of linear equations instead of a quadratic programming problem. Experimental results showed the LS-SVM method to be successful for simultaneous multicomponent determination even where severe overlap of spectra was present. The relative standard errors of prediction (RSEP) obtained for total components using LS-SVM and PLS were compared. It is found that the LS-SVM method is better than the conventional PLS methods.","Support vector machines,
Fluorescence,
Kernel,
Mathematical model,
Standards,
Training,
Equations"
Estimating the wake losses in large wind farms: A machine learning approach,"Estimating the wake losses in a wind farm is critical in the short term forecast of wind power, following the Numerical Weather Prediction (NWP) approach. Understanding the intensity of the wakes and the nature of its propagation within the wind farm still remains a challenge to scientist, engineers and utility operators. In this paper, five different machine learning methods are used to estimate the power deficit experienced by wind turbines due to the wake losses. Production data from the Horns Rev offshore wind farm, Denmark, have been used for the study. The methods used are linear regression, linear regression with feature engineering, nonlinear regression, Artificial Neural Networks (ANN) and Support Vector Regression (SVR). Power developed by individual turbines located at different positions within the farm were computed based on the above methods and compared with the actual power measurements. With the respective Variance Normalized Root Mean Square Error (VNRMSE) of 0.21 and 0.22, models based on ANN and SVR could estimate the wind farm wake effects at an acceptable accuracy level. The study shows that suitable machine learning methods can effectively be used in estimating the power deficits due to wake effects experienced in large wind farms.","Wind farms,
Wind turbines,
Wind speed,
Artificial neural networks,
Linear regression,
Wind power generation"
Contrastive divergence learning for the Restricted Boltzmann Machine,"The Deep Belief Network (DBN) recently introduced by Hinton is a kind of deep architectures which have been applied with success in many machine learning tasks. The DBN is based on Restricted Boltzmann Machine (RBM), which is a particular energy-based model. In this paper, we lay more emphasis on the modeling process and learning algorithm of the RBM. Furthermore, we design two kinds of experiments to prove the efficiency of the algorithm based on synthetic dataset and real dataset. The reconstruction data experiments are aimed at proving the convergence of the learning algorithm. The classification experiments are designed to testify the efficiency of the trained models. The result shows that contrastive divergence learning is an effective training algorithm for the RBM model.","Computational modeling,
Training,
Vectors,
Markov processes,
Data models,
Mathematical model,
Approximation methods"
Prototype Classification: Insights from Machine Learning,"We shed light on the discrimination between patterns belonging to two different classes by casting this decoding problem into a generalized prototype framework. The discrimination process is then separated into two stages: a projection stage that reduces the dimensionality of the data by projecting it on a line and a threshold stage where the distributions of the projected patterns of both classes are separated. For this, we extend the popular mean-of-class prototype classification using algorithms from machine learning that satisfy a set of invariance properties. We report a simple yet general approach to express different types of linear classification algorithms in an identical and easy-to-visualize formal framework using generalized prototypes where these prototypes are used to express the normal vector and offset of the hyperplane. We investigate non-margin classifiers such as the classical prototype classifier, the Fisher classifier, and the relevance vector machine. We then study hard and soft margin classifiers such as the support vector machine and a boosted version of the prototype classifier. Subsequently, we relate mean-of-class prototype classification to other classification algorithms by showing that the prototype classifier is a limit of any soft margin classifier and that boosting a prototype classifier yields the support vector machine. While giving novel insights into classification per se by presenting a common and unified formalism, our generalized prototype framework also provides an efficient visualization and a principled comparison of machine learning classification.",
Grasp Recognition for Uncalibrated Data Gloves: A Machine Learning Approach,"This paper presents a comparison of various machine learning methods applied to the problem of recognizing grasp types involved in object manipulations performed with a data glove. Conventional wisdom holds that data gloves need calibration in order to obtain accurate results. However, calibration is a time-consuming process, inherently user-specific, and its results are often not perfect. In contrast, the present study aims at evaluating recognition methods that do not require prior calibration of the data glove. Instead, raw sensor readings are used as input features that are directly mapped to different categories of hand shapes. An experiment was carried out in which test persons wearing a data glove had to grasp physical objects of different shapes corresponding to the various grasp types of the Schlesinger taxonomy. The collected data was comprehensively analyzed using numerous classification techniques provided in an open-source machine learning toolbox. Evaluated machine learning methods are composed of (a) 38 classifiers including different types of function learners, decision trees, rule-based learners, Bayes nets, and lazy learners; (b) data preprocessing using principal component analysis (PCA) with varying degrees of dimensionality reduction; and (c) five meta-learning algorithms under various configurations where selection of suitable base classifier combinations was informed by the results of the foregoing classifier evaluation. Classification performance was analyzed in six different settings, representing various application scenarios with differing generalization demands. The results of this work are twofold: (1) We show that a reasonably good to highly reliable recognition of grasp types can be achieved—depending on whether or not the glove user is among those training the classifier—even with uncalibrated data gloves. (2) We identify the best performing classification methods for the recognition of various grasp types. To conclude, cumbersome calibration processes before productive usage of data gloves can be spared in many situations.",
A Machine Learning Evaluation of an Artificial Immune System,"ARTIS is an artificial immune system framework which contains several adaptive mechanisms. LISYS is a version of ARTIS specialized for the problem of network intrusion detection. The adaptive mechanisms of LISYS are characterized in terms of their machine-learning counterparts, and a series of experiments is described, each of which isolates a different mechanism of LISYS and studies its contribution to the system's overall performance. The experiments were conducted on a new data set, which is more recent and realistic than earlier data sets. The network intrusion detection problem is challenging because it requires one-class learning in an on-line setting with concept drift. The experiments confirm earlier experimental results with LISYS, and they study in detail how LISYS achieves success on the new data set.","computer security,
Anomaly detection,
artificial immune systems,
machine learning,
immune system,
network intrusion detection"
Nondegenerate Piecewise Linear Systems: A Finite Newton Algorithm and Applications in Machine Learning,"We investigate Newton-type optimization methods for solving piecewise linear systems (PLSs) with nondegenerate coefficient matrix. Such systems arise, for example, from the numerical solution of linear complementarity problem, which is useful to model several learning and optimization problems. In this letter, we propose an effective damped Newton method, PLS-DN, to find the exact (up to machine precision) solution of nondegenerate PLSs. PLS-DN exhibits provable semiiterative property, that is, the algorithm converges globally to the exact solution in a finite number of iterations. The rate of convergence is shown to be at least linear before termination. We emphasize the applications of our method in modeling, from a novel perspective of PLSs, some statistical learning problems such as box-constrained least squares, elitist Lasso (Kowalski & Torreesani, 2008), and support vector machines (Cortes & Vapnik, 1995). Numerical results on synthetic and benchmark data sets are presented to demonstrate the effectiveness and efficiency of PLS-DN on these problems.",
Predicting an Orchestral Conductor's Baton Movements Using Machine Learning,"Telematic musical performance, in which performers at two or more sites collaborate via networked audio and video, suffers significantly from latency. In the extreme case, performers at all sites slow to match their delayed counterparts, resulting in a steadily decreasing tempo. Introducing video of a conductor does not immediately solve the problem, as conductor video is also subjected to network latencies. This article lays the groundwork for an alternative approach to mitigating the effects of latency in distributed orchestral performances, based on generation of a predicted version of the conductor's baton trajectory. The prediction step is the most fundamental problem in this scheme, for which we propose the use of conventional machine learning techniques. Specifically, we demonstrate a particle filter and an extended Kalman filter that each track the location of the baton's tip and predict it multiple beats into the future; we compare these with a conventional feature-based method. We also describe a generic two-part framework that prescribes the incorporation of rehearsal data into a probabilistic model, which is then adapted during live performance. Finally, we suggest a framework and experimental methodology for establishing perceptually based metrics for predicted baton paths. Note that the perceptual efficacy of the presented methods requires experimental confirmation beyond the scope of this article.",
A Large Committee Machine Learning Noisy Rules,"Statistical mechanics is used to study generalization in a tree committee machine with K hidden units and continuous weights trained on examples generated by a teacher of the same structure but corrupted by noise. The corruption is due to additive gaussian noise applied in the input layer or the hidden layer of the teacher. In the large K limit the generalization error ϵg as function of α, the number of patterns per adjustable parameter, shows a qualitatively similar behavior for the two cases: It does not approach its optimal value and is nonmonotonic if training is done at zero temperature. This remains true even when replica symmetry breaking is taken into account. Training at a fixed positive temperature leads, within the replica symmetric theory, to an α-k decay of ϵg toward its optimal value. The value of k is calculated and found to depend on the model of noise. By scaling the temperature with α, the value of k can be increased to an optimal value kopt. However, at one step of replica symmetry breaking at a fixed positive temperature ϵg decays as α−kopt. So, although ϵg will approach its optimal value with increasing sample size for any fixed K, the convergence is only uniform in K when training at a positive temperature.",
A Machine Learning Method for Extracting Symbolic Knowledge from Recurrent Neural Networks,"Neural networks do not readily provide an explanation of the knowledge stored in their weights as part of their information processing. Until recently, neural networks were considered to be black boxes, with the knowledge stored in their weights not readily accessible. Since then, research has resulted in a number of algorithms for extracting knowledge in symbolic form from trained neural networks. This article addresses the extraction of knowledge in symbolic form from recurrent neural networks trained to behave like deterministic finite-state automata (DFAs). To date, methods used to extract knowledge from such networks have relied on the hypothesis that networks' states tend to cluster and that clusters of network states correspond to DFA states. The computational complexity of such a cluster analysis has led to heuristics that either limit the number of clusters that may form during training or limit the exploration of the space of hidden recurrent state neurons. These limitations, while necessary, may lead to decreased fidelity, in which the extracted knowledge may not model the true behavior of a trained network, perhaps not even for the training set. The method proposed here uses a polynomial time, symbolic learning algorithm to infer DFAs solely from the observation of a trained network's input-output behavior. Thus, this method has the potential to increase the fidelity of the extracted knowledge.",
Machine Learning of Jazz Grammars,,
Ventricular Fibrillation and Tachycardia Classification Using a Machine Learning Approach,"Correct detection and classification of ventricular fibrillation (VF) and rapid ventricular tachycardia (VT) is of pivotal importance for an automatic external defibrillator and patient monitoring. In this paper, a VF/VT classification algorithm using a machine learning method, a support vector machine, is proposed. A total of 14 metrics were extracted from a specific window length of the electrocardiogram (ECG). A genetic algorithm was then used to select the optimal variable combinations. Three annotated public domain ECG databases (the American Heart Association Database, the Creighton University Ventricular Tachyarrhythmia Database, and the MIT-BIH Malignant Ventricular Arrhythmia Database) were used as training, test, and validation datasets. Different window sizes, varying from 1 to 10 s were tested. An accuracy (Ac) of 98.1%, sensitivity (Se) of 98.4%, and specificity (Sp) of 98.0% were obtained on the in-sample training data with 5 s-window size and two selected metrics. On the out-of-sample validation data, an Ac of 96.3% ± 3.4%, Se of 96.2% ± 2.7%, and Sp of 96.2% ± 4.6% were obtained by fivefold cross validation. The results surpass those of current reported methods.","Electrocardiography,
Training,
Measurement,
Databases,
Support vector machines,
Accuracy,
Genetic algorithms"
Machine learning in district heating system energy optimization,"This paper introduces a work in progress, where we intend to investigate the application of Reinforcement Learning (RL) and online Supervised Learning (SL) to achieve energy optimization in District-Heating (DH) systems. We believe RL is an ideal approach since this task falls under the control-optimization problem where RL has yielded optimal results in previous work. The magnitude and scale of a DH system complexity incurs the curse of dimensionalities and model, hereby making RL a good choice since it provides a solution for the problem. To assist RL even further with the curse of dimensionalities, we intend to investigate the use of SL to reduce the state space. To achieve this, we shall use historical data to generate a heat load sub-model for each home. We believe using the output of these sub-models as feedback to the RL algorithm could significantly reduce the complexity of the learning task. Also, it could reduce convergence time for the RL algorithm. The desired goal is to achieve a realtime application, which takes operational actions when it receives new direct feedback. However, considering the dynamics of DH system such as large time delay and dissipation in DH network due to various factors, we hope to investigate things such as the appropriate data sampling rate and new parameters / sensors that could improve knowledge about the state of the system, especially on the consumer side of the DH network.","DH-HEMTs,
Space heating,
Water heating,
Cogeneration,
Learning (artificial intelligence),
Load modeling"
Preliminary design of estimation heart disease by using machine learning ANN within one year,"In this paper discussed the development of heart disease prediction using machine learning (in this case the Artificial Neural Network or ANN). There are 13 variables that can determine heart disease according to Miss Chaitrali paper. Prediction of a person's heart disease one year ahead is performed by studying the model heart rate data. Data is taken by using tool such as smart mirror, smart mouse, smart phones and smart chair. Heart rate data were collected through the Internet and collected in a server. Learning in this system is performed for a period of one year to get enough data to make predictions. Predictive of future heart disease in one year can increase a person's awareness of heart disease itself. The system is also expected to reduce the number of patients and the number of deaths from heart disease.","Diseases,
Heart rate,
Prediction algorithms,
Mice,
Artificial neural networks,
Servers"
Automated human behavioral analysis framework using facial feature extraction and machine learning,"Emotional intelligence is essential in understanding and predicting human behavior. Although human emotion is best captured using non-intrusive methods, due to factors such as system complexity, computation time and decision response time, the reality of automated behavioral analysis is hindered. In this paper, we propose a framework capable of recognizing emotions of an individual to identify any suspicious behavior. Our research shows 91.1% of emotion classification accuracy for cooperative individuals using facial feature extraction and machine learning techniques, thus outperforming existing state-of-the-art approaches.","Feature extraction,
Emotion recognition,
Facial features,
Cameras,
Face,
Discrete cosine transforms,
Complexity theory"
Machine Learning-Based Runtime Scheduler for Mobile Offloading Framework,"Remote offloading techniques have been proposed to overcome the limited resources of mobile platforms by leveraging external powerful resources such as personal work-stations or cloud servers. Prior studies have primarily focused on core mechanisms for offloading. Yet, adaptive scheduling in such systems is important because offloading effectiveness can be influenced by varying network conditions, workload requirements, and load at the target device. In this paper, we present a study on the feasibility of applying machine learning techniques to address the adaptive scheduling problem in mobile offloading framework. The study considers 19 different machine learning algorithms and four workloads, with a dataset obtained through the deployment of an Android-based remote offloading framework prototype on actual mobile and cloud resources. From this set, a subset of machine learning algorithms, which have relatively high scheduling accuracy, is selected to implement an offline offloading scheduler. Finally, by taking computational cost and the scheduling performance into account, we use Instance-Based Learning to evaluate an online adaptive scheduler for mobile offloading. In our evaluation, we observe that an Instance Learning-based online offloading scheduler selects the best scheduling decision in 87.5% instances, in an experiment setup in which an image processing workload is offloaded while subject to varying network bandwidth conditions and the amount of data transfer.","Mobile communication,
Servers,
Runtime,
Processor scheduling,
Scheduling,
Bandwidth,
Mobile computing"
Removing JPEG blocking artifacts using machine learning,"JPEG is a commonly used image compression method. While it normally yields very good compression ratios, it also introduces blocking artifacts and quantization noise. In this paper, we present a method to remove noise and blocking effects from JPEG-compressed images. We use machine learning techniques to predict DCT coefficients and pixel values in a compressed image. Results show a decrease in mean square error between our predicted images and the original uncompressed images when compared to the compressed images, as well as a clear reduction of blocking artifacts.","Image coding,
PSNR,
Transform coding,
Visualization,
Image color analysis,
Image edge detection,
Smoothing methods"
Breast cancer mass localization based on machine learning,"According to Breast Cancer Institute (BCI), Breast cancer is one of the most dangerous types of cancer that affects women all around the world. Based on clinical guidelines, the use of mammogram for an early detection of this cancer is an important step in reducing its danger. Thus, computer aided detection using image processing techniques in analyzing mammogram images and localizing abnormalities such as mass has been used. A False Positive (FP) rate is considered a challenge in localizing mass in mammogram images. Hence, in this paper, the rejection model based on the Support Vector Machine (SVM) has been used in reducing the FP rate of segmented mammogram images using the Chan-Vese method, initialized by the Marker Controller Watershed (MCWS) algorithm. Firstly, a mammogram image is segmented using the MCWS algorithm. Then, the segmentation is refined using Chan-Vese. After that, the SVM rejection model is built and is used in rejecting the non-correct segmented nodules. The dataset which consists of 16 nodules and 28 non-nodules has been obtained from the UKM Medical Centre. The experiment has shown the effectiveness of the SVM rejection model in reducing the FP rate compared to the result without the use of the SVM rejection model.","Support vector machines,
Image segmentation,
Breast cancer,
Signal processing algorithms,
Predictive models"
"An integrated approach to spam classification on Twitter using URL analysis, natural language processing and machine learning techniques","In the present day world, people are so much habituated to Social Networks. Because of this, it is very easy to spread spam contents through them. One can access the details of any person very easily through these sites. No one is safe inside the social media. In this paper we are proposing an application which uses an integrated approach to the spam classification in Twitter. The integrated approach comprises the use of URL analysis, natural language processing and supervised machine learning techniques. In short, this is a three step process.","Twitter,
Natural language processing,
Training,
Unsolicited electronic mail,
Accuracy,
Machine learning algorithms"
Toward a Semiautomatic Machine Learning Retrieval of Biophysical Parameters,"Biophysical parameters such as leaf chlorophyll content (LCC) and leaf area index (LAI) are standard vegetation products that can be retrieved from Earth observation imagery. This paper introduces a new machine learning regression algorithms (MLRAs) toolbox into the scientific Automated Radiative Transfer Models Operator (ARTMO) software package. ARTMO facilitates retrieval of biophysical parameters from remote observations in a MATLAB graphical user interface (GUI) environment. The MLRA toolbox enables analyzing the predictive power of various MLRAs in a semiautomatic and systematic manner, and applying a selected MLRA to multispectral or hyperspectral imagery for mapping applications. It contains both linear and nonlinear state-of-the-art regression algorithms, in particular linear feature extraction via principal component regression (PCR), partial least squares regression (PLSR), decision trees (DTs), neural networks (NNs), kernel ridge regression (KRR), and Gaussian processes regression (GPR). The performance of multiple implemented regression strategies has been evaluated against the SPARC dataset (Barrax, Spain) and simulated Sentinel-2 (8 bands), CHRIS (62 bands) and HyMap (125 bands) observations. In general, nonlinear regression algorithms (NN, KRR, and GPR) outperformed linear techniques (PCR and PLSR) in terms of accuracy, bias, and robustness. Most robust results along gradients of training/validation partitioning and noise variance were obtained by KRR while GPR delivered most accurate estimations. We applied a GPR model to a hyperspectral HyMap flightline to map LCC and LAI. We exploited the associated uncertainty intervals to gain insight in the per-pixel performance of the model.","Training,
Artificial neural networks,
Biological system modeling,
Kernel,
Ground penetrating radar,
Remote sensing,
Graphical user interfaces"
Energy-Efficient Time-of-Flight Estimation in the Presence of Outliers: A Machine Learning Approach,"The time-of-flight (ToF) estimation problem is common in sonar, ultrasound, radar, and other remote sensing applications. The conventional ToF maximum-likelihood estimator (MLE) exhibits a rapid deterioration in the accuracy when the signal-to-noise ratio (SNR) falls below a certain threshold. This threshold effect emerges mostly due to appearance of outliers associated with the side lobes in the autocorrelation function of a narrowband source signal. In our previous work, we have introduced a bank of unmatched filters and biased ToF estimators derived using these filters. These biased estimators form a feature vector for training a classifier which, subsequently, is used for reducing the bias and the variance parts induced by outliers in the mean-square error (MSE) of the MLE. In this paper, we extend the above method by introducing an adaptive scheme for controlling the number of measurements (pulses) required to achieve a desired accuracy. We show that using the information provided by a classifier, it is possible to achieve the estimation error of the MLE but by using significantly less number of pulses and thus energy on average.","Maximum likelihood estimation,
Correlation,
Pulse measurements,
Measurement uncertainty,
Signal to noise ratio,
Remote sensing"
Detecting spongiosis in stained histopathological specimen using multispectral imaging and machine learning,"Pathologists spend nearly 80% of their time analysing pathological tissue samples. In addition, the diagnosis is subject to inter/intra-observer variability. Thus to increase productivity and repeatability, a new field known as Computational Pathology has emerged which combines the field of pathology with computer vision, pattern recognition and machine learning. This research develops a new computational pathology framework specifically to aid with detecting a condition known as spongiosis caused by Newcastle Disease Virus infection in poultry. It combines the use of multispectral imaging with feature extraction and classification to detect areas of spongiosis in tissue of infected poultry. The success of this framework is the first step towards a completely automated diagnosis tool for histopathology.","Feature extraction,
Pathology,
Support vector machines,
Cameras,
Multispectral imaging,
Training"
MaSiF: Machine learning guided auto-tuning of parallel skeletons,"Parallel skeletons provide a predefined set of parallel templates that can be combined, nested and parameterized with sequential code to produce complex parallel programs. The implementation of each skeleton includes parameters that have a significant effect on performance; so carefully tuning them is vital. The optimization space formed by these parameters is complex, non-linear, exhibits multiple local optima and is program dependent. This makes manual tuning impractical. Effective automatic tuning is therefore essential for the performance of parallel skeleton programs. In this paper we present MaSiF, a novel tool to auto-tune the parallelization parameters of skeleton parallel programs. It reduces the size of the parameter space using a combination of machine learning, via nearest neighbor classification, and linear dimensionality reduction using Principal Components Analysis. To auto-tune a new program, a set of program features is determined statically and used to compute k nearest neighbors from a set of training programs. Previously collected performance data for the nearest neighbors is used to reduce the size of the search space using Principal Components Analysis. Good parallelization parameters are found quickly by searching this smaller search space. We evaluate MaSiF for two existing parallel frameworks: Threading Building Blocks and FastFlow. MaSiF achieves 89% of the performance of the oracle on average. This exploration requires just 45 parameters values on average, which is ~0.05% of the optimization space. In contrast, a state-of-the-art machine learning approach achieves 51%. MaSiF achieves an average speedup of 1.32× over parallelization parameters chosen by human experts.","Skeleton,
Training,
Principal component analysis,
Optimization,
Feature extraction,
Vectors,
Tuning"
Dynamic Feature Selection for Machine-Learning Based Concurrency Regulation in STM,"In this paper we explore machine-learning approaches for dynamically selecting the well suited amount of concurrent threads in applications relying on Software Transactional Memory (STM). Specifically, we present a solution that dynamically shrinks or enlarges the set of input features to be exploited by the machine-learner. This allows for tuning the concurrency level while also minimizing the overhead for input-features sampling, given that the cardinality of the input-feature set is always tuned to the minimum value that still guarantees reliability of workload characterization. We also present a fully heedged implementation of our proposal within the TinySTM open source framework, and provide the results of an experimental study relying on the STAMP benchmark suite, which show significant reduction of the response time with respect to proposals based on static feature selection.","Concurrent computing,
Proposals,
Correlation,
Instruction sets,
Benchmark testing,
Artificial neural networks,
Reliability"
Providing Transaction Class-Based QoS in In-Memory Data Grids via Machine Learning,"Elastic architectures and the ""pay-as-you-go"" resource pricing model offered by many cloud infrastructure providers may seem the right choice for companies dealing with data centric applications characterized by high variable workload. In such a context, in-memory transactional data grids have demonstrated to be particularly suited for exploiting advantages provided by elastic computing platforms, mainly thanks to their ability to be dynamically (re-)sized and tuned. Anyway, when specific QoS requirements have to be met, this kind of architectures have revealed to be complex to be managed by humans. Particularly, their management is a very complex task without the stand of mechanisms supporting run-time automatic sizing/tuning of the data platform and the underlying (virtual) hardware resources provided by the cloud. In this paper, we present a neural network-based architecture where the system is constantly and automatically re-configured, particularly in terms of computing resources, in order to achieve transaction class-based QoS while minimizing costs of the infrastructure. We also present some results showing the effectiveness of our architecture, which has been evaluated on top of Future Grid IaaS Cloud using Red Hat Infinispan in-memory data grid and the TPC-C benchmark.","Servers,
Time factors,
Neural networks,
Memory management,
Data models,
Cloud computing"
Zebrafish Larva Locomotor Activity Analysis Using Machine Learning Techniques,"Zebra fish larvae have become a popular model organism to investigate genetic and environmental factors affecting behavior. However, difficulties exist in the analysis of complex behaviors from a large array of larvae. In this paper, we present the new application of machine learning techniques in bioinformatics to automatically detect and investigate the locomotor activities of zebra fish larvae. To achieve this, twelve features were defined and seven unsupervised learning methods were implemented. Next, seven performance measures were applied to evaluate and compare these methods. In order to empirically evaluate the machine learning algorithms, a large dataset was collected that contained 6847 valid instances. Using this dataset, the characteristics of the features were analyzed and the most appropriate unsupervised learning algorithm, i.e., Unweighted Pair Group Method with Arithmetic mean (UPGMA), for locomotor activity analysis was identified. In addition, UPGMA's ability to reveal underlying patterns of zebra fish locomotor activities was demonstrated. In general, this study shows that machine learning techniques have the potential to construct effective, high-throughput systems to automate the process of identifying zebra fish behaviors influenced by genetic manipulation, pharmaceuticals, and environmental toxins.","Unsupervised learning,
Turning,
Measurement,
Clustering algorithms,
Videos,
Machine learning algorithms,
Stability analysis"
Aiding Intrusion Analysis Using Machine Learning,"Intrusion analysis, i.e., the process of combing through IDS alerts and audit logs to identify real successful and attempted attacks, remains a difficult problem in practical network security defense. The major contributing cause to this problem is the high false-positive rate in the sensors used by IDS systems to detect malicious activities. The goal of our work is to examine whether a machine-learned classifier can help a human analyst filter out non-interesting scenarios reported by an IDS alert correlator, so that analysts' time can be saved. This research is conducted in the open-source SnIPS intrusion analysis framework. Throughout observing the output of SnIPS running on our departmental network, we found that an analyst would need to perform repetitive tasks in pruning out the false positives in the correlation graphs produced by it. We hypothesized that such repetitive tasks can yield (limited) labeled data that can enable the use of a machine learning-based approach to prune SnIPS' output based on the human analysts' feedback, much similar to spam filters that can learn from users' past judgment to prune emails. Our goal is to classify the correlation graphs produced from SnIPS into ""interesting"" and ""non-interesting"", where ""interesting"" means that a human analyst would want to conduct further analysis on the events. We spent significant amount of time manually labeling SnIPS' output correlations based on this criterion, and built prediction models using both supervised and semi-supervised learning approaches. Our experiments revealed a number of interesting observations that give insights into the pitfalls and challenges of applying machine learning in intrusion analysis. The experimentation results also indicate that semi-supervised learning is a promising approach towards practical machine learning-based tools that can aid human analysts, when a limited amount of labeled data is available.","Correlation,
Security,
Labeling,
Servers,
Sensors,
Predictive models,
Production"
Simplifying the Utilization of Machine Learning Techniques for Bioinformatics,"The domain of bioinformatics has a number of challenges such as handling datasets which exhibit extreme levels of high dimensionality (large number of features per sample) and datasets which are particularly difficult to work with. These datasets contain many pieces of data (features) which are irrelevant and redundant to the problem being studied, which makes analysis quite difficult. However, techniques from the domain of machine learning and data mining are well suited to combating these difficulties. Techniques like feature selection (choosing an optimal subset of features for subsequent analysis by removing irrelevant or redundant features) and classifiers (used to build inductive models in order to classify unknown instances) can assist researchers in working with such difficult datasets. Unfortunately, many practitioners of bioinformatics do not have the machine learning knowledge to choose the correct techniques in order to achieve good classification results. If the choices could be simplified or predetermined then it would be easier to apply the techniques. This study is a comprehensive analysis of machine learning techniques on twenty-five bioinformatics datasets using six classifiers, and twenty-four feature rankers. We analyzed the factors at each of four feature subset sizes chosen for being large enough to be effective in creating inductive models but small enough to be of use for further research. Our results shows that Random Forest with 100 trees is the top performing classifier and that the choice of feature ranker is of little importance as long as feature selection occurs. Statistical analysis confirms our results. By choosing these parameters, machine learning techniques are more accessible to bioinformatics.","Bioinformatics,
Support vector machines,
Logistics,
Vegetation,
DNA,
Lungs,
Biological system modeling"
An efficient flow-based botnet detection using supervised machine learning,"Botnet detection represents one of the most crucial prerequisites of successful botnet neutralization. This paper explores how accurate and timely detection can be achieved by using supervised machine learning as the tool of inferring about malicious botnet traffic. In order to do so, the paper introduces a novel flow-based detection system that relies on supervised machine learning for identifying botnet network traffic. For use in the system we consider eight highly regarded machine learning algorithms, indicating the best performing one. Furthermore, the paper evaluates how much traffic needs to be observed per flow in order to capture the patterns of malicious traffic. The proposed system has been tested through the series of experiments using traffic traces originating from two well-known P2P botnets and diverse non-malicious applications. The results of experiments indicate that the system is able to accurately and timely detect botnet traffic using purely flow-based traffic analysis and supervised machine learning. Additionally, the results show that in order to achieve accurate detection traffic flows need to be monitored for only a limited time period and number of packets per flow. This indicates a strong potential of using the proposed approach within a future on-line detection framework.","Feature extraction,
Accuracy,
Training,
Bayes methods,
Protocols,
Support vector machines,
Vegetation"
Using Support Vector Machines to Classify Student Attentiveness for the Development of Personalized Learning Systems,"There have been many studies in which researchers have attempted to classify student attentiveness. Many of these approaches depended on a qualitative analysis and lacked any quantitative analysis. Therefore, this work is focused on bridging the gap between qualitative and quantitative approaches to classify student attentiveness. Thus, this research applies machine learning algorithms (K-means and SVM) to automatically classify students as attentive or inattentive using data from a consumer RGB-D sensor. Results of this research can be used to improve teaching strategies for instructors at all levels and can aid instructors in implementing personalized learning systems, which is a National Academy of Engineering Grand Challenge. This research applies machine learning algorithms to an educational setting. Data from these algorithms can be used by instructors to provide valuable feedback on the effectiveness of their instructional strategies and pedagogies. Instructors can use this feedback to improve their instructional strategies, and students will benefit by achieving improved learning and subject mastery. Ultimately, this will result in the students' increased ability to do work in their respective areas. Broadly, this work can help advance efforts in many areas of education and instruction. It is expected that improving instructional strategies and implementing personalized learning will help create more competent, capable, and prepared persons available for the future workforce.","Clustering algorithms,
Classification algorithms,
Support vector machines,
Education,
Databases,
Machine learning algorithms,
Learning systems"
An Evaluation of Machine Learning Methods to Detect Malicious SCADA Communications,"Critical infrastructure Supervisory Control and Data Acquisition (SCADA) systems have been designed to operate on closed, proprietary networks where a malicious insider posed the greatest threat potential. The centralization of control and the movement towards open systems and standards has improved the efficiency of industrial control, but has also exposed legacy SCADA systems to security threats that they were not designed to mitigate. This work explores the viability of machine learning methods in detecting the new threat scenarios of command and data injection. Similar to network intrusion detection systems in the cyber security domain, the command and control communications in a critical infrastructure setting are monitored, and vetted against examples of benign and malicious command traffic, in order to identify potential attack events. Multiple learning methods are evaluated using a dataset of Remote Terminal Unit communications, which included both normal operations and instances of command and data injection attack scenarios.","Pipelines,
Learning systems,
Telemetry,
Intrusion detection,
SCADA systems,
Machine learning algorithms"
Virtual Metrology in Semiconductor Manufacturing by Means of Predictive Machine Learning Models,"Advanced Process Control (APC) is an important research area in Semiconductor Manufacturing (SM) to improve process stability crucial for product quality. In low-volume-high-mixture fabrication plants (fabs), Knowledge Discovery in Databases is extremely challenging due to complex technology mixtures and reduced availability of data for comparable process steps. High Density Plasma Chemical Vapor Deposition (HDP CVD) appears to be a process area in SM predestinated for application of Data Mining (DM). Enhancing physical metrology by predictive models leads to smart future fabs. Actual research focuses on Virtual Metrology (VM) using high sophisticated Machine Learning (ML) methods to model unknown functional interrelations and to predict the thickness of dielectric layers deposited onto a metallization layer of the manufactured wafers. Decision Trees (DT), Neural Networks (NN) and Support Vector Regression (SVR) have been investigated to maximize the accuracy of the regression. For data of various logistical granularities promising results have been achieved by implementing these statistical models.","Metrology,
Accuracy,
Predictive models,
Artificial neural networks,
Process control,
Semiconductor device measurement,
Manufacturing"
Machine Learning Techniques for LV Wall Motion Classification Based on Spatio-temporal Profiles from Cardiac Cine MRI,"In this paper, we propose an automated method to classify normal/abnormal wall motion in Left Ventricle (LV) function in cardiac cine-Magnetic Resonance Imaging (MRI). Without the need of pre-processing and by exploiting all the images of a cardiac cycle, spatio-temporal profiles are extracted from a subset of diametrical lines crossing opposites segments of the ventricular cavity. Two machine learning techniques are adapted and tested. The first one, is based on classical Support Vector Machines (SVM) and the second one that is proposed is based on dictionary learning (DL), adapted for classification in a supervised learning fashion. The experiments are evaluated based on features extracted from gray levels of the spatio-temporal profile as well as their representations in other basis such as Fourier and Wavelet domains under the assumption that the data may be sparse in one of those domains. The best classification performance has been obtained with a three-level db4 2-Dimensional wavelet transform using Fisher Discriminative Dictionary Learning as technique of classification.","Dictionaries,
Support vector machines,
Image segmentation,
Kernel,
Vectors,
Training,
Wavelet domain"
Learning Finite-State Machines: Conserving Fitness Function Evaluations by Marking Used Transitions,"This paper is dedicated to the problem of learning finite-state machines (FSMs), which plays a key role in automata-based programming. Metaheuristic algorithms commonly applied to this problem often use FSM mutations (small changes in the FSM structure) for solution construction. Most of them do not employ the specifics of FSMs in their work. We propose a new simple method for improving performance of these algorithms. The basic idea is to mark those transitions of FSMs that were used during fitness evaluation. Then, if a FSM mutation changes a transition that was not used in fitness evaluation, the fitness function value need not be calculated for the mutated FSM. This observation allows to conserve fitness evaluations, which often have high computational costs. The proposed method has been incorporated into several traditional and recent FSM learning algorithms based on evolutionary strategies, genetic algorithms and ant colony optimization. Experimental results are reported showing that the new method significantly improves performance of two methods based on evolutionary strategies and ant colony optimization.","Programming,
Ant colony optimization,
Sociology,
Statistics,
Algorithm design and analysis,
Tuning,
Genetic algorithms"
Machine Learning Techniques Applied to Sensor Data Correction in Building Technologies,"Since commercial and residential buildings account for nearly half of the United States' energy consumption, making them more energy-efficient is a vital part of the nation's overall energy strategy. Sensors play an important role in this research by collecting data needed to analyze performance of components, systems, and whole-buildings. Given this reliance on sensors, ensuring that sensor data are valid is a crucial problem. The solution we are researching is machine learning techniques, namely: artificial neural networks and Bayesian Networks. Types of data investigated in this study are: (1) temperature, (2) humidity, (3) refrigerator energy consumption, (4) heat pump liquid pressure, and (5) water flow. These data are taken from Oak Ridge National Laboratory's (ORNL) ZEBRAlliance research project which is composed of four single-family homes in Oak Ridge, TN. Results show that for the temperature, humidity, pressure, and flow sensors, data can mostly be predicted with root-mean-square error of less than 10% of the respective sensor's mean value. Results for the energy sensor were not as good, root-mean-square errors were centered about 100% of the mean value and were often well above 200%. Bayesian networks had smaller errors, but took substantially longer to train.","Temperature sensors,
Robot sensing systems,
Bayes methods,
Humidity,
Liquids,
Buildings,
Refrigerators"
Automatic Grading of Computer Programs: A Machine Learning Approach,"The automatic evaluation of computer programs is a nascent area of research with a potential for large-scale impact. Extant program assessment systems score mostly based on the number of test-cases passed, providing no insight into the competency of the programmer. In this paper, we present a machine learning framework to automatically grade computer programs. We propose a set of highly-informative features, derived from the abstract representations of a given program, that capture the program's functionality. These features are then used to learn a model to grade the programs, which are built against evaluations done by experts on the basis of a rubric. We show that regression modeling based on the given features provide much better grading than the ubiquitous test-case-pass based grading and rivals the grading accuracy of other open-response problems such as essay grading. We also show that our novel features add significant value over and above basic keyword/expression count features. In addition to this, we propose a novel way of posing computer-program grading as a one-class modeling problem. Our preliminary investigations in the same show promising results and suggest an implicit correlation of our features with the proposed grading-levels (rubric). To the best of the authors' knowledge, this is the first time machine learning has been applied to the problem of grading programs. The work is timely with regard to the recent boom in Massively Online Open Courseware (MOOCs), which promises to produce a significant amount of hand-graded digitized data.","Programming,
Context,
Feature extraction,
Computers,
Abstracts,
Measurement,
Grammar"
Applying Machine Learning and Audio Analysis Techniques to Insect Recognition in Intelligent Traps,"Throughout the history, insects have had an intimate relationship with humanity, both positive and negative. Insects are vectors of diseases that kill millions of people every year and, at the same time, insects pollinate most of the world's food production. Consequently, there is a demand for new devices able to control the populations of harmful insects while having a minimal impact on beneficial insects. In this paper, we present an intelligent trap that uses a laser sensor to selectively classify and catch insects. We perform an extensive evaluation of different feature sets from audio analysis and machine learning algorithms to construct accurate classifiers for the insect classification task. Support Vector Machines achieved the best results with a MFCC feature set, which consists of coefficients from frequencies scaled according to the human auditory system. We evaluate our classifiers in multiclass and binary class settings, and show that a binary class classifier that recognizes the mosquito species achieved almost perfect accuracy, assuring the applicability of the proposed intelligent trap.","Insects,
Support vector machines,
Accuracy,
Mel frequency cepstral coefficient,
Radio frequency,
Feature extraction,
Vectors"
Automated Shmoo data analysis: A machine learning approach,"In silicon testing, a Shmoo plot is commonly used to give us an insight into the silicon manufacturing development health. Shmoo plots and other silicon characterization data has high value, however, analysis of them is a time-consuming work. This paper establishes a machine learning based model to improve and automate the procedure in silicon data analysis for HVM test content development. Our experiment shows that the supervised learning model has good accuracy on VMIN estimation across various kinds of Shmoo issues (crack/sprinkle/ceiling). The accuracy attained is greatly improved over previous tools. The framework can be easily integrated into any automated tester software and would save time to market during first silicon characterization. Additionally, the methodology discussed in this work can be extended to the HVM test flow for silicon behavior.","Classification algorithms,
Algorithm design and analysis,
Decision trees,
Silicon,
Training,
Accuracy,
Prediction algorithms"
Pedestrian detection using heuristic statistics and machine learning,"Pedestrian detection is an important research field in advanced driver assistance system (ADAS). This paper puts forward a pedestrian detection framework based on both heuristic statistics and machine learning. First, a restriction of region of interest (ROI) is set on the captured image. Second, the template matching coarsely detects candidate pedestrians by using a set of template images, the edge image of the current frame, and the difference image from previous and current frames. Next, the histogram analysis again roughly filters out the candidate pedestrians. Finally, Histogram of Oriented Gradients (HOG) combined with library support vector machine (LIBSVM) is used to verify those candidate pedestrians. The experimental results show that the proposed method can run in real-time, where the false negative rate is 1.43%, and the false positive rate is 0.16%.","Image edge detection,
Histograms,
Roads,
Vehicles,
Cameras,
Accidents,
Training data"
Statistical Features Identification for Sentiment Analysis Using Machine Learning Techniques,"Due to increasing fascinating trend of using internet and online social media, user-generated contents are growing exponentially on the Web, containing users' opinion on various products. In this paper, we have proposed a sentiment analysis system which combines rule-based and machine learning approaches to identify feature-opinion pairs and their polarity. The efficiency of the proposed system is established through experimentation over customer reviews on different electronic products.","Feature extraction,
Context,
Data mining,
Semantics,
Noise measurement,
Computational linguistics,
Mutual information"
An examination of TNM staging of melanoma by a machine learning algorithm,"Accurate estimation of mortality in patients with cancer is important when discussing prognosis and selecting treatment. Survival estimation for many cancers is based on Tumor-Node-Metastasis (TNM) staging systems that involve three factors: tumor extent, lymph node involvement, and metastasis. The most recent clinical staging of melanoma uses TNM staging but does not include a growing number of other prognostic features. The Ensemble Algorithm of Clustering of Cancer Data (EACCD) by Chen et al. is a machine learning algorithm that regroups patients with different prognostic factors according to the survival dissimilarity. This algorithm has the potential to integrate emerging prognostic factors to more accurately stage melanoma. In this study, we use EACCD to examine the current AJCC staging of melanoma by analyzing a melanoma dataset from the National Cancer Centers Surveillance, Epidemiology, and End Rresults (SEER) database. Our results demonstrates that the EACCD algorithm generates results in-line with AJCC staging and may provide a mechanism to incorporate other prognostic factors to produce a more nuanced estimation of prognosis and survival.","Malignant tumors,
Metastasis,
Clustering algorithms,
Lymph nodes,
Prognostics and health management"
Machine learning techniques for data mining: A survey,"Data mining (DM) is a most popular knowledge acquisition method for knowledge discovery. Classification is one of the data mining (machining learning) technique that maps the data into the predefined class and group's. It is used to predict group membership for data instance. There are many areas that adapt Data Mining techniques such as medical, marketing, telecommunications, and stock, health care and so on. This paper presents the various classification techniques including decision tree, Support vector Machine, Nearest Neighbor etc. This survey provides a comparative Analysis of various classification algorithms.","support vector machines,
data mining,
decision trees,
learning (artificial intelligence),
pattern classification"
Personality Traits Identification Using Rough Sets Based Machine Learning,"Prediction of human behavior from his/her traits has long been sought by cognitive scientists. Human traits are often embedded in one's writings. Although some work has been done on identification of traits from essays, very little work can be found on extracting personality traits from written texts. Psychological studies suggest that extraction and prediction of rules from a data has been long pursued, and several methods have been proposed. In the present work we used Rough sets to extract the rules for prediction of personality traits. Rough Set is a comparatively recent method that has been effective in various fields such as medical, geological and other fields where intelligent decision making is required. Our experiments with rough sets in predicting personality traits produced encouraging results.","Rough sets,
Feature extraction,
Accuracy,
Approximation methods,
Psychology,
Pragmatics,
Data mining"
Using machine learning to identify benign cases with non-definitive biopsy,"When mammography reveals a suspicious finding, a core needle biopsy is usually recommended. In 5% to 15% of these cases, the biopsy diagnosis is non-definitive and a more invasive surgical excisional biopsy is recommended to confirm a diagnosis. The majority of these cases will ultimately be proven benign. The use of excisional biopsy for diagnosis negatively impacts patient quality of life and increases costs to the healthcare system. In this work, we employ a multi-relational machine learning approach to predict when a patient with a non-definitive core needle biopsy diagnosis need not undergo an excisional biopsy procedure because the risk of malignancy is low.","Heating,
Medical services,
Biomedical imaging"
Anomaly Detection in Sensor Systems Using Lightweight Machine Learning,"The maturing field of Wireless Sensor Networks (WSN) results in long-lived deployments that produce large amounts of sensor data. Lightweight online on-mote processing may improve the usage of their limited resources, such as energy, by transmitting only unexpected sensor data (anomalies). We detect anomalies by analyzing sensor reading predictions from a linear model. We use Recursive Least Squares (RLS) to estimate the model parameters, because for large datasets the standard Linear Least Squares Estimation (LLSE) is not resource friendly. We evaluate the use of fixed-point RLS with adaptive thresholding, and its application to anomaly detection in embedded systems. We present an extensive experimental campaign on generated and real-world datasets, with floating-point RLS, LLSE, and a rule-based method as benchmarks. The methods are evaluated on prediction accuracy of the models, and on detection of anomalies, which are injected in the generated dataset. The experimental results show that the proposed algorithm is comparable, in terms of prediction accuracy and detection performance, to the other LS methods. However, fixed-point RLS is efficiently implement able in embedded devices. The presented method enables online on-mote anomaly detection with results comparable to offline LS methods.","Wireless sensor networks,
Prediction algorithms,
Noise,
Adaptation models,
Least squares approximations,
Predictive models,
Embedded systems"
Automated Plant Disease Analysis (APDA): Performance Comparison of Machine Learning Techniques,"Plant disease analysis is one of the critical tasks in the field of agriculture. Automatic identification and classification of plant diseases can be supportive to agriculture yield maximization. In this paper we compare performance of several Machine Learning techniques for identifying and classifying plant disease patterns from leaf images. A three-phase framework has been implemented for this purpose. First, image segmentation is performed to identify the diseased regions. Then, features are extracted from segmented regions using standard feature extraction techniques. These features are then used for classification into disease type. Experimental results indicate that our proposed technique is significantly better than other techniques used for Plant Disease Identification and Support Vector Machines outperforms other techniques for classification of diseases.","Diseases,
Feature extraction,
Image segmentation,
Support vector machines,
Discrete cosine transforms,
Discrete wavelet transforms,
Accuracy"
Behaviour analysis of machine learning algorithms for detecting P2P botnets,"Botnets have emerged as a powerful threat on the Internet as it is being used to carry out cybercrimes. In this paper, we have analysed some machine learning techniques to detect peer to peer (P2P) botnets. As the detection of P2P botnets is widely unexplored area, we have focused on it. We experimented with different machine learning (ML) algorithms to compare their ability to classify the botnet traffic from the normal traffic by selecting distinguishing features of the network traffic. Experiments are performed on the dataset containing the traces of various P2P botnets. Results and tradeoffs obtained of different ML algorithms on different metrics are presented at the end of the paper.","Training,
Classification algorithms,
Testing,
Algorithm design and analysis,
Data mining,
Feature extraction,
Niobium"
Board-Level Functional Fault Diagnosis Using Multikernel Support Vector Machines and Incremental Learning,"Advanced machine learning techniques offer an unprecedented opportunity to increase the accuracy of board-level functional fault diagnosis and reduce product cost through successful repair. Ambiguous or incorrect diagnosis results lead to long debug times and even wrong repair actions, which significantly increase repair cost. We propose a smart diagnosis method based on multikernel support vector machines (MK-SVMs) and incremental learning. The MK-SVM method leverages a linear combination of single kernels to achieve accurate faulty-component classification based on the errors observed. The MK-SVMs thus generated can also be updated based on incremental learning, which allows the diagnosis system to quickly adapt to new error observations and provide even more accurate fault diagnosis. Two complex boards from industry, currently in volume production, are used to validate the proposed diagnosis approach in terms of diagnosis accuracy (success rate) and quantifiable improvements over previously proposed machine-learning methods based on several single-kernel SVMs and artificial neural networks.","Kernel,
Support vector machines,
Fault diagnosis,
Training,
Maintenance engineering,
Accuracy,
Circuit faults"
Risk stratification for Arrhythmic Sudden Cardiac Death in heart failure patients using machine learning techniques,"Arrhythmic Sudden Cardiac Death (SCD) is still a major clinical challenge even though much research has been done in the field. Machine learning techniques give a powerful tool for stratifying arrhythmic risk. We analyzed 40 Holter recordings from heart failure patients, 20 of which were characterized as high arrhythmia risk after 16 months follow up. The two groups (high and low risk) were not statistically different in basic clinical characteristics. We performed windowed analysis and computed 25 Heart Rate Variability (HRV) indices. We fed these indices as input to two classifiers: Support Vector Machines (SVM) and Random Forests (RF). The classification results showed that the automatic classification of the two groups of subjects is possible.","Heart rate variability,
Support vector machines,
Vegetation,
Educational institutions,
Accuracy,
Indexes"
A machine learning regularization of the inverse problem in electrocardiography imaging,"Radio-frequency ablation is one of the most efficient treatments of atrial fibrillation. The idea behind it is to stop the propagation of ectopic beats coming from the pulmonary vein and the abnormal conduction pathways. Medical doctors need to use invasive catheters to localize the position of the triggers and they have to decide where to ablate during the intervention. ElectroCardioGraphy Imaging (ECGI) provides the opportunity to reconstruct the electrical potential and activation maps on the heart surface and analyze data prior to the intervention. The mathematical problem behind the reconstruction of heart potential is known to be ill posed. In this study we propose to regularize the inverse problem with a statistically reconstructed heart potential, and we test the method on synthetically data produced using an ECG simulator.","Electric potential,
Inverse problems,
Electrocardiography,
Heart,
Torso,
Mathematical model,
Biological system modeling"
Supervised machine learning scheme for tri-axial accelerometer-based fall detector,"Fall events can cause trauma, disability and death among older people. Accelerometer-based devices are able to detect falls in controlled environments. The paper presents a computationally low-power approach for feature extraction and supervised clustering for people fall detection by using a 3-axial MEMS wearable accelerometer, managed by an stand-alone PC through ZigBee connection. The paper extends a previous work in which fall events were detected according to a threshold-based scheme. The proposed approach allows to generalize the detection of falls in several practical conditions, after a short period of calibration. The clustering scheme appears invariant to age, weight, height of people and relative positioning area (even in the upper part of the waist), overcoming the drawbacks of well-known threshold-based approaches in which several parameters need to be manually estimated, according to the specific features of the end-user. In order to limit the workload, the specific study on posture analysis has been avoided and a polynomial kernel function is used while maintaining high performance in terms of specificity and sensitivity. The supervised clustering step is achieved by implementing an One-Class Support Vector Machine classifier.","Acceleration,
Feature extraction,
Support vector machines,
Calibration,
Accelerometers,
Kernel,
Polynomials"
About analysis and robust classification of searchlight fMRI-data using machine learning classifiers,"In the present paper we investigate the analysis of functional magnetic resonance image (fMRI) data based on voxel response analysis. All voxels in local spatial area (volume) of a considered voxel form its so-called searchlight. The searchlight for a presented task is taken as a complex pattern. Task dependent discriminant analysis of voxel is then performed by assessment of the discrimination behavior of the respective searchlight pattern for a given task. Classification analysis of these patterns is usually done using linear support vector machines (linSVMs) as a machine learning approach or another statistical classifier like linear discriminant classifier. The test classification accuracy determining the task sensitivity is interpreted as the discrimination ability of the related voxel. However, frequently, the number of voxels contributing to a searchlight is much larger than the number of available pattern samples in classification learning, i.e. the dimensionality of patterns is higher than the number of samples. Therefore, the respective underlying mathematical classification problem has not an unique solution such that a certain solution obtained by the machine learning classifier contains arbitrary (random) components. For this situation, the generalization ability of the classifier may drop down. We propose in this paper another data processing approach to reduce this problem. In particular, we reformulate the classification problem within the searchlight. Doing so, we avoid the dimensionality problem: We obtain a mathematically well-defined classification problem, such that generalization ability of a trained classifier is kept high. Hence, a better stability of the task discrimination is obtained. Additionally, we propose the utilization of generalized learning vector quantizers as an alternative machine learning classifier system compared to SVMs, to improve further the stability of the classifier model due to decreased model complexity.","Accuracy,
Vectors,
Training,
Support vector machines,
Data models,
Standards,
Complexity theory"
Machine learning to predict extubation outcome in premature infants,"Though treatment of the ventilated premature infant has experienced many advances over the past decades, determining the best time point for extubation of these infants remains challenging and the incidence of extubation failures largely unchanged. The objective was to provide clinicians with a decision-support tool to determine whether to extubate a mechanically ventilated premature infant by using a set of machine learning algorithms on a dataset assembled from 486 premature infants receiving mechanical ventilation. Algorithms included artificial neural networks (ANN), support vector machine (SVM), naïve Bayesian classifier (NBC), boosted decision trees (BDT), and multivariable logistic regression (MLR). Results for ANN, MLR, and NBC were satisfactory (area under the curve [AUC]: 0.63-0.76); however, SVM and BDT consistently showed poor performance (AUC ~0.5). Complex medical data such as the data set used for this study require further preprocessing steps before prediction models can be developed that achieve similar or better performance than clinicians.","Artificial neural networks,
Input variables,
Prediction algorithms,
Pediatrics,
Support vector machines,
Machine learning algorithms,
Classification algorithms"
Music classification using extreme learning machines,"Over the last years, automatic music classification has become a standard benchmark problem in the machine learning community. This is partly due to its inherent difficulty, and also to the impact that a fully automated classification system can have in a commercial application. In this paper we test the efficiency of a relatively new learning tool, Extreme Learning Machines (ELM), for several classification tasks on publicly available song datasets. ELM is gaining increasing attention, due to its versatility and speed in adapting its internal parameters. Since both of these attributes are fundamental in music classification, ELM provides a good alternative to standard learning models. Our results support this claim, showing a sustained gain of ELM over a feedforward neural network architecture. In particular, ELM provides a great decrease in computational training time, and has always higher or comparable results in terms of efficiency.","Standards,
Neural networks,
Training,
Feature extraction,
Speech,
Signal processing,
Multiple signal classification"
Short-term vs. long-term analysis of diabetes data: Application of machine learning and data mining techniques,"Chronic care of diabetes comes with large amounts of data concerning the self- and clinical management of the disease. In this paper, we propose to treat that information from two different perspectives. Firstly, a predictive model of short-term glucose homeostasis relying on machine learning is presented with the aim of preventing hypoglycemic events and prolonged hyperglycemia on a daily basis. Second, data mining approaches are proposed as a tool for explaining and predicting the long-term glucose control and the incidence of diabetic complications.","Diabetes,
Sugar,
Data mining,
Predictive models,
Insulin,
Monitoring"
"Real-time road surface mapping using stereo matching, v-disparity and machine learning","We present and evaluate a computer vision approach for real-time mapping of traversable road surfaces ahead of an autonomous vehicle that relies only on a stereo camera. Our system first determines the camera position with respect to the ground plane using stereo vision algorithms and probabilistic methods, and then reprojects the camera raw image to a bidimensional grid map that represents the ground plane in world coordinates. After that, it generates a road surface grid map from the bidimensional grid map using an online trained pixel classifier based on mixture of Gaussians. Finally, to build a high quality map, each road surface grid map is integrated to a probabilistic bidimensional grid map using a binary Bayes filter for estimating the occupancy probability of each grid cell. We evaluated the performance of our approach for road surface mapping in comparison to manually classified images. Our experimental results show that our approach is able to correctly map regions at 50 m ahead of an autonomous vehicle, with True Positive Rate (TPR) of 90.32% for regions between 20 and 35 m ahead and False Positive Rate (FPR) not superior to 4.23% for any range.","Cameras,
Roads,
Stereo vision,
Sensors,
Image color analysis,
Real-time systems,
Mobile robots"
Machine Learning for Supporting Diagnosis of Amyotrophic Lateral Sclerosis Using Surface Electromyogram,"Needle electromyogram (EMG) is routinely used in clinical neurophysiology for examination of neuromuscular diseases. This study presents a noninvasive surface EMG method for supporting diagnosis of amyotrophic lateral sclerosis (ALS). Three diagnostic markers including the clustering index, the kurtosis of EMG amplitude histogram, and the kurtosis of EMG crossing-rate expansion, were used respectively to characterize surface EMG patterns recorded during different levels of voluntary muscle contraction. We then applied a linear discriminant analysis classifier to discriminate the ALS subjects from the neurologically intact subjects, using the statistics derived from all the three markers as input feature sets to the classifier. The method was tested in 10 ALS subjects and 11 neurologically intact subjects. Combination of the three surface EMG markers achieved 90% diagnostic sensitivity and 100% diagnostic specificity, which were higher than solely using a single surface EMG marker. Given the high diagnostic yield, the proposed surface EMG analysis can be used as a supplement to needle EMG examination in supporting the diagnosis of ALS.",
Keynote addresses: From auditory masking to binary classification: Machine learning for speech separation,"Summary form only given. Speech separation, or the cocktail party problem, is a widely acknowledged challenge. Part of the challenge stems from the confusion of what the computational goal should be. While the separation of every sound source in a mixture is considered the gold standard, I argue that such an objective is neither realistic nor what the human auditory system does. Motivated by the auditory masking phenomenon, we have suggested instead the ideal time-frequency binary mask as a main goal for computational auditory scene analysis. This leads to a new formulation to speech separation that classifies time-frequency units into two classes: those dominated by the target speech and the rest. In supervised learning, a paramount issue is generalization to conditions unseen during training. I describe novel methods to deal with the generalization issue where support vector machines (SVMs) are used to estimate the ideal binary mask. One method employs distribution fitting to adapt to unseen signal-to-noise ratios and iterative voice activity detection to adapt to unseen noises. Another method learns more linearly separable features using deep neural networks (DNNs) and then couples DNN and linear SVM for training on a variety of noisy conditions. Systematic evaluations show high quality separation in new acoustic environments.","Educational institutions,
Speech,
Biological neural networks,
Awards activities,
Conferences,
Acoustics,
Speech processing"
Intelligent speed profile prediction on urban traffic networks with machine learning,"Accurate prediction of traffic information such as flow, density, speed, and travel time is an important component for traffic control systems and optimizing vehicle operation. Prediction of an individual speed profile on an urban network is a challenging problem because traffic flow on urban routes is frequently interrupted and delayed by traffic lights, stop signs, and intersections. In this paper, we present an Intelligent Speed Profile Prediction on Urban Traffic Network (ISPP_UTN) that can predict a speed profile of a selected urban route with available traffic information at the trip starting time. ISPP_UTN consists of four speed prediction Neural Networks (NNs) that can predict speed in different traffic areas. ISPP_UTN takes inputs from three different categories of traffic information such as the historical individual driving data, geographical information, and traffic pattern data. Experimental results show that the proposed algorithm gave good prediction results on real traffic data and the predicted speed profiles are close to the real recorded speed profiles.","Artificial neural networks,
Vehicles,
Traffic control,
Testing,
Training,
Shape"
Keynote addresses: From auditory masking to binary classification: Machine learning for speech separation,"Speech separation, or the cocktail party problem, is a widely acknowledged challenge. Part of the challenge stems from the confusion of what the computational goal should be. While the separation of every sound source in a mixture is considered the gold standard, I argue that such an objective is neither realistic nor what the human auditory system does. Motivated by the auditory masking phenomenon, we have suggested instead the ideal time-frequency binary mask as a main goal for computational auditory scene analysis. This leads to a new formulation to speech separation that classifies time-frequency units into two classes: those dominated by the target speech and the rest. In supervised learning, a paramount issue is generalization to conditions unseen during training. I describe novel methods to deal with the generalization issue where support vector machines (SVMs) are used to estimate the ideal binary mask. One method employs distribution fitting to adapt to unseen signal-to-noise ratios and iterative voice activity detection to adapt to unseen noises. Another method learns more linearly separable features using deep neural networks (DNNs) and then couples DNN and linear SVM for training on a variety of noisy conditions. Systematic evaluations show high quality separation in new acoustic environments.",
Using machine learning techniques to detect metamorphic relations for programs without test oracles,"Much software lacks test oracles, which limits automated testing. Metamorphic testing is one proposed method for automating the testing process for programs without test oracles. Unfortunately, finding the appropriate metamorphic relations required for use in metamorphic testing remains a labor intensive task, which is generally performed by a domain expert or a programmer. In this work we present a novel approach for automatically predicting metamorphic relations using machine learning techniques. Our approach uses a set of features developed using the control flow graph of a function for predicting likely metamorphic relations. We show the effectiveness of our method using a set of real world functions often used in scientific applications.","Testing,
Arrays,
Predictive models,
Feature extraction,
Decision trees,
Support vector machines,
Software"
A random-forest-based efficient comparative machine learning predictive DNA-codon metagenomics binning technique for WMD events & applications,"In many counter-terrorism, or natural disasters, geographically distributed large scale sensor-based bio-chemicals agent or microorganisms target identification and prediction applications, such as in WMD events, as well as in many health care and medical applications, efficient large scale metagenomics is crucial for purpose such as rapid and timely decontamination for normal environment restoration, or rapid and timely discovery of the right drug/therapy for the injured individuals. Metagenomics is the study of all bio-chemical and organisms collected directly from large natural environments including geographically distributed disastrous ones. Most of these collected bio-chemical and organisms cannot be cultivated in a laboratory and hence cannot be sequenced as individual organisms. Thus, metagenomics methods allow relatively rapid sequencing of organism genomes obtained directly from a natural environment, and which cannot be cultured in a laboratory. Sequencing random fragments obtained from whole genome shotgun into taxa-based groups is known as binning. Currently, there are two different methods of binning: sequence similarity methods and sequence composition methods. Sequence similarity methods are usually based on sequence alignment to known genome like BLAST, or MEGAN. Sequence composition methods are based on compositional features of a given DNA sequence like K-mers, or other genomic signature(s) like TETRA, Phylopythia, CompostBin, likelyBin. In this paper we propose a machine learning predictive DNA sequence feature selection algorithms to solve binning problems. In our prior work we showed feature selection/reduction and binning prediction based on direct nucleotide k-mers. Here we use a combination of 2 Codons (amino acids) as features to differentiate between sequences. There are 20 different amino acids which are found proteins. The combination of 2 amino acids produce 400 features which we use to differentiate between the metagenomics sequence. The data reads used in this work has an average length of 250, 500, 1000, and 2000 base pairs. Experimental results of the codon-based feature reduction and binning prediction algorithms, namely using respectively a Random forest classifier and a Bayes classifier, are presented along with their comparison to their DNA-based k-mers counterparts. The proposed algorithm accuracy is tested on a variety of data sets and our findings show that the classification/prediction accuracy achieved is between 59%-92% for various data sets using Random forest classifier and 44%-64% using Naïve Bayes classifier. Random forest Classifier did better in classification in all the datasets compared to Naïve Bayes.","Accuracy,
Genomics,
DNA,
Amino acids,
Organisms,
Sequential analysis,
Testing"
Telecommunication subscribers' churn prediction model using machine learning,"During the last two decades, we have seen mobile communication becoming the dominant medium of communication. In numerous countries, especially the developed ones, the market is saturated to the extent that each new customer must be won over from the competitors. At the same time, public policies and standardization of mobile communication now allow customers to easily switch over from one carrier to another, resulting in a fluid market. Since the cost of winning a new customer is far greater than the cost of retaining an existing one, mobile carriers have now shifted their focus from customer acquisition to customer retention. As a result, churn prediction has emerged as the most crucial Business Intelligence (BI) application that aims at identifying customers who are about to transfer their business to a competitor i.e. to churn. This paper aims to present commonly used data mining techniques for the identification of customers who are about to churn. Based on historical data, these methods try to find patterns which can identify possible churners. Some of the well-known algorithms used during this research are Regression analysis, Decision Trees and Artificial Neural Networks (ANNs). The data set used in this study was obtained from Customer DNA website. It contains traffic data of 106,000 customers and their usage behavior for 3 months. We also discuss the use of re-sampling method in order to solve the problem of class imbalance. Our results show that in case of the data set used, decision trees is the most accurate classifier algorithm while identifying potential churners.","Decision trees,
Prediction algorithms,
Logistics,
Predictive models,
Correlation,
Linear regression"
Machine Learning Based Knowledge Acquisition on Spectrum Usage for LTE Femtocells,"The decentralised and ad hoc nature of femtocell deployments calls for distributed learning strategies to mitigate interference. We propose a distributed spectrum awareness scheme for femtocell networks, based on combined payoff and strategy reinforcement learning (RL) models. We present two different learning strategies, based on modifications to the Bush Mosteller (BM) RL and the Roth-Erev RL algorithms. The simulation results show the convergence behaviour of the learning strategies under a dynamic robust game. As compared to the Bush Mosteller (BM) RL, our modified BM (MBM) converges smoothly to a stable satisfactory solution. Moreover, the MBM significantly reduces the interference collision cost during the learning process. Both the MBM and the modified Roth-Erev (MRE) algorithms are stochastic-based learning strategies which require less computation than the gradient follower (GF) learning strategy and have the capability to escape from suboptimal solution.","Femtocells,
Games,
Interference,
Heuristic algorithms,
Macrocell networks,
Convergence,
Femtocell networks"
A Machine Learning and Wavelet-Based Fault Location Method for Hybrid Transmission Lines,"This paper presents a single-ended traveling wave-based fault location method for a hybrid transmission line: an overhead line combined with an underground cable. Discrete wavelet transformation (DWT) is used to extract transient information from the measured voltages. Support vector machine (SVM) classifiers are utilized to identify the faulty-section and faulty-half. Bewley diagrams are observed for the traveling wave patterns and the wavelet coefficients of the aerial mode voltage are used to locate the fault. The transient simulation for different fault types and locations are obtained by ATP using frequency-dependent line and cable models. MATLAB is used to process the simulated transients and apply the proposed method. The performance of the method is tested for different fault inception angles (FIA), different fault resistances, non-linear high impedance faults (NLHIF), and non-ideal faults with satisfactory results. The impact of cable aging on the proposed method accuracy is also investigated.","Fault location,
Support vector machines,
Power cables,
Transient analysis,
Transmission line measurements"
An Efficient Method for Antenna Design Optimization Based on Evolutionary Computation and Machine Learning Techniques,"In recent years, various methods from the evolutionary computation (EC) field have been applied to electromagnetic (EM) design problems and have shown promising results. However, due to the high computational cost of the EM simulations, the efficiency of directly using evolutionary algorithms is often very low (e.g., several weeks' optimization time), which limits the application of these methods for many industrial applications. To address this problem, a new method, called surrogate model assisted differential evolution for antenna synthesis (SADEA), is presented in this paper. The key ideas are: (1) A Gaussian Process (GP) surrogate model is constructed on-line to predict the performances of the candidate designs, saving a lot of computationally expensive EM simulations. (2) A novel surrogate model-aware evolutionary search mechanism is proposed, directing effective global search even when a traditional high-quality surrogate model is not available. Three complex antennas and two mathematical benchmark problems are selected as examples. Compared with the widely used differential evolution and particle swarm optimization, SADEA can obtain comparable results, but achieves a 3 to 7 times speed enhancement for antenna design optimization.","Computational modeling,
Antennas,
Optimization,
Mathematical model,
Training data,
Predictive models,
Databases"
Meta-learning for large scale machine learning with MapReduce,"We have entered the big data age. Knowledge extraction from massive data is becoming more and more rewarding and urgent. MapReduce has provided a feasible framework for programming machine learning algorithms in Map and Reduce functions. The relatively simple programming interface has helped to solve machine learning algorithms' scalability problems. However, this framework suffers from an obvious weakness: it does not support iterations. This makes those algorithms requiring iterations difficult to fully explore the efficiency of MapReduce. In this paper, we propose to apply Meta-learning programmed with MapReduce to avoid parallelizing machine learning algorithms while also improving their scalability to big datasets. The experiments conducted on Hadoop fully distributed mode on Amazon EC2 demonstrate that our algorithm PML reduces the training computational complexity significantly when the number of computing nodes increases while gaining smaller error rates than those on one single node. The comparison of PML with the contemporary parallelized AdaBoost algorithm: AdaBoost.PL shows that PML has lower error rates.","Machine learning algorithms,
Error analysis,
Training,
Classification algorithms,
Training data,
Computational modeling,
Algorithm design and analysis"
Intelligent tuning for microwave filters based on multi-kernel machine learning model,"This paper presents an intelligent alignment method for the automatic tuning device of microwave filters. In the method, a model that reveals the relationships between the tunable screws and filter electrical performance is firstly developed by using an improved machine learning algorithm which can incorporate multi-kernel into the traditional linear programming support vector regression to improve the modeling accuracy. Then, a tuning procedure of filters is constructed by using the developed machine-learning model. Finally, some experiments are carried out, and the experimental results confirm the effectiveness of the method. The approach is particularly suited to the computer-aided tuning devices or an automatic tuning robot of volume-producing filters.","Microwave filters,
Tuning,
Fasteners,
Couplings,
Kernel,
Support vector machines,
Linear programming"
Learning from multiple data sets with different missing attributes and privacy policies: Parallel distributed fuzzy genetics-based machine learning approach,"This paper discusses parallel distributed genetics-based machine learning (GBML) of fuzzy rule-based classifiers from multiple data sets. We assume that each data set has a similar but different set of attributes. In other words, each data set has different missing attributes. Our task is the design of a fuzzy rule-based classifier from those data sets. In this paper, we first show that fuzzy rules can handle missing attributes easily. Next we explain how parallel distributed fuzzy GBML can handle multiple data sets with different missing attributes. Then we examine the accuracy of obtained fuzzy rule-based classifiers from various settings of available training data such as a single data set with no missing attribute and multiple data sets with many missing attributes. Experimental results show that the use of multiple data sets often increases the accuracy of obtained fuzzy rule-based classifiers even when they have missing attributes. We also discuss the learning from a data set under a severe privacy preserving policy where only the error rate of each candidate classifier is available. It is assumed that no information about each individual pattern is available. This means that we cannot use any information on the class label or the attribute values of each pattern. We explain how such a black-box data set can be utilized for classifier design.","Training data,
Classification algorithms,
Sociology,
Statistics,
Distributed databases,
Data privacy,
Fuzzy sets"
Rapid Deployment for Machine Learning in Educational Cloud,"In the cloud era, the acquisition of new cloud skills is a constant requirement of IT specialists. Educational organizations such as universities have a need to provide educational cloud curriculums for their students. In our current research, we are constructing a private cloud based on super-saturation, which is defined as the allocation of a much greater amount of logical resources than physical resources. Super-saturated clouds therefore realize up to 10 times more running instances than conventional clouds. While the performance of super-saturated clouds decreases somewhat compared with conventional clouds, their costs also greatly decrease. Moreover, in the post-cloud era, i.e., the big data era, data scientists will be increasingly required to process big data in the cloud. Mahout and Hadoop are two popular tools used in the fields of data science and machine learning. However, a certain level of skill is required to build such machine learning systems, and because it takes a long time to build such systems, the curriculums available to learners are limited. In this paper, we propose a method of rapid deployment for machine learning systems in the educational cloud. We show that our proposed method can reduce the required preparation time.","Information management,
Data handling,
Data storage systems,
Cloud computing,
Conferences"
Power-Aware Multi-data Center Management Using Machine Learning,"The cloud relies upon multi-data center (multi-DC) infrastructures distributed along the world, where people and enterprises pay for resources to offer their web-services to worldwide clients. Intelligent management is required to automate and manage these infrastructures, as the amount of resources and data to manage exceeds the capacities of human operators. Also, it must take into account the cost of running the resources (energy) and the quality of service towards web-services and clients. (De-)consolidation and priming proximity to clients become two main strategies to allocate resources and properly place these web-services in the multi-DC network. Here we present a mathematical model to describe the scheduling problem given web-services and hosts across a multi-DC system, enhancing the decision makers with models for the system behavior obtained using machine learning. After running the system on real DC infrastructures we see that the model drives web-services to the best locations given quality of service, energy consumption, and client proximity, also (de-)consolidating according to the resources required for each web-service given its load.","Quality of service,
Time factors,
Mathematical model,
Monitoring,
Predictive models,
Energy consumption,
Measurement"
Applications of the extension theory in machine learning field,This paper presents a new type of genetic algorithm of environment exploration artificial intelligence automata based on various elements of Extenics theory.,"Telecommunications,
Learning automata,
Extremities"
Flow-Based P2P Network Traffic Classification Using Machine Learning,"With the introduction of new and new services in the market every day, the internet is growing rapidly. The network traffic generated by these network protocols and applications needs to be categorised which is an important task of network management. Among these, p2p has the largest share of the bandwidth. This great demand in the bandwidth has increased the importance of network traffic engineering. So, in order to meet the current demand and develop new architectures which help in improving the network performance, a broad understanding of the network traffic properties is required. The flow based methods classify p2p and non-p2p traffic using the characteristics of flows on the internet. In this paper, Naïve Bayes estimator is used to categorize the traffic into p2p and non-p2p. Our results show that with the right set of features and good training data, high level of accuracy is achievable with the simplest of Naïve Bayes estimator.","Accuracy,
Machine learning algorithms,
Classification algorithms,
Ports (Computers),
Bayes methods,
Peer-to-peer computing,
Telecommunication traffic"
PANFIS: A Novel Incremental Learning Machine,"Most of the dynamics in real-world systems are compiled by shifts and drifts, which are uneasy to be overcome by omnipresent neuro-fuzzy systems. Nonetheless, learning in nonstationary environment entails a system owning high degree of flexibility capable of assembling its rule base autonomously according to the degree of nonlinearity contained in the system. In practice, the rule growing and pruning are carried out merely benefiting from a small snapshot of the complete training data to truncate the computational load and memory demand to the low level. An exposure of a novel algorithm, namely parsimonious network based on fuzzy inference system (PANFIS), is to this end presented herein. PANFIS can commence its learning process from scratch with an empty rule base. The fuzzy rules can be stitched up and expelled by virtue of statistical contributions of the fuzzy rules and injected datum afterward. Identical fuzzy sets may be alluded and blended to be one fuzzy set as a pursuit of a transparent rule base escalating human's interpretability. The learning and modeling performances of the proposed PANFIS are numerically validated using several benchmark problems from real-world or synthetic datasets. The validation includes comparisons with state-of-the-art evolving neuro-fuzzy methods and showcases that our new method can compete and in some cases even outperform these approaches in terms of predictive fidelity and model complexity.","Training,
Fuzzy sets,
Ellipsoids,
Fuzzy systems,
Covariance matrices,
Fuzzy logic,
Vectors"
Computation using mismatch: Neuromorphic extreme learning machines,"In this paper, we describe a low power neuromorphic machine learner that utilizes device mismatch prevalent in today's VLSI processes to perform a significant part of the computation while a digital back end enables precision in the final output. The particular machine learning algorithm we use is extreme learning machine (ELM). Mismatch in silicon spiking neurons and synapses are used to perform the vector-matrix multiplication (VMM) that forms the first stage of this classifier and is the most computationally intensive. System simulations are presented to evaluate the dependence of performance (in a classification and a regression task) on analog and digital parameters like weight resolution, maximum spike frequency etc. SPICE simulations show that the proposed implementation is ≈ 92X more energy efficient as opposed to custom digital implementations for a classification task with 100 dimensional inputs. Measurement results for a regression task from a field programmable analog array (FPAA) fabricated in 0.35μm CMOS are presented as a proof of concept.","Neurons,
Mirrors,
Very large scale integration,
Computational modeling,
Transistors,
Hardware,
Biomedical measurement"
On the Application of Supervised Machine Learning to Trustworthiness Assessment,"State-of-the art trust and reputation systems seek to apply machine learning methods to overcome generalizability issues of experience-based Bayesian trust assessment. These approaches are, however, often model-centric instead of focussing on data and the complex adaptive system that is driven by reputation-based service selection. This entails the risk of unrealistic model assumptions. We outline the requirements for robust probabilistic trust assessment using supervised learning and apply a selection of estimators to a real-world dataset, in order to show the effectiveness of supervised methods. Furthermore, we provide a representational mapping of estimator output to a belief logic representation for the modular integration of supervised methods with other trust assessment methodologies.","Predictive models,
Computational modeling,
Data models,
Estimation,
Bayes methods,
Vegetation,
Aggregates"
Wireless Tomography in Noisy Environments Using Machine Learning,"This paper, one in a continuing series, describes a new initiative in wireless tomography. Our goal is to combine two technologies: wireless communication and radio frequency tomography, for the close-in remote sensing. The hybrid system, including wireless communication devices for wireless tomography is proposed in this paper. Noise reduction, modified standard phase reconstruction, and imaging are exploited sequentially to perform wireless tomography in noisy environments. The performance given in this paper illustrates the significance and prospect of wireless tomography. The contributions of this paper are threefold: 1) the hybrid system provides a strong and flexible infrastructure for wireless tomography; 2) machine learning, especially nonlinear dimensionality reduction, is explored to execute noise reduction and combat the nonlinear noise effect; and 3) modified standard phase reconstruction is well achieved using the de-noised amplitude-only total fields from the simple sensors and the received accurate full-data total fields from the advanced sensors. Experimental data provided by the Institute Fresnel in Marseille, France are used to demonstrate the concept of wireless tomography and validate the corresponding algorithms.","Tomography,
Wireless communication,
Sensors,
Wireless sensor networks,
Noise reduction,
Kernel,
Image reconstruction"
Machine learning approaches for predicting software maintainability: a fuzzy-based transparent model,"Software quality is one of the most important factors for assessing the global competitive position of any software company. Thus, the quantification of the quality parameters and integrating them into the quality models is very essential.Many attempts have been made to precisely quantify the software quality parameters using various models such as Boehm's Model, McCall's Model and ISO/IEC 9126 Quality Model. A major challenge, although, is that effective quality models should consider two types of knowledge: imprecise linguistic knowledge from the experts and precise numerical knowledge from historical data.Incorporating the experts' knowledge poses a constraint on the quality model; the model has to be transparent.In this study, the authorspropose a process for developing fuzzy logic-based transparent quality prediction models.They applied the process to a case study where Mamdani fuzzy inference engine is used to predict software maintainability.Theycompared the Mamdani-based model with other machine learning approaches.The resultsshow that the Mamdani-based model is superior to all.","software quality,
computational linguistics,
DP industry,
fuzzy logic,
fuzzy reasoning,
learning (artificial intelligence),
software maintenance"
Machines that learn and teach seamlessly,"This paper describes an investigation into creating agents that can learn how to perform a task by observing an expert, then seamlessly turn around and teach the same task to a less proficient person. These agents are taught through observation of expert performance and thereafter refined through unsupervised practice of the task, all on a simulated environment. A less proficient human is subsequently taught by the now-trained agent through a third approach-coaching, executed through a haptic device. This approach addresses tasks that involve complex psychomotor skills. A machine-learning algorithm called PIGEON is used to teach the agents. A prototype is built and then tested on a task involving the manipulation of a crane to move large container boxes in a simulated shipyard. Two evaluations were performed-a proficiency test and a learning rate test. These tests were designed to determine whether this approach improves the human learning more than self-experimentation by the human. While the test results do not conclusively show that our approach provides improvement over self-learning, some positive aspects of the results suggest great potential for this approach.","Haptic interfaces,
Machine learning,
Adaptation models,
Computer graphics,
Real-time systems"
Data mining with machine learning applied for email deception,"Spam is also known as junk mail or Unsolicited Commercial Email (UCE) which has become major problem for the sustainability of the internet and global commerce. Everyday millions of the spam mails are sent over internet to targeted population to advertise services, products and dangerous software etc. A number of spam detection algorithms have been proposed to classify emails on content based, but could not gain accuracy. Our proposed work mainly focuses on cognitive (spam) words for classification. This feature is sequential unique and closed patterns which are extracted from the message content. We show that this feature have good impact in classifying spam from legitimate messages. Our method, which can be easily implemented, compares amiably with respect to popular algorithms, like Logistic Regression, Neural Network, Naive Bayes and Random Forest using polynomial kernel as filter. We outperform the accuracy higher compared to related methods. In addition our method is resilient against irrelevant and bothersome words.","Postal services,
Unsolicited electronic mail,
Accuracy,
Filtering algorithms,
Classification algorithms,
Polynomials"
An Analysis of Machine Learning Algorithms for Condensing Reverse Engineered Class Diagrams,"There is a range of techniques available to reverse engineer software designs from source code. However, these approaches generate highly detailed representations. The condensing of reverse engineered representations into more high-level design information would enhance the understandability of reverse engineered diagrams. This paper describes an automated approach for condensing reverse engineered diagrams into diagrams that look as if they are constructed as forward designed UML models. To this end, we propose a machine learning approach. The training set of this approach consists of a set of forward designed UML class diagrams and reverse engineered class diagrams (for the same system). Based on this training set, the method 'learns' to select the key classes for inclusion in the class diagrams. In this paper, we study a set of nine classification algorithms from the machine learning community and evaluate which algorithms perform best for predicting the key classes in a class diagram.","Prediction algorithms,
Algorithm design and analysis,
Unified modeling language,
Measurement,
Machine learning algorithms,
Software,
Couplings"
Analysis of Android malware detection performance using machine learning classifiers,"As mobile devices have supported various services and contents, much personal information such as private SMS messages, bank account information, etc. is scattered in mobile devices. Thus, attackers extend the attack range not only to the existing environment of PC and Internet, but also to the mobile device. Previous studies evaluated the malware detection performance of machine learning classifiers through collecting and analyzing event, system call, and log information generated in Android mobile devices. However, monitoring of unnecessary features without understanding Android architecture and malware characteristics generates resource consumption overhead of Android devices and low ratio of malware detection. In this paper, we propose new feature sets which solve the problem of previous studies in mobile malware detection and analyze the malware detection performance of machine learning classifiers.","Malware,
Smart phones,
Performance evaluation,
Monitoring,
Feature extraction,
Androids,
Humanoid robots"
Browserbite: Accurate Cross-Browser Testing via Machine Learning over Image Features,"Cross-browser compatibility testing is a time consuming and monotonous task. In its most manual form, Web testers open Web pages one-by-one on multiple browser-platform combinations and visually compare the resulting page renderings. Automated cross-browser testing tools speed up this process by extracting screenshots and applying image processing techniques so as to highlight potential incompatibilities. However, these systems suffer from insufficient accuracy, primarily due to a large percentage of false positives. Improving accuracy in this context is challenging as the criteria for classifying a difference as an incompatibility are to some extent subjective. We present our experience building a cross-browser testing tool (Browser bite) based on image segmentation and differencing in conjunction with machine learning. An experimental evaluation involving a dataset of 140 pages, each rendered in 14 browser-system combinations, shows that the use of machine learning in this context leads to significant accuracy improvement, allowing us to attain an F-score of over 90%.","Testing,
Web pages,
Neurons,
Biological neural networks,
Classification tree analysis,
Accuracy"
Hardware specialization of machine-learning kernels: Possibilities for applications and possibilities for the platform design space (Invited),"This paper considers two challenging trends affecting low-power sensing systems: (1) the applications of interest increasingly involve embedded signals that are very complex to analyze; and (2) the platforms themselves face elevating constraints in terms of energy and possibly cost. Motivated by the complexities of analyzing the application signals, we emphasize the benefits of data-driven approaches. Most notably, these approaches are based on machine learning, as opposed to traditional DSP. We consider how the algorithms lend themselves to specialized signal-analysis platforms. Hardware specialization is well-regarded as an approach to address issues of computational efficiency, performance, and capacity, thus playing a key role in leveraging Moore's Law. However, we describe how hardware specialization of machine-learning kernels, this time with an explicit focus on error resilience, can also play a powerful role in enabling system-wide fault tolerance, thereby aiding Moore's Law on another dimension.","Kernel,
Hardware,
Data models,
Brain modeling,
Feature extraction,
Computer architecture,
Training"
Code Smell Detection: Towards a Machine Learning-Based Approach,"Several code smells detection tools have been developed providing different results, because smells can be subjectively interpreted and hence detected in different ways. Usually the detection techniques are based on the computation of different kinds of metrics, and other aspects related to the domain of the system under analysis, its size and other design features are not taken into account. In this paper we propose an approach we are studying based on machine learning techniques. We outline some common problems faced for smells detection and we describe the different steps of our approach and the algorithms we use for the classification.","Measurement,
Machine learning algorithms,
Detectors,
Software,
Conferences,
Labeling,
Accuracy"
Improving Statistical Approach for Memory Leak Detection Using Machine Learning,"Memory leaks are major problems in all kinds of applications, depleting their performance, even if they run on platforms with automatic memory management, such as Java Virtual Machine. In addition, memory leaks contribute to software aging, increasing the complexity of software maintenance. So far memory leak detection was considered to be a part of development process, rather than part of software maintenance. To detect slow memory leaks as a part of quality assurance process or in production environments statistical approach for memory leak detection was implemented and deployed in a commercial tool called Plumbr. It showed promising results in terms of leak detection precision and recall, however, even better detection quality was desired. To achieve this improvement goal, classification algorithms were applied to the statistical data, which was gathered from customer environments where Plumbr was deployed. This paper presents the challenges which had to be solved, method that was used to generate features for supervised learning and the results of the corresponding experiments.","Resource management,
Leak detection,
Training,
Software,
Java,
Aging,
Memory management"
A machine learning-based faulty line identification for smart distribution network,This paper presents a machine learning-based faulty-line identification method in smart distribution networks. The proposed method utilizes postfault root-mean-square (rms) values of voltages measured at the main substation and at selected nodes as well as fault information obtained by fault current identifiers (FCIs) and intelligent electronic re-closers (IE-CRs). The information from FCIs and IE-RCs are first used to identify the faulty region in the network. The normalized rms values of voltages are then utilized as the input to the support vector machine (SVM) classifiers to identify the faulty-line according to the pre-determined fault type. The IEEE 123-node distribution test system is simulated in ATP software. MATLAB is used to process the simulated transients and to apply the proposed method. The performance of the method is tested for different fault inception angles (FIA) and different fault resistances with satisfactory results.,"Fault location,
Support vector machines,
Voltage measurement,
Accuracy,
Substations,
Kernel"
Interactive Machine Learning in Data Exploitation,"The goal of interactive machine learning is to help scientists and engineers exploit more specialized data from within their deployed environment in less time, with greater accuracy and fewer costs. A basic introduction to the main components is provided here, untangling the many ideas that must be combined to produce practical interactive learning systems. This article also describes recent developments in machine learning that have significantly advanced the theoretical and practical foundations for the next generation of interactive tools.","Machine learning,
Interactive systems,
Image segmentation,
Vocabulary,
Learning systems,
Random variables,
Data processing,
Information processing"
Revisiting Computational Thermodynamics through Machine Learning of High-Dimensional Data,"A new perspective on alloy thermodynamics computation uses data-driven analysis and machine learning for the design and discovery of materials. The focus is on an integrated machine-learning framework, coupling different genres of supervised and unsupervised informatics techniques, and bridging two distinct viewpoints: continuum representations based on solid solution thermodynamics and discrete high-dimensional elemental descriptions.","Informatics,
Machine learning,
Thermodynamics,
Principal component analysis,
Semiconductor materials,
Atomic measurements,
Computational modeling"
Climate Informatics: Accelerating Discovering in Climate Science with Machine Learning,"Given the impact of climate change, understanding the climate system is an international priority. The goal of climate informatics is to inspire collaboration between climate scientists and data scientists, in order to develop tools to analyze complex and ever-growing amounts of observed and simulated climate data, and thereby bridge the gap between data and understanding. Here, recent climate informatics work is presented, along with details of some of the remaining challenges.","Atmospheric measurements,
Machine learning,
Meteorology,
Climate change,
Informatics"
Dynamic Extreme Learning Machine and Its Approximation Capability,"Extreme learning machines (ELMs) have been proposed for generalized single-hidden-layer feedforward networks which need not be neuron alike and perform well in both regression and classification applications. The problem of determining the suitable network architectures is recognized to be crucial in the successful application of ELMs. This paper first proposes a dynamic ELM (D-ELM) where the hidden nodes can be recruited or deleted dynamically according to their significance to network performance, so that not only the parameters can be adjusted but also the architecture can be self-adapted simultaneously. Then, this paper proves in theory that such D-ELM using Lebesgue p-integrable hidden activation functions can approximate any Lebesgue p-integrable function on a compact input set. Simulation results obtained over various test problems demonstrate and verify that the proposed D-ELM does a good job reducing the network size while preserving good generalization performance.","Approximation methods,
Machine learning,
Computer architecture,
Linear systems,
Educational institutions,
Cybernetics,
Feedforward neural networks"
The Relevance Sample-Feature Machine: A Sparse Bayesian Learning Approach to Joint Feature-Sample Selection,"This paper introduces a novel sparse Bayesian machine-learning algorithm for embedded feature selection in classification tasks. Our proposed algorithm, called the relevance sample feature machine (RSFM), is able to simultaneously choose the relevance samples and also the relevance features for regression or classification problems. We propose a separable model in feature and sample domains. Adopting a Bayesian approach and using Gaussian priors, the learned model by RSFM is sparse in both sample and feature domains. The proposed algorithm is an extension of the standard RVM algorithm, which only opts for sparsity in the sample domain. Experimental comparisons on synthetic as well as benchmark data sets show that RSFM is successful in both feature selection (eliminating the irrelevant features) and accurate classification. The main advantages of our proposed algorithm are: less system complexity, better generalization and avoiding overfitting, and less computational cost during the testing stage.","Bayes methods,
Sampling methods,
Machine learning"
Utilisation of on-line machine learning for SCADA system alarms forecasting,"This paper describes a prototype design and implementation of a real-time (on-line) knowledge generation component which can be utilised in industrial Supervisory Control and Data Acquisition (SCADA) systems. The overall architecture of our SCADA scenario, which utilise proposed knowledge generation is based on a multi-agent approach. This design is different from what we can see in conventional commercial SCADA solutions. Nowadays, there is a big pressure on operators to precisely analyse a huge amount of data coming from technological processes and make right decisions in the right time. This is where a real-time knowledge generation can highly improve decision making strategies in complex industrial processes. Nevertheless, the actual state of the art solutions are usually not using the knowledge generation directly, or there are often restricted so called off-line learning approaches. The recent development in the area of machine learning lead to the creation of distributed solutions which could process real-time data and dynamically adapt the generated knowledge. We applied this on-line machine learning approach in our proposed prototype. The experimental agent is focused on the specific scenario of the process alarm forecasting, which is considered to be a binary classification problem. We describe our solution for useful classifier feature vector construction. The classifier itself is based on Passive-Aggressive algorithm. Furthermore, in order to evaluate a performance of the classification the results from knowledge generation experiments were provided in form of Matthews Correlation Coefficient (MCC) together with Receiver Operating Characteristic (ROC). The proposed prototype shows how to design and implement an on-line knowledge generation component for novel SCADA solutions.","Support vector machine classification,
Data models,
Forecasting,
Real-time systems,
Solid modeling,
Virtual manufacturing,
Training"
An adaptive prolog programming language with machine learning,"Prolog is a well-known logic programming language. A Prolog program is essentially a set of knowledge predicates. A query can be executed on the knowledge set by the Prolog engine, which searches and matches the query against the knowledge set automatically by conducting a depth-first search (DFS). While deterministic, DFS does not always produce the best efficiency in Prolog execution. UCT, based on UCB algorithms, is a machine learning algorithm for solving multi-stage Markov Decision Process (MDP) problems, with a good balance between exploitation and exploration. This paper introduces a UCB gauge for each of the predicates, which can be used as a heuristic measurement for selection of predicate search. This results in a best-first search strategy for Prolog execution, which is referred to as Adaptive Prolog. Adaptive Prolog enhance its execution engine by adjusting its search path to reflect current machine learning results, and as such produce better execution efficiency than traditional Prolog.","Machine learning algorithms,
Algorithm design and analysis,
Knowledge based systems,
Engines,
Motion pictures,
Logic programming,
Search problems"
FPGA prototype of machine learning analog-to-feature converter for event-based succinct representation of signals,"Sparse signal models with learned dictionaries of morphological features provide efficient codes in a variety of applications. Such models can be useful to reduce sensor data rates and simplify the communication, processing and analysis of information, provided that the algorithm can be realized in an efficient way and that the signal allows for sparse coding. In this paper we outline an FPGA prototype of a general purpose “analog-to-feature converter”, which learns an over-complete dictionary of features from the input signal using matching pursuit and a form of Hebbian learning. The resulting code is sparse, event-based and suitable for analysis with parallel and neuromorphic processors. We present results of two case studies. The first case is a blind source separation problem where features are learned from an artificial signal with known features. We demonstrate that the learned features are qualitatively consistent with the true features. In the second case, features are learned from ball-bearing vibration data. We find that vibration signals from bearings with faults have characteristic features and codes, and that the event-based code enable a reduction of the data rate by at least one order of magnitude.","Matching pursuit algorithms,
Field programmable gate arrays,
Dictionaries,
MATLAB,
Vibrations,
Encoding,
Noise"
Accelerated learning in machine learning-based resource allocation methods for Heterogenous Networks,"Heterogeneous Networks, such as those with Femtocells and Macrocell Basestations, face the task of resource allocation to ensure all users, both primary (mobile user) and secondary (femtocell user), receive assurances of quality of service. One method of performing this allocation, Q-learning, involves the use of a reward function (defining objectives) and a Q-table (storing policy information). This Q-table can be shared between users to speed up convergence on a policy ensuring a desired quality of service. In this paper, a reward function and state structure are presented and compared to another Q-learning reward function. The designed RF is shown to increase the sum femtocell user capacity in most scenarios while maintaining the desired quality of service for the mobile user. The sharing of Q-tables formed using th e designed reward function and state structure with nodes entering the network is shown to significantly speed up convergence in most scenarios when compared to convergence without sharing Q-tables.","Femtocells,
Radio frequency,
Interference,
Resource management,
Mathematical model,
Mobile communication,
Quality of service"
A Fault Classification and Localization Method for Three-Terminal Circuits Using Machine Learning,"This paper presents a traveling-wave-based method for fault classification and localization for three-terminal power transmission systems. In the proposed method, the discrete wavelet transform is utilized to extract transient information from the recorded voltages. Support-vector-machine classifiers are then used to classify the fault type and faulty line/half in the transmission networks. Bewley diagrams are observed for the traveling-wave patterns and the wavelet coefficients of the aerial mode voltage are used to locate the fault. Alternate Transients Program software is used for transients simulations. The performance of the method is tested for different fault inception angles, different fault resistances, nonlinear high impedance faults, and nontypical faults with satisfactory results.","Circuit faults,
Support vector machines,
Fault location,
Discrete wavelet transforms,
Machine learning,
Transient analysis,
Power transmission"
Osteoporosis risk prediction using machine learning and conventional methods,"A number of clinical decision tools for osteoporosis risk assessment have been developed to select postmenopausal women for the measurement of bone mineral density. We developed and validated machine learning models with the aim of more accurately identifying the risk of osteoporosis in postmenopausal women, and compared with the ability of a conventional clinical decision tool, osteoporosis self-assessment tool (OST). We collected medical records from Korean postmenopausal women based on the Korea National Health and Nutrition Surveys (KNHANES V-1). The training data set was used to construct models based on popular machine learning algorithms such as support vector machines (SVM), random forests (RF), artificial neural networks (ANN), and logistic regression (LR) based on various predictors associated with low bone density. The learning models were compared with OST. SVM had significantly better area under the curve (AUC) of the receiver operating characteristic (ROC) than ANN, LR, and OST. Validation on the test set showed that SVM predicted osteoporosis risk with an AUC of 0.827, accuracy of 76.7%, sensitivity of 77.8%, and specificity of 76.0%. We were the first to perform comparisons of the performance of osteoporosis prediction between the machine learning and conventional methods using population-based epidemiological data. The machine learning methods may be effective tools for identifying postmenopausal women at high risk for osteoporosis.","Osteoporosis,
Support vector machines,
Artificial neural networks,
Predictive models,
Radio frequency,
Accuracy,
Learning systems"
Increasing anomaly handling efficiency in large organizations using applied machine learning,"Maintenance costs can be substantial for large organizations (several hundreds of programmers) with very large and complex software systems. By large we mean lines of code in the range of hundreds of thousands or millions. Our research objective is to improve the process of handling anomaly reports for large organizations. Specifically, we are addressing the problem of the manual, laborious and time consuming process of assigning anomaly reports to the correct design teams and the related issue of localizing faults in the system architecture. In large organizations, with complex systems, this is particularly problematic because the receiver of an anomaly report may not have detailed knowledge of the whole system. As a consequence, anomaly reports may be assigned to the wrong team in the organization, causing delays and unnecessary work. We have so far developed two machine learning prototypes to validate our approach. The latest, a re-implementation and extension, of the first is being evaluated on four large systems at Ericsson AB. Our main goal is to investigate how large software development organizations can significantly improve development efficiency by replacing manual anomaly report assignment and fault localization with machine learning techniques. Our approach focuses on training machine learning systems on anomaly report databases; this is in contrast to many other approaches that are based on test case execution combined with program sampling and/or source code analysis.","Organizations,
Accuracy,
Semantics,
Databases,
Ontologies,
Routing,
Software"
Sparse fuzzy techniques improve machine learning,"On the example of diagnosing cancer based on the microarray gene expression data, we show that fuzzy-technique description of imprecise knowledge can improve the efficiency of the existing machine learning algorithms. Specifically, we show that the fuzzy-technique description leads to a formulation of the learning problem as a problem of sparse optimization, and we use l1-techniques to solve the resulting optimization problem.","Vectors,
Optimization,
Support vector machines,
Tumors,
Cancer,
Educational institutions,
Gene expression"
Automated method for extraction of lung tumors using a machine learning classifier with knowledge of radiation oncologists on data sets of planning CT and FDG-PET/CT images,"We have developed an automated method for extraction of lung tumors using a machine learning classifier with knowledge of radiation oncologists on data sets of treatment planning computed tomography (CT) and 18F-fluorodeoxyglucose (FDG)-positron emission tomography (PET)/CT images. First, the PET images were registered with the treatment planning CT images through the diagnostic CT images of PET/CT. Second, six voxel-based features including voxel values and magnitudes of image gradient vectors were derived from each voxel in the planning CT and PET /CT image data sets. Finally, lung tumors were extracted by using a support vector machine (SVM), which learned 6 voxel-based features inside and outside each true tumor region determined by radiation oncologists. The results showed that the average DSCs for 3 and 6 features for three cases were 0.744 and 0.899, and thus the SVM may need 6 features to learn the distinguishable characteristics. The proposed method may be useful for assisting treatment planners in delineation of the tumor region.","Computed tomography,
Positron emission tomography,
Tumors,
Planning,
Support vector machines,
Image segmentation,
Lungs"
Difficulties in choosing a single final classifier from non-dominated solutions in multiobjective fuzzy genetics-based machine learning,"A large number of non-dominated fuzzy rule-based classifiers are often obtained by applying a multiobjective fuzzy genetics-based machine learning (MoFGBML) algorithm to a pattern classification problem. The obtained set of non-dominated classifiers can be used to analyze their accuracy-interpretability tradeoff relation. One important issue, which has not been discussed in many studies on MoFGBML, is the choice of a single final classifier from a large number of non-dominated classifiers. The selected classifier is used for the classification of new input patterns. In this paper, we focus on this important research issue: classifier selection from a large number of non-dominated fuzzy rule-based classifiers. In general, it is not easy to choose a single final solution from non-dominated solutions in multiobjective optimization. This is because further information on the decision maker's preference is needed to choose the single final solution. In addition to this general difficulty in multiobjective optimization, MoFGBML has its own difficulty in classifier selection, which is the difference between training data accuracy and test data accuracy. While our true objective is to maximize the test data accuracy (i.e., classifier's generalization ability), only the training data accuracy is available for fitness evaluation and classifier selection. In this paper, we discuss why classifier selection is difficult in MoFGBML.","Accuracy,
Training data,
Error analysis,
Complexity theory,
Classification algorithms,
Search problems,
Fuzzy sets"
Machine learning approach to an otoneurological classification problem,"In this paper we applied altogether 13 classification methods to otoneurological disease classification. The main point was to use Half-Against-Half (HAH) architecture in classification. HAH structure was used with Support Vector Machines (SVMs), k-Nearest Neighbour (k-NN) method and Naïve Bayes (NB) methods. Furthermore, Multinomial Logistic Regression (MNLR) was tested for the dataset. HAH-SVM with the linear kernel achieved clearly the best accuracy being 76.9% which was a good result with the dataset tested. From the other classification methods HAH-k-NN with cityblock metric, HAH-NB and MNLR methods achieved above 60% accuracy. Around 77% accuracy is a good result compared to previous researches with the same dataset.","Kernel,
Diseases,
Support vector machines,
Accuracy,
Educational institutions,
Polynomials,
Niobium"
Hardware requirements of communication-centric machine learning algorithms,"Machine learning type neuromorphic algorithms have the potential to enable the brains behind small autonomous robots, provided these algorithms can be implemented energy efficiently. The implementation difficulties are mainly extremely large memory size and high memory bandwidth making Von Neumann computational model realizations inefficient. However, these algorithms are inherently error robust, which may be taken advantage of in the realizations. For example, in with low operating voltage stochastic hardware. To determine the hardware requirements for an Application Specific Integrated Circuit (ASIC) realization, an example algorithm, Hierarchical Temporal Memory (HTM), is realized here. A 64×64 HTM network with 1440 connections per elementary processing element requires 530 mm2 area, 340 Mb memory, and a 10MHz clock frequency in 65nm technology. With point-to-point connections these processing elements would have a communication radius of circa 50 elements and on-chip wideband technologies can increase the range further.","Microprocessors,
Algorithm design and analysis,
Hardware,
Neuromorphics,
Machine learning algorithms,
Memory management"
Mathematical models for machine learning and pattern recognition,"In this tutorial, we provide an in depth analysis of some important issues within the field of Machine Learning and Pattern Recognition. We intend to reflect recent developments and provide a comprehensive introduction to some fundamental issues pertaining to the field of machine learning and pattern recognition. We target advanced undergraduates or first year Ph.D. students as well as researchers and practitioners. The mathematical models covered during this tutorial include Machine Learning for Pattern Recognition, Hidden Markov Models and feature space Dimensionality Reduction. MATLAB projects are provided as experiments to the theory covered.","Decision support systems,
Conferences,
Signal processing"
Localizing and Comparing Weight Maps Generated from Linear Kernel Machine Learning Models,"Recently, machine learning models have been applied to neuroimaging data, allowing to make predictions about a variable of interest based on the pattern of activation or anatomy over a set of voxels. These pattern recognition based methods present undeniable assets over classical (univariate) techniques, by providing predictions for unseen data, as well as the weights of each voxel in the model. However, the obtained weight map cannot be thresholded to perform regionally specific inference, leading to a difficult localization of the variable of interest. In this work, we provide local averages of the weights according to regions defined by anatomical or functional atlases (e.g. Brodmann atlas). These averages can then be ranked, thereby providing a sorted list of regions that can be (to a certain extent) compared with univariate results. Furthermore, we defined a ""ranking distance"", allowing for the quantitative comparison between localized patterns. These concepts are illustrated with two datasets.","Computational modeling,
Predictive models,
Neuroimaging,
Biological system modeling,
Data models,
Support vector machines,
Accuracy"
Mapping the Internet: Geolocating Routers by Using Machine Learning,"Knowing the geolocation of a router can help to predict the geolocation of an Internet user, which is important for local advertising, fraud detection, and geo-fencing applications. For example, the geolocation of the last router on the path to a user is a reasonable guess for the user's geolocation. Current methods for geolocating a router are based on parsing a router's name to find geographic hints. Unfortunately, these methods are noisy and often provide no hints. This paper presents results on using machine learning methods to ""sharpen"" a router's noisy location based on the time delay between one or more routers and a target router or end user IP address. The novelty of this approach is that geolocation of the one or more routers is not required to be known.","Geology,
IP networks,
Computers,
Internet,
Training,
Noise measurement,
Clustering algorithms"
Balancing Clinical and Pathologic Relevance in the Machine Learning Diagnosis of Epilepsy,"The application of machine learning to epilepsy can be used both to develop clinically useful computer-aided diagnostic tools, and to reveal pathologically relevant insights into the disease. Such studies most frequently use neurologically normal patients as the control group to maximize the pathologic insight yielded from the model. This practice yields potentially inflated accuracy because the groups are quite dissimilar. A few manuscripts, however, opt to mimic the clinical comparison of epilepsy to non-epileptic seizures, an approach we believe to be more clinically realistic. In this manuscript, we describe the relative merits of each control group. We demonstrate that in our clinical quality FDG-PET database the performance achieved was similar using each control group. Based on these results, we find that the choice of control group likely does not hinder the reported performance. We argue that clinically applicable computer-aided diagnostic tools for epilepsy must directly address the clinical challenge of distinguishing patients with epilepsy from those with non-epileptic seizures.","Epilepsy,
Artificial neural networks,
Electroencephalography,
Accuracy,
Temporal lobe,
Biomedical imaging,
Diseases"
A no-reference machine learning based video quality predictor,"The growing need of quick and online estimation of video quality necessitates the study of new frontiers in the area of no-reference visual quality assessment. Bitstream-layer model based video quality predictors use certain visual quality relevant features from the encoded video bitstream to estimate the quality. Contemporary techniques vary in the number and nature of features employed and the use of prediction model. This paper proposes a prediction model with a concise set of bitstream based features and a machine learning based quality predictor. Several full reference quality metrics are predicted using the proposed model with reasonably good levels of accuracy, monotonicity and consistency.","Quality assessment,
Measurement,
Video recording,
Predictive models,
Artificial neural networks,
Encoding,
Video sequences"
A Database-Hadoop Hybrid Approach to Scalable Machine Learning,"There are two popular schools of thought for performing large-scale machine learning that does not fit into memory. One is to run machine learning within a relational database management system, and the other is to push analytical functions into MapReduce. As each approach has its own set of pros and cons, we propose a database-Hadoop hybrid approach to scalable machine learning where batch-learning is performed on the Hadoop platform, while incremental-learning is performed on PostgreSQL. We propose a purely relational approach that removes the scalability limitation of previous approaches based on user-defined aggregates and also discuss issues and resolutions in applying the proposed approach to Hadoop/Hive. Experimental evaluations of classification performance and training speed were conducted using a commercial advertisement dataset provided in the KDD Cup 2012, Track 2. The experimental results show that our scheme has competitive classification performance and superior training speed compared with state-of-the-art scalable machine learning frameworks, 5 and 7.65 times faster than Vow pal Wabbit and Bismarck, respectively, for a regression task.","Predictive models,
Training,
Machine learning algorithms,
Relational databases,
Aggregates,
Computational modeling"
Machine learning of engineering diagnostic knowledge from unstructured verbatim text descriptions,"This paper presents our research in text mining for discovering important engineering fault diagnostic knowledge from unstructured and verbatim text descriptions. In particular we focus on developing machine learning algorithms for detecting documents that contain descriptions of systematic failures and root causes to the faults. We developed a machine algorithm based on entropy analysis to extract an A-word list, a list of words that are important to characterize the documents of interests, a vector space model to represent features of important documents, and a constraint based k-means clustering algorithm to generate high purity clusters for use in detecting important documents. We applied the algorithms to automotive diagnostic text data, which are unstructured and verbatim descriptions by customers and technicians that contain many typos and self-invented terms. We were able to reduce a list of 2183 words to a list of 137 important words. The classification system generated by these machine learning algorithms showed high recall and accuracy in detecting important diagnostic descriptions.","Vehicles,
Entropy,
Clustering algorithms,
Machine learning algorithms,
Training data,
Vectors,
Automotive engineering"
Automotive diagnosis typo correction using domain knowledge and machine learning,"Text description of engineering diagnoses recorded during and after vehicle repair process plays an important role in root cause analyzing and vehicle maintenance. The fact that such text is unstructured, lack of grammar, has a lot of spelling errors and a large amount of self-invented domain specific terminologies introduces challenges and difficulties for automatic information retrieving and categorization. This paper presents our research in text mining in vehicle diagnostic applications. Specifically, an automatic typo correction system is proposed and implemented. We build multiple knowledge bases to detect and correct typos, and a neural network classifier to select good candidates for correcting typos. Experiment results show that our system outperforms state-of-art spell checking systems.","Vehicles,
Dictionaries,
Knowledge based systems,
Text processing,
Computational intelligence,
Text mining"
Machine learning based search space optimisation for drug discovery,"Drug discovery research has progressed to a place where it essentially counts on high performance computer systems and huge databases for its victory. As such, Virtual Screening (VS), a computationally intensive process, plays a major role in the systematic drug designing process for pressing diseases. Therefore, it is imperative that the VS process has to be made as fast as possible in order to efficiently dock the ligands from huge databases to a selected protein receptor, targeting a drug. The extremely high rate of increase of the number of ligands in the databases makes it impossible to tackle this problem only by improving the computing resources. Therefore, researchers work on an orthogonal technique, where they use soft computing to reduce the search space through identifying the ligands that are non-dockable, hence improving the throughput as a whole. Machine Learning (ML) can be used to train a binary classifier that can classify the ligands into two known classes: dockable and non-dockable ligands. In this paper, for the first time, we use three ML techniques (Support Vector Machines, Artificial Neural Networks and Random Forest) on a single problem domain (a Protease receptor of HIV) and evaluate the performance rendered by the respective models. We show that such classification improves the throughput by two folds with around 90% accuracy. In addition, we propose and use a technique for constructing a training set to be used for ML in VS applications in the instance of a non-synthesised receptor.","Training,
Drugs,
Support vector machines,
Accuracy,
Machine learning algorithms,
Databases,
Proteins"
User's utterance classification using machine learning for Arabic Conversational Agents,"This paper presents a novel technique for the classification of Arabic sentences as Dialogue Acts, based on structural information contained in Arabic function words. It focuses on classifying questions and non-questions utterances as they are used in Conversational Agents. The proposed technique extracts function words features by replacing them with numeric tokens and replacing each content word with a standard numeric token. The Decision Tree has been chosen for this work to extract the classification rules. Experiments provide evidence for highly effective classification. The extracted classification rules will be embedded into a Conversational Agent called ArabChat in order to classify Arabic utterances before further processing on these utterances. This paper presents a complement work for the ArabChat to improve its performance by differentiating among question-based and non question-based utterances.","Educational institutions,
Information technology,
Feature extraction,
Computer science,
Standards,
Decision trees,
Intelligent systems"
Applying machine learning classifiers to dynamic Android malware detection at scale,"The widespread adoption and contextually sensitive nature of smartphone devices has increased concerns over smartphone malware. Machine learning classifiers are a current method for detecting malicious applications on smartphone systems. This paper presents the evaluation of a number of existing classifiers, using a dataset containing thousands of real (i.e. not synthetic) applications. We also present our STREAM framework, which was developed to enable rapid large-scale validation of mobile malware machine learning classifiers.","Malware,
Vectors,
Testing,
Androids,
Humanoid robots,
Training,
Mobile communication"
Signal Processing and Machine Learning with Differential Privacy: Algorithms and Challenges for Continuous Data,"Private companies, government entities, and institutions such as hospitals routinely gather vast amounts of digitized personal information about the individuals who are their customers, clients, or patients. Much of this information is private or sensitive, and a key technological challenge for the future is how to design systems and processing techniques for drawing inferences from this large-scale data while maintaining the privacy and security of the data and individual identities. Individuals are often willing to share data, especially for purposes such as public health, but they expect that their identity or the fact of their participation will not be disclosed. In recent years, there have been a number of privacy models and privacy-preserving data analysis algorithms to answer these challenges. In this article, we will describe the progress made on differentially private machine learning and signal processing.","Privacy,
Data privacy,
Signal processing algorithms,
Approximation methods,
Noise measurement,
Computer security"
Diagnosis of Vertebral Column Disorders Using Machine Learning Classifiers,"Medical science is characterized by the correct diagnosis of a disease and its accurate classification to avoid complexities at treatment/medication stage. This is often accomplished by a physician based on experience without much signal processing aids. It is envisioned that a sophisticated and intelligent medical diagnostic/classification system may be helpful in making right decisions especially at remote areas where specialist physicians are not present. With this in mind, this paper proposes diagnosis and classification of vertebral column disorders using machine learning classifiers including feed forward back propagation neural network, generalized regression neural network and support vector machine and evaluates their performance. The dataset is collected using the information from the magnetic resonance images (MRI) and is classified into three different classes which are disk hernia, spondylolisthesis and normal. The classifiers are trained using 50% ratio and 10 fold cross validation approaches and are comprehensively evaluated with different architectures, activation and kernel functions. Experimental results demonstrate that feed forward back propagation neural network is 93.87% accurate on unknown test cases and performs better than the other methods.","Support vector machines,
Training,
Artificial neural networks,
Neurons,
Feeds,
Accuracy"
Machine Learning-Based Software Quality Prediction Models: State of the Art,"Quantification of parameters affecting the software quality is one of the important aspects of research in the field of software engineering. In this paper, we present a comprehensive literature survey of prominent quality molding studies. The survey addresses two views: (1) quantification of parameters affecting the software quality; and (2) using machine learning techniques in predicting the software quality. The paper concludes that, model transparency is a common shortcoming to all the surveyed studies.","Software quality,
Object oriented modeling,
Predictive models,
Fuzzy logic,
Software engineering,
Biological system modeling"
Machine learning for attack vector identification in malicious source code,"As computers and information technologies become ubiquitous throughout society, the security of our networks and information technologies is a growing concern. As a result, many researchers have become interested in the security domain. Among them, there is growing interest in observing hacker communities for early detection of developing security threats and trends. Research in this area has often reported hackers openly sharing cybercriminal assets and knowledge with one another. In particular, the sharing of raw malware source code files has been documented in past work. Unfortunately, malware code documentation appears often times to be missing, incomplete, or written in a language foreign to researchers. Thus, analysis of such source files embedded within hacker communities has been limited. Here we utilize a subset of popular machine learning methodologies for the automated analysis of malware source code files. Specifically, we explore genetic algorithms to resolve questions related to feature selection within the context of malware analysis. Next, we utilize two common classification algorithms to test selected features for identification of malware attack vectors. Results suggest promising direction in utilizing such techniques to help with the automated analysis of malware source code.","Malware,
Communities,
Computer hacking,
Biological cells,
Accuracy,
Support vector machine classification"
Behavior recognition based on machine learning algorithms for a wireless canine machine interface,"Training and handling working dogs is a costly process and requires specialized skills and techniques. Less subjective and lower-cost training techniques would not only improve our partnership with these dogs but also enable us to benefit from their skills more efficiently. To facilitate this, we are developing a canine body-area-network (cBAN) to combine sensing technologies and computational modeling to provide handlers with a more accurate interpretation for dog training. As the first step of this, we used inertial measurement units (IMU) to remotely detect the behavioral activity of canines. Decision tree classifiers and Hidden Markov Models were used to detect static postures (sitting, standing, lying down, standing on two legs and eating off the ground) and dynamic activities (walking, climbing stairs and walking down a ramp) based on the heuristic features of the accelerometer and gyroscope data provided by the wireless sensing system deployed on a canine vest. Data was collected from 6 Labrador Retrievers and a Kai Ken. The analysis of IMU location and orientation helped to achieve high classification accuracies for static and dynamic activity recognition.","Hidden Markov models,
Sensors,
Accelerometers,
Dogs,
Legged locomotion,
Gyroscopes,
Accuracy"
Microblogging sentiment analysis with lexical based and machine learning approaches,"The Digital World encounters rapid development nowadays, especially through the proliferation of social media in Indonesia. Twitter has become one of social media with expanded users within every sectors of society. There are so many part both individual as well as organization/enterprise which utilize twitter as tool for communication, business, customer relation, and other activities. Through the twitter's ever-expanding users with those particular purposes, the precise method to effectively and efficiently analyzing opinion-contained sentences become crucially needed. Therefore this research made for method analyzing through lexical based and model based approaches by machine learning to classify opinion-contained tweets using those 2 methods. The tested machine learning method are Support Vector Machine (SVM), Maximum Entropy (ME), Multinomial Naive Bayes (MNB), and k-Nearest Neighbor (k-NN). Based on the test outcome, lexical based approach highly depended on lexical database which became opinion classification matrix. Whilst machine learning approach can produce better accuracy due to its capability in new training data modeling based on outcome model. However, machine learning model based approach depends on various factors in analyzing sentiment.","Support vector machines,
Databases,
Accuracy,
Data mining,
Data models,
Entropy,
Communications technology"
A statistical machine learning approach for ticket mining in IT service delivery,"Ticketing is a fundamental management process of IT service delivery. Customers typically express their requests in the form of tickets related to problems or configuration changes of existing systems. Tickets contain a wealth of information which, when connected with other sources of information such as asset and configuration information, monitoring information, can yield new insights that would otherwise be impossible to gain from one isolated source. Linking these various sources of information requires a common key shared by these data sources. The key is the server names. Unfortunately, due to historical as well as practical reasons, the server names are not always present in the tickets as a standalone field. Rather, they are embedded in unstructured text fields such as abstract and descriptions. Thus, automatically identifying server names in tickets is a crucial step in linking various information sources. In this paper, we present a statistical machine learning method called Conditional Random Field (CRF) that can automatically identify server names in tickets with high accuracy and robustness. We then illustrate how such linkages can be leveraged to create new business insights.","Servers,
Dictionaries,
Data models,
Training data,
Business,
Training,
Robustness"
A Survey on Machine-Learning Techniques in Cognitive Radios,"In this survey paper, we characterize the learning problem in cognitive radios (CRs) and state the importance of artificial intelligence in achieving real cognitive communications systems. We review various learning problems that have been studied in the context of CRs classifying them under two main categories: Decision-making and feature classification. Decision-making is responsible for determining policies and decision rules for CRs while feature classification permits identifying and classifying different observation models. The learning algorithms encountered are categorized as either supervised or unsupervised algorithms. We describe in detail several challenging learning issues that arise in cognitive radio networks (CRNs), in particular in non-Markovian environments and decentralized networks, and present possible solution methods to address them. We discuss similarities and differences among the presented algorithms and identify the conditions under which each of the techniques may be applied.","Sensors,
Machine learning,
Radio frequency,
Cognition,
Cognitive radio,
Unsupervised learning"
Predicting cloud resource provisioning using machine learning techniques,"In order to meet Service Level Agreement (SLA) requirements, Virtual Machine (VM) resources must be provisioned few minutes ahead due to the VM boot-up time. One way to do this is by predicting future resource demands. In this research, we have developed and evaluated cloud client prediction models for TPCW benchmark web application using three machine learning techniques: Support Vector Machine (SVM), Neural Networks (NN) and Linear Regression (LR). We included the SLA metrics for Response Time and Throughput to the prediction model with the aim of providing the client with a more robust scaling decision choice. Our results show that Support Vector Machine provides the best prediction model.","Predictive models,
Support vector machines,
Measurement,
Time factors,
Artificial neural networks,
Throughput,
Linear regression"
A machine learning approach for generating temporal logic classifications of complex model behaviours,"Systems biology aims to facilitate the understanding of complex interactions between components in biological systems. Petri nets (PN), and in particular Coloured Petri Nets (CPN) have been demonstrated to be a suitable formalism for modelling biological systems and building computational models over multiple spatial and temporal scales. To explore the complex and high-dimensional solution space over the behaviours generated by such models, we propose a clustering methodology which combines principal component analysis (PCA), distance similarity and density factors through the application of DBScan. To facilitate the interpretation of clustering results and enable further analysis using model checking we apply a pattern mining approach aimed at generating high-level classificatory descriptions of the clusters' behaviour in temporal logic. We illustrate the power of our approach through the analysis of two case studies: multiple knockdown of the Mitogen-activated protein-kinase (MAPK) pathway, and selective knockout of Planar Cell Polarity (PCP) signalling in Drosophila wing.","Time series analysis,
Biological system modeling,
Clustering algorithms,
Mathematical model,
Computational modeling,
Analytical models,
Principal component analysis"
Work in progress: A machine learning approach for assessment and prediction of teamwork effectiveness in software engineering education,"One of the challenges in effective software engineering (SE) education is the lack of objective assessment methods of how well student teams learn the critically needed teamwork practices, defined as the ability: (i) to learn and effectively apply SE processes in a teamwork setting, and (ii) to work as a team to develop satisfactory software (SW) products. In addition, there are no effective methods for predicting learning effectiveness in order to enable early intervention in the classroom. Most of the current approaches to assess achievement of SE teamwork skills rely solely on qualitative and subjective data taken as surveys at the end of the class and analyzed only with very rudimentary data analysis. In this paper we present a novel approach to address the assessment and prediction of student learning of teamwork effectiveness in software engineering education based on: a) extracting only objective and quantitative student team activity data during their team class project; b) pairing these data with related independent observations and grading of student team effectiveness in SE process and SE product components in order to create “training database” and c) applying a machine learning (ML) approach, namely random forest classification (RF), to the above training database in order to create ML models, ranked factors and rules that can both explain (e.g. assess) as well as provide prediction of the student teamwork effectiveness. These student team activity data are being collected in joint and already established (since 2006) SE classes at San Francisco State University (SFSU), Florida Atlantic University (FAU) and Fulda University, Germany (Fulda), from approximately 80 students each year, working in about 15 teams, both local and global (with students from multiple schools).","Teamwork,
Software engineering,
Machine learning,
Educational institutions,
Training,
Software"
An incremental learning algorithm for improved least squares twin support vector machine,"In this paper, we mainly propose an incremental version of improved least squares twin support vector machine (IILSTSVM), based on inverse matrix-free method. This algorithm can meet the requirement of online learning to update the existing model. In the case of low dimension data, this method effectively improves training speed of incremental learning. According to updating inverse matrix, we can implement the incremental learning for ILSTSVM. Experiments prove that this algorithm has excellent performance on runtime and recognition rate in the low dimensional space.","Support vector machines,
Classification algorithms,
Training,
Machine learning,
Accuracy,
Algorithm design and analysis,
Approximation algorithms"
Retinal blood vessel segmentation using an Extreme Learning Machine approach,"Diabetic retinopathy is a vascular disorder caused by changes in the blood vessels of the retina. The proposed work uses an Extreme Learning Machine (ELM) approach for blood vessel detection in digital retinal images. This approach is based on pixel classification using a 7-D feature vector obtained from preprocessed retinal images and given as input to an ELM. Classification results categorizes each pixel into two classes namely vessel and non-vessel. Finally, post processing is done to fill pixel gaps in detected blood vessels and removes falsely-detected isolated vessel pixels. The proposed technique was assessed on the publicly available DRIVE and STARE datasets. The approach proves vessel detection is accurate for both datasets.","Blood vessels,
Biomedical imaging,
Retina,
Feature extraction,
Machine learning,
Databases,
Image segmentation"
A machine learning system for human-in-the-loop video surveillance,"We propose a novel human-in-the-loop surveillance system that continuously learns the properties of objects that are interesting for a human operator. The interesting objects are automatically learned by tracking the eye gaze positions of the operator while he or she monitors the surveillance video. The system automatically detects interesting objects in the surveillance video and forms a new synthetic video that contains interesting objects at earlier positions in the time dimension. The operator always views this synthetically formed video which makes manual video retrieval tasks more convenient. Sensitivity to operator interests and interest changes are other major advantages. We tested our system both on synthetic and real videos, which are provided as supplementary materials [1]. The results show the effectiveness of the proposed system.","Surveillance,
Humans,
Feature extraction,
Streaming media,
Training,
Cameras"
Keynote speaker [Machine learning for assistive technology],"Summary form only given, as follows. Assistive technologies (AT) are created to help people with disabilities participate more effectively and independently in all aspects of society. Simple assistive technologies would be crutches and manual wheelchairs. Complex assistive technologies include smart homes, navigation systems, prompting systems and brain-computer interfaces for numerous activities including wheelchair control. There is an increasing trend to use machine learning as a key component in a variety of assistive technology applications. In this talk we will survey current ML applications for AT, discuss the challenges that this application domain presents and discuss future research possibilities.",
The Training Set Selection Methods of microRNA Precursors Prediction Based on Machine Learning Approaches,"Micro RNAs (miRNAs) are single-stranded, endogenous ~22nt small non-coding RNAs (sncRNAs) that can play important regulatory roles in animals and plants by targeting mRNA for cleavage or translational repression. miRNAs which have very low expression levels or are expressed at specific stage are difficult to find by biological experiments. Also biological experiment only can find a small amount of miRNAs. Computational approaches have become another important way of miRNA prediction, especially machine learning approaches. miRNA prediction based on machine learning approaches requires a lot of positive and negative samples. The number of miRNA precursors that are experimentally validated is rare. However, the number of the sequence fragments, which are similar to real miRNA precursors in whole genome, is up to millions and millions. It is important to select reasonable samples for constructing high-performance classifier. In this review, the training set samples used for predicting miRNA precursors based on machine learning approaches are summarized.",Intelligent systems
Machine learning algorithms for data categorization and analysis in communication,"Machine learning and pattern recognition contains well-defined algorithms with the help of complex data, provides the accuracy of the traffic levels, heavy traffic hours within a cluster. In this paper the base stations and also the noise levels in the busy hour can be predicted. 348 pruned tree contains 23 nodes with busy traffic hour provided in east Godavari. Signal to noise ratio has been predicted at 55, based on CART results. About 53% instances provided inside the cluster and 47% provided outside the cluster. DBScan clustering provided maximum noise from srikakulam. MOR (Number of originating calls successful) predicted as best associated attribute based on Apriori and Genetic search 12:1 ratio.","Data mining,
Machine learning,
Clustering algorithms,
Mobile communication,
Algorithm design and analysis,
Classification algorithms,
Prediction algorithms"
Accurate Prediction of Response to Interferon-based Therapy in Egyptian Patients with Chronic Hepatitis C Using Machine-learning Approaches,"Hepatitis C virus' patients with genotypes 1 & 4 have break-even response rates to Pegylated-Interferon (Peg-IFN) and Ribavirin (RBV) treatment. Furthermore, the incompliance to the treatment because of its high cost and related unfavorable effects makes its prediction of paramount importance. By using machine-learning techniques, a significantly accurate predictive model constructed to predict Egyptian patients' response based on their clinical and biochemical data. The model uses Artificial Neural Networks (ANN) and Decision Trees (DT) to achieve this goal. Two-hundred patients treated with Peg-IFN and RBV, 83 responders (41%), and 117 non-responders (59%) retrospectively studied to extract informative features and train the Neural Networks and Decision Trees. Optimization was done by using six different Neural Network architectures, starting with an input layer of 12 neurons, a hidden layer of 70 to 180 neurons and an output layer containing a single neuron. For decision Trees (DTs), the CART classification algorithm was used. Six DTs with two classes, pruning levels of 9, 11, 13, and 17, and nodes from 45 to 61 were investigated. Among the 12 features in the study, the most statistically significant informative features were the patient's Histology activity index, fibrosis, viral-load, Alfa-feta protein and albumin. Validation of the models on a 20% test set was then performed. The best and average accuracy for the ANN and DT models were 0.76 and 0.69, and 0.80 and 0.72 respectively. Sensitivity and specificity were 0.95 and 0.39, and 0.89 and 0.78 respectively. We conclude that decision trees gave a higher accuracy in predicting response, and would help in proper therapy options for patients.","Artificial neural networks,
Medical treatment,
Decision trees,
Accuracy,
Training,
Sensitivity and specificity,
Mathematical model"
A novel human hand finger gesture recognition using machine learning,Human-Computer Interaction (HCI) using intelligent artificial computing interface is a fast emerging and revolutionary field of study of computer vision. This present study is concerned with making computers responsive to human gestures and postures. In this paper a simple alternative method for hand gesture recognition system has been proposed. The system takes various fingers postures and try to recognize them using machine learning. A pattern of gestures is trained and tested to show the results using linear artificial neural network.,"Thumb,
Testing,
Phase locked loops"
A method for fetal assessment using data mining and machine learning,"If a woman is pregnant, it is important for both her and her doctor/clinician to be aware if there are problems with the developing fetus. There are currently ways to discover problems using both noninvasive and invasive techniques. The University of Arkansas for Medical Sciences (UAMS) has recently developed a noninvasive system called the Squid Array for Reproductive Assessment (SARA) that can be used to gather fetal heartbeat data. This raw data, however, must then be analyzed by a human being to determine if there is a problem with a given fetus. In this paper, we propose a method to enable a computer to determine if a fetus is in a healthy or unhealthy state by the employment of a technique that will allow for rapid analysis using data mining.","Fetus,
Heart beat,
Pediatrics,
Accuracy,
Data models,
Heuristic algorithms,
Data mining"
RPig: A scalable framework for machine learning and advanced statistical functionalities,In many domains such as Telecom various scenarios necessitate the processing of large amounts of data using statistical and machine learning algorithms. A noticeable effort has been made to move the data management systems into MapReduce parallel processing environments such as Hadoop and Pig. Nevertheless these systems lack the features of advanced machine learning and statistical analysis. Frame-works such as Mahout on top of Hadoop support machine learning but their implementations are at the preliminary stage. For example Mahout does not provide Support Vector Machine (SVM) algorithms and it is difficult to use. On the other hand traditional statistical software tools such as R containing comprehensive statistical algorithms for advanced analysis are widely used. But such software can only run on a single computer and therefore it is not scalable. In this paper we propose an integrated solution RPig which takes the advantages of R (for machine learning and statistical analysis capabilities) and parallel data processing capabilities of Pig. The RPig framework offers a scalable advanced data analysis solution for machine learning and statistical analysis. Analysis jobs can be easily developed with RPig script in high level languages. We describe the design implementation and an eclipse-based RPigEditor for the RPig framework. Using application scenarios from the Telecom domain we show the usage of RPig and how the framework can significantly reduce the development effort. The results demonstrate the scalability of our framework and the simplicity of deployment for analysis jobs.,"telecommunication computing,
authoring languages,
data analysis,
learning (artificial intelligence),
parallel processing,
software tools,
statistical analysis"
Predicting Social Network Measures Using Machine Learning Approach,"The link prediction problem in social networks defined as a task to predict whether a link between two particular nodes will appear in the future is still a broadly researched topic in the field of social network analysis. However, another relevant problem is solved in the paper instead of individual link forecasting: prediction of key network measures values, what is a more time saving approach. Two machine learning techniques were examined: time series forecasting and classification. Both of them were tested on two real-life social network datasets.","Social network services,
Forecasting,
Accuracy,
Time measurement,
Classification algorithms,
Educational institutions,
Machine learning"
Newborn Screening for Phenylketonuria: Machine Learning vs Clinicians,"The metabolic disorders may hinder an infant's normal physical or mental development during the neonatal period. The metabolic diseases can be treated by effective therapies if the diseases are discovered in the early stages. Therefore, newborn screening program is essential to prevent neonatal from these damages. In the paper, a support vector machine (SVM) based algorithm is introduced in place of cut-off value decision to evaluate the analyte elevation raw data associated with Phenylketonuria. The data were obtained from tandem mass spectrometry (MS/MS) for newborns. In addition, a combined feature selection mechanism is proposed to compare with the cut-off scheme. By adapting the mechanism, the number of suspected cases is reduced substantially, it also handles the medical resources effectively and efficiently.","Pediatrics,
Support vector machines,
Hospitals,
Training,
Diseases,
Sensitivity,
Accuracy"
Driving skill analysis using machine learning The full curve and curve segmented cases,"Analysis of driving skill/driver state can be used in building driver support and infotainment systems that can be adapted to individual needs of a driver. In this paper we present a machine learning approach to analyzing driving maneuver skills of a driver that covers both longitudinal and lateral controls. The concept is to learn a driver model from sensor data that are related to driving environment, driving behavior and vehicle response. Once the model is built, driving skills of an unknown run can be classified automatically. In this paper, we demonstrate the feasibility of the proposed method for driving skill analysis based on a driving simulator experiment in a curve driving scene for both the full curve and curve segmented cases.","Vehicles,
Accuracy,
Support vector machines,
Feature extraction,
Acceleration,
Principal component analysis,
Data analysis"
Layout Analysis for Arabic Historical Document Images Using Machine Learning,"Page layout analysis is a fundamental step of any document image understanding system. We introduce an approach that segments text appearing in page margins (a.k.a side-notes text) from manuscripts with complex layout format. Simple and discriminative features are extracted in a connected-component level and subsequently robust feature vectors are generated. Multilayer perception classifier is exploited to classify connected components to the relevant class of text. A voting scheme is then applied to refine the resulting segmentation and produce the final classification. In contrast to state-of-the-art segmentation approaches, this method is independent of block segmentation, as well as pixel level analysis. The proposed method has been trained and tested on a dataset that contains a variety of complex side-notes layout formats, achieving a segmentation accuracy of about 95%.","Layout,
Image segmentation,
Training,
Feature extraction,
Shape,
Context,
Accuracy"
Automatic classification of unequal lexical stress patterns using machine learning algorithms,"Technology based speech therapy systems are severely handicapped due to the absence of accurate prosodic event identification algorithms. This paper introduces an automatic method for the classification of strong-weak (SW) and weak-strong (WS) stress patterns in children speech with American English accent, for use in the assessment of the speech dysprosody. We investigate the ability of two sets of features used to train classifiers to identify the variation in lexical stress between two consecutive syllables. The first set consists of traditional features derived from measurements of pitch, intensity and duration, whereas the second set consists of energies of different filter banks. Three different classifiers were used in the experiments: an Artificial Neural Network (ANN) classifier with a single hidden layer, Support Vector Machine (SVM) classifier with both linear and Gaussian kernels and the Maximum Entropy modeling (MaxEnt). these features. Best results were obtained using an ANN classifier and a combination of the two sets of features. The system correctly classified 94% of the SW stress patterns and 76% of the WS stress patterns.","Stress,
Speech,
Artificial neural networks,
Support vector machines,
Accuracy,
Acoustics,
Filter banks"
Combining machine learning and clinical rules to build an algorithm for predicting ICU mortality risk,In this study we aim to develop a decision support application for predicting ICU mortality risk that starts with a clinical analysis of the problem that also leverages machine learning to help create an algorithm with good performance characteristics. By starting from a clear basis in clinical practice we hope to improve algorithm development and the transparency of the resulting system. We start with a general model structure for a fuzzy rule based system (FIS). The model can be specified by clinicians who identify the inputs and the rules. An optimizer based on a genetic algorithm generates the coefficients for the final solution. Using the 2012 PhysioNet/CinC Challenge data set we constructed a Phase 1 system using minimal clinical guidance. Our initial FIS's achieved scores of 0.39 for Event 1 and 94 for Event 2. In Phase 2 we updated the FIS based on clinician interviews. At the end of Phase 2 we achieved 0.40 for Event 1 and 60 for Event 2. We hope to show that machine learning techniques that are modeled on the clinical understanding of a problem can be competitive with more abstract machine learning approaches but may be preferable because of their explainability and transparency.,"Training,
Algorithm design and analysis,
Market research,
Genetic algorithms,
Machine learning,
Artificial neural networks,
Noise"
From body surface potential to activation maps on the atria: A machine learning technique,"The treatment of atrial fibrillation has greatly changed in the past decade. Ablation therapy, in particular pulmonary vein ablation, has quickly evolved. However, the sites of the trigger remain very difficult to localize. In this study we propose a machine-learning method able to non-invasively estimate a single site trigger. The machine learning technique is based on a kernel ridge regression algorithm. In this study the method is tested on a simulated data. We use the monodomain model in order to simulate the electrical activation in the atria. The ECGs are computed on the body surface by solving the Laplace equation in the torso.","Mathematical model,
Electric potential,
Heart,
Torso,
Training,
Kernel,
Training data"
Adaptive Intrusion Detection System via online machine learning,"Adaptation of Intrusion Detection Systems (IDSs) in the heterogeneous and adversarial network environments is crucial. We design an adaptive IDS that has 10% higher accuracy than the best of four different baseline IDSs. Rather than creating a new `super' IDS, we combine the outputs of the IDSs by using the online learning framework proposed by Bousquet and Warmuth [1]. The combination framework allows to dynamically determine the best IDSs performed in different segments of a dataset. Moreover, to increase the accuracy and reliability of the intrusion detection results, the fusion between outputs of the four IDSs is taken into account by a new expanded framework. We conduct the experiments on two different datasets for benchmarking Web Application Firewalls: the ECML-PKDD 2007 HTTP dataset and the CISIC HTTP 2010. Experimental results show the high adaptability of the proposed IDS.","TV,
Accuracy"
A machine learning approach for LQT1 vs LQT2 discrimination,"Long QT syndrome (LQT) is a congenital disease caused by a mutation of genes that leads to a distortion and a prolongation of the T-wave on standard ECG. The present study proposes an algorithm to automatically discriminate between patients with type 1 or type 2 LQT syndrom. The core of the method is the modeling of the T-wave recomputed on its principal lead by a single parameterized function named Bi-Gaussian Function (BGF). From all the features computed from this model, a statistical analysis was performed to select only the most relevant ones for the discrimination. A classifier was then designed through a Linear Discriminant Analysis (LDA). A database composed of 410 LQTS patients whose genotype is known was used to train the classifier and evaluate its performances.","Vectors,
Electrocardiography,
Computational modeling,
Probes,
Principal component analysis,
Feature extraction,
Databases"
Autonomous mobile robot navigation using machine learning,"This paper develops a decision-making system based on the BP Neural Network to navigate a robot in an unknown environment. Based on the neural network model, the robot can move out of specific mazes successfully through adjusting its direction and speed continuously. A BP neural network, which includes three input nodes and nine output nodes, are designed for the navigation system. The information of the surrounding environment is returned by six ultrasonic sensors on the front and bilateral sides of the robot. After thousands of training, the robot learns the navigation knowledge successfully from the samples, and move out of the mazes autonomously. The performance of the robot is validated with the simulation results and two physical experiments. The results show that the robot could navigate autonomously in unknown environments.","Navigation,
Robot sensing systems,
Neural networks,
Training,
Mobile robots"
Image-based classification of diabetic retinopathy using machine learning,"In this paper we present experimental results of an automated method for image-based classification of diabetic retinopathy. The method is divided into three stages: image processing, feature extraction and image classification. In the first stage we have used two image processing techniques in order to enhance their features. Then, the second stage reduces the dimensionality of the images and finds features using the statistical method of principal component analysis. Finally, in the third stage the images are classified using machine learning algorithms, particularly, the naive Bayes classifier, neural networks, k-nearest neighbors and support vector machines. In our experimental study we classify two types of retinopathy: non-proliferative and proliferative. Preliminary results show that k-nearest neighbors obtained the best result with 68.7% using f-measure as metric, for a data set of 151 images with different resolutions.","Diabetes,
Retinopathy,
Histograms,
Image edge detection,
Machine learning algorithms,
Detectors"
Predicting the priority of a reported bug using machine learning techniques and cross project validation,"In bug repositories, we receive a large number of bug reports on daily basis. Managing such a large repository is a challenging job. Priority of a bug tells that how important and urgent it is for us to fix. Priority of a bug can be classified into 5 levels from PI to P5 where PI is the highest and P5 is the lowest priority. Correct prioritization of bugs helps in bug fix scheduling/assignment and resource allocation. Failure of this will result in delay of resolving important bugs. This requires a bug prediction system which can predict the priority of a newly reported bug. Cross project validation is also an important concern in empirical software engineering where we train classifier on one project and test it for prediction on other projects. In the available literature, we found very few papers for bug priority prediction and none of them dealt with cross project validation. In this paper, we have evaluated the performance of different machine learning techniques namely Support Vector Machine (SVM), Naive Bayes (NB), K-Nearest Neighbors (KNN) and Neural Network (NNet) in predicting the priority of the newly coming reports on the basis of different performance measures. We performed cross project validation for 76 cases of five data sets of open office and eclipse projects. The accuracy of different machine learning techniques in predicting the priority of a reported bug within and across project is found above 70% except Naive Bayes technique.","Intelligent systems,
Hafnium compounds,
Decision support systems"
Brunnstrom stage automatic evaluation for stroke patients using extreme learning machine,"Brunnstrom stage is widely used to evaluate the movement function of stroke patients during rehabilitation by physicians. In this paper, a new method, which is based on extreme learning machine (ELM) and the Internet technology, is proposed to realize intelligent Brunnstrom stages evaluation for upper limb movement function of stroke patients. Preliminary experiment has been conducted with movement data collected from 23 stroke patients and 4 healthy people. The experiment results show that, compared with the experienced physicians evaluation results, the accuracy of the established ELM model can reach 92.1%, which means the proposed method is helpful for physicians to remotely evaluate those stroke patients who finish rehabilitation exercises at home or community, and is helpful to solving the problem of the lack of medical resource and the high cost of inpatient rehabilitation.","Medical services,
Feature extraction,
Machine learning,
Accuracy,
Neurons,
Training,
Sensors"
A Machine Learning Approach for Identifying and Classifying Faults in Wireless Sensor Network,"Wireless Sensor Network (WSN) deployment experiences show that collected data is prone to be faulty. Faults are due to internal and external influences, such as calibration, low battery, environmental interference and sensor aging. However, only few solutions exist to deal with faulty sensory data in WSN. We develop a statistical approach to detect and identify faults in a WSN. In particular, we focus on the identification and classification of data and system fault types as it is essential to perform accurate recovery actions. Our method uses Hidden Markov Models (HMMs) to capture the fault-free dynamics of an environment and dynamics of faulty data. It then performs a structural analysis of these HMMs to determine the type of data and system faults affecting sensor measurements. The approach is validated using real data obtained from over one month of samples from motes deployed in an actual living lab.","Hidden Markov models,
Wireless sensor networks,
Calibration,
Batteries,
Fault diagnosis,
Data models,
Temperature sensors"
Machine learning algorithms applied in automatic classification of social network users,"This work shows the results of an analysis of machine learning algorithms applied in automatic classification for the users of the social network called Scientia.Net. The tests were done using a database with 2000 users. The analysis identifies which algorithm performs better in automatic classification of users within a social network. The algorithms tested were Multilayer Perceptron, Support Vector Machine, Kohonen Network and K-means Algorithm.","Machine learning algorithms,
Databases,
Support vector machines,
Social network services,
Error analysis,
Training,
Clustering algorithms"
Parametric reconfigurable designs with Machine Learning Optimizer,"We investigate the use of meta-heuristics and machine learning to automate reconfigurable application parameter optimization. The traditional approach involves two steps: (a) analyzing the application in order to create models and tools for exploration of the parameter space, and (b) exploring the parameter space using such tools. The proposed approach, called the Machine Learning Optimizer (MLO), involves a Particle Swarm Optimization (PSO) algorithm with an underlying surrogate fitness function model based on Gaussian Process (GP) and Support Vector Machines (SVMs). We present a case study of a quadrature based financial application with varied precision. We evaluate our approach by comparing the amount of benchmark evaluations and bit-stream generations when using MLO and when using the traditional approach.","Optimization,
Benchmark testing,
Gaussian processes,
Analytical models,
Throughput,
Training,
Machine learning"
Machine learning approach for argument extraction of bio-molecular events,"The main goal of Biomedical Natural Language Processing (BioNLP) is to capture biomedical phenomena from textual data by extracting relevant entities and information or relations between biomedical entities such as proteins and genes. Previous research was focussed on extracting only binary relations, but in recent times the focus is shifted towards extracting more complex relations in the form of bio-molecular events that may include several entities or other relations. In this paper we propose a machine learning approach based on Conditional Random Field (CRF) to extract the arguments of bio-molecular events. The overall task involves identification of event triggers from texts, classification of them into some predefined categories and determining the arguments of these events. We identify and implement a set of features in the forms of statistical and linguistic features that represent various morphological, syntactic and contextual information. Experiments on the benchmark setup of BioNLP 2009 shared task show the recall, precision and F-measure values of 45.75%, 78.93% and 57.91%, respectively.","Feature extraction,
Proteins,
Context,
Training,
Machine learning,
Data mining,
Event detection"
Machine learning attacks on 65nm Arbiter PUFs: Accurate modeling poses strict bounds on usability,"Arbiter Physically Unclonable Functions (PUFs) have been proposed as efficient hardware security primitives for generating device-unique authentication responses and cryptographic keys. However, the assumed possibility of modeling their underlying challenge-response behavior causes uncertainty about their actual applicability. In this work, we apply well-known machine learning techniques on challenge-response pairs (CRPs) from 64-stage Arbiter PUFs realized in 65nm CMOS, in order to evaluate the effectiveness of such modeling attacks on a modern silicon implementation. We show that a 90%-accurate model can be built from a training set of merely 500 CRPs, and that 5000 CRPs are sufficient to perfectly model the PUFs. To study the implications of these attacks, there is need for a new methodology to assess the security of PUFs suffering from modeling. We propose such a methodology and apply it to our machine learning results, yielding strict bounds on the usability of Arbiter PUFs. We conclude that plain 64-stage Arbiter PUFs are not secure for challenge-response authentication, and the number of extractable secret key bits is limited to at most 600.","Support vector machines,
Training,
Artificial neural networks,
Machine learning,
Semiconductor device modeling,
Authentication"
Chinese Review Spam Classification Using Machine Learning Method,"With great development of the e-commerce, the number of product reviews grows rapidly on the e-commerce website. Review mining has recently received a lot of attention, which aims to discover valuable information from the massive product reviews. An important subject of review mining is review spam classification, which classifies reviews into reviews or spam reviews, offering high-quality data to review mining. In this paper, we first present a categorization of Chinese review spam, and then classify the reviews by using machine learning method with different features, finally analyze the impact of different features on classification performance. The experiments show that Chinese review spam classification will obtain high accuracy by using machine learning method with appropriate features.","Feature extraction,
Semantics,
Data mining,
Support vector machines,
Mobile handsets,
Accuracy,
Classification algorithms"
A New Network Security Model Based on Machine Learning,"Rough set classifier or SVM (Support Vector Machine) classifier is a typical machine learning model. The Rough set classifier and SVM classifier are used to classify nodes as trust nodes, strange nodes and malicious nodes. We use the Rough set classifier to replace the method by settings of the threshold. The innovation of the article is to improve the computation accuracy and the efficiency of the classification computation by using Rough set combined with SVM classifier. In the cases where according to the value of an attribute or the values of two attributes the corresponding classification result can be determined, we use the Rough set classifier. In other cases, we use SVM classifier. Compared with existing security models, experiment results indicate that the model can obtain the higher examination rate of malicious nodes and the higher transaction success rate.","Support vector machines,
Peer to peer computing,
Training,
Security,
Educational institutions,
Accuracy,
Computational efficiency"
Differentiation between resting-state fMRI data from ADHD and normal subjects: Based on functional connectivity and machine learning,"Attention-deficit/hyperactivity disorder (ADHD) is a neuropsychiatric disorder which is quite common in childhood, with an estimated prevalence of 5-8%, and often persists into adolescence and adulthood. It is further characterized as inappropriate developmentally symptoms of inattention, impulsiveness, motor over-activity and restlessness. The aim of this study is to evaluate the feasibility of diagnosing ADHD by analyzing the resting-state functional magnetic resonance imaging (fMRI) data. In addition to confirming the previously observed three areas including anterior cingulate cortex (ACC), posterior cingulated cortex (PCC) and ventro medial prefrontal cortex (vmPFC), we also found significant differences in cerebellum, motor cortex and temporal lobe between ADHD and normal humans based on regional homogeneity analysis of the dataset from 73 children with ADHD and 76 normal children. Extracting features from these seven brain areas and utilizing the LDA classifier, the average accuracy of distinguishing normal and ADHD children reaches 80.08% though 50 times of 2-fold validation. Experimental results demonstrate the feasibility of ADHD diagnosis based on the combination of functional connectivity of resting-state fMRI and machine learning technique.","Correlation,
Feature extraction,
Educational institutions,
Accuracy,
Humans,
Magnetic resonance imaging,
Time series analysis"
The State of Machine Learning Methodology in Software Fault Prediction,The aim of this paper is to investigate the quality of methodology in software fault prediction studies using machine learning. Over two hundred studies of fault prediction have been published in the last 10 years. There is evidence to suggest that the quality of methodology used in some of these studies does not allow us to have confidence in the predictions reported by them. We evaluate the machine learning methodology used in 21 fault prediction studies. All of these studies use NASA data sets. We score each study from 1 to 10 in terms of the quality of their machine learning methodology (e.g. whether or not studies report randomising their cross validation folds). Only 10 out of the 21 studies scored 5 or more out of 10. Furthermore 1 study scored only 1 out of 10. When we plot these scores over time there is no evidence that the quality of machine learning methodology is better in recent studies. Our results suggest that there remains much to be done by both researchers and reviewers to improve the quality of machine learning methodology used in software fault prediction. We conclude that the results reported in some studies need to be treated with caution.,"Machine learning,
Software,
Predictive models,
NASA,
Cleaning,
Data models,
Software engineering"
Machine Learning and Text Mining of Trophic Links,"Machine Learning has been used to automatically generate a probabilistic food-web from Farm Scale Evaluation (FSE) data. The initial food web proposed by machine learning has been examined by domain experts and comparison with the literature shows that many of the links are corroborated. The FSE data were collected using two different sampling techniques, namely Vortis and pitfall. The corroboration of the initial Vortis food web, generated by machine learning, was performed manually by the domain experts. However, manual corroboration of hypothetical trophic links is difficult and requires significant amounts of time. In this paper we review the method and the main results on machine learning of trophic links. We study common trophic links from Vortis and pitfall data. We also describe a new method and present initial results on automatic corroboration of trophic links using text mining.","Text mining,
Machine learning,
Probabilistic logic,
Correlation,
Manuals,
Accuracy,
Tunneling magnetoresistance"
The Assessment of the Quality of Sugar Using Electronic Tongue and Machine Learning Algorithms,"The correct classification of sugar according to its physico-chemical characteristics directly influences the value of the product and its acceptance by the market. This study shows that using an electronic tongue system along with established techniques of supervised learning leads to the correct classification of sugar samples according to their qualities. In this paper, we offer two new real, public and non-encoded sugar datasets whose attributes were automatically collected using an electronic tongue, with and without pH controlling. Moreover, we compare the performance achieved by several established machine learning methods. Our experiments were diligently designed to ensure statistically sound results and they indicate that k-nearest neighbors method outperforms other evaluated classifiers and, hence, it can be used as a good baseline for further comparison.","Sugar,
Tongue,
Sugar industry,
Machine learning,
Consumer electronics,
Learning systems,
Image color analysis"
Integrating Machine Learning Into a Medical Decision Support System to Address the Problem of Missing Patient Data,"In this paper, we present a framework which enables medical decision making in the presence of partial information. At its core is ontology-based automated reasoning, machine learning techniques are integrated to enhance existing patient datasets in order to address the issue of missing data. Our approach supports interoperability between different health information systems. This is clarified in a sample implementation that combines three separate datasets (patient data, drug-drug interactions and drug prescription rules) to demonstrate the effectiveness of our algorithms in producing effective medical decisions. In short, we demonstrate the potential for machine learning to support a task where there is a critical need from medical professionals by coping with missing or noisy patient data and enabling the use of multiple medical datasets.","Machine learning,
Drugs,
Medical diagnostic imaging,
Knowledge based systems,
Decision making,
Accuracy,
Semantics"
Intelligent Corporate Sustainability report scoring solution using machine learning approach to text categorization,"Development of an intelligent software system to analyze and score Corporate Sustainability reports within the Global Reporting Initiative (GRI) framework has been well foreseen and in a high demand since the latest framework's publication in 2000's. As the number of reporting organizations and published reports is increasing exponentially, development of a software system to automate the daunting manual scoring process seems even more vital. We describe our preliminary efforts and the related results of our efforts in building such software through application of machine learning approach to text classification. Conduction of earlier training on thousands of sample documents to construct machine learning based classifiers inductively is our primary approach to solving this problem.","Classification algorithms,
Machine learning,
Filtering algorithms,
Text categorization,
Training,
Neural networks,
Organizations"
Optimal Tile Size Selection Problem Using Machine Learning,"One of the key feature of modern architectures is deep memory hierarchies. In order to exploit this feature, one has to expose data locality with-in a program. Loop tiling is an optimization phase in modern compilers which is used to transform a loop for exposing data locality. Selecting the best tile size for a given architecture and compiler is known as Optimal Tile Size Selection Problem. It is a NP-hard problem. People have build cost models for this problem that characterize the performance of a program as a function of tile sizes. The best tile size for a given loop is determined directly by using these models. Hand crafting an accurate tile size selection cost model is hard. Can we automatically learn a tile size selection model? This is an important question. In this paper, we have shown that a fairly accurate model can be learned using simple program dynamic features with standard machine learning techniques. We evaluate our approach on different architecture and compiler combinations. The model given by us consistently shows near-optimal performance (within 4% of the optimal) across all architecture and compiler combinations.","Tiles,
Kernel,
Machine learning,
Artificial neural networks,
Computer architecture,
Predictive models"
Semantic Data Types in Machine Learning from Healthcare Data,"Healthcare is particularly rich in semantic information and background knowledge describing data. This paper discusses a number of semantic data types that can be found in healthcare data, presents how the semantics can be extracted from existing sources including the Unified Medical Language System (UMLS), discusses how the semantics can be used in both supervised and unsupervised learning, and presents an example rule learning system that implements several of these types. Results from three example applications in the healthcare domain are used to further exemplify semantic data types.","Semantics,
Machine learning,
Medical services,
Unified modeling language,
Terminology,
Cognition,
Compounds"
Stochastic optimization for learning Non-Convex Linear Support Vector Machines,"In this paper, a fast optimization algorithm was proposed to learn the Non-Convex Linear Support Vector Machines (LSVM-NC) based on stochastic optimization, in which the non-convex function, Ramp Loss, was used to suppress the influence of noisy data in the case of large-scale learning problems. As for solving the non-convex linear SVMs, the traditional methods make use of the ConCave-Convex Procedure (CCCP) based on the Sequential Minimal Optimization (SMO) algorithm from dual, which is a time-consuming process and impractical for learning large-scale problems. To tackle this, we resorted to CCCP based on Stochastic Gradient Descent (SGD) algorithm from primal, and experimental results proved that our method could reduce the training time largely and improve the generalization performance.","Optimization,
Training,
Support vector machines,
Machine learning,
Testing,
Stochastic processes,
Machine learning algorithms"
An adaptive police duty scheduling system based on machine learning,"The research concerns police duty scheduling based on machine learning technology, instead of using manual arrangements. The aim is to have fair duty scheduling for police officers, and to make police projects more efficient. The system carries out police duty scheduling using database tools which we call &ldquo;dbtools&rdquo;. It is implemented by analyzing and proposing five working tables from modern police duty scheduling in Taiwan. We carried out analysis on practical police work and port machine learning methodologies onto our system. This results in a more human-oriented and user friendly system. Our system pioneers police duty research on scheduling. We hope that it results in better police management","scheduling,
adaptive systems,
law administration,
learning (artificial intelligence),
police data processing"
"Non-Local Morphological PDEs and
p
-Laplacian Equation on Graphs With Applications in Image Processing and Machine Learning","In this paper, we introduce a new class of non-local p-Laplacian operators that interpolate between non-local Laplacian and infinity Laplacian. These operators are discrete analogous of the game p -laplacian operators on Euclidean spaces, and involve discrete morphological gradient on graphs. We study the Dirichlet problem associated with the new p-Laplacian equation and prove existence and uniqueness of it's solution. We also consider non-local diffusion on graphs involving these operators. Finally, we propose to use these operators as a unified framework for solution of many inverse problems in image processing and machine learning.","Equations,
Laplace equations,
Morphology,
Image processing,
Machine learning,
Games,
Manifolds"
Learning and Relearning in Boltzmann Machines,,
Learning and Relearning in Boltzmann Machines,,
Efficient Learning in Boltzmann Machines Using Linear Response Theory,,
Machine learning for computational sustainability,"To avoid ecological collapse, we must manage Earth's ecosystems sustainably. Viewed as a control problem, the two central challenges of ecosystem management are to acquire a model of the system that is sufficient to guide good decision making and then optimize the control policy against that model. This paper describes three efforts aimed at addressing the first of these challenges-machine learning methods for modeling ecosystems. The first effort focuses on automated quality control of environmental sensor data. Next, we consider the problem of learning species distribution models from citizen science observational data. Finally, we describe a novel approach to modeling the migration of birds. A major challenge for all of these methods is to scale up to large, spatially-distributed systems.","Hidden Markov models,
Biological system modeling,
Ecosystems,
Data models,
Sociology,
Statistics,
Birds"
Sensitive Information Acquisition Based on Machine Learning,"With the rapid development of Internet, online information has greatly enriched. The Internet becomes a vast treasure of information, but simultaneously it is also flooding various trash information, such as: viruses, Trojans, violence, pornography, gambling and so on. The hostile forces outside of country and criminal elements are using the Internet to engage in illegal activities that endanger national security. So how to recognize this information to find the corresponding website and to carry on the effective supervision has become an urgent problem. For these reasons, this paper designs a new web information extraction system, which calls the extraction rule corresponding to the template by calculating the structural similarity among pages. In addition, a new method based on STU-DOM tree to construct decision tree is proposed. This method can use the classification of decision tree to determine sensitive information node.","Feature extraction,
Decision trees,
Data mining,
Databases,
Information retrieval,
Accuracy,
Algorithm design and analysis"
A machine learning framework of functional biomarker discovery for different microbial communities based on metagenomic data,"As more than 90% of microbial community could not be isolated and cultivated, the metagenomic methods have been commonly used to analyze the microbial community as a whole. With the fast acumination of metagenomic samples, it is now intriguing to find simple biomarkers, especially functional biomarkers, which could distinguish different metagenomic samples. Next-generation sequencing techniques have enabled the detection of very accurate gene-presence (abundance) values in metagenomic studies. And the presence/absence or different abundance values for a set of genes could be used as appropriate biomarker for identification of the corresponding microbial community's phenotype. However, it is not yet clear how to select such a set of genes (features), and how accurate would it be for such a set of selected genes on prediction of microbial community's phenotype. In this study, we have evaluated different machine learning methods, including feature selection methods and classification methods, for selection of biomarkers that could distinguish different samples. Then we proposed a machine learning framework, which could discover biomarkers for different microbial communities from the mining of metagenomic data. Given a set of features (genes) and their presence values in multiple samples, we first selected discriminative features as candidate by feature selection, and then selected the feature sets with low error rate and classification accuracies as biomarkers by classification method. We have selected whole genome sequencing data from simulation, public domain and in-house metagenomic data generation facilities. We tested the framework on prediction and evaluation of the biomarkers. Results have shown that the framework could select functional biomarkers with very high accuracy. Therefore, this framework would be a suitable tool to discover functional biomarkers to distinguish different microbial communities.","Error analysis,
Sensitivity,
Communities,
Mice,
Bioinformatics,
Systems biology,
Gaussian distribution"
Action classification of humanoid soccer robots using machine learning,"This paper presents an alternative approach on humanoid soccer robots action classification in order to seize the ball control and better ball possession using machine learning and data mining classification algorithms. Categorizing proper actions regarding to positional and environmental features is a prerequisite for proper acting in robotics. In this paper we present an approach to gather information and extracting useful features out of that information from SimSpark simulation server logs. These gathered data will generate a meaningful multi-class dataset, afterwards data processing and running appropriate data mining algorithms on the dataset and evaluating our experiments are the most important issues in this paper. In order to achieve a model for classifying our multi-class dataset, we applied two well-known applications from the domain of data mining: TANAGRA and WEKA and finally we have visualized our experimental results as far as possible.","Robots,
Vegetation,
Classification algorithms,
Data mining,
Accuracy,
Bagging,
Servers"
Based on Machine Learning of Data Mining to Further Explore,"Machine learning and data mining is a powerful tool of the analysis of massive data, is emerging interdisciplinary of computer science and statistics, which plays an important role in the front field of modern science and technology. Introduction of machine learning information technology, further expounds the contact of the machine learning and data mining. Stratified discusses the meaning of data mining, in view of the different professions and different areas, points out how to use the data mining technology, give play to the role of machine learning. The emphasis is on data gathered information, use data information, and development data information resources.","Machine learning,
Argon"
Application of entropic value-at-risk in machine learning with corrupted input data,"The entropic value-at-risk (EVaR) is a coherent risk measure that is efficiently computable for the sum of independent random variables. This paper shows how this risk measure can be used in machine learning when uncertainty affects the input data. For this purpose, we consider here a support vector machine with corrupted input data.","Optimization,
Machine learning,
Random variables,
Uncertainty,
Linear programming,
Reactive power,
Support vector machines"
Map-Reduce for Machine Learning on Multicore,,
Machine learning approaches for electric appliance classification,"We report on the development of an innovative system which can automatically recognize home appliances based on their electric consumption profiles. The purpose of our system is to apply adequate rules to control electric appliance in order to save energy and money. The novelty of our approach is in the use of plug-based low-end sensors that measure the electric consumption at low frequency, typically every 10 seconds. Another novelty is the use of machine learning approaches to perform the classification of the appliances. In this paper, we present the system architecture, the data acquisition protocol and the evaluation framework. More details are also given on the feature extraction and classification models being used. The evaluation showed promising results with a correct rate of identification of 85%.","Home appliances,
Sensors,
Vectors,
Feature extraction,
Training,
Electricity,
Monitoring"
A Scalable Machine Learning Approach to Go,,
Discovery of video websites based on machine learning,"The supervision of video websites has become extremely important with the rapid development of web, The first problem to overcome is automatically detecting of video websites. For the traditional limitations of artificial discrimination, this paper presents a video website discovery method based on machine learning. First we adopt a crawler to crawl video page description information, then we preprocess these text information. After these steps, we adopt a Naïve Bayes classifier to classify this text information. In addition, through the combination of sensitive words matching, we can realize the supervision of sensitive information.","Accuracy,
Crawlers,
YouTube"
Design and Analysis of Machine Learning Experiments,,
Kernel Classifiers from a Machine Learning Perspective,,
General Signal Processing and Machine Learning Tools for BCI Analysis,,
Hyperspectral image compression using 3D discrete cosine transform and support vector machine learning,"Hyperspectral images exhibit significant spectral correlation, whose exploitation is crucial for compression. In this paper, an efficient method for hyperspectral image compression is presented using the three-dimensional discrete cosine transform (3D-DCT) and support vector machine (SVM). The core idea behind our proposed technique is to apply SVM on the 3D-DCT coefficients of hyperspectral images in order to determine which coefficients (support vectors) are more critical for being preserved. Our method not only exploits redundancies between the bands, but also uses spatial correlations of every image band. Consequently, as simulation results applied to real hyperspectral images demonstrate, the proposed method leads to a remarkable compression ratio and quality.","Support vector machines,
Hyperspectral imaging,
Discrete cosine transforms,
Image coding,
Image reconstruction"
The Berlin Brain-Computer Interface: Machine Learning-Based Detection of User Specific Brain States,,
Foundations of Machine Learning,,
Machine Learning and Adaptivity,,
The Improved Fast Gauss Transform with Applications to Machine Learning,,
Statistical Machine Learning and Dissolved Gas Analysis: A Review,"Dissolved gas analysis (DGA) of the insulation oil of power transformers is an investigative tool to monitor their health and to detect impending failures by recognizing anomalous patterns of DGA concentrations. We handle the failure prediction problem as a simple data-mining task on DGA samples, optionally exploiting the transformer's age, nominal power and voltage, and consider two approaches: 1) binary classification and 2) regression of the time to failure. We propose a simple logarithmic transform to preprocess DGA data in order to deal with long-tail distributions of concentrations. We have reviewed and evaluated 15 standard statistical machine-learning algorithms on that task, and reported quantitative results on a small but published set of power transformers and on proprietary data from thousands of network transformers of a utility company. Our results confirm that nonlinear decision functions, such as neural networks, support vector machines with Gaussian kernels, or local linear regression can theoretically provide slightly better performance than linear classifiers or regressors. Software and part of the data are available at http://www.mirowski.info/pub/dga.","Dissolved gas analysis,
Oil insulation,
Predictive models,
Support vector machines,
Pattern recognition,
Statistical analysis,
Power transformers"
Dependance of critical dimension on learning machines and ranking methods,"Feature reduction is a major problem in data mining. Though traditional methods such as feature ranking and subset selection have been widely used, there has been little consideration given to assuring satisfactory performance of a learning machine in relation to the minimum of features required or the “critical dimension”. This critical dimension is unique to a specific dataset, learning machine, and ranking algorithm combination. The empirical methods demonstrate that many datasets show the existence of critical dimension. The dependence of this critical dimension on the learning machines and ranking algorithms could provide newer insights in understanding datasets, machine learning classifiers and ranking algorithms. The preliminary results of analysis show that the critical dimension depends largely on the feature ranking algorithm and that learning machines are less significant in determining the critical dimension.","Machine learning,
Accuracy,
Machine learning algorithms,
Support vector machines,
Niobium,
Feature extraction,
Radio frequency"
Autonomous robotic ground penetrating radar surveys of ice sheets; Using machine learning to identify hidden crevasses,"This paper presents methods to continue development of a completely autonomous robotic system employing ground penetrating radar imaging of the glacier sub-surface. We use well established machine learning algorithms and appropriate un-biased processing, particularly those which are also suitable for real-time image analysis and detection. We tested and evaluated three processing schemes in conjunction with a Support Vector Machine (SVM) trained on 15 examples of Antarctic GPR imagery, collected by our robot and a Pisten Bully tractor in 2010 in the shear zone near McMurdo Station. Using a modified cross validation technique, we correctly classified all examples with a radial basis kernel SVM trained and evaluated on down-sampled and texture-mapped GPR images of crevasses, compared to 60% classification rate using raw data. We also test the most successful processing scheme on a larger dataset, comprised of 94 GPR images of crevasse crossings recorded in the same deployment. Our experiments demonstrate the promise and reliability of real-time object detection and classification with robotic GPR imaging surveys.","Ground penetrating radar,
Support vector machines,
Real time systems,
Robots,
Diffraction,
Snow,
Antarctica"
Sentiment Classification for Microblog by Machine Learning,"With the development of microblog, many studies pay special attention to sentiment classification of the reviews in microblog. This paper summarizes three well-known methods for text classification and then improves one of them for sentiment analysis. We come up with a new model in which we introduce efficient approaches to select features, calculate weights, train samples and evaluate classifier. The new model is based on Bayesian algorithm and machine learning that is one of the most popular methods for sentiment classification. Our model can enhance the overall efficiency of the sentiment classifier.","Machine learning,
Text categorization,
Feature extraction,
Bayesian methods,
Support vector machines,
Training"
"Robust, reliable and applicable tool wear monitoring and prognostic: Approach based on an improved-extreme learning machine","Although efforts in this field are significant around the world, real prognostics systems are still scarce in industry. Indeed, it is hard to provide efficient approaches that are able to handle with the inherent uncertainty of prognostics and nobody is able to a priori ensure that an accurate prognostic model can be built. As for an example of remaining problems, consider data-driven prognostics approaches: how to ensure that a model will be able to face with inputs variation with respect to those ones that have been learned, how to ensure that a learned-model will face with unknown data, how to ensure convergence of algorithms, etc. In other words, robustness, reliability and applicability of a prognostic approach are still open areas. Following that, the aim of this paper is to address these challenges by proposing a new neural network (structure and algorithm) that enhances reliability of RUL estimates while improving applicability of the approach. Robustness, reliability and applicability aspects are first discussed and defined according to literature. On this basis, a new connexionist system is proposed for prognostics: the Improved-Extreme Learning machine (Imp-ELM). This neural network, based on complex activation functions, enables to reduce the influence of human choices and initial parameterization, while improving accuracy of estimates and speeding the learning phase. The whole proposition is illustrated by performing tests on a real industrial case of cutting tools from a Computer Numerical Control (CNC) machine. This is achieved by predicting tool condition (wear) in terms of remaining cuts successfully made. Thorough comparisons with adaptive neuro fuzzy inference system (ANFIS) and existing ELM algorithm are also given. Results show improved robustness, reliability and applicability performances.","Robustness,
Neurons,
Cutting tools,
Computer numerical control,
Machine learning,
Data models"
Machine Learning Methodology for Enhancing Automated Process in IT Incident Management,"Operating system experienced a rise in number of incidents in recent years. Analysis and reemployment of past solution therefore may make a contribution in reducing service interrupt time and minimizing business losses. The training and retaining of human resources is another primary disbursement source for enterprise. Thus, it is of great significance for enterprises to find reasonable solutions automatically. Combined with keyword tokenization, data mining, numerical optimization and neural network, this paper presents a system that compares and finds the most similar incident solution in the past, based on the description provided by customers in natural language. We try to improve the automated process by increasing the efficiency and accuracy through machine learning methodology and also devote to presenting a practical decision support method.","Machine learning,
Accuracy,
Training,
Biological neural networks,
Computer architecture,
Optimization"
Machine Learning-Based Self-Adjusting Concurrency in Software Transactional Memory Systems,"One of the problems of Software-Transactional-Memory (STM) systems is the performance degradation that can be experienced when applications run with a non-optimal concurrency level, namely number of concurrent threads. When this level is too high a loss of performance may occur due to excessive data contention and consequent transaction aborts. Conversely, if concurrency is too low, the performance may be penalized due to limitation of both parallelism and exploitation of available resources. In this paper we propose a machine-learning based approach which enables STM systems to predict their performance as a function of the number of concurrent threads in order to dynamically select the optimal concurrency level during the whole lifetime of the application. In our approach, the STM is coupled with a neural network and an on-line control algorithm that activates or deactivates application threads in order to maximize performance via the selection of the most adequate concurrency level, as a function of the current data access profile. A real implementation of our proposal within the TinySTM open-source package and an experimental study relying on the STAMP benchmark suite are also presented. The experimental data confirm how our self-adjusting concurrency scheme constantly provides optimal performance, thus avoiding performance loss phases caused by non-suited selection of the amount of concurrent threads and associated with the above depicted phenomena.","Concurrent computing,
Instruction sets,
Training,
Proposals,
Artificial neural networks,
Throughput"
A Machine Learning Approach to Android Malware Detection,"With the recent emergence of mobile platforms capable of executing increasingly complex software and the rising ubiquity of using mobile platforms in sensitive applications such as banking, there is a rising danger associated with malware targeted at mobile devices. The problem of detecting such malware presents unique challenges due to the limited resources avalible and limited privileges granted to the user, but also presents unique opportunity in the required metadata attached to each application. In this article, we present a machine learning-based system for the detection of malware on Android devices. Our system extracts a number of features and trains a One-Class Support Vector Machine in an offline (off-device) manner, in order to leverage the higher computing power of a server or cluster of servers.","Kernel,
Feature extraction,
Malware,
Androids,
Humanoid robots,
Vectors,
Data mining"
Computer aided diagnosis system based on machine learning techniques for lung cancer,"Cancer is a leading cause of death worldwide. Lung cancer is a type of cancer that is considered as one of the most leading causes of death globally. In Malaysia, it is the 3rd common cancer type and the 2nd type of cancer among men. In this paper, machine learning techniques have been utilized to develop a computer-aided diagnosis system for lung cancer. The system consists of feature extraction phase, feature selection phase and classification phase. For feature extraction/selection, different wavelets functions have been applied in order to find the one that produced the highest accuracy. Clustering-K-nearest-neighbor algorithm has been developed/utilized for classification. Japanese Society of Radiological Technology's standard dataset of lung cancer has been used to test the system. The data set has 154 nodule regions (abnormal) and 92 non-nodule regions (normal). Accuracy levels of over 96% for classification have been achieved which demonstrate the merits of the proposed approach.","Accuracy,
Feature extraction,
Lead,
Lungs"
Cooperative Machine Learning with Information Fusion for Dynamic Decision Making in Diagnostic Applications,"In many applications, use of all relevant data to extract more information from multiple sources of information and achieve higher accuracy in prediction is desirable. Cooperative learning is observed in human and some animal societies. Sound knowledge and information acquisition, cooperation in learning amongst multi-agent systems may result in a higher effectiveness compared to individual learning. Cooperative learning in multi agent systems is generally expected to improve both quality & speed of learning. According to survey maximum research papers focus on coordinated approach of agents. Multiple sources of data can be viewed as different, related views of the same learning problem, where dependencies between the views could potentially take on complex structures. This gives rise to interesting and challenging machine learning problems where data sources are combined for learning. This framework encompasses several data fusion tasks and related topics, such as transfer learning, multitask learning, multiview learning, and learning under covariate shift. The advantages of the multiple source learning paradigm is seen in situations where individual data sources are noisy, incomplete, and learning from more than one source can filter out problem-independent noise. Cooperative learning is an approach where one or more team of learners work together towards reaching a better understanding of a specified task. The purpose of this paper is to use this approach to describe a proposal for designing and building a cooperative machine learning system (Multi-Learning system) that contains two or more machine learners that cooperate together.","Learning systems,
Educational institutions,
Machine learning,
Learning,
Decision making,
Multiagent systems,
Computers"
Machine Learning Algorithms,,
Machine-Learning Foundations: The Probabilistic Framework,,
Support Vector Machine Learning for Interdependent and Structured Output Spaces,,
Point stabilization of two-wheeled vehicle based on machine learning,"The configuration of a two-wheeled vehicle, such as a Segway, involves non-holonomic constraints, and thus it cannot be stabilized by continuous and time-invariant state feedback. Because of the nonlinear nature of the nonholonomic constraints, the realization of a model predictive control (MPC) algorithm for this class of vehicles is a difficult task. This paper proposes an MPC method that can achieve a long prediction horizon and has a short computation time. First, the optimization of an input (i.e., velocity and steering) sequence is formulated as a graph search problem by restricting the inputs to discrete values. Next, the optimized control result is learned by a machine learning method, such as support vector machine (SVM). Compared to nonlinear optimization, a longer horizon MPC can be realized. The advantages of the proposed method are demonstrated with simulation and experimental results.","Support vector machines,
Vehicles,
Vectors,
Optimization,
Training data,
Performance analysis,
Prediction algorithms"
Adaptive sampling in context-aware systems: A machine learning approach,"As computing systems become ever more pervasive, there is an increasing need for them to understand and adapt to the state of the environment around them: that is, their context. This understanding comes with considerable reliance on a range of sensors. However, portable devices are also very constrained in terms of power, and hence the amount of sensing must be minimised. In this paper, we present a machine learning architecture for context awareness which is designed to balance the sampling rates (and hence energy consumption) of individual sensors with the significance of the input from that sensor. This significance is based on predictions of the likely next context. The architecture is implemented using a selected range of user contexts from a collected data set. Simulation results show reliable context identification results. The proposed architecture is shown to significantly reduce the energy requirements of the sensors with minimal loss of accuracy in context identification.","software reliability,
learning (artificial intelligence),
mobile computing,
software architecture"
Extreme Learning Machine based fast object recognition,"Extreme Learning Machine (ELM) as a type of generalized single-hidden layer feed-forward networks (SLFNs) has demonstrated its good generalization performance with extreme fast learning speed in many benchmark and real applications. This paper further studies the performance of ELM and its variants in object recognition using two different feature extraction methods. The first method extracts texture features, intensity features from Histogram and features from two types of color space: HSV & RGB. The second method extracts shape features based on Radon transform. The classification performances of ELM and its variants are compared with the performance of Support Vector Machines (SVMs). As verified by simulation results, ELM achieves better testing accuracy with much less training time on majority cases than SVM for both feature extraction methods. Besides, the parameter tuning process for ELM is much easier than SVM as well.","Feature extraction,
Support vector machines,
Kernel,
Image color analysis,
Object recognition,
Machine learning,
Shape"
A comparison of machine learning algorithms applied to hand gesture recognition,"Hand gesture recognition for human computer interaction is an area of active research in computer vision and machine learning. The primary goal of gesture recognition research is to create a system, which can identify specific human gestures and use them to convey information or for device control. This paper presents a comparative study of four classification algorithms for static hand gesture classification using two different hand features data sets. The approach used consists in identifying hand pixels in each frame, extract features and use those features to recognize a specific hand pose. The results obtained proved that the ANN had a very good performance and that the feature selection and data preparation is an important phase in the all process, when using low-resolution images like the ones obtained with the camera in the current work.","Gesture recognition,
Feature extraction,
Machine learning,
Classification algorithms,
Support vector machines,
Artificial neural networks,
Human computer interaction"
Second-order methods for L1 regularized problems in machine learning,"This paper proposes a Hessian-free Newton method for solving large-scale convex functions with an L1 regularization term. These problems arise in supervised machine learning models in which it is important to seek a sparse parameter vector. The proposed method operates in a batch setting, which is well suited for parallel computing environments, and employs sub-sampled Hessian information to accelerate progress of the iteration. The method consists of two phases, an active-set prediction phase that employs first-order and second-order information, and subspace phase that performs a Newton-like step. Numerical results on a speech recognition problem illustrate the practical behavior of the method.","Optimization,
Machine learning,
Newton method,
Training,
OWL,
Vectors,
Minimization"
Anomaly detection in gamma ray spectra: A machine learning perspective,"With Canadian security and the safety of the general public in mind, physicists at Health Canada (HC) have begun to develop techniques to identify persons concealing radioactive material that may represent a threat to attendees at public gatherings, such as political proceedings and sporting events. To this end, Health Canada has initiated field trials that include the deployment of gamma-ray spectrometers. In particular, a series of these detectors, which take measurements every minute and produce 1,024 channel gamma-ray spectrum, were deployed during the Vancouver 2010 olympics. Simple computerized statistics and human expertise were used as the primary line of defence. More specifically, if a measured spectrum deviated significantly from the background, an internal alarm was sounded and an HC physicist undertook further analysis into the nature of the alarming spectrum. This strategy, however, lead to a significant number of costly and time consuming false positives. This research applies sophisticated machine learning algorithms to reduce the number of false positives to an acceptable level, the results of which are detailed in this paper. In addition, we emphasize the primary findings of our work and highlight avenues available to further improve upon our current results.","Rain,
Isotopes,
Machine learning algorithms,
Machine learning,
Training,
Testing,
Support vector machines"
Dynamic switching and real-time machine learning for improved human control of assistive biomedical robots,"A general problem for human-machine interaction occurs when a machine's controllable dimensions outnumber the control channels available to its human user. In this work, we examine one prominent example of this problem: amputee switching between the multiple functions of a powered artificial limb. We propose a dynamic switching approach that learns during ongoing interaction to anticipate user behaviour, thereby presenting the most effective control option for a given context or task. Switching predictions are learned in real time using temporal difference methods and reinforcement learning, and demonstrated within the context of a robotic arm and a multifunction myoelectric controller. We find that a learned, dynamic switching order is able to out-perform the best fixed (non-adaptive) switching regime on a standard prosthetic proficiency task, increasing the number of optimal switching suggestions by 23%, and decreasing the expected transition time between degrees of freedom by more than 14%. These preliminary results indicate that real-time machine learning, specifically online prediction and anticipation, may be an important tool for developing more robust and intuitive controllers for assistive biomedical robots. We expect these techniques will transfer well to near-term use by patients. Future work will describe clinical testing of this approach with a population of amputee patients.","Switches,
Robots,
Electromyography,
Joints,
Vectors,
Humans"
The Impact of Evasion on the Generalization of Machine Learning Algorithms to Classify VoIP Traffic,We propose a novel approach to generate well generalized signatures to classify Skype VoIP traffic using a machine learning based approach. Results show that the performance of the signatures did not degrade significantly when they were evaluated on traffic that was captured from different locations and at different times as well as employed against evasion attacks. Our results on the evasion of Skype classifier demonstrate that the performance of the signatures are very promising even if the user tries maliciously to alter the characteristics of Skype traffic to evade the classifier.,"Bit rate,
Payloads,
Cryptography,
Protocols,
Training,
Internet"
Reaching optimally over the workspace: A machine learning approach,"Recent theories of Human Motor Control explain our outstanding coordination capabilities by calling upon an Optimal Control (OC) framework. But OC methods are generally too expensive to be applied on-line and in realtime as would be required to perform everyday movements. An alternative method consists in obtaining a pre-computed feedback policy that performs optimally while being executed reactively. One way to get such a pre-computed policy consists in tuning a parametrized reactive controller so that it converges to optimal behavior. In this paper, we demonstrate a method to obtain such a reactive controller that (i) adapts the time of movement based on a compromise between the amount of reward and the effort required to get it, (ii) provides an efficient trajectory from any point to any point in the workspace, (iii) learns from demonstrations of optimal trajectories, (iv) is improving its performance over accumulated experience.","Trajectory,
Sociology,
Statistics,
Aerospace electronics,
Predictive models,
Machine learning,
Noise"
Machine learning for user modeling in a multilingual learning system,"Towards the successful creation of user models that can be incorporated into foreign language learning systems, we have used algorithmic approaches residing in the field of machine learning. The creation of user models is even more demanding in the area of Computer Assisted Multilanguage Learning, since modeling is an even more complex process that concurrently handles information from multiple domains. These domains have important similarities but also basic differences. This paper describes the implementation of student modeling through machine learning techniques, which aims to ameliorate future multiple language learning systems. The incorporation of k-means clustering is used to address several barriers posed by the heterogeneous learning audience. The resulting system both generates and discovers user profiles, based on students' characteristics, performance and preferences. Through our system, we promote the adaptivity and individualization to each user that interacts with the application, by providing individualized help, error diagnosis and error proneness along with advice generator components.",Knowledge based systems
A comparison of machine learning techniques for modeling human-robot interaction with children with autism,"Several machine learning techniques are used to model the behavior of children with autism interacting with a humanoid robot, comparing a static model to a dynamic model using hand-coded features. Good accuracy (over 80%) is achieved in predicting child vocalizations; directions for future approaches to modeling the behavior of children with autism are suggested.","Autism,
Robots,
Decision trees,
Machine learning,
Computational modeling,
Error analysis,
USA Councils"
Machine learning for the automatic identification of terrorist incidents in worldwide news media,"The RAND Database of Worldwide Terrorism Incidents (RDWTI) seeks to index information about all terrorist incidents that occur and are mentioned in worldwide news media, providing a useful resource for policy researchers and decision makers. We examined automated classification methods that could be used to identify news articles about terrorist incidents, thus enabling analysts to read a smaller number of news articles and maintain the database with less effort and cost. The support vector machine (SVM) and Lasso methods were only modestly successful, but a classifier based on the gradient boosting method (GBM) appeared to be very successful, correctly ranking 80% of the relevant articles at the “top of the pile” for examination by a human analyst.","Support vector machines,
Terrorism,
Standards,
Databases,
Humans,
Training data,
Training"
Machine learning approach towards email management,"Originally supposed to be a communication application, email is now used for a wide range of functions. Nowadays, it is increasingly becoming very difficult for email users to keep track of their email messages and to organize them such that can be retrieved easily, changed, and managed. Email tools, however have not progressed correspondingly to support users with information management based on users' need. These tools do not understand the information they contain, so they are unable to help user manage it. The goal of this research is to provide a better intelligent email manager that can help to understand, manage, organize and use the overwhelming amount of information received and stored to capture the semantic content relations and also to build intelligent tools to assist with the management of the semantic relations.","Electronic mail,
Semantics,
Postal services,
Machine learning,
Visualization,
Humans"
Dynamic Linear Solver Selection for Transient Simulations Using Machine Learning on Distributed Systems,"Many transient simulations spend a significant portion of the overall runtime solving a linear system. A wide variety of preconditioned linear solvers have been developed to quickly and accurately solve different types of linear systems, each having options to customize the preconditioned solver for a given linear system. Transient simulations may produce significantly different linear systems as the simulation progresses due to special events occurring that make the linear systems more difficult to solve or the model moving closer to a state of equilibrium where the linear systems are easier to solve. Machine learning algorithms provide the ability to dynamically select the preconditioned linear solver for each linear system produced by a simulation. We can generate databases by computing attributes for each linear system, physical attributes for the transient simulation, computational attributes, and running times for a set of preconditioned solvers on each linear system. Machine learning algorithms can then use these databases to generate classifiers capable of dynamically selecting a preconditioned solver for each linear system given a set of attributes. This allows us to quickly and accurately compute each transient simulation using different preconditioned solvers throughout the simulation. This also provides the potential to produce speedups in comparison with using a single preconditioned solver for an entire transient simulation.","Linear systems,
Computational modeling,
Databases,
Machine learning,
Numerical models,
Transient analysis,
Machine learning algorithms"
The partitioned kernel machine algorithm for online learning,"Kernel machines have been successfully applied to many engineering problems requiring pattern recognition and regression. Kernel machines are a family of machine learning algorithms including support vector machines (SVM) [1], kernel least mean squares adaptive filter (KLMS) [2], and kernel recursive least squares (KRLS) adaptive filter [3] to name a few. In this paper we present the partitioned kernel machine algorithm for use in online learning in virtual environments. The PKM algorithm enhances the accuracy of the computationally efficient KLMS algorithm. The PKM algorithm is an iterative update procedure that focuses on a subset of the stored vectors in the kernel machine buffer. We use a similarity measure for the selection of kernel machine vectors that allow more common vectors to be updated more frequently, and outlier vectors to be updated less frequently. We validate the increased accuracy of our novel algorithm in two separate experimental settings.","Kernel,
Vectors,
Time series analysis,
Partitioning algorithms,
Equations,
Machine learning,
Accuracy"
Analysis of how the choice of Machine Learning algorithms affects the prediction of a clinical outcome prior to minimally invasive treatments for Benign Pro Static Hyperplasia BPH,"Benign Pro Static Hyperplasia (BPH) is estimated to effect 50% of men by the age of 50, and 75% by the age of 80. Predicting a clinical outcome prior to minimally invasive treatments for BPH would be very useful, but has not been reliable in spite of multiple assessment parameters such as symptom indices and flow rates. I our prior work we have shown the effect of greater impact feature selection has on prediction of the BPH clinical outcomes. In this work we take an in depth look at how changes to the Artificial Intelligence and Machine Learning methods can have an affect on how well the process does at predicting the outcome of the patients in the testing group. The affect of which classifier is used, to predict BPH surgical outcomes, is investigated to see if certain classifiers perform better with the data. The affect of which metric is selected for analyzing the performance of the classifier prediction is also observed. The affect of which features and how many are selected to train and predict the data is observed. Finally, the affect of using the original, unchanged, date versus a discretized version of the data is also observed. The objective in this paper is to determine, in this case, which of the above-mentioned factors affect the outcome of the predictive models, to allow the best factor selection in each case so that the best predictive method of NPH for this data, can be determined. In particular, the data is analyzed to determine if some of these factors have a larger effect on the outcome than others. Through experimental results we show which and how some factors are found to have no real influence on clinical outcome prediction, and show how in some other cases there are a few equally good choices. Here four machine learning algorithms, namely Decision Tree, Naïve Bayes, LDA, and ADABoost are selected and used in the comparison. For prediction performance metrics comparison we use the Area Under the Curve (AUC), Accuracy (ACC), and the Matthew Correlation Coefficient (MCC). Both internal cross-validation and external validation are used to analyze the performance and results of the predictive models considered.","Surgery,
Bladder,
Decision trees,
Learning systems,
Measurement,
Accuracy"
Machine Learning Algorithms in Bipedal Robot Control,"Over the past decades, machine learning techniques, such as supervised learning, reinforcement learning, and unsupervised learning, have been increasingly used in the control engineering community. Various learning algorithms have been developed to achieve autonomous operation and intelligent decision making for many complex and challenging control problems. One of such problems is bipedal walking robot control. Although still in their early stages, learning techniques have demonstrated promising potential to build adaptive control systems for bipedal robots. This paper gives a review of recent advances on the state-of-the-art learning algorithms and their applications to bipedal robot control. The effects and limitations of different learning techniques are discussed through a representative selection of examples from the literature. Guidelines for future research on learning control of bipedal robots are provided in the end.","Legged locomotion,
Supervised learning,
Robot control,
Machine learning algorithms,
Learning,
Unsupervised learning"
Introduction to Reinforcement and Systemic Machine Learning,,
"Fundamentals of Whole-System, Systemic, and Multiperspective Machine Learning",,
Knowledge Augmentation: A Machine Learning Perspective,,
Systemic Machine Learning and Model,,
Application of parallel distributed genetics-based machine learning to imbalanced data sets,"Real world data sets are often imbalanced with respect to the class distribution. Classifier design from those data sets is relatively new challenge. The main problem is the lack of positive class patterns in the data sets. To deal with this problem, there are two main approaches. One is to additionally sample minority class patterns (i.e., over-sampling). The other is to sample a part of majority class patterns (i.e., under-sampling). In our previous research, we have proposed a parallel distributed genetics-based machine learning for large data sets. In our method, not only a population but also a training data set is divided into subgroups, respectively. A pair of a sub-population and a training data subset is assigned to an individual CPU core in order to reduce the computation time. In this paper, our parallel distributed approach is applied to imbalanced data sets. The training data subsets are constructed by a composition of subsets divided majority class patterns with the entire set of non-divided minority class patterns. Through computational experiments, we show the effectiveness of our parallel distributed approach with the proposed data subdivision schemes for imbalanced data sets.","Distributed databases,
Training data,
Computational modeling,
Data models,
Machine learning,
Training,
Fuzzy sets"
Application of machine learning (reinforcement learning) for routing in Wireless Sensor Networks (WSNs),"Traditionally, protocols and applications in the networking domain have been designed to work in large-scale heterogeneous, hierarchically organized networks with low failure rate. In a Wireless Sensor Network (WSN) scenario, new problems arise and traditional routing protocols cannot be successfully applied. Additionally, in energy-restricted environments like WSNs the overhead of keeping routing information fresh becomes unbearable. In this problem context problem context, many researchers have turned their attention to the domain of machine learning (ML). The goal of this paper is to analyze the application of the Reinforcement Learning (specifically Q-learning) for an energy- aware routing scenario.","Routing,
Wireless sensor networks,
Energy states,
Topology,
Algorithm design and analysis,
Network topology,
Load management"
Machine learning models for classification of BGP anomalies,"Worms such as Slammer, Nimda, and Code Red I are anomalies that affect performance of the global Internet Border Gateway Protocol (BGP). BGP anomalies also include Internet Protocol (IP) prefix hijacks, miss-configurations, and electrical failures. Statistical and machine learning techniques have been recently deployed to classify and detect BGP anomalies. In this paper, we introduce new classification features and apply Support Vector Machine (SVM) models and Hidden Markov Models (HMMs) to design anomaly detection mechanisms. We apply these multi classification models to correctly classify test datasets and identify the correct anomaly types. The proposed models are tested with collected BGP traffic traces and are employed to successfully classify and detect various BGP anomalies.","Hidden Markov models,
Feature extraction,
Support vector machines,
Accuracy,
Training,
Protocols,
Grippers"
A Comparative Assessment of Ranking Accuracies of Conventional and Machine-Learning-Based Scoring Functions for Protein-Ligand Binding Affinity Prediction,"Accurately predicting the binding affinities of large sets of protein-ligand complexes efficiently is a key challenge in computational biomolecular science, with applications in drug discovery, chemical biology, and structural biology. Since a scoring function (SF) is used to score, rank, and identify drug leads, the fidelity with which it predicts the affinity of a ligand candidate for a protein's binding site has a significant bearing on the accuracy of virtual screening. Despite intense efforts in developing conventional SFs, which are either force-field based, knowledge-based, or empirical, their limited ranking accuracy has been a major roadblock toward cost-effective drug discovery. Therefore, in this work, we explore a range of novel SFs employing different machine-learning (ML) approaches in conjunction with a variety of physicochemical and geometrical features characterizing protein-ligand complexes. We assess the ranking accuracies of these new ML-based SFs as well as those of conventional SFs in the context of the 2007 and 2010 PDBbind benchmark data sets on both diverse and protein-family-specific test sets. We also investigate the influence of the size of the training data set and the type and number of features used on ranking accuracy. Within clusters of protein-ligand complexes with different ligands bound to the same target protein, we find that the best ML-based SF is able to rank the ligands correctly based on their experimentally determined binding affinities 62.5 percent of the time and identify the top binding ligand 78.1 percent of the time. For this SF, the Spearman correlation coefficient between ranks of ligands ordered by predicted and experimentally determined binding affinities is 0.771. Given the challenging nature of the ranking problem and that SFs are used to screen millions of ligands, this represents a significant improvement over the best conventional SF we studied, for which the corresponding ranking performance values are 57.8 percent, 73.4 percent, and 0.677.","Proteins,
Feature extraction,
Training,
Databases,
Drugs,
Accuracy,
Three dimensional displays"
Improved EASI ECG model obtained using various machine learning and regression techniques,"Main idea of this study was to increase efficiency of the EASI ECG method introduced by Dover in 1988 using various regression techniques. EASI was proven to have high correlation with standard 12 lead ECG. Apart from that it is less susceptible to artefacts, increase mobility of patients and is easier to use because of smaller number of electrodes. Multilayer Perceptron (Artificial Neural Network), Support Vector Machines, Linear Regression, Pace Regression and Least Median of Squares Regression methods were used to improve the quality of the 12-lead electrocardiogram derived from four (EASI) electrodes.","Electrocardiography,
Support vector machines,
Electrodes,
Standards,
Kernel,
Machine learning,
Multilayer perceptrons"
Bidirectional Extreme Learning Machine for Regression Problem and Its Learning Effectiveness,"It is clear that the learning effectiveness and learning speed of neural networks are in general far slower than required, which has been a major bottleneck for many applications. Recently, a simple and efficient learning method, referred to as extreme learning machine (ELM), was proposed by Huang , which has shown that, compared to some conventional methods, the training time of neural networks can be reduced by a thousand times. However, one of the open problems in ELM research is whether the number of hidden nodes can be further reduced without affecting learning effectiveness. This brief proposes a new learning algorithm, called bidirectional extreme learning machine (B-ELM), in which some hidden nodes are not randomly selected. In theory, this algorithm tends to reduce network output error to 0 at an extremely early learning stage. Furthermore, we find a relationship between the network output error and the network output weights in the proposed B-ELM. Simulation results demonstrate that the proposed method can be tens to hundreds of times faster than other incremental ELM algorithms.","Machine learning,
Training,
Testing,
Learning systems,
Helium,
Equations,
Computer architecture"
Connective prediction using machine learning for implicit discourse relation classification,"Implicit discourse relation classification is a challenge task due to missing discourse connective. Some work directly adopted machine learning algorithms and linguistically informed features to address this task. However, one interesting solution is to automatically predict implicit discourse connective. In this paper, we present a novel two-step machine learning-based approach to implicit discourse relation classification. We first use machine learning method to automatically predict the discourse connective that can best express the implicit discourse relation. Then the predicted implicit discourse connective is used to classify the implicit discourse relation. Experiments on Penn Discourse Treebank 2.0 (PDTB) and Biomedical Discourse Relation Bank (BioDRB) show that our method performs better than the baseline system and previous work.","Support vector machines,
Optimization"
Extreme learning machines for intrusion detection,"We consider the problem of intrusion detection in a computer network, and investigate the use of extreme learning machines (ELMs) to classify and detect the intrusions. With increasing connectivity between networks, the risk of information systems to external attacks or intrusions has increased tremendously. Machine learning methods like support vector machines (SVMs) and neural networks have been widely used for intrusion detection. These methods generally suffer from long training times, require parameter tuning, or do not perform well in multi-class classification. We propose a basic ELM method based on random features, and a kernel based ELM method for classification. We compare our methods with commonly used SVM techniques in both binary and multi-class classifications. Simulation results show that the proposed basic ELM approach outperforms SVM in training and testing speed, while the proposed kernel based ELM achieves higher detection accuracy than SVM in multi-class classification case.","Support vector machines,
Intrusion detection,
Training,
Kernel,
Neurons,
Computer crime,
Feature extraction"
Empirical study based on machine learning approach to assess the QoS/QoE correlation,"The appearance of new emerging multimedia services have created new challenges for cloud service providers, which have to react quickly to end-users experience and offer a better Quality of Service (QoS). Cloud service providers should use such an intelligent system that can classify, analyze, and adapt to the collected information in an efficient way to satisfy end-users' experience. This paper investigates how different factors contributing the Quality of Experience (QoE), in the context of video streaming delivery over cloud networks. Important parameters which influence the QoE are: network parameters, characteristics of videos, terminal characteristics and types of users' profiles. We describe different methods that are often used to collect QoE datasets in the form of a Mean Opinion Score (MOS). Machine Learning (ML) methods are then used to classify a preliminary QoE dataset collected using these methods. We evaluate six classifiers and determine the most suitable one for the task of QoS/QoE correlation.","Indexes,
Quality of service,
Fires,
Atmospheric measurements,
Particle measurements,
Delay"
A novel image watermarking scheme using Extreme Learning Machine,"In this paper, a novel digital image watermarking algorithm based on a fast neural network known as Extreme Learning Machine (ELM) for two grayscale images is proposed. The ELM algorithm is very fast and completes its training in milliseconds unlike its other counterparts such as BPN. The proposed watermarking algorithm trains the ELM by using low frequency coefficients of the grayscale host image in transform domain. The trained ELM produces a sequence of 1024 real numbers, normalized as per N(0, 1) as an output. This sequence is used as watermark to be embedded within the host image using Cox's formula to obtain the signed image. The visual quality of the signed images is evaluated by PSNR. High PSNR values indicate that the quality of signed images is quite good. The computed high value of SIM (X, X*) establishes that the extraction process is quite successful and overall the algorithm finds good practical applications, especially in situations that warrant meeting time constraints.","Watermarking,
Training,
Discrete cosine transforms,
Neurons,
Vectors,
Mathematical model,
Machine learning"
Automatic feature selection for BCI: An analysis using the davies-bouldin index and extreme learning machines,"In this work, we present a novel framework for automatic feature selection in brain-computer interfaces (BCIs). The proposal, which manipulates features generated in the frequency domain by an estimate of the power spectral density of the EEG signals, is based on feature optimization (with both binary and real coding) using a state-of-the-art artificial immune network, the cob-aiNet. In order to analyze the performance of the proposed framework, two approaches are adopted: a direct use of the Davies-Bouldin index and the use of metrics associated with the operation of an extreme learning machine (ELM) in the role of a classifier. The results reveal that the proposal has the potential of improving the performance of a BCI system, and also provide elements for an analysis of the spectral content of EEG signals and of the performance of ELMs in motor imagery paradigms.","Filtering algorithms,
Indexes,
Training,
Electroencephalography,
Optimization,
Machine learning,
Feature extraction"
"Learning interactions among objects, tools and machines for planning","We propose a method for learning interactions among objects when intermediate state information is not available. Learning is accomplished by observing a given sequence of actions on different objects. We have selected the Incredible Machine game as a suitable domain for analyzing and learning object interactions. We first present how behaviors are represented by finite state machines using the given input. Then, we analyze the impact of the knowledge about relations on the overall performance. Our analysis includes four different types of input: a knowledge base including part relations; spatial information; temporal information; and spatio-temporal information. We show that if a knowledge base about relations is provided, learning is accomplished to a desired extent. Our analysis also indicates that the spatio-temporal approach is superior to the spatial and the temporal approaches and gives close results to that of the knowledge-based approach.","Switches,
Planning,
Knowledge based systems,
Tutorials,
Games,
Machine learning,
Mixers"
Classification of HTTP traffic based on C5.0 Machine Learning Algorithm,"Our previous work demonstrated the possibility of distinguishing several kinds of applications with accuracy of over 99%. Today, most of the traffic is generated by web browsers, which provide different kinds of services based on the HTTP protocol: web browsing, file downloads, audio and voice streaming through third-party plugins, etc. This paper suggests and evaluates two approaches to distinguish various HTTP content: distributed among volunteers' machines and centralized running in the core of the network. We also assess accuracy of the global classifier for both HTTP and non-HTTP traffic. We achieved accuracy of 94%, which supposed to be even higher in real-life usage. Finally, we provided graphical characteristics of different kinds of HTTP traffic.","Streaming media,
Browsers,
Multimedia communication,
Accuracy,
Quality of service,
Training,
Computer networks"
"Promoter recognition with machine learning algorithms keREM, RULSE-3 and ANN","Data mining has become an important and active area of research because of theoretical challenges and practical applications associated with the problem of discovering interesting and previously unknown knowledge from very large real world database. These databases contain potential gold mine of valuable information, but it is beyond human ability to analyze massive amount of data and elicit meaningful patterns by using conventional techniques. In this study, DNA sequence was analyzed to locate promoter which is a regulatory region of DNA located upstream of a gene, providing a control point for regulated gene transcription. In this study, some supervised learning algorithms such as artificial neural network (ANN), RULES-3 and newly developed keREM rule induction algorithm were used to analyse to DNA sequence. In the experiments different option of keREM, RULES-3 and ANN were used, and according to the empirical comparisons, the algorithms appeared to be comparable to well-known algorithms in terms of the accuracy of the extracted rule in classifying unseen data.","Artificial neural networks,
DNA,
Classification algorithms,
Machine learning algorithms,
Neurons,
Algorithm design and analysis,
Biological neural networks"
Online detection of freezing of gait with smartphones and machine learning techniques,"Freezing of gait (FoG) is a common gait deficit in advanced Parkinson's disease (PD). FoG events are associated with falls, interfere with daily life activities and impair quality of life. FoG is often resistant to pharmacologic treatment; therefore effective non-pharmacologic assistance is needed. We propose a wearable assistant, composed of a smartphone and wearable accelerometers, for online detection of FoG. The system is based on machine learning techniques for automatic detection of FoG episodes. When FoG is detected, the assistant provides rhythmic auditory cueing or vibrotactile feedback that stimulates the patient to resume walking. We tested our solution on more than 8h of recorded lab data from PD patients that experience FoG in daily life. We characterize the system performance on user-dependent and user-independent experiments, with respect to different machine learning algorithms, sensor placement and preprocessing window size. The final system was able to detect FoG events with an average sensitivity and specificity of more than 95%, and mean detection latency of 0.34s in user-dependent settings.","Manuals,
Automatic voltage control,
Optimization,
Entropy,
Sensitivity"
Automatic software architecture recovery: A machine learning approach,"Automatically recovering functional architecture of the software can facilitate the developer's understanding of how the system works. In legacy systems, original source code is often the only available source of information about the system and it is very time consuming to understand source code. Current architecture recovery techniques either require heavy human intervention or fail to recover quality components. To alleviate these shortcomings, we propose use of machine learning techniques which use structural, runtime behavioral, domain, textual and contextual (e.g. code authorship, line co-change) features. These techniques will allow us to experiment with a large number of features of the software artifacts without having to establish a priori our own insights about what is important and what is not important. We believe this is a promising approach that may finally start to produce usable solutions to this elusive problem.","Software,
Computer architecture,
Documentation,
Software algorithms,
Machine learning,
Clustering algorithms,
Feature extraction"
A machine learning technique for MRI brain images,"This study presents a proposed hybrid intelligent machine learning technique for Computer-Aided detection system for automatic detection of brain tumor through magnetic resonance images. The technique is based on the following computational methods; the feedback pulse-coupled neural network for image segmentation, the discrete wavelet transform for features extraction, the principal component analysis for reducing the dimensionality of the wavelet coefficients, and the feed forward backpropagation neural network to classify inputs into normal or abnormal. The experiments were carried out on 101 images consisting of 14 normal and 87 abnormal (malignant and benign tumors) from a real human brain MRI dataset. The classification accuracy on both training and test images is 99 % which was significantly good. Moreover, The proposed technique demonstrates its effectiveness compared with the other machine learning recently published techniques.","Magnetic resonance imaging,
Design automation,
Discrete wavelet transforms,
Tumors,
Feature extraction,
Biological neural networks"
A new approach to automatic disc localization in clinical lumbar MRI: Combining machine learning with heuristics,"Lower back pain (LBP) is widely prevalent in people all over the world and negatively affects the quality of life due to chronic pain and change in posture. Automatic localization of intervertebral discs from lumbar MRI is the first step towards computer-aided diagnosis of lower back ailments. Till date, most of the research has been useful in determining a point within each lumbar disc, hence we go one step further and propose a localization method which outputs a tight bounding box for each disc. We use HOG (Histogram of Oriented Gradients) features along with SVM (Support Vector Machine) as classifier and successfully combine these machine learning techniques with heuristics to achieve 99% disc localization accuracy on 53 clinical cases (318 lumbar discs). We also devise our own metrics to evaluate the accuracy and tightness of our disc bounding box and compare our results with previous research.","Magnetic resonance imaging,
Manuals,
Support vector machines,
Feature extraction,
Measurement,
Training,
Accuracy"
Game theoretic mechanism design applied to machine learning classification,"The field of machine learning strives to develop algorithms that, through learning, lead to generalization; that is, the ability of a machine to perform a task that it was not explicitly trained for. Numerous approaches have been developed ranging from neural network models striving to replicate neurophysiology to more abstract mathematical manipulations which identify numerical similarities. Nevertheless a common theme amongst the varied approaches is that learning techniques incorporate a strategic component to try and yield the best possible decision or classification. The mathematics of game theory formally analyzes strategic interactions between competing players and is consequently quite appropriate to apply to the field of machine learning with potential descriptive as well as functional insights. Furthermore, game theoretic mechanism design seeks to develop a framework to achieve a desired outcome, and as such is applicable for defining a paradigm capable of performing classification. In this work we present a game theoretic chip-fire classifier which as an iterated game is able to perform pattern classification.","Games,
Game theory,
Support vector machines,
Machine learning,
Lattices,
Mathematical model,
Conferences"
Attention: A machine learning perspective,"We review a statistical machine learning model of top-down task driven attention based on the notion of `gist'. In this framework we consider the task to be represented as a classification problem with two sets of features - a gist of coarse grained global features and a larger set of low-level local features. Attention is modeled as the choice process over the low-level features given the gist. The model takes its departure in a classical information theoretic framework for experimental design. This approach requires the evaluation over marginalized and conditional distributions. By implementing the classifier within a Gaussian Discrete mixture it is straightforward to marginalize and condition, hence, we obtained a relatively simple expression for the feature dependent information gain - the top-down saliency. As the top-down attention mechanism is modeled as a simple classification problem, we can evaluate the strategy simply by estimating error rates on a test data set. We illustrate the attention mechanism on a simple simulated visual domain in which the choice is over nine patches in which a binary pattern has to be classified. The performance of the classifier equipped with the attention mechanism is almost as good as one that has access to all low-level features and clearly improving over a simple `random attention' alternative.","Computational modeling,
Visualization,
Conferences,
Error analysis,
Machine learning,
Training,
Information processing"
Network traffic anomaly detection using machine learning approaches,"One of the biggest challenges for both network administrators and researchers is detecting anomalies in network traffic. If they had a tool that could accurately and expeditiously detect these anomalies, they would prevent many of the serious problems caused by them. We conducted experiments in order to study the relationship between interval-based features of network traffic and several types of network anomalies by using two famous machine learning algorithms: the naıve Bayes and k-nearest neighbor. Our findings will help researchers and network administrators to select effective interval-based features for each particular type of anomaly, and to choose a proper machine learning algorithm for their own network system.","Machine learning algorithms,
Intrusion detection,
Testing,
Feature extraction,
Machine learning,
Signal processing algorithms,
Classification algorithms"
Comparitive analysis of machine learning techniques for classification of arbovirus,"This paper studies classification methods, comparing svm and Naïve's Bayes analysis as applied to viral disease medical data mining. The objective of this study is to explore possibility of applying machine learning techniques such as SVM and Naïve Bayes algorithm for classification to predict the susceptibility for complex disease-Dengue. Both of these algorithms were chosen for their simple, amazing and accurate results. The proposed work is to experiment machine learning algorithms to the available arbovirus that is causing frequent recurrent epidemics. In this paper, we discuss the application of machine learning techniques that make a distinction between dengue and other feverish illnesses in the primary care setting and predict severe arboviral disease among population. By investigating the arboviral dataset from one of the largest outbreaks that affected India in recent times, we master the methodology and validate classification performance as a measurement of the salience for the discovered associations. The result of the comparison between the methods showed that SVM outperforms the Naïve Bayes in Dengue disease diagnosis.","Support vector machines,
Laboratories,
Classification algorithms,
Accuracy,
Information services,
Electronic publishing,
Internet"
Bussiness-driven automatic IT change management based on machine learning,"Growing complexity of customer needs is one of the prevailing problems faced by IT enterprises at present, leading to increasingly complex IT service management systems. At the same time, quick response to unexpected problems and externally imposed requirements are testing the IT change management. In order to solve the problems mentioned above and satisfy the customer needs timely, we consider automating the change management process with business-driven perspective so as to reduce the service interruption time and cost brings by changes. This paper proposes a solution for automation of the whole change management process and also assesses and validates the change solution we selected.","Business,
Machine learning,
Training,
Accuracy,
Biological neural networks,
Data mining"
Gesture imitation using machine learning techniques,"This study is a part of an ongoing project which aims to assist in teaching Sign Language (SL) to hearing-impaired children by means of non-verbal communication and imitation-based interaction games between a humanoid robot and a child. In this paper, the problem is geared towards a robot learning to imitate basic upper torso gestures (SL signs) using different machine learning techniques. RGBD sensor (Microsoft Kinect) is employed to track the skeletal model of humans and create a training set. A novel method called Decision Based Rule is proposed. Additionally, linear regression models are compared to find which learning technique has a higher accuracy on gesture prediction. The learning technique with the highest accuracy is then used to simulate an imitation system where the Nao robot imitates these learned gestures as observed by the users.","Joints,
Mathematical model,
Humans,
Vectors,
Robot sensing systems,
Accuracy"
Emotion classification based on physiological signals induced by negative emotions: Discriminantion of negative emotions by machine learning algorithm,"Recently, the one of main topic of emotion recognition or classification research is to recognize human's feeling or emotion using various physiological signals. It is one of the core processes to implement emotional intelligence in human computer interaction (HCI) research. The purpose of this study was to identify the best algorithm being able to discriminate negative emotions, such as sadness, fear, surprise, and stress using physiological features. Electrodermal activity (EDA), electrocardiogram (ECG), skin temperature (SKT), and photoplethysmography (PPG) are recorded and analyzed as physiological signals. And emotional stimuli used in this study are audio-visual film clips which have examined for their appropriateness and effectiveness through preliminary experiment. For classification of negative emotions, five machine learning algorithms, i.e., LDF, CART, SOM, Naïve Bayes and SVM are used. Result of emotion classification shows that an accuracy of emotion classification by SVM (100.0%) was the highest and by LDA (50.7%) was the lowest. CART showed emotion classification accuracy of 84.0%, SOM was 51.2% and Naïve Bayes was 76.2%. This can be helpful to provide the basis for the emotion recognition technique in HCI.","Stress,
Physiology,
Emotion recognition,
Support vector machines,
Accuracy,
Classification algorithms,
Feature extraction"
Dermoscopic image segmentation and classification using machine learning algorithms,"Dermoscopy is the method of examining the skin lesions. It is especially used for diagnosing melanoma, a type of skin cancer. Image segmentation and classification are important tools to provide the information about the Dermoscopic images clinically in terms of its size and shape. Many algorithms were developed for classification and segmentation of Dermoscopic images. This work proposes the tasks of extracting, classifying and segmenting the Dermoscopic image using the machine learning algorithms. The algorithms such as Back Propagation network (BPN), Radial Basis Function Network (RBF) and Extreme Learning Machine (ELM) are used. The features are extracted from the Dermoscopic image and these features are used to train the classifiers. The trained networks are used for segmentation. The results are compared with the ground truth images and their performance is evaluated. The results proved that the ELM has better accuracy, faster training period and it provides better segmentation than the BPN and RBF neural networks.","Image segmentation,
Training,
Feature extraction,
Computer languages,
Manuals,
Indexes,
Data mining"
Knowledge Management Toolbox: Machine Learning for Cognitive Radio Networks,"Learning mechanisms are essential for the attainment of experience and knowledge in cognitive radio (CR) systems, exposed to high dynamics with often unpredictable states [1]. These mechanisms can be associated with user and device profiles, context, and decisions. The focus learning user preferences is the dynamic inference and estimation of current and future user preferences. The acquisition and learning of context information encompasses mechanisms for the system to perceive its current status and conditions in its present environment, as well as estimating (and forecasting) the capabilities of available network configurations. Finally, learning related to decisions addresses the building of knowledge with respect to the efficiency of solutions that can be applied to specific situations encountered. Based on knowledge obtained through learning, decision-making mechanisms can become faster, since the CR system can learn and immediately apply solutions that have been identified as being efficient in the past. Moreover, knowledge obtained through learning mechanisms may be shared among nodes of a system. Thus, more reliable and more optimal decisions can be made by exploiting knowledge obtained through learning mechanisms.","Knowledge acquisition,
Quality of service,
Knowledge engineering,
Object recognition,
Optimization,
Decision making,
Learning systems"
Stochastic Subset Selection for Learning With Kernel Machines,"Kernel machines have gained much popularity in applications of machine learning. Support vector machines (SVMs) are a subset of kernel machines and generalize well for classification, regression, and anomaly detection tasks. The training procedure for traditional SVMs involves solving a quadratic programming (QP) problem. The QP problem scales super linearly in computational effort with the number of training samples and is often used for the offline batch processing of data. Kernel machines operate by retaining a subset of observed data during training. The data vectors contained within this subset are referred to as support vectors (SVs). The work presented in this paper introduces a subset selection method for the use of kernel machines in online, changing environments. Our algorithm works by using a stochastic indexing technique when selecting a subset of SVs when computing the kernel expansion. The work described here is novel because it separates the selection of kernel basis functions from the training algorithm used. The subset selection algorithm presented here can be used in conjunction with any online training technique. It is important for online kernel machines to be computationally efficient due to the real-time requirements of online environments. Our algorithm is an important contribution because it scales linearly with the number of training samples and is compatible with current training techniques. Our algorithm outperforms standard techniques in terms of computational efficiency and provides increased recognition accuracy in our experiments. We provide results from experiments using both simulated and real-world data sets to verify our algorithm.","Kernel,
Training,
Noise,
Support vector machines,
Vectors,
Machine learning,
Computational complexity"
Genetic & Evolutionary Biometrics: Feature extraction from a Machine Learning perspective,"Genetic & Evolutionary Biometrics (GEB) is a newly emerging area of study devoted to the design, analysis, and application of genetic and evolutionary computing to the field of biometrics. In this paper, we present a GEB application called GEFEML (Genetic and Evolutionary Feature Extraction - Machine Learning). GEFEML incorporates a machine learning technique, referred to as cross validation, in an effort to evolve a population of local binary pattern feature extractors (FEs) that generalize well to unseen subjects. GEFEML was trained on a dataset taken from the FRGC database and generalized well on two test sets of unseen subjects taken from the FRGC and MORPH databases. GEFEML evolved FEs that used fewer patches, had comparable accuracy, and were 54% less expensive in terms of computational complexity.","Iron,
FETs,
Complexity theory,
Optimization,
Vectors"
A machine learning based approach for predicting undisclosed attributes in social networks,"Online Social Networks have gained increased popularity in recent years. However, besides their many advantages, they also represent privacy risks for the users. In order to control access to their private information, users of OSNs are typically allowed to set the visibility of their profile attributes, but this may not be sufficient, because visible attributes, friendship relationships, and group memberships can be used to infer private information. In this paper, we propose a fully automated approach based on machine learning for inferring undisclosed attributes of OSN users. Our method can be used for both classification and regression tasks, and it makes large scale privacy attacks feasible. We also provide experimental results showing that our method achieves good performance in practice.","Correlation,
Input variables,
Social network services,
Communities,
Neurons,
Training,
Privacy"
A Machine Learning Perspective on Predictive Coding with PAQ8,"PAQ8 makes use of several simple machine learning models and algorithms. We show how understanding PAQ8 enables us to improve the algorithms. We also present a broad range of new applications of PAQ8 to machine learning tasks including language modeling and adaptive text prediction, adaptive game playing, classification, and lossy compression using features acquired via unsupervised learning.","Context,
Predictive models,
Image coding,
Adaptation models,
Mixers,
History,
Prediction algorithms"
Evaluation of machine learning techniques for face detection and recognition,"Biometric identification (BI) is one of the most explored topics in recent years. One of the most important techniques for BI is face recognition. Face recognition systems (FRSs) are an important field in computer vision, because it represents a non-invasive BI technique. In this paper, a FRS is proposed. In the first step, a face detection algorithm is used for extracting faces from video frames (training videos) and generating a face database. In a second step, filtering and preprocessing are applied to face images obtained in the previous step. In a third step, a collection of machine learning algorithms are trained using as input data the faces obtained in the previous step. Finally, the classifiers are used for classify faces obtained from video frames (test videos). The obtained results shows the suitability of this approach for analyzing large collections of videos where previous face labels are not available.","Face,
Training,
Face recognition,
Face detection,
Decision trees,
Databases,
Accuracy"
Towards open machine learning: Mloss.org and mldata.org,"Machine Learning (ML) is a scientific field comprised of both theoretical and empirical results. For methodological advances, one key aspect of reproducible research is the ability to compare a proposed approach with the current state of the art. Such a comparison can be theoretical in nature, but often a detailed theoretical analysis is not possible or may not tell the whole story. In such cases, an empirical comparison is necessary. To produce reproducible machine learning research, there are three main required components that need to be easily available: - The paper describing the method clearly and comprehensively. - The data on which the results are computed. - Software (possibly source code) that implements the method and produces the figures and tables of results in the paper. We share our experiences about mloss.org and mldata.org, community efforts towards encouraging open source software and open data in machine learning.","Machine learning,
Educational institutions,
Neuroscience,
Search engines,
Open source software,
Systems biology,
Learning systems"
A Decision-Making Method Using Knowledge-Based Machine Learning,"This paper discusses how to use knowledge in machine learning, so that decision-making with knowledge-based machine learning can be obtained. Objectives, anti-objectives and it is support of decisions are formally defined, and the knowledge base, evidence base, sample base and qualified samples for decision problem are discussed in details. This paper discusses the uncertainty in the dicision problem with a method of how to use knowledge for measuring the supports of our objectives form knowledge in our hands. Three functions, namely: the initial evaluation function, sample evaluation function, making the evaluation function are proposed.","Decision making,
Machine learning,
Learning systems,
Knowledge based systems,
Cognition,
Reliability,
Computers"
Information extraction from ultrawideband ground penetrating radar data: A machine learning approach,"To detect and characterize pipes and cables buried in the ground and to track their course we propose a new approach, which consists of an ultrawideband radar system employed as Ground Penetrating Radar (GPR) and a machine learning algorithm for the objects' hyperbola identification and evaluation directly in the recorded radargram.","Ground penetrating radar,
Machine learning algorithms,
Machine learning,
Generators,
Antenna measurements,
Noise"
Challenges of Machine Learning Based Monitoring for Industrial Control System Networks,"Detecting network intrusions and anomalies in industrial control systems is growing in urgency. Such systems used to be isolated but are now being connected to the outside world. Even in the case of isolated networks, privileged users may still present various threats to the system, either accidentally or intentionally. Also malfunctions in devices may cause anomalous traffic. Anomaly detection based network monitoring and intrusion detection systems could be capable of discerning normal and aberrant traffic in industrial control systems, detecting security incidents in an early phase. In this paper we discuss the challenges for such a monitoring system. One of the challenges is which features best differentiate between anomalous and normal behaviour. In the analysis, special focus is placed on this selection.","Machine learning,
Monitoring,
Production facilities,
Protocols,
Industrial control,
Intrusion detection"
Machine learning methods for human-computer interaction [tutorial],"In this tutorial, I will cover various machine learning methods for pattern recognition at an overview level illustrated with case studies mostly taken from haptics applications, and further lay out the space covered by other methods without reviewing them specifically. I will only talk about basic statistical pattern recognition methods applied for supervised learning; namely, Bayesian decision theory, linear discriminant, and k-nearest neighbor methods; emphasizing the distinction between generative and discriminative approaches. I will close by mentioning commonly used extensions of the introduced methods and by providing resources for the participants to follow up with. I will also provide some guidelines on parameter selection and optimization for the classifiers, which is still a research problem in pattern recognition.",
Autonomous driving: A comparison of machine learning techniques by means of the prediction of lane change behavior,"In the presented work we compare machine learning techniques in the context of lane change behavior performed by humans in a semi-naturalistic simulated environment. We evaluate different learning approaches using differing feature combinations in order to identify appropriate feature, best feature combination, and the most appropriate machine learning technique for the described task. Based on the data acquired from human drivers in the traffic simulator NISYS TRS1, we trained a recurrent neural network, a feed forward neural network and a set of support vector machines. In the followed test drives the system was able to predict lane changes up to 1.5 sec in beforehand.","Roads,
Vehicles,
Support vector machines,
Machine learning,
Training,
Neurons,
Humans"
Machine learning in bioinformatics,"This article reviews machine learning methods for bioinformatics. Applications in genomics, proteomics, systems biology, evolution and text mining are also shown. It presents the trends of machine learning methods, such as ANNs, SVMs, Random forests and so on.","Genomics,
Bioinformatics,
Predictive models,
Computational modeling,
Biological system modeling,
Markov processes"
A machine learning framework for trait based genomics,"Microbial communities perform many important ecological functions across a wide range of natural and man-made environments. Recently, the utility of trait based approaches for microbial communities has been identified. Increasing availability of whole genome sequences provide the opportunity to explore the genetic foundations of a variety of functional traits. In this paper, we proposed a machine learning framework to quantitatively link the genotype with functional traits. Genes from bacteria genomes belonging to different functional trait groups were grouped to Cluster of Orthologs (COGs), and were used as features. Then, TF-IDF technique from the text mining domain was applied to transform the data to accommodate the abundance and importance of each COG. After TF-IDF processing, COGs were ranked using feature selection methods to identify their relevance to the functional trait of interest. We focused on a binary functional trait in this paper, but plan to extend our approach to continuous functional traits in the future. Experimental results demonstrated that functional trait related genes can be detected using our method.","Bioinformatics,
Genomics,
Microorganisms,
Support vector machines,
Accuracy,
Communities"
Autonomous parameter optimization of a heterogeneous wireless network aggregation system using machine learning algorithms,"By increase of various radio access network (RAN) services, available spectrum resources for mobile communications get decrease, and efficient use of the radio resource is becoming a very important issue. In order to optimize the radio resource usage and maxmize the throughput and quality of service (QoS), the link aggregation technologies to utilize multiple different available RANs have been studied. However, in such heterogeneous wireless networks, it is difficult to improve the throughput by their aggregation because of the differences among the QoSs of the different RANs. In this paper, we propose an autonomous parameter optimization scheme using a machine learning algorithm, which maximize the throughput of the heterogeneous RAN aggregation system. We evaluate the performance of the proposed scheme implemented on a cognitive wireless network system called Cognitive Wireless Cloud (CWC) system, connected to real wireless network services, such as HSDPA, WiMAX and W-CDMA. Our experimental results of the proposed system show that the aggregation throughput can be improved with increase of the training samples, which are collected autonomously.","Radio access networks,
Throughput,
Quality of service,
Wireless networks,
Training,
Resource management"
Online sequential extreme learning machine for classification of mycobacterium tuberculosis in ziehl-neelsen stained tissue,"The application of image processing and artificial intelligence for computer-aided tuberculosis (TB) diagnosis has received considerable attention over the past several years and still is an active research area. Several approaches have been proposed to improve the diagnostic performance in term of diagnostic accuracy and processing efficiency. This paper studies the performance of a recent training algorithm called Online Sequential Extreme Learning Machine (OS-ELM) for detection and classification of TB bacilli in tissue specimens. The algorithm is used to train a single hidden layer feedforward network (SLFN) using a set of data consists of simple geometrical features, such as area, perimeter, eccentricity and shape factor as feature vectors. All of these features are extracted from tissue images which consist of TB bacilli and further classified into three types; TB, overlapped TB and non-TB. Promising result with 91.33% of testing accuracy has been achieved for the OS-ELM using sigmoid activation function and 40-by-40 learning mode.","Training,
Classification algorithms,
Learning systems,
Machine learning,
Feature extraction,
Joining processes"
An End-to-End Machine Learning System for Harmonic Analysis of Music,"We present a new system for the harmonic analysis of popular musical audio. It is focused on chord estimation, although the proposed system additionally estimates the key sequence and bass notes. It is distinct from competing approaches in two main ways. First, it makes use of a new improved chromagram representation of audio that takes the human perception of loudness into account. Furthermore, it is the first system for joint estimation of chords, keys, and bass notes that is fully based on machine learning, requiring no expert knowledge to tune the parameters. This means that it will benefit from future increases in available annotated audio files, broadening its applicability to a wider range of genres. In all of three evaluation scenarios, including a new one that allows evaluation on audio for which no complete ground truth annotation is available, the proposed system is shown to be faster, more memory efficient, and more accurate than the state-of-the-art.","Hidden Markov models,
Harmonic analysis,
Vectors,
Topology,
Maximum likelihood estimation,
Humans"
Machine Learning Techniques as a Helpful Tool Toward Determination of Plaque Vulnerability,"Atherosclerotic cardiovascular disease results in millions of sudden deaths annually, and coronary artery disease accounts for the majority of this toll. Plaque rupture plays main role in the majority of acute coronary syndromes. Rupture has been usually associated with stress concentrations, which are determined mainly by tissue properties and plaque geometry. The aim of this study is develop a tool, using machine learning techniques to assist the clinical professionals on decisions of the vulnerability of the atheroma plaque. In practice, the main drawbacks of 3-D finite element analysis to predict the vulnerability risk are the huge main memories required and the long computation times. Therefore, it is essential to use these methods which are faster and more efficient. This paper discusses two potential applications of computational technologies, artificial neural networks and support vector machines, used to assess the role of maximum principal stress in a coronary vessel with atheroma plaque as a function of the main geometrical features in order to quantify the vulnerability risk.","Support vector machines,
Lipidomics,
Atherosclerosis,
Kernel,
Machine learning,
Neurons"
Extreme Learning Machine for Regression and Multiclass Classification,"Due to the simplicity of their implementations, least square support vector machine (LS-SVM) and proximal support vector machine (PSVM) have been widely used in binary classification applications. The conventional LS-SVM and PSVM cannot be used in regression and multiclass classification applications directly, although variants of LS-SVM and PSVM have been proposed to handle such cases. This paper shows that both LS-SVM and PSVM can be simplified further and a unified learning framework of LS-SVM, PSVM, and other regularization algorithms referred to extreme learning machine (ELM) can be built. ELM works for the “generalized” single-hidden-layer feedforward networks (SLFNs), but the hidden layer (or called feature mapping) in ELM need not be tuned. Such SLFNs include but are not limited to SVM, polynomial network, and the conventional feedforward neural networks. This paper shows the following: 1) ELM provides a unified learning platform with a widespread type of feature mappings and can be applied in regression and multiclass classification applications directly; 2) from the optimization method point of view, ELM has milder optimization constraints compared to LS-SVM and PSVM; 3) in theory, compared to ELM, LS-SVM and PSVM achieve suboptimal solutions and require higher computational complexity; and 4) in theory, ELM can approximate any target continuous function and classify any disjoint regions. As verified by the simulation results, ELM tends to have better scalability and achieve similar (for regression and binary class cases) or much better (for multiclass cases) generalization performance at much faster learning speed (up to thousands times) than traditional SVM and LS-SVM.","Support vector machines,
Optimization,
Kernel,
Training,
Feedforward neural networks,
Machine learning,
Approximation methods"
SVD-Based Quality Metric for Image and Video Using Machine Learning,"We study the use of machine learning for visual quality evaluation with comprehensive singular value decomposition (SVD)-based visual features. In this paper, the two-stage process and the relevant work in the existing visual quality metrics are first introduced followed by an in-depth analysis of SVD for visual quality assessment. Singular values and vectors form the selected features for visual quality assessment. Machine learning is then used for the feature pooling process and demonstrated to be effective. This is to address the limitations of the existing pooling techniques, like simple summation, averaging, Minkowski summation, etc., which tend to be ad hoc. We advocate machine learning for feature pooling because it is more systematic and data driven. The experiments show that the proposed method outperforms the eight existing relevant schemes. Extensive analysis and cross validation are performed with ten publicly available databases (eight for images with a total of 4042 test images and two for video with a total of 228 videos). We use all publicly accessible software and databases in this study, as well as making our own software public, to facilitate comparison in future research.","Visualization,
Measurement,
Machine learning,
Feature extraction,
Quality assessment,
Discrete Fourier transforms,
Image quality"
Combining Statistical Machine Learning with Transformation Rule Learning for Vietnamese Word Sense Disambiguation,"Word Sense Disambiguation (WSD) is the task of determining the right sense of a word depending on the context it appears. Among various approaches developed for this task, statistical machine learning methods have been showing their advantages in comparison with others. However, there are some cases which cannot be solved by a general statistical model. This paper proposes a novel framework, in which we use the rules generated by transformation based learning (TBL) to improve the performance of a statistical machine learning model. This framework can be considered as a combination of a rule-based method and statistical based method. We have developed this method for the problem of Vietnamese WSD and achieved some promising results.","Context,
Training,
Niobium,
Machine learning,
Accuracy,
Learning systems,
Data models"
A method for classification of network traffic based on C5.0 Machine Learning Algorithm,"Monitoring of the network performance in highspeed Internet infrastructure is a challenging task, as the requirements for the given quality level are service-dependent. Backbone QoS monitoring and analysis in Multi-hop Networks requires therefore knowledge about types of applications forming current network traffic. To overcome the drawbacks of existing methods for traffic classification, usage of C5.0 Machine Learning Algorithm (MLA) was proposed. On the basis of statistical traffic information received from volunteers and C5.0 algorithm we constructed a boosted classifier, which was shown to have ability to distinguish between 7 different applications in test set of 76,632-1,622,710 unknown cases with average accuracy of 99.3-99.9%. This high accuracy was achieved by using high quality training data collected by our system, a unique set of parameters used for both training and classification, an algorithm for recognizing flow direction and the C5.0 itself. Classified applications include Skype, FTP, torrent, web browser traffic, web radio, interactive gaming and SSH. We performed subsequent tries using different sets of parameters and both training and classification options. This paper shows how we collected accurate traffic data, presents arguments used in classification process, introduces the C5.0 classifier and its options, and finally evaluates and compares the obtained results.","Accuracy,
Training,
Payloads,
Decision trees,
Machine learning algorithms,
Error analysis,
Quality of service"
Using machine learning to detect problems in ECG data collection,"We describe a data-driven approach, using a combination of machine learning algorithms to solve the 2011 Physionet/Computing in Cardiology (CinC) challenge - identifying data collection problems at 12 leads electrocardiography (ECG). Our data-driven approach reaches an internal (cross-validation) accuracy of almost 93% on the training set, and accuracy of 91.2% on the test set.","Electrocardiography,
Accuracy,
Training,
Vegetation,
Lead,
Machine learning,
Medical services"
Learning to Estimate Slide Comprehension in Classrooms with Support Vector Machines,"Comprehension assessment is an essential tool in classroom learning. However, the judgment often relies on experience of an instructor who makes observation of students' behavior during the lessons. We argue that students should report their own comprehension explicitly in a classroom. With students' comprehension made available at the slide level, we apply a machine learning technique to classify presentation slides according to comprehension levels. Our experimental result suggests that presentation-based features are as predictive as bag-of-words feature vector which is proved successful in text classification tasks. Our analysis on presentation-based features reveals possible causes of poor lecture comprehension.","Support vector machines,
Machine learning,
Materials,
Training,
Accuracy,
Kernel,
Feature extraction"
A machine learning based algorithm for routing bandwidth-guaranteed paths in MPLS TE: Improvements and performance assessment,"The RRATE algorithm opened a new class of solutions for the computation of bandwidth-guaranteed paths in MPLS networks giving significant increase in both path selection speed and efficiency. Nevertheless, analyzing RRATE behavior while dealing with complex and big (in terms of nodes and link number) networks, has been shown that are possible more enhancements if changes are made to the core function that evaluates the “goodness” of a path over other paths. That function takes into account the critical links and the residual bandwidths available on the network. More specifically, changes have been made to the way those values are computed taking into account no more the scalar value of residual bandwidths and critical links but a ranking of the first values and an appropriate weighting of the second parameters. The obtained results reveal steady performance gains over the original RRATE algorithm and, in larger extent, over legacy algorithms like MIRA.","Bandwidth,
Routing,
Algorithm design and analysis,
Machine learning algorithms,
Heuristic algorithms,
Multiprotocol label switching,
Routing protocols"
Clustering and Managing Data Providing Services Using Machine Learning Technique,"In service-oriented computing, a user usually needs to locate a desired service for (i) fulfilling her requirements, or (ii) replacing a service, which disappears or is unavailable for some reasons, to perform an interaction. With the increasing number of services available within an enterprise and over the internet, locating a service online may not be appropriate from the performance perspective, especially in large internet-based service repositories. Instead, services usually need to be clustered offline according to their similarity. Thereafter, services in one or several clusters are necessary to be examined online during dynamic service discovery. In this paper we propose to cluster data providing (DP) services using a refined fuzzy C-means algorithm. We consider the composite relation between DP service elements (i.e., input, output, and semantic relation between them) when representing DP services in terms of vectors. A DP service vector is assigned to one or multiple clusters with certain degrees. When grouping similar services into one cluster, while partitioning different services into different clusters, the capability of service search engine is improved significantly.","Vectors,
Ontologies,
Drugs,
Semantics,
Clustering methods,
Internet"
Machine learning and pattern classification in identification of indigenous retinal pathology,"Diabetic retinopathy (DR) is a complication of diabetes, which if untreated leads to blindness. DR early diagnosis and treatment improve outcomes. Automated assessment of single lesions associated with DR has been investigated for sometime. To improve on classification, especially across different ethnic groups, we present an approach using points-of-interest and visual dictionary that contains important features required to identify retinal pathology. Variation in images of the human retina with respect to differences in pigmentation and presence of diverse lesions can be analyzed without the necessity of preprocessing and utilizing different training sets to account for ethnic differences for instance.","Lesions,
Training,
Dictionaries,
Visualization,
Retina,
Image color analysis,
Pathology"
Automatic classification of pathological gait patterns using ground reaction forces and machine learning algorithms,"An automated gait classification method is developed in this study, which can be applied to analysis and to classify pathological gait patterns using 3D ground reaction force (GRFs) data. The study involved the discrimination of gait patterns of healthy, cerebral palsy (CP) and multiple sclerosis subjects. The acquired 3D GRFs data were categorized into three groups. Two different algorithms were used to extract the gait features; the GRFs parameters and the discrete wavelet transform (DWT), respectively. Nearest neighbor classifier (NNC) and artificial neural networks (ANN) were also investigated for the classification of gait features in this study. Furthermore, different feature sets were formed using a combination of the 3D GRFs components (mediolateral, anterioposterior, and vertical) and their various impacts on the acquired results were evaluated. The best leave-one-out (LOO) classification accuracy 85% was achieved. The results showed some improvement through the application of a features selection algorithm based on M-shaped value of vertical force and the statistical test ANOVA of mediolateral and anterioposterior forces. The optimal feature set of six features enhanced the accuracy to 95%. This work can provide an automated gait classification tool that may be useful to the clinician in the diagnosis and identification of pathological gait impairments.","Accuracy,
Artificial neural networks,
Feature extraction,
Pathology,
Discrete wavelet transforms,
Force,
Classification algorithms"
Online System for Grid Resource Monitoring and Machine Learning-Based Prediction,"Resource allocation and job scheduling are the core functions of grid computing. These functions are based on adequate information of available resources. Timely acquiring resource status information is of great importance in ensuring overall performance of grid computing. This work aims at building a distributed system for grid resource monitoring and prediction. In this paper, we present the design and evaluation of a system architecture for grid resource monitoring and prediction. We discuss the key issues for system implementation, including machine learning-based methodologies for modeling and optimization of resource prediction models. Evaluations are performed on a prototype system. Our experimental results indicate that the efficiency and accuracy of our system meet the demand of online system for grid resource monitoring and prediction.","Monitoring,
Predictive models,
Information services,
Registers,
Data models,
Computer architecture,
Containers"
"Reinforcing conceptual engineering design with a hybrid computer vision, machine learning and knowledge based system framework","We propose a novel system that aids engineers in the conceptual stage of design. Our system's goal is to support the engineer without limiting his creative role; thus, our proposed method does not produce ready study solutions but rather actively monitors the design procedure, verifying design stages and pointing out potential mistakes. This is achieved with a hybrid computer vision, machine learning and knowledge based system framework. Design stage identification is performed with a novel algorithm which comprises a classification stage based on Random Forests and examination of the temporal relationships between the engineer's actions with the aid of statistical graphical models. Experimental results captured in a complex, real life scenario demonstrate our system's ability to efficiently support the engineer's decisions during the conceptual stage of design.","Hidden Markov models,
Radio frequency,
Bridges,
Monitoring,
Sections,
Vegetation,
Computer vision"
A machine learning approach to modeling power and performance of chip multiprocessors,"Exploring the vast microarchitectural design space of chip multiprocessors (CMPs) through the traditional approach of exhaustive simulations is impractical due to the long simulation times and its super-linear increase with core scaling. Kernel based statistical machine learning algorithms can potentially help predict multiple performance metrics with non-linear dependence on the CMP design parameters. In this paper, we describe and evaluate a machine learning framework that uses Kernel Canonical Correlation Analysis (KCCA) to predict the power dissipation and performance of CMPs. Specifically we focus on modeling the microarchitecture of a highly multithreaded CMP targeted towards packet processing. We use a cycle accurate CMP simulator to generate training samples required to build the model. Despite sampling only 0.016% of the design space we observe a median error of 6-10% in the KCCA predicted processor power dissipation and performance.","Power dissipation,
Microarchitecture,
Vectors,
Instruction sets,
Measurement,
Kernel,
Predictive models"
Adaptive Scheduling on Power-Aware Managed Data-Centers Using Machine Learning,"Energy-related costs have become one of the major economic factors in IT data-centers, and companies and the research community are currently working on new efficient power-aware resource management strategies, also known as ""Green IT"". Here we propose a framework for autonomic scheduling of tasks and web-services on cloud environments, optimizing the profit taking into account revenue for task execution minus penalties for service-level agreement violations, minus power consumption cost. The principal contribution is the combination of consolidation and virtualization technologies, mathematical optimization methods, and machine learning techniques. The data-center infrastructure, tasks to execute, and desired profit are casted as a mathematical programming model, which can then be solved in different ways to find good task scheduling. We use an exact solver based on mixed linear programming as a proof of concept but, since it is an NP-complete problem, we show that approximate solvers provide valid alternatives for finding approximately optimal schedules. The machine learning is used to estimate the initially unknown parameters of the mathematical model. In particular, we need to predict a priori resource usage (such as CPU consumption) by different tasks under current workloads, and estimate task service-level-agreement (such as response time) given workload features, host characteristics, and contention among tasks in the same host. Experiments show that machine learning algorithms can predict system behavior with acceptable accuracy, and that their combination with the exact or approximate schedulers manages to allocate tasks to hosts striking a balance between revenue for executed tasks, quality of service, and power consumption.","Time factors,
Power demand,
Mathematical model,
Load modeling,
Machine learning,
Predictive models,
Web services"
Machine self-learning applications in security systems,"A set of conflict resolution methods is investigated with the purpose to construct knowledge refinement and self-learning tools applicable in a wide range of security systems (SS). The introduction of ontologies simplifies the detection process and lowers the complexity of the machine learning procedures. Different conflict resolution ways lead to particular autonomous/intelligent applications in different types of SS. The proposed self-learning methods are combinable with other web/data mining, anomaly detection, statistical methods, and show new ways in the development of collective evolutionary systems.","Security,
Ontologies,
Training,
Multiagent systems,
Software,
Machine learning"
Achieving Memetic Adaptability by Means of Agent-Based Machine Learning,"Over recent years, there has been increasing interest of the research community towards evolutionary algorithms, i.e., algorithms that exploit computational models of natural processes to solve complex optimization problems. In spite of their ability to explore promising regions of the search space, they present two major drawbacks: 1) they can take a relatively long time to locate the exact optimum and 2) may sometimes not find the optimum with sufficient precision. Memetic Algorithms are evolutionary algorithms inspired by both Darwinian principles and Dawkins' notion of a meme, able not only to converge to high-quality solutions, but also search more efficiently than their conventional evolutionary counterparts. However, memetic approaches are affected by several design issues related to the different choices that can be made to implement them. This paper introduces a multiagent-based memetic algorithm which executes in a parallel way different cooperating optimization strategies in order to solve a given problem's instance in an efficient way. The algorithm adaptation is performed by jointly exploiting a knowledge extraction process together with a decision making framework based on fuzzy methodologies. The effectiveness of our approach is tested in several experiments in which our results are compared with those obtained by nonadaptive memetic algorithms. The superiority of the proposed strategy is manifest in the majority of cases.","Algorithm design and analysis,
Multiagent systems,
Memetics,
Machine learning,
Data mining"
Mobility Prediction Based on Machine Learning,Mobile applications are required to operate in highly dynamic pervasive computing environments of dynamic nature and predict the location of mobile users in order to act proactively. We focus on the location prediction and propose a new model/framework. Our model is used for the classification of the spatial trajectories through the adoption of Machine Learning (ML) techniques. Predicting location is treated as a classification problem through supervised learning. We perform the performance assessment of our model through synthetic and real-world data. We monitor the important metrics of prediction accuracy and training sample size.,"Trajectory,
Predictive models,
Hidden Markov models,
Complexity theory,
Training,
Accuracy,
Computational modeling"
Kernel based learning approach for satellite image classification using support vector machine,"Machine learning is a scientific computing discipline to automatically learn to recognize complex patterns and make intelligent decisions based on the set of observed examples (training data). Support Vector Machine (SVM) is a supervised machine learning method used for classification. An SVM kernel based algorithm builds a model for transforming a low dimension feature space into high dimension feature space to find the maximum margin between the classes. In the field of geospatial data processing, there is a high degree of interest to find an optimal image classifier technique. Many image classification methods such as maximum likelihood, K-Nearest are being used for determining crop patterns, land use and mining other useful geospatial information. But SVM is now considered to be one of the powerful kernel based classifier that can be adopted for resolving classification problems. The objective of the study is to use SVM technique for classifying multi spectral satellite image dataset and compare the overall accuracy with the conventional image classification method. LISS-3 and AWIFS sensors data from Resourcesat-1, Indian Remote Sensing (IRS) platform were used for this analysis. In this study, some of the open source tools were used to find out whether SVM can be a potential classification technique for high performance satellite image classification.","Support vector machines,
Kernel,
Accuracy,
Training,
Image classification,
Satellites,
Remote sensing"
A machine learning approach for fingerprint based gender identification,"This paper deals with the problem of gender classification using fingerprint images. Our attempt to gender identification follows the use of machine learning to determine the differences between fingerprint images. Each image in the database was represented by a feature vector consisting of ridge thickness to valley thickness ratio (RTVTR) and the ridge density values. By using a support vector machine trained on a set of 150 male and 125 female images, we obtain a robust classifying function for male and female feature vector patterns.","Fingerprint recognition,
Image matching,
Support vector machines,
Feature extraction,
Training,
Indexes"
Internet Traffic Classification Using Machine Learning: A Token-based Approach,"Due to the increasing unreliability of traditional port-based methods, Internet traffic classification has attracted a lot of research efforts in recent years. Quite a lot of previous papers have focused on using statistical characteristics as discriminators and applying machine learning techniques to classify the traffic flows. In this paper, we propose a novel machine learning based approach where the features are extracted from packet payload instead of flow statistics. Specifically, every flow is represented by a feature vector, in which each item indicates the occurrence of a particular token, i.e., a common substring, in the payload. We have applied various machine learning algorithms to evaluate the idea and used different feature selection schemes to identify the critical tokens. Experimental result based on a real-world traffic data set shows that the approach can achieve high accuracy with low overhead.","Machine learning,
Payloads,
Internet,
Training,
Classification algorithms,
Protocols,
Bayesian methods"
Adaptive functional module selection using machine learning: Framework for intelligent robotics,"In robotics, it is a common problem that for a given task many algorithms are available. For a particular environmental context and some computational constraints some algorithms will perform better and others will perform worse. Consequently, a robot, evolving in a real world environment where both the context and the constraints change in real time, should be able to select in real time algorithms that will provide it with the most accurate world description as well as will allow it to extract the currently most vital information and artifacts. In this paper we propose a machine learning based approach for the real-time selection of computational resources (algorithms) based on both the high level objectives of the robot as well as on the low level environmental requirements (image quality, etc.). The learning mechanism described is using a Genetic Algorithm and the learning method is based on supervised learning; an initial set of algorithms with input data is provided as examples that are used for learning.","Robots,
Machine learning algorithms,
Image segmentation,
Heuristic algorithms,
Algorithm design and analysis,
Real time systems,
Machine learning"
A machine learning approach to falling detection and avoidance for biped robots,"A falling avoidance of biped robots is an important research topic to use the robot in a human life environment. In this paper, we propose a machine learning approach to falling detection and avoidacne for biped robots. Support Vector Machine (SVM) is used as the machine learning algorithm and it detects the falling state of the robot based on acceleration value of torso and center of pressure value of the robot. When the falling is detected, the reaction module produces gait for extending areas of supporting polygon of the robot. The main contribution of the paper is falling detection of the biped robot based on the sensor data and machine learning algorithm without explicit dynamic parameters of the robot and predefined threshold value.","Robot sensing systems,
Legged locomotion,
Support vector machines,
Force,
Robot kinematics,
Torso"
Effect of feature selection on machine learning algorithms for more accurate predictor of surgical outcomes in Benign Pro Static Hyperplasia cases (BPH),"Predicting the clinical outcome prior to minimally invasive treatments for Benign Prostatic Hperlasia (BPH) cases would be very useful. However, clinical prediction has not been reliable in spite of multiple assessment parameters, such as symptom indices and flow rates. In our prior study, Artificial Intelligence (AI) algorithms were used to train computers to predict the surgical outcome in BPH patients treated by TURP or VLAP. Our aim was to investigate whether, based on eleven clinical biomarker features, AI can reproduce the clinical outcome of known cases and assist the urologist in predicting surgical outcomes. In this paper, the objective is to perform data analysis to investigate if specific features have a greater impact on predicting whether the patients had the desired outcome after a surgical procedure is done. Finally, how the number of significant features ought to be weighted to predict the outcome after surgery, is determined to create the most accurate prediction method. Here both the Decision Tree and Naïve Bayse machine learning methods are used and compared.","Surgery,
Bladder,
Accuracy,
Artificial intelligence,
Prediction algorithms,
Decision trees,
Training data"
Controller design for discrete input control system based on machine-learning,"Switching control is an effective control technique for control systems equipped with low-resolution actuators. It is modeled as a control system that restricts its input to discrete values. Designing a discrete-valued controller is equivalent to determining the switching surface. In this paper, a controller design method based on a machine learning technique is discussed. In particular, this paper proposes that the supper vector machine (SVM) is used to construct the switching condition. The relation between the current situation (previous input sequence and previous output sequence), applied input sequence, and output evolution is learned by a support vector machine (SVM) from the database of previous control results. As a result, the learned SVM can determine the most suitable input for the current situation and the reference output. The effectiveness of the proposed method is verified for a discrete input system via experiments.","Training data,
Switches,
Support vector machine classification,
Artificial neural networks,
Vectors"
A machine learning approach for tracing regulatory codes to product specific requirements,"Regulatory standards, designed to protect the safety, security, and privacy of the public, govern numerous areas of software intensive systems. Project personnel must therefore demonstrate that an as-built system meets all relevant regulatory codes. Current methods for demonstrating compliance rely either on after-the-fact audits, which can lead to significant refactoring when regulations are not met, or else require analysts to construct and use traceability matrices to demonstrate compliance. Manual tracing can be prohibitively time-consuming; however automated trace retrieval methods are not very effective due to the vocabulary mismatches that often occur between regulatory codes and product level requirements. This paper introduces and evaluates two machine-learning methods, designed to improve the quality of traces generated between regulatory codes and product level requirements. The first approach uses manually created traceability matrices to train a trace classifier, while the second approach uses web-mining techniques to reconstruct the original trace query. The techniques were evaluated against security regulations from the USA government's Health Insurance Privacy and Portability Act (HIPAA) traced against ten healthcare related requirements specifications. Results demonstrated improvements for the subset of HIPAA regulations that exhibited high fan-out behavior across the requirements datasets.","Training,
Medical services,
Software,
Measurement,
Probabilistic logic,
Encryption"
Relating the Semantics of Dialogue Acts to Linguistic Properties: A Machine Learning Perspective through Lexical Cues,"This paper describes a corpus-based investigation of dialogue acts. In particular, it attempts to answer questions about the empirical distribution of dialogue acts and to what extent dialogue acts can be automatically predicted from their lexical features. The Switchboard Dialogue Act Corpus is adopted and the SWBD-DAMSL tags used for automatic prediction. We show that 60-70% of the dialogue acts can be predicted from lexical features alone depending on different levels of granularity. We also present a mapping from SWBD-DAMSL tags to the tags of the new ISO standard for dialogue act annotation, as part of an ongoing investigation into the relationship between the structure and granularity of the tag set and classification accuracy. The paper concludes with discussions and suggestions for future work.","ISO,
ISO standards,
Pragmatics,
Accuracy,
Switches,
Semantics,
Educational institutions"
Classifying network attack types with machine learning approach,"The growing rate of network attacks including hacker, cracker, and criminal enterprises have been increasing, which impact to the availability, confidentiality, and integrity of critical information data. In this paper, we propose a network-based Intrusion Detection and Classification System (IDCS) using well-known machine learning technique to classify an online network data that is preprocessed to have only 12 features. The number of features affects to the detection speed and resource consumption. Unlike other intrusion detection approaches where a few attack types are classified, our IDCS can classify normal network activities and identify 17 different attack types. Hence, our detection and classification approach can greatly reduce time to diagnose and prevent the network attacks.","Machine learning,
Intrusion detection,
Probes,
Training,
Feature extraction,
Decision trees,
Testing"
A new fuzzy support vector machines for class imbalance learning,"A bilateral-weighted fuzzy support vector machine (B-FSVM) proposed by Wang is to evaluate bank's credit risk. However, it also suffers from the problem of class imbalance datasets in most cases. In this paper, we present a method to impve B-FSVM for class imbalance learning (called NFSVM-CIL) to handle the class imbalance problem in the presence of outliroers and noise. We evaluate and compare its performance with support vector machine, fuzzy support vector machine and FSVM-CIL.","Support vector machines,
Training,
Noise,
Machine learning,
Accuracy,
Educational institutions,
Learning systems"
A machine learning approach for optimising image segmentation algorithms,"Image segmentation is one of the most fundamental steps of image analysis. Almost all vision based systems need segmented images using a good segmentation algorithm. The problem of automatically finding the best algorithm, from a pool of algorithms, on a per image basis has been largely ignored in the vision community. In this paper we present a novel solution to this problem based on classification complexity and image edge analysis.","Image segmentation,
Machine learning algorithms,
Image color analysis,
Algorithm design and analysis,
Clustering algorithms,
Classification algorithms,
Prediction algorithms"
Ionic Channel Current Burst Analysis by a Machine Learning Based Approach,"A new method to analyze single ionic channel current conduction is presented. It is based on an automatic classification by K-means algorithm and on the concept of information entropy. This method is used to study the conductance of multistate ion current jumps induced by tetanus toxin in planar lipid bilayers. A comparison is presented with the widely used Gaussian best fit approach, whose main drawback is the fact that it is based on the manual choice of the base line and of meaningful fragments of current signal. On the contrary, the proposed method is able to automatically process a great amount of information and to remove spurious transitions and multichannels. The number of levels and their amplitudes do not have to be known a priori. In this way the presented method is able to produce a reliable evaluation of the conductance levels and their characteristic parameters in a short time.","Noise,
Machine learning,
Histograms,
Lipidomics,
Entropy,
Biomembranes,
Mutual information"
A Bayesian approach to localized multi-kernel learning using the relevance vector machine,"Multi-kernel learning has become a popular method to allow classification models greater flexibility in representing the relationships between data points. This approach has evolved into localized multi-kernel learning, which creates classification models that have the ability to adapt to a multi-scale feature-space. The advantages of such an approach are often hampered by additional parameters and hyper-parameters involved in creating this model, not to mention the greater likelihood of over-training. Additionally, existing methods to create a localized multi-kernel classifier rely on partitioning the feature-space, followed by applying a multi-kernel to the partitioned data points. We introduce a Bayesian approach to the localized multi-kernel machine. The new model is shown to provide greater classification abilities by learning the local scales of the feature-space without the need to partition the data. Also, the Bayesian formulation helps the model to be resistant to over-training. We demonstrate the models effectiveness on two landmine detection datasets, each from a different sensor type.","Kernel,
Mathematical model,
Machine learning,
Adaptation models,
Bayesian methods,
Equations,
Pattern recognition"
High Performance Lithography Hotspot Detection With Successively Refined Pattern Identifications and Machine Learning,"Under the real and evolving manufacturing conditions, lithography hotspot detection faces many challenges. First, real hotspots become hard to identify at early design stages and hard to fix at post-layout stages. Second, false alarms must be kept low to avoid excessive and expensive post-processing hotspot removal. Third, full chip physical verification and optimization require very fast turn-around time. Last but not least, rapid technology advancement favors generic hotspot detection methodologies to avoid exhaustive pattern enumeration and excessive development/update as technology evolves. To address the above issues, we propose a high performance hotspot detection methodology consisting of: 1) a fast layout analyzer; 2) powerful hotspot pattern identifiers; and 3) a generic and efficient flow with successive performance refinements. We implement our algorithms with industry-strength engine under real manufacturing conditions and show that it significantly outperforms state-of-the-art algorithms in false alarms (2.4X to 2300X reduction) and runtime (5X to 237X reduction), meanwhile achieving similar or better hotspot accuracies. Compared with pattern matching, our method achieves higher prediction accuracy for hotspots that are not previously characterized, therefore, more detection generality when exhaustive pattern enumeration is too expensive to perform a priori. Such high performance hotspot detection is especially suitable for lithography-friendly physical design.","Machine learning,
Learning systems,
Lithography,
Design for manufacture,
Feature extraction,
Pattern recognition"
Application research of support vector machine in E-Learning for personality,"In order to accurately build the learner's learning style in E-Learning, according to the needs and preferences to provide personalized learning materials and harmonious human-computer interaction environment. This paper combines Felder-Silverman learning style with support vector machine technology, and use machine learning technologies for learners to build dynamic learning style. Through the analysis of the Emotion and recognition interaction of the personalized E-Learning based on statistical learning theory and support vector machine technology, it demonstrates the correctness and feasibility using support vector machine to build learning styles. The combination of support vector machine, emotion and recognition interaction in the personalized E-Learning makes great contribution to build human-computer interaction environment.","Machine learning,
Support vector machines,
Electronic learning,
Training,
Educational institutions,
Materials,
Brain modeling"
Production test of an RF receiver chain based on ATM combining RF BIST and machine learning algorithm,"Testing an RF device in Production is expensive and technically difficult. At Wafer Test level, the RF probing technologies hardly fulfil the industrial test requirements in terms of accuracy, reliability and cost. At Package test level testing the RF parameters requires expensive RF equipments (RF automated test equipments (ATE)) and for complex RF transceivers, which address multi-modes (RF multi-paths and/or requiring different impedance matchings), it usually leads to prohibitive test time. In order to reduce the test costs for RF devices, different methods are proposed and evaluated in NXP and at competitions. These methods mainly target test time reduction (e.g. by testing parts in parallel) or propose ways of limiting the needs of expensive RF tester in Production (e.g. by using Alternate Test Methods, Design For Test, or Built In Test). In the proposed presentation we will focus on ATM and RF BI(s)T, providing some results on DC-RF correlation and an example of a real case BIT implementation into a fully integrated single-chip receiver operating in the sub-GHz ISM bands 315 MHz to 920 MHz.","Radio frequency,
Phase locked loops,
Semiconductor device measurement,
Production,
Correlation,
Receivers,
Integrated circuits"
Predicting Software Anomalies Using Machine Learning Techniques,"In this paper, we present a detailed evaluation of a set of well-known Machine Learning classifiers in front of dynamic and non-deterministic software anomalies. The system state prediction is based on monitoring system metrics. This allows software proactive rejuvenation to be triggered automatically. Random Forest approach achieves validation errors less than 1% in comparison to the well-known ML algorithms under a valuation. In order to reduce automatically the number of monitored parameters, needed to predict software anomalies, we analyze Lasso Regularization technique jointly with the Machine Learning classifiers to evaluate how the prediction accuracy could be guaranteed within an acceptable threshold. This allows to reduce drastically (around 60% in the best case) the number of monitoring parameters. The framework, based on ML and Lasso regularization techniques, has been validated using an ecommerce environment with Apache Tomcat server, and MySql database server.","Prediction algorithms,
Instruction sets,
Machine learning algorithms,
Predictive models,
Monitoring,
Computer crashes"
Machine Learning Approach to the Power Management of Server Clusters,"Network services are often provided by a server cluster. From the viewpoint of operational expenditure as well as the global environment, power consumption of a server cluster must be decreased. Power consumption is decreased by operating as few computers as possible to offer sufficiently good performance against changes in load. For this operation, it is necessary to decide exactly how many computers should be turned on or off for the measured load metrics. The number of computers should be determined by estimating multiple load metrics, because a single metric does not adequately represent the statuses of various bottlenecked resources. Moreover, the mapping from these metrics to the performance experienced by users is not simple. Thus, the number of computers must be calculated considering the complicated relationship among the metrics. In addition, decision rules should be appropriately updated if the service content or computer specification changes. To satisfy these requirements, this study proposes a scheme based on a machine learning technique as a method of deciding the number of server computers. This paper first presents how a machine learning technique is applied for deciding the number of computers. Then, its implementation is presented, and the effectiveness of the scheme is confirmed through experiments.","Servers,
Computers,
Machine learning,
Central Processing Unit,
Bit rate,
Power measurement"
Embracing Uncertainty: The New Machine Learning,"Summary form only given. Computers are based on logic, but must increasingly deal with real-world data that is full of uncertainty and ambiguity. Modern approaches to machine learning use probability theory to quantify and compute with this uncertainty, and have led to a proliferation in the applications of machine learning, ranging from recommendation systems to web search, and from spam filters to voice recognition. Most recently, the Kinect 3D full-body motion sensor, which has become the fastest-selling consumer electronics product in history, relies crucially on machine learning. Furthermore, the advent of widespread internet connectivity, with centralised data storage and processing, coupled with recently developed algorithms for computationally efficient probabilistic inference, will create many new opportunities for machine learning over the coming years. The talk will be illustrated with tutorial examples, demonstrations, and real-world case studies.","Machine learning,
Uncertainty,
Inference algorithms,
History,
Information filters,
Filtering theory,
Computational efficiency"
Human-machine design considerations in advanced machine-learning systems,"This paper explores issues related to human–computer interaction with the new class of machine-learning systems that represent an exciting development on the frontiers of information technology. These systems represent a significant breakthrough in humanity's decades-long endeavor to build computers that are “more like us”—fundamentally designed to map to and support human abilities and where the needs of people are central to the design process. This paper examines the human aspects of developing such new technology, with a focus on question-and-answer machine-learning systems. Considerations such as how human behavior should be addressed in the design and development of such systems are presented, followed by a series of potential application domains for the new technology.","Knowledge based systems,
Human computer interaction,
User interfaces,
Web search,
Global Positioning System,
Behavioral science,
Machine learning"
On-line Cache Strategy Reconfiguration for Elastic Caching Platform: A Machine Learning Approach,"Cloud computing provide scalability and high availability for web applications using such techniques as distributed caching and clustering. As one database offloading strategy, elastic caching platforms (ECPs) are introduced to speed up the performance or handle application state management with fault tolerance. Several cache strategies for ECPs have been proposed, say replicated strategy, partitioned strategy and near strategy. We first evaluate the impact of the three cache strategies using the TPC-W benchmark and find that there is no single cache strategy suitable for all conditions, the selection of the best strategy is related with workload patterns, cluster size and the number of concurrent users. This raises the question of when and how the cache strategy should be reconfigured as the condition varies which has received comparatively less attention. In this paper, we present a machine learning based approach to solving this problem. The key features of the approach are off-line training coupled with on-line system monitoring and robust synchronization process after triggering a reconfiguration, at the same time the performance model is periodically updated. More explicitly, first a rule set used to identify which cache strategy is optimal under the current condition are trained with the system statistics and performance results. We then introduce a framework to switch the cache strategy on-line as the workload varies and keep its overhead to acceptable levels. Finally, we illustrate the advantages of this approach by carrying out a set of experiments.","Servers,
Software,
Machine learning,
Monitoring,
Availability,
Scalability,
Benchmark testing"
Quest for efficient option pricing prediction model using machine learning techniques,"Prediction of option prices has always been a challenging task. Various models have been used in the past but there has been no effort to point out which model is suited best for predicting option prices. Computational time plays an important role in prediction of option prices since these time series are usually large. It is computationally expensive to employ a traditional statistical model which comprises of two phases namely model identification and prediction. A good fitting model may not always be good for prediction due to high fluctuation in the market. Various non parametric models like Multilayer perceptron (MLP), Radial Basis function (RBF) Neural Network and Support Vector regression (SVR) have been employed in the past. MLP and RBF networks take enormous amount of time since the network is learned after a number of iterations. In this paper, prediction of American stock option prices (both call and put options) for companies belonging to various sectors and also prediction of European option prices of Nifty index futures has been attempted using GRNN which has not been attempted so far in the literature. Comparative performance evaluation of GRNN has been done with Support Vector Regression (SVR), MLP and Black Scholes Model. It has been shown that the performance of GRNN is superior to the well known Black Sholes model and other non parametric models like MLP and RBF both in terms of accuracy and time and it performs at par with SVR.","Pricing,
Neural networks,
Support vector machines,
Computational modeling,
Predictive models,
Machine learning,
Kernel"
The contribution of machine learning to analyze and evaluate the safety of automated transport systems,"This paper presents our contribution to the improvement of the usual methods of analysis and safety assessment used as part of the certification of automatic guided land transport systems. This contribution, based on the use of artificial intelligence techniques, including machine learning, has been realized by the development of several approaches and tools for modeling, capitalization and evaluation Knowledge of safety. The software tool presented in this article called REXCAS has two main purposes: first to check in and perpetuate the experience in security analysis and second to help those involved in the development and certification of transport systems in their arduous task of assessing safety studies.","Security,
Safety,
Cognition,
Accidents,
Learning systems,
Machine learning"
A cheating detection framework for Unreal Tournament III: A machine learning approach,"Cheating reportedly affects most of the multi-player online games and might easily jeopardize the game experience by providing an unfair competitive advantage to one player over the others. Accordingly, several efforts have been made in the past years to find reliable and scalable approaches to solve this problem. Unfortunately, cheating behaviors are rather difficult to detect and existing approaches generally require human supervision. In this work we introduce a novel framework to automatically detect cheating behaviors in Unreal Tournament III by exploiting supervised learning techniques. Our framework consists of three main components: (i) an extended game-server responsible for collecting the game data; (ii) a processing backend in charge of preprocessing data and detecting the cheating behaviors; (iii) an analysis frontend. We validated our framework with an experimental analysis which involved three human players, three game maps and five different supervised learning techniques, i.e., decision trees, Naive Bayes, random forest, neural networks, support vector machines. The results show that all the supervised learning techniques are able to classify correctly almost 90% of the test examples.","Games,
Supervised learning,
Radar,
Humans,
Engines,
Weapons,
Support vector machines"
Disaster management in real time simulation using machine learning,"A series of carefully chosen decisions by an Emergency Responder during a disaster are vital in mitigating the loss of human lives and the recovery of critical infrastructures. In this paper we propose to assist a human Emergency Responder by modeling and simulating an intelligent agent using Reinforcement Learning. The goal of the agent will be to maximize the number of patients discharged from hospitals or on-site emergency units. It is suggested that by exposing such an intelligent agent to a large sequence of simulated disaster scenarios, the agent will capture enough experience and knowledge to enable it to select those actions which mitigate damage and casualties. This paper describes early results of our work that indicate that the use of Q-learning can successfully train an agent to make good choices, during a simulated disaster.","Humans,
Table lookup,
Mathematical model,
Production,
Learning,
Intelligent agents,
Learning systems"
On the analysis of a new Markov chain which has applications in AI and machine learning,"In this paper, we consider the analysis of a fascinating Random Walk (RW) that contains interleaving random steps and random ""jumps"". The characterizing aspect of such a chain is that every step is paired with its counterpart random jump. RWs of this sort have applications in testing of entities, where the entity is never allowed to make more than a pre-specified number of consecutive failures. This paper contains the analysis of the chain, some fascinating limiting properties, and some initial simulation results. The reader will find more detailed results in [12].","Testing,
Biological system modeling,
Computational modeling,
Educational institutions,
Markov processes,
Limiting,
Steady-state"
Keynotes: Data mining and machine learning applications in MMOs,"Summary form only given. Database abstractions, such as TinyDB [1] and TikiriDB [2], have been used successfully in wireless sensor networks (WSN). These abstractions provide an interface for SQL like queries to extract data from sensors. This has proven to be a convenient programming interface in these environments. However, a relational database model that guarantees ACID (Atomicity, Consistency, Isolation, and Durability) properties is not a good option for a wireless sensor network, since consistent connectivity or the uninterrupted operation of the sensor nodes cannot be expected. We noted that the NoSQL approach which does not rely on the ACID properties is a better match for a database abstraction for WSNs. We also noted that this idea can be extended to wireless ad hoc networks in general and implemented a NoSQL based database abstraction for smartphones which uses Android operating system.",
Machine learning for very early Alzheimer's Disease diagnosis; a 18F-FDG and PiB PET comparison,"This paper shows a machine learning approach based on Principal Component Analysis (PCA) and Support Vector Machine (SVM) to compare the diagnostic accuracy on very early Alzheimer's Disease (AD) patients with 18F FDG and Pittsburg Compound B (PiB) PET imaging. The Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset is used for testing, making use of the longitudinal character. Mild Cognitive Impairment (MCI) individuals that after a two years follow up converted into possible AD where used as very early AD patients. While 18F FDG and PiB have similar diagnostic accuracy in AD, PiB is shown to have higher discriminative power in very early AD with respect to FDG.","Positron emission tomography,
Accuracy,
Support vector machines,
Kernel,
Alzheimer's disease,
Sensitivity,
Principal component analysis"
A machine-learning approach to time discrimination,Machine-Learning methods can be used to provide fast and accurate time discrimination of pulses from detectors used in medical scanner applications. This approach uses multiple samples before and after a pulse crosses a threshold to estimate the starting time of an input pulse.,"Finite impulse response filter,
Timing,
Training,
Low pass filters,
Computational fluid dynamics,
Noise,
Information filters"
Statistical atlases and machine learning tools applied to optimized prostate biopsy for cancer detection and estimation of volume and Gleason score,"We discuss the use of statistical atlases and machine learning tools for determining optimized biopsy procedures. Prostate cancer diagnosis most often involves the sampling of prostate tissue via placement of a number of biopsy needles in locations that are somewhat random but try to cover the gland. The purpose of this work is to establish optimal strategies for sampling the prostate tissue, using population statistics. In particular, a statistical atlas reflecting the spatial distribution of prostate cancer has been constructed via elastic registration of expert-labeled histological 3D volumes of radical prostatectomy patients. This atlas reflects the probability of encountering prostate carcinoma at a given location in the gland.","Biopsy,
Needles,
Cancer,
Cancer detection,
Biological system modeling,
Estimation,
Machine learning"
Fast cell detection in high-throughput imagery using GPU-accelerated machine learning,"High-throughput microscopy allows fast imaging of large tissue samples, producing an unprecedented amount of sub-cellular information. The size and complexity of these data sets often out-scale current reconstruction algorithms. Overcoming this computational bottleneck requires extensive parallel processing and scalable algorithms. As high-throughput imaging techniques move into main stream research, processing must also be inexpensive and easily available. In this paper, we describe a method for cell soma detection in Knife-Edge Scanning Microscopy (KESM) using machine learning. The proposed method requires very little training data and can be mapped to consumer graphics hardware, allowing us to perform real-time cell detection at a rate that exceeds the data rate of KESM.","Graphics processing unit,
Artificial neural networks,
Computer architecture,
Training,
Feature extraction,
Microscopy"
Machine learning algorithms for task identification,Context awareness plays an essential role in implementing the Ambient Intelligence vision (AmI). Instrumentalized data gloves are used for task tracking based on the information provided by sensors embedded into them. This article presents some preliminary results of the use of Genetic Algorthms for sensor selection in person independent task recognition system.,"Fingers,
Sensors,
Genetic algorithms,
Data gloves,
Ambient intelligence,
Machine learning,
Object recognition"
Nonlinear internal model control with inverse model based on extreme learning machine,"Extreme learning machine is a novel single hidden layer feedforward neural networks with a strong abilities, for example simple net structure, fast learning speed, good generalization and so on. Aimed at the system with a delay unit, A new control strategy for internal model control is proposed to set up an inverse model of the minimal phase subsystem by using extreme learning machine with in-out system datum. Moreover, the relative stable error for internal model control system with a delay unit is presented to evaluate the system performance. The features for the internal model control system based on extreme learning machine are compared with that based on neural network. The experimental results indicate that the internal model control system based on extreme learning machine has small stable error and strong robustness.","Steady-state,
Delay,
Buildings,
Control systems,
Machine learning,
Mathematical model,
Robustness"
Classification of surrounding rocks in tunnel based on Gaussian process machine learning,"Classification of surrounding rocks in tunnel is very important for design and construction. Aiming to the fact that it is still difficult to reasonably determine the classification of surrounding rocks in tunnel, the model based on Gaussian process machine learning is proposed for classifying surrounding rocks. With the help of simple learning process, the uncertain mapping relationship between classification of surrounding rocks and its influencing factors is established by Gaussian process for binary classification model. The model is applied to a real engineering. The results of case study show that Gaussian process for binary classification model is feasible and has the same results with artificial neural networks and support vector machine. Nevertheless, compared with artificial neural networks and support vector machine, it has attractive merit of self-adaptive parameters determination.","Rocks,
Gaussian processes,
Artificial neural networks,
Support vector machines,
Machine learning,
Stability analysis,
Water resources"
A Study on Machine Learning Algorithms for Fall Detection and Movement Classification,"Falls among the elderly is an important health issue. Fall detection and movement tracking are therefore instrumental in addressing this issue. This paper responds to the challenge of classifying different movements as a part of a system designed to fulfill the need for a wearable device to collect data for fall and near-fall analysis. Four different fall trajectories (forward, backward, left and right), three normal activities (standing, walking and lying down) and near-fall situations are identified and detected. Different machine learning algorithms are compared and the best one is used for real time classification. The comparison is made using Waikato Environment for Knowledge Analysis (WEKA), one of the most popular machine learning software. The system also has the ability to adapt to the different gait characteristics of each individual. A feature selection algorithm is also introduced to reduce the number of features required for the classification problem.","Accuracy,
Gyroscopes,
Machine learning,
Acceleration,
Accelerometers,
Prediction algorithms,
Correlation"
A Method for Generating Emergent Behaviors Using Machine Learning to Strategy Games,"This work proposes the use of machine learning for the creation of a basic library of experiences, which will be used for the generation of emergent behaviors for characters in a strategy game. In order to create a high diversification of the agents' story elements, the characteristics of the agents are manipulated based on their adaptation to the environment and interaction with enemies. We start by defining important requirements that should be observed when modeling the instances. Then, we propose a new architecture paradigm and suggest what would be the most appropriate classification algorithm for this architecture. Results are obtained with an implementation of a prototype strategy game, called Darwin Kombat, which validated the definition of the best classifier.","Games,
Machine learning,
Decision trees,
Classification algorithms,
Algorithm design and analysis,
Intelligent agents,
Sockets"
Metrics for characterizing machine learning-based hotspot detection methods,"Machine learning techniques have recently been applied to the problem of lithographic hotspot detection. It is widely believed that they are capable of identifying hotspot patterns unknown to the trained model. The quality of a machine learning method is conventionally measured by the accuracy rates determined from experiments employing random partitioning of benchmark samples into training and testing sets. In this paper, we demonstrate that these accuracy rates may not reflect the predictive capability of a method. We introduce two metrics - the predictive and memorizing accuracy rates - that quantitatively characterize the method's capability to capture hotspots. We also claim that the number of false alarms per detected hotspot reflects both the method's performance and the difficulty of detecting hotspots in the test set. By adopting the proposed metrics, a designer can conduct a fair comparison between different hotspot detection tools and adopt the one better suited to the verification needs.","Training,
Accuracy,
Encoding,
Testing,
Measurement,
Machine learning,
Layout"
Predicting dominance judgements automatically: A machine learning approach,"The amount of multimodal devices that surround us is growing everyday. In this context, human interaction and communication have become a focus of attention and a hot topic of research. A crucial element in human relations is the evaluation of individuals with respect to facial traits, what is called a first impression. Studies based on appearance have suggested that personality can be expressed by appearance and the observer may use such information to form judgments. In the context of rapid facial evaluation, certain personality traits seem to have a more pronounced effect on the relations and perceptions inside groups. The perception of dominance has been shown to be an active part of social roles at different stages of life, and even play a part in mate selection. The aim of this paper is to study to what extent this information is learnable from the point of view of computer science. Specifically we intend to determine if judgments of dominance can be learned by machine learning techniques. We implement two different descriptors in order to assess this. The first is the histogram of oriented gradients (HOG), and the second is a probabilistic appearance descriptor based on the frequencies of grouped binary tests. State of the art classification rules validate the performance of both descriptors, with respect to the prediction task. Experimental results show that machine learning techniques can predict judgments of dominance rather accurately (accuracies up to 90%) and that the HOG descriptor may characterize appropriately the information necessary for such task.","Face,
Support vector machines,
Histograms,
Context,
Machine learning,
Accuracy,
Humans"
SystemML: Declarative machine learning on MapReduce,"MapReduce is emerging as a generic parallel programming paradigm for large clusters of machines. This trend combined with the growing need to run machine learning (ML) algorithms on massive datasets has led to an increased interest in implementing ML algorithms on MapReduce. However, the cost of implementing a large class of ML algorithms as low-level MapReduce jobs on varying data and machine cluster sizes can be prohibitive. In this paper, we propose SystemML in which ML algorithms are expressed in a higher-level language and are compiled and executed in a MapReduce environment. This higher-level language exposes several constructs including linear algebra primitives that constitute key building blocks for a broad class of supervised and unsupervised ML algorithms. The algorithms expressed in SystemML are compiled and optimized into a set of MapReduce jobs that can run on a cluster of machines. We describe and empirically evaluate a number of optimization strategies for efficiently executing these algorithms on Hadoop, an open-source MapReduce implementation. We report an extensive performance evaluation on three ML algorithms on varying data and cluster sizes.","Clustering algorithms,
Machine learning algorithms,
Optimization,
Semantics,
Machine learning,
Runtime,
Computer architecture"
Machine learning based blog classification personal vs. official facet,"Since the blog service brings a wealth of information resources, blog search and classification are showing their great research value. This paper focuses on the blog classification on the personal vs. official facet. Our system adopts a two-stage strategy; in training model, lexicons are built automatically; in classification model, scoring and ranking are carried out orderly. Our experimental results reveal that feature selection, Mutual Information weighting are good for lexicons with significant results. However, sentiment words can only slightly improve the results.","Blogs,
Training,
Measurement,
Machine learning,
Testing,
Buildings,
Mutual information"
Using machines to learn method-specific compilation strategies,"Support Vector Machines (SVMs) are used to discover method-specific compilation strategies in Testarossa, a commercial Just-in-Time (JiT) compiler employed in the IBM® J9 Java™ Virtual Machine. The learning process explores a large number of different compilation strategies to generate the data needed for training models. The trained machine-learned model is integrated with the compiler to predict a compilation plan that balances code quality and compilation effort on a per-method basis. The machine-learned plans outperform the original Testarossa for start-up performance, but not for throughput performance, for which Testarossa has been highly hand-tuned for many years.","Optimization,
Radiation detectors,
Training,
Support vector machines,
Java,
Arrays,
Data models"
Tuberculosis bacilli detection in Ziehl-Neelsen-stained tissue using affine moment invariants and Extreme Learning Machine,"This paper describes an approach to automate the detection and classification of tuberculosis (TB) bacilli in tissue section using image processing technique and feedforward neural network trained by Extreme Learning Machine. It aims to assist pathologists in TB diagnosis and give an alternative to the conventional manual screening process, which is time-consuming and labour-intensive. Images are captured from Ziehl-Neelsen (ZN) stained tissue slides using light microscope as it is commonly used approach for diagnosis of TB. Then colour image segmentation is used to locate the regions correspond to the bacilli. After that, affine moment invariants are extracted to represent the segmented regions. These features are invariant under rotation, scale and translation, thus useful to represent the bacilli. Finally, a single layer feedforward neural network (SLFNN) trained by Extreme Learning Machine (ELM) is used to detect and classify the features into three classes: `TB', `overlapped TB' and `non-TB'. The results indicate that the ELM gives acceptable classification performance with shorter training period compared to the standard backpropagation training algorithms.","Training,
Signal processing algorithms,
Image segmentation,
Classification algorithms,
Microscopy,
Image color analysis,
Machine learning"
Cycle-Time Key Factor Identification and Prediction in Semiconductor Manufacturing Using Machine Learning and Data Mining,"Within the complex and competitive semiconductor manufacturing industry, lot cycle time (CT) remains one of the key performance indicators. Its reduction is of strategic importance as it contributes to cost decreasing, time-to-market shortening, faster fault detection, achieving throughput targets, and improving production-resource scheduling. To reduce CT, we suggest and investigate a data-driven approach that identifies key factors and predicts their impact on CT. In our novel approach, we first identify the most influential factors using conditional mutual information maximization, and then apply the selective naive Bayesian classifier (SNBC) for further selection of a minimal, most discriminative key-factor set for CT prediction. Applied to a data set representing a simulated fab, our SNBC-based approach improves the accuracy of CT prediction in nearly 40% while narrowing the list of factors from 182 to 20. It shows comparable accuracy to those of other machine learning and statistical models, such as a decision tree, a neural network, and multinomial logistic regression. Compared to them, our approach also demonstrates simplicity and interpretability, as well as speedy and efficient model training. This approach could be implemented relatively easily in the fab promoting new insights to the process of wafer fabrication.","Accuracy,
Entropy,
Manufacturing,
Data models,
Uncertainty,
Classification algorithms,
Semiconductor device modeling"
Predicting user comfort level using machine learning for Smart Grid environments,"Smart Grid with Time-of-Use (TOU) pricing brings new ways of cutting costs for energy consumers and conserving energy. It is done by utilities suggesting the user ways to use devices to lower their energy bills keeping in mind its own benefits in smoothening the peak demand curve. However, as suggested in previous related research, user's comfort need must be addressed in order to make the system work efficiently. In this work, we validate the hypothesis that user preferences and habits can be learned and user comfort level for new patterns of device usage can be predicted. We investigate how machine learning algorithms specifically supervised machine learning algorithms can be used to achieve this. We also compare the prediction accuracies of three commonly used supervised learning algorithms, as well as the effect that the number of training samples has on the prediction accuracy. Further more, we analyse how sensitive prediction accuracies yielded by each algorithm are to the number of training samples.","Training,
Accuracy,
Support vector machines,
Prediction algorithms,
Niobium,
Smart grids,
Machine learning algorithms"
A Machine Learning Approach for Identifying Disease-Treatment Relations in Short Texts,"The Machine Learning (ML) field has gained its momentum in almost any domain of research and just recently has become a reliable tool in the medical domain. The empirical domain of automatic learning is used in tasks such as medical decision support, medical imaging, protein-protein interaction, extraction of medical knowledge, and for overall patient management care. ML is envisioned as a tool by which computer-based systems can be integrated in the healthcare field in order to get a better, more efficient medical care. This paper describes a ML-based methodology for building an application that is capable of identifying and disseminating healthcare information. It extracts sentences from published medical papers that mention diseases and treatments, and identifies semantic relations that exist between diseases and treatments. Our evaluation results for these tasks show that the proposed methodology obtains reliable outcomes that could be integrated in an application to be used in the medical care domain. The potential value of this paper stands in the ML settings that we propose and in the fact that we outperform previous results on the same data set.","Diseases,
Medical diagnostic imaging,
Semantics,
Classification algorithms,
Machine learning"
Support Vector Machine Active Learning Through Significance Space Construction,"Active learning is showing to be a useful approach to improve the efficiency of the classification process for remote sensing images. This letter introduces a new active learning strategy specifically developed for support vector machine (SVM) classification. It relies on the idea of the following: 1) reformulating the original classification problem into a new problem where it is needed to discriminate between significant and nonsignificant samples, according to a concept of significance which is proper to the SVM theory; and 2) constructing the corresponding significance space to suitably guide the selection of the samples potentially useful to better deal with the original classification problem. Experiments were conducted on both multi- and hyperspectral images. Results show interesting advantages of the proposed method in terms of convergence speed, stability, and sparseness.","Training,
Support vector machines,
Accuracy,
Learning systems,
Machine learning,
Hyperspectral imaging"
Automated Text Binary Classification Using Machine Learning Approach,"The increased number of documents in digital format available on the Web and its useful information for different purposes entail an essential need to organize them. However, this task must be automated in order to save costs and manpower. In the community research, the main approach to face this problem is based on the application of machine learning techniques. This article studies the main machine learning approaches to reach an automated text classification.","Training,
Support vector machines,
Niobium,
Machine learning,
Artificial neural networks,
Electronic mail,
Testing"
A machine learning approach to document retrieval: an overview and an experiment,We provide an overview of artificial intelligence techniques and then present a machine learning based document retrieval system we developed. GANNET (Genetic Algorithms and Neural Nets System) performed concept (keyword) optimization for user-selected documents during document retrieval using genetic algorithms. It then used the optimized concepts to perform concept exploration in a large network of related concepts through the Hopfield net parallel relaxation procedure. Our preliminary experiment showed that GANNET helped improve search recall by identifying the underlying concepts (keywords) which best describe the user-selected documents.,"Learning systems,
Database systems, query processing,
Hopfield neural networks,
Database systems, searching"
Inductive query by examples (IQBE): a machine learning approach,"This paper presents an incremental, inductive learning approach to query-by-examples for information retrieval (IR) and database management systems (DBMS). After briefly reviewing conventional information retrieval techniques and the prevailing database query paradigms, we introduce the ID5R algorithm, previously developed by Utgoff (1989), for ""intelligent"" and system-supported query processing.","Learning systems,
Database systems, query processing,
Information retrieval"
Machine Learning Approaches for the Neuroimaging Study of Alzheimer's Disease,Machine learning tools aid many Alzheimer's disease-related investigations by enabling multisource data fusion and biomarker identification as well as analysis of functional brain connectivity.,"Magnetic resonance imaging,
Positron emission tomography,
Machine learning,
Neuroimaging,
Genetics,
Alzheimer's disease"
Near optimal machine learning based random test generation,Optimized test generation techniques are required to overcome the ever increasing test cost of digital systems. In this work a near optimal machine learning based approach is proposed to improve the random test generation techniques. The improvements of the proposed method over previous works are exercised in an HDL environment and results for ISCAS benchmarks are reported.,"Circuit faults,
Machine learning,
Monte Carlo methods,
Machine learning algorithms,
Hardware design languages,
Databases,
Genetic algorithms"
An introduction to machine learning for students in secondary education,"We have developed a platform for exposing high school students to machine learning techniques for signal processing problems, making use of relatively simple mathematics and engineering concepts. Along with this platform we have created two example scenarios which give motivation to the students for learning the theory underlying their solutions. The first scenario features a recycling sorting problem in which the students must setup a system so that the computer may learn the different types of objects to recycle so that it may automatically place them in the proper receptacle. The second scenario was motivated by a high school biology curriculum. The students are to develop a system that learns the different types of bacteria present in a pond sample. The system will then group the bacteria together based on similarity. One of the key strengths of this platform is that virtually any type of scenario may be built upon the concepts conveyed in this paper. This then permits student participation from a wide variety of educational motivation.","Computers,
Microorganisms,
Containers,
Algorithm design and analysis,
Measurement,
Recycling,
Feature extraction"
Machine learning based supper-resolution algorithm robust to registration errors,"In this work, a novel two phase approach is proposed for robust super-resolution in the presence of registration errors and outliers. In the first phase machine learning method is used to create a weight matrix for every LR image indicating the presence of registration errors. In the second phase, super-resolution is performed using all of the LR images and the associated weight matrices, creating an image which is free of error artifacts.","Decision support systems,
Digital signal processing"
"Multiview, Broadband Acoustic Classification of Marine Fish: A Machine Learning Framework and Comparative Analysis","Multiview, broadband, acoustic classification of individual fish was investigated using a recently developed laboratory scattering system. Scattering data from nine different species of saltwater fish were collected. Using custom software, these data were processed and filtered to yield a data set of 36 individuals, and between 200 and 500 echoes per individual. These data were sampled uniformly randomly in fish orientation. Feature-, decision-, and collaborative-fusion algorithms were then developed and tested using support vector machines (SVMs) as the underlying classifiers. Decision fusion was implemented by cascading two levels of support vectors machines. Collaborative fusion was implemented by using SVM outputs to estimate confidence levels and performing weighted averaging of probabilities computed from each view with feedback from other views. Collaborative fusion performed as well or better than the others, and did so without requiring assumptions about view geometry. In addition to a comparison between classification algorithms and feature transformations, two data collection geometries were explored, including random observation geometries. In all cases, combining multiple, broadband views yielded significant reductions in classification error (>;50%) over single-view methods, for uniformly random fish orientation.","Support vector machines,
Acoustics,
Broadband communication,
Shape,
Classification algorithms,
Scattering,
Collaboration"
Identifying Relevant Data for a Biological Database: Handcrafted Rules versus Machine Learning,"With well over 1,000 specialized biological databases in use today, the task of automatically identifying novel, relevant data for such databases is increasingly important. In this paper, we describe practical machine learning approaches for identifying MEDLINE documents and Swiss-Prot/TrEMBL protein records, for incorporation into a specialized biological database of transport proteins named TCDB. We show that both learning approaches outperform rules created by hand by a human expert. As one of the first case studies involving two different approaches to updating a deployed database, both the methods compared and the results will be of interest to curators of many specialized databases.","Databases,
Machine learning,
Proteins,
Humans,
Bioinformatics,
Data mining,
Computer science,
Information retrieval,
Genomics,
Association rules"
No-Reference Metric Design With Machine Learning for Local Video Compression Artifact Level,"In decoded digital video, the local perceptual compression artifact level depends on the global compression ratio and the local video content. In this paper, we show how to build a highly relevant metric for video compression artifacts using supervised learning. To obtain the ground truth for training, we first build a reference metric for local estimation of the artifact level, which is robust to scaling and sensitive to all types of compression artifacts. Next, we design a large feature set and use AdaBoost to create no-reference metrics trained with the output of the reference metric. Two separate trained no-reference metrics, one for flat and one for detailed areas, respectively, are necessary to cover all types of artifacts. The relevance of these metrics is validated in a compression artifact reduction application, using objective scores like PSNR and BIM, but also a subjective evaluation as proof. We conclude that our created reference metric is an accurate local estimator of the compression artifact level. We were able to copy the performance to two no-reference metrics, based on a weighted mixture of low-level features. Our new metrics enable a far superior performance of artifact reduction compared to relevant alternative proposals.","Machine learning,
Video compression,
Image coding,
Image storage,
Decoding,
Transform coding,
Discrete cosine transforms,
Image quality,
Optical losses,
Pixel"
Automate session setup based on machine learning,"This paper proposes a system of using machine learning algorithms to extract communication session information, such as conference bridge number and participant code, from users' emails or appointments. Our system can then use the retrieved information to easily setup a communication session, for example, dialing conference bridge number and participant code, as well as popping up web conference links with simply one click. Our system can also verify the retrieved information by monitoring users' communication sessions. This paper presents the overall architecture and uses the one-click-conferencing feature as an example to illustrate our system.","Training,
Decision trees,
Electronic mail,
Classification algorithms,
Machine learning,
Servers,
Bridges"
Estimating design quality of digital systems via machine learning,"Although the term design quality of digital systems can be assessed from many aspects, the distribution and density of bugs are two decisive factors. This paper presents the application of machine learning techniques to model the relationship between specified metrics of high-level design and its associated bug information. By employing the project repository (i.e., high level design and bug repository), the resultant models can be used to estimate the quality of associated designs, which is very beneficial for design, verification and even maintenance processes of digital systems. A real industrial microprocessor is employed to validate our approach. We hope that our work can shed some light on the application of software techniques to help improve the reliability of various digital designs.","Measurement,
Artificial neural networks,
Training"
Predicting Breast Screening Attendance Using Machine Learning Techniques,Machine learning-based prediction has been effectively applied for many healthcare applications. Predicting breast screening attendance using machine learning (prior to the actual mammogram) is a new field. This paper presents new predictor attributes for such an algorithm. It describes a new hybrid algorithm that relies on back-propagation and radial basis function-based neural networks for prediction. The algorithm has been developed in an open source-based environment. The algorithm was tested on a 13-year dataset (1995-2008). This paper compares the algorithm and validates its accuracy and efficiency with different platforms. Nearly 80% accuracy and 88% positive predictive value and sensitivity were recorded for the algorithm. The results were encouraging; 40-50% of negative predictive value and specificity warrant further work. Preliminary results were promising and provided ample amount of reasons for testing the algorithm on a larger scale.,"Breast,
Machine learning algorithms,
Prediction algorithms,
Cancer,
Databases,
Algorithm design and analysis"
High performance lithographic hotspot detection using hierarchically refined machine learning,"Under real and continuously improving manufacturing conditions, lithography hotspot detection faces several key challenges. First, real hotspots become less but harder to fix at post-layout stages; second, false alarm rate must be kept low to avoid excessive and expensive post-processing hotspot removal; third, full chip physical verification and optimization require fast turn-around time. To address these issues, we propose a high performance lithographic hotspot detection flow with ultra-fast speed and high fidelity. It consists of a novel set of hotspot signature definitions and a hierarchically refined detection flow with powerful machine learning kernels, ANN (artificial neural network) and SVM (support vector machine). We have implemented our algorithm with industry-strength engine under real manufacturing conditions in 45nm process, and showed that it significantly outperforms previous state-of-the-art algorithms in hotspot detection false alarm rate (2.4X to 2300X reduction) and simulation run-time (5X to 237X reduction), meanwhile archiving similar or slightly better hotspot detection accuracies. Such high performance lithographic hotspot detection under real manufacturing conditions is especially suitable for guiding lithography friendly physical design.","Layout,
Kernel,
Artificial neural networks,
Support vector machines,
Manufacturing,
Predictive models,
Accuracy"
BELM: Bayesian Extreme Learning Machine,"The theory of extreme learning machine (ELM) has become very popular on the last few years. ELM is a new approach for learning the parameters of the hidden layers of a multilayer neural network (as the multilayer perceptron or the radial basis function neural network). Its main advantage is the lower computational cost, which is especially relevant when dealing with many patterns defined in a high-dimensional space. This brief proposes a Bayesian approach to ELM, which presents some advantages over other approaches: it allows the introduction of a priori knowledge; obtains the confidence intervals (CIs) without the need of applying methods that are computationally intensive, e.g., bootstrap; and presents high generalization capabilities. Bayesian ELM is benchmarked against classical ELM in several artificial and real datasets that are widely used for the evaluation of machine learning algorithms. Achieved results show that the proposed approach produces a competitive accuracy with some additional advantages, namely, automatic production of CIs, reduction of probability of model overfitting, and use of a priori knowledge.","Bayesian methods,
Machine learning,
Training,
Mathematical model,
Computational modeling,
Optimization,
Artificial neural networks"
How Far You Can Get Using Machine Learning Black-Boxes,"Supervised Learning (SL) is a machine learning research area which aims at developing techniques able to take advantage from labeled training samples to make decisions over unseen examples. Recently, a lot of tools have been presented in order to perform machine learning in a more straightforward and transparent manner. However, one problem that is increasingly present in most of the SL problems being solved is that, sometimes, researchers do not completely understand what supervised learning is and, more often than not, publish results using machine learning black-boxes. In this paper, we shed light over the use of machine learning black-boxes and show researchers how far they can get using these out-of-the-box solutions instead of going deeper into the machinery of the classifiers. Here, we focus on one aspect of classifiers namely the way they compare examples in the feature space and show how a simple knowledge about the classifier’s machinery can lift the results way beyond out-of-the-box machine learning solutions.","Machine learning,
Shape,
Measurement,
Pixel,
Support vector machines,
Accuracy,
Kernel"
Segmentation and Classification of Histological Images - Application of Graph Analysis and Machine Learning Methods,"The characterization and quantitative description of histological images is not a simple problem. To reach a final diagnosis, usually the specialist relies on the analysis of characteristics easily observed, such as cells size, shape, staining and texture, but also depends on the hidden information of tissue localization, physiological and pathological mechanisms, clinical aspects, or other etiological agents. In this paper, Mathematical Morphology (MM) and Machine Learning (ML) methods were applied to characterize and classify histological images. MM techniques were employed for image analysis. The measurements obtained from image and graph analysis were fed into Machine Learning algorithms, which were designed and developed to automatically learn to recognize complex patterns and make intelligent decisions based on data. Specifically, a linear Support Vector Machine (SVM) was used to evaluate the discriminatory power of the used measures. The results show that the methodology was successful in characterizing and classifying the differences between the architectural organization of epithelial and adipose tissues. We believe that this approach can be also applied to classify and help the diagnosis of many tissue abnormalities, such as cancers.","Support vector machines,
Cancer,
Training,
Pixel,
Image segmentation,
Machine learning,
Feature extraction"
Real-time transient stability assessment model using extreme learning machine,"In recent years, computational intelligence and machine learning techniques have gained popularity to facilitate very fast dynamic security assessment for earlier detection of the risk of blackouts. However, many of the current state-of-the-art models usually suffer from excessive training time and complex parameters tuning problems, leading to inefficiency for real-time implementation and on-line model updating. In this study, a new transient stability assessment model using the increasingly prevalent extreme learning machine theory is developed. It has significantly improved the learning speed and can enable effective on-line updating. The proposed model is examined on the New England 39-bus test system, and compared with some state-of-the-art methods in terms of computation time and prediction accuracy. The simulation results show that the proposed model possesses significant superior computation speed and competitively high accuracy.","power system transient stability,
learning (artificial intelligence),
power engineering computing"
Preimage Problem in Kernel-Based Machine Learning,"While the nonlinear mapping from the input space to the feature space is central in kernel methods, the reverse mapping from the feature space back to the input space is also of primary interest. This is the case in many applications, including kernel principal component analysis (PCA) for signal and image denoising. Unfortunately, it turns out that the reverse mapping generally does not exist and only a few elements in the feature space have a valid preimage in the input space. The preimage problem consists of finding an approximate solution by identifying data in the input space based on their corresponding features in the high dimensional feature space. It is essentially a dimensionality-reduction problem, and both have been intimately connected in their historical evolution, as studied in this article.","Kernel,
Principal component analysis,
Machine learning,
Noise reduction,
Optimization,
Classification algorithms,
Signal processing algorithms"
Polyphonic Word Disambiguation with Machine Learning Approaches,"Five different classification models, namely RFR_SUM, CRFs, Maximum Entropy, SVM and Semantic Similarity Model, are employed for polyphonic disambiguation. Based on observation of the experiment outcome of these models, an additional ensemble method based on majority voting is proposed. The ensemble method obtains an average precision of 96.78%, which is much better than the results obtained in previous literatures.","Semantics,
Support vector machines,
Entropy,
Context,
Context modeling,
Machine learning,
Kernel"
Interactive machine learning for incorporating user emotions in automatic music harmonization,"Harmonization enriches piano melodies by adding variations such as mood, sound enhancements and beats that are the key building blocks of piano music. However, not all piano players and song writers are gifted with the musical talent of harmonizing piano melodies effectively since it requires keeping track of an extensive set of western music rules and concepts, years of training and practice and also musicality within them to harmonize a melody accurately. This paper discusses a solution for the tedious task of harmonization by introducing ‘ChordATune’, an interactive tool for harmonizing melodies and generating chord progressions according to user emotions. Further, ChordATune provides a mechanism to arrange chords according to different genres, drum beats and tempi based on user preference. A machine learning approach with Hidden Markov Model (HMM), along with dynamic programming is used to generate the chord progression for a given melody and embed the emotional factor of the user. The melody is taken in as an audio file to the system, where a pitch class profile is created at run time representing the pitch content of the file over time. In order to embed the emotional factor, the Hidden Markov Model is dynamically created, and HMM properties are generated at run time according to the selected emotional factor and the input pitch classes (melody). Around 250 lead sheets were used to train the system using data driven and heuristic approaches, and the evaluation results represented 80% user satisfaction of the prototype. This research further opens a path for research concerning chord progression generation for vocals, taking into account the extraction of words, emotional factor and the tune extracted from the actual voice of the user.","Hidden Markov models,
Music,
Artificial intelligence,
Generators,
Usability,
Lead,
Training"
An Autonomic Context Management Model Based on Machine Learning,"In this paper we approach the context management problem by defining a self-healing algorithm that uses a policy-driven reinforcement learning mechanism to take run-time decisions. The situation calculus and information system theories are used to define and formalize self-healing concepts such as context situation entropy and equivalent context situations. The self-healing property is enforced by monitoring the system's execution environment to evaluate the degree of fulfilling the context policies for a context situation, and to determine the actions to be executed in order to keep the system in a consistent healthy state.","Context,
Entropy,
Learning,
Context-aware services,
Information systems,
Calculus,
Context modeling"
Applying machine learning algorithms for automatic Persian text classification,"Automatic document classification due to its various applications in data mining and information technology is one of the important topics in computer science. Classification plays a vital role in many information management and retrieval tasks. Document classification, also known as document categorization, is the process of assigning a document to one or more predefined category labels. Classification is often posed as a supervised learning problem in which a set of labeled data is used to train a classifier which can be applied to label future examples [1]. Document classification includes different parts such as text processing, feature extraction, feature vector construction and final classification. Thus improvement in each part should lead to better results in document classification. In this paper, we apply machine learning methods for automatic Persian news classification. In this regard, we first try to exert some language preprocess in Hamshahri dataset [2], and then we extract a feature vector for each news text by using feature weighting and feature selection algorithms. After that we train our classifier by support vector machine (SVM) and K-nearest neighbor (KNN) algorithms. In Experiments, although both algorithms show acceptable results for Persian text classification, the performance of KNN is better in comparison to SVM.","Classification algorithms,
Text categorization,
Feature extraction,
Support vector machine classification,
Machine learning algorithms,
Kernel"
Spatial Based Feature Generation for Machine Learning Based Optimization Compilation,"Modern compilers provide optimization options to obtain better performance for a given program. Effective selection of optimization options is a challenging task. Recent work has shown that machine learning can be used to select the best compiler optimization options for a given program. Machine learning techniques rely upon selecting features which represent a program in the best way. The quality of these features is critical to the performance of machine learning techniques. Previous work on feature selection for program representation is based on code size, mostly executed parts, parallelism and memory access patterns with-in a program. Spatial based information–how instructions are distributed with-in a program–has never been studied to generate features for the best compiler options selection using machine learning techniques. In this paper, we present a framework that address how to capture the spatial information with-in a program and transform it to features for machine learning techniques. An extensive experimentation is done using the SPEC2006 and MiBench benchmark applications. We compare our work with the IBM Milepost-gcc framework. The Milepost work gives a comprehensive set of features for using machine learning techniques for the best compiler options selection problem. Results show that the performance of machine learning techniques using spatial based features is better than the performance using the Milepost framework. With 66 available compiler options, we are also able to achieve 70% of the potential speed up obtained through an iterative compilation.","Machine learning,
Optimization,
Histograms,
Program processors,
Predictive models,
Benchmark testing,
Training"
A Dynamic Integrated Model in Machine Learning and Its Application,"In the past decades, machine learning has got great progress. The technique of machine learning is widely used in many areas, such as banking and scientific research. Different from traditional research directions, a dynamic integration is getting more important, which could offer an improvement and optimize the learning efficiency with the known models and algorithms. This paper is aimed to offer a dynamic integrated model on which many models and algorithms could be dynamic integrated. Three dimensions are used to describe the knowledge space. Through the superimposing of the learning clouds, which are used to describe the state of study in one area, we describe the progress of learning. Another discrete model is offered as a simplification of the model, and it also works as a platform for simulation.","Heuristic algorithms,
Machine learning,
Computational modeling,
Machine learning algorithms,
Algorithm design and analysis,
Data mining,
Training"
The Influence Machine: Nonnegative Instance-Space Learning with Differentiated Regularization,"We introduce a new method for classification called the influence machine. The influence machine assigns influence powers to the instances in the training sample so that they can apply their influence to other instances through the connections between the instances specified by a connection matrix. A new instance is classified to be positive if the overall influence it receives is positive and vice versa. Similar to support vector machine (SVM), the influence machine selects a small subset of the training instances to give influence power. However, this selection is very different from how the support vectors are selected by SVM. Experiment results show that the classification performance of the influence machine is comparable to that of the SVM. In a few cases, the influence machine shows much better classification accuracy. The influence machine has other advantages: any similarity matrix can be applied with the influence machine, not like SVM which requires that the kernel be positive definite. Furthermore, the influence machine uses linear optimization, instead of the quadratic optimization used by SVM. It may be more suitable for large scale learning problems.","Support vector machines,
Training,
Kernel,
Optimization,
Accuracy,
Machine learning,
Loss measurement"
Pre-Processing Structured Data for Standard Machine Learning Algorithms by Supervised Graph Propositionalization - A Case Study with Medicinal Chemistry Datasets,"Graph propositionalization methods can be used to transform structured and relational data into fixed-length feature vectors, enabling standard machine learning algorithms to be used for generating predictive models. It is however not clear how well different propositionalization methods work in conjunction with different standard machine learning algorithms. Three different graph propositionalization methods are investigated in conjunction with three standard learning algorithms: random forests, support vector machines and nearest neighbor classifiers. An experiment on 21 datasets from the domain of medicinal chemistry shows that the choice of propositionalization method may have a significant impact on the resulting accuracy. The empirical investigation further shows that for datasets from this domain, the use of the maximal frequent item set approach for propositionalization results in the most accurate classifiers, significantly outperforming the two other graph propositionalization methods considered in this study, SUBDUE and MOSS, for all three learning methods.","Itemsets,
Classification algorithms,
Machine learning,
Machine learning algorithms,
Data mining,
Chemistry,
Kernel"
Determination of Vocational Fields with Machine Learning Algorithm,"The importance of vocational and technical training is growing day by day in parallel to the developing technology. It is inevitable to utilise opportunities presented by information and communication technologies in order to determine vocational fields in vocational and technical training in the most efficient manner. In this respect, it is possible to create a more efficient tool compared to the current methods by utilising machine learning which is an artificial intelligence model in energy applications that predicts events in the future depending on the past experiences. In the current study, a software is developed that ensures that the system learns about the successful and unsuccessful choices made in the past by applying “Naive Bayes” algorithm, which is a machine learning algorithm, to the data collected concerning the individuals who turned out to be successful or unsuccessful in the vocational technical training process in energy applications. In the software developed, it is aimed that the system recommends the most suitable vocational field for the individual by according to the data collected from the individual who is in the occupation selection process in field energy applications.","Classification algorithms,
Data mining,
Prediction algorithms,
Training data,
Training,
Machine learning algorithms,
Probability"
Boolean Factor Analysis for Data Preprocessing in Machine Learning,"We present two input data preprocessing methods for machine learning (ML). The first one consists in extending the set of attributes describing objects in input data table by new attributes and the second one consists in replacing the attributes by new attributes. The methods utilize formal concept analysis (FCA) and boolean factor analysis, recently described by FCA, in that the new attributes are defined by so-called factor concepts computed from input data table. The methods are demonstrated on decision tree induction. The experimental evaluation and comparison of performance of decision trees induced from original and preprocessed input data is performed with standard decision tree induction algorithms ID3 and C4.5 on several benchmark datasets.","Decision trees,
Machine learning,
Matrix decomposition,
Data preprocessing,
Data mining,
Learning systems,
Bismuth"
Identifying Abbreviation Definitions Machine Learning with Naturally Labeled Data,"The rapid growth of biomedical literature requires accurate text analysis and text processing tools. Detecting abbreviations and identifying their definitions is an important component of such tools. In this work, we develop a machine learning algorithm for abbreviation definition identification in text. Most existing approaches for abbreviation definition identification employ rule-based methods. While achieving high precision, rule-based methods are limited to the rules defined and fail to capture many uncommon definition patterns. Supervised learning techniques, which offer more flexibility in detecting abbreviation definitions, have also been applied to the problem. However, they require manually labeled training data. In this study, we make use of what we term naturally labeled data. Positive training examples are extracted from text, which provides naturally occurring potential abbreviation-definition pairs. Negative training examples are generated randomly by mixing potential abbreviations with unrelated potential definitions. The machine learner is trained to distinguish between these two sets of examples. Then, the learned feature weights are used to identify the abbreviation full form. This approach does not require manually labeled training data. We evaluate the performance of our algorithm on the Ab3P, BIOADI and Meds tract corpora. We achieve an F-score that is comparable to the earlier existing systems yet with a higher recall.","Compounds,
Training,
Humans,
Machine learning algorithms,
Hardware design languages,
Lipidomics,
Training data"
Plant Species Classification Using a 3D LIDAR Sensor and Machine Learning,"In the domain of agricultural robotics, one major application is crop scouting, e.g., for the task of weed control. For this task a key enabler is a robust detection and classification of the plant and species. Automatically distinguishing between plant species is a challenging task, because some species look very similar. It is also difficult to translate the symbolic high level description of the appearances and the differences between the plants used by humans, into a formal, computer understandable form. Also it is not possible to reliably detect structures, like leaves and branches in 3D data provided by our sensor. One approach to solve this problem is to learn how to classify the species by using a set of example plants and machine learning methods. In this paper we are introducing a method for distinguishing plant species using a 3D LIDAR sensor and supervised learning. For that we have developed a set of size and rotation invariant features and evaluated experimentally which are the most descriptive ones. Besides these features we have also compared different learning methods using the toolbox Weka. It turned out that the best methods for our application are simple logistic regression functions, support vector machines and neural networks. In our experiments we used six different plant species, typically available at common nurseries, and about 20 examples of each species. In the laboratory we were able to identify over 98% of these plants correctly.","Three dimensional displays,
Robot sensing systems,
Agriculture,
Lasers,
Histograms,
Artificial neural networks"
Mahalanobis Space Learning Machine for Quality Diagnosis,"In this paper, a pattern classification algorithm based on Mahalanobis space and its solving via second order cone programming were discussed. The method of selecting feature through Mahalanobis Taguchi System and integrating the Mahalanobis Taguchi System and Mahalanobis space learning machine for pattern classification were proposed. At last, an example of gear quality diagnosis was presented on real data, and the effect of this methodology was proved.","Training,
Machine learning,
Covariance matrix,
Kernel,
Testing,
Gears,
Support vector machines"
A two-stage machine learning approach for pathway analysis,"Analysis of gene expression data has emerged as an important approach to discover active pathways related to biological phenotypes. Previous pathway analysis methods use all genes in a pathway for linking it to a particular phenotype. Using only a subset of informative genes, however, could better classify samples. Here, we propose a two-stage machine learning approach for pathway analysis. During the first stage, informative genes that can represent a pathway are selected using feature selection methods. These “representative genes” are mostly associated with the phenotype of interest. In the second stage, pathways are ranked based on their “representative genes” using classification methods. We applied our two-stage approach on three gene expression datasets. The results indicate our method does outperform methods that consider every gene in a pathway.","Gene expression,
Error analysis,
Bioinformatics,
Redundancy,
Breast cancer,
Machine learning,
Support vector machines"
Machine learning approaches for the investigation of features beyond seed matches affecting miRNA binding,"MicroRNAs are one type of noncoding RNA that regulate their target mRNAs before mRNAs are translated into proteins. Although it has been demonstrated that the regulation is through partial binding of the seed region of a miRNA and its targets, the mechanism of this process is not fully discovered. Some biological experiments have shown that even perfect base pairing in the seed region does not always guarantee the down-regulation of the targets. It has been suspected that some other characteristics of mRNAs may facilitate the regulation. An earlier study (1) has identified five additional features beyond seed matching that seem to significantly affect repressions. However, the observation that evolutionally conserved targets have shown significantly more destabilization comparing to nonconserved targets with the same score using these five features leads to the suspicion that additional features remain to be discovered. This motivates our study to identify additional features that may differentiate down-regulated mRNAs (positive set) from those not down-regulated ones (negative set) provided both sets have perfect seed matches with miRNAs. Our first attempt to search for different sequence motifs around seed site regions in the two different sets is not successful. We further construct a set of 18 sequence/structure features based on domain knowledge and evaluate them individually and jointly. By employing feature selection techniques in combination with several classification methods, we have been able to identify a subset of features that may facilitate the down-regulation of mRNAs. Our results can be incorporated into target prediction algorithms to further improve target specificities.","Classification algorithms,
Prediction algorithms,
Machine learning algorithms,
Artificial neural networks,
Accuracy,
Support vector machines,
Decision trees"
Intracranial pressure level prediction in traumatic brain injury by extracting features from multiple sources and using machine learning methods,"This paper proposes a non-intrusive method to predict/estimate the intracranial pressure (ICP) level based on features extracted from multiple sources. Specifically, these features include midline shift measurement and texture features extracted from CT slices, as well as patient's demographic information, such as age. Injury Severity Score is also considered. After aggregating features from slices, a feature selection scheme is applied to select the most informative features. Support vector machine (SVM) is used to train the data and build the prediction model. The validation is performed with 10 fold cross validation. To avoid overfitting, all the feature selection and parameter selection are done using training data during the 10 fold cross validation for evaluation. This results an nested cross validation scheme implemented using Rapidminer. The final classification result shows the effectiveness of the proposed method in ICP prediction.","Computed tomography,
Feature extraction,
Iterative closest point algorithm,
Blood,
Entropy,
Brain,
Injuries"
Machine learning with text recognition,"The prime objective of this Research is the development of effective reading skills in Machines. After reading the text and comprehending the meaning, it would self-program itself and according to the program it would implement the instructions. Here we are exploring a new era of computer vision and related Research. The current investigation presents an algorithm and software which detects, recognizes text and character with specific protocol in a live streaming video and programs itself according to the text. The protocol used for text consists of a Start word and an end word with embedded c instructions between them in special font. The proposed algorithm first detects the frame having text from live video streaming. The character is at its best view which means no broken point in a single character and no merge between groups of characters. The image processor tallies the images and burns the main microcontroller accordingly. This technology can prove to be immensely important to various human activities in day to day life.","Character recognition,
Streaming media,
Humans,
Robots,
Text recognition,
Protocols,
Machine learning"
An empirical model for clustering and classification of instrumental music using machine learning technique,"To extract implicit knowledge and data relationships from the audio and audio similarity measure, this paper uses the audio mining techniques. A model for audio clustering and classification technique is proposed. Neural networks are used for classifying the data. The working prototype of the Music classification system has been developed and tested in MATLAB 6.5 using the signal Processing Toolbox under Windows XP operating system in a normal Pentium range of desktop computer.","Feature extraction,
Training,
Data mining,
Multimedia communication,
Artificial neural networks,
Mathematical model,
Fluctuations"
Machine Learning and keyword-matching integrated Protocol Identification,"Identifying the underlying protocol carried in the data traffic (i.e., Protocol Identification) is of fundamental important to QoS, Security, Network management and many other purposes. Port-based, content-based and behavior-based are commonly used identification methods in today's networks. However, all of these methods have their own shortcomings. In this paper, a new Machine Learning and Keyword-matching Integrated (MALKI) protocol identification method is proposed to overcome the shortcomings brought by these existing methods. The proposed method combines the content and behavior-based technologies together to identify the underlying protocol in the data flow. A prototype is implemented on a high performance multi-core processor platform. From the experimental results, we can see the proposed method is effective and efficient when applied into the protocol identification.",Protocols
A novel approach to memory power estimation using machine learning,"Reducing power consumption has become a priority in microprocessor design as more devices become mobile and as the density and speed of components lead to power dissipation issues. Power allocation strategies for individual components within a chip are being researched to determine optimal configurations to balance power and performance. Modelling and estimation tools are necessary in order to understand the behaviour of energy consumption in a run time environment. This paper discusses a novel approach to power metering by estimating it using a set of observed variables that share a linear or non-linear correlation to the power consumption. The machine learning approaches exploit the statistical relationship among potential variables and power consumption. We show that Support Vector Machine regression (SVR), Genetic Algorithms (GA) and Neural Networks (NN) can all be used to cheaply and easily predict memory power usage based on these observed variables.","Artificial neural networks,
Power demand,
Estimation,
Benchmark testing,
Biological system modeling,
Analytical models,
Accuracy"
A time-series approach for shock outcome prediction using machine learning,"Chances of successful defibrillation, and that of subsequent return of spontaneous circulation (ROSC), worsen rapidly with passage of time during cardiac arrest. The Electrocardiogram (ECG) signal of ventricular fibrillation (VF) has been analyzed for certain characteristics which may be predictive of successful defibrillation. Time-series features were extracted. A total of 59 counter shocks (CS) were analyzed. They were best classified as successful or unsuccessful by employing the Random Tree method. An average accuracy of 71% was achieved for 6 randomized runs of 6-fold cross validation. Classification could be performed on ECG tracings of 40 seconds.","Electric shock,
Feature extraction,
Training,
Accuracy,
Electrocardiography,
Heart,
Fibrillation"
Machine Learning Approach for Quality of Experience Aware Networks,Efficient management of multimedia services necessitates the understanding of how the quality of these services is perceived by the users. Estimation of the perceived quality or Quality of Experience (QoE) of the service is a challenging process due to the subjective nature of QoE. This process usually incorporates complex subjective studies that need to recreate the viewing conditions of the service in a controlled environment. In this paper we present Machine Learning techniques for modeling the dependencies of different network and application layer quality of service parameters to the QoE of network services using subjective quality feedback. These accurate QoE prediction models allow us to further develop a geometrical method for calculating the possible remedies per network stream for reaching the desired level of QoE. Finally we present a set of possible network techniques that can deliver the desired improvement to the multimedia streams.,"Videos,
Predictive models,
Adaptation model,
Bit rate,
Data models,
Multimedia communication,
Quality of service"
Adaptive agent generation using machine learning for dynamic difficulty adjustment,"Player experience is a significant parameter in evaluating the overall success of a game, both technically and commercially. It is necessary to provide the player a game that provides: (1) satisfaction and (2) challenge. To enhance player experience, the game difficulty needs to be dynamically adjusted with respect to the player. Dynamic scripting is an importantlearning technique used for dynamic difficulty adjustment (DDA), already implemented successfully in commercial games of different genres. However the DDA systems are not sufficiently consistent in creating equally competent agents and do not provide equal opportunity to human players of different capabilities. This paper focuses on solving these issues by introducing these concepts: (a) dynamic weight clipping, (b) differential learning and (c) adrenalin rush. Experimental results indicate that dynamic scripting, in combination with these features, can implement an ideal DDA system for creating a equally competent computer agent who can engage the human player in absorbing games.","Games,
Complexity theory,
Artificial intelligence,
Equations,
Humans,
Mathematical model,
Aggregates"
Retrieving Tract Variables From Acoustics: A Comparison of Different Machine Learning Strategies,"Many different studies have claimed that articulatory information can be used to improve the performance of automatic speech recognition systems. Unfortunately, such articulatory information is not readily available in typical speaker-listener situations. Consequently, such information has to be estimated from the acoustic signal in a process which is usually termed “speech-inversion.” This study aims to propose and compare various machine learning strategies for speech inversion: Trajectory mixture density networks (TMDNs), feedforward artificial neural networks (FF-ANN), support vector regression (SVR), autoregressive artificial neural network (AR-ANN), and distal supervised learning (DSL). Further, using a database generated by the Haskins Laboratories speech production model, we test the claim that information regarding constrictions produced by the distinct organs of the vocal tract (vocal tract variables) is superior to flesh-point information (articulatory pellet trajectories) for the inversion process.","Speech,
Tongue,
Speech recognition,
Machine learning,
Supervised learning,
Artificial neural networks"
A Machine Learning Based Language Specific Web Site Crawler,"We propose an approach for gathering web pages written in a specific language. The approach consists of a language predictor and a web site crawler. The language predictor is a machine learning based component that can learn from an example host graph some characteristics of relevant hosts, and is used to calculate the language degree of a web server whether it has a high probability to serve web pages written in a target language. The site crawler, on the other hand, chooses to download the web pages from a prioritized list of relevant servers. We have evaluated the crawling performance in terms of coverage and harvest rates. Preliminary experiments using a Thai web data set show a promising result, comparing with the traditional language-specific crawling methods recently proposed in the literatures.","Web pages,
Crawlers,
Feature extraction,
Testing,
Web server"
Evaluating Application-Layer Classification Using a Machine Learning Technique over Different High Speed Networks,"Application–layer classification is needed in many monitoring applications. Classification based on machine learning offers an alternative method to methods based on port or payload based techniques. It is based on statistical features computed from network flows. Several works investigated the efficiency of machine learning techniques and found algorithms suitable for network classification. A classifier based on machine learning is built by learning from a training data set that consists of data from known application traces. In this paper, we evaluate the efficiency of application-layer classification based on C4.5~machine learning algorithm used for classification network flows from different high speed networks, such as 100~Mbit, 1~Gbit and 10~Gbit networks. We find a significant decrease in the classification efficiency when classifier built for one network is used to classify other network. We recommend to build classifier from data collected from all available networks for best results. However, if different networks are not available, good results can be obtained from data traces to the commodity Internet.","Accuracy,
Machine learning algorithms,
Internet,
Machine learning,
Protocols,
Classification algorithms,
Measurement"
A Machine Learning Approach to Performance Prediction of Total Order Broadcast Protocols,"Total Order Broadcast (TOB) is a fundamental building block at the core of a number of strongly consistent, fault-tolerant replication schemes. While it is widely known that the performance of existing TOB algorithms varies greatly depending on the workload and deployment scenarios, the problem of how to forecast their performance in realistic settings is, at current date, still largely unexplored. In this paper we address this problem by exploring the possibility of leveraging on machine learning techniques for building, in a fully decentralized fashion, performance models of TOB protocols. Based on an extensive experimental study considering heterogeneous workloads and multiple TOB protocols, we assess the accuracy and efficiency of alternative machine learning methods including neural networks, support vector machines, and decision tree-based regression models. We propose two heuristics for the feature selection phase, that allow to reduce its execution time up to two orders of magnitude incurring in a very limited loss of prediction accuracy.","Monitoring,
Measurement,
Accuracy,
Predictive models,
Protocols,
Machine learning algorithms,
Benchmark testing"
Bayesian Statistical Inference in Machine Learning Anomaly Detection,"Intrusion Detection is an important component of the security of the Network, through the critical information of the network and host system, it can determine the user’s invasion of illegal and legal acts of the user misuse of resources, and make an adequate response. According to the problem, which machine learning anomaly detection effect is not ideal when the user behavior changes and a separate anomaly detection. Based on the Bayesian inference anomaly detection, applying the Bayesian inference of statistical methods to machine learning anomaly detection, this paper established a decision tree corresponding to the method. This method overcomes the satisfactory of the anomaly detection individual test, and improves the machine learning in the predictive ability of anomaly detection and anomaly detection efficiency.",Information security
A machine learning based method for classification of fractal features of forearm sEMG using Twin Support vector machines,"Classification of surface electromyogram (sEMG) signal is important for various applications such as prosthetic control and human computer interface. Surface EMG provides a better insight into the strength of muscle contraction which can be used as control signal for different applications. Due to the various interference between different muscle activities, it is difficult to identify movements using sEMG during low-level flexions. A new set of fractal features - fractal dimension and Maximum fractal length of sEMG has been previously reported by the authors. These features measure the complexity and strength of the muscle contraction during the low-level finger flexions. In order to classify and identify the low-level finger flexions using these features based on the fractal properties, a recently developed machine learning based classifier, Twin Support vector machines (TSVM) has been proposed. TSVM works on basic learning methodology and solves the classification tasks as two SVMs for each classes. This paper reports the novel method on the machine learning based classification of fractal features of sEMG using the Twin Support vector machines. The training and testing was performed using two different kernel functions - Linear and Radial Basis Function (RBF).","Fractals,
Support vector machines,
Kernel,
Muscles,
Electromyography,
Testing,
Accuracy"
Application of Machine Learning Methods in Active Power Security Correction of Power System,"A method for active power security correction based on BP neural network is presented in this paper. The active power security correction is used to give a minimum regulation of the active power output of generators in order to prevent or alleviate overload of transmission lines and tie line groups. This paper first presents the problem of the traditional method based on sensitivity analysis, and then introduces machine learning methods, and applies the method of BP neural network in the active power security correction in power system. The mathematical model of this problem is given, and the principle of BP neural network and their application procedures are explained briefly as well. The method is proved to be valid, according to the results of active power security correction calculation for the IEEE-30 node system.","Generators,
Artificial neural networks,
Security,
Load flow,
Training,
Learning systems"
Applying machine learning to detect individual heart beats in ballistocardiograms,"Ballistocardiography is a technique in which the mechanical activity of the heart is recorded. We present a novel algorithm for the detection of individual heart beats in ballistocardiograms (BCGs). In a training step, unsupervised learning techniques are used to identify the shape of a single heart beat in the BCG. The learned parameters are combined with so-called “heart valve components” to detect the occurrence of individual heart beats in the signal. A refinement step improves the accuracy of the estimated beat-to-beat interval lengths. Compared to other algorithms this new approach offers heart rate estimates on a beat-to-beat basis and is designed to cope with arrhythmias. The proposed algorithm has been evaluated in laboratory and home settings for its agreement with an ECG reference. A beat-to-beat interval error of 14.16 ms with a coverage of 96.87% was achieved. Averaged over 10 s long epochs, the mean heart rate error was 0.39 bpm.","Heart beat,
Training,
Electrocardiography,
Estimation,
Valves"
Automatic Classification of Software Change Request Using Multi-label Machine Learning Methods,"Automatic text classification of the software change request (CR) can be used for automating impact analysis, bug triage and effort estimation. In this paper, we focus on the automation of the process for assigning CRs to developers and present a solution that is based on automatic text classification of CRs. In addition our approach provides the list of source files, which are required to be modified and an estimate for the time required to resolve a given CR. To perform experiments, we downloaded the set of resolved CRs from the OSS project's repository for Mozilla. We labeled each CR with multiple labels i.e., the developer name, the list of source files, and the time spent to resolve the CR. To train the classifier, our approach applies the Problem Transformation and Algorithm Adaptation methods of multi-label machine learning to the multi-labeled CR data. With this approach, we have obtained precision levels up to 71.3% with 40.1% recall.","Software,
Indexing,
Time division multiplexing,
Machine learning algorithms,
Information retrieval,
Semantics,
Large scale integration"
Power-saving algorithms in electricity usage - comparison between the power saving algorithms and machine learning techniques,"In this paper, we propose a collaborative comparison between the online/offline algorithms for energy demand power saving purposes. Based on the Gaia Power model, resource-buffering algorithms are considered a practical peak-shaving model to effectively minimize the excessive power request. Although the algorithmic infrastructure is focused on a battery, this energy demand power saving problem is analogous to traditional demand and supply problem. In light of the similarity, we implement various machine-learning techniques, including Multiple-Layer Perceptron(MLP), Radial Basis Functions(RBF), Recurrent Neural Networks(RNN) to the identical peak-shaving model problem. In addition, the traditional naïve forecasting model and linear regression will also be discussed. Our findings suggest that the neural networks not only show faster demand smoothing in power saving algorithms, but being a nature of online algorithms is also theoretically and statistically more efficient than resource buffering algorithm and DCEC technology.","Algorithm design and analysis,
Artificial neural networks,
Batteries,
Training,
Poles and towers,
Machine learning algorithms,
Recurrent neural networks"
A Sparse Learning Machine for High-Dimensional Data with Application to Microarray Gene Analysis,"Extracting features from high-dimensional data is a critically important task for pattern recognition and machine learning applications. High-dimensional data typically have much more variables than observations, and contain significant noise, missing components, or outliers. Features extracted from high-dimensional data need to be discriminative, sparse, and can capture essential characteristics of the data. In this paper, we present a way to constructing multivariate features and then classify the data into proper classes. The resulting small subset of features is nearly the best in the sense of Greenshtein's persistence; however, the estimated feature weights may be biased. We take a systematic approach for correcting the biases. We use conjugate gradient-based primal-dual interior-point techniques for large-scale problems. We apply our procedure to microarray gene analysis. The effectiveness of our method is confirmed by experimental results.","Machine learning,
Pattern recognition,
Support vector machines,
Support vector machine classification,
Data mining,
Feature extraction,
Bioinformatics,
Pattern analysis,
Large-scale systems,
Cancer"
Learning Machine Learning: A Case Study,"This correspondence reports on a case study conducted in the Master's-level Machine Learning (ML) course at Blekinge Institute of Technology, Sweden. The students participated in a self-assessment test and a diagnostic test of prerequisite subjects, and their results on these tests are correlated with their achievement of the course's learning objectives.","Machine learning,
Learning systems,
Mathematics,
Artificial intelligence"
Blind Image Deconvolution Using Machine Learning for Three-Dimensional Microscopy,"In this work, we propose a novel method for the regularization of blind deconvolution algorithms. The proposed method employs example-based machine learning techniques for modeling the space of point spread functions. During an iterative blind deconvolution process, a prior term attracts the point spread function estimates to the learned point spread function space. We demonstrate the usage of this regularizer within a Bayesian blind deconvolution framework and also integrate into the latter a method for noise reduction, thus creating a complete blind deconvolution method. The application of the proposed algorithm is demonstrated on synthetic and real-world three-dimensional images acquired by a wide-field fluorescence microscope, where the need for blind deconvolution algorithms is indispensable, yielding excellent results.","Deconvolution,
Machine learning,
Microscopy,
Convolution,
Kernel,
Optoelectronic and photonic sensors,
Machine learning algorithms,
Principal component analysis,
Degradation,
Pixel"
A machine-learning approach to discovering company home pages,"For many marketing and business applications, it is necessary to know the home page of a company specified only by its company name. If we require the home page for a small number of big companies, this task is readily accomplished via use of Internet search engines or access to domain registration lists. However, if the entities of interest are small companies, these approaches can lead to mismatches, particularly if a specified company lacks a home page. We address this problem using a supervised machine-learning approach in which we train a binary classification model. We classify potential website matches for each company name based on a set of explanatory features extracted from the content on each candidate website. Our approach is related to web-based business intelligence in two ways: (1) we build the training set for our learning algorithms through crowdsourcing tools and illustrate their potential for business research, and (2) the success of our model allows one to easily use corporate home pages as data inputs into other research projects. Through the successful use of crowdsourcing, our approach is able to identify a correct home page or recognize that a valid home page does not exist with an accuracy that is 57% better than simply taking the highest ranked search engine result as the correct match.","Companies,
Web pages,
Training,
Logistics,
Feature extraction,
Biological system modeling,
Search engines"
Tera-Scale Performance Machine Learning SoC (MLSoC) With Dual Stream Processor Architecture for Multimedia Content Analysis,"A new machine learning SoC (MLSoC) for multimedia content analysis is implemented with 16-mm2 area in 90-nm CMOS technology. Different from traditional VLSI architectures, it focuses on the coacceleration of computer vision and machine learning algorithms, and two stream processors with massively parallel processing elements are integrated to achieve tera-scale performance. In the dual stream processor (DSP) architecture, the data are transferred between processors and the high-bandwidth dual memory (HBDM) through the local media bus without consuming the AMBA AHB bandwidth. The image stream processor (ISP) of the MLSoC can handle common window-based operations for image processing, and the feature stream processor (FSP) can deal with machine learning algorithms with different dimensions. The power efficiency of the proposed MLSoC is 1.7 TOPS/W, and the area efficiency is 81.3 GOPS/mm 2.","Streaming media,
Pixel,
Computer architecture,
Bandwidth,
Machine learning algorithms,
Digital signal processing,
Multimedia communication"
GPUMLib: A new Library to combine Machine Learning algorithms with Graphics Processing Units,"The Graphics Processing Unit (GPU) is a highly parallel, many-core device with enormous computational power, especially well-suited to address Machine Learning (ML) problems that can be expressed as data-parallel computations. As problems become increasingly demanding, parallel implementations of ML algorithms become critical for developing hybrid intelligent real-world applications. The relative low cost of GPUs combined with the unprecedent computational power they offer, make them particularly well-positioned to automatically analyze and capture relevant information from large amounts of data. In this paper, we propose the creation of an open source GPU Machine Learning Library (GPUMLib) that aims to provide the building blocks for the scientific community to develop GPU ML algorithms. Experimental results on benchmark datasets demonstrate that the GPUMLib components already implemented achieve significant savings over the counterpart CPU implementations.","Graphics processing unit,
Libraries,
Artificial neural networks,
Software algorithms,
Machine learning,
Graphics,
Machine learning algorithms"
Performance of neural network architectures: Cascaded MLP versus extreme learning machine on cervical cell image classification,"In Malaysia, the screening coverage for cervical cancer is poor, which was at 2% in 1992, 3.5% in 1995, and at 6.2% in 1996, due to the shortage in pathologist workforce being one of the major cause. Study has been done before to overcome this by developing a diagnosis system based on neural networks, so that diagnosis can be done by an automated system with pathologist-like knowledge. Cell's features were used as input to the neural network architecture, and cell's classification into NORMAL, Low-Squamous Intraepithelial Lession (LSIL), or High-Squamous Intraepithelial Lession (HSIL) were used as output target. This paper focused on finding the best neural network to be used as classifier tool for cervical cancer diagnostic system with cervical cells' features as input. Two architectures of neural network system were proposed; Cascaded Multilayered Perceptron (c-MLP) and Extreme Learning Machine (ELM). Result suggests that all the features selected which are area, grey level, perimeter, red, green, blue, intensity1, intensity2 and saturation are more suitable to be used with c-MLP neural network architecture compared to ELM, with the accuracy of 96.02%.","Testing,
Classification algorithms,
Cells (biology),
Machine learning,
Artificial neural networks,
Jacobian matrices"
Deep Machine Learning - A New Frontier in Artificial Intelligence Research [Research Frontier],"This article provides an overview of the mainstream deep learning approaches and research directions proposed over the past decade. It is important to emphasize that each approach has strengths and ""weaknesses, depending on the application and context in ""which it is being used. Thus, this article presents a summary on the current state of the deep machine learning field and some perspective into how it may evolve. Convolutional Neural Networks (CNNs) and Deep Belief Networks (DBNs) (and their respective variations) are focused on primarily because they are well established in the deep learning field and show great promise for future work.","Artificial neural networks,
Machine learning,
Training,
Computer architecture,
Robustness,
Computational modeling,
Brain modeling"
Projection Vector Machine: One-stage learning algorithm from high-dimension small-sample data,"The presence of fewer samples and large number of input features increases the complexity of the classifier and degrades the stability. Thus, dimension reduction was always carried before supervised learning algorithms such as neural network. This two-stage framework is somewhat redundant in dimension reduction and network training. This paper proposes a novel one-stage learning algorithm for high-dimension small-sample data, called Projection Vector Machine (PVM), which combines dimension reduction with network training and removes the redundancy. Through dimension reduction operation such as singular vector decomposition (SVD), we not only reduce the dimension but also obtain the size of single-hidden layer feedforward neural network (SLFN) and input weight values simultaneously. This size-fixed network will become linear programming system and thus the output weights can be determined by simple least square method. Unlike traditional backpropagation feedforward neural network (BP), parameters in PVM don't need iterative tuning and thus its training speed is much faster than BP. Unlike extreme learning machine (ELM) proposed by Huang [G.-B. Huang, Q.-Y. Zhu, C.-K. Siew, Extreme learning machine: theory and applications, Neurocomputing 70 (2006) 489–501] which assigns input weights randomly, PVM's input weights are ranked by singular values and select the optimal weights order by singular value. We give proof that PVM is a universal approximator for high-dimension small-sample data. Experimental results show that the proposed one-stage algorithm PVM is faster than two-stage learning approach such as SVD+BP and SVD+ELM.","Training,
Neurons,
Artificial neural networks,
Machine learning,
Algorithm design and analysis,
Accuracy,
Classification algorithms"
A comparative study of machine learning techniques in blog comments spam filtering,"In this paper we compare four machine learning techniques for blog comments spam filtering. the machine learning techniques are the Naïve Bayes, K-nearest neighbor, neural networks and the support vector machines. For this comparative study we used a blog comment corpus that has been affected by spam, which is our study case in this work. We classify the comments of this blog comments corpus, which have 50 pages and 1024 blog comments are classified in spam an non-spam. The percentage of spam of this corpus is 67%.","Training,
Neurons,
Internet,
Information services,
Classification algorithms,
Web sites,
Machine learning"
On-line distinction methods of human falling motions based on machine learning,"A hip protector system using an airbag for prevention of femoral neck fractures is under developing by our group. In the system, instance detection of falling motions by using an appropriate on-line algorithm based on sensor signals is required. The purpose of the present paper is to propose online distinction procedures for human falling motions based on the machine learning, such as the support vector machine and the neural network. Three-types of falling motions which cause a femoral neck fracture for elderly people are considered in the present paper. Five distinction procedures for detecting falling motions are proposed in the present paper. In the proposed procedures, one axis gyro sensor and two axis accelerometers are used. The detection performances of the five procedures are evaluated for the three-types of falling motions as well as the many kinds of daily motions. As the results, the procedure based on the neural network considering time series data of sensor signals provides 100% detection rates for the three-types of falling motions. In addition, robust performances of sensor installation errors of the position/angle are evaluated. We confirmed that the proposed method based on the neural network can ensure robust performances for sensor installation errors of the position/angle.","Support vector machines,
Neck,
Accelerometers,
Artificial neural networks,
Observers,
Equations"
Active learning support vector machines to classify imbalanced reservoir simulation data,"Reservoir modeling is an on-going activity during the production life of a reservoir. One challenge to constructing accurate reservoir models is the time required to carry out a large number of computer simulations. To address this issue, we have constructed surrogate models (proxies) for the computer simulator to reduce the simulation time. The quality of the proxies, however, relies on the quality of the computer simulation data. Frequently, the majority of the simulation outputs match poorly to the production data collected from the field. In other words, most of the data describe the characteristics of what the reservoir is not (negative samples), rather than what the reservoir is (positive samples). Applying machine learning methods to train a simulator proxy based on these data faces the challenge of imbalanced training data. This work applies active learning support vector machines to incrementally select a subset of informative simulation data to train a classifier as the simulator proxy. We compare the results with the results produced by the standard support vector machines combined with other imbalanced training data handling techniques. Based on the support vectors in the trained classifiers, we analyze high impact parameters that separating good-matching reservoir models from bad-matching models.","Support vector machines,
Reservoirs,
Computational modeling,
Data models,
Production,
Petroleum,
Training data"
Machine learning approach for crude oil price prediction with Artificial Neural Networks-Quantitative (ANN-Q) model,"The volatility of crude oil market and its chain effects to the world economy augmented the interest and fear of individuals, public and private sectors. Previous statistical and econometric techniques used for prediction, offer good results when dealing with linear data. Nevertheless, crude oil price series deal with high nonlinearity and irregular events. The continuous usage of statistical and econometric techniques for crude oil price prediction might demonstrate demotions to the prediction performance. Machine Learning and Computational Intelligence approach through combination of historical quantitative data with qualitative data from experts' view and news is a remedy proposed to predict this. This paper will discuss the first part of the research, focusing on to (i) the development of Hierarchical Conceptual (HC) model and (ii) the development of Artificial Neural Networks-Quantitative (ANN-Q) model.","Petroleum,
Biological system modeling,
Predictive models,
Artificial neural networks,
Feature extraction,
Data models,
Computational modeling"
Parallel tempering is efficient for learning restricted Boltzmann machines,"A new interest towards restricted Boltzmann machines (RBMs) has risen due to their usefulness in greedy learning of deep neural networks. While contrastive divergence learning has been considered an efficient way to learn an RBM, it has a drawback due to a biased approximation in the learning gradient. We propose to use an advanced Monte Carlo method called parallel tempering instead, and show experimentally that it works efficiently.","Neurons,
Temperature distribution,
Probability distribution,
Training data,
Machine learning,
Training,
Monte Carlo methods"
Model selection for support vector machines: Advantages and disadvantages of the Machine Learning Theory,"A common belief is that Machine Learning Theory (MLT) is not very useful, in pratice, for performing effective SVM model selection. This fact is supported by experience, because well-known hold-out methods like cross-validation, leave-one-out, and the bootstrap usually achieve better results than the ones derived from MLT. We show in this paper that, in a small sample setting, i.e. when the dimensionality of the data is larger than the number of samples, a careful application of the MLT can outperform other methods in selecting the optimal hyperparameters of a SVM.",Support vector machines
Acquisition of adaptive walking behaviors using machine learning with Central Pattern Generator,"Recently, biologically inspired approaches have received much attention for robot control. A typical example of them is control of rhythmic behaviors by Central Pattern Generator (CPG). However, this control has a problem that there are few theories to determine parameters of CPG. For this reason, they are determined experimentally. In this paper, we propose a combination method of Genetic Algorithm and Reinforcement Learning for determining parameters of CPG, and apply to a quadruped robot with the CPG controller. Simulation results show that the robot obtains walking behaviors automatically through learning process without using the parameters set by knowledge of designers.","Leg,
Legged locomotion,
Biological system modeling,
Propulsion,
Joints"
Machine learning in fuel consumption prediction of aircraft,"Nowadays, as fuel is an important resource for the whole world, researchers are trying a variety machine learning models for fuel flow prediction in industry, aerospace specifically. Different machine learning models have been applied in different applications. This paper will analyze these applications. Many useful points have been found by comparison of those experimental results.","Machine learning,
Fuels,
Aircraft,
Atmospheric modeling,
Artificial neural networks,
Predictive models,
Trajectory"
A Machine Learning Approach for Optimizing Parallel Logic Simulation,"Parallel discrete event simulation can be applied as a fast and cost effective approach for the gate level simulation of current VLSI circuits. In this paper we combine a dynamic load balancing algorithm and a bounded window algorithm for optimistic gate level simulation. The bounded time window prevents the simulation from being too optimistic and from excessive rollbacks. We utilize a machine learning algorithm (Qlearning) to effect this combination. We introduce two dynamic load-balancing algorithms for balancing the communication and computational load and use two learning agents to combine these algorithms. One learning agent combines the two learning algorithms and learns their corresponding parameters, while the second optimizes the value of the time window. Experimental results show up to a 46% improvement in the simulation time using this combined algorithm for several open source circuits. To the best of our knowledge, this is the first time that Q-learning has been used to optimize an optimistic gate level simulation.","Program processors,
Heuristic algorithms,
Load modeling,
Integrated circuit modeling,
Logic gates,
Hardware design languages,
Learning"
Archetypal analysis for machine learning,"Archetypal analysis (AA) proposed by Cutler and Breiman in [1] estimates the principal convex hull of a data set. As such AA favors features that constitute representative 'corners' of the data, i.e. distinct aspects or archetypes. We will show that AA enjoys the interpretability of clustering - without being limited to hard assignment and the uniqueness of SVD - without being limited to orthogonal representations. In order to do large scale AA, we derive an efficient algorithm based on projected gradient as well as an initialization procedure inspired by the FURTHESTFIRST approach widely used for K-means [2]. We demonstrate that the AA model is relevant for feature extraction and dimensional reduction for a large variety of machine learning problems taken from computer vision, neuroimaging, text mining and collaborative filtering.","Feature extraction,
Principal component analysis,
Data models,
Computational modeling,
Machine learning,
Data mining,
Face"
A machine learning algorithm for expert system based on subjective Bayesian method,"Based on subjective Bayesian method, knowledge representation model and inference method were analyzed, and the formula for certainty in reasoning process was given, and what lead to the inaccurate sufficient factor and necessary factor were discussed, and a correction algorithm to adjust the sufficient factor and the necessary factor to the true one based on case studies was given in this paper. Experiments proved that even the sufficient factor and necessary factor were inaccurate at the beginning, by using the correction algorithm, the sufficient factor and necessary factor could converge at actual values after adequate number of case studies.","Machine learning algorithms,
Expert systems,
Bayesian methods,
Uncertainty,
Knowledge representation,
Inference algorithms,
Probability,
Computer science,
Petroleum,
Algorithm design and analysis"
Machine Learning in Medical Imaging,"This article will discuss very different ways of using machine learning that may be less familiar, and we will demonstrate through examples the role of these concepts in medical imaging. Although the term machine learning is relatively recent, the ideas of machine learning have been applied to medical imaging for decades, perhaps most notably in the areas of computer-aided diagnosis (CAD) and functional brain mapping. We will not attempt in this brief article to survey the rich literature of this field. Instead our goals will be 1) to acquaint the reader with some modern techniques that are now staples of the machine-learning field and 2) to illustrate how these techniques can be employed in various ways in medical imaging.","Machine learning,
Biomedical imaging,
Predictive models,
Support vector machines,
Brain mapping,
Supervised learning,
Cancer,
Computer aided diagnosis,
Image retrieval,
Content based retrieval"
Learning Microarray Cancer Datasets by Random Forests and Support Vector Machines,"Analyzing gene expression data from microarray devices has many important applications in medicine and biology: the diagnosis of disease, accurate prognosis for particular patients, and understanding the response of a disease to drugs, to name a few. Two classifiers, random forests and support vector machines are studied in application to micro array cancer data sets. Performance of classifiers with different numbers of genes were evaluated in hope to find out if a smaller number of good genes gives a better classification rate.","Machine learning,
Cancer,
Support vector machines,
Support vector machine classification,
Diseases,
Gene expression,
Kernel,
Error analysis,
Medical diagnostic imaging,
Drugs"
Global Strategy of Active Machine Learning for Complex Systems: Embryogenesis Application on Cell Division Detection,"The intrinsic complexity of biological systems creates huge amounts of unlabeled experimental data. The exploitation of such data can be achieved by performing active machine learning accompanied by a high-level symbolic expert who defines categories and their best boundaries using as little data as possible. We present a global strategy for designing active machine learning methods suited for the observation and analysis of complex systems, such as embryonic development. We developed a procedure that uses all available knowledge, whether gathered manually or automatically, and is able to readjust when new data is provided. We show that it is a powerful method for the investigation of the morphogenetic features of embryogenesis and specifically mitosis detection. It will make possible to properly reconstruct the in vivo cell morphodynamics, a main challenge of the post-genomic era.","Machine learning,
Embryo,
Drugs,
Learning systems,
Cancer,
Animal structures,
Microscopy,
Phase detection,
Conferences,
Biological systems"
Performance of Machine Learning Techniques in Protein Fold Recognition Problem,"In protein fold recognition problem an effort is made to assign a fold to given proteins, this is of practical importance and has diverse application in the field of bioinformatics such as the discovery of new drugs, the individual implication of amino acid in a protein and bringing improvement in a specific protein function. In this paper, we have studied various machine learning techniques for protein fold recognition problem, and compared Support Vector Machine (SVM) with Radial Basis Function (RBF) kernel and Multilayer Perceptron (MLP) on a number of measures like the recognition accuracy of protein fold, the 10-fold cross validation accuracies and Kappa statistics. These techniques are applied to the well known Structural Classification of Proteins (SCOP) dataset in extensive experimentations. In this study Multilayer Perceptron (MLP) shows better accuracy on single protein feature (C, S, H, P, V, Z) of the SCOP dataset as compared to Support Vector Machine (SVM). A plausible reason of the better performance of MLP is that it uses all the available data for classification where as the SVM model cannot exploit all the available data.","Machine learning,
Proteins,
Support vector machines,
Support vector machine classification,
Multilayer perceptrons,
Bioinformatics,
Drugs,
Amino acids,
Kernel,
Statistics"
Determination of Water Content in Automobile Lubricant Using Near-Infrared Spectroscopy Improved by Machine Learning Analysis,"The main objective of this paper is to determine the water content of automobile lubricant based on the near-infrared (NIR) spectra collected and to observe whether NIR spectroscopy could be used for predicting water content. Least square support vector machine (LS-SVM), back-propagation neural networks (BPNN) and Gaussian processes regression (GPR) were employed to develop prediction models. There were 150 samples for training set and test set, 6 inputs for one sample obtained by principle component analysis (PCA). LS-SVM models were developed with a grid search technique and RBF kernel function. The Levenberg-Marquardt algorithm was employed to optimize back-propagation neural network (BPNN) and models with 5 and 6 neurons in hidden layer were developed, respectively. The BPNN model with 5 neurons in hidden layer outperformed the one with 6 neurons. Three GPR models were built based on full data points (full GPR), subset of regressors (SR GPR) and subset of datasets (SD GPR), respectively, with Squared exponential (SE) covariance function. The full GPR outperformed SR GPR and SD GPR.The overall results indicted that the Gaussian processes model outperformed LS-SVM and BPNN model. GPR was an effective way for the regress prediction. NIR spectroscopy combined with PCA and GPR had the capability to determine the water content of automobile lubricant with high accuracy.","Automobiles,
Lubricants,
Spectroscopy,
Machine learning,
Ground penetrating radar,
Neurons,
Neural networks,
Gaussian processes,
Principal component analysis,
Strontium"
A zero-norm feature selection method for improving the performance of the one-class machine learning for microRNA target detection,"The application of one-class machine learning is gaining attention in the computational biology community. Different studies have described the use of two-class machine learning to predict microRNAs (miRNAs) gene target. Most of these methods require the generation of an artificial negative class. However, designation of the negative class can be problematic and if it is not properly done can affect the performance of the classifier dramatically and/or yield a biased estimate of performance. We present study using one-class machine learning for miRNA target discovery and compare one-class to two-class approaches using a zero-norm for feature selection. The usage of this simple feature selection cause an improving of the one-class results, where in some cases reaches the performance of the two-class approach.","Machine learning,
Object detection,
Sequences,
Machine learning algorithms,
RNA,
Humans,
Computer science,
Degradation,
Genomics,
Bioinformatics"
Machine learning techniques to diagnose breast cancer,"Machine learning is a branch of artificial intelligence that employs a variety of statistical, probabilistic and optimization techniques that allows computers to “learn” from past examples and to detect hard-to-discern patterns from large, noisy or complex data sets. As a result, machine learning is frequently used in cancer diagnosis and detection. In this paper, support vector machines, K-nearest neighbours and probabilistic neural networks classifiers are combined with signal-to-noise ratio feature ranking, sequential forward selection-based feature selection and principal component analysis feature extraction to distinguish between the benign and malignant tumours of breast. The best overall accuracy for breast cancer diagnosis is achieved equal to 98.80% and 96.33% respectively using support vector machines classifier models against two widely used breast cancer benchmark datasets.","Machine learning,
Breast cancer,
Support vector machines,
Support vector machine classification,
Artificial intelligence,
Cancer detection,
Neural networks,
Signal to noise ratio,
Principal component analysis,
Feature extraction"
North Atlantic Right Whale acoustic signal processing: Part I. comparison of machine learning recognition algorithms,"This paper compares three different approaches currently used in recognizing contact calls made from the North Atlantic Right Whale (NRW), Eubalaena glacialis. We present two new approaches consisting of machine learning algorithms based on artificial neural networks (NET) and the classification and regression tree classifiers (CART), and compare their performance with earlier work that employs multi-Stage feature vector testing (FVT) approach. A combined total of over 100,000 noise and NRW up-call events were used in the study. Calls were primarily recorded from two areas, Cape Cod Bay and Great South Channel. Of the three classifiers, the CART had the highest assignment rates, overall 86.45% with highest false positive rates (≪100 per hour). The FVT Method had exceptionally low false positive rates, with ≪50 per hour. However, it had an overall assignment rate less than the NET. The CART had statistically the same false positive rate as the NET with the highest assignment rates, 2.2% higher than the NET and 11.75% greater than the FVT Method. Details of the results are shown and extensions to the research are discussed.","Whales,
Acoustic signal processing,
Machine learning,
Machine learning algorithms,
Signal processing algorithms,
Acoustic signal detection,
Acoustic noise,
Neural networks,
Noise level,
Classification tree analysis"
Interaction prediction of PDZ domains using a machine learning approach,"Protein interaction domains play crucial roles in many complex cellular pathways. PDZ domains are one of the most common protein interaction domains. Prediction of binding specificity of PDZ domains by a computational manner could eliminate unnecessary, time-consuming experiments. In this study, interactions of PDZ domains are predicted by using a machine learning approach in which only primary sequences of PDZ domains and peptides are used. In order to encode feature vectors for each interaction, trigram frequencies of primary sequences of PDZ domains and corresponding peptides are calculated. After construction of numerical interaction dataset, we compared different classifiers and ended up with Random Forest (RF) algorithm which gave the top performance. We obtained very high prediction accuracy (91.4%) for binary interaction prediction which outperforms all previous similar methods.","Machine learning,
Sequences,
Peptides,
Protein engineering,
Radio frequency,
Amino acids,
Predictive models,
Chemical engineering,
Biological information theory,
Biology computing"
Machine Learning Methods and Asymmetric Cost Function to Estimate Execution Effort of Software Testing,"Planning and scheduling of testing activities play an important role for any independent test team that performs tests for different software systems, developed by different development teams. This work studies the application of machine learning tools and variable selection tools to solve the problem of estimating the execution effort of functional tests. An analysis of the test execution process is developed and experiments are performed on two real databases. The main contributions of this paper are the approach of selecting the significant variables for database synthesis and the use of an artificial neural network trained with an asymmetric cost function.","Learning systems,
Cost function,
Software testing,
System testing,
Performance evaluation,
Databases,
Software systems,
Application software,
Machine learning,
Input variables"
Research Intrusion Detection Techniques from the Perspective of Machine Learning,"With the rapid development of the Internet services and the fast increasing of intrusion problems, the traditional intrusion detection methods cannot work well with the more and more complicated intrusions. So introducing machine learning into intrusion detection systems to improve the performance has become one of the major concerns in the research of intrusion detection. Intrusion detection systems were proposed to complement prevention-based security measures. In this paper, we first introduces the basic structure of the intrusion detection system, then analysis intrusion Detection Techniques Based on Machine Learning Method, including the Bayesian based method, the neural network based method, the data mining based method and the SVM based method.","Intrusion detection,
Machine learning,
Data security,
Web and internet services,
Data mining,
Support vector machines,
Information technology,
Learning systems,
Bayesian methods,
Neural networks"
FSVM-CIL: Fuzzy Support Vector Machines for Class Imbalance Learning,"Support vector machines (SVMs) is a popular machine learning technique, which works effectively with balanced datasets. However, when it comes to imbalanced datasets, SVMs produce suboptimal classification models. On the other hand, the SVM algorithm is sensitive to outliers and noise present in the datasets. Therefore, although the existing class imbalance learning (CIL) methods can make SVMs less sensitive to class imbalance, they can still suffer from the problem of outliers and noise. Fuzzy SVMs (FSVMs) is a variant of the SVM algorithm, which has been proposed to handle the problem of outliers and noise. In FSVMs, training examples are assigned different fuzzy-membership values based on their importance, and these membership values are incorporated into the SVM learning algorithm to make it less sensitive to outliers and noise. However, like the normal SVM algorithm, FSVMs can also suffer from the problem of class imbalance. In this paper, we present a method to improve FSVMs for CIL (called FSVM-CIL), which can be used to handle the class imbalance problem in the presence of outliers and noise. We thoroughly evaluated the proposed FSVM-CIL method on ten real-world imbalanced datasets and compared its performance with five existing CIL methods, which are available for normal SVM training. Based on the overall results, we can conclude that the proposed FSVM-CIL method is a very effective method for CIL, especially in the presence of outliers and noise in datasets.","Support vector machines,
Machine learning,
Support vector machine classification,
Machine learning algorithms,
Solids,
Noise reduction"
RF Specification Test Compaction Using Learning Machines,"We present a machine learning approach to the problem of RF specification test compaction. The proposed compaction flow relies on a multi-objective genetic algorithm, which searches in the power-set of specification tests to select appropriate subsets, and a classifier, which makes pass/fail decisions based solely on these subsets. The method is demonstrated on production test data from an RF device fabricated by IBM. The results indicate that machine learning can identify intricate correlations between specification tests, which allows us to infer the outcome of all tests from a subset of tests. Thereby, the number of tests that need to be explicitly carried out and the corresponding cost are reduced significantly without adversely impacting test accuracy.","Radio frequency,
Compaction,
Machine learning,
Circuit testing,
Costs,
Automatic testing,
Predictive models,
Genetic algorithms,
Production,
Radiofrequency identification"
Model-Based Testing Using Symbolic Animation and Machine Learning,"We present in this paper a technique based on symbolic animation of models that aims at producing model-based tests. In order to guide the animation of the model, we rely on the use of a deterministic finite automaton (DFA) of the model that is built using a well-known machine learning algorithm, that considers a complex model as a black-box component, whose behavior is inferred. Since the DFA obtained in this way may be an over-approximation and, thus, admit traces that were not admitted on the original model, this abstraction is refined using counter-examples made of unfeasible traces. The computation of counter-examples is performed using a systematic coverage of the DFA states and transitions, producing test sequences that are replayed on the model, providing either test cases for offline testing, or counter-examples that aim at refining the abstraction.","Animation,
Machine learning,
System testing,
Doped fiber amplifiers,
Machine learning algorithms,
Software testing,
State-space methods,
Learning automata,
Context modeling,
Refining"
Relevance Vector Machine Learning for Neonate Pain Intensity Assessment Using Digital Imaging,"Pain assessment in patients who are unable to verbally communicate is a challenging problem. The fundamental limitations in pain assessment in neonates stem from subjective assessment criteria, rather than quantifiable and measurable data. This often results in poor quality and inconsistent treatment of patient pain management. Recent advancements in pattern recognition techniques using relevance vector machine (RVM) learning techniques can assist medical staff in assessing pain by constantly monitoring the patient and providing the clinician with quantifiable data for pain management. The RVM classification technique is a Bayesian extension of the support vector machine (SVM) algorithm, which achieves comparable performance to SVM while providing posterior probabilities for class memberships and a sparser model. If classes represent “pure” facial expressions (i.e., extreme expressions that an observer can identify with a high degree of confidence), then the posterior probability of the membership of some intermediate facial expression to a class can provide an estimate of the intensity of such an expression. In this paper, we use the RVM classification technique to distinguish pain from nonpain in neonates as well as assess their pain intensity levels. We also correlate our results with the pain intensity assessed by expert and nonexpert human examiners.","Machine learning,
Pediatrics,
Pain,
Digital images,
Support vector machines,
Support vector machine classification,
Medical treatment,
Quality management,
Pattern recognition,
Biomedical imaging"
Classification of Alzheimer's Disease and Parkinson's Disease by Using Machine Learning and Neural Network Methods,"Data mining is a fast evolving technology, is being adopted in biomedical sciences and research. Data mining in medicine is an emerging field of high importance for providing prognosis and a deeper understanding of the classification of neurodegenerative diseases. Given a data set of consists of 487 patients records collected from ADRC, USA. Around eight hundred and ninety patients were recruited to ADRC and diagnosed for AD (65%) and PD (40%), according to the established criteria. In our study we concentrated particularly on the major risk factors which are responsible for Alzheimer’s disease and Parkinson’s disease. This paper proposes a new model for the classification of Alzheimer’s disease and Parkinson’s disease by considering the most influencing risk factors. The main focus was on the selection of most influencing risk factors for both AD and PD using various attribute evaluation scheme with ranker search method. Different models for the classification of AD and PD using various classification techniques such as Neural Networks (NN) and Machine Learning (ML) methods were also developed. It was found that some specific genetic factors, diabetes, age and smoking were the strongest risk factors for Alzheimer’s disease. Similarly, for the classification of Parkinson’s disease, the risk factors such as stroke, diabetes, genes and age were the vital factors.","Alzheimer's disease,
Parkinson's disease,
Machine learning,
Neural networks,
Data mining,
Diabetes,
Medical diagnostic imaging,
Recruitment,
Search methods,
Genetics"
Genetic Algorithm and Machine Learning Based Void Fraction Measurement of Two-Phase Flow,"Machine learning and Genetic Algorithm based void fraction measurement method is provided in this paper. Because there are some relationships between the void fraction and the differential pressure (DP) signal acquired near the pipe wall when the two phases are flowing along the pipeline, it is possible to measure the void fraction according to the DP signal. However, the expression between the void fraction and the DP signal is complicated and is not easy to be developed because of the complexity of the characteristics of two-phase flow. In this paper, SVM is adopted to investigate the relationship between the void fraction and the DP signal. GA is used to estimate the parameters involved in SVM. The experimental results show that machine learning and genetic algorithm based void fraction measurement method provided in this paper is available.","Genetic algorithms,
Machine learning,
Fluid flow measurement,
Support vector machines,
Phase measurement,
Volume measurement,
Fluctuations,
Instruments,
Pipelines,
Risk management"
Hybrid Machine Learning Approach in Data Mining,In this paper we discuss various machine learning approaches used in mining of data. Further we distinguish between symbolic and sub-symbolic data mining methods. We also attempt to propose a hybrid method with the combination of Artificial Neural Network (ANN) and Cased Based Reasoning (CBR) in mining of data.,"Machine learning,
Data mining,
Artificial neural networks,
Data analysis,
Databases,
Information systems,
Learning systems,
Classification tree analysis,
Data visualization,
Hospitals"
Extreme Learning Machine for financial distress prediction for listed company,"To overcome the shortages of the existing financial prediction models such as strict hypothesis, poor generalization ability, low prediction accuracy and low learning rate etc., a new early warning model of financial crisis have established for listed company using Extreme Learning Machine. From five dimensions of solvency, operating-ability, profitability, cash-ability and grow-ability, fifteen financial indexes were selected as the input variables; and the output variable was defined as whether the listed company had been special treated or not. The empirical analysis results show the training and validation accuracy of the model are 100% and 92% respectively, which concludes that learning and generalization abilities of this model are excellent, and which can meet the requirements of financial distress prediction for listed company.","Machine learning,
Predictive models,
Neural networks,
Feedforward neural networks,
Feedforward systems,
Multi-layer neural network,
Accuracy,
Input variables,
Algorithm design and analysis,
Electronic mail"
Fast Preliminary Evaluation of New Machine Learning Algorithms for Feasibility,"Traditionally, researchers compare the performance of new machine learning algorithms against those of locally executed simulations that serve as benchmarks. This process requires considerable time, computation resources, and expertise. In this paper, we present a method to quickly evaluate the performance feasibility of new algorithms – offering a preliminary study that either supports or opposes the need to conduct a full-scale traditional evaluation, and possibly saving valuable resources for researchers. The proposed method uses performance benchmarks obtained from results reported in the literature rather than local simulations. Furthermore, an alternate statistical technique is suggested for comparative performance analysis, since traditional statistical significance tests do not fit the problem well. We highlight the use of the proposed evaluation method in a study that compared a new algorithm against 47 other algorithms across 46 datasets.","Machine learning algorithms,
Machine learning,
Computer science,
Computational modeling,
Robustness,
Benchmark testing,
Performance analysis,
Algorithm design and analysis,
Statistical analysis"
Learning image similarity from Flickr groups using Stochastic Intersection Kernel MAchines,"Measuring image similarity is a central topic in computer vision. In this paper, we learn similarity from Flickr groups and use it to organize photos. Two images are similar if they are likely to belong to the same Flickr groups. Our approach is enabled by a fast Stochastic Intersection Kernel MAchine (SIKMA) training algorithm, which we propose. This proposed training method will be useful for many vision problems, as it can produce a classifier that is more accurate than a linear classifier, trained on tens of thousands of examples in two minutes. The experimental results show our approach performs better on image matching, retrieval, and classification than using conventional visual features.","Machine learning,
Stochastic processes,
Kernel,
Support vector machines,
Histograms,
Computer vision,
Support vector machine classification,
Large-scale systems,
Feedback,
Computer science"
A Machine Learning Approach for Identifying Expert Stakeholders,"Requirements gathering, analysis, and specification are humanintensive activities that rely upon finding and engaging a relevant set of informed stakeholders. In many projects initial requirements are captured through the use of wikis or forums, or through ini tial facetoface brainstorming meetings. In this paper we introduce a technique for analyzing stakeholders' contributions, extracting domain topics, and construct ing profiles which depict stakeholders' interests in each of the topics. Content and collaborative filtering techniques are then used to identify a diverse set of stakeholders for a given topic. The approach, which can be used to support requirements related activities throughout the software development lifecycle, is illus trated through an example of an Amazonlike student webportal.","Machine learning,
Programming,
Knowledge management,
Engineering management,
Systems engineering and theory,
Collaboration,
Filtering,
Open source software,
Software design,
Project management"
Fault diagnosis of analog circuits based on machine learning,"We discuss a fault diagnosis scheme for analog integrated circuits. Our approach is based on an assemblage of learning machines that are trained beforehand to guide us through diagnosis decisions. The central learning machine is a defect filter that distinguishes failing devices due to gross defects (hard faults) from failing devices due to excessive parametric deviations (soft faults). Thus, the defect filter is key in developing a unified hard/soft fault diagnosis approach. Two types of diagnosis can be carried out according to the decision of the defect filter: hard faults are diagnosed using a multi-class classifier, whereas soft faults are diagnosed using inverse regression functions. We show how this approach can be used to single out diagnostic scenarios in an RF low noise amplifier (LNA).","Fault diagnosis,
Analog circuits,
Machine learning,
Circuit faults,
Filters,
Analog integrated circuits,
Assembly,
Radio frequency,
Low-noise amplifiers,
Radiofrequency amplifiers"
The Application of Machine Learning Algorithms to the Analysis of Electromyographic Patterns From Arthritic Patients,"The main aim of our study was to investigate the possibility of applying machine learning techniques to the analysis of electromyographic patterns (EMG) collected from arthritic patients during gait. The EMG recordings were collected from the lower limbs of patients with arthritis and compared with those of healthy subjects (CO) with no musculoskeletal disorder. The study involved subjects suffering from two forms of arthritis, viz, rheumatoid arthritis (RA) and hip osteoarthritis (OA). The analysis of the data was plagued by two problems which frequently render the analysis of this type of data extremely difficult. One was the small number of human subjects that could be included in the investigation based on the terms specified in the inclusion and exclusion criteria for the study. The other was the high intra- and inter-subject variability present in EMG data. We identified some of the muscles differently employed by the arthritic patients by using machine learning techniques to classify the two groups and then identified the muscles that were critical for the classification. For the classification we employed least-squares kernel (LSK) algorithms, neural network algorithms like the Kohonen self organizing map, learning vector quantification and the multilayer perceptron. Finally we also tested the more classical technique of linear discriminant analysis (LDA). The performance of the different algorithms was compared. The LSK algorithm showed the highest capacity for classification. Our study demonstrates that the newly developed LSK algorithm is adept for the treatment of biological data. The muscles that were most important for distinguishing the RA from the CO subjects were the soleus and biceps femoris. For separating the OA and CO subjects however, it was the gluteus medialis muscle. Our study demonstrates how classification with EMG data can be used in the clinical setting. While such procedures are unnecessary for the diagnosis of the type of arthritis present, an understanding of the muscles which are responsible for the classification can help to better identify targets for rehabilitative measures.","Machine learning algorithms,
Pattern analysis,
Algorithm design and analysis,
Muscles,
Electromyography,
Arthritis,
Machine learning,
Linear discriminant analysis,
Musculoskeletal system,
Hip"
An Anomaly Detection Scheme Based on Machine Learning for WSN,"Security is one of the most important research issues in wireless sensor network (WSN). A Machine Learning (ML) based anomaly detection scheme is proposed, where Bayesian classification algorithm is used to detect anomalous nodes. By the tool NS2, a small number of samples are given and learned, and intrusion detection rules are built, network attack traffic is generated and simulated. And based on this, its detection rate, average detection rate, false positive rate and average false positive rate are evaluated. Experimental results demonstrate that the scheme achieves higher accuracy rate of detection and lower false positive rate than the current important intrusion detection schemes of WSN.",
QELAR: A Machine-Learning-Based Adaptive Routing Protocol for Energy-Efficient and Lifetime-Extended Underwater Sensor Networks,"Underwater sensor network (UWSN) has emerged in recent years as a promising networking technique for various aquatic applications. Due to specific characteristics of UWSNs, such as high latency, low bandwidth, and high energy consumption, it is challenging to build networking protocols for UWSNs. In this paper, we focus on addressing the routing issue in UWSNs. We propose an adaptive, energy-efficient, and lifetime-aware routing protocol based on reinforcement learning, QELAR. Our protocol assumes generic MAC protocols and aims at prolonging the lifetime of networks by making residual energy of sensor nodes more evenly distributed. The residual energy of each node as well as the energy distribution among a group of nodes is factored in throughout the routing process to calculate the reward function, which aids in selecting the adequate forwarders for packets. We have performed extensive simulations of the proposed protocol on the Aqua-sim platform and compared with one existing routing protocol (VBF) in terms of packet delivery rate, energy efficiency, latency, and lifetime. The results show that QELAR yields 20 percent longer lifetime on average than VBF.","Routing protocols,
Energy efficiency,
Acoustic sensors,
Sensor phenomena and characterization,
Media Access Protocol,
Monitoring,
Delay,
Bandwidth,
Energy consumption,
Underwater acoustics"
Applying machine learning to automated information graphics generation,"Information graphics, which include graphs, charts, and diagrams, are visual illustrations that facilitate human comprehension of information. In this paper, we present our work on applying machine learning to the automated generation of information graphics. Our approach is embodied in a hybrid graphics generation systemIMPROVISE*, which uses both rule-based and example-based generation engines. We discuss the use of machine learning to support such systems from three aspects. First, we introduce an object-oriented, integrated hierarchical feature representation for annotating information graphics. Second, we describe how to use decision-tree learning to automatically extract design rules from a set of annotated graphic examples. Our results demonstrate that we can acquire, with quantitative confidence, concise rules that are difficult to obtain in handcrafted rules. Third, we present a case-based learning method to retrieve matched graphic examples based on user requests. Specifically, we use a semantics-based, quantitative visual similarity measuring algorithm to retrieve the top-k matched examples from a visual database. To help users browse a graphics database and understand the inherent relations among different examples, we combine our similarity measuring model with a hierarchical clustering algorithm to dynamically organize our graphic examples based on user interests.",
Some studies in machine learning using the game of checkers,"Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.",
Some Studies in Machine Learning Using the Game of Checkers,"Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.",
Machine learning in a multimedia document retrieval framework,"The Pen Technologies group at IBM Research has recently been investigating methods for retrieving handwritten documents based on user queries. This paper investigates the use of typed and handwritten queries to retrieve relevant handwritten documents. The IBM handwriting recognition engine was used to generate N-best lists for the words in each of 108 short documents. These N-best lists are concise statistical representations of the handwritten words. These statistical representations enable the retrieval methods to be robust when there are machine transcription errors, allowing retrieval of documents that would be missed by a traditional transcription-based retrieval system. Our experimental results demonstrate that significant improvements in retrieval performance can be achieved compared to standard keyword text searching of machine-transcribed documents. We have developed a software architecture for a multimedia document retrieval framework into which machine learning algorithms for feature extraction and matching may be easily integrated. The framework provides a “plug-and-play” mechanism for the integration of new media types, new feature extraction methods, and new document types.",
Machine learning methods for transcription data integration,"Gene expression is modulated by transcription factors (TFs), which are proteins that generally bind to DNA adjacent to coding regions and initiate transcription. Each target gene can be regulated by more than one TF, and each TF can regulate many targets. For a complete molecular understanding of transcriptional regulation, researchers must first associate each TF with the set of genes that it regulates. Here we present a summary of completed work on the ability to associate 104 TFs with their binding sites using support vector machines (SVMs), which are classification algorithms based in statistical learning theory. We use several types of genomic datasets to train classifiers in order to predict TF binding in the yeast genome. We consider motif matches, subsequence counts, motif conservation, functional annotation, and expression profiles. A simple weighting scheme varies the contribution of each type of genomic data when building a final SVM classifier, which we evaluate using known binding sites published in the literature and in online databases. The SVM algorithm works best when all datasets are combined, producing 73% coverage of known interactions, with a prediction accuracy of almost 0.9. We discuss new ideas and preliminary work for improving SVM classification of biological data.",
Arthur Samuel: Pioneer in Machine Learning,"Professor Emeritus Arthur L. Samuel died July 29, 1990, in Stanford, california at the age of 89. He was a pioneer of artificial intelligence research whose life spanned a broad personal and scientific history.",
"Prostate-cancer imaging using machine-learning classifiers: Potential value for guiding biopsies, targeting therapy, and monitoring treatment","Prostate cancer (PCa) remains a major health concern in many countries. However, it cannot be imaged reliably by any commonly used imaging modality. Therefore, needle biopsies and treatments cannot be targeted to suspicious regions. Our objective is to develop and test an ultrasonic method based on spectrum analysis of radio-frequency (RF) ultrasound echo signals and on classification using current machine-learning tools for reliably imaging PCa and thereby guiding biopsies, targeting therapy, and eventually, monitoring treatment of PCa. RF data were acquired in the biopsy plane of 617 prostate biopsy cores obtained from 64 suspected prostate-cancer (PCa) patients. For each patient, clinical data such as PSA level also were recorded. A level of suspicion (LOS) was assigned based on the conventional image. Spectral computations were performed on acquired RF data in a region of interest that spatially matched the tissue-sampling location. Four non-linear classifiers were trained from these data using biopsy results as the gold standard: multi-layer-perceptron artificial neural networks (ANNs), logitboost algorithms (LBAs), support-vector machines (SVMs), and stacked, restricted Boltzmann machines (S-RBMs). Cross-validation methods were employed to obtain tissue-category scores. Areas under ROC curves (AUCs) were used to assess classifier performance in comparison with LOS-based performance. AUCs for the ANN, LBA, SVM, and RBM respectively were 0.84 ± 0.02, 0.87 ± 0.04, 0.89 ± 0.04, and 0.91 ± 0.04. In comparison, the LOS-based AUC was 0.64 ± 0.03. Tissue-type images (TTIs) based on these methods revealed cancerous foci that subsequently were identified histologically, but were undetected prior to prostatectomy pathology. The ultrasonic imaging methods described here show significant potential for achieving needed reliability. A clinically significant beneficial reduction in false-negative biopsy procedures would be possible if TTIs were used to guide biopsies. Benefits also would result from using TTIs to target focal treatment and reduce toxic side effects. Potentially, TTIs also could be used to assess tissue changes over time for active surveillance and therapy monitoring.","Biopsy,
Medical treatment,
Condition monitoring,
Principal component analysis,
Ultrasonic imaging,
Radio frequency,
Artificial neural networks,
Prostate cancer,
Needles,
Testing"
Using Domain Top-page Similarity Feature in Machine Learning-Based Web Phishing Detection,"This paper presents a study on using a concept feature to detect web phishing problem. Following the features introduced in Carnegie Mellon Anti-phishing and Network Analysis Tool (CANTINA), we applied additional domain top-page similarity feature to a machine learning based phishing detection system. We preliminarily experimented with a small set of 200 web data, consisting of 100 phishing webs and another 100 non-phishing webs. The evaluation result in terms of f-measure was up to 0.9250, with 7.50% of error rate.","Machine learning,
Computer crime,
Computer vision,
Web pages,
Uniform resource locators,
Support vector machines,
Error analysis,
Data mining,
Knowledge engineering,
Neural networks"
A multimedia semantic analysis SoC (SASoC) with machine-learning engine,"Advances in semiconductors and developments in machine learning [1] have led to versatile multimedia applications with semantic processing abilities. Real-time applications, such as face detection, facial-expression recognition, scene analysis [2] and object recognition [3], have become indispensable functionality for Consumer Electronic (CE) products. To deal with complicated video-processing algorithms for multimedia content analysis, many powerful processors have been reported [2–5]. Although these processors can speed up video-processing tasks with massively parallel processing elements, they only focus on the feature-extraction parts, and there is no specialized hardware to support different kinds of advanced machine-learning algorithms, which require extensive computations. In this paper, a Semantic Analysis SoC (SASoC) that accelerates video processing and machine learning simultaneously, is developed to meet the demands of the near future.","Engines,
Frequency,
Clocks,
Energy consumption,
Streaming media,
Image retrieval,
Image analysis,
Feature extraction,
Face detection,
Pipelines"
Machine learning of shooting technique for controlling a robot camera,"We propose a machine learning method of a TV cameraman's shooting technique with a neural network so that a robot camera can automatically shoot like an experienced cameraman. An experiment using a simulator that we developed shows that, by using our method, the cameraman's shooting technique can be quickly and easily embodied in the control system of the robot camera. Moreover, we derive guidelines for performing machine learning for shooting actual TV programs by analyzing the relationship of the learning data and the learning parameter with the performance of the learned neural network.","Machine learning,
Robot control,
Robot vision systems,
Cameras,
Robotics and automation,
Automatic control,
TV,
Neural networks,
Learning systems,
Control system synthesis"
Low complexity H.264 video encoder design using machine learning techniques,"H.264/AVC encoder complexity is mainly due to variable size in Intra and Inter frames. This makes H.264/AVC very difficult to implement, especially for real time applications and mobile devices. The current technological challenge is to conserve the compression capacity and quality that H.264 offers but reduce the encoding time and, therefore, the processing complexity. This paper applies machine learning technique for video encoding mode decisions and investigates ways to improve the process of generating more general low complexity H.264/AVC video encoders. The proposed H.264 encoding method decreases the complexity in the mode decision inside the Inter frames. Results show, in a 67.81% average reduction of complexity and 0.2 average decreases in PSNR and an average bit rate increase of 0.04% for different kinds of videos and formats.","Machine learning,
Encoding,
Automatic voltage control,
Video coding,
IEC standards,
ISO standards,
Decision trees,
Computer science,
Video compression,
Code standards"
Source detection of atmospheric releases using symbolic machine learning classification and remote sensing,"This paper introduces the National Polar-orbiting Operational Environmental Satellite System (NPOESS) and its use for the identification of the source of atmospheric pollutants. NPOESS is the next generation satellite program, and can be used for the source detection of atmospheric pollutants. The iterative methodology proposed herein uses a combination of ground measurements, atmospheric models, machine learning and remote sensing to identify the characteristics of an unknown atmospheric emission.","Machine learning,
Remote sensing,
Sensor phenomena and characterization,
Pollution measurement,
Atmospheric modeling,
Satellites,
Data analysis,
Iterative methods,
Atmospheric measurements,
Chemical industry"
An unsupervised image segmentation algorithm based on the machine learning of appropriate features,"This paper proposes a new approach to the feature based unsupervised image segmentation. The difficulty with the conventional unsupervised segmentation lies in finding appropriate features that discriminate a meaningful region from the others. In this paper, the appropriate features are automatically learnt by machine learning with boosting scheme. At the initial step, the image is split into many small regions (blocks at first) and strong classifiers for every region, which discriminate the region from the others, are found by AdaBoosting. Each strong classifier so obtained is the weighted sum of several popular weak classifiers (features), which best describes the coherence of the region and thus well discriminates the region from the others. The output of this classifier is used in designing the energy function for the labeling, in the form of Conditional Random Fields (CRFs). Minimization of the energy function produces the labeling result which reflects the property learnt by the classifier. For the labeling result, the machine learning is again performed and the process iterates until some conditions are met. Experimental results show that the proposed method provides competitive result compared to the conventional feature based methods.","Image segmentation,
Machine learning algorithms,
Machine learning,
Labeling,
Clustering algorithms,
Partitioning algorithms,
Image analysis,
Spatial coherence,
Boosting,
Layout"
Using Machine Learning Techniques to Explore 1H-MRS Data of Brain Tumors,"Machine learning is a powerful paradigm to analyze Proton Magnetic Resonance Spectroscopy 1H-MRS spectral data for the classification of brain tumor pathologies. An important characteristic of this task is the high dimensionality of the involved data sets. In this work we apply filter feature selection methods on three types of 1H-MRS spectral data: long echo time, short echo time and an ad hoc combination of both. The experimental findings show that feature selection permits to drastically reduce the dimension, offering at the same time very attractive solutions both in terms of prediction accuracy and the ability to interpret the involved spectral frequencies. A linear dimensionality reduction technique that preserves the class discrimination capabilities is additionally used for visualization of the selected frequencies.","Machine learning,
Neoplasms,
Frequency,
Magnetic analysis,
Protons,
Magnetic resonance,
Spectroscopy,
Pathology,
Magnetic separation,
Filters"
Genetic Machine Learning algorithms in the optimization of communication efficiency in Wireless Sensor Networks,"Wireless Sensor Networks (WSN) can be used to monitor hazardous and inaccessible areas. In these situations, the power supply (e.g. battery) in each node can not be easily replaced. One solution is to deploy a large number of sensor nodes, since the lifetime and dependability of the network can be increased through cooperation among nodes. In addition to energy consumption, applications for WSN may also have other concerns, such as, meeting deadlines and maximizing the quality of information. In this paper, we present a Genetic Machine Learning algorithm aimed at applications that make use of trade-offs between different metrics. Simulations were performed on random topologies assuming different levels of faults. Our approach showed a significant improvement when compared with the use of IEEE 802.15.4 protocol.","Genetics,
Machine learning algorithms,
Wireless sensor networks,
Manufacturing automation,
Informatics"
Feature Selection using an SVM learning machine,In this paper we suggest an approach to select features for the support vector machines (SVM). Feature selection is efficient in searching the most descriptive features which would contribute in increasing the effectiveness of the classifier algorithm. The process described here consists in backward elimination strategy based on the criterion of the rate of misclassification. We used the tabu algorithm to guide the search of the optimal set of features; each set of features is assessed according to its goodness of fit. This procedure is exploited in the regulation of urban transport network systems. It was first applied in a binary case and then it was extended to the multiclass case thanks to the MSVM technique: binary tree.,"Support vector machines,
Machine learning,
Support vector machine classification,
Circuits and systems,
Binary trees,
Multidimensional systems,
Image processing,
Bioinformatics,
Engines"
Machine Learning Approach for Prediction of Human Mitochondrial Proteins,,
Incorporating Prior Information in Machine Learning by Creating Virtual Examples,,
A machine learning algorithm for GPR sub-surface prospection,"The paper presents a novel approach for the (semi-) automatic extraction of sub-surface layers' properties from GPR data. The methodology solves the inverse scattering problem by means of artificial neural networks which are able to map proper features derived from the electromagnetic signal onto the dielectric permittivity and thickness of the layer which has backscattered the radiation. The whole procedure is first described and then tested over a set of simulated scenarios and their corresponding GPR traces, showing high reconstruction accuracies and denoting the opportunity of a wide range of applicability.","Machine learning algorithms,
Ground penetrating radar,
Electromagnetic radiation,
Data mining,
Inverse problems,
Artificial neural networks,
Electromagnetic scattering,
Dielectrics,
Permittivity,
Testing"
Bio-inspired machine learning in microarray gene selection and cancer classification,"Microarray technology today has the ability of having the whole genome spotted on a single chip. It allows the biologist to inspect thousands of gene activities simultaneously. Machine learning approaches are suited and used to discovering the complex relationships between genes under controlled experimental conditions and classify microarray data by identifying a subset of informative genes embedded in a large data set that involves multiple classes and is infected with the high dimensionality noise. In this paper, a hybrid system integrates genetic algorithms and decision tree is proposed for genes expression analysis and prediction to their functionality for cancer classification. The learning capacity of decision trees used in the base learning systems is boosted by feature selection method. Experiments present preliminary results to demonstrate the capability of hybrid system to mine accurate classification rules for classifying prediction in comparable to traditional machine learning algorithms.","Machine learning,
Cancer,
Decision trees,
Genomics,
Bioinformatics,
Genetic algorithms,
Genetic expression,
Algorithm design and analysis,
Classification tree analysis,
Learning systems"
Deformation prediction of foundation pit using Gaussian Process machine learning,"Prediction of deformation of foundation pit by means of conventional method such as mechanics analysis or numerical method often has a large error because the deformation process of foundation pit is a highly complicated nonlinear evolution process. A novel method based on Gaussian Process (GP) machine learning is proposed for solving the problem of deformation prediction of foundation pit. GP is a power tool for solving high dimensional, nonlinear, small sample problems. A GP model for deformation prediction of foundation pit is proposed firstly. The nonlinear reflective relationship between deformations in deep foundation pit and their effective factors is built easily by learning the historical knowledge using the GP model. Furthermore, the other GP model for displacement time series prediction of foundation pit is also proposed. The results of cases studies show two models are feasible. The models both have advantages in low calculation cost and higher precision comparing to the traditional methods. The results of studies also show that GP are more suitable for solving small samples prediction problems comparing to artificial neural networks and Grey method.","Gaussian processes,
Machine learning,
Deformable models,
Predictive models,
Artificial neural networks,
Support vector machines,
Learning systems,
Safety,
Computational intelligence,
Computer industry"
Dataset threshold for the performance estimators in supervised machine learning experiments,"The establishment of dataset threshold is one among the first steps when comparing the performance of machine learning algorithms. It involves the use of different datasets with different sample sizes in relation to the number of attributes and the number of instances available in the dataset. Currently, there is no limit which has been set for those who are unfamiliar with machine learning experiments on the categorisation of these datasets, as either small or large, based on the two factors. In this paper we perform experiments in order to establish dataset threshold. The established dataset threshold will help unfamiliar supervised machine learning experimenters to categorize datasets based on the number of instances and attributes and then choose the appropriate performance estimation method. The experiments will involve the use of four different datasets from UCI machine learning repository and two performance estimators. The performance of the methods will be measured using f1-score.","Machine learning,
Machine learning algorithms,
Knowledge acquisition,
Decision trees,
Support vector machines,
Radio frequency,
Performance evaluation,
Error analysis,
Communications technology,
Context"
A distributed machine learning framework,"A distributed online learning framework for support vector machines (SVMs) is presented and analyzed. First, the generic binary classification problem is decomposed into multiple relaxed subproblems. Then, each of them is solved iteratively through parallel update algorithms with minimal communication overhead. This computation can be performed by individual processing units, such as separate computers or processor cores, in parallel and possibly having access to only a subset of the data. Convergence properties of continuous- and discrete-time variants of the proposed parallel update schemes are studied. A sufficient condition is derived under which synchronous and asynchronous gradient algorithms converge to the approximate solution. Subsequently, a class of stochastic update algorithms, which may arise due to distortions in the information flow between units, is shown to be globally stable under similar sufficient conditions. Active set methods are utilized to decrease communication and computational overhead. A numerical example comparing centralized and distributed learning schemes indicates favorable properties of the proposed framework such as configurability and fast convergence.","Machine learning,
Support vector machines,
Sufficient conditions,
Iterative algorithms,
Concurrent computing,
Stochastic processes,
Multicore processing,
Support vector machine classification,
Wireless sensor networks,
Multiprocessing systems"
A neural network approach for least squares support vector machines learning,"A new neural network for least squares support vector machines (LS-SVM) learning, which combines LS-SVM with recurrent neural networks, is proposed based on the learning network of standard SVM. It is obtained using Lagrange multipliers directly which eliminates the nonlinear parts of the standard SVM learning network. The proposed network can be used for classification and regression application, whose topology easily adapts to the implementation of analog circuits implementation. The simulation experiment results based on Simulink and Spice illustrate the effectiveness of the proposed neural network.","Neural networks,
Least squares methods,
Support vector machines,
Machine learning,
Support vector machine classification,
Recurrent neural networks,
Lagrangian functions,
Network topology,
Circuit topology,
Analog circuits"
Impact of machine learning algorithms on analysis of stream ciphers,Stream ciphers are widely used for information security. The keystream produced by a cipher must be unpredictable. Attacks on stream ciphers typically exploit some underlying patterns existing in the keystream. The objective of this paper is to develop such an attack with the help of machine learning algorithms. The Linear Feedback Shift Register (LFSR) has been solved for several test cases using machine learning algorithms. We also study some variants of LFSR and Geffe Generator and propose a model for predicting the future bits of a keystream generator. The results for Geffe Generator using this model have been presented. However the approach did not yield encouraging results when confronted with the keystream generators of the eSTREAM project.,"Machine learning algorithms,
Algorithm design and analysis,
Computer science,
Information analysis,
Linear feedback shift registers,
Polynomials,
Cryptography,
Clocks,
Information security,
Testing"
On improving the conditioning of extreme learning machine: A linear case,"Recently Extreme Learning Machine (ELM) has been attracting attentions for its simple and fast training algorithm, which randomly selects input weights. Given sufficient hidden neurons, ELM has a comparable performance for a wide range of regression and classification problems. However, in this paper we argue that random input weight selection may lead to an ill-conditioned problem, for which solutions will be numerically unstable. In order to improve the conditioning of ELM, we propose an input weight selection algorithm for an ELM with linear hidden neurons. Experiment results show that by applying the proposed algorithm accuracy is maintained while condition is perfectly stable.","Machine learning,
Neurons,
Iterative algorithms,
Neural networks,
Numerical stability,
Joining processes,
Australia,
Linear algebra,
Feedforward neural networks,
Root mean square"
A Machine Learning Approach to the Stabilization of Emotional Dynamics in Emotion-Logic Encounter,"The paper provides a new direction to controlling psychological balance in emotion-logic encounter. A dynamics of co-existing multiple emotions is considered, and the condition for stability of the dynamics is determined by Lyapunov approach. Stabilization of the emotional dynamics is ascertained through Hebbian learning by autonomous adaptation of parameters of the dynamics. A novel scheme for stabilization of emotion-logic encounter is proposed by over-powering logic over emotion or vice-versa, based on the personality of the subject.","Machine learning,
Stability analysis,
Logic,
Psychology,
Difference equations,
Computer science,
Hebbian theory,
Lyapunov method,
Humans,
Differential equations"
Adaptive Multi-versioning for OpenMP Parallelization via Machine Learning,"The introduction of multi-core architectures generates a higher demand for parallelism in order to fully exploit the potential of modern computers. It is of vital importance that a compiler can allocate parallel workload in a cost-aware manner in order to achieve optimal performance on a multi-core architecture. This paper presents an adaptive OpenMP-based mechanism capable of generating a reasonable number of representative multi-threaded versions for a given loop, and selecting at runtime a suitable version to execute on a multi-core architecture. Preliminary experimental results show that, on average, it achieves 87% of the highest performance improvement across a whole spectrum of input sizes on two multi-core platforms.","Machine learning,
Yarn,
Computer architecture,
Runtime,
Costs,
Computer science,
Parallel processing,
Concurrent computing,
Programming profession,
Hardware"
Remote Operation System Detection Base on Machine Learning,A machine learning method for remote operation system recognition through their detection signatures with support vector machine (SNM) is proposed. A vector space model of Nmap fingerprint database and techniques for translating the host responses to SVM input vectors are also suggested. Experimental result on identification of signatures in the fingerprint database of Nmap 4.90RC1 but not known for Nmap 4.76 show that our method is effective in the discovery of new signatures not included in current fingerprint database.,"Machine learning,
Fingerprint recognition,
Support vector machines,
Support vector machine classification,
Databases,
Space technology,
Learning systems,
Protocols,
Computer science,
Educational institutions"
Application of Machine Learning Techniques for Prediction of Radiation Pneumonitis in Lung Cancer Patients,"Lung cancer patients who receive radiotherapy as part of their treatment are at risk radiation-induced lung injury known as radiation pneumonitis (RP). RP is a potentially fatal side effect to treatment. Hence, new methods are needed to guide physicians to prescribe targeted therapy dosage to patients at high risk of RP. Several predictive models based on traditional statistical methods and machine learning techniques have been reported, however, no guidance to variation in performance has not been provided to date. Therefore, in this study, we compare several widely used classification algorithms in the machine learning field are used to distinguish between different risk groups of RP. The performance of these classification algorithms is evaluated in conjunction with several feature selection strategy and the impact of the feature selection on performance is further evaluated.","Machine learning,
Lungs,
Cancer,
Medical treatment,
Classification algorithms,
Support vector machines,
Support vector machine classification,
Filters,
Neoplasms,
Machine learning algorithms"
Mandarin pitch accent prediction using hierarchical model based ensemble machine learning,"In this study, we combine the Mandarin characteristics with Mandarin acoustic attribute and text information and use hierarchical model based ensemble machine learning to predict Mandarin pitch accent. Our model could make the best of advantages of prosody hierarchical structure and ensemble machine learning. When comparing our model with classification and regression tree (CART), support vector machine (SVM), adaboost with CART at different experimental conditions, the hierarchical model obtains the best results, it can achieve 84.75% accuracy rate to Mandarin read speech. At the same time, we compare our proposed method with previous proposed method at the same training set and test set. There are 2.25% and 0.82% absolute accuracy rate improvements.","Predictive models,
Machine learning"
Machine Learning for Intra-Fraction Tumor Motion Modeling with Respiratory Surrogates,"Advances in radiation therapy for cancer have made it possible to deliver conformal doses to the tumor while sparing normal healthy tissues. However, one of the difficulties radiation oncologists face is targeting moving tumors, such as those in the lung, which can change position during normal respiration. Tumor motion can be determined by directly monitoring tumor position using continuous xray imaging or electromagnetic transponders placed in the tumor that emit a signal. These approaches require potentially unnecessary radiation to the patient or acquisition of expensive technology. Alternatively, one can image the patient intermittently to determine tumor location and external markers placed on the patient’s torso. The external surrogates can then be used to determine an inferential model that would determine the tumor position as a function of external surrogates. These external surrogates can be monitored continuously in order to determine the real-time position of the tumor. In this study, we evaluate a machine learning algorithm for inferring intra-fraction tumor motion from external markers using a database of Cyberknife Synchrony system.","Machine learning,
Patient monitoring,
Biomedical applications of radiation,
Cancer,
Lung neoplasms,
Optical imaging,
Electromagnetic radiation,
Transponders,
Torso,
Machine learning algorithms"
