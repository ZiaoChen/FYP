Title,Abstract,Keywords
A classification and comparison framework for software architecture description languages,"Software architectures shift the focus of developers from lines-of-code to coarser-grained architectural elements and their overall interconnection structure. Architecture description languages (ADLs) have been proposed as modeling notations to support architecture-based development. There is, however, little consensus in the research community on what is an ADL, what aspects of an architecture should be modeled in an ADL, and which of several possible ADLs is best suited for a particular problem. Furthermore, the distinction is rarely made between ADLs on one hand and formal specification, module interconnection, simulation and programming languages on the other. This paper attempts to provide an answer to these questions. It motivates and presents a definition and a classification framework for ADLs. The utility of the definition is demonstrated by using it to differentiate ADLs from other modeling notations. The framework is used to classify and compare several existing ADLs, enabling us, in the process, to identify key properties of ADLs. The comparison highlights areas where existing ADLs provide extensive support and those in which they are deficient, suggesting a research agenda for the future.","Software architecture,
Computer architecture,
LAN interconnection,
Architecture description languages,
Application software,
Formal specifications,
Computer languages,
Connectors,
Computer science,
Computer Society"
Stochastic ranking for constrained evolutionary optimization,"Penalty functions are often used in constrained optimization. However, it is very difficult to strike the right balance between objective and penalty functions. This paper introduces a novel approach to balance objective and penalty functions stochastically, i.e., stochastic ranking, and presents a new view on penalty function methods in terms of the dominance of penalty and objective functions. Some of the pitfalls of naive penalty methods are discussed in these terms. The new ranking method is tested using a (/spl mu/, /spl lambda/) evolution strategy on 13 benchmark problems. Our results show that suitable ranking alone (i.e., selection), without the introduction of complicated and specialized variation operators, is capable of improving the search performance significantly.","Stochastic processes,
Constraint optimization,
Counting circuits,
Evolutionary computation,
Benchmark testing,
Functional programming,
Mechanical engineering,
Computer science,
Optimization methods"
Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response,"Describes an automated method to locate and outline blood vessels in images of the ocular fundus. Such a tool should prove useful to eye care specialists for purposes of patient screening, treatment evaluation, and clinical study. The authors' method differs from previously known methods in that it uses local and global vessel features cooperatively to segment the vessel network. The authors evaluate their method using hand-labeled ground truth segmentations of 20 images. A plot of the operating characteristic shows that the authors' method reduces false positives by as much as 15 times over basic thresholding of a matched filter response (MFR), at up to a 75% true positive rate. For a baseline, they also compared the ground truth against a second hand-labeling, yielding a 90% true positive and a 4% false positive detection rate, on average. These numbers suggest there is still room for a 15% true positive rate improvement, with the same false positive rate, over the authors' method. They are making all their images and hand labelings publicly available for interested researchers to use in evaluating related methods.","Blood vessels,
Biomedical imaging,
Retina,
Matched filters,
Image segmentation,
Medical treatment,
Labeling,
Diabetes,
Hypertension,
Arteriosclerosis"
RRT-connect: An efficient approach to single-query path planning,"A simple and efficient randomized algorithm is presented for solving single-query path planning problems in high-dimensional configuration spaces. The method works by incrementally building two rapidly-exploring random trees (RRTs) rooted at the start and the goal configurations. The trees each explore space around them and also advance towards each other through, the use of a simple greedy heuristic. Although originally designed to plan motions for a human arm (modeled as a 7-DOF kinematic chain) for the automatic graphic animation of collision-free grasping and manipulation tasks, the algorithm has been successfully applied to a variety of path planning problems. Computed examples include generating collision-free motions for rigid objects in 2D and 3D, and collision-free manipulation motions for a 6-DOF PUMA arm in a 3D workspace. Some basic theoretical analysis is also presented.","Path planning,
Computer science,
Space exploration,
Algorithm design and analysis,
Humans,
Animation,
Robotic assembly,
Buildings,
Tree graphs,
Kinematics"
Summary cache: a scalable wide-area Web cache sharing protocol,"The sharing of caches among Web proxies is an important technique to reduce Web traffic and alleviate network bottlenecks. Nevertheless it is not widely deployed due to the overhead of existing protocols. In this paper we demonstrate the benefits of cache sharing, measure the overhead of the existing protocols, and propose a new protocol called ""summary cache"". In this new protocol, each proxy keeps a summary of the cache directory of each participating proxy, and checks these summaries for potential hits before sending any queries. Two factors contribute to our protocol's low overhead: the summaries are updated only periodically, and the directory representations are very economical, as low as 8 bits per entry. Using trace-driven simulations and a prototype implementation, we show that, compared to existing protocols such as the Internet cache protocol (ICP), summary cache reduces the number of intercache protocol messages by a factor of 25 to 60, reduces the bandwidth consumption by over 50%, eliminates 30% to 95% of the protocol CPU overhead, all while maintaining almost the same cache hit ratios as ICP. Hence summary cache scales to a large number of proxies. (This paper is a revision of Fan et al. 1998; we add more data and analysis in this version.).","Cache memory,
Internet,
Bandwidth,
Computer science,
Routing protocols,
Telecommunication traffic,
Particle measurements,
Virtual prototyping,
Bit rate,
Data analysis"
Interpolation revisited [medical images application],"Based on the theory of approximation, this paper presents a unified analysis of interpolation and resampling techniques. An important issue is the choice of adequate basis functions. The authors show that, contrary to the common belief, those that perform best are not interpolating. By opposition to traditional interpolation, the authors call their use generalized interpolation; they involve a prefiltering step when correctly applied. The authors explain why the approximation order inherent in any basis function is important to limit interpolation artifacts. The decomposition theorem states that any basis function endowed with approximation order ran be expressed as the convolution of a B spline of the same order with another function that has none. This motivates the use of splines and spline-based functions as a tunable way to keep artifacts in check without any significant cost penalty. The authors discuss implementation and performance issues, and they provide experimental evidence to support their claims.","Interpolation,
Spline,
Performance analysis,
Biomedical imaging,
Convolution,
Cost function,
Kernel,
Heart,
Sampling methods"
Approximating the Nondominated Front Using the Pareto Archived Evolution Strategy,"We introduce a simple evolution scheme for multiobjective optimization problems, called the Pareto Archived Evolution Strategy (PAES). We argue that PAES may represent the simplest possible nontrivial algorithm capable of generating diverse solutions in the Pareto optimal set. The algorithm, in its simplest form, is a (1+1) evolution strategy employing local search but using a reference archive of previously found solutions in order to identify the approximate dominance ranking of the current and candidate solution vectors. (1+1)-PAES is intended to be a baseline approach against which more involved methods may be compared. It may also serve well in some real-world applications when local search seems superior to or competitive with population-based methods. We introduce (1+λ) and (μ+λ) variants of PAES as extensions to the basic algorithm. Six variants of PAES are compared to variants of the Niched Pareto Genetic Algorithm and the Nondominated Sorting Genetic Algorithm over a diverse suite of six test functions. Results are analyzed and presented using techniques that reduce the attainment surfaces generated from several optimization runs into a set of univariate distributions. This allows standard statistical analysis to be carried out for comparative purposes. Our results provide strong evidence that PAES performs consistently well on a range of multiobjective optimization tasks.","multiobjective performance assessment.,
Genetic algorithms,
evolution strategies,
multiobjective optimization,
test functions"
Performance comparison of two on-demand routing protocols for ad hoc networks,"Ad hoc networks are characterized by multi-hop wireless connectivity, frequently changing network topology and the need for efficient dynamic routing protocols. We compare the performance of two prominent on-demand routing protocols for mobile ad hoc networks - dynamic source routing (DSR) and ad hoc on-demand distance vector routing (AODV). A detailed simulation model with MAC and physical layer models is used to study inter-layer interactions and their performance implications. We demonstrate that even though DSR and AODV share a similar on-demand behavior the differences in the protocol mechanics can lead to significant performance differentials. The performance differentials are analyzed using varying network load, mobility and network size. Based on the observations, we make recommendations about how the performance of either protocol can be improved.","Routing protocols,
Ad hoc networks,
Computer science,
Tin,
Network topology,
Portable computers,
Personal digital assistants,
Mobile ad hoc networks,
IP networks"
This site can’t be reached,,
AODV-BR: backup routing in ad hoc networks,"Nodes in mobile ad hoc networks communicate with one another via packet radios on wireless multihop links. Because of node mobility and power limitations, the network topology changes frequently. Routing protocols therefore play an important role in mobile multihop network communications. A trend in ad hoc network routing is the reactive on-demand philosophy where routes are established only when required. Most of the protocols in this category, however, use a single route and do not utilize multiple alternate paths. We propose a scheme to improve existing on-demand routing protocols by creating a mesh and providing multiple alternate routes. Our algorithm establishes the mesh and multipaths without transmitting any extra control message. We apply our scheme to the Ad-hoc On-Demand Distance Vector (AODV) protocol and evaluate the performance improvements by simulation.",
Speech parameter generation algorithms for HMM-based speech synthesis,"This paper derives a speech parameter generation algorithm for HMM-based speech synthesis, in which the speech parameter sequence is generated from HMMs whose observation vector consists of a spectral parameter vector and its dynamic feature vectors. In the algorithm, we assume that the state sequence (state and mixture sequence for the multi-mixture case) or a part of the state sequence is unobservable (i.e., hidden or latent). As a result, the algorithm iterates the forward-backward algorithm and the parameter generation algorithm for the case where the state sequence is given. Experimental results show that by using the algorithm, we can reproduce clear formant structure from multi-mixture HMMs as compared with that produced from single-mixture HMMs.","Speech synthesis,
Hidden Markov models,
Databases,
Context modeling,
Computer science,
Character generation,
Runtime,
Interpolation,
Cepstral analysis"
Learning Overcomplete Representations,"In an overcomplete basis, the number of basis vectors is greater than the dimensionality of the input, and the representation of an input is not a unique combination of basis vectors. Overcomplete representations have been advocated because they have greater robustness in the presence of noise, can be sparser, and can have greater flexibility in matching structure in the data. Overcomplete codes have also been proposed as a model of some of the response properties of neurons in primary visual cortex. Previous work has focused on finding the best representation of a signal using a fixed overcomplete basis (or dictionary). We present an algorithm for learning an overcomplete basis by viewing it as probabilistic model of the observed data. We show that overcomplete bases can yield a better approximation of the underlying statistical distribution of the data and can thus lead to greater coding efficiency. This can be viewed as a generalization of the technique of independent component analysis and provides a method for Bayesian reconstruction of signals in the presence of noise and for blind source separation when there are more sources than mixtures.",
Resolution and noise properties of MAP reconstruction for fully 3-D PET,"Derives approximate analytical expressions for the local impulse response and covariance of images reconstructed from fully three-dimensional (3-D) positron emission tomography (PET) data using maximum a posteriori (MAP) estimation. These expressions explicitly account for the spatially variant detector response and sensitivity of a 3-D tomograph. The resulting spatially variant impulse response and covariance are computed using 3-D Fourier transforms. A truncated Gaussian distribution is used to account for the effect on the variance of the nonnegativity constraint used in MAP reconstruction. Using Monte Carlo simulations and phantom data from the microPET small animal scanner, the authors show that the approximations provide reasonably accurate estimates of contrast recovery and covariance of MAP reconstruction for priors with quadratic energy functions. They also describe how these analytical results can be used to achieve near-uniform contrast recovery throughout the reconstructed volume.",
Imaging heart motion using harmonic phase MRI,"Describes a new image processing technique for rapid analysis and visualization of tagged cardiac magnetic resonance (MR) images. The method is based on the use of isolated spectral peaks in spatial modulation of magnetization (SPAMM)-tagged magnetic resonance images. The authors call the calculated angle of the complex image corresponding to one of these peaks a harmonic phase (HARP) image and show that HARP images can be used to synthesize conventional tag lines, reconstruct displacement fields for small motions, and calculate two-dimensional (2-D) strain. The performance of this new approach is demonstrated using both real and simulated tagged MR images. Potential for use of HARP images in fast imaging techniques and three-dimensional (3-D) analyses are discussed.","Heart,
Magnetic resonance imaging,
Image analysis,
Magnetic resonance,
Image processing,
Magnetic analysis,
Visualization,
Magnetic modulators,
Magnetization,
Image reconstruction"
A real-time algorithm for mobile robot mapping with applications to multi-robot and 3D mapping,"We present an incremental method for concurrent mapping and localization for mobile robots equipped with 2D laser range finders. The approach uses a fast implementation of scan-matching for mapping, paired with a sample-based probabilistic method for localization. Compact 3D maps are generated using a multi-resolution approach adopted from the computer graphics literature, fed by data from a dual laser system. Our approach builds 3D maps of large, cyclic environments in real-time, and it is robust. Experimental results illustrate that accurate maps of large, cyclic environments can be generated even in the absence of any odometric data.","Mobile robots,
Robustness,
Indoor environments,
Robot sensing systems,
Error correction,
Application software,
Computer science,
Laser theory,
Computer graphics,
Iterative algorithms"
Recovering non-rigid 3D shape from image streams,"The paper addresses the problem of recovering 3D non-rigid shape models from image sequences. For example, given a video recording of a talking person, we would like to estimate a 3D model of the lips and the full face and its internal modes of variation. Many solutions that recover 3D shape from 2D image sequences have been proposed; these so-called structure-from-motion techniques usually assume that the 3D object is rigid. For example, C. Tomasi and T. Kanades' (1992) factorization technique is based on a rigid shape matrix, which produces a tracking matrix of rank 3 under orthographic projection. We propose a novel technique based on a non-rigid model, where the 3D shape in each frame is a linear combination of a set of basis shapes. Under this model, the tracking matrix is of higher rank, and can be factored in a three-step process to yield pose, configuration and shape. To the best of our knowledge, this is the first model free approach that can recover from single-view video sequences nonrigid shape models. We demonstrate this new algorithm on several video sequences. We were able to recover 3D non-rigid human face and animal models with high accuracy.","Shape,
Streaming media,
Image sequences,
Electrical capacitance tomography,
Humans,
Face,
Video sequences,
Animals,
Matrix decomposition,
Computer science"
Parametric estimate of intensity inhomogeneities applied to MRI,"Presents a new approach to the correction of intensity inhomogeneities in magnetic resonance imaging (MRI) that significantly improves intensity-based tissue segmentation. The distortion of the image brightness values by a low-frequency bias field impedes visual inspection and segmentation. The new correction method called parametric bias field correction (PABIC) is based on a simplified model of the imaging process, a parametric model of tissue class statistics, and a polynomial model of the inhomogeneity field. The authors assume that the image is composed of pixels assigned to a small number of categories with a priori known statistics. Further they assume that the image is corrupted by noise and a low-frequency inhomogeneity field. The estimation of the parametric bias field is formulated as a nonlinear energy minimization problem using an evolution strategy (ES). The resulting bias field is independent of the image region configurations and thus overcomes limitations of methods based on homomorphic filtering. Furthermore, PABIC can correct bias distortions much larger than the image contrast. Input parameters are the intensity statistics of the classes and the degree of the polynomial function. The polynomial approach combines bias correction with histogram adjustment, making it well suited for normalizing the intensity histogram of datasets from serial studies. The authors present simulations and a quantitative validation with phantom and test images. A large number of MR image data acquired with breast, surface, and head coils, both in two dimensions and three dimensions, have been processed and demonstrate the versatility and robustness of this new bias correction scheme.","Magnetic resonance imaging,
Parametric statistics,
Polynomials,
Image segmentation,
Nonlinear distortion,
Histograms,
Brightness,
Impedance,
Inspection,
Pixel"
Collaborative multi-robot exploration,"In this paper we consider the problem of exploring an unknown environment by a team of robots. As in single-robot exploration the goal is to minimize the overall exploration time. The key problem to be solved therefore is to choose appropriate target points for the individual robots so that they simultaneously explore different regions of their environment. We present a probabilistic approach for the coordination of multiple robots which, in contrast to previous approaches, simultaneously takes into account the costs of reaching a target point and the utility of target points. The utility of target points is given by the size of the unexplored area that a robot can cover with its sensors upon reaching a target position. Whenever a target point is assigned to a specific robot, the utility of the unexplored area visible from this target position is reduced for the other robots. This way, a team of multiple robots assigns different target points to the individual robots. The technique has been implemented and tested extensively in real-world experiments and simulation runs. The results given in this paper demonstrate that our coordination technique significantly reduces the exploration time compared to previous approaches.","Collaboration,
Robot kinematics,
Robot sensing systems,
Computer science,
Costs,
Mobile robots,
Multirobot systems,
Testing,
Traveling salesman problems,
Redundancy"
An efficient remote use authentication scheme using smart cards,"Based on the discrete logarithm problem, Hwang and Li (see ibid., vol.46, no.1, p.28-30, Feb. 2000) proposed a remote user authentication scheme using smart cards. Their scheme is very novel because no password table is required to keep in a system. In this paper, we further propose an efficient and practical remote user authentication scheme using smart cards. The proposed scheme not only provides the same advantages as that of Hwang and Li's scheme, but also significantly reduces the communication and computation costs.","Authentication,
Smart cards,
Sun,
Computational efficiency,
Data security,
Computer science,
Communication system security,
Cryptography,
Random number generation,
Resists"
Dynamic self-organizing maps with controlled growth for knowledge discovery,"The growing self-organizing map (GSOM) algorithm is presented in detail and the effect of a spread factor, which can be used to measure and control the spread of the GSOM, is investigated. The spread factor is independent of the dimensionality of the data and as such can be used as a controlling measure for generating maps with different dimensionality, which can then be compared and analyzed with better accuracy. The spread factor is also presented as a method of achieving hierarchical clustering of a data set with the GSOM. Such hierarchical clustering allows the data analyst to identify significant and interesting clusters at a higher level of the hierarchy, and continue with finer clustering of the interesting clusters only. Therefore, only a small map is created in the beginning with a low spread factor, which can be generated for even a very large data set. Further analysis is conducted on selected sections of the data and of smaller volume. Therefore, this method facilitates the analysis of even very large data sets.","Self organizing feature maps,
Data mining,
Unsupervised learning,
Data analysis,
Clustering algorithms,
Neural networks,
Humans,
Pattern analysis,
Computer science,
Software engineering"
Cooperative Coevolution: An Architecture for Evolving Coadapted Subcomponents,"To successfully apply evolutionary algorithms to the solution of increasingly complex problems, we must develop effective techniques for evolving solutions in the form of interacting coadapted subcomponents. One of the major difficulties is finding computational extensions to our current evolutionary paradigms that will enable such subcomponents to “emerge” rather than being hand designed. In this paper, we describe an architecture for evolving such subcomponents as a collection of cooperating species. Given a simple string- matching task, we show that evolutionary pressure to increase the overall fitness of the ecosystem can provide the needed stimulus for the emergence of an appropriate number of interdependent subcomponents that cover multiple niches, evolve to an appropriate level of generality, and adapt as the number and roles of their fellow subcomponents change over time. We then explore these issues within the context of a more complicated domain through a case study involving the evolution of artificial neural networks.","neural networks,
Coevolution,
genetic algorithms,
evolution strategies,
emergent decomposition"
Acceptance of e-commerce services: the case of electronic brokerages,"This paper examines human motivations underlying individual acceptance of business-to-consumer (B2C) electronic commerce services. Such acceptance is the key to the survival of firms in this intensely competitive industry. A modified theory of planned behavior (TPB) is used to hypothesize a model of e-commerce service acceptance, which is then tested using a field survey of 172 e-brokerage users. We found TPB was useful in explaining e-commerce service acceptance, however, acceptance motivations were significantly different from that of typical IS products. Based on a broader conceptualization of TPB's subjective norm to include both external (mass-media) and interpersonal influences, we report that subjective norm is an important predictor of e-commerce acceptance, behavioral control has minimal impact on e-commerce acceptance, and external influence is a significant determinant of subjective norm. Implications of these findings in light of e-commerce research and practice are discussed.","Computer aided software engineering,
Humans,
Electronic commerce,
Testing,
Web and internet services,
Procurement,
Management information systems,
Banking,
Consumer electronics"
New variants of a method of MRI scale standardization,"One of the major drawbacks of magnetic resonance imaging (MRI) has been the lack of a standard and quantifiable interpretation of image intensities. Unlike in other modalities, such as X-ray computerized tomography, MR images taken for the same patient on the same scanner at different times may appear different from each other due to a variety of scanner-dependent variations and, therefore, the absolute intensity values do not have a fixed meaning. The authors have devised a two-step method wherein all images (independent of patients and the specific brand of the MR scanner used) can be transformed in such a may that for the same protocol and body region, in the transformed images similar intensities will have similar tissue meaning. Standardized images can be displayed with fixed windows without the need of per-case adjustment. More importantly, extraction of quantitative information about healthy organs or about abnormalities can be considerably simplified. This paper introduces and compares new variants of this standardizing method that can help to overcome some of the problems with the original method.","Magnetic resonance imaging,
Standardization,
Protocols,
Body regions,
Humans,
Image segmentation,
X-ray imaging,
Computed tomography,
Data mining,
Displays"
Perils of Internet fraud: an empirical investigation of deception and trust with experienced Internet consumers,"This study examines consumer evaluations of a real commercial web site and a fraudulent site that imitates it. The forged site contains malicious manipulations designed to increase trust in the site, decrease perceived risk, and ultimately increase the likelihood that visitors would buy from it. Besides measuring the consumer's willingness to buy from the site, this study recorded the actual ordering of a laptop. Results show that most subjects failed to detect the fraud manipulations, albeit a few succeeded. The fraud has the effect of increasing the consumers' reliance in assurance mechanisms and trust mechanisms, which in turn decrease perceived risk and increase trust in the store. The study confirms hypothesized relationships between purchase behavior, willingness to buy, attitudes toward the store, risk, and trust that are consistent with other trust models found in the literature. Overall, the study sheds light on consumers' vulnerability to attack by hackers posing as a legitimate site.","Internet,
Computer hacking,
Information management,
Management information systems,
Law,
Security,
Portable computers,
Technology management,
Business,
Watches"
Architecture for an Artificial Immune System,"An artificial immune system (ARTIS) is described which incorporates many properties of natural immune systems, including diversity, distributed computation, error tolerance, dynamic learning and adaptation, and self-monitoring. ARTIS is a general framework for a distributed adaptive system and could, in principle, be applied to many domains. In this paper, ARTIS is applied to computer security in the form of a network intrusion detection system called LISYS. LISYS is described and shown to be effective at detecting intrusions, while maintaining low false positive rates. Finally, similarities and differences between ARTIS and Holland's classifier systems are discussed.","network intrusion detection.,
Immune system,
distributed,
adaptive,
classifier systems,
anomaly detection"
Comparative study of turbo decoding techniques: an overview,"We provide an overview of the novel class of channel codes referred to as turbo codes, which have been shown to be capable of performing close to the Shannon limit. We commence with a discussion on turbo encoding, and then move on to describing the form of the iterative decoder most commonly used to decode turbo codes. We then elaborate on various decoding algorithms that can be used in an iterative decoder, and give an example of the operation of such a decoder using the so-called soft output Viterbi (1996) algorithm (SOVA). Lastly, the effect of a range of system parameters is investigated in a systematic fashion, in order to gauge their performance ramifications.","Turbo codes,
Iterative decoding,
Iterative algorithms,
Delay systems,
Bit error rate,
Computer science,
Viterbi algorithm,
Land mobile radio,
Performance gain,
Multimedia communication"
Randomized rumor spreading,"Investigates the class of epidemic algorithms that are commonly used for the lazy transmission of updates to distributed copies of a database. These algorithms use a simple randomized communication mechanism to ensure robustness. Suppose n players communicate in parallel rounds in each of which every player calls a randomly selected communication partner. In every round, players can generate rumors (updates) that are to be distributed among all players. Whenever communication is established between two players, each one must decide which of the rumors to transmit. The major problem is that players might not know which rumors their partners have already received. For example, a standard algorithm forwarding each rumor form the calling to the called players for /spl Theta/(ln n) rounds needs to transmit the rumor /spl Theta/(n ln n) times in order to ensure that every player finally receives the rumor with high probability. We investigate whether such a large communication overhead is inherent to epidemic algorithms. On the positive side, we show that the communication overhead can be reduced significantly. We give an algorithm using only O(n ln ln n) transmissions and O(ln n) rounds. In addition, we prove the robustness of this algorithm. On the negative side, we show that any address-oblivious algorithm needs to send /spl Omega/(n ln ln n) messages for each rumor, regardless of the number of rounds. Furthermore, we give a general lower bound showing that time and communication optimality cannot be achieved simultaneously using random phone calls, i.e. every algorithm that distributes a rumor in O(ln n) rounds needs /spl omega/(n) transmissions.","Robustness,
Computer science,
Distributed databases,
Computer crashes"
A multiscale dynamic programming procedure for boundary detection in ultrasonic artery images,"Ultrasonic measurements of human carotid and femoral artery walls are conventionally obtained by manually tracing interfaces between tissue layers. The drawbacks of this method are the interobserver variability and inefficiency. Here, the authors present a new automated method which reduces these problems. By applying a multiscale dynamic programming (DP) algorithm, approximate vessel wall positions are first estimated in a coarse-scale image, which then guide the detection of the boundaries in a fine-scale image. In both cases, DP is used for finding a global optimum for a cost function. The cost function is a weighted sum of terms, in fuzzy expression forms, representing image features and geometrical characteristics of the vessel interfaces. The weights are adjusted by a training procedure using human expert tracings. Operator interventions, if needed, also take effect under the framework of global optimality. This reduces the amount of human intervention and, hence, variability due to subjectiveness. By incorporating human knowledge and experience, the algorithm becomes more robust. A thorough evaluation of the method in the clinical environment shows that interobserver variability is evidently decreased and so is the overall analysis time. The authors conclude that the automated procedure can replace the manual procedure and leads to an improved performance.","Dynamic programming,
Arteries,
Humans,
Cost function,
Ultrasonic variables measurement,
Councils,
Anthropometry,
Heuristic algorithms,
Robustness,
Ultrasonic imaging"
Opportunistic data structures with applications,"We address the issue of compressing and indexing data. We devise a data structure whose space occupancy is a function of the entropy of the underlying data set. We call the data structure opportunistic since its space occupancy is decreased when the input is compressible and this space reduction is achieved at no significant slowdown in the query performance. More precisely, its space occupancy is optimal in an information-content sense because text T[1,u] is stored using O(H/sub k/(T))+o(1) bits per input symbol in the worst case, where H/sub k/(T) is the kth order empirical entropy of T (the bound holds for any fixed k). Given an arbitrary string P[1,p], the opportunistic data structure allows to search for the occurrences of P in T in O(p+occlog/sup /spl epsiv//u) time (for any fixed /spl epsiv/>0). If data are uncompressible we achieve the best space bound currently known (Grossi and Vitter, 2000); on compressible data our solution improves the succinct suffix array of (Grossi and Vitter, 2000) and the classical suffix tree and suffix array data structures either in space or in query time or both. We also study our opportunistic data structure in a dynamic setting and devise a variant achieving effective search and update time bounds. Finally, we show how to plug our opportunistic data structure into the Glimpse tool (Manber and Wu, 1994). The result is an indexing tool which achieves sublinear space and sublinear query time complexity.","Data structures,
Indexing,
Entropy,
Costs,
Tree data structures,
Plugs,
Computer science,
Fault tolerance,
Postal services,
Data engineering"
Evolutionary ensembles with negative correlation learning,"Based on negative correlation learning and evolutionary learning, this paper presents evolutionary ensembles with negative correlation learning (EENCL) to address the issues of automatic determination of the number of individual neural networks (NNs) in an ensemble and the exploitation of the interaction between individual NN design and combination. The idea of EENCL is to encourage different individual NNs in the ensemble to learn different parts or aspects of the training data so that the ensemble can learn better the entire training data. The cooperation and specialization among different individual NNs are considered during the individual NN design. This provides an opportunity for different NNs to interact with each other and to specialize. Experiments on two real-world problems demonstrate that EENCL can produce NN ensembles with good generalization ability.","Neural networks,
Training data,
Humans,
Artificial neural networks,
Computer science,
Laboratories,
Algorithm design and analysis,
Robustness,
Degradation,
Problem-solving"
Clustering data streams,"We study clustering under the data stream model of computation where: given a sequence of points, the objective is to maintain a consistently good clustering of the sequence observed so far, using a small amount of memory and time. The data stream model is relevant to new classes of applications involving massive data sets, such as Web click stream analysis and multimedia data analysis. We give constant-factor approximation algorithms for the k-median problem in the data stream model of computation in a single pass. We also show negative results implying that our algorithms cannot be improved in a certain sense.","Clustering algorithms,
Streaming media,
Approximation algorithms,
Computer science,
Computational modeling,
Data analysis,
Application software,
Telephony,
Web pages,
Laboratories"
Voxel similarity measures for 3-D serial MR brain image registration,"The authors have evaluated eight different similarity measures used for rigid body registration of serial magnetic resonance (MR) brain scans. To assess their accuracy the authors used 33 clinical three-dimensional (3-D) serial MR images, with deformable extradural tissue excluded by manual segmentation and simulated 3-D MR images with added intensity distortion. For each measure the authors determined the consistency of registration transformations for both sets of segmented and unsegmented data. They have shown that of the eight measures tested, the ones based on joint entropy produced the best consistency. In particular, these measures seemed to be least sensitive to the presence of extradural tissue. For these data the difference in accuracy of these joint entropy measures, with or without brain segmentation, was within the threshold of visually detectable change in the difference images.","Brain,
Image registration,
Image segmentation,
Distortion measurement,
Biomedical imaging,
Magnetic resonance,
Testing,
Entropy,
Radiology,
Hospitals"
Intelligent medium access for mobile ad hoc networks with busy tones and power control,"In mobile ad hoc networks (MANETs), one essential issue is how to increase channel utilization while avoiding the hidden-terminal and the exposed-terminal problems. Several MAC protocols, such as RTS/CTS-based and busy-tone-based schemes, have been proposed to alleviate these problems. In this paper, we explore the possibility of combining the concept of power control with the RTS/CTS-based and busy-tone-based protocols to further increase channel utilization. A sender will use an appropriate power level to transmit its packets so as to increase the possibility of channel reuse. The possibility of using discrete, instead of continuous, power levels is also discussed. Through analyses and simulations, we demonstrate the advantage of our new MAC protocol. This, together with the extra benefits such as saving battery energy and reducing cochannel interference, does show a promising direction to enhance the performance of MANETs.","Intelligent networks,
Mobile ad hoc networks,
Power control,
Media Access Protocol,
Access protocols,
Batteries,
Interchannel interference,
Computer science,
Analytical models,
Wireless networks"
A fast iterative nearest point algorithm for support vector machine classifier design,"In this paper we give a new fast iterative algorithm for support vector machine (SVM) classifier design. The basic problem treated is one that does not allow classification violations. The problem is converted to a problem of computing the nearest point between two convex polytopes. The suitability of two classical nearest point algorithms, due to Gilbert, and Mitchell et al., is studied. Ideas from both these algorithms are combined and modified to derive our fast algorithm. For problems which require classification violations to be allowed, the violations are quadratically penalized and an idea due to Cortes and Vapnik and Friess is used to convert it to a problem in which there are no classification violations. Comparative computational evaluation of our algorithm against powerful SVM methods such as Platt's sequential minimal optimization shows that our algorithm is very competitive.",
Generalized fuzzy c-means clustering strategies using L/sub p/ norm distances,Fuzzy c-means (FCM) is a useful clustering technique. Modifications of FCM using L/sub 1/ norm distances increase robustness to outliers. Object and relational data versions of FCM clustering are defined for the more general case where the L/sub p/ norm (p/spl ges/1) or semi-norm (00 in order to facilitate the empirical examination of the object data models. Both object and relational approaches are included in a numerical study.,
A CAD system for the automatic detection of clustered microcalcifications in digitized mammogram films,"Clusters of microcalcifications in mammograms are an important early sign of breast cancer. This paper presents a computer-aided diagnosis (CAD) system for the automatic detection of clustered microcalcifications in digitized mammograms. The proposed system consists of two main steps. First, potential microcalcification pixels in the mammograms are segmented out by using mixed features consisting of wavelet features and gray level statistical features, and labeled into potential individual microcalcification objects by their spatial connectivity. Second, individual microcalcifications are detected by using a set of 31 features extracted from the potential individual microcalcification objects. The discriminatory power of these features is analyzed using general regression neural networks via sequential forward and sequential backward selection methods. The classifiers used in these two steps are both multilayer feedforward neural networks. The method is applied to a database of 40 mammograms (Nijmegen database) containing 105 clusters of microcalcifications. A free-response operating characteristics (FROC) curve is used to evaluate the performance. Results show that the proposed system gives quite satisfactory detection performance. In particular, a 90% mean true positive detection rate is achieved at the cost of 0.5 false positive per image when mixed features are used in the first step and 15 features selected by the sequential backward selection method are used in the second step. However, one must be cautious when interpreting the results, since the 20 training samples are also used in the testing step.","Neural networks,
Multi-layer neural network,
Spatial databases,
Breast cancer,
Computer aided diagnosis,
Object detection,
Feature extraction,
Feedforward neural networks,
Image databases,
Costs"
A new multi-channel MAC protocol with on-demand channel assignment for multi-hop mobile ad hoc networks,"The wireless mobile ad hoc network (MANET) architecture has received a lot of attention recently. This paper considers the access of multiple channels in a MANET with multi-hop communication behavior. We point out several interesting issues when using multiple channels. We then propose a new multi-channel MAC protocol, which is characterized by the following features: (i) it follows an ""on-demand"" style to assign channels to mobile hosts, (ii) the number of channels required is independent of the network topology and degree, (iii) it flexibly adapts to host mobility and only exchanges few control messages to achieve channel assignment and medium access, and (iv) no clock synchronization is required. Compared to existing protocols, some assign channels to hosts statically (thus a host will occupy a channel even when it has no intention to transmit) some require a number of channels which is a function of the maximum connectivity, and some necessitate a clock synchronization among all hosts in the MANET. Extensive simulations are conducted to evaluate the proposed protocol.","Media Access Protocol,
Access protocols,
Mobile ad hoc networks,
Spread spectrum communication,
Computer science,
Mobile communication,
Clocks,
Synchronization,
Propagation delay,
Quality of service"
"Can recursive bisection alone produce routable, placements?",,
Robust coding schemes for indexing and retrieval from large face databases,"This paper introduces two new coding schemes, probabilistic reasoning models (PRM) and enhanced FLD (Fisher linear discriminant) models (EFM), for indexing and retrieval of large image databases with applications to face recognition. The unifying theme of the new schemes is that of lowering the space dimension (""data compression"") subject to increased fitness for the discrimination index.","Robustness,
Indexing,
Information retrieval,
Face recognition,
Principal component analysis,
Image coding,
Image databases,
Image retrieval,
Computer science,
Application software"
Multiple description wavelet based image coding,"We consider the problem of coding images for transmission over error-prone channels. The impairments we target are transient channel shutdowns, as would occur in a packet network when a packet is lost, or in a wireless system during a deep fade: when data is delivered it is assumed to be error-free, but some of the data may never reach the receiver. The proposed algorithms are based on a combination of multiple description scalar quantizers with techniques successfully applied to the construction of some of the most efficient subband coders. A given image is encoded into multiple independent packets of roughly equal length. When packets are lost, the quality of the approximation computed at the receiver depends only on the number of packets received, but does not depend on exactly which packets are actually received. When compared with previously reported results on the performance of robust image coders based on multiple descriptions, on standard test images, our coders attain similar PSNR values using typically about 50-60% of the bit rate required by these other state-of-the-art coders, while at the same time providing significantly more freedom in the mechanism for allocation of redundancy among descriptions.","Image coding,
Error correction codes,
Computer errors,
Computer science,
Engineering profession,
Communication switching,
Robustness,
Code standards,
Testing,
PSNR"
Nonlinear multiresolution signal decomposition schemes. II. Morphological wavelets,"For pt.I see ibid., vol.9, no.11, p.1862-76 (2000). In its original form, the wavelet transform is a linear tool. However, it has been increasingly recognized that nonlinear extensions are possible. A major impulse to the development of nonlinear wavelet transforms has been given by the introduction of the lifting scheme by Sweldens (1995, 1996, 1998). The aim of this paper, which is a sequel to a previous paper devoted exclusively to the pyramid transform, is to present an axiomatic framework encompassing most existing linear and nonlinear wavelet decompositions. Furthermore, it introduces some, thus far unknown, wavelets based on mathematical morphology, such as the morphological Haar wavelet, both in one and two dimensions. A general and flexible approach for the construction of nonlinear (morphological) wavelets is provided by the lifting scheme. This paper briefly discusses one example, the max-lifting scheme, which has the intriguing property that preserves local maxima in a signal over a range of scales, depending on how local or global these maxima are.",
An ultra-fast user-steered image segmentation paradigm: live wire on the fly,"The authors have been developing general user steered image segmentation strategies for routine use in applications involving a large number of data sets. In the past, they have presented three segmentation paradigms: live wire, live lane, and a three-dimensional (3-D) extension of the live-wire method. Here, they introduce an ultra-fast live-wire method, referred to as live wire on the fly for further reducing user's time compared to the basic live-wire method. In live wire, 3-D/four-dimensional (4-D) object boundaries are segmented in a slice-by-slice fashion. To segment a two-dimensional (2-D) boundary, the user initially picks a point on the boundary and all possible minimum-cost paths from this point to all other points in the image are computed via Dijkstra's algorithm. Subsequently, a live wire is displayed in real time from the initial point to any subsequent position taken by the cursor. If the cursor is close to the desired boundary, the live wire snaps on to the boundary. The cursor is then deposited and a new live-wire segment is found next. The entire 2-D boundary is specified via a set of live-wire segments in this fashion. A drawback of this method is that the speed of optimal path computation depends on image size. On modestly powered computers, for images of even modest size, some sluggishness appears in user interaction, which reduces the overall segmentation efficiency. In this work, the authors solve this problem by exploiting some known properties of graphs to avoid unnecessary minimum-cost path computation during segmentation. In live wire on the fly, when the user selects a point on the boundary the live-wire segment is computed and displayed in real time from the selected point to any subsequent position of the cursor in the image, even for large images and even on low-powered computers. Based on 492 tracing experiments from an actual medical application, the authors demonstrate that live wire on the fly is 1.331 times faster than live wire for actual segmentation for varying image sizes, although the pure computational part alone is found to be about 120 times faster.","Image segmentation,
Wire,
Computer displays,
Object recognition,
Two dimensional displays,
Medical services,
Biomedical equipment,
Application software,
Image recognition,
Radiology"
High performance parametric modeling with Nimrod/G: killer application for the global grid?,"This paper examines the role of parametric modeling as an application for the global computing grid, and explores some heuristics which make it possible to specific soft real time deadlines for larger computational experiments. We demonstrate the scheme with a case study utilizing the Globus toolkit running on the GUSTO testbed.","Parametric statistics,
Grid computing,
Application software,
Australia,
Computational modeling,
Biological system modeling,
Road safety,
Security,
Computer science,
Cyclic redundancy check"
A robust DCT-based watermarking for copyright protection,"A novel technique for embedding watermarks into a host image in the frequency domain is proposed. Unlike the traditional techniques, the method addresses that the watermark is embedded at low frequency. The weighted correction is also used to improve the imperceptibility of the watermark. Moreover the watermark is self-extractable and the algorithm is simple. Experimental results demonstrate that the watermark is robust to various attacks.","Robustness,
Watermarking,
Copyright protection,
Discrete cosine transforms,
Signal processing algorithms,
Random sequences,
Pixel,
Frequency domain analysis,
Random number generation,
Computer science"
A computational framework for simulating and analyzing human and animal movement,"Computer simulations provide a framework for exploring the biomechanics of movement. The authors have developed a software system that lets users create computer graphics simulations of human and animal movement. This software provides a platform on which the biomechanics community can build a library of simulations that can be exchanged, tested and improved through multi-institutional collaboration.","Computational modeling,
Analytical models,
Humans,
Computer simulation,
Biomechanics,
Collaborative software,
Software systems,
Computer graphics,
Animals,
Software libraries"
Gradient and texture analysis for the classification of mammographic masses,"Computer-aided classification of benign and malignant masses on mammograms is attempted in this study by computing gradient-based and texture-based features. Features computed based on gray-level co-occurrence matrices (GCMs) are used to evaluate the effectiveness of textural information possessed by mass regions in comparison with the textural information present in mass margins. A method involving polygonal modeling of boundaries is proposed for the extraction of a ribbon of pixels across mass margins. Two gradient-based features are developed to estimate the sharpness of mass boundaries in the ribbons of pixels extracted from their margins. A total of 54 images (28 benign and 26 malignant) containing 39 images from the Mammographic Image Analysis Society (MIAS) database and 15 images from a local database are analyzed. The best benign versus malignant classification of 82.1%, with an area (A/sub z/) of 0.85 under the receiver operating characteristics (ROC) curve, was obtained with the images from the MIAS database by using GCM-based texture features computed from mass margins. The classification method used is based on posterior probabilities computed from Mahalanobis distances. The corresponding accuracy using jack-knife classification was observed to be 74.4%, with A/sub x/=0.67. Gradient-based features achieved A/sub x/=0.6 on the MIAS database and A/sub z/=0.76 on the combined database. The corresponding values obtained using jack-knife classification were observed to be 0.52 and 0.73 for the MIAS and combined databases, respectively.","Spatial databases,
Benign tumors,
Malignant tumors,
Image databases,
Breast cancer,
Data mining,
Mammography,
Biopsy,
Biomedical engineering,
Image texture analysis"
Using humanoid robots to study human behavior,"Our understanding of human behavior advances as our humanoid robotics work progresses-and vice versa. This team's work focuses on trajectory formation and planning, learning from demonstration, oculomotor control and interactive behaviors. They are programming robotic behavior based on how we humans ""program"" behavior in-or train-each other.","Humanoid robots,
Humans,
Robot sensing systems,
Legged locomotion,
Information processing,
Laboratories,
Leg,
Torso,
Hydraulic actuators,
Pelvis"
Characterization of well-posedness of piecewise-linear systems,"One of the basic issues in the study of hybrid systems is the well-posedness (existence and uniqueness of solutions) problem of discontinuous dynamical systems. The paper addresses this problem for a class of piecewise-linear discontinuous systems under the definition of solutions of Caratheodory. The concepts of jump solutions or of sliding modes are not considered here. In this sense, the problem to be discussed is one of the most basic problems in the study of well-posedness for discontinuous dynamical systems. First, we derive necessary and sufficient conditions for bimodal systems to be well-posed, in terms of an analysis based on lexicographic inequalities and the smooth continuation property of solutions. Next, its extensions to the multimodal case are discussed. As an application to switching control, in the case that two state feedback gains are switched according to a criterion depending on the state, we give a characterization of all admissible state feedback gains for which the closed loop system remains well-posed.","Piecewise linear techniques,
Automatic control,
Control system synthesis,
Automata,
Stability,
State feedback,
Computer science,
Controllability,
Sufficient conditions,
Control systems"
Placement of dispersed generation systems for reduced losses,"Recent improvements in fuel cell technology along with an increasing demand for small generator units have led to renewed interest in dispersed generation units. This work demonstrates a methodology for deploying dispersed fuel cell generators throughout a power system to allow for more efficient operation. A detailed study of the system losses and sensitivities on Eastern Washington system as part of the larger WSCC system has been completed. This work presents an algorithm to determine the near optimal, with respect to system losses, placement of these units on the power grid. Further, the impacts of dispersed generation at the distribution level are performed with an emphasis on resistive losses, and capacity savings. The results show the importance of placement for minimizing losses and maximizing capacity savings.","Costs,
Distributed power generation,
Power generation,
Fuel cells,
Photovoltaic systems,
Computer science,
Power systems,
Power grids,
Propagation losses,
Stress"
Grammar-based codes: a new class of universal lossless source codes,"We investigate a type of lossless source code called a grammar-based code, which, in response to any input data string x over a fixed finite alphabet, selects a context-free grammar G/sub x/ representing x in the sense that x is the unique string belonging to the language generated by G/sub x/. Lossless compression of x takes place indirectly via compression of the production rules of the grammar G/sub x/. It is shown that, subject to some mild restrictions, a grammar-based code is a universal code with respect to the family of finite-state information sources over the finite alphabet. Redundancy bounds for grammar-based codes are established. Reduction rules for designing grammar-based codes are presented.",Redundancy
Gradual distributed real-coded genetic algorithms,"A major problem in the use of genetic algorithms is premature convergence. One approach for dealing with this problem is the distributed genetic algorithm model. Its basic idea is to keep, in parallel, several subpopulations that are processed by genetic algorithms, with each one being independent of the others. Making distinctions between the subpopulations by applying genetic algorithms with different configurations, we obtain the so-railed heterogeneous distributed genetic algorithms. These algorithms represent a promising way for introducing a correct exploration/exploitation balance in order to avoid premature convergence and reach approximate final solutions. This paper presents the gradual distributed real-coded genetic algorithms, a type of heterogeneous distributed real-coded genetic algorithms that apply a different crossover operator to each sub-population. Experimental results show that the proposals consistently outperform sequential real-coded genetic algorithms.","Genetic algorithms,
Convergence,
Proposals,
Biological cells,
Spatial resolution,
Diversity methods,
Computer science,
Artificial intelligence,
Hardware"
A survey of longest common subsequence algorithms,"The aim of this paper is to give a comprehensive comparison of well-known longest common subsequence algorithms (for two input strings) and study their behaviour in various application environments. The performance of the methods depends heavily on the properties of the problem instance as well as the supporting data structures used in the implementation. We want to make also a clear distinction between methods that determine the actual lcs and those calculating only its length, since the execution time and more importantly, the space demand depends crucially on the type of the task. To our knowledge, this is the first time this kind of survey has been done. Due to the page limits, the paper gives only a coarse overview of the performance of the algorithms; more detailed studies are reported elsewhere.","Costs,
Sequences,
Computer science,
Application software,
Data structures,
Error correction,
Dictionaries,
DNA,
Proteins,
Books"
Alpha estimation in natural images,"Many boundaries between objects in the world project onto curves in an image. However, boundaries involving natural objects (e.g., trees, hair, water, smoke) are often unworkable under this model because many pixels receive light from more than one object. We propose a technique for estimating alpha, the proportion in which two colors mix to produce a color at the boundary. The technique extends blue screen matting to backgrounds that have almost arbitrary color distributions, though coarse knowledge of the boundary's location is required. Results show a number of different objects moved from one image to another while maintaining naturalism.","Electrical capacitance tomography,
Hair,
History,
Computer graphics,
Application software,
Filling,
Computer science,
Color,
Rendering (computer graphics),
Focusing"
Tracing traitors,"We give cryptographic schemes that help trace the source of leaks when sensitive or proprietary data is made available to a large set of parties. A very relevant application is in the context of pay television, where only paying customers should be able to view certain programs. In this application, the programs are normally encrypted, and then the sensitive data is the decryption keys that are given to paying customers. If a pirate decoder is found, it is desirable to reveal the source of its decryption keys. We describe fully resilient schemes which can be used against any decoder which decrypts with nonnegligible probability. Since there is typically little demand for decoders which decrypt only a small fraction of the transmissions (even if it is nonnegligible), we further introduce threshold tracing schemes which can only be used against decoders which succeed in decryption with probability greater than some threshold. Threshold schemes are considerably more efficient than fully resilient schemes.",Cryptography
Regularization for uniform spatial resolution properties in penalized-likelihood image reconstruction,"Traditional space-invariant regularization methods in tomographic image reconstruction using penalized-likelihood estimators produce images with nonuniform spatial resolution properties. The local point spread functions that quantify the smoothing properties of such estimators are space variant, asymmetric, and object-dependent even for space invariant imaging systems. The authors propose a new quadratic regularization scheme for tomographic imaging systems that yields increased spatial uniformity and is motivated by the least-squares fitting of a parameterized local impulse response to a desired global response. The authors have developed computationally efficient methods for PET systems with shift-invariant geometric responses. They demonstrate the increased spatial uniformity of this new method versus conventional quadratic regularization schemes in simulated PET thorax scans.","Spatial resolution,
Image reconstruction,
Positron emission tomography,
Smoothing methods,
Image resolution,
Convergence,
Maximum likelihood estimation,
Anisotropic magnetoresistance,
Lymph nodes,
Computational modeling"
List-mode maximum-likelihood reconstruction applied to positron emission mammography (PEM) with irregular sampling,"Presents a preliminary study of list-mode likelihood reconstruction of images for a rectangular positron emission tomograph (PET) specifically designed to image the human breast. The prospective device consists of small arrays of scintillation crystals for which depth of interaction is estimated. Except in very rare instances, the number of annihilation events detected is expected to be far less than the number of distinguishable events. If one were to histogram the acquired data, most histogram bins would remain vacant. Therefore, it seems natural to investigate the efficacy of processing events one at a time rather than processing the data in histogram format. From a reconstruction perspective, the new tomograph presents a challenge in that the rectangular geometry leads to irregular radial and angular sampling, and the field of view extends completely to the detector faces. Simulations are presented that indicate that the proposed tomograph can detect 8-mm-diameter spherical tumors with a tumor-to-background tracer density ratio of 3:1 using realistic image acquisition parameters. Spherical tumors of 4-mm diameter are near the limit of detectability with the image acquisition parameters used. Expressions are presented to estimate the loss of image contrast due to Compton scattering.","Radioactive decay,
Mammography,
Sampling methods,
Image reconstruction,
Histograms,
Image sampling,
Face detection,
Neoplasms,
Maximum likelihood detection,
Maximum likelihood estimation"
Efficient matching of pictorial structures,"A pictorial structure is a collection of parts arranged in a deformable configuration. Each part is represented using a simple appearance model and the deformable configuration is represented by spring-like connections between pairs of parts. While pictorial structures were introduced a number of years ago, they have not been broadly applied to matching and recognition problems. This has been due in part to the computational difficulty of matching pictorial structures to images. In this paper we present an efficient algorithm for finding the best global match of a pictorial stucture to an image. With this improved algorithm, pictorial structures provide a practical and powerful framework for quantitative descriptions of objects and scenes, and are suitable for many generic image recognition problems. We illustrate the approach using simple models of a person and a car.","Layout,
Image recognition,
Integrated circuit modeling,
Wheels,
Artificial intelligence,
Laboratories,
Computer science,
Deformable models,
Bayesian methods,
Contracts"
Application-layer anycasting: a server selection architecture and use in a replicated Web service,"Server replication improves the ability of a service to handle a large number of clients. One of the important factors in the efficient utilization of replicated servers is the ability to direct client requests to the ""best"" server, according to some optimality criteria. In the anycasting communication paradigm, a sender communicates with a receiver chosen from an anycast group of equivalent receivers. As such, anycasting is well suited to the problem of directing clients to replicated servers. This paper examines the definition and support of the anycasting paradigm at the application-layer, providing a service that uses an anycast resolver to map an anycast domain name and a selection criteria into an IP address. By realizing anycasting in the application-layer, we achieve flexibility in the optimization criteria and ease the deployment of the service. As a case study, we examine the performance of our system for a key service: replicated Web servers. To this end, we develop an approach for estimating the response time that a client will experience when accessing given servers. Such information is maintained in the anycast resolver that clients query to obtain the identity of the server with the best estimated response time. Our performance collection technique combines server push with resolver probes to estimate the expected response time without undue overhead. Our experiments show that selecting a server using our architecture and estimation technique can improve the client response time by a factor of two over nearest server selection and by a factor of four over random server selection.","Service oriented architecture,
Web services,
Network servers,
Web server,
Delay,
Computer science,
Probes,
Web and internet services,
Maintenance,
Quality of service"
Selective buck-boost equalizer for series battery packs,"To maximize the capacity and reliability of a series connected battery pack, a new selective equalizer developed from the earlier ramp equalizer is proposed. A set of bipolar junction transistors (BJTs) controlled by a microcontroller is used to route equalization current to the lowest voltage batteries. Since only the lowest voltage batteries are connected to the equalizer, the need for uniform transformer leakage inductance is avoided, and a lower power level can be used since no excess current flows to the other batteries. An equalization experiment has shown that a 37 W selective equalizer had a slightly better effect on a 24-battery pack than a 63 W ramp equalizer.","Equalizers,
Batteries,
Hybrid electric vehicles,
Low voltage,
Electrodes,
Circuit testing,
Voltage control,
Microcontrollers,
Inductance,
Computer science"
Multiple-model adaptive estimation using a residual correlation Kalman filter bank,"We propose a modified multiple model adaptive estimation (MMAE) algorithm that uses the time correlation of the Kalman filter residuals, in place of their scaled magnitude, to assign conditional probabilities for each of the modeled hypotheses. This modified algorithm, denoted the residual correlation Kalman filter bank (RCKFB), uses the magnitude of an estimate of the correlation of the residual with a slightly modified version of the usual MMAE hypothesis testing algorithm to assign the conditional probabilities to the various hypotheses that are modeled in the Kalman filter bank within the MMAE. This concept is used to detect flight control actuator failures, where the existence of a single frequency sinusoid (which is highly time correlated) in the residual of an elemental filter within an MMAE is indicative of that filter having the wrong actuator failure status hypothesis. This technique results in a delay in detecting the flight control actuator failure because several samples of the residual must be collected before the residual correlation can be estimated. However, it allows a significant reduction of the amplitude of the required system inputs for exciting the various system modes to enhance identifiability, to the point where they may possibly be subliminal, so as not to be objectionable to the pilot and passengers.","Adaptive estimation,
State estimation,
Testing,
Actuators,
Aerospace control,
History,
Filter bank,
Delay estimation,
Frequency,
Computer science"
Corner block list: an effective and efficient topological representation of non-slicing floorplan,"In this paper, a corner block list-a new efficient topological representation for non-slicing floorplan is proposed with applications to VLSI floorplan and building block placement. Given a corner block list, it takes only linear time to construct the floorplan. Unlike the O-tree structure, which determines the exact floorplan based on given block sizes, corner block list defines the floorplan independent of the block sizes. Thus, the structure is better suited for floorplan optimization with various size configurations of each block. Based on this new structure and the simulated annealing technique, an efficient floorplan algorithm is given. Soft blocks and the aspect ratio of the chip are taken into account in the simulated annealing process. The experimental results demonstrate the algorithm is quite promising.","Very large scale integration,
Computer science,
Simulated annealing,
Circuit simulation,
Binary trees,
Application software"
A quality of service architecture that combines resource reservation and application adaptation,"Reservation and adaptation are two well-known and effective techniques for enhancing the end-to-end performance of network applications. However, both techniques also have limitations, particularly when dealing with high-bandwidth, dynamic flows: fixed-capability reservations tend to be wasteful of resources and hinder graceful degradation in the face of congestion, while adaptive techniques fail when congestion becomes excessive. We propose an approach to quality of service (QoS) that overcomes these difficulties by combining features of reservations and adaptation. In this approach, a combination of online control interfaces for resource management, a sensor permitting online monitoring, and decision procedures embedded in resources enable a rich variety of dynamic feedback interactions between applications and resources. We describe a QoS architecture, GARA, that has been extended to support these mechanisms, and use three examples of application-level adaptive strategies to show how this framework can permit applications to adapt both their resource requests and behavior in response to online sensor information.","Quality of service,
Resource management,
Degradation,
Application software,
Mathematics,
Computer science,
Condition monitoring,
Feedback,
Sensor phenomena and characterization,
Bandwidth"
A performance comparison study of ad hoc wireless multicast protocols,"In this paper we investigate the performance of multicast routing protocols in wireless mobile ad hoc networks. An ad hoc network is composed of mobile nodes without the presence of a wired support infrastructure. In this environment, routing/multicasting protocols are faced with the challenge of producing multihop routes under host mobility and bandwidth constraints. In recent years, a number of new multicast protocols of different styles have been proposed for ad hoc networks. However, systematic performance evaluations and comparative analysis of these protocols in a common realistic environment has not yet been performed. In this study, we simulate a set of representative wireless ad hoc multicast protocols and evaluate them in various network scenarios. The relative strengths, weaknesses, and applicability of each multicast protocol to diverse situations are studied and discussed.","Multicast protocols,
Ad hoc networks,
Routing protocols,
Performance analysis,
Network topology,
Analytical models,
Access protocols,
Laboratories,
Computer science,
Wireless application protocol"
Unmatched projector/backprojector pairs in an iterative reconstruction algorithm,"Computational burden is a major concern when an iterative algorithm is used to reconstruct a three-dimensional (3-D) image with attenuation, detector response, and scatter corrections. Most of the computation time is spent executing the projector and backprojector of an iterative algorithm. Usually, the projector and the backprojector are transposed operators of each other. The projector should model the imaging geometry and physics as accurately as possible. Some researchers have used backprojectors that are computationally less expensive than the projectors to reduce computation time. This paper points out that valid backprojectors should satisfy a condition that the projector/backprojector matrix must not contain negative eigenvalues. This paper also investigates the effects when unmatched projector/backprojector pairs are used.","Reconstruction algorithms,
Iterative algorithms,
Image reconstruction,
Attenuation,
Detectors,
Scattering,
Solid modeling,
Geometry,
Physics,
Eigenvalues and eigenfunctions"
Fast delineation and visualization of vessels in 3-D angiographic images,"A method is presented which aids the clinician in obtaining quantitative measures and a three-dimensional (3-D) representation of vessels from 3-D angiographic data with a minimum of user interaction. Based on two user defined starting points, an iterative procedure tracks the central vessel axis. During the tracking process, the minimum diameter and a surface rendering of the vessels are computed, allowing for interactive inspection of the vasculature. Applications of the method to CTA, contrast enhanced (CE)-MRA and phase contrast (PC)-MRA images of the abdomen are shown. In all applications, a long stretch of vessels with varying width is tracked, delineated, and visualized, in less than 10 s on a standard clinical workstation.","Data visualization,
Rendering (computer graphics),
Inspection,
Surgery,
Monitoring,
Abdomen,
Workstations,
Diseases,
Data mining,
Telematics"
Design and evaluation of a system for microscope-assisted guided interventions (MAGI),"The problem of providing surgical navigation using image overlays on the operative scene can be split into four main tasks-calibration of the optical system; registration of preoperative images to the patient; system and patient tracking, and display using a suitable visualization scheme. To achieve a convincing result in the magnified microscope view a very high alignment accuracy is required. The authors have simulated an entire image overlay system to establish the most significant sources of error and improved each of the stages involved. The microscope calibration process has been automated. The authors have introduced bone-implanted markers for registration and incorporated a locking acrylic dental stent (LADS) for patient tracking. The LADS can also provide a less-invasive registration device with mean target error of 0.7 mm in volunteer experiments. These improvements have significantly increased the alignment accuracy of the authors' overlays. Phantom accuracy is 0.3-0.5 mm and clinical overlay errors were 0.5-1.0 mm on the bone fiducials and 0.5-4 mm on target structures. The authors have improved the graphical representation of the stereo overlays. The resulting system provides three-dimensional surgical navigation for microscope-issisted guided interventions (MAGI).",
Enhanced 3-D-reconstruction algorithm for C-arm systems suitable for interventional procedures,"Increasingly, three dimensional (3-D) imaging technologies are used in medical diagnosis, for therapy planning, and during interventional procedures. The authors describe the possibilities of fast 3-D-reconstruction of high-contrast objects with high spatial resolution from only a small series of two-dimensional (2-D) planar radiographs. The special problems arising from the intended use of an open, mechanically unstable C-arm system are discussed. For the description of the irregular sampling geometry, homogeneous coordinates are used thoroughly. The well-known Feldkamp algorithm is modified to incorporate corresponding projection matrices without any decomposition into intrinsic and extrinsic parameters. Some approximations to speed up the whole reconstruction procedure and the tradeoff between image quality and computation time are also considered. Using standard hardware the reconstruction of a 256/sup 3/ cube is now possible within a few minutes, a time that is acceptable during interventions. Examples for cranial vessel imaging from some clinical test installations will be shown as well as promising results for bone imaging with a laboratory C-arm system.","Image reconstruction,
High-resolution imaging,
Medical diagnosis,
Medical treatment,
Technology planning,
Spatial resolution,
Two dimensional displays,
Diagnostic radiography,
Image sampling,
Geometry"
"Mixture of experts for classification of gender, ethnic origin, and pose of human faces","We describe the application of mixtures of experts on gender and ethnic classification of human faces, and pose classification, and show their feasibility on the FERET database of facial images. The mixture of experts is implemented using the ""divide and conquer"" modularity principle with respect to the granularity and/or the locality of information. The mixture of experts consists of ensembles of radial basis functions (RBFs). Inductive decision trees (DTs) and support vector machines (SVMs) implement the ""gating network"" components for deciding which of the experts should be used to determine the classification output and to restrict the support of the input space. Both the ensemble of RBF's (ERBF) and SVM use the RBF kernel (""expert"") for gating the inputs. Our experimental results yield an average accuracy rate of 96% on gender classification and 92% on ethnic classification using the ERBF/DT approach from frontal face images, while the SVM yield 100% on pose classification.","Humans,
Support vector machines,
Support vector machine classification,
Face detection,
Image databases,
Classification tree analysis,
Computer science,
Kernel,
Laboratories,
Decision trees"
Stochastic models for the Web graph,"The Web may be viewed as a directed graph each of whose vertices is a static HTML Web page, and each of whose edges corresponds to a hyperlink from one Web page to another. We propose and analyze random graph models inspired by a series of empirical observations on the Web. Our graph models differ from the traditional G/sub n,p/ models in two ways: 1. Independently chosen edges do not result in the statistics (degree distributions, clique multitudes) observed on the Web. Thus, edges in our model are statistically dependent on each other. 2. Our model introduces new vertices in the graph as time evolves. This captures the fact that the Web is changing with time. Our results are two fold: we show that graphs generated using our model exhibit the statistics observed on the Web graph, and additionally, that natural graph models proposed earlier do not exhibit them. This remains true even when these earlier models are generalized to account for the arrival of vertices over time. In particular, the sparse random graphs in our models exhibit properties that do not arise in far denser random graphs generated by Erdos-Renyi models.","Stochastic processes,
Web pages,
HTML,
Statistics,
Computer science,
Couplings,
Predictive models"
Replacement policies for a proxy cache,"In this paper, we analyze access traces to a Web proxy, looking at statistical parameters to be used in the design of a replacement policy for documents held in the cache. In the first part of this paper, we present a number of properties of the lifetime and statistics of access to documents, derived from two large trace sets coming from very different proxies and spanning over time intervals of up to five months. In the second part, we propose a novel replacement policy, called LRV, which selects for replacement the document with the lowest relative value among those in cache. In LRV, the value of a document is computed adaptively based on information readily available to the proxy server. The algorithm has no hardwired constants, and the computations associated with the replacement policy require only a small constant time. We show how LRV outperforms least recently used (LRU) and other policies and can significantly improve the performance of the cache, especially for a small one.","Statistics,
Network servers,
Communication networks,
Delay,
Telecommunication traffic,
Computer science,
Cache memory,
Computer architecture,
Operating systems"
Case study of feature location using dependence graph,"Software change requests are often formulated as requests to modify or to add a specific feature or concept. To implement these changes, the features or concepts must be located in the code. We describe the scenarios of the feature and concept location. The scenarios utilize a computer-assisted search of software dependence graph. Scenarios are demonstrated by a case study of NCSA Mosaic source code.","Computer aided software engineering,
Programming profession,
Software systems,
Feedback,
Computer science,
Tellurium,
Identity-based encryption,
Software maintenance,
Natural languages,
Terminology"
Predictive schemes for handoff prioritization in cellular networks based on mobile positioning,"We propose and evaluate new schemes for channel reservation motivated by the rapidly evolving technology of mobile positioning. The schemes, called predictive channel reservation (PCR), work by sending reservation requests to neighboring cells based on extrapolating the motion of mobile stations (MSs). A number of design enhancements are incorporated to minimize the effect of false reservations and to improve the throughput of the cellular system. These enhancements include: (1) reservation pooling; (2) queuing of reservation requests; (3) hybrid approach for integrating guard channels (GCs); and (4) using a threshold distance (TD) to control the timing of reservation requests. The design enhancements have produced a set of highly efficient schemes that achieve significant reduction in handoff blocking rates while only incurring remarkably small increases in the new call blocking rates. The PCR approach has also been used to solve the MINBLOCK optimization problem and has given significant improvement over the fractional guard channel (FGC) protocol. Detailed performance results of the different variations of the PCR scheme and comparisons with conventional channel reservation schemes are presented. An analytical Markov model for the hybrid predictive version of the scheme is developed and its applicability and numerical results are discussed.","Intelligent networks,
Land mobile radio cellular systems,
Channel allocation,
Computer science,
Position measurement,
Protocols,
Quality of service,
Telecommunication traffic,
Transmitters,
Throughput"
Scheduling with advanced reservations,"Some computational grid applications have very large resource requirements and need simultaneous access to resources from more than one parallel computer. Current scheduling systems do not provide mechanisms to gain such simultaneous access without the help of human administrators of the computer systems. In this work, we propose and evaluate several algorithms for supporting advanced reservation of resources in supercomputing scheduling systems. These advanced reservations allow users to request resources from scheduling systems at specific times. We find that the wait times of applications submitted to the queue increases when reservations are supported and the increase depends on how reservations are supported. Further, we find that the best performance is achieved when we assume that applications can be terminated and restarted, backfilling is performed, and relatively accurate run-time predictions are used.","Grid computing,
Processor scheduling,
Application software,
Concurrent computing,
Distributed computing,
Mathematics,
Computer science,
Humans,
Computer applications,
Instruments"
Multiobjective programming using uniform design and genetic algorithm,"The notion of Pareto-optimality is one of the major approaches to multiobjective programming. While it is desirable to find more Pareto-optimal solutions, it is also desirable to find the ones scattered uniformly over the Pareto frontier in order to provide a variety of compromise solutions to the decision maker. We design a genetic algorithm for this purpose. We compose multiple fitness functions to guide the search, where each fitness function is equal to a weighted sum of the normalized objective functions and we apply an experimental design method called uniform design to select the weights. As a result, the search directions guided by these fitness functions are scattered uniformly toward the Pareto frontier in the objective space. With multiple fitness functions, we design a selection scheme to maintain a good and diverse population. In addition, we apply the uniform design to generate a good initial population and design a new crossover operator for searching the Pareto-optimal solutions. The numerical results demonstrate that the proposed algorithm can find the Pareto-optimal solutions scattered uniformly over the Pareto frontier.","Algorithm design and analysis,
Genetic algorithms,
Scattering,
Design for experiments,
Design methodology,
Computer science,
Mathematics,
Genetic mutations,
Evolution (biology)"
Unwarping of unidirectionally distorted EPI images,"Echo-planar imaging (EPI) is a fast nuclear magnetic resonance imaging (MRI) method. Unfortunately, local magnetic field inhomogeneities induced mainly by the subject's presence cause significant geometrical distortion, predominantly along the phase-encoding direction, which must be undone to allow for meaningful further processing. So far, this aspect has been too often neglected. In this paper, the authors suggest a new approach using an algorithm specifically developed for the automatic registration of distorted EPI images with corresponding anatomically correct MRI images. They model the deformation field with splines, which gives us a great deal of flexibility, while comprising the affine transform as a special case. The registration criterion is least squares. Interestingly, the complexity of its evaluation does not depend on the resolution of the control grid. The spline model gives the authors good accuracy thanks to its high approximation order. The short support of splines leads to a fast algorithm. A multiresolution approach yields robustness and additional speedup. The algorithm was tested on real as well as synthetic data, and the results were compared with a manual method. A wavelet-based Sobolev-type random deformation generator was developed for testing purposes. A blind test indicates that the proposed automatic method is faster, more reliable, and more precise than the manual one.","Magnetic resonance imaging,
Magnetic fields,
Magnetic field measurement,
Phase distortion,
Testing,
Deformable models,
Least squares methods,
Least squares approximation,
Spatial resolution,
Signal resolution"
"Minimum description length induction, Bayesianism, and Kolmogorov complexity","The relationship between the Bayesian approach and the minimum description length approach is established. We sharpen and clarify the general modeling principles minimum description length (MDL) and minimum message length (MML), abstracted as the ideal MDL principle and defined from Bayes's rule by means of Kolmogorov complexity. The basic condition under which the ideal principle should be applied is encapsulated as the fundamental inequality, which in broad terms states that the principle is valid when the data are random, relative to every contemplated hypothesis and also these hypotheses are random relative to the (universal) prior. The ideal principle states that the prior probability associated with the hypothesis should be given by the algorithmic universal probability, and the sum of the log universal probability of the model plus the log of the probability of the data given the model should be minimized. If we restrict the model class to finite sets then application of the ideal principle turns into Kolmogorov's minimal sufficient statistic. In general, we show that data compression is almost always the best strategy, both in model selection and prediction.",Probability
Emergence of Phase- and Shift-Invariant Features by Decomposition of Natural Images into Independent Feature Subspaces,"Olshausen and Field (1996) applied the principle of independence maximization by sparse coding to extract features from natural images. This leads to the emergence of oriented linear filters that have simultaneous localization in space and in frequency, thus resembling Gabor functions and simple cell receptive fields. In this article, we show that the same principle of independence maximization can explain the emergence of phase- and shift-invariant features, similar to those found in complex cells. This new kind of emergence is obtained by maximizing the independence between norms of projections on linear subspaces (instead of the independence of simple linear filter outputs). The norms of the projections on such “independent feature subspaces” then indicate the values of invariant features.",
3-D reconstruction of coronary arterial tree to optimize angiographic visualization,"Due to vessel overlap and foreshortening, multiple projections are necessary to adequately evaluate the coronary tree with arteriography. Catheter-based interventions can only be optimally performed when these visualization problems are successfully solved. The traditional method provides multiple selected views in which overlap and foreshortening are subjectively minimized based on two dimensional (2-D) projections. A pair of images acquired from routine angiographic study at arbitrary orientation using a single-plane imaging system were chosen for three-dimensional (3-D) reconstruction. After the arterial segment of interest (e.g., a single coronary stenosis or bifurcation lesion) was selected, a set of gantry angulations minimizing segment foreshortening was calculated. Multiple computer-generated projection images with minimized segment foreshortening were then used to choose views with minimal overlapped vessels relative to the segment of interest. The optimized views could then be utilized to guide subsequent angiographic acquisition and interpretation. Over 800 cases of coronary arterial trees have been reconstructed, in which more than 40 cases were performed in room during cardiac catheterization. The accuracy of 3-D length measurement was confirmed to be within an average root-mean-square (rms) 3.5% error using eight different pairs of angiograms of an intracoronary guidewire of 105-mm length with eight radiopaque markers of 15-mm interdistance. The accuracy of similarity between the additional computer-generated projections versus the actual acquired views was demonstrated with the average rms errors of 3.09 mm and 3.13 mm in 20 LCA and 20 RCA cases, respectively. The projections of the reconstructed patient-specific 3-D coronary tree model can be utilized for planning optimal clinical views: minimal overlap and foreshortening. The assessment of lesion length and diameter narrowing can be optimized in both interventional cases and studies of disease progression and regression.","Three dimensional displays,
Visualization,
Image segmentation,
Image reconstruction,
Lesions,
Computer errors,
Angiography,
Two dimensional displays,
Bifurcation,
Catheterization"
Robust image registration using log-polar transform,"This paper describes a hierarchical image registration algorithm for affine motion recovery. The algorithm estimates the affine transformation parameters necessary to register any two digital images misaligned due to rotation, scale, shear, and translation. The parameters are computed iteratively in a coarse-to-fine hierarchical framework using a variation of the Levenberg-Marquadt nonlinear least squares optimization method. This approach yields a robust solution that precisely registers images with subpixel accuracy. A log-polar registration module is introduced to accommodate arbitrary rotation angles and a wide range of scale changes. This serves to furnish a good initial estimate for the optimization-based affine registration stage. We demonstrate the hybrid algorithm on pairs of digital images subjected to large affine motion.","Robustness,
Image registration,
Registers,
Digital images,
Fourier transforms,
Least squares methods,
Optimization methods,
Parameter estimation,
Least squares approximation,
Computer science"
Fisheye state routing: a routing scheme for ad hoc wireless networks,"This paper presents a novel routing protocol for wireless ad hoc networks-fisheye state routing (FSR). FSR introduces the notion of multi-level fisheye scope to reduce routing update overhead in large networks. Nodes exchange link state entries with their neighbors with a frequency which depends on distance to destination. From link state entries, nodes construct the topology map of the entire network and compute optimal routes. Simulation experiments show that FSR is a simple, efficient and scalable routing solution in a mobile, ad hoc environment.","Wireless networks,
Network topology,
Computer science,
Routing protocols,
Ad hoc networks,
Bandwidth,
Telecommunication traffic,
Convergence,
Contracts,
Floods"
Penalized weighted least-squares image reconstruction for dual energy X-ray transmission tomography,"Presents a dual-energy (DE) transmission computed tomography (CT) reconstruction method. It is statistically motivated and features nonnegativity constraints in the density domain. A penalized weighted least squares (PWLS) objective function has been chosen to handle the non-Poisson noise added by amorphous silicon (aSi:H) detectors. A Gauss-Seidel algorithm has been used to minimize the objective function. The behavior of the method in terms of bias/standard deviation tradeoff has been compared to that of a DE method that is based on filtered back projection (FBP). The advantages of the DE PWLS method are largest for high noise and/or low flux cases. Qualitative results suggest this as well. Also, the reconstructed images of an object with opaque regions are presented. Possible applications of the method are: attenuation correction for positron emission tomography (PET) images, various quantitative computed tomography (QCT) methods such as bone mineral densitometry (BMD), and the removal of metal streak artifacts.","Image reconstruction,
X-ray imaging,
Computed tomography,
Positron emission tomography,
Reconstruction algorithms,
Least squares methods,
Amorphous silicon,
Detectors,
Gaussian processes,
Attenuation"
Multichannel CSMA with signal power-based channel selection for multihop wireless networks,"We describe a new carrier-sense multiple access (CSMA) protocol for multihop wireless networks, using multiple channels and a distributed channel selection scheme. The proposed protocol divides the available bandwidth into N channels where the transmitting station selects an appropriate channel for packet transmission. The selection criterion is based on the interference power measurements on the channels. We show via simulations that this multichannel CSMA protocol provides a higher throughput compared to its single channel counterpart by reducing the packet loss due to collisions.","Multiaccess communication,
Spread spectrum communication,
Wireless networks,
Access protocols,
Wireless application protocol,
Media Access Protocol,
Wireless sensor networks,
Computer science,
Electronic mail,
Throughput"
Detector response models for statistical iterative image reconstruction in high resolution PET,"One limitation in a practical implementation of statistical iterative image reconstruction is to compute a transition matrix accurately modeling the relationship between projection and image spaces. Detector response function (DRF) in positron emission tomography (PET) is broad and spatially-variant, leading to large transition matrices taking too much space to store. In this work, the authors investigate the effect of simpler DRF models on image quality in maximum likelihood expectation maximization reconstruction. The authors studied 6 cases of modeling projection/image relationship: tube/pixel geometric overlap with tubes centered on detector face; same as previous with tubes centered on DRF maximum; two different fixed-width Gaussian functions centered on DRF maximum weighing tube/pixel overlap; same as previous with a Gaussian of the same spectral resolution as DRF; analytic DRF based on linear attenuation of /spl gamma/-rays in detector arrays weighing tube/pixel overlap. The authors found that DRF oversimplification may affect visual image quality and image quantification dramatically, including artefact generation. They showed that analytic DRF yielded images of excellent quality for a small animal PET system with long, narrow detectors and generated a transition matrix for 2-D reconstruction that could be easily fitted into the memory of current stand-alone computers.",
Object localization and border detection criteria design in edge-based image segmentation: automated learning from examples,"This paper provides methodology for fully automated model-based image segmentation. All information necessary to perform image segmentation is automatically derived from a training set that is presented in a form of segmentation examples. The training set is used to construct two models representing the objects-shape model and border appearance model. A two-step approach to image segmentation is reported. In the first step, an approximate location of the object of interest is determined. In the second step, accurate border segmentation is performed. The shape-variant Hough transform method was developed that provides robust object localization automatically. It finds objects of arbitrary shape, rotation, or scaling and can handle object variability. The border appearance model was developed to automatically design cost functions that can be used in the segmentation criteria of edge-based segmentation methods. The authors' method was tested in five different segmentation tasks that included 489 objects to be segmented. The final segmentation was compared to manually defined borders with good results [rms errors in pixels: 1.2 (cerebellum), 1.1 (corpus callosum), 1.5 (vertebrae), 1.4 (epicardial), and 1.6 (endocardial) borders]. Two major problems of the state-of-the-art edge-based image segmentation algorithms were addressed: strong dependency on a close-to-target initialization, and necessity for manual redesign of segmentation criteria whenever new segmentation problem is encountered.","Image edge detection,
Object detection,
Image segmentation,
Active shape model,
Cost function,
Robustness,
Image analysis,
Algorithm design and analysis,
Dynamic programming,
Cities and towns"
Cryptanalysis of RSA with private key d less than N/sup 0.292/,We show that if the private exponent d used in the RSA (Rivest-Shamir-Adleman (1978)) public-key cryptosystem is less than N/sup 0.292/ then the system is insecure. This is the first improvement over an old result of Wiener (1990) showing that when d is less than N/sup 0.25/ the RSA system is insecure. We hope our approach can be used to eventually improve the bound to d less than N/sup 0.5/.,Public key cryptography
"The what, how, and why of wavelet shrinkage denoising","Wavelet shrinkage denoising provides a novel method of reducing noise in signals. The author demonstrates 1D and 2D examples, tests the performance of various ideal and practical Fourier- and wavelet-based denoising procedures, and makes recommendations for practitioners. It is is concluded that it is unlikely that one particular wavelet shrinkage denoising procedure will be suitable, no less optimal, for all practical problems. However, it is likely that there will be many practical problems, for which after appropriate experimentation, wavelet-based denoising with either hard or soft thresholding proves to be the most effective procedure. Using wavelet-based denoising of the log-periodogram to estimate the power spectrum might prove to be one such important application with great promise for further development.","Noise reduction,
Wavelet transforms,
Wavelet domain,
Smoothing methods,
Fourier transforms,
Frequency,
Parameter estimation,
Testing,
Filters,
Data engineering"
Certificate revocation and certificate update,"We present a solution for the problem of certificate revocation. This solution represents certificate revocation lists by authenticated dictionaries that support: (1) efficient verification whether a certificate is in the list or not and (2) efficient updates (adding/removing certificates from the list). The suggested solution gains in scalability, communication costs, robustness to parameter changes, and update rate. Comparisons to the following solutions (and variants) are included: ""traditional"" certificate revocation lists (CRLs), Micali's (see Tech. Memo MIT/LCS/TM-542b, 1996) certificate revocation system (CRS), and Kocher's (see Financial Cryptography-FC'98 Lecture Notes in Computer Science. Berlin: Springer-Verlag, 1998, vol.1465, p.172-7) certificate revocation trees (CRT). We also consider a scenario in which certificates are not revoked, but frequently issued for short-term periods. Based on the authenticated dictionary scheme, a certificate update scheme is presented in which all certificates are updated by a common message. The suggested solutions for certificate revocation and certificate update problems are better than current solutions with respect to communication costs, update rate, and robustness to changes in parameters, and are compatible, e.g., with X.500 certificates.",
Context-based lossless interband compression-extending CALIC,"This paper proposes an interband version of CALIC (context-based, adaptive, lossless image codec) which represents one of the best performing, practical and general purpose lossless image coding techniques known today. Interband coding techniques are needed for effective compression of multispectral images like color images and remotely sensed images. It is demonstrated that CALIC's techniques of context modeling of DPCM errors lend themselves easily to modeling of higher-order interband correlations that cannot be exploited by simple interband linear predictors alone. The proposed interband CALIC exploits both interband and intraband statistical redundancies, and obtains significant compression gains over its intrahand counterpart. On some types of multispectral images, interband CALIC can lead to a reduction in bit rate of more than 20% as compared to intraband CALIC. Interband CALIC only incurs a modest increase in computational cost as compared to intraband CALIC.","Image coding,
Multispectral imaging,
Context modeling,
Image reconstruction,
Ice,
Codecs,
Color,
Humans,
Computer science,
Remote sensing"
Flat panel detector-based cone-beam volume CT angiography imaging: system evaluation,"Preliminary evaluation of recently developed large-area flat panel detectors (FPDs) indicates that FPDs have some potential advantages: compactness, absence of geometric distortion and veiling glare with the benefits of high resolution, high detective quantum efficiency (DQE), high frame rate and high dynamic range, small image lag (<1%), and excellent linearity (/spl sim/1%). The advantages of the new FPD make it a promising candidate for cone-beam volume computed tomography (CT) angiography (CBVCTA) imaging. The purpose of this study is to characterize a prototype FPD-based imaging system for CBVCTA applications. A prototype FPD-based CBVCTA imaging system has been designed and constructed around a modified GE 8800 CT scanner. This system is evaluated for a CBVCTA imaging task in the head and neck using four phantoms and a frozen rat. The system is first characterized in terms of linearity and dynamic range of the detector. Then, the optimal selection of kVps for CBVCTA is determined and the effect of image lag and scatter on the image quality of the CBVCTA system is evaluated. Next, low-contrast resolution and high-contrast spatial resolution are measured. Finally, the example reconstruction images of a frozen rat are presented. The results indicate that the FPD-based CBVCT can achieve 2.75-1p/mm spatial resolution at 0% modulation transfer function (MTF) and provide more than enough low-contrast resolution for intravenous CBVCTA imaging in the head and neck with clinically acceptable entrance exposure level. The results also suggest that to use an FPD for large cone-angle applications, such as body angiography, further investigations are required.",
Detection and location of people in video images using adaptive fusion of color and edge information,"A new method of finding people in video images is presented. The detection is based on a novel background modeling and subtraction approach which uses both color and edge information. We introduce confidence maps gray-scale images whose intensity is a function of confidence that a pixel has changed - to fuse intermediate results and represent the results of background subtraction. The latter is used to delineate a person's body by guiding contour collection to segment the person from the background. The method is tolerant to scene clutter, slow illumination changes, and camera noise, and runs in near real time on a standard platform.","Image edge detection,
Layout,
Pixel,
Lighting,
Cameras,
Educational institutions,
Computer science,
Fuses,
Image segmentation,
Standards development"
Topological analysis of trabecular bone MR images,"Recently, imaging techniques have become available which permit nondestructive analysis of the three-dimensional (3-D) architecture of trabecular bone (TB), which forms a network of interconnected plates and rods. Most osteoporotic fractures occur at locations rich in TB, which has spurred the search for architectural parameters as determinants of bone strength. Here, the authors present a new approach to quantitative characterization of the 3-D microarchitecture of TB, based on digital topology. The method classifies each voxel of the 3-D structure based on the connectivity information of neighboring voxels. Following conversion of the 3-D digital image to a skeletonized surface representation containing only one-dimensional (1-D) and two-dimensional (2-D) structures, each voxel is classified as a curve, surface, or junction. The method has been validated by means of synthesized images and has subsequently been applied to TB images from the human wrist. The topological parameters were found to predict Young's modulus (YM) for uniaxial loading, specifically, the surface-to-curve ratio was found to be the single strongest predictor of YM (r/sup 2/=0.69). Finally, the method has been applied to TB images from a group of patients showing very large variations in topological parameters that parallel much smaller changes in bone volume fraction (BVF).","Image analysis,
Cancellous bone,
Network topology,
Microarchitecture,
Wrist,
Magnetic resonance imaging,
Lattices,
In vivo,
Radiology,
Material properties"
What knowledge is important to a software professional?,"Efforts to develop licensing requirements, curricula, or training programs for software professionals should consider the experience of the practitioners who actually perform the work. We surveyed software professionals representing a wide variety of industries, job functions, and countries to learn which educational topics have proved most important to them in their careers and to identify the topics for which their education or current knowledge could be improved.","Educational institutions,
Software engineering,
Licenses,
Engineering profession,
Computer science education,
Computer science,
Educational programs,
Design engineering,
Knowledge engineering,
Computer industry"
Task execution time modeling for heterogeneous computing systems,"A distributed heterogeneous computing (HC) system consists of diversely capable machines harnessed together to execute a set of tasks that vary in their computational requirements. Heuristics are needed to map (match and schedule) tasks onto machines in an HC system so as to optimize some figure of merit. This paper characterizes a simulated HC environment by using the expected execution times of the tasks that arrive in the system onto the different machines present in the system. This information is arranged in an ""expected time to compute"" (ETC) matrix as a model of the given HC system, where the entry (i, j) is the expected execution time of task i on machine j. This model is needed to simulate different HC environments to allow testing of relative performance of different mapping heuristics under different circumstances. In particular the ETC model is used to express the heterogeneity among the runtimes of the tasks to be executed, and among the machines in the HC system. An existing range-based technique to generate ETC matrices is described. A coefficient-of-variation based technique to generate ETC matrices is proposed, and compared with the range-based technique. The coefficient-of-variation-based ETC generation method provides a greater control over the spread of values (i.e., heterogeneity) in any given row or column of the ETC matrix than the range-based method.","Computational modeling,
Testing,
Ice,
Distributed computing,
Subcontracting,
Computer science,
Operating systems,
Runtime,
Indium tin oxide"
Eliciting security requirements by misuse cases,"Use case diagrams (L. Jacobson et al., 1992) have proven quite helpful in requirements engineering, both for eliciting requirements and getting a better overview of requirements already stated. However, not all kinds of requirements are equally well supported by use case diagrams. They are good for functional requirements, but poorer at e.g., security requirements, which often concentrate on what should not happen in the system. With the advent of e- and m-commerce applications, security requirements are growing in importance, also for quite simple applications where a short lead time is important. Thus, it would be interesting to look into the possibility for applying use cases on this arena. The paper suggests how this can be done, extending the diagrams with misuse cases. This new construct makes it possible to represent actions that the system should prevent, together with those actions which it should support.","Computer aided software engineering,
Information security,
User centered design,
Computer crime,
Information science,
Programming,
User interfaces,
Protection,
Data security,
Resists"
Comments on the performance of measurement-based admission control algorithms,"Relaxed real time services that do not provide guaranteed loss rates or delay bounds are of considerable interest in the Internet, since these services can achieve higher utilization than hard real time services while still providing adequate service to adaptive real-time applications. Achieving this higher level of utilization depends on an admission control algorithm that does not rely on worst-case bounds to guide its admission decisions. Measurement-based admission control is one such approach, and several measurement-based admission control algorithms have been proposed in the literature. In this paper, we use simulations to compare the performance of several of these algorithms. We find that all of them achieve nearly the same utilization for a given packet loss rate, and that none of them are capable of accurately meeting loss targets.","Admission control,
Delay,
Communication system traffic control,
Loss measurement,
Web and internet services,
Engineering profession,
Computer science,
Programmable control,
Adaptive control,
Performance loss"
Load-balancing clusters in wireless ad hoc networks,"Ad hoc networks consist of a set of identical nodes that move freely and independently and communicate with other node via wireless links. Such networks may be logically represented as a set of clusters by grouping together nodes that are in close proximity with one another. Clusterheads form a virtual backbone and may be used to route packets for nodes in their cluster. Nodes are assumed to have non-deterministic mobility pattern. Clusters are formed by diffusing node identities along the wireless links. Different heuristics employ different policies to elect clusterheads. Several of these policies are biased in favor of some nodes. As a result, these nodes shoulder greater responsibility and may deplete their energy faster, causing them to drop out of the network. Therefore, there is a need for load-balancing among clusterheads to allow all nodes the opportunity to serve as a clusterhead. We propose a load-balancing heuristic to extend the life of a clusterhead to the maximum budget before allowing the clusterhead to retire and give way to another node. This helps to evenly distribute the responsibility of acting as clusterheads among all nodes. Thus, the heuristic ensures fairness and stability. Simulation experiments demonstrate that the proposed heuristic does provide longer clusterhead durations than with no load-balancing.","Intelligent networks,
Ad hoc networks,
Spine,
Mobile communication,
Batteries,
Routing,
Time division multiple access,
Ambient intelligence,
Computer science,
Stability"
Semiautomatic 3-D image registration as applied to interventional MRI liver cancer treatment,"The authors evaluated semiautomatic, voxel-based registration methods for a new application, the assessment and optimization of interventional magnetic resonance imaging (I-MRI) guided thermal ablation of liver cancer. The abdominal images acquired on a low-field-strength, open I-MRI system contain noise, motion artifacts, and tissue deformation. Dissimilar images can be obtained as a result of different MRI acquisition techniques and/or changes induced by treatments. These features challenge a registration algorithm. The authors evaluated one manual and four automated methods on clinical images acquired before treatment, immediately following treatment, and during several follow-up studies. Images were T2-weighted, T1-weighted Gd-DTPA enhanced, T1-weighted, and short-inversion-time inversion recovery (STIR). Registration accuracy was estimated from distances between anatomical landmarks. Mutual information gave better results than entropy, correlation, and variance of gray-scale ratio. Preprocessing steps such as masking and an initialization method that used two-dimensional (2-D) registration to obtain initial transformation estimates were crucial. With proper preprocessing, automatic registration was successful with all image pairs having reasonable image quality. A registration accuracy of /spl ap/3 mm was achieved with both manual and mutual information methods. Despite motion and deformation in the liver, mutual information registration is sufficiently accurate and robust for useful applications in I-MRT thermal ablation therapy.","Image registration,
Magnetic resonance imaging,
Liver,
Cancer,
Mutual information,
Optimization methods,
Abdomen,
Magnetic noise,
Entropy,
Gray-scale"
A scalable approach to thread-level speculation,"While architects understand how to build cost-effective parallel machines across a wide spectrum of machine sizes (ranging from within a single chip to large-scale servers), the real challenge is how to easily create parallel software to effectively exploit all of this raw performance potential. One promising technique for overcoming this problem is Thread-Level Speculation (TLS), which enables the compiler to optimistically create parallel threads despite uncertainty as to whether those threads are actually independent. In this paper we propose and evaluate a design for supporting TLS that seamlessly scales to any machine size because it is a straightforward extension of writeback invalidation-based cache coherence (which itself scales both up and down). Our experimental results demonstrate that our scheme performs well on both single-chip multiprocessors and on larger-scale machines where communication latencies are twenty times larger.","Yarn,
Parallel processing,
Permission,
Runtime,
Computer science,
Parallel machines,
Large-scale systems,
Software performance,
Optimizing compilers,
Uncertainty"
Intelligent CFAR processor based on data variability,"An intelligent constant false alarm rate (CFAR) processor to perform adaptive threshold target detection is presented. It employs a composite approach based on the well-known cell averaging CFAR (CA-CFAR), smallest of CFAR (SO-CFAR), and greatest of CFAR (GO-CFAR) processors. Data in the reference window is used to compute a second-order statistic called the variability index (VI) and the ratio of the means of the leading and lagging windows. Based on these statistics, the VI-CFAR dynamically tailors the background estimation algorithm. The VI-CFAR processor provides low loss CFAR performance in a homogeneous environment and also performs robustly in nonhomogeneous environments including multiple targets and extended clutter edges.","Clutter,
Working environment noise,
Radar detection,
Object detection,
Performance loss,
Testing,
Statistics,
Robustness,
Computer science,
Background noise"
M-PAES: a memetic algorithm for multiobjective optimization,"A memetic algorithm for tackling multiobjective optimization problems is presented. The algorithm employs the proven local search strategy used in the Pareto archived evolution strategy (PAES) and combines it with the use of a population and recombination. Verification of the new M-PAES (memetic PAES) algorithm is carried out by testing it on a set of multiobjective 0/1 knapsack problems. On each problem instance, a comparison is made between the new memetic algorithm, the (1+1)-PAES local searcher, and the strength Pareto evolutionary algorithm (SPEA) of E. Zitzler and L. Thiele (1998, 1999).","Testing,
Evolutionary computation,
Convergence,
Simulated annealing,
Computer science,
Cybernetics,
Genetic algorithms,
Decision making,
Operations research,
Aggregates"
What is evolutionary computation?,"Taking a page from Darwin's 'On the origin of the species', computer scientists have found ways to evolve solutions to complex problems. Harnessing the evolutionary process within a computer provides a means for addressing complex engineering problems-ones involving chaotic disturbances, randomness, and complex nonlinear dynamics-that traditional algorithms have been unable to conquer. Indeed, the field of evolutionary computation is one of the fastest growing areas of computer science and engineering for just this reason; it is addressing many problems that were previously beyond reach, such as rapid design of medicines, flexible solutions to supply-chain management problems, and rapid analysis of battlefield tactics for defense. Potentially, the field may fulfil the dream of artificial intelligence: a computer that can learn on its own and become an expert in any chosen area.","Evolutionary computation,
Evolution (biology),
Cost function,
Sea measurements,
Joining processes,
Organisms,
Accidents,
Chaos,
Heuristic algorithms,
Computer science"
Understanding the performance of TCP pacing,"Many researchers have observed that TCP's congestion control mechanisms can lead to bursty traffic flows on modern high-speed networks, with a negative impact on overall network efficiency. A proposed solution to this problem is to evenly space, or ""pace"", data sent into the network over an entire round-trip time, so that data is not sent in a burst. In this paper, we quantitatively evaluate this approach. Pacing offers better fairness, throughput, and lower drop rates in some situations. However, we show that contrary to intuition, pacing often has significantly worse throughput than regular TCP because it is susceptible to synchronized losses and it delays congestion signals. We propose and evaluate approaches for eliminating this effect.",
Gray-scale skeletonization of small vessels in magnetic resonance angiography,"Interpretation of magnetic resonance angiography (MRA) is problematic due to complexities of vascular shape and to artifacts such as the partial volume effect. The authors present new methods to assist in the interpretation of MRA. These include methods for detection of vessel paths and for determination of branching patterns of vascular trees. They are based on the ordered region growing (ORG) algorithm that represents the image as an acyclic graph, which can be reduced to a skeleton by specifying vessel endpoints or by a pruning process. Ambiguities in the vessel branching due to vessel overlap are effectively resolved by heuristic methods that incorporate a priori knowledge of bifurcation spacing. Vessel paths are detected at interactive speeds on a 500-MHz processor using vessel endpoints. These methods apply best to smaller vessels where the image intensity peaks at the center of the lumen which, for the abdominal MRA, includes vessels whose diameter is less than 1 cm.","Gray-scale,
Magnetic resonance,
Angiography,
Tree graphs,
Visualization,
Shape,
Abdomen,
Arteries,
Anatomical structure,
Image segmentation"
Bandera: extracting finite-state models from Java source code,"Finite-state verification techniques, such as model checking, have shown promise as a cost-effective means for finding defects in hardware designs. To date, the application of these techniques to software has been hindered by several obstacles. Chief among these is the problem of constructing a finite-state model that approximates the executable behavior of the software system of interest. Current best-practice involves hand construction of models which is expensive (prohibitive for all but the smallest systems), prone to errors (which can result in misleading verification results), and difficult to optimize (which is necessary to combat the exponential complexity of verification algorithms). The authors describe an integrated collection of program analysis and transformation components, called Bandera, that enables the automatic extraction of safe, compact finite-state models from program source code. Bandera takes as input Java source code and generates a program model in the input language of one of several existing verification tools; Bandera also maps verifier outputs back to the original source code. We discuss the major components of Bandera and give an overview of how it can be used to model check correctness properties of Java programs.","Java,
Hardware,
Explosions,
Data mining,
Computer languages,
Mathematical model,
Computer science,
Logic,
Humans,
Manufacturing"
A comparison of predictive measures of problem difficulty in evolutionary algorithms,"This paper studies a number of predictive measures of problem difficulty, among which epistasis variance and fitness distance correlation are the most widely known. Our approach is based on comparing the reference class of a measure to a number of known easy function classes. First, we generalize the reference classes of fitness distance correlation and epistasis variance, and construct a new predictive measure that is insensitive to nonlinear fitness scaling. We then investigate the relations between the reference classes of the measures and a number of intuitively easy classes. We also point out the need to further identify which functions are easy for a given class of evolutionary algorithms in order to design more efficient hardness indicators for them. We finally restrict attention to the genetic algorithm (GA), and consider both GA-easy and GA-hard fitness functions, and give experimental evidence that the values of the measures, based on random samples, can be completely unreliable and entirely uncorrelated to the convergence quality and convergence speed of GA instances using either proportional or ranking selection.","Evolutionary computation,
Algorithm design and analysis,
Genetic algorithms,
Convergence,
Velocity measurement,
Mathematics,
Computer science,
Time measurement"
Towards a taxonomy of software connectors,"Software systems of today are frequently composed of prefabricated, heterogeneous components that provide complex functionality and engage in complex interactions. Existing research on component based development has mostly focused on component structure, interfaces, and functionality. Recently, software architecture has emerged as an area that also places significant importance on component interactions, embodied in the notion of software connectors. However, the current level of understanding and support for connectors has been insufficient. This has resulted in their inconsistent treatment and a notable lack of understanding of what the fundamental building blocks of software interaction are and how they can be composed into more complex interactions. The paper attempts to address this problem. It presents a comprehensive classification framework and taxonomy of software connectors. The taxonomy is obtained through an extensive analysis of existing component interactions. The taxonomy is used both to understand existing software connectors and to suggest new, unprecedented connectors. We demonstrate the use of the taxonomy on the architecture of a large, existing system.","Taxonomy,
Connectors,
Software systems,
Middleware,
Computer science,
Software architecture,
Computer architecture,
Packaging,
Programming,
Software standards"
An imaging system with calibrated color image acquisition for use in dermatology,"The authors propose a novel imaging system useful in dermatology, more precisely, for the follow-up of patients with an increased risk of skin cancer. The system consists of a Pentium PC equipped with an RGB frame grabber, a three-chip charge coupled devices (CCD) camera controlled by the serial port and equipped with a zoom lens and a halogen annular light source. Calibration of the imaging system provides a way to transform the acquired images, which are defined in an unknown color space, to a standard, well-defined color space called SRGB, sRGB has a known relation to the CIE/sup 1/ XYZ and CIE L*a*b* colorimetric spaces. These CIE color spaces are based on the human vision, and they allow the computation of a color difference metric called CIE /spl Delta/E/sub ab/*, which is proportional to the color difference, as seen by a human observer. Several types of polynomial RGB to sRGB transforms will be tried, including some optimized in perceptually uniform color spaces. The use of a standard and well-defined color space also allows meaningful exchange of images, e.g., in teledermatology. The calibration procedure is based on 24 patches with known color properties, and it takes about 5 minutes to perform. It results in a number of settings called a profile that remains valid for tens of hours of operation. Such a profile is checked before acquiring images using just one color patch, and is adjusted on the fly to compensate for short-term drift in the response of the imaging system. Precision or reproducibility of subsequent color measurements is very good with =0.3 and /spl Delta/E/sub ab/*<1.2. Accuracy compared with spectrophotometric measurements is fair with =6.2 and /spl Delta/E/sub ab/*><13.3.","Color,
Charge-coupled image sensors,
Calibration,
Humans,
Skin cancer,
Charge coupled devices,
Lighting control,
Control systems,
Lenses,
Light sources"
Context awareness by analysing accelerometer data,In this paper we describe continuing work being carried out as part of the Bristol Wearable Computing Initiative. We are researching processing techniques for data from accelerometers which enable the wearable computer to determine the user's activity. Techniques already employed by others have been explored and we propose new methods for analysing the data delivered by these devices. We try to minimise the number of devices needed by using a single X-Y accelerometer device. Using our techniques we have adapted our GPS based Tourist Guide application to vary its multimedia presentation according to the user's activity as well as location.,"Context awareness,
Data analysis,
Accelerometers,
Wearable computers,
Wearable sensors,
Legged locomotion,
Sensor phenomena and characterization,
Computer science,
Global Positioning System,
Data processing"
Comparison of 3-D maximum a posteriori and filtered backprojection algorithms for high-resolution animal imaging with microPET,"The authors have evaluated the performance of two three-dimensional (3-D) reconstruction algorithms with data acquired from microPET, a high resolution tomograph dedicated to small animal imaging. The first was a linear filtered-backprojection algorithm (FBP) with reprojection of the missing data, and the second was a statistical maximum a posteriori probability algorithm (MAP). The two algorithms were evaluated in terms of their resolution performance, both in phantoms and in vivo. Sixty independent realizations of a phantom simulating the brain of a baby monkey were acquired, each containing three million counts. Each of these realizations was reconstructed independently with both algorithms. The ensemble of the 60 reconstructed realizations was used to estimate the standard deviation as a measure of the noise for each reconstruction algorithm. More detail was recovered in the MAP reconstruction without an increase in noise relative to FBP. Studies in a simple cylindrical compartment phantom demonstrated improved recovery of known activity ratios with MAP. Finally, in vivo studies also demonstrated a clear improvement in spatial resolution using the MAP algorithm. The quantitative accuracy of the MAP reconstruction was also evaluated by comparison with autoradiography and direct well counting of tissue samples and was shown to be superior.","Animals,
High-resolution imaging,
Image reconstruction,
Imaging phantoms,
Nonlinear filters,
Reconstruction algorithms,
In vivo,
Image resolution,
Probability,
Brain modeling"
Rapid 3-D cone-beam reconstruction with the simultaneous algebraic reconstruction technique (SART) using 2-D texture mapping hardware,"Algebraic reconstruction methods, such as the algebraic reconstruction technique (ART) and the related simultaneous ART (SART), reconstruct a two-dimensional (2-D) or three-dimensional (3-D) object from its X-ray projections. The algebraic methods have, in certain scenarios, many advantages over the more popular Filtered Backprojection approaches and have also recently been shown to perform well for 3-D cone-beam reconstruction. However, so far the slow speed of these iterative methods have prohibited their routine use in clinical applications. Here, the authors address this shortcoming and investigate the utility of widely available 2-D texture mapping graphics hardware for the purpose of accelerating the 3-D algebraic reconstruction. They find that this hardware allows 3-D cone-beam reconstructions to be obtained at almost interactive speeds, with speed-ups of over 50 with respect to implementations that only use general-purpose CPUs. However the authors also find that the reconstruction quality is rather sensitive to the resolution of the framebuffer, and to address this critical issue they propose a scheme that extends the precision of a given framebuffer by 4 bits, using the color channels. With this extension, a 12-bit framebuffer delivers useful reconstructions for 0.5% tissue contrast, while an 8-bit framebuffer requires 4%. Since graphics hardware generates an entire image for each volume projection, it is most appropriately used with an algebraic reconstruction method that performs volume correction at that granularity as well, such as SART or SIRT. The authors chose SART for its faster convergence properties.","Simultaneous localization and mapping,
Hardware,
Image reconstruction,
Reconstruction algorithms,
Subspace constraints,
Graphics,
Two dimensional displays,
Iterative methods,
Acceleration,
Image generation"
RMX: reliable multicast for heterogeneous networks,"Although IP multicast is an effective network primitive for best-effort, large-scale, multi-point communication, many multicast applications such as shared whiteboards, multi-player games and software distribution require reliable data delivery. Building services like reliable sequenced delivery on top of IP multicast has proven to be a hard problem. The enormous extent of network and end-system heterogeneity in multipoint communication exacerbates the design of scalable end-to-end reliable multicast protocols. In this paper, we propose a radical departure from the traditional end-to-end model for reliable multicast and instead propose a hybrid approach that leverages the successes of unicast reliability protocols such as TCP while retaining the efficiency of IP multicast for multi-point data delivery. Our approach splits a large heterogeneous reliable multicast session into a number of multicast data groups of co-located homogeneous participants. A collection of application-aware agents-reliable multicast proxies (RMX)-organizes these data groups into a spanning tree using an overlay network of TCP connections. Sources transmit data to their local group, and the RMX in that group forwards the data towards the rest of the data groups. RMX use detailed knowledge of application semantics to adapt to the effects of heterogeneity in the environment. To demonstrate the efficacy of our architecture, we have built a prototype implementation that can be customized for different kinds of applications.","Telecommunication network reliability,
Multicast protocols,
Computer network reliability,
Unicast,
TCPIP,
Application software,
Web and internet services,
Bandwidth,
Computer science,
Large-scale systems"
FIRM: a class of distributed scheduling algorithms for high-speed ATM switches with multiple input queues,"Advanced input queuing is an attractive, promising architecture for high-speed ATM switches, because it combines the low cost of input queuing with the high performance of output queuing. The need for scalable schedulers for advanced input queuing switch architectures has led to the development of efficient distributed scheduling algorithms. We introduce a new distributed scheduling algorithm, FIRM, which provides improved performance characteristics over alternative distributed algorithms. FIRM achieves saturation throughput 1 with lower delay than the most efficient alternative (up to 50% at high load). Furthermore, it provides improved fairness (it approximates FCFS) and tighter service guarantee than others. FIRM provides a basis for a class of distributed scheduling algorithms, many of which provide even more improved performance characteristics.","Scheduling algorithm,
Asynchronous transfer mode,
Switches,
Communication switching,
Scalability,
Costs,
Throughput,
Processor scheduling,
Computer science,
Electronic mail"
Making scientific computations reproducible,"To verify a research paper's computational results, readers typically have to recreate them from scratch. ReDoc is a simple software filing system for authors that lets readers easily reproduce computational results using standardized rules and commands.","Software maintenance,
Reproducibility of results,
Computer interfaces,
Laboratories,
Documentation,
Electronic publishing,
Organizing,
Software testing,
Software systems,
Technological innovation"
A novel approach to extract colon lumen from CT images for virtual colonoscopy,"An automatic method has been developed for segmentation of abdominal computed tomography (CT) images for virtual colonoscopy obtained after a bowel preparation of a low-residue diet with ingested contrast solutions to enhance the image intensities of residual colonic materials. Removal of the enhanced materials was performed electronically by a computer algorithm. The method is a multistage approach that employs a modified self-adaptive on-line, vector quantization technique for a low-level image classification and utilizes a region-growing strategy for a high-level feature extraction. The low-level classification labels each voxel based on statistical analysis of its three-dimensional intensity vectors consisting of nearby voxels. The high-level processing extracts the labeled stool, fluid and air voxels within the colon, and eliminates bone and lung voxels which have similar image intensities as the enhanced materials and air, but are physically separated from the colon. This method was evaluated by volunteer studies based on both objective and subjective criteria. The validation demonstrated that the method has a high reproducibility and repeatability and a small error due to partial volume effect. As a result of this electronic colon cleansing, routine physical bowel cleansing prior to virtual colonoscopy may not be necessary.","Colonography,
Colon,
Computed tomography,
Virtual colonoscopy,
Image segmentation,
Abdomen,
Vector quantization,
Image classification,
Feature extraction,
Statistical analysis"
Reachability analysis of real-time systems using time Petri nets,"Time Petri nets (TPNs) are a popular Petri net model for specification and verification of real-time systems. A fundamental and most widely applied method for analyzing Petri nets is reachability analysis. The existing technique for reachability analysis of TPNs, however, is not suitable for timing property verification because one cannot derive end-to-end delay in task execution, an important issue for time-critical systems, from the reachability tree constructed using the technique. In this paper, we present a new reachability based analysis technique for TPNs for timing property analysis and verification that effectively addresses the problem. Our technique is based on a concept called clock-stamped state class (CS-class). With the reachability tree generated based on CS-classes, we can directly compute the end-to-end time delay in task execution. Moreover, a CS-class can be uniquely mapped to a traditional state class based on which the conventional reachability tree is constructed. Therefore, our CS-class-based analysis technique is more general than the existing technique. We show how to apply this technique to timing property verification of the TPN model of a command and control (C2) system.","Reachability analysis,
Real time systems,
Petri nets,
Timing,
Command and control systems,
Computer science,
Delay effects,
Concurrent computing,
System recovery,
Time factors"
Automated anatomical labeling of the bronchial branch and its application to the virtual bronchoscopy system,"This paper describes a method for the automated anatomical labeling of the bronchial branch extracted from a three-dimensional (3-D) chest X-ray CT image and its application to a virtual bronchoscopy system (VBS). Automated anatomical labeling is necessary for implementing an advanced computer-aided diagnosis system of 3-D medical images. This method performs the anatomical labeling of the bronchial branch using the knowledge base of the bronchial branch name. The knowledge base holds information on the bronchial branch as a set of rules for its anatomical labeling. A bronchus region is automatically extracted from a given 3-D CT image. A tree structure representing the essential structure of the extracted bronchus is recognized from the bronchus region. Anatomical labeling is performed by comparing this tree structure of the bronchus with the knowledge base. As an application, the authors implemented the function to automatically present the anatomical names of the branches that are shown in the currently rendered image in real time on the VBS. The result showed that the method could segment about 57% of the branches from CT images and extracted a tree structure of about 91% in branches in the segmented bronchus. The anatomical labeling method could assign the correct branch name to about 93% of the branches in the extracted tree structure. Anatomical names were appropriately displayed in the endoscopic view.","Labeling,
Bronchoscopy,
Tree data structures,
Computed tomography,
Application software,
Data mining,
Image segmentation,
X-ray imaging,
Computer aided diagnosis,
Biomedical imaging"
Modeling of ship trajectory in collision situations by an evolutionary algorithm,"For a given circumstance (i.e., a collision situation at sea), a decision support system for navigation should help the operator to choose a proper manoeuvre, teach him good habits, and enhance his general intuition on how to behave in similar situations in the future. By taking into account certain boundaries of the maneuvering region along with information on navigation obstacles and other moving ships, the problem of avoiding collisions is reduced to a dynamic optimization task with static and dynamic constraints. This paper presents experiments with a modified version of the Evolutionary Planner/Navigator (EP/N). Its new version, /spl thetav/EP/N++, is a major component of a such decision support system. This new extension of EP/N computes a safe-optimum path of a ship in given static and dynamic environments. A safe trajectory of the ship in a collision situation is determined on the basis of this algorithm. The introduction of a time parameter, the variable speed of the ship, and time-varying constraints representing movable ships are the main features of the new system. Sample results of ship trajectories obtained for typical navigation situations are presented.","Marine vehicles,
Evolutionary computation,
Navigation,
Radar,
Decision support systems,
Displays,
Automation,
Computer science,
Constraint optimization,
Time varying systems"
The analysis of decomposition methods for support vector machines,"The support vector machine (SVM) is a promising technique for pattern recognition. It requires the solution of a large dense quadratic programming problem. Traditional optimization methods cannot be directly applied due to memory restrictions. Up to now, very few methods can handle the memory problem and an important one is the ""decomposition method."" However, there is no convergence proof so far. We connect this method to projected gradient methods and provide theoretical proofs for a version of decomposition methods. An extension to bound-constrained formulation of SVM is also provided. We then show that this convergence proof is valid for general decomposition methods if their working set selection meets a simple requirement.","Support vector machines,
Support vector machine classification,
Pattern recognition,
Quadratic programming,
Optimization methods,
Gradient methods,
Convergence,
Upper bound,
Computer science,
Newton method"
Efficient subgraph isomorphism detection: a decomposition approach,"Graphs are a powerful and universal data structure useful in various subfields of science and engineering. In this paper, we propose a new algorithm for subgraph isomorphism detection from a set of a priori known model graphs to an input graph that is given online. The new approach is based on a compact representation of the model graphs that is computed offline. Subgraphs that appear more than once within the same or within different model graphs are represented only once, thus reducing the computational effort to detect them in an input graph. In the extreme case where all model graphs are highly similar, the run-time of the new algorithm becomes independent of the number of model graphs. Both a theoretical complexity analysis and practical experiments characterizing the performance of the new approach are given.","Optimization methods,
Data structures,
Data engineering,
Power engineering and energy,
Tree graphs,
Computer Society,
Runtime,
Performance analysis,
Chemicals,
Documentation"
Off-line dictionary-based compression,"Dictionary-based modeling is a mechanism used in many practical compression schemes. In most implementations of dictionary-based compression the encoder operates on-line, incrementally inferring its dictionary of available phrases from previous parts of the message. An alternative approach is to use the full message to infer a complete dictionary in advance, and include an explicit representation of the dictionary as part of the compressed message. In this investigation, we develop a compression scheme that is a combination of a simple but powerful phrase derivation method and a compact dictionary encoding. The scheme is highly efficient, particularly in decompression, and has characteristics that make it a favorable choice when compressed data is to be searched directly. We describe data structures and algorithms that allow our mechanism to operate in linear time and space.","Dictionaries,
Encoding,
Computer science,
Decoding,
Data structures,
Australia Council,
Data compression,
Software engineering,
Resource management"
Robot dynamics: equations and algorithms,"This paper reviews some of the accomplishments in the field of robot dynamics research, from the development of the recursive Newton-Euler algorithm to the present day. Equations and algorithms are given for the most important dynamics computations, expressed in a common notation to facilitate their presentation and comparison.","Robots,
Equations,
Heuristic algorithms,
Orbital robotics,
Manipulator dynamics,
Acceleration,
Computational efficiency,
Computational modeling,
Lagrangian functions,
Computer science"
Worst-case utilization bound for EDF scheduling on real-time multiprocessor systems,"Presents the utilization bound for earliest deadline first (EDF) scheduling on homogeneous multiprocessor systems with partitioning strategies. Assuming that tasks are pre-emptively scheduled on each processor according to the EDF algorithm, and allocated according to the first-fit (FF) heuristic, we prove that the worst-case achievable utilization is 0.5(n+1), where n is the number of processors. This bound is valid for arbitrary utilization factors. Moreover, if all the tasks have utilization factors under a value /spl alpha/, the previous bound is raised, and the new utilization bound considering /spl alpha/ is calculated. In addition, we prove that no uniprocessor scheduling algorithm/allocation algorithm pair can provide a higher worst-case achievable utilization than that of EDF-FF. Finally, simulation provides the average-case achievable utilization for EDF-FF.","Real time systems,
Multiprocessing systems,
Optimized production technology,
Processor scheduling,
Optimal scheduling,
Partitioning algorithms,
Scheduling algorithm,
Computer science,
Heuristic algorithms,
Performance evaluation"
Probabilistic noninterference for multi-threaded programs,"We present a probability-sensitive confidentiality specification-a form of probabilistic noninterference-for a small multi-threaded programming language with dynamic thread creation. Probabilistic covert channels arise from a scheduler which is probabilistic. Since scheduling policy is typically outside the language specification for multi-threaded languages, we describe how to generalise the security condition in order to define how to generalise the security condition in order to define robust security with respect to a wide class of schedulers, not excluding the possibility of deterministic (e.g., round-robin) schedulers and program-controlled thread priorities. The formulation is based on an adaptation of Larsen and Skou's (1991) notion of probabilistic bisimulation. We show how the security condition satisfies compositionality properties which facilitate straightforward proofs of correctness for, e.g., security type systems. We illustrate this by defining a security type system which improves on previous multi-threaded systems, and by proving it correct with respect to our stronger scheduler-independent security condition.","Information security,
Yarn,
Processor scheduling,
Information analysis,
Computer science,
Data security,
Safety,
High performance computing,
Application software,
Timing"
On-demand routing in large ad hoc wireless networks with passive clustering,"This paper presents on-demand routing scalability improvements achieved using a ""passive"" clustering. Any on-demand routing typically requires some form of flooding. Clustering can dramatically reduce transmission overhead during flooding. In fact, by using clustering, we restrict the set of forwarding nodes during flood search and thus reduce the energy cost and traffic overhead of routing in dynamic traffic and topology environments. However existing ""active"" clustering mechanisms require periodic refresh of neighborhood information and tend to introduce quite a large amount of communication maintenance overhead. We introduce a passive clustering protocol scheme which is mostly supported/maintained by user data packets instead of explicit control packets. The passive scheme is consistent with the on-demand routing philosophy. Simulation results show significant performance improvements when passive clustering is used.","Routing,
Intelligent networks,
Wireless networks,
Scalability,
Floods,
Nominations and elections,
Mobile ad hoc networks,
Computer science,
Costs,
Topology"
On the approximability of trade-offs and optimal access of Web sources,"We study problems in multiobjective optimization, in which solutions to a combinatorial optimization problem are evaluated with respect to several cost criteria, and we are interested in the trade-off between these objectives (the so-called Pareto curve). We point out that, under very general conditions, there is a polynomially succinct curve that /spl epsiv/-approximates the Pareto curve, for any /spl epsiv/>0. We give a necessary and sufficient condition under which this approximate Pareto curve can be constructed in time polynomial in the size of the instance and 1//spl epsiv/. In the case of multiple linear objectives, we distinguish between two cases: when the underlying feasible region is convex, then we show that approximating the multi-objective problem is equivalent to approximating the single-objective problem. If however the feasible region is discrete, then we point out that the question reduces to an old and recurrent one: how does the complexity of a combinatorial optimization problem change when its feasible region is intersected with a hyperplane with small coefficients; we report some interesting new findings in this domain. Finally, we apply these concepts and techniques to formulate and solve approximately a cost-time-quality trade-off for optimizing access to the World-Wide Web, in a model first studied by Etzioni et al. (1996) (which was actually the original motivation for this work).","Pareto optimization,
Polynomials,
Computer science,
Cost function,
Delay effects,
Operations research,
Microeconomics,
Sufficient conditions"
Structure from motion without correspondence,"A method is presented to recover 3D scene structure and camera motion from multiple images without the need for correspondence information. The problem is framed as finding the maximum likelihood structure and motion given only the 2D measurements, integrating over all possible assignments of 3D features to 2D measurements. This goal is achieved by means of an algorithm which iteratively refines a probability distribution over the set of all correspondence assignments. At each iteration a new structure from motion problem is solved, using as input a set of 'virtual measurements' derived from this probability distribution. The distribution needed can be efficiently obtained by Markov Chain Monte Carlo sampling. The approach is cast within the framework of Expectation-Maximization, which guarantees convergence to a local maximizer of the likelihood. The algorithm works well in practice, as will be demonstrated using results on several real image sequences.","Cameras,
Layout,
Motion measurement,
Shape,
Computer science,
Robot vision systems,
Iterative algorithms,
Read only memory,
Probability distribution,
Monte Carlo methods"
Generating network topologies that obey power laws,"Recent studies have shown that Internet graphs and other network systems follow power-laws. Are these laws obeyed by the artificial network topologies used in network simulations? Does it matter? In this paper we show that current topology generators do not obey all of the power-laws, and we present two new topology generators that do. We also re-evaluate a multicast study to show the impact of using power-law topologies.","Power generation,
Network topology,
Internet,
Computer science,
Multicast algorithms,
Storms,
IP networks,
Computational modeling,
Sorting,
Frequency"
IRM enforcement of Java stack inspection,"Two implementations are given for Java's stack inspection access-control policy. Each implementation is obtained by generating an inlined reference monitor (IRM) for a different formulation of the policy. Performance of the implementations is evaluated, and one is found to be competitive with Java's less flexible, JVM-resident implementation. The exercise illustrates the power of the IRM approach for enforcing security policies.","Java,
Inspection,
Runtime,
Virtual machining,
Decoding,
Genetics,
Computer science,
Computerized monitoring,
Power system security,
Binary codes"
Emerging challenges: Mobile networking for “Smart Dust”,"Large-scale networks of wireless sensors are becoming increasingly tractable. Advances in hardware technology and engineering design have led to dramatic reductions in size, power consumption and cost for digital circuitry, wireless communications and Micro ElectroMechanical Systems (MEMS). This has enabled very compact, autonomous and mobile nodes, each containing one or more sensors, computation and communication capabilities, and a power supply. The missing ingredient is the networking and applications layers needed to harness this revolutionary capability into a complete system. We review the key elements of the emergent technology of “Smart Dust” and outline the research challenges they present to the mobile networking and systems community, which must provide coherent connectivity to large numbers of mobile network nodes co-located within a small volume.","Optical transmitters,
Laser beams,
Optical receivers,
Bit rate,
Intelligent sensors"
Surface-bounded growth modeling applied to human mandibles,"From a set of longitudinal three-dimensional scans of the same anatomical structure, the authors have accurately modeled the temporal shape and size changes using a linear shape model. On a total of 31 computed tomography scans of the mandible from six patients, 14,851 semilandmarks are found automatically using shape features and a new algorithm called geometry-constrained diffusion. The semilandmarks are mapped into Procrustes space. Principal component analysis extracts a one-dimensional subspace, which is used to construct a linear growth model. The worst case mean modeling error in a cross validation study is 3.7 mm.","Humans,
Shape,
Computed tomography,
Surgery,
Principal component analysis,
Deformable models,
Image analysis,
Mathematical model,
Anatomical structure,
Teeth"
A miniature robotic system for reconnaissance and surveillance,"Presents a miniature robotic system (""scout"") useful for reconnaissance and surveillance missions. A large number of scout robots are deployed and controlled by humans and/or larger ""ranger"" robots. The specially designed and constructed scouts are extremely small (roughly 116cc volume) yet are readily deployable (by tossing or launching), have multiple mobility modes, have multiple sensing capabilities, can transmit and receive data and instructions, and have a limited capability for autonomous action. The rangers are significantly larger vehicles, based on a commercial-off-the-shelf platform, augmented with scout launchers, radios, and additional sensors. Together, the scouts and rangers form a hierarchical team capable of carrying out complex missions in a wide variety of environments.","Reconnaissance,
Surveillance,
Robot kinematics,
Computer science,
Robot sensing systems,
Orbital robotics,
Mobile robots,
Humans,
Remotely operated vehicles,
Chemical sensors"
Fast scene change detection using direct feature extraction from MPEG compressed videos,"In order to process video data efficiently, a video segmentation technique through scene change detection must be required. This is a fundamental operation used in many digital video applications such as digital libraries, video on demand (VOD), etc. Many of these advanced video applications require manipulations of compressed video signals. So, the scene change detection process is achieved by analyzing the video directly in the compressed domain, thereby avoiding the overhead of decompressing video into individual frames in the pixel domain. In this paper, we propose a fast scene change detection algorithm using direct feature extraction from MPEG compressed videos, and evaluate this technique using sample video data, First, we derive binary edge maps from the AC coefficients in blocks which were discrete cosine transformed. Second, we measure edge orientation, strength and offset using correlation between the AC coefficients in the derived binary edge maps. Finally, we match two consecutive frames using these two features (edge orientation and strength). This process was made possible by a new mathematical formulation for deriving the edge information directly from the discrete cosine transform (DCT) coefficients. We have shown that the proposed algorithm is faster or more accurate than the previously known scene change detection algorithms.","Layout,
Feature extraction,
Transform coding,
Discrete cosine transforms,
Detection algorithms,
Video sequences,
Software libraries,
Video on demand,
Change detection algorithms,
Computer science"
"Design, implementation, and deployment of the iKP secure electronic payment system","This paper discusses the design, implementation, and deployment of a secure and practical payment system for electronic commerce on the Internet. The system is based on the iKP family of protocols-(i=1,2,3)-developed at IBM Research. The protocols implement credit card-based transactions between buyers and merchants while the existing financial network is used for payment clearing and authorization. The protocols are extensible and can be readily applied to other account-based payment models, such as debit cards. They are based on careful and minimal use of public-key cryptography, and can be implemented in either software or hardware. Individual protocols differ in both complexity and degree of security. In addition to being both a precursor and a direct ancestor of the well-known SET standard, iKP-based payment systems have been in continuous operation on the Internet since mid-1996. This longevity-as well as the security and relative simplicity of the underlying mechanisms-makes the iKP experience unique. For this reason, this paper also reports on, and addresses, a number of practical issues arising in the course of implementation and real-world deployment of a secure payment system.","Electronic commerce,
Security,
Cryptographic protocols,
Public key cryptography,
Internet,
Laboratories,
Credit cards,
Authorization,
Hardware,
Computer science"
On the optimality of the gridding reconstruction algorithm,"Gridding reconstruction is a method to reconstruct data onto a Cartesian grid from a set of nonuniformly sampled measurements. This method is appreciated for being robust and computationally fast. However, it lacks solid analysis and design tools to quantify or minimize the reconstruction error. Least squares reconstruction (LSR), on the other hand, is another method which is optimal in the sense that it minimizes the reconstruction error. This method is computationally intensive and, in many cases, sensitive to measurement noise. Hence, it is rarely used in practice. Despite their seemingly different approaches, the gridding and LSR methods are shown to be closely related. The similarity between these two methods is accentuated when they are properly expressed in a common matrix form. It is shown that the gridding algorithm can be considered an approximation to the least squares method. The optimal gridding parameters are defined as the ones which yield the minimum approximation error. These parameters are calculated by minimizing the norm of an approximation error matrix. This problem is studied and solved in the general form of approximation using linearly structured matrices. This method not only supports more general forms of the gridding algorithm, it can also be used to accelerate the reconstruction techniques from incomplete data. The application of this method to a case of two-dimensional (2-D) spiral magnetic resonance imaging shows a reduction of more than 4 dB in the average reconstruction error.","Reconstruction algorithms,
Image reconstruction,
Least squares methods,
Least squares approximation,
Approximation error,
Robustness,
Solids,
Noise measurement,
Approximation algorithms,
Linear approximation"
Immersive VR for scientific visualization: a progress report,"Immersive virtual reality (IVR) has the potential to be a powerful tool for the visualization of burgeoning scientific data sets and models. We sketch a research agenda for the hardware and software technology underlying IVR for scientific visualization. In contrast to Brooks' (1999) excellent survey which reported on the state of IVR and provided concrete examples of its production use, this article is somewhat speculative. It does not present solutions but rather a progress report, a hope, and a call to action, to help scientists cope with a major crisis that threatens to impede their progress.",
Automatic recovery of relative camera rotations for urban scenes,"In this paper we describe a formulation of extrinsic camera calibration that decouples rotation from translation by exploiting properties inherent in urban scenes. We then present an algorithm which uses edge features to robustly and accurately estimate relative rotations among multiple cameras given intrinsic calibration and approximate initial pose. The algorithm is linear both in the number of images and the number of features. We estimate the number and directions of vanishing points (VPs) with respect to each camera using a hybrid approach that combines the robustness of the Hough transform with the accuracy of expectation maximization. Matching and labeling methods identify unique VPs and correspond them across all cameras. Finally, a technique akin to bundle adjustment produces globally optimal estimates of relative camera rotations by bringing all VPs into optimal alignment. Uncertainty is modeled and used at every stage to improve accuracy. We assess the algorithm's performance on both synthetic and real data, and compare our results to those of semi-automated photogrammetric methods for a large set of real hemispherical images, using several consistency and error metrics.","Cameras,
Layout,
Calibration,
Image edge detection,
Parameter estimation,
Computer graphics,
Laboratories,
Computer science,
Labeling,
Uncertainty"
Power optimization of real-time embedded systems on variable speed processors,Power efficient design of real-time embedded systems based on programmable processors becomes more important as system functionality is increasingly realized through software. This paper presents a power optimization method for real-time embedded applications on a variable speed processor. The method combines off-line and on-line components. The off-line component determines the lowest possible maximum processor speed while guaranteeing deadlines of all tasks. The on-line component dynamically varies the processor speed or brings a processor into a power-down mode according to the status of task set in order to exploit execution time variations and idle intervals. Experimental results show that the proposed method obtains a significant power reduction across several kinds of applications.,"Real time systems,
Embedded system,
Processor scheduling,
Energy consumption,
Collaborative software,
Optimization methods,
Clocks,
Runtime,
Computer industry,
Computer science"
A case study of open source software development: the Apache server,"According to its proponents, open source style software development has the capacity to compete successfully, and perhaps in many cases displace, traditional commercial development methods. In order to begin investigating such claims, we examine the development process of a major open source application, the Apache web server. By using email archives of source code change history and problem reports we quantify aspects of developer participation, core team size, code ownership, productivity, defect density, and problem resolution interval for this OSS project. This analysis reveals a unique process, which performs well on important measures. We conclude that hybrid forms of development that borrow the most effective techniques from both the OSS and commercial worlds may lead to high performance software processes.","Computer aided software engineering,
Open source software,
Programming,
System-level design,
Job shop scheduling,
Computer science,
Application software,
Web server,
History,
Productivity"
A survey of smoothing techniques for ME models,"In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood (ML) training for exponential models, and like other ML methods is prone to overfitting of training data. Several smoothing methods for ME models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods compare with smoothing methods for other types of related models. In this work, we survey previous work in ME smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between ME and conventional n-gram models, this domain is well-suited to gauge the performance of ME smoothing methods. Over a large number of data sets, we find that fuzzy ME smoothing performs as well as or better than all other algorithms under consideration. We contrast this method with previous n-gram smoothing methods to explain its superior performance.","Smoothing methods,
Entropy,
Context modeling,
Training data,
Fuzzy sets,
Natural languages,
Associate members,
Performance evaluation,
Glass,
Computer science"
How bad is selfish routing?,"We consider the problem of routing traffic to optimize the performance of a congested network. We are given a network, a rate of traffic between each pair of nodes, and a latency function for each edge specifying the time needed to traverse the edge given its congestion; the objective is to route traffic such that the sum of all travel times-the total latency-is minimized. In many settings, including the Internet and other large-scale communication networks, it may be expensive or impossible to regulate network traffic so as to implement an optimal assignment of routes. In the absence of regulation by some central authority, we assume that each network user routes its traffic on the minimum-latency path available to it, given the network congestion caused by the other users. In general such a ""selfishly motivated"" assignment of traffic to paths will not minimize the total latency; hence, this lack of regulation carries the cost of decreased network performance. We quantify the degradation in network performance due to unregulated traffic. We prove that if the latency of each edge is a linear function of its congestion, then the total latency of the routes chosen by selfish network users is at most 4/3 times the minimum possible total latency (subject to the condition that all traffic must be routed). We also consider the more general setting in which edge latency functions are assumed only to be continuous and non-decreasing in the edge congestion.","Routing,
Telecommunication traffic,
Delay,
Computer science,
Communication networks,
Game theory,
Nash equilibrium,
Costs,
Degradation,
Large-scale systems"
Computer-aided diagnosis of breast lesions in medical images,"Given current error rates, this article surveys various approaches and techniques for improved breast lesion diagnosis in medical images, including mammography, ultrasound and magnetic resonance imaging. The author also includes studies that compare human versus computer-aided diagnosis.","Computer aided diagnosis,
Breast,
Lesions,
Biomedical imaging,
Medical diagnostic imaging,
Error analysis,
Mammography,
Ultrasonic imaging,
Magnetic resonance imaging,
Humans"
Fast distance queries with rectangular swept sphere volumes,"We present new distance computation algorithms using hierarchies of rectangular swept spheres. Each bounding volume of the tree is described as the Minkowski sum of a rectangle and a sphere, and fits tightly to the underlying geometry. We present accurate and efficient algorithms to build the hierarchies and perform distance queries between the bounding volumes. We also present traversal techniques for accelerating distance queries using coherence and priority directed search. These algorithms have been used to perform proximity queries for applications including virtual prototyping, dynamic simulation, and motion planning on complex models. As compared to earlier algorithms based on bounding volume hierarchies for separation distance and approximate distance computation, our algorithms have achieved significant speedups on many benchmarks.","Computational modeling,
Geometry,
Virtual prototyping,
Motion planning,
Algorithm design and analysis,
Haptic interfaces,
Computer science,
Coherence,
Robot motion,
Contracts"
Point-tracked quantitative analysis of left ventricular surface motion from 3-D image sequences,"Proposes and validates the hypothesis that one can use differential shape properties of the myocardial surfaces to recover dense field motion from standard three-dimensional (3-D) image sequences (MRI and CT). Quantitative measures of left ventricular regional function can be further inferred from the point correspondence maps. The noninvasive, algorithm-derived results are validated on two levels. First, the motion trajectories are compared to those of implanted imaging-opaque markers of a canine model in two imaging modalities, where subpixel accuracy is achieved. Second, the validity of using motion parameters (path length and thickness changes) for detecting myocardial injury area is tested by comparing algorithms derived results to postmortem analysis TTC staining of myocardial tissue, where the achieved Pearson product-moment correlation value is 0.968.","Image analysis,
Image motion analysis,
Image sequence analysis,
Motion analysis,
Image sequences,
Myocardium,
Shape,
Magnetic resonance imaging,
Computed tomography,
Motion detection"
High-throughput asynchronous pipelines for fine-grain dynamic datapaths,"This paper introduces several new asynchronous pipeline designs which offer high throughput as well as low latency. The designs target dynamic datapaths, both dual-rail as well as single-rail. The new pipelines are latch-free and therefore are particularly well-suited for fine-grain pipelining, i.e., where each pipeline stage is only a single gate deep. The pipelines employ new control structures and protocols aimed at reducing the handshaking delay, the principal impediment to achieving high throughput in asynchronous pipelines. As a test vehicle, a 4-bit FIFO was designed using 0.6 micron technology. The results of careful HSPICE simulations of the FIFO designs are very encouraging. The dual-rail designs deliver a throughput of up to 860 million data items per second. This performance represents an improvement by a factor of 2 over a widely-used comparable approach by T.E. Williams (1991). The new single-rail designs deliver a throughput of up to 1208 million data items per second.","Pipeline processing,
Throughput,
Protocols,
Delay,
Logic,
Computer science,
Rails,
Impedance,
Testing,
Liver"
Reconstruction algorithm for polychromatic CT imaging: application to beam hardening correction,"This paper presents a new reconstruction algorithm for both single- and dual-energy computed tomography (CT) imaging. By incorporating the polychromatic characteristics of the X-ray beam into the reconstruction process, the algorithm is capable of eliminating beam hardening artifacts. The single energy version of the algorithm assumes that each voxel in the scan field can be expressed as a mixture of two known substances, for example, a mixture of trabecular bone and marrow, or a mixture of fat and flesh. These assumptions are easily satisfied in a quantitative computed tomography (QCT) setting. The authors have compared their algorithm to three commonly used single-energy correction techniques. Experimental results show that their algorithm is much more robust and accurate. The authors have also shown that QCT measurements obtained using their algorithm are five times more accurate than that from current QCT systems (using calibration). The dual-energy mode does not require any prior knowledge of the object in the scan field, and can be used to estimate the attenuation coefficient function of unknown materials. The authors have tested the dual-energy setup to obtain an accurate estimate for the attenuation coefficient function of K/sub 2/HPO/sub 4/ solution.","Reconstruction algorithms,
Computed tomography,
Optical imaging,
X-ray imaging,
Attenuation,
Image reconstruction,
Cancellous bone,
Robustness,
Current measurement,
Calibration"
Analysis of single-event effects in combinational logic-simulation of the AM2901 bitslice processor,"Using SEUTool (a synthesized VHDL based simulator of single-event fault propagation in combinational circuitry), we have performed a single-event study on a custom-designed CMOS AM2901, a 4-bit bit-slice processor. Analysis shows interesting general trends for single-event upset effects in complex combinational/sequential circuits.","Latches,
Circuit faults,
Circuit simulation,
Combinational circuits,
Sequential circuits,
Registers,
CMOS logic circuits,
Microprocessors,
Clocks,
Semiconductor device modeling"
Authentication theory and hypothesis testing,"By interpreting message authentication as a hypothesis testing problem, this paper provides a generalized treatment of information-theoretic lower bounds on an opponent's probability of cheating in one-way message authentication. We consider the authentication of an arbitrary sequence of messages, using the same secret key shared between sender and receiver. The adversary tries to deceive the receiver by forging one of the messages in the sequence. The classical two types of cheating are considered, impersonation and substitution attacks, and lower bounds on the cheating probability for any authentication system are derived for three types of goals the adversary might wish to achieve. These goals are: (1) that the fraudulent message should be accepted by the receiver, or, in addition, (2) that the adversary wishes to know or (3) wants to even choose the value of the plaintext message obtained by the legitimate receiver after decoding with the secret key.",Data security
Direct least-squares estimation of spatiotemporal distributions from dynamic SPECT projections using a spatial segmentation and temporal B-splines,"Artifacts can result when reconstructing a dynamic image sequence from inconsistent, as well as insufficient and truncated, cone beam single photon emission computed tomography (SPECT) projection data acquired by a slowly rotating gantry. The artifacts can lead to biases in kinetic model parameters estimated from time-activity curves generated by overlaying volumes of interest on the images. However, the biases in time-activity curve estimates and subsequent kinetic parameter estimates can be reduced significantly by first modeling the spatial and temporal distribution of the radiopharmaceutical throughout the projected field of view and then estimating the time-activity curves directly from the projections. This approach is potentially useful for clinical SPECT studies involving slowly rotating gantries, particularly those using a single-detector system or body contouring orbits with a multidetector system. The authors have implemented computationally efficient methods for fully four-dimensional (4-D) direct estimation of spatiotemporal distributions from dynamic SPECT projection data. Temporal B-splines providing various orders of temporal continuity, as well as various time samplings, were used to model the time-activity curves for segmented blood pool and tissue volumes in simulated cone beam and parallel beam cardiac data acquisitions. Least-squares estimates of time-activity curves were obtained quickly using a workstation. Given faithful spatial modeling, accurate curve estimates were obtained using cubic, quadratic, or linear B-splines and a relatively rapid time sampling during initial tracer uptake. From these curves, kinetic parameters were estimated accurately for noiseless data and with some bias for noisy data. A preliminary study of spatial segmentation errors showed that spatial model mismatch adversely affected quantitative accuracy, but also resulted in structured errors (projected model versus raw data) that were easily detected in the authors' simulations. This suggests iterative refinement of the spatial model to reduce structured errors as an area of future research.","Spatiotemporal phenomena,
Spline,
Kinetic theory,
Parameter estimation,
Sampling methods,
Image segmentation,
Image reconstruction,
Image sequences,
Single photon emission computed tomography,
Orbits"
Service-based software: the future for flexible software,"For the past 40 years, the techniques, processes and methods of software development have been dominated by supply-side issues, giving rise to a software industry oriented towards developers rather than users. To achieve the levels of functionality, flexibility and time-to-market required by users, a radical shift is required in the development of software, with a more demand-centric view, leading to software which will be delivered as a service within the framework of an open marketplace. Already, there are some signs that this approach is being adopted by industry, but in a very limited way. We summarise research and a research method which has resulted in a long-term strategic view of software engineering innovation. Based on this foundation, we describe more recent work, which has resulted in an innovative demand-side model for the future of software. We propose a service architecture in which components may be bound instantly, just at the time they are needed, and then the binding may be discarded. A major benefit of this approach is that it leads to highly flexible and agile software that should be able to meet rapidly changing business needs.","Programming,
Software engineering,
Computer science,
Computer architecture,
Software quality,
Computer industry,
Time to market,
Marine vehicles,
Technological innovation,
Failure analysis"
"On clusterings-good, bad and spectral","We propose a new measure for assessing the quality of a clustering. A simple heuristic is shown to give worst-case guarantees under the new measure. Then we present two results regarding the quality of the clustering found by a popular spectral algorithm. One proffers worst case guarantees whilst the other shows that if there exists a ""good"" clustering then the spectral algorithm will find one close to it.","Clustering algorithms,
Mathematics,
Partitioning algorithms,
Engineering profession,
Spectral analysis,
Computer science,
Performance analysis,
Algorithm design and analysis"
The n-PI-method for helical cone-beam CT,"A new class of acquisition schemes for helical cone-beam computed tomography (CB-CT) scanning is introduced, and their effect on the reconstruction methods is analyzed. These acquisition schemes are based on a new detector shape that is bounded by the helix. It will be shown that the data acquired with these schemes are compatible with exact reconstruction methods, and the adaptation of exact reconstruction algorithms to the new acquisition geometry is described. At the same time, the so-called PI-sufficiency condition is fulfilled. Moreover, a good fit to the acquisition requirements of the various medical applications of cone-beam CT is achieved. In contrast to other helical cone-beam acquisition and reconstruction methods, the n-PI-method introduced in this publication allows for variable pitches of the acquisition helix. This additional feature will introduce a higher flexibility into the acquisition protocols of future medical cone-beam scanners. An approximative n-PI-filtered backprojection (n-PI-FBP) reconstruction method is presented and verified. It yields convincing image quality.",
Exact Radon rebinning algorithm for the long object problem in helical cone-beam CT,"This paper addresses the long object problem in helical cone-beam computed tomography. The authors present the PHI-method, a new algorithm for the exact reconstruction of a region-of-interest (ROI) of a long object from axially truncated data extending only slightly beyond the ROI. The PHI-method is an extension of the Radon-method, published by Kudo et al. in Phys. in Med. and Biol., vol. 43, p. 2885-909 (1998). The key novelty of the PHI-method is the introduction of a virtual object f/sub /spl phi//(x) for each value of the azimuthal angle /spl phi/ in the image space, with each virtual object having the property of being equal to the true object f(x) in some ROI /spl Omega//sub m/. The authors show that, for each /spl phi/, one can calculate exact Radon data corresponding to the two-dimensional (2-D) parallel-beam projection of f/sub /spl phi//(x) onto the meridian plane of angle /spl phi/. Given an angular range of length /spl pi/ of such parallel-beam projections, the ROI /spl Omega//sub m/ can be exactly reconstructed because f(x) is identical to f/sub /spl phi//(x) in /spl Omega//sub m/. Simulation results are given for both the Radon-method and the PHI-method indicating that (1) for the case of short objects, the Radon- and PHI-methods produce comparable image quality, (2) for the case of long objects, the PHI-method delivers the same image quality as in the short object case, while the Radon-method fails, and (3) the image quality produced by the PHI-method is similar for a large range of pitch values.",
TCP Vegas revisited,"The innovative techniques of TCP Vegas have been the subject of much debate in recent years. Several studies have reported that TCP Vegas provides better performance than TCP Reno. However, the question of which of the new techniques are responsible for the impressive performance gains remains unanswered so far. This paper presents a detailed performance evaluation of TCP Vegas. By decomposing TCP Vegas into the various novel mechanisms proposed and assessing the effect of each of these mechanisms on performance, we show that the reported performance gains are achieved primarily by TCP Vegas's new techniques for slow start and congestion recovery. TCP Vegas's innovative congestion avoidance mechanism is shown to have only a minor influence on throughput. Furthermore, we find that the congestion avoidance mechanism exhibits fairness problems even if all competing connections operate with the same round trip time.",
Support vector machines for speaker verification and identification,"The performance of the support vector machine (SVM) on a speaker verification task is assessed. Since speaker verification requires binary decisions, support vector machines seem to be a promising candidate to perform the task. A new technique for normalising the polynomial kernel is developed and used to achieve performance comparable to other classifiers on the YOHO database. We also present results on a speaker identification task.","Support vector machines,
Support vector machine classification,
Databases,
Kernel,
Humans,
Polynomials,
Speech,
Computer science,
Training data,
Books"
Volume illustration: non-photorealistic rendering of volume models,"Accurately and automatically conveying the structure of a volume model is a problem that has not been fully solved by existing volume rendering approaches. Physics-based volume rendering approaches create images which may match the appearance of translucent materials in nature but may not embody important structural details. Transfer function approaches allow flexible design of the volume appearance but generally require substantial hand-tuning for each new data set in order to be effective. We introduce the volume illustration approach, combining the familiarity of a physics-based illumination model with the ability to enhance important features using non-photorealistic rendering techniques. Since the features to be enhanced are defined on the basis of local volume characteristics rather than volume sample values, the application of volume illustration techniques requires less manual tuning than the design of a good transfer function. Volume illustration provides a flexible unified framework for enhancing structural perception of volume models through the amplification of features and the addition of illumination effects.",
"Reciprocation, square root, inverse square root, and some elementary functions using small multipliers","This paper deals with the computation of reciprocals, square roots, inverse square roots, and some elementary functions using small tables, small multipliers, and, for some functions, a final ""large"" (almost full-length) multiplication. We propose a method, based on argument reduction and series expansion, that allows fast evaluation of these functions in high precision. The strength of this method is that the same scheme allows the computation of all these functions. We estimate the delay, the size/number of tables, and the size/number of multipliers and compare with other related methods.",
Generalized hidden Markov models. I. Theoretical frameworks,"In this paper, we present the theoretical framework for the generalization of classical hidden Markov models using fuzzy measures and fuzzy integrals. The main characteristic of the generalization is the relaxation of the usual additivity constraint of probability measures. Fuzzy integrals are defined with respect to fuzzy measures, whose key property is monotonicity with respect to set inclusion. This property is far weaker than the usual additivity property of probability measures. As a result of the new formulation, the statistical independence assumption of the classical hidden Markov models is relaxed. Two attractive properties of this generalization are: the generalized hidden Markov model reduces to the classical hidden Markov model if we used the Choquet fuzzy integral and probability measures; and the establishment of a relation between the generalized hidden Markov model and the classical nonstationary hidden Markov model in which the transitional parameters vary with time.",
The Jalapeño virtual machine,"Jalapeño is a virtual machine for Java™ servers written in the Java language. To be able to address the requirements of servers (performance and scalability in particular), Jalapeño was designed “from scratch“ to be as self-sufficient as possible. Jalapeño's unique object model and memory layout allows a hardware null-pointer check as well as fast access to array elements, fields, and methods. Run-time services conventionally provided in native code are implemented primarily in Java. Java threads are multiplexed by virtual processors (implemented as operating system threads). A family of concurrent object allocators and parallel type-accurate garbage collectors is supported. Jalapeño's interoperable compilers enable quasi-preemptive thread switching and precise location of object references. Jalapeño's dynamic optimizing compiler is designed to obtain high quality code for methods that are observed to be frequently executed or computationally intensive.",
Cost-based modeling for fraud and intrusion detection: results from the JAM project,"We describe the results achieved using the JAM distributed data mining system for the real world problem of fraud detection in financial information systems. For this domain we provide clear evidence that state-of-the-art commercial fraud detection systems can be substantially improved in stopping losses due to fraud by combining multiple models of fraudulent transaction shared among banks. We demonstrate that the traditional statistical metrics used to train and evaluate the performance of learning systems (ie. statistical accuracy or ROC analysis) are misleading and perhaps inappropriate for this application. Cost-based metrics are more relevant in certain domains, and defining such metrics poses significant and interesting research questions both in evaluating systems and alternative models, and in formalizing the problems to which one may wish to apply data mining technologies. This paper also demonstrates how the techniques developed for fraud detection can be generalized and applied to the important area of intrusion detection in networked information systems. We report the outcome of recent evaluations of our system applied to tcpdump network intrusion data specifically with respect to statistical accuracy. This work involved building additional components of JAM that we have come to call, MADAM ID (Mining Audit Data for Automated Models for Intrusion Detection). However, taking the next step to define cost-based models for intrusion detection poses interesting new research questions. We describe our initial ideas about how to evaluate intrusion detection systems using cost models learned during our work on fraud detection.",
An enabling framework for master-worker applications on the Computational Grid,"Describes MW (Master-Worker) - a software framework that allows users to quickly and easily parallelize scientific computations using the master-worker paradigm on the Computational Grid. MW provides both a ""top-level"" interface to application software and a ""bottom-level"" interface to existing Grid computing toolkits. Both interfaces are briefly described. We conclude with a case study, where the necessary Grid services are provided by the Condor high-throughput computing system, and the MW-enabled application code is used to solve a combinatorial optimization problem of unprecedented complexity.",
"A high-speed residue-to-binary converter for three-moduli (2/sup k/, 2/sup k/-1, 2/sup k-1/-1) RNS and a scheme for its VLSI implementation","In this paper, a high-speed residue-to-binary converter for the moduli set (2/sup k/, 2/sup k/-1, 2/sup k-1/-1) is proposed. Compared to the previous converter based on this moduli set, the proposed one is 40% faster. Also, the time-complexity product is improved by 20%. Following the top-down very large scale integration design flow, the proposed converter is implemented in 0.5 micron CMOS technology. Based on this moduli set, layouts of the 8-, 16-, 32-, and 64-bit residue-to-binary converters, which can be used in further residue number system designs, are generated and simulation results obtained.",
A Scalable Cross-Platform Infrastructure for Application Performance Tuning Using Hardware Counters,"The purpose of the PAPI project is to specify a standard API for accessing hardware performance counters available on most modern microprocessors. These counters exist as a small set of registers that count ""events"", which are occurrences of specific signals and states related to the processor's function. Monitoring these events facilitates correlation between the structure of source/object code and the efficiency of the mapping of that code to the underlying architecture. This correlation has a variety of uses in performance analysis and tuning. The PAPI project has proposed a standard set of hardware events and a standard cross-platform library interface to the underlying counter hardware. The PAPI library has been or is in the process of being implemented on all major HPC platforms. The PAPI project is developing end-user tools for dynamically selecting and displaying hardware counter performance data. PAPI support is also being incorporated into a number of third-party tools.",
Multibuffer delay line architectures for efficient contention resolution in optical switching nodes,"This paper proposes an efficient contention resolution switching architecture which can serve as the basis for all-optical switching nodes. The presented solution builds on fiber delay lines used as temporary optical storage and 2/spl times/2 space photonic switches, a solution principle also known as Quadro or switched delay lines (SDLs). The efficiency of SDLs is fundamentally linked to its storage capacity, i.e., the length of the fiber delay lines, while its cost depends on the number of 2/spl times/2 photonic switches, i.e., the number of stages in the switch. This work presents a solution that makes use of multibuffer fiber delay lines which allow multiple packets to be concurrently stored (propagated) on each line. With a novel switch control, it is shown that this solution increases the total storage capacity and significantly improves switch and network performance, without increasing the number of the 2/spl times/2 switches in the system, i.e., its cost.",
Spiral interpolation algorithm for multislice spiral CT. I. Theory,"This paper presents the adaptive axial interpolator (AAI), a novel spiral interpolation approach for multislice spiral computed tomography (CT) implemented in a clinical multislice CT scanner, the SOMATOM Volume Zoom (Siemens Medical Systems, Forchheim, Germany). The method works on parallel-beam data generated from the acquired fan-beam data by azimuthal rebinning. Spiral interpolation is performed by distance-dependent weighting; i.e., for each ray, its distance to the image plane is evaluated and serves as an argument to a freely selectable weighting function, resulting in a weight factor. A normalization step is applied to the weight factors to ensure that the sum of all corresponding weights (i.e., the weights applied to rays that contribute to the same ray in the interpolated sinogram) is 1. By selection of appropriate weighting functions and suitable adjustment of the tube current, it is possible to keep the slice sensitivity profiles (SSP) as well as the pixel noise constant for all pitch values in the relevant range. Also, a large range of slice-thickness can be reconstructed from a given collimation. The method is, thus, very versatile. Further advantages are that it uses the entire applied dose for imaging and allows for efficient implementation using a table lookup approach.",
ECG-correlated imaging of the heart with subsecond multislice spiral CT,"The new spiral multislice computed tomography (CT) scanners and the significant increase in rotation speed offer great potential for cardiac imaging with X-ray CT. The authors have therefore developed the dedicated cardiac reconstruction algorithms 180/spl deg/ multislice cardio interpolation (MCI) and 180/spl deg/ multislice cardio delta (MCD) and here offer further details and validation. The algorithm 180/spl deg/ MCI is an electrocardiogram (ECG)-correlated filtering (or weighting) algorithm in both the cardiac phase and in the z-position. Effective scan times (absolute temporal resolution) of as low as t/sub eff/=56 ms are possible, assuming M=4 simultaneously measured slices at a rotation time of t/sub r0t/=0.5 s and S/spl les/d/spl les/3S for the table feed d per rotation, where S denotes the collimated slice thickness. The relative temporal resolution w (fraction of the heart cycle depicted in the image), which is the more important parameter in cardiac imaging, will then be as low as w=12.5% of the heart cycle. The second approach, 180/spl deg/MCD, is an ECG-correlated partial scan reconstruction of 180/spl deg/+/spl delta/ data with /spl delta//spl Lt//spl Phi/ (fan-angle). Its absolute temporal resolution lies in the order of 250 ms (for the central ray, i.e., for the center of rotation), and the relative temporal resolution w increases with increasing heart rate, e.g., from typically w=25% at f/sub H/=60 min/sup -1/ to w=50% at f/sub H/=120 min/sup -1/, assuming again t/sub r0t/=0.5 s. For validation purposes, the authors have done simulations of a virtual cardiac motion phantom, measurements of a dedicated cardiac calibration and motion phantom, and they have reconstructed patient data with simultaneously acquired ECG. Both algorithms significantly improve the image quality compared with the standard reconstruction algorithms 180/spl deg/ multislice linear interpolation (MLI) and 180/spl deg/ multislice filtered interpolation (MFI). However, 180/spl deg/ MCI is clearly superior to 180/spl deg/MCD for all heart rates. This is best illustrated by multiplanar reformations (MPR) or other three-dimensional (3-D) displays of the volume, 180/spl deg/ MCI, due to its higher temporal resolution, is best for spatial and temporal four-dimensional (4-D) tracking of the anatomy. A tunable scanner rotation time to avoid resonance behavior of the heart rate and the scanner's rotation and shorter rotation times would be of further benefit.",
Granular neural networks for numerical-linguistic data fusion and knowledge discovery,"We present a neural-networks-based knowledge discovery and data mining (KDDM) methodology based on granular computing, neural computing, fuzzy computing, linguistic computing, and pattern recognition. The major issues include 1) how to make neural networks process both numerical and linguistic data in a database, 2) how to convert fuzzy linguistic data into related numerical features, 3) how to use neural networks to do numerical-linguistic data fusion, 4) how to use neural networks to discover granular knowledge from numerical-linguistic databases, and 5) how to use discovered granular knowledge to predict missing data. In order to answer the above concerns, a granular neural network (GNN) is designed to deal with numerical-linguistic data fusion and granular knowledge discovery in numerical-linguistic databases. From a data granulation point of view the GNN can process granular data in a database. From a data fusion point of view, the GNN makes decisions based on different kinds of granular data. From a KDDM point of view the GNN is able to learn internal granular relations between numerical-linguistic inputs and outputs, and predict new relations in a database. The GNN is also capable of greatly compressing low-level granular data to high-level granular knowledge with some compression error and a data compression rate. To do KDDM in huge databases, parallel GNN and distributed GNN will be investigated in the future.",
Chinese remaindering with errors,"The Chinese remainder theorem states that a positive integer m is uniquely specified by its remainder module k relatively prime integers p/sub 1/, /spl middot//spl middot//spl middot/, p/sub k/, provided m",
LROC analysis of detector-response compensation in SPECT,"Localization ROC (LROC) observer studies examined whether detector response compensation (DRC) in ordered-subset, expectation-maximization (OSEM) reconstructions helps in the detection and localization of hot tumors. Simulated gallium (Ga-67) images of the thoracic region were used in the study. The projection data modeled the acquisition of attenuated 93- and 185-keV photons with a medium-energy parallel-hole collimator, but scatter was not modeled. Images were reconstructed with five strategies: (1) OSEM with no DRC; (2) OSEM preceded by restoration filtering; (3) OSEM with iterative DRC; (4) OSEM with an ideal DRC; and (5) filtered backprojection (FBP) with no DRC. All strategies included attenuation correction. There were four LROC studies conducted. In a study using a single tumor activity, the ideal DRC offered the best performance, followed by iterative DRC, restoration filtering, OSEM with no DRC, and FBP. Statistical significance at the 5% level was found between all pairs of strategies except for restoration filtering and OSEM with no DRC. A similar ranking was found for a more realistic study using multiple tumor activities. Additional studies considered the effects of OSEM iteration number and tumor activity on the detection improvement that iterative DRC offered with respect to OSEM with no DRC.",
Reconstruction of MR images from data acquired on a general nonregular grid by pseudoinverse calculation,"A minimum-norm least-squares image-reconstruction method for the reconstruction of magnetic resonance images from non-Cartesian sampled data is proposed. The method is based on a general formalism for continuous-to-discrete mapping and pseudoinverse calculation. It does not involve any regridding or interpolation of the data and therefore the methodology differs fundamentally from existing regridding-based methods. Moreover, the method uses a continuous representation of objects in the image domain instead of a discretized representation. Simulations and experiments show the possibilities of the method in both radial and spiral imaging. Simulations revealed that minimum-norm least-squares image reconstruction can result in a drastic decrease of artifacts compared with regridding-based reconstruction. Besides, both in vivo and phantom experiments showed that minimum-norm least-squares image reconstruction leads to contrast improvement and increased signal-to-noise ratio compared with image reconstruction based on regridding. As an appendix, an analytical calculation of the raw data corresponding to the well-known Shepp and Logan software head phantom is presented.",
Noise characterization of block-iterative reconstruction algorithms. I. Theory,"Researchers have shown increasing interest in block-iterative image reconstruction algorithms due to the computational and modeling advantages they provide. Although their convergence properties have been well documented, little is known about how they behave in the presence of noise. In this work, the authors fully characterize the ensemble statistical properties of the rescaled block-iterative expectation-maximization (RBI-EM) reconstruction algorithm and the rescaled block-iterative simultaneous multiplicative algebraic reconstruction technique (RBI-SMART). Also included in the analysis are the special cases of RBI-EM, maximum-likelihood EM (ML-EM) and ordered-subset EM (OS-EM), and the special case of RBI-SMART, SMART. A theoretical formulation strategy similar to that previously outlined for ML-EM is followed for the RBI methods. The theoretical formulations in this paper rely on one approximation, namely, that the noise in the reconstructed image is small compared to the mean image. In a second paper, the approximation will be justified through Monte Carlo simulations covering a range of noise levels, iteration points, and subset orderings. The ensemble statistical parameters could then be used to evaluate objective measures of image quality.",
Quantizer design for distributed estimation with communication constraints and unknown observation statistics,We consider the problem of quantizer design in a distributed estimation system with communication constraints in the case where only a training sequence is available. Our approach is based on a generalization of regression trees. The look-ahead method that we also propose improves significantly the performance. The final system performs similarly to the one that assumes known statistics.,
Wavelet-based salient points for image retrieval,"The use of interest points in content-based image retrieval allows the image index to represent local properties of the image. Classic corner detectors can be used for this purpose. However, they have drawbacks when applied to various natural images for image retrieval, because visual features need not be corners and corners may gather in small regions. We present a salient point detector that extract points where variations occur in the image, whether they are corner-like or not. The detector is based on the wavelet transform to detect global variations as well as local ones. The wavelet-based salient points are evaluated for image retrieval with a retrieval system using texture features. In this experiment our method provides better retrieval performance compared with other point detectors.",
Surface interpolation from sparse cross sections using region correspondence,"The ability to estimate a surface from a set of cross sections allows calculation of the enclosed volume and the display of the surface in three-dimensions. This process has increasingly been used to derive useful information from medical data. However, extracting the cross sections (segmenting) can be very difficult, and automatic segmentation methods are not sufficiently robust to handle all situations. Hence, it is an advantage if the surface reconstruction algorithm can work effectively on a small number of cross sections. In addition, cross sections of medical data are often quite complex. Shape-based interpolation is a simple and elegant solution to this problem, although it has known limitations when handling complex shapes. In this paper, the shape-based interpolation paradigm is extended to interpolate a surface through sparse, complex cross sections, providing a significant improvement over the authors' previously published maximal disc-guided interpolation. The performance of this algorithm is demonstrated on various types of medical data (X-ray computed tomography, magnetic resonance imaging and three-dimensional ultrasound). Although the correspondence problem in general remains unsolved, it is demonstrated that correct surfaces can be estimated from a limited amount of real data, through the use of region rather than object correspondence.",
A step towards sequence-to-sequence alignment,"The paper presents an approach for establishing correspondences in time and in space between two different video sequences of the same dynamic scene, recorded by stationary uncalibrated video cameras. The method simultaneously estimates both spatial alignment as well as temporal synchronization (temporal alignment) between the two sequences, using all available spatio-temporal information. Temporal variations between image frames (such as moving objects or changes in scene illumination) are powerful cues for alignment, which cannot be exploited by standard image-to-image alignment techniques. We show that by folding spatial and temporal cues into a single alignment framework, situations which are inherently ambiguous for traditional image-to-image alignment methods, are often uniquely resolved by sequence-to-sequence alignment. We also present a ""direct"" method for sequence-to-sequence alignment. The algorithm simultaneously estimates spatial and temporal alignment parameters directly from measurable sequence quantities, without requiring prior estimation of point correspondences, frame correspondences, or moving object detection. Results are shown on real image sequences taken by multiple video cameras.","Layout,
Cameras,
Video sequences,
Electrical capacitance tomography,
Lighting,
Spatial resolution,
Pixel,
Computer science,
Space stations,
Image resolution"
Topological persistence and simplification,"We formalize a notion of topological simplification within the framework of a filtration, which is the history of a growing complex. We classify a topological change that happens during growth as either a feature or noise, depending on its life-time or persistence within the filtration. We give fast algorithms for completing persistence and experimental evidence for their speed and utility.",
Single point active alignment method (SPAAM) for optical see-through HMD calibration for AR,"Augmented reality (AR) is a technology in which a user's view of the real world is enhanced or augmented with additional information generated from a computer model. In order to have a working AR system, the see-through display system must be calibrated so that the graphics is properly rendered. The optical see-through systems present an additional challenge because we do not have access to the image data directly as in video see-through systems. This paper reports on a method we developed for optical see-through head-mounted displays. The method integrates the measurements for the camera and the magnetic tracker which is attached to the camera in order to do the calibration. The calibration is based on the alignment of image points with a single 3D point in the world coordinate system from various viewpoints. The user interaction to do the calibration is extremely easy compared to prior methods, and there is no requirement for keeping the head static while doing the calibration.","Calibration,
Cameras,
Computer graphics,
Augmented reality,
Displays,
Optical computing,
Biomedical optical imaging,
Application software,
Layout,
Information science"
Power-aware localized routing in wireless networks,"Two metrics where transmission power depends on distance between nodes, and a cost aware metric based on remaining battery power at nodes (assuming constant transmission power), together with corresponding non-localized shortest path routing algorithms, were recently proposed. We define a new power-cost metric based on the combination of both node's lifetime and distance based power metrics. We then propose power, cost, and power-cost GPS based localized routing algorithms, where nodes make routing decisions solely on the basis of location of their neighbors and destination. Power aware localized routing algorithm attempts to minimize the total power needed to route a message between a source and a destination. Cost-aware localized algorithm is aimed at extending battery's worst case lifetime. The combined power-cost algorithm attempts to minimize the total power needed and to avoid nodes with short remaining lifetime. We prove that these localized power, cost, and power-cost efficient routing algorithms are loop-free.","Routing,
Intelligent networks,
Wireless networks,
Wireless sensor networks,
Costs,
Mobile ad hoc networks,
Computer science,
Global Positioning System,
Remote monitoring,
Online Communities/Technical Collaboration"
A flow-guided streamline seeding strategy,"The paper presents a seed placement strategy for streamlines based on flow features in the dataset. The primary goal of our seeding strategy is to capture flow patterns in the vicinity of critical points in the flow field, even as the density of streamlines is reduced. Secondary goals are to place streamlines such that there is sufficient coverage in non-critical regions, and to vary the streamline placements and lengths so that the overall presentation is aesthetically pleasing (avoid clustering of streamlines, avoid sharp discontinuities across several streamlines, etc.). The procedure is straightforward and non-iterative. First, critical points are identified. Next, the flow field is segmented into regions, each containing a single critical point. The critical point in each region is then seeded with a template depending on the type of critical point. Finally, additional seed points are randomly distributed around the field using a Poisson disk distribution to minimize closely spaced seed points. The main advantage of this approach is that it does not miss the features around critical points. Since the strategy is not image-guided, and hence not view dependent, significant savings are possible when examining flow fields from different viewpoints, especially for 3D flow fields.","Streaming media,
Visualization,
Computer science,
NASA"
Memory hierarchy reconfiguration for energy and performance in general-purpose processor architectures,"Conventional microarchitectures choose a single memory hierarchy design point targeted at the average application. In this paper, we propose a cache and TLB layout and design that leverages repeater insertion to provide dynamic low-cost configurability trading off size and speed on a per application phase basis. A novel configuration management algorithm dynamically detects phase changes and reacts to an application's hit and miss intolerance in order to improve memory hierarchy performance while taking energy consumption into consideration. When applied to a two-level cache and TLB hierarchy at 0.1 /spl mu/m technology, the result is an average 15% reduction in cycles per instruction (CPI), corresponding to an average 27% reduction in memory-CPI, across a broad class of applications compared to the best conventional two-level hierarchy of comparable size. Projecting to sub-.1 /spl mu/m technology design considerations that call for a three-level conventional cache hierarchy for performance reasons, we demonstrate that a configurable L2/L3 cache hierarchy coupled with a conventional LI results in an average 43% reduction in memory hierarchy energy in addition to improved performance.","Delay,
Microprocessors,
Performance gain,
Power dissipation,
Energy efficiency,
Computer architecture,
Computer science,
Microarchitecture,
Repeaters,
Energy management"
The AppLeS Parameter Sweep Template: User-Level Middleware for the Grid,"The Computational Grid is a promising platform for the efficient execution of parameter sweep applications over large parameter spaces. To achieve performance on the Grid, such applications must be scheduled so that shared data files are strategically placed to maximize reuse, and so that the application execution can adapt to the deliverable performance potential of target heterogeneous, distributed and shared resources. Parameter sweep applications are an important class of applications and would greatly benefit from the development of Grid middleware that embeds a scheduler for performance and targets Grid resources transparently. In this paper we describe a user-level Grid middleware project, the AppLeS Parameter Sweep Template (APST), that uses application-level scheduling techniques [1] and various Grid technologies to allow the efficient deployment of parameter sweep applications over the Grid. We discuss several possible scheduling algorithms and detail our software design. We then describe our current implementation of APST using systems like Globus [2], NetSolve [3] and the Network Weather Service [4], and present experimental results.","Middleware,
Application software,
Processor scheduling,
Grid computing,
Adaptive scheduling,
Large-scale systems,
Computer science,
USA Councils,
Contracts,
Scheduling algorithm"
Approximately optimal assignment for unequal loss protection,This paper describes an algorithm that achieves an approximately optimal assignment of forward error correction to progressive data within the unequal loss protection framework. It first finds the optimal assignment under convex hull and fractional bit allocation assumptions. It then relaxes those constraints to find an assignment that approximates the global optimum. The algorithm has a running time of O(hNlogN) where h is the number of points on the convex hull of the source's utility-cost curve and N is the number of packets transmitted.,"Protection,
Computer networks,
Forward error correction,
Decoding,
Added delay,
Satellite broadcasting,
Cost function,
Computer science,
Bit rate,
IP networks"
Inferring body pose without tracking body parts,"A novel approach for estimating articulated body posture and motion from monocular video sequences is proposed. Human pose is defined as the instantaneous two dimensional configuration (i.e. the projection onto the image plane) of a single articulated body in terms of the position of a predetermined sets of joints. First, statistical segmentation of the human bodies from the background is performed and low-level visual features are found given the segmented body shape. The goal is to be able to map these generally low level visual features to body configurations. The system estimates different mappings, each one with a specific cluster in the visual feature space. Given a set of body motion sequences for training, unsupervised clustering is obtained via the Expectation Maximization algorithm. For each of the clusters, a function is estimated to build the mapping between low-level features to 2D pose. Given new visual features, a mapping from each cluster is performed to yield a set of possible poses. From this set, the system selects the most likely pose given the learned probability distribution and the visual feature of the proposed approach is characterized using real and artificially generated body postures, showing promising results.",
Quickly detecting relevant program invariants,"Explicitly stated program invariants can help programmers by characterizing certain aspects of program execution and identifying program properties that must be preserved when modifying code. Unfortunately, these invariants are usually absent from code. Previous work showed how to dynamically detect invariants from program traces by looking for patterns in and relationships among variable values. A prototype implementation, Daikon, accurately recovered invariants from formally-specified programs, and the invariants it detected in other programs assisted programmers in a software evolution task. However, Daikon suffered from reporting too many invariants, many of which were not useful, and also failed to report some desired invariants. The paper presents, and gives experimental evidence of the efficacy of, four approaches for increasing the relevance of invariants reported by a dynamic invariant detector. One of them (exploiting unused polymorphism), adds desired invariants to the output. The other three (suppressing implied invariants, limiting which variables are compared to one another, and ignoring unchanged values), eliminate undesired invariants from the output and also improve runtime by reducing the work done by the invariant detector.","Runtime,
Programming profession,
Computer science,
Detectors,
Testing,
Permission,
Engines,
Java,
Costs,
Data mining"
Integer programming for combinatorial auction winner determination,"Combinatorial auctions are important as they enable bidders to place bids on combinations of items. Compared to other auction mechanisms, they often increase the efficiency of the auction, while keeping low risks for bidders. However, the determination of an optimal winner combination in combinatorial auctions is a complex computational problem. In this paper we: 1) compare recent algorithms for winner determination to traditional algorithms; 2) present and benchmark a mixed integer programming approach to the problem, which enables very general auctions to be treated efficiently by standard integer programming algorithms (and hereby also by commercially available software); and 3) discuss the impact of the probability distributions chosen for benchmarking.","Linear programming,
Partitioning algorithms,
Software standards,
Software algorithms,
Benchmark testing,
Information technology,
Probability distribution,
Computer science,
Operations research,
Software testing"
The Metropolis Algorithm,"The Metropolis Algorithm has been the most successful and influential of all the members of the computational species that used to be called the ""Monte Carlo method"". Today, topics related to this algorithm constitute an entire field of computational science supported by a deep theory and having applications ranging from physical simulations to the foundations of computational complexity. Since the rejection method invention (J. von Neumann), it has been developed extensively and applied in a wide variety of settings. The Metropolis Algorithm can be formulated as an instance of the rejection method used for generating steps in a Markov chain.","Physics computing,
Sampling methods,
Monte Carlo methods,
Computational modeling,
Computational complexity,
Hospitals,
Probability distribution,
Distributed computing,
Distribution functions"
Choosing good distance metrics and local planners for probabilistic roadmap methods,"This paper presents a comparative evaluation of different distance metrics and local planners within the context of probabilistic roadmap methods for planning the motion of rigid objects in three-dimensional workspaces. The study concentrates on cluttered three-dimensional workspaces typical of, for example, virtual prototyping applications such as maintainability studies in mechanical CAD designs. Our results include recommendations for selecting appropriate combinations of distance metrics and local planners for such applications. Our study of distance metrics shows that the importance of the translational distance increases relative to the rotational distance as the environment becomes more crowded. We find that each local planner makes some connections that none of the others does-indicating that better connected roadmaps will be constructed using multiple local planners. We propose a new local planning method we call rotate-at-s that often outperforms the common straight-line in C-space method in crowded environments.","Motion planning,
Robotics and automation,
Joining processes,
Computer science,
Design automation,
Robot kinematics,
Virtual prototyping,
Application software,
Virtual reality,
Scholarships"
A new approach for TU complex characterization,"Presents a new TU complex detection and characterization algorithm that consists of two stages; the first is a mathematical modeling of the electrocardiographic segment after QRS complex; the second uses classic threshold comparison techniques, over the signal and its first and second derivatives, to determine the significant points of each wave. Later, both T and U waves are morphologically classified. Amongst the principal innovations of this algorithm is the inclusion of U-wave characterization and a mathematical modeling stage, that avoids many of the problems of classic techniques when there is a low signal-to-noise ratio or when wave morphology is atypical. The results of the algorithm validation with the recently appeared QT database are also shown. For T waves these results are better when compared to other existing algorithms. U-wave results cannot be contrasted with other algorithms as, to the authors' knowledge, none are available. Examples showing the causes of principal discrepancies between the authors' algorithm and the QT database annotations are also given, and some ways of attempting to improve and benefit from the proposed algorithm are suggested.","Signal processing algorithms,
Mathematical model,
Morphology,
Databases,
Electrocardiography,
Computer science,
Technological innovation,
Signal to noise ratio,
Signal processing,
Cardiology"
Using Presence Questionnaires in Reality,"A between-group experiment was carried out to assess whether two different presence questionnaires can distinguish between real and virtual experiences. One group of ten subjects searched for a box in a real office environment. A second group of ten subjects carried out the same task in a virtual environment that simulated the same office. Immediately after their experience, subjects were given two different presence questionnaires in randomized order: the Witmer and Singer Presence (WS), and the questionnaire developed by Slater, Usoh, and Steed (SUS). The paper argues that questionnaires should be able to pass a “reality test” whereby under current conditions the presence scores should be higher for real experiences than for virtual ones. Nevertheless, only the SUS had a marginally higher mean score for the real compared to the virtual, and there was no significant difference at all between the WS mean scores. It is concluded that, although such questionnaires may be useful when all subjects experience the same type of environment, their utility is doubtful for the comparison of experiences across environments, such as immersive virtual compared to real, or desktop compared to immersive virtual.",
A direction-based location update scheme with a line-paging strategy for PCS networks,"On the problem of location update and terminal paging, many schemes using ring-paging strategies have been proposed. However, sequentially paging the rings surrounding the mobile user's last updated location may cause large paging cost. We propose a direction-based location update (DBLU) scheme using a line-paging strategy to reduce the paging cost. A moving direction identification mechanism using only simple computations detects the change of moving direction and updates the mobile's location. The numerical results show that our DBLU scheme achieves good performance when the paging cost is high.",
Reconstruction of attenuation map using discrete consistency conditions,"Methods of quantitative emission computed tomography require compensation for linear photon attenuation. A current trend in single-photon emission computed tomography (SPECT) and positron emission tomography (PET) is to employ transmission scanning to reconstruct the attenuation map. Such an approach, however, considerably complicates both the scanner design and the data acquisition protocol. A dramatic simplification could be made if the attenuation map could be obtained directly from the emission projections, without the use of a transmission scan. This can be done by applying the consistency conditions that enable one to identify the operator of the problem and, thus, to reconstruct the attenuation map. Here, the authors propose a new approach based on the discrete consistency conditions. One of the main advantages of the suggested method over previously used continuous conditions is that it can easily be applied in various scanning configurations, including fully three-dimensional (3-D) data acquisition protocols. Also, it provides a stable numerical implementation, allowing one to avoid the crosstalk between the attenuation map and the source function. A computationally efficient algorithm is implemented by using the QR and Cholesky decompositions. Application of the algorithm to computer-generated and experimentally measured SPECT data is considered.",
An empirical evaluation of client-side server selection algorithms,"Efficient server selection algorithms reduce retrieval time for objects replicated on different servers and are an important component of Internet cache architectures. This paper empirically evaluates six client-side server selection algorithms. The study compares two statistical algorithms, one using median bandwidth and the other median latency, a dynamic probe algorithm, two hybrid algorithms, and random selection. The server pool includes a topologically dispersed set of United States state government Web servers. Experiments were run on three clients in different cities and on different regional networks. The study examines the effects of time-of day, client resources, and server proximity. Differences in performance highlight the degree of algorithm adaptability and the effect that network upgrades can have on statistical estimators. Dynamic network probing performs as well or better than the statistical bandwidth algorithm and the two probe bandwidth hybrid algorithms. The statistical latency algorithm is clearly worse, but does outperform random selection.",
Test-case generator for nonlinear continuous parameter optimization techniques,"The experimental results reported in many papers suggest that making an appropriate a priori choice of an evolutionary method for a nonlinear parameter optimization problem remains an open question. It seems that the most promising approach at this stage of research is experimental, involving the design of a scalable test suite of constrained optimization problems, in which many features could be tuned easily. It would then be possible to evaluate the merits and drawbacks of the available methods, as well as to test new methods efficiently. In this paper, we propose such a test-case generator for constrained parameter optimization techniques. This generator is capable of creating various test problems with different characteristics including: 1) problems with different relative sizes of the feasible region in the search space; 2) problems with different numbers and types of constraints; 3) problems with convex or nonconvex evaluation functions, possibly with multiple optima; and 4) problems with highly nonconvex constraints consisting of (possibly) disjoint regions. Such a test-case generator is very useful for analyzing and comparing different constraint-handling techniques.",
Utility-based decision-making in wireless sensor networks,"We consider challenges associated with application domains in which a large number of distributed, networked sensors must perform a sensing task repeatedly over time. We address issues such as resource constraints, utility associated with a sensing task, and achieving global objectives with only local information. We present a model for such applications, in which we define appropriate global objectives based on utility functions and specify a cost model for energy consumption.",
Buffer overflows: attacks and defenses for the vulnerability of the decade,"Buffer overflows have been the most common form of security vulnerability for the last ten years. Moreover, buffer overflow vulnerabilities dominate the area of remote network penetration vulnerabilities, where an anonymous Internet user seeks to gain partial or total control of a host. If buffer overflow vulnerabilities could be effectively eliminated, a very large portion of the most serious security threats would also be eliminated. We survey the various types of buffer overflow vulnerabilities and attacks and survey the various defensive measures that mitigate buffer overflow vulnerabilities, including our own StackGuard method. We then consider which combinations of techniques can eliminate the problem of buffer overflow vulnerabilities, while preserving the functionality and performance of existing systems.",
The equivalence between fuzzy logic systems and feedforward neural networks,"Demonstrates that fuzzy logic systems and feedforward neural networks are equivalent in essence. First, we introduce the concept of interpolation representations of fuzzy logic systems and several important conclusions. We then define mathematical models for rectangular wave neural networks and nonlinear neural networks. With this definition, we prove that nonlinear neural networks can be represented by rectangular wave neural networks. Based on this result, we prove the equivalence between fuzzy logic systems and feedforward neural networks. This result provides us a very useful guideline when we perform theoretical research and applications on fuzzy logic systems, neural networks, or neuro-fuzzy systems.",
Source-adaptive multilayered multicast algorithms for real-time video distribution,"Layered transmission of data is often recommended as a solution to the problem of varying bandwidth constraints in multicast video applications. Multilayered encoding, however, is not sufficient to provide high video quality and high network utilization, since bandwidth constraints frequently change over time. Adaptive techniques capable of adjusting the rates of video layers are required to maximize video quality and network utilization. We define a class of algorithms known as source-adaptive multilayered multicast (SAMM) algorithms. In SAMM algorithms, the source uses congestion feedback to adjust the number of generated layers and the bit rate of each layer. We contrast two specific SAMM algorithms: an end-to-end algorithm, in which only end systems monitor available bandwidth and report the amount of available bandwidth to the source, and a network-based algorithm, in which intermediate nodes also monitor and report available bandwidth. Using simulations that incorporate multilayered video codecs, we demonstrate that SAMM algorithms can exhibit better scalability and responsiveness to congestion than algorithms that are not source-adaptive. We also study the performance trade-offs between end-to-end and network-based SAMM algorithms.",
A fully associative software-managed cache design,"As DRAM access latencies approach a thousand instruction-execution times and on-chip caches grow to multiple megabytes, it is not clear that conventional cache structures continue to be appropriate. Two key features-full associativity and software management-have been used successfully in the virtual-memory domain to cope with disk access latencies. Future systems will need to employ similar techniques to deal with DRAM latencies. This paper presents a practical, fully associative, software-managed secondary cache system that provides performance competitive with or superior to traditional caches without OS or application involvement. We see this structure as the first step toward OS- and application-aware management of large on-chip caches. This paper has two primary contributions a practical design for a fully associative memory structure, the indirect index cache (IIC), and a novel replacement algorithm, generational replacement, that is specifically designed to work with the IIC. We analyze the behavior of an IIC with generational replacement as a drop-in, transparent substitute for a conventional secondary cache. We achieve miss rate reductions from 8% to 85% relative to a 4-way associative LRU organization, matching or beating a (practically infeasible) fully associative true LRU cache. Incorporating these miss rates into a rudimentary timing model indicates that the IIC/generational replacement cache could be competitive with a conventional cache at today's DRAM latencies, and will outperform a conventional cache as these CPU-relative latencies grow.",
Absolute exponential stability of neural networks with a general class of activation functions,"The authors investigate the absolute exponential stability (AEST) of neural networks with a general class of partially Lipschitz continuous (defined in Section II) and monotone increasing activation functions. The main obtained result is that if the interconnection matrix T of the network system satisfies that -T is an H-matrix with nonnegative diagonal elements, then the neural network system is absolutely exponentially stable (AEST); i.e., that the network system is globally exponentially stable (GES) for any activation functions in the above class, any constant input vectors and any other network parameters. The obtained AEST result extends the existing ones of absolute stability (ABST) of neural networks with special classes of activation functions in the literature.","Neural networks,
Stability analysis,
Integrated circuit interconnections,
Neurons,
Quadratic programming,
Councils,
Computer science,
Automation"
The CRONE toolbox for Matlab,"A new CAD tool, the CRONE Matlab toolbox, is presented for engineers and researchers in mathematics and engineering sciences, particularly for electrical and electronic engineering and automatic control. The originality, aims, form and use of the toolbox are presented. The three main modules are described: their content, principle, algorithms, and applications for identification and control. Examples of their use are given. The original theoretical mathematical concepts developed in our laboratory based on noninteger differentiation are used for the computation. CRONE control is a typical application. The toolbox is not only for researchers, including Ph.D students, but also for industrials whose growing interest in noninteger integration, and their readiness to invest in the development of its applications, are manifest. The paper is divided into four sections: aims and organization of the toolbox, ""mathematical tools"" module, ""identification by noninteger model"" module, and the ""CRONE CSD"" module. The final section discusses perspectives.",
"ROSS: a high-performance, low memory, modular time warp system","We introduce a new time warp system called ROSS: Rensselaer's Optimistic Simulation System. ROSS is an extremely modular kernel that is capable of achieving event rates as high as 1,250,000 events per second when simulating a wireless telephone network model (PCS) on a quad processor PC server. In a head-to-head comparison, we observe that ROSS out performs the Georgia Tech Time Warp (GTW) system on the same computing platform by up to 180%. ROSS only requires a small constant amount of memory buffers greater than the amount needed by the sequential simulation for a constant number of processors. The driving force behind these high-performance and low memory utilization results is the coupling of an efficient pointer-based implementation framework, Fujimoto's (1989) fast GVT algorithm for shared memory multiprocessors, reverse computation and the introduction of kernel processes (KPs). KPs lower fossil collection overheads by aggregating processed event lists. This aspect allows fossil collection to be done with greater frequency, thus lowering the overall memory necessary to sustain stable, efficient parallel execution.",
Discovering relevant scientific literature on the Web,"Scientific literature on the Web makes up a massive, noisy, disorganized database. Unlike large, single-source databases such as a corporate customer database, the Web database draws from many sources, each with its own organization. Also, owing to its diversity, most records in this database are irrelevant to an individual researcher. Furthermore, the database is constantly growing in content and changing in organization. All these characteristics make the Web a difficult domain for knowledge discovery. To quickly and easily gather useful knowledge from such a database, users need the help of an information filtering system that automatically extracts only relevant records as they appear in a stream of incoming records. To this end, we have developed the CiteSeer. CiteSeer is an automatic generator of digital libraries of scientific literature. It uses sophisticated acquisition, parsing, and presentation methods to eliminate most of the manual effort of finding useful publications on the Web.",
On simultaneous global external and global internal stabilization of critically unstable linear systems with saturating actuators,"This paper deals with simultaneous global external as well as global internal stabilization of linear systems with saturating actuators. The paper proposes a new family of scheduled low-and-high gain state feedback laws that yields a closed-loop system that is both globally finite gain L/sub p/ stable and globally asymptotically stable. Moreover, the controller has an explicit design parameter that can be adjusted to make the L/sub p/ gain of the closed-loop system arbitrarily small.",
Weight adaptation and oscillatory correlation for image segmentation,"We propose a method for image segmentation based on a neural oscillator network. Unlike previous methods, weight adaptation is adopted during segmentation to remove noise and preserve significant discontinuities in an image. Moreover, a logarithmic grouping rule is proposed to facilitate grouping of oscillators representing pixels with coherent properties. We show that weight adaptation plays the roles of noise removal and feature preservation. In particular, our weight adaptation scheme is insensitive to termination time and the resulting dynamic weights in a wide range of iterations lead to the same segmentation results. A computer algorithm derived from oscillatory dynamics is applied to synthetic and real images, and simulation results show that the algorithm yields favorable segmentation results in comparison with other recent algorithms. In addition, the weight adaptation scheme can be directly transformed to a novel feature-preserving smoothing procedure. We also demonstrate that our nonlinear smoothing algorithm achieves good results for various kinds of images.",
Extracting randomness from samplable distributions,"The standard notion of a randomness extractor is a procedure which converts any weak source of randomness into an almost uniform distribution. The conversion necessarily uses a small amount of pure randomness, which can be eliminated by complete enumeration in some, but not all, applications. We consider the problem of deterministically converting a weak source of randomness into an almost uniform distribution. Previously, deterministic extraction procedures were known only for sources satisfying strong independence requirements. We look at sources which are samplable, i.e. can be generated by an efficient sampling algorithm. We seek an efficient deterministic procedure that, given a sample from any samplable distribution of sufficiently large min-entropy, gives an almost uniformly distributed output. We explore the conditions under which such deterministic extractors exist. We observe that no deterministic extractor exists if the sampler is allowed to use more computational resources than the extractor. On the other hand, if the extractor is allowed (polynomially) more resources than the sampler, we show that deterministic extraction becomes possible. This is true unconditionally in the nonuniform setting (i.e., when the extractor can be computed by a small circuit), and (necessarily) relies on complexity assumptions in the uniform setting.",
Tiling Optimizations for 3D Scientific Computations,"Compiler transformations can significantly improve data locality for many scientific programs. In this paper, we show iterative solvers for partial differential equations (PDEs) in three dimensions require new compiler optimizations not needed for 2D codes, since reuse along the third dimension cannot fit in cachefor larger problem sizes. Tiling is a program transformation compilers can apply to capture this reuse, but successful application of tiling requires selection of non-conflicting tiles and/or padding array dimensions to eliminate conflicts. We present new algorithms and cost models for selecting tiling shapes and array pads. We explain why tiling is rarely needed for 2D PDE solvers, but can be helpful for 3D stencil codes. Experimental results show tiling 3D codes can reduce miss rates and achieve performance improvements of 17-121% for key scientific kernels, including a 27% average improvement for the key computational loop nest in the SPEC/NAS benchmark MGRID.","Jacobian matrices,
Program processors,
Kernel,
Partial differential equations,
Microprocessors,
Computer science,
Educational institutions,
Optimizing compilers,
Tiles,
Costs"
"A low latency, loss tolerant architecture and protocol for wide area group communication","Group communication systems are proven tools upon which to build fault-tolerant systems. As the demands for fault-tolerance increase and more applications require reliable distributed computing over wide area networks, wide area group communication systems are becoming very useful. However, building a wide area group communication system is a challenge. This paper presents the design of the transport protocols of the spread wide area group communication system. We focus on two aspects of the system. First, the value of using overlay networks for application level group communication services. Second, the requirements and design of effective low latency link protocols used to construct wide area group communication. We support our claims with the results of live experiments conducted over the Internet.",
"Acquiring robust, force-based assembly skills from human demonstration","Robots have been used successfully in structured settings, where the environment is controlled; this research is inspired by the vision of robots moving beyond structured, controlled settings. The work focuses on the problem of teaching robots force-based assembly skills from human demonstration. To avoid position dependencies, force-based discrete states (contact formations) are used to describe qualitatively how contact is being made with the environment. Sensorimotor skills are modeled using a hybrid control model, which provides a mechanism for combining continuous low-level force control with higher-level discrete event control. A change in qualitative, discrete state constitutes an event and triggers a new control command to the robot, which moves the assembly toward a new contact formation. In this way, the skill execution is not dependent on absolute position but rather responds to changes in the force-based qualitative state. Experimental results are presented which validate the approach and show how skill acquisition can be accomplished even with an imperfect demonstration.",
Monitoring head/eye motion for driver alertness with one camera,"We describe a system for analyzing human driver alertness. It relies on optical flow and color predicates to robustly track a person's head and facial features. Our system classifies rotation in all viewing directions, detects eye/mouth occlusion, detects eye blinking, and recovers the 3D gaze of the eyes. We show results and discuss how this system can be used for monitoring driver alertness.","Cameras,
Eyes,
Image motion analysis,
Head,
Colored noise,
Motion pictures,
Vehicles,
Testing,
Computerized monitoring,
Computer science"
Application-layer mobility using SIP,"Supporting mobile Internet multimedia applications requires more than just the ability to maintain connectivity across subnet changes. We describe how the Session Initiation Protocol (SIP) can help provide terminal, personal, session and service mobility to applications ranging from Internet telephony to presence and instant messaging. We also discuss application-layer mobility for streaming multimedia applications initiated by RTSP.",
A task duplication based scheduling algorithm for heterogeneous systems,"Optimal scheduling of tasks of a directed acyclic graph (DAG) onto a set of processors is a strong NP-hard problem. In this paper we present a scheduling scheme called TDS to schedule tasks of a DAG onto a heterogeneous system. This models a network of workstations, with processors of varying computing power. The primary objective of this scheme is to minimize schedule length and scheduling time itself. The existing task duplication based scheduling scheme is primarily done for totally homogeneous systems. We compare the performance of this algorithm with an existing scheduling scheme for heterogeneous processors called BIL. In initial simulations TDS has been observed to generate scheduling lengths shorter than that of BIL, for communication-to-computation cost ratios (CCR) of 0.2 to 1. Moreover TDS is far more superior than BIL as far as scheduling time is concerned.",
Preliminary experiments in cooperative human/robot force control for robot assisted microsurgical manipulation,"Reports preliminary experiments with a robot system designed to cooperatively extend a human's ability to perform fine manipulation tasks requiring human judgement, sensory integration and hand-eye coordination. A completed steady-hand robot is reported. A stable force control law is reviewed. Preliminary experiments validate theoretical predictions of stable one-dimensional control of tool-tip forces in contact with both linearly and nonlinearly compliant objects. Preliminary feasibility experiments demonstrate stable one-dimensional robotic augmentation and ""force scaling"" of a human operator's tactile input.",
Scalable reliable multicast using multiple multicast channels,"We examine an approach for providing reliable, scalable multicast communication, involving the use of multiple multicast channels for reducing receiver processing costs and reducing network bandwidth consumption in a multicast session. In this approach a single multicast channel is used for the original transmission of packets. Retransmissions of packets are done on separate multicast channels, which receivers dynamically join and leave. We first show that protocols using an infinite number of multicast channels incur much less processing overhead at the receivers compared to protocols that use only a single multicast channel. This is due to the fact that receivers do not receive retransmissions of packets they have already received correctly. Next, we derive the number of unwanted redundant packets at a receiver due to using only a finite number of multicast channels, for a specific negative acknowledgment (NAK)-based protocol. We then explore the minimum number of multicast channels required to keep the cost of processing unwanted packets to a sufficiently low value. For an application consisting of a single sender transmitting reliably to many receivers we find that only a small number of multicast channels are required for a wide range of system parameters. In the case of an application where all participants simultaneously act as both senders and receivers a moderate number of multicast channels is needed. Finally, we present two mechanisms for implementing multiple multicast channels, one using multiple IP multicast groups and the other using additional router support for selective packet forwarding. We discuss the impact of both mechanisms on performance in terms of end-host and network resources.","Multicast protocols,
Telecommunication network reliability,
Bandwidth,
Costs,
Associate members,
Multicast communication,
Computer science,
Computer network reliability"
BGRP: Sink-tree-based aggregation for inter-domain reservations,"Resource reservation must operate in an efficient and scalable fashion, to accommodate the rapid growth of the Internet. In this paper, we describe a distributed architecture for inter-domain aggregated resource reservation for unicast traffic. We also present an associated protocol, called the Border Gateway Reservation Protocol (BGRP), that scales well, in terms of message processing load, state storage and bandwidth. Each stub or transit domain may use its own intra-domain resource reservation protocol. BGRP builds a sink tree for each of the stub domains. Each sink tree aggregates bandwidth reservations from all data sources in the network. Since backbone routers maintain only the sink tree information, the total number of reservations at each router scales linearly with the number of Internet domains N. (Even aggregated versions of the current protocol RSVP have a reservation count that can grow like O(N2).) BGRP maintains these aggregated reservations using “soft state.” To further reduce the protocol message traffic, routers may reserve bandwidth beyond the current load, so that some sources can join or leave the tree without sending messages all the way to the tree root. BGRP relies on Differentiated Services for data forwarding, hence the number of packet classifier entries is extremely small.",
Normalization of local contrast in mammograms,"Equalizing image noise has been shown to be an important step in automatic detection of microcalcifications in digital mammograms. In this study, an accurate adaptive approach for noise equalization is presented and investigated. No additional information obtained from phantom recordings is improved in the method, which makes the approach robust and independent of film type and film development characteristics. Furthermore, it is possible to apply the method on direct digital mammograms as well. In this study, the adaptive approach is optimized by investigating a number of alternative approaches to estimate the image noise. The estimation of high-frequency noise as a function of the grayscale is improved by a new technique for dividing the grayscale in sample intervals and by using a model for additive high-frequency noise. It is shown that the adaptive noise equalization gives substantially better detection results than does a fixed noise equalization. A large database of 245 digitized mammograms with 341 clusters was used for evaluation of the method.",
Comparison of theory and experiment for dispersion-managed solitons in a recirculating fiber loop,We have developed a model that accurately predicts the dynamics of the signal pulses and the growth of amplified spontaneous emission noise in a dispersion-managed soliton pulse train propagating in a recirculating fiber-loop experiment. Theoretically predicted dependencies of the amplitude and phase margins for the marks and the amplitude margin for the spaces as a function of distance are in remarkable agreement with the experiments. This model allows us to determine the key physical effects that limit the propagation distance in our experiments.,
TSA-tree: a wavelet-based approach to improve the efficiency of multi-level surprise and trend queries on time-series data,"We introduce a novel wavelet based tree structure, termed TSA-tree, which improves the efficiency of multi-level trend and surprise queries on time sequence data. With the explosion of scientific observation data conceptualized as time sequences, we are facing the challenge of efficiently storing, retrieving and analyzing this data. Frequent queries on this data set are to find trends (e.g., global warming) or surprises (e.g., undersea volcano eruption) within the original time series. The challenge, however is that these trend and surprise queries are needed at different levels of abstractions. To support these multi-level trend and surprise queries, sometimes a huge subset of raw data needs to be retrieved and processed. To expedite this process, we utilize our TSA-tree. Each node of the TSA-tree contains pre-computed trends and surprises at different levels. A wavelet transform is used recursively to construct TSA nodes. As a result, each node of TSA tree is readily available for visualization of trends and surprises. In addition, the size of each node is significantly smaller than that of the original time series, resulting in faster I/O operations. However a limitation of TSA-tree is that its size is larger than the original time series. To address this shortcoming, first we prove that the storage space required to store the optimal subtree of TSA-tree (OTSA-tree) is no more than that required to store the original time series without losing any information. Next, we propose two alternative techniques to reduce the size of the OTSA-tree even further while maintaining an acceptable query precision as compared to querying the original time sequences. Utilizing real and synthetic time sequence databases, we compare our techniques with some well known algorithms.","Databases,
Satellite ground stations,
Data analysis,
NASA,
Land surface temperature,
Computer science,
Tree data structures,
Explosions,
Global warming,
Volcanoes"
Nonholonomic Orthogonal Learning Algorithms for Blind Source Separation,"Independent component analysis or blind source separation extracts independent signals from their linear mixtures without assuming prior knowledge of their mixing coefficients. It is known that the independent signals in the observed mixtures can be successfully extracted except for their order and scales. In order to resolve the indeterminacy of scales, most learning algorithms impose some constraints on the magnitudes of the recovered signals. However, when the source signals are nonstationary and their average magnitudes change rapidly, the constraints force a rapid change in the magnitude of the separating matrix. This is the case with most applications (e.g., speech sounds, electroencephalogram signals). It is known that this causes numerical instability in some cases. In order to resolve this difficulty, this article introduces new nonholonomic constraints in the learning algorithm. This is motivated by the geometrical consideration that the directions of change in the separating matrix should be orthogonal to the equivalence class of separating matrices due to the scaling indeterminacy. These constraints are proved to be nonholonomic, so that the proposed algorithm is able to adapt to rapid or intermittent changes in the magnitudes of the source signals. The proposed algorithm works well even when the number of the sources is overestimated, whereas the existent algorithms do not (assuming the sensor noise is negligibly small), because they amplify the null components not included in the sources. Computer simulations confirm this desirable property.",
Text detection and segmentation in complex color images,"Text is a very powerful index in content-based image and video indexing. We propose a new text detection and segmentation algorithm that is especially designed for being applied to color images with complicated background. Our goal is to minimize the number of false alarms and to binarize efficiently the detected text areas so that they can be processed by standard OCR software. First, potential areas of text are detected by enhancement and clustering processes, considering most of constraints related to the texture of words. Then, classification and binarization of potential text areas are achieved in a single scheme performing color quantization and characters periodicity analysis. We report a high rate of good detection results with very few false alarms and reliable text binarization.",
Toward computers that recognize and respond to user emotion,"For a long time emotions have been kept out of the deliberate tools of science; scientists have expressed emotion, but no tools could sense and respond to their affective information. This paper highlights research at the MIT Media Laboratory aimed at giving computers the ability to comfortably sense, recognize, and respond to the human communication of emotion, especially affective states such as frustration, confusion, interest, distress, anger, and joy. Two main themes of sensing—self-report and concurrent expression—are described, together with examples of systems that give users new ways to communicate emotion to computers and, through computers, to other people. In addition to building systems that try to elicit and detect frustration, our research group has built a system that responds to user frustration in a way that appears to help alleviate it. This paper highlights applications of this research to interface design, wearable computing, entertainment, and education, and briefly presents some potential ethical concerns and how they might be addressed.",
Variance of aggregated Web traffic,"If data traffic were Poisson, increases in the amount of traffic aggregated on a network would rapidly decrease the relative size of bursts. The discovery of pervasive long-range dependence demonstrates that real network traffic is burstier than any possible Poisson model. We present evidence that, despite being non-Poisson, aggregating Web traffic causes it to smooth out as rapidly as Poisson traffic. That is, the relationship between changes in mean bandwidth and changes in variance is the same for Web traffic as it is for Poisson traffic. We derive our evidence from traces of real traffic in two ways: first, by observing how variance changes over the large range of mean bandwidths present in 24-hour traces; second, by observing the relationship of variance and mean bandwidth for individual users and combinations of users. Our conclusion, that variance changes linearly with mean bandwidth, should be useful (and encouraging) to anyone provisioning a network for a large aggregate load of Web traffic.",
"Firewall security: policies, testing and performance evaluation","Explores the firewall security and performance relationships for distributed systems. Experiments are conducted to set firewall security into seven different levels and to quantify their performance impacts. These firewall security levels are formulated, designed, implemented and tested, phase by phase, under an experimental environment in which all performed tests are evaluated and compared. Based on the test results, the impacts of the various firewall security levels on system performance with respect to transaction time and latency are measured and analyzed. It is interesting to note that the intuitive belief about security's relationship to performance, i.e. that more security would result in less performance, does not always hold in firewall testing. The results reveal that a significant impact of enhanced security on performance could only be observed under some particular scenarios, and thus their relationship is not necessarily inversely related. We also discuss the tradeoff between security and performance.",
Knowledge discovery from data?,"The knowledge discovery and data mining (KDD) field draws on findings from statistics, databases, and artificial intelligence to construct tools that let users gain insight from massive data sets. People in business, science, medicine, academia, and government collect such data sets, and several commercial packages now offer general-purpose KDD tools. An important KDD goal is to ""turn data into knowledge"". For example, knowledge acquired through such methods on a medical database could be published in a medical journal. Knowledge acquired from analyzing a financial or marketing database could revise business practice and influence a management school's curriculum. In addition, some US laws require reasons for rejecting a loan application, which knowledge from the KDD could provide. Occasionally, however, you must explain the learned decision criteria to a court, as in the recent lawsuit Blue Mountain filed against Microsoft for a mail filter that classified electronic greeting cards as spam mail. We expect more from knowledge discovery tools than simply creating accurate models as in machine learning, statistics, and pattern recognition. We can fully realize the benefits of data mining by paying attention to the cognitive factors that make the resulting models coherent, credible, easy to use, and easy to communicate to others.",
Statistical gait description via temporal moments,"Statistical recognition techniques have already been shown to achieve good performance in automatic gait recognition. However, the metrics were only statistical in nature and did not describe the intimate nature of gait. Accordingly, new velocity moments have been developed to describe an object and its motion throughout an image sequence. These moments are an extended form of centralised moments and compute descriptions of the object and its behaviour evaluation shows that the velocity moments have the required descriptive capability and analysis on synthetic imagery shows that the velocity moments are less sensitive to noise than an averaged comparator moment. This is largely due to the integration of data from the whole sequence. An extraction procedure has been developed to find moving human subjects and we are currently evaluating the performance of this promising new approach in automatic gait recognition.",
Private quantum channels,"We investigate how a classical private key can be used by two players, connected by an insecure one-way quantum channel, to perform private communication of quantum information. In particular, we show that in order to transmit n qubits privately, 2n bits of shared private key are necessary and sufficient. This result may be viewed as the quantum analogue of the classical one-time pad encryption scheme.",
The relationship between public key encryption and oblivious transfer,"In this paper we study the relationships among some of the most fundamental primitives and protocols in cryptography: public-key encryption (i.e. trapdoor predicates), oblivious transfer (which is equivalent to general secure multi-party computation), key agreement and trapdoor permutations. Our main results show that public-key encryption and oblivious transfer are incomparable under black-box reductions. These separations are tightly matched by our positive results where a restricted (strong) version of one primitive does imply the other primitive. We also show separations between oblivious transfer and key agreement. Finally, we conclude that neither oblivious transfer nor trapdoor predicates imply trapdoor permutations. Our techniques for showing negative results follow the oracle separations of R. Impagliazzo and S. Rudich (1989).",
Simulating the immune system,"When a foreign substance (antigen) is introduced into our bodies, our immune system acts to eliminate that substance. This response is a complex process involving the collective and coordinated response of approximately 10/sup 12/ cells, which is comparable to the number of synapses in the human brain. In an effort to fit detailed experimental observations into a comprehensive model of the immune system, computer simulations are just beginning to play a role. The approach that we describe uses a modified cellular automaton (or lattice gas). Although our automaton is much more complex than the automata usually considered by mathematicians and is not subject to analytical analysis by presently known methods, it has several advantages over traditional ODE models.",
Interior-point methodology for 3-D PET reconstruction,"Interior-point methods have been successfully applied to a wide variety of linear and nonlinear programming applications. This paper presents a class of algorithms, based on path-following interior-point methodology, for performing regularized maximum-likelihood (ML) reconstructions on three-dimensional (3-D) emission tomography data. The algorithms solve a sequence of subproblems that converge to the regularized maximum likelihood solution from the interior of the feasible region (the nonnegative orthant). The authors propose two methods, a primal method which updates only the primal image variables and a primal-dual method which simultaneously updates the primal variables and the Lagrange multipliers. A parallel implementation permits the interior-point methods to scale to very large reconstruction problems. Termination is based on well-defined convergence measures, namely, the Karush-Kuhn-Tucker first-order necessary conditions for optimality. The authors demonstrate the rapid convergence of the path-following interior-point methods using both data from a small animal scanner and Monte Carlo simulated data. The proposed methods can readily be applied to solve the regularized, weighted least squares reconstruction problem.",
Classifying range images of human faces with Hausdorff distance,"In this paper, an application of range image analysis in the area of face recognition is described. The process from image acquisition to recognition is presented in detail. Range images are captured with a multisensor system, merged in a fusion step, and preprocessed. Then, a canonical position of the sensed face is determined, and two different representation of the range data, based on point sets and voxel arrays, respectively, are computed. For the recognition step, a 3D version of the partial Hausdorff distance is introduced. Experiments on a database with 240 images of 24 persons yielded nearly perfect results.",
The kinematics for redundantly actuated omnidirectional mobile robots,"Omnidirectional mobile robots have been popularly employed in several application areas. However, the kinematics for these systems have not been clearly identified, specially for the redundantly actuated case which is common in omnidirectional mobile robot such as Nomadic model. For such mobile robot systems, exploitation of redundant actuation as well as singularity analysis has not been extensively addressed. In light of this fact, this paper introduces two different kinematic approaches for omnidirectional mobile robots and examine singularity configurations of such systems. Then, a singular-free load distribution scheme for a redundantly actuated three-wheeled omnidirectional mobile robot is proposed. Through simulation, several advantages of the redundantly actuated mobile robot with respect to singularity avoidance and exploiting several sub-tasks, are presented.",
"Studio courses: How information technology is changing the way we teach, on campus and off","Over the last decide, the Rensselaer Polytechnic Institute studio classroom model has been applied in various engineering, science, mathematics, and other courses, both at Rensselaer and at other campuses. The studio classroom was designed to provide an interactive learning environment that incorporates the advances in computing and communication and builds upon the cognitive science research on how people learn. In many cases, the studio course replaces the large enrolment course, combining lecture, rectitation, and laboratory into one interactive faculty that is as comfortable as it is high tech. The introduction of the studio courses has led to a better learning environment for both the students and the faculty members. Attendance in classes and student evaluations both improved. To some extent the studio classroom works to change the focus from the lecturer to the student. It requires that the student take some of the responsibility for the learning process. The studio classroom was recognized by the Theodore Hesburgh Award, the Pew Prize, the Boeing Award, and other honors.",
An object-oriented web test model for testing Web applications,"In recent years, Web applications have grown rapidly. As Web applications become complex, there is a growing concern about their quality and reliability. In this paper we present a methodology that uses an object-oriented Web Test Model (WTM) to support Web application testing. The test model captures both structural and behavioral test artifacts of Web applications and represents the artifacts form the object, behavior and structure perspectives. Based on the test model, both structural and behavioral test cases can be derived automatically to ensure the quality of Web applications. Moreover the model also can be used as a road map to identify change ripple effects and to find cost-effective testing strategies for reducing test efforts required in the regression testing.",
Distributed Rendering for Scalable Displays,"We describe a novel distributed graphics system that allows an application to render to a large tiled display. Our system, called WireGL, uses a cluster of off-the-shelf PCs connected with a high-speed network. WireGL allows an unmodified existing application to achieve scalable output resolution on such a display. This paper presents an efficient sorting algorithm which minimizes the network traffic for a scalable display. We will demonstrate that for most applications, our system provides scalable output resolution with minimal performance impact.",
Architectural design recovery using data mining techniques,"The paper presents a technique for recovering the high level design of legacy software systems according to user defined architectural plans. Architectural plans are represented using a description language and specify system components and their interfaces. Such descriptions are viewed as queries that are applied on a large database which stores information extracted from the source code of the subject legacy system. Data mining techniques and a modified branch and bound search algorithm are used to control the matching process, by which the query is satisfied and query variables are instantiated. The matching process allows the alternative results to be ranked according to data mining associations and clustering techniques and, finally, be presented to the user.",
An object-oriented Web test model for testing Web applications,"Recently the number of Web applications has increased immensely. Many businesses have been using Web applications for their mission-critical operations. As Web applications become complex, their quality and reliability become crucial. The paper describes an object oriented test model that captures both structural and behavioral test artifacts of Web applications. The model represents the entities of Web applications as objects and describes their structures, relationships, and dynamic behaviors. Based on the test model, test methods are presented to derive test cases automatically for ensuring the structures and behaviors of Web applications.",
The fast multipole algorithm,"Accurate computation of the mutual interactions of N particles through electrostatic or gravitational forces has impeded progress in many areas of simulation science. The fast multipole algorithm (FMA) provides an efficient scheme for reducing computational complexity. Researchers are studying very large astrophysical simulations with hybrids of the FMA and the earlier Barnes-Hut scheme (J.E. Barnes and P. Hut, 1986). In the biophysical-simulation world, the Ewald summation method is an additional competitor. Since the development of the FMA, scientists have created various fast versions of the nearly 80-year-old Ewald method that are faster than multipole codes in some cases, although their error behavior is harder to quantify. The Ewald codes also handle periodic boundary conditions automatically; FMA-derived codes can be extended to this case with extra effort. None the less, FMA and its offspring remain important, and the newest formulations promise to again challenge Ewald codes for the title of fastest electrostatic solver.",
An evolvable hardware FPGA for adaptive hardware,"Can we realise the opportunities that lie in design by evolution by using traditional technologies or are there better technologies which will allow us to fully realise the potential inherent in evolvable hardware? The authors consider the characteristics of evolvable hardware, especially for adaptive design, and discuss the demands that these characteristics place on the underlying technology. They suggest a potential alternative to today's FPGA technology. The proposed architecture is particularly focused at reducing the genotype required for a given design by reducing the configuration data required for unused routing resources and allowing partial configuration down to a single CLB. In addition, to support adaptive hardware, self-reconfiguration is enabled.",
H-BLOB: a hierarchical visual clustering method using implicit surfaces,"We present a new hierarchical clustering and visualization algorithm called H-BLOB, which groups and visualizes cluster hierarchies at multiple levels-of-detail. Our method is fundamentally different to conventional clustering algorithms, such as C-means, K-means, or linkage methods that are primarily designed to partition a collection of objects into subsets sharing similar attributes. These approaches usually lack an efficient level-of-detail strategy that breaks down the visual complexity of very large datasets for visualization. In contrast, our method combines grouping and visualization in a two stage process constructing a hierarchical setting. In the first stage a cluster tree is computed making use of an edge contraction operator. Exploiting the inherent hierarchical structure of this tree, a second stage visualizes the clusters by computing a hierarchy of implicit surfaces. We believe that H-BLOB is especially suited for the visualization of very large datasets and for visual decision making in information visualization. The versatility of the algorithm is demonstrated using examples from visual data mining.",
Designing and implementing hands-on robotics labs,"As part of a rigorous robotics curriculum at Carnegie Mellon University, the authors have developed and implemented open-ended design and construction lab experiences. The hands-on, heads-on laboratory component to the new robotics course described provides a self-learning experience for engineering and computer science undergraduates, enriching their robotics education and teaching interdisciplinary teamwork.",
Circuits for wide-window superscalar processors,"Our program benchmarks and simulations of novel circuits indicate that large-window processors are feasible. Using our redesigned superscalar components, a large-window processor implemented in today's technology can achieve an increase of 10-60% (geometric mean of 31%) in program speed compared to today's processors. The processor operates at clock speeds comparable to today's processors, but achieves significantly higher ILP. To measure the impact of a large window on clock speed, we design and simulate new implementations of the logic components that most limit the critical path of our large-window processor: the schedule logic and the wake-up logic. We use log-depth cyclic segmented prefix (CSP) circuits to reimplement these components. Our layouts and simulations of critical paths through these circuits indicate that our large-window processor could be clocked at frequencies exceeding 500 MHz in today's technology. Our commit logic and rename logic can also run at these speeds. To measure the impact of a large window on ILP, we compare two microarchitectures, the first has a 128-instruction window, an 8-wide fetch unit, and 20-wide issue (four integer, branch, multiply, float, and memory units), whereas the second has a 32-instruction window, and a 4-wide fetch unit and is comparable to today's processors. For each, we simulate different window reuse and bypass policies. Our simulations show that the large-window processor achieves significantly higher IPC. This performance increase comes despite the fact that the large-window processor uses a wrap-around window while the small-window processor uses a compressing window, thus effectively increasing its number of outstanding instructions. Furthermore, the large-window processor sometimes pays an extra clock cycle for bypassing.",
Fast inhomogeneous plane wave algorithm for electromagnetic solutions in layered medium structures: Two-dimensional case,"An accurate and efficient algorithm is proposed for solving two-dimensional electromagnetic scattering problems in a layered medium. As a natural extension of the previously developed fast inhomogeneous plane wave algorithm (FIPWA), this approach has several inherent merits, such as being simple, versatile, and error controllable. The basic idea is first to express the spatial Green's function, encountered in layered medium studies, as Sommerfeld-type integrals. By observing the similarity of these integrals with the integral representation of the free space Green's function, FIPWA can be applied with ease to accelerate the matrix-vector multiplication. The multilevel scheme has been implemented, and the computational complexity of O(NlogN) is achieved. It is noted that the proposed approach is more accurate and more efficient than the existing fast multipole method coupled with the discrete complex image method.",
Application of fractals to the detection and classification of shoeprints,The most common clues left at a crime scene when a crime is committed are shoeprint impressions. These impressions are useful in the detection of criminals and the linking of crime scenes. A novel technique for use in the detection and classification of shoeprint impressions has been developed. The technique is based on fractal based feature extraction and pattern matching methods. The computerized system developed has been extensively tested on a large database of real shoeprint impressions and is robust to small variations of image orientations and/or translations.,
de Morgan bisemilattices,"We study de Morgan bisemilattices, which are algebras of the form (S, /spl cup/, /spl and/, /sup -/, 1, 0), where (S, /spl cup/, /spl and/) is a bisemilattice, 1 and 0 are the unit and zero elements of S, and /sup -/ is a unary operation, called quasi-complementation, that satisfies the involution law and de Morgan's laws. de Morgan bisemilattices are generalizations of de Morgan algebras, and have applications in multi-valued simulations of digital circuits. We present some basic observations about bisemilattices, and provide a set-theoretic characterization for a subfamily of de Morgan bisemilattices, which we call locally distributive de Morgan bilattices.",
A universal distribution protocol for video-on-demand,Most existing distribution protocols for video-on-demand are tailored for a specific range of request arrival rates and do not perform well beyond that range. We present a universal distribution protocol based on L. Juhn and L. Tseng's (1998) fast broadcasting protocol. Our protocol performs as well as the best reactive protocol at low-to-moderate request arrival rates and reverts to the fast broadcasting protocol at high arrival rates.,
On fault location in networks by passive testing,"In this paper, we employ a variant of the communicating finite state machine (CFSM) model for networks to investigate fault detection and location using passive testing. First, we introduce the concept of passive testing, then we introduce the model with necessary assumptions and justification. Then, the model for the observer process is described and a 3-node case is studied to show how fault location information can be deduced. Extending this result, we propose a multiple node-cut approach for a general network, applying our technique for fault detection and location. An abstraction of a node-cut shows how the 3-node case can be used in the general case. We then illustrate our technique through a simulation of a practical X.25 example. Finally future extensions and potential trends are-discussed.","Fault location,
Intelligent networks,
Fault detection,
Protocols,
Telecommunication traffic,
Computer science,
Educational institutions,
Automata,
System testing,
Computer network management"
Video cut detection using frequency domain correlation,"A common video indexing technique is to segment a video sequence into shots and then select representative key-frames. The process of shot break detection is a fundamental component in automatic video indexing, editing, and archiving. This paper introduces a novel video cut detection technique which performs in the frequency domain. It is computationally tractable and robust with respect to sudden changes in mean intensity within a shot. The method uses the average interframe correlation coefficients to determine whether an abrupt shot change has occurred. We compare our method against three established techniques and present our results using different video sequences.",
Global optimization algorithms for training product unit neural networks,"Product units in the hidden layer of multilayer neural networks provide a powerful mechanism for neural networks to efficiently learn higher-order combinations of inputs. Training product unit networks using local optimization algorithms is difficult due to an increased number of local minima and increased chances of network paralysis. The paper discusses the problems with using gradient descent to train product unit neural networks, and shows that particle swarm optimization, genetic algorithms and LeapFrog are efficient alternatives to successfully train product unit neural networks.",
An open architecture for holonic cooperation and autonomy,"The paper examines some issues relating to an open architecture for holon cooperation and autonomy. We identify the requirements of a holonic system architecture and discuss the merits of our approach in comparison with classic agent-based models. A suitable architecture to satisfy these requirements is also presented, together with a discussion of the holonic kernel needed to support distributed holonic control. The material presented is based on results from the international programme on Holonic Manufacturing Systems (HMS).",
Recognition of partially occluded and/or imprecisely localized faces using a probabilistic approach,"New face recognition approaches are needed, because although much progress has been recently achieved in the field (e.g. within the eigenspace domain), still many problems are to be robustly solved. Two of these problems are occlusions and the imprecise localization of faces (which ultimately imply a failure in identification). While little has been done to account for the first problem, almost nothing has been proposed to account for the second. This paper presents a probabilistic approach that attempts to solve both problems while using an eigenspace representation. To resolve the localization problem, we need to find the subspace (within the feature space, e.g. eigenspace) that represents this error for each of the training images. To resolve the occlusion problem, each face is divided into n local regions which are analyzed in isolation. In contrast with other previous approaches, where a simple voting space is used, we present a probabilistic method that analyzes how ""good"" a local match is. Our method has proven to be superior to a local voting PCA on a set of 2600 face images.",
Java model checking,"This paper presents initial results in model checking multi-threaded Java programs. Java programs are translated into the SAL (Symbolic Analysis Laboratory) intermediate language, which supports dynamic constructs such as object instantiations and thread call stacks. The SAL model checker then exhaustively checks the program description for deadlocks and assertion failures, using traditional model checking optimizations to curb the state explosion problem. Most of the advanced features of the Java language are modeled within our framework.",
The QR algorithm,"After a brief sketch of the early days of eigenvalue hunting, the author describes the QR (or orthogonal triangular) matrix factorization algorithm and its major virtues. The symmetric case brings with it guaranteed convergence and an elegant implementation. An account of the impressive discovery of the algorithm brings the article to a close.",
Omniview cameras with curved surface mirrors,Omnidirectional cameras are important for a variety of applications like videoconferencing and robot navigation. This paper discusses several ideas for building such cameras using mirrors with curved surfaces. It turns out that hyperbolic and parabolic mirror profiles are optimally suited for omniviewing.,
Low-level analysis of a portable Java byte code WCET analysis framework,"To support portability, worst-case execution time (WCET) analysis of Java byte code is performed at two levels - machine-independent program flow analysis at a higher level and machine-dependent timing analysis of individual program constructs at a lower level. This paper contributes a WCET analysis that computes worst-case execution frequencies of Java byte codes within the software being analysed and accounts for platform-dependent information, i.e. the processor's pipeline. The main part of the approach is platform-independent; only a limited analysis is needed on a per-platform basis.",
Optimistic Virtual Synchrony,"We present Optimistic Virtual Synchrony (OVS), a new form of group communication which provides the same capabilities as Virtual Synchrony with better performance. It does so by allowing applications to send messages during periods in which services implementing Virtual Synchrony block. OVS also allows applications to determine the policy as to when messages sent optimistically should be delivered and when they should be discarded. Thus, OVS gives applications fine grain control over the specific semantics they require, and does not impose costs for enforcing any semantics that they do not require. At the same time, OVS provides a single easy-to-use interface for all applications.",
A resource broker model with integrated reservation scheme,"We present a resource broker model with a new admission control at a single resource level. The admission control supports advance reservation as well as immediate reservation. By separating the brokerage part from the scheduling part, we can obtain fast and constant response time of brokerage requests for resource reservation, modification, allocation and release.",
Ant colonies are good at solving constraint satisfaction problems,"We define an ant algorithm for solving random binary constraint satisfaction problems (CSPs). We empirically investigate the behavior of the algorithm on this type of problems and establish the parameter settings under which the ant algorithm performs best for a specific class of CSPs. The ant algorithm is compared to six other state-of-the-art stochastic algorithms from the field of evolutionary computing. It turns out that the ant algorithm outperforms all other algorithms and that bivariate distribution algorithms perform worse than the univariate ones, the latter largely due to the fact that they cannot model the randomly generated instances.","Scheduling algorithm,
Stochastic processes,
Intelligent systems,
Mathematics,
Computer science,
Ant colony optimization,
Vehicles,
Routing,
Evolutionary computation"
A wavelet-frame based image force model for active contouring algorithms,"This paper proposes a directional image force (DIF) for active contouring. DIF is the inner product of the zero crossing strength (ZCS) of wavelet frame coefficients, and the normal of a snake, by representing strength and orientation of edges at multiple resolution levels. DIF markedly improves the immunity of snakes to noise and convexity.",
The Berkeley Highway Laboratory-building on the I-880 field experiment,"This paper presents the development of the Berkeley Highway Laboratory. The laboratory includes surveillance component, eight dual loop speed detector stations and 14 video cameras dedicated research, as well as an analytical component to advance traffic operations and traffic flow theory. The work builds on our experience with the I-880 field experiment and several other traffic studies. The lessons learned from the laboratory will be used to improve traffic surveillance and control, provide new traffic metrics, and enhance traffic models. This paper also presents correction factors that should be beneficial to researchers using the I-880 database.","Road transportation,
Laboratories,
Traffic control,
Detectors,
Statistics,
Vehicle detection,
Civil engineering,
Surveillance,
Databases,
Computer science"
Frame-discriminative and confidence-driven adaptation for LVCSR,"Maximum likelihood linear regression (MLLR) has become the most popular approach for adapting speaker-independent hidden Markov models to a specific speaker's characteristics. However, it is well known, that discriminative training objectives outperform maximum likelihood training approaches, especially in cases where training data is very limited, as it always is the case in adaptation tasks. Therefore, this paper explores the application of a frame-based discriminative training objective for adaptation. It presents evaluations for supervised as well as for unsupervised adaption on the 1993 WSJ adaptation tests of native and non-native speakers. Relative improvements in word error rate of up to 25% could be measured compared to the MLLR adapted recognition systems. Along with unsupervised adaptation, the paper also presents the improvements achieved by the application of confidence measures. They provided an average relative improvement of 10% compared to ordinary unsupervised MLLR.",
An improved quantum Fourier transform algorithm and applications,"We give an algorithm for approximating the quantum Fourier transform over an arbitrary Z/sub p/ which requires only O(n log n) steps where n=log p to achieve an approximation to within an arbitrary inverse polynomial in n. This improves the method of A.Y. Kitaev (1995) which requires time quadratic in n. This algorithm also leads to a general and efficient Fourier sampling technique which improves upon the quantum Fourier sampling lemma of L. Hales and S. Hallgren (1997). As an application of this technique, we give a quantum algorithm which finds the period of an arbitrary periodic function, i.e. a function which may be many-to-one within each period. We show that this algorithm is efficient (polylogarithmic in the period of the function) for a large class of periodic functions. Moreover, using standard quantum lower-bound techniques, we show that this characterization is right. That is, this is the maximal class of periodic functions with an efficient quantum period-finding algorithm.","Fourier transforms,
Quantum computing,
Sampling methods,
Machinery,
Application software,
Logic,
Computer science,
Polynomials,
Eigenvalues and eigenfunctions,
Multidimensional systems"
Bayes-optimality motivated linear and multilayered perceptron-based dimensionality reduction,"Dimensionality reduction is the process of mapping high-dimension patterns to a lower dimension subspace. When done prior to classification, estimates obtained in the lower dimension subspace are more reliable. For some classifiers, there is also an improvement in performance due to the removal of the diluting effect of redundant information. A majority of the present approaches to dimensionality reduction are based on scatter matrices or other statistics of the data which do not directly correlate to classification accuracy. The optimality criteria of choice for the purposes of classification is the Bayes error. Usually however, Bayes error is difficult to express analytically. We propose an optimality criteria based on an approximation of the Bayes error and use it to formulate a linear and a nonlinear method of dimensionality reduction. The nonlinear method we propose, relies on using a multilayered perceptron which produces as output the lower dimensional representation. It thus differs from autoassociative like multilayered perceptrons which have been proposed and used for dimensionality reduction. Our results show that the nonlinear method is, as anticipated, superior to the linear method in that it can perform unfolding of a nonlinear manifold. In addition, the nonlinear method we propose provides substantially better lower dimension representation (for classification purposes) than Fisher's linear discriminant (FLD) and two other nonlinear methods of dimensionality reduction that are often used.",
Unknown object grasping using statistical pressure models,"Grasping is one of the most fundamental and challenging tasks in robotics. Applications range from space missions (e.g., collection of rock samples) to industrial automation. In this work, we use a camera mounted on the end-effector of a manipulator to grasp an unknown object in the workspace. A novel deformable contour model is used to determine plausible grasp axes of the target object. Potential grasp point pairs are generated, ranked based upon measurements taken from the contour, and a vision-guided grasp of the object using the highest ranked grasp point pair is executed. Several experimental results are presented.",
"A problem-specific fault-tolerance mechanism for asynchronous, distributed systems","The idle computers on a local area, campus area, or even wide area network represent a significant computational resource-one that is, however, also unreliable, heterogeneous, and opportunistic. We describe an algorithm that allows branch-and-bound problems to be solved in such environments. In designing this algorithm, we faced two challenges: (1) scalability, to effectively exploit the variably sized pools of resources available, and (2) fault tolerance, to ensure the reliability of services. We achieve scalability through a fully decentralized algorithm, in which the dynamically available resources are managed through a membership protocol. We guarantee fault tolerance in the sense that the loss of up to all but one resource will not affect the quality of the solution. For propagating information reliably, we use epidemic communication for both the membership protocol and the fault-tolerance mechanism. We have developed a simulation framework that allows us to evaluate design alternatives. Results obtained in this framework suggest that our techniques can execute scalably and reliably.",
Design of frequency-reconfigurable rectangular slot ring antennas,Summary form only given. This paper presents the concept of a frequency-reconfigurable rectangular ring slot antennas fed by a single slotline or CPW line. The reconfiguration is carried out by switching in or out appropriate slot-line lengths in a rectangular slot-ring antenna. An example of this configuration for a two-frequency antenna is shown. The total periphery of the outer loop determines the lower of the two frequencies; and the inner loop determines the higher of the two reconfigurable frequencies. The concept can be extended to a multiple frequency reconfigured antenna by adding more slotline segments to the configuration shown. Full-wave EM simulations using HP's Momentum have been carried out to demonstrate the feasibility of the proposed concept. A sample antenna has been designed with two operating frequencies being 3.0 and 8.3 GHz. The two radiation patterns are close to each other. It has been found that a single CPW line of about 100-ohm impedance can feed the antennas at both the frequencies. A two-stage quarter-wave transformer is used to connect the antenna to the measurement system. Values of the input match at the two frequencies are - dB and -22 dB respectively.,
Agent-based simulation of dynamic online auctions,"The need to understand dynamic behavior in auctions is increasing with the popularization of online auctions. Applications include designing auction mechanisms, bidding strategies, and server systems. We describe simulations of a typical online auction, where the duration is fixed, and the second-highest price is continuously posted and determines the winner's payment. We modeled agents of exactly two types, idealizations and simplifications of those observed in practice: early bidders, who can bid any time during the auction period, and snipers, who wait till the last moments to bid. This allows us to study the interactions of the two types of bidders during the course of auctions, and the effects of the two strategies on the probability of winning, the final price, and the formation of price consensus in iterated auctions. Results show that: 1) early bidders can win with a lower price on average than snipers, but much less often; 2) the late bidding strategy of snipers is effective; and 3) in iterated auctions, adjustment feedback of motivational parameters can lead to effective price consensus with small fluctuations.",
"Strategic directions in verification, validation, and accreditation research","The authors, six simulation professionals, present their views on the directions that they believe that verification, validation, and accreditation research should take. Two of the six are active verification, validation, and accreditation researchers from academia, two develop industry simulation models, and two work in verification, validation, and accreditation of military simulation models. A number of areas and topics for research in verification, validation, and accreditation are identified. It appears that application domains of simulation models affect what topics need verification, validation, and accreditation research.",
Knowledge objects and mental models,"This paper describes knowledge components that are thought to be appropriate and sufficient to precisely describe certain types of cognitive subject matter content (knowledge). It also describes knowledge structures that show the relationships among these knowledge components and among other knowledge objects. It suggests that a knowledge structure is a form of schema such as those that learners use to represent knowledge in memory. A mental model is a schema plus cognitive processes for manipulating and modifying the knowledge stored in a schema. We suggested processes that enable learners to manipulate the knowledge components of conceptual network knowledge structures for purposes of classification, generalization, and concept elaboration. We further suggested processes that enable learners to manipulate the knowledge components of process knowledge structures (PEAnets) for purposes of explanation, prediction, and troubleshooting. The hypothesis of this paper is that knowledge components and knowledge structures, such as those described in this paper, could serve as meta mental models that would enable learners to more easily acquire conceptual and causal networks and their associated processes. The resulting specific mental models would facilitate their ability to solve problems of conceptualization and interpretation.",
An efficient cache maintenance scheme for mobile environment,"We present a new cache maintenance scheme, called AS, suitable for the wireless mobile environment. Our scheme integrates the mobility management scheme of Mobile IP with cache maintenance scheme used in the CODA file system. As opposed to broadcasting invalidation report schemes, AS supports arbitrary disconnection patterns and uses less wireless bandwidth. We present analytical and simulation results to show the superiority of our caching scheme.",
Task assignment with unknown duration,"We consider a distributed server system and ask which policy should be used for assigning tasks to hosts. In our server tasks are not preemptible. Also, the task's service demand is not known a priori. We are particularly concerned with the case where the workload is heavy-tailed, as is characteristic of many empirically measured computer workloads. We analyze several natural task assignment policies and propose a new one TAGS (Task Assignment based on Guessing Size). The TAGS algorithm is counterintuitive in many respects, including load unbalancing, non-work-conserving and fairness. We find that under heavy-tailed workloads, TAGS can outperform all task assignment policies known to us by several orders of magnitude with respect to both mean response time and mean slowdown, provided the system load is not too high.",
New Validation and Test Problems for High Performance Deep Sub-micron VLSI Circuits,,
On choosing test criteria for behavioral level hardware design verification,"This paper proposes criteria for the verification of behavioral designs for hardware written in VHDL. The criteria are analogous to testing criteria for software, but were adapted to the specific needs and constructs of hardware designs written in VHDL. We examine the potential value of these criteria with respect to desirable properties for evaluation criteria that were originally developed for software. Then we apply the VHDL criteria to several design examples with varying complexities to demonstrate their practical usefulness. Although, applying software testing techniques to hardware design at the behavioral level is not new, this work, to the best of our knowledge, is the first attempt to analyze the approach from the theoretical point of view and to lay the groundwork for achieving error-free design at the behavioral level.",
Prediction-based admission control using FARIMA models,"The FARIMA (p,d,q) model is a good traffic model capable of capturing both the long-range and short-range behavior of a network traffic stream in time. In this paper, we propose a prediction-based admission control algorithm for an integrated service packet network. We suggest a method to simplify the FARIMA model fitting procedure and hence to reduce the time of traffic modeling and prediction. Our feasibility-study experiments showed that FARIMA models which have number of parameters could be used to model and predict actual traffic on quite a large time scale.",
Concept hierarchy based text database categorization in a metasearch engine environment,"Document categorization, as a technique to improve the retrieval of useful documents, has been extensively investigated. One important issue in a large-scale meta-search engine is to select text databases that are likely to contain useful documents for a given query. We believe that database categorization can be a potentially effective technique for good database selection, especially in the Internet environment, where short queries are usually submitted. In this paper, we propose and evaluate several database categorization algorithms. This study indicates that, while some document categorization algorithms could be adopted for database categorization, algorithms that take into consideration the special characteristics of databases may be more effective. Preliminary experimental results are provided to compare the proposed database categorization algorithms.",
"A taxonomy of branch mispredictions, and alloyed prediction as a robust solution to wrong-history mispredictions","The need for accurate conditional-branch prediction is well known: mispredictions waste large numbers of cycles, inhibit out-of-order execution, and waste power on mis-speculated computation. Prior work on branch-predictor organization has focused mainly on how to reduce conflicts in the branch-predictor structures, while relatively little work has explored other causes of mispredictions. Some prior work has identified other categories of mispredictions, but this paper organizes these categories into a broad taxonomy of misprediction types. Using the taxonomy, this paper goes on to show that other categories- especially wrong history mispredictions-are often more important than conflicts. This is true even if just a very simple conflict-reduction technique is used. Based on these observations, this paper proposes alloying local and global history together in a two-level branch predictor structure. This simple technique, a generalization of the bi-mode predictor, attacks wrong-history mispredictions by making both global and local history simultaneously available. Unlike bi-mode prediction, however, alloying gives robust performance for branch-predictor hardware budgets ranging from very large to very small. Finally, this paper shows that individual branch references can also suffer wrong-history mispredictions as they alternate between using global and local history, a phenomenon that favors dynamic rather than static selection in hybrid predictors.",
A general method for calculating error probabilities over fading channels,"This paper presents a general method for calculating the average error rates and outage performance of a broad class of coherent, differentially coherent and noncoherent communication systems with/without diversity reception in a myriad of fading environments. Unlike the moment generating function (MGF) technique, the proposed characteristic function (CHF) method based on Parseval's theorem enables us to unify the average error rate analysis of different modulation formats and all commonly used predetection diversity techniques (i.e., maximal-ratio combining (MR), equal-gain combining (EG), selection diversity (SD), switched diversity (SW) and hybrid diversity systems) in a single common framework. The CHF method also lends itself to the averaging of the conditional error probability (CEP) involving the complementary incomplete Gamma function and the confluent hypergeometric function over fading amplitudes, which heretofore resisted to a simple form. As an aside, we show previous results as special instances of our unified framework.",
Visualizing high-dimensional predictive model quality,"Using inductive learning techniques to construct classification models from large, high-dimensional data sets is a useful way to make predictions in complex domains. However, these models can be difficult for users to understand. We have developed a set of visualization methods that help users to understand and analyze the behavior of learned models, including techniques for high-dimensional data space projection, display of probabilistic predictions, variable/class correlation, and instance mapping. We show the results of applying these techniques to models constructed from a benchmark data set of census data, and draw conclusions about the utility of these methods for model understanding.",
An efficient algorithm for the physical mapping of clustered task graphs onto multiprocessor architectures,"The most important issue in sequential program parallelisation is the efficient assignment of computations into different processing elements. In the past, too many approaches were devoted in efficient program parallelization considering various models for the parallel programs and the target architectures. The most widely used parallelism description model is the task graph model with precedence constraints. Nevertheless, as far as physical mapping of tasks onto parallel architectures is concerned little research has given practical results. It is well known that the physical mapping problem is NP-hard in the strong sense, thus allowing only for heuristic approaches. Most researchers or tool programmers use exhaustive algorithms, or the classical method of simulated annealing. This paper presents an alternative approach onto the mapping problem. Given the graph of clustered tasks, and the graph of the target distributed architecture, our heuristic finds a mapping by first placing the highly communicative tasks on adjacent nodes of the processor network. Once these ""backbone"" tasks are mapped there is no backtracking, thus achieving low complexity. Therefore, the remaining tasks are placed beginning from those close to the ""backbone"" tasks. The paper concludes with performance and comparison results which reveal the method's efficiency.",
SQUEEZE: fast and progressive decompression of triangle meshes,"An ideal triangle mesh compression technology would simultaneously support the following objectives: (1) progressive refinements of the received mesh during decompression, (2) nearly optimal compression ratios for both geometry and connectivity, and (3) in-line, real-time decompression algorithms for hardware or software implementations. Because these three objectives impose contradictory constraints, previously reported efforts have focused primarily on one (sometimes two) of these objectives. The SQUEEZE technique introduced in this paper addresses all three constraints simultaneously, and attempts to provide the best possible compromise. For a mesh of T triangles, SQUEEZE compresses the connectivity to 3.7T bits, which is competitive with the best progressive compression techniques reported so far. The geometric prediction error encoding technique introduced in this paper leads to a geometry compression that is improved by 20% over that of previous schemes. Our initial implementation on a 300-MHz CPU achieved a decompression rate of up to 46,000 triangles per second. SQUEEZE downloads a model through a number of successive refinement stages, providing the benefit of progressivity.",
Rover maneuvering for autonomous vision-based dexterous manipulation,"Manipulators mounted on-board rovers have limited dexterity due to power and weight constraints imposed by rover designs. However, to perform science operations, it is necessary to be able to position and orient these manipulators on science targets in order to carry out in-situ measurements. This article describes how we enhance manipulator dexterity using the rover mobility system. The lack of omni-directional driving capability and the constraints imposed by the mobility mechanism requires vehicle maneuvering to supplement the manipulators' motions. Target tracking using stereo vision is integrated with rover maneuvering to perform two types of operations: rock sample acquisition for return to earth and instrument placement for in-situ science measurements. We describe the computational architecture, tools, and algorithms that we developed for this task. We have successfully demonstrated these operations on a self-contained Mars Rover prototype, Rocky 7. We have demonstrated grasping a small rock sample from a distance of more than one meter away and placing an instrument on a boulder from a distance of more than five meters away.",
Handling dynamic schema change in process models,"Workflow technology has emerged as an appropriate platform for consolidating the distributed information resources of an enterprise, promoting interoperability across cross-platform systems and for providing a global view and understanding of business process models. However, the business processes that workflows represent, are dynamic by nature, that is, they encounter frequent and unavoidable changes. It is through this dynamism that organizations maintain their competitive edge. Workflow technology to date does not provide sufficient support for dynamically changing processes. Managing schema change of workflow processes with multiple active instances is a complex issue. In this paper, we present an analysis of workflow changes in relation to business process change, and present a classification of workflow changes that dictate the scope of the problem. Based on this classification we lay the foundation for a generic framework to support dynamically changing workflow processes.",
A secure data hiding scheme for two-color images,"In this paper we propose a new steganography scheme for hiding a piece of critical information in a host binary image (such as facsimiles). A secret key and a weight matrix are used to protect the hidden data. Given an image block of size m/spl times/n, our scheme can hide as many as [log/sub 2/(mn+1)] bits of data in the image by changing at most 2 bits in the image. This scheme, as compared to an existing scheme by M. Y. Wu et al. 1998), can provide higher security, embed more data, and maintain higher quality of the host image.",
Code selection for media processors with SIMD instructions,"Media processors show special instruction sets for fast execution of signal processing algorithms on different media data types. They provide SIMD instructions, capable of executing one operation on multiple data in parallel within a single instruction cycle. Unfortunately, their use in compilers is so far very restricted and requires either assembly libraries or compiler intrinsics. This paper presents a novel code selection technique capable of exploiting SIMD instructions also when compiling plain C source code. It permits one to take advantage of SIMD instructions for multimedia applications, while still using portable source code.",
MSXmin: a modular multicast ATM packet switch with low delay and hardware complexity,"We propose and analyze the architecture for a large-scale high-speed multicast switch called MSXmin. The hardware complexity of MSXmin is O(N log/sup 2/ N) which compares favorably with existing architectures. Further, the internal latency of the MSXmin is O(log/sup 2/ N) bits. While it is superior to the existing architectures in terms of the hardware complexity and the internal latency, it is comparable to other multicast switches in terms of the header overhead and translation table complexity. MSXmin is output buffered and based on the group knockout principle. Moreover, MSXmin is a dual-bit-controlled tree-based switch.",
A microprocessor design project in an introductory VLSI course,"An introductory very large scale integration (VLSI) design course has been taught at the University of Michigan (USA) since 1980. In 1990, it was redesigned around a simple 8-bit microprocessor project in the format described in this paper; in 1996, the project was updated to a 16 bit reduced instruction set computer (RISC) processor. The authors describe the course philosophy, content, and the baseline architecture from which class projects begin. The key features of the course are: close coordination of lectures and project activity; prompt and regular feedback on design work; and a schedule which spreads the workload over the full term. In this course, students learn VLSI fundamentals and good design methodology that will be important throughout their careers.",
Automatic base station selection and configuration in mobile networks,"We consider the automatic selection and configuration of base station sites for mobile cellular networks. A solution framework based on simulated annealing is used for site selection and for base station configuration. The configuration of each base station involves selecting various parameters. Results are presented for several design scenarios with between 250 and 750 candidate sites, and show that the solution framework can generate network designs with desired characteristics such as high area coverage and high traffic capacity.",
An on-demand QoS routing protocol for mobile ad hoc networks,"We propose a on-demand bandwidth routing protocol for QoS (quality of service) support in mobile ad hoc networks. The QoS routing feature is important for a mobile network to interconnect wired networks with QoS support (e.g., ATM, Internet, etc.). The QoS routing protocol can also work in a stand-alone mobile ad hoc network for real-time applications. Under such a routing protocol, the source (or the ATM gateway) is informed of the bandwidth and QoS available to any destination in the mobile network. This knowledge enables the establishment of QoS connections within the mobile network and the efficient support of real time applications. In addition, it enables more efficient call admission control. In case of ATM interconnection, the bandwidth information can be used to carry out intelligent handoff between ATM gateways and/or to extend the ATM virtual circuit service to the mobile network with possible renegotiation of QoS parameters at the gateway. Simulation results suggest distinct performance advantages of our protocol calculating the bandwidth information. It is particularly useful in call admission control. Furthermore, ""on-demand"" routing enhances the performance in the mobile environment because the source can keep more connectivity to a receiver in the path-finding duration. Simulation experiments show this improvement.",
A comparison of questionnaire-based and GUI-based requirements gathering,"Software development includes gathering information about tasks, work practices and design options from users. Traditionally requirements gathering takes two forms. Interviews and participatory design (PD) practices gather rich information about the task and the domain but require face-to-face communication between the software engineers and the users. When such communication is not possible, traditional software engineering frequently relies on questionnaires and other paper-based methods. Unfortunately, questionnaires often fail to capture implicit aspects of user tasks that may be identified through one-on-one interactions. This project investigates a method of gathering requirements whereby users, working independently of software engineers, construct rough interfaces augmented with textual argumentation. Our initial study has compared the use of GRC (Graphical Requirements Collector) with questionnaire-based requirements gathering.",
An interactive multimedia software house simulation for postgraduate software engineers,"The Open University's M880 Software Engineering is a postgraduate distance education course aimed at software professionals. The case study element of the course (approximately 100 hours of study) is presented through an innovative interactive multimedia simulation of a software house Open Software Solutions (OSS). The student 'joins' OSS as an employee and performs various tasks as a member of the company's project teams. The course is now in its sixth presentation and has been studied by over 1500 students. The authors present the background to the development, and a description of the environment and student tasks.",
Semantic conditions for correctness at different isolation levels,"Many transaction processing applications execute at isolation levels lower than serializable in order to increase throughput and reduce response time. The problem is that non-serializable schedules are not guaranteed to be correct for all applications. The semantics of a particular application determines whether that application will run correctly at a lower isolation level, and in practice it appears that many applications do. Unfortunately, we know of an analysis technique that has been developed to test an application for its correctness at a particular level. Apparently decisions of this nature are made on an informal basis. In this paper we describe such a technique in a formal way. We use a new definition of correctness, semantic correctness, which is weaker than serializability, to investigate the correctness of such executions. For each isolation level, we prove a condition under which transactions that execute at that level will be semantically correct. In addition to the ANSI/ISO isolation levels of read uncommitted, read committed, and repeatable read, we also prove a condition for correct execution at the read committed with first-committer-wins (a variation of read committed) and at the snapshot isolation level. We assume that different transactions can be executing at different isolation levels, but that each transaction is executing at least at the read uncommitted level.",
Defining and using ideal teammate and opponent agent models: a case study in robotic soccer,"A common challenge for agents in multiagent systems is trying to predict what other agents are going to do in the future. Such knowledge can help an agent determine which of its current action options are most likely to help it achieve its goals. Ideally, an agent could learn a model of other agents' behavior patterns via direct observation of their past actions. However, that is only possible when agents have many repeated interactions with one another. We explore the use of agent models in an application where extensive interactions with a particular agent are not possible, namely robotic soccer. In robotic soccer tournaments, such as RoboCup (Kitano et al., 1997), a team of agents plays against another team for a single, short (typically 10-minute) period. The opponents' behaviors are usually not observable prior to this game and there are not enough interactions during the game to build a useful model. We introduce ""ideal-model-based behavior outcome prediction"" (IMBBOP). This technique predicts an agent's future actions in relation to the optimal behavior in its given situation, This optimal behavior is agent-independent and can therefore be computed based solely on a model of the world dynamics. IMBBOP does not assume that the other agent will act according to the theoretical optimum, but rather characterizes its expected behavior in terms of deviation from this optimum.",
From weak to strong information-theoretic key agreement,"In the original definitions of information-theoretic secret-key agreement, the required secrecy condition was too weak. We show, by a generic reduction, that it can be strengthened without any effect on the achievable key-generation rate.",
Gaussian process regression: active data selection and test point rejection,"We consider active data selection and test point rejection strategies for Gaussian process regression based on the variance of the posterior over target values. Gaussian process regression is viewed as transductive regression that provides target distributions for given points rather than selecting an explicit regression function. Since not only the posterior mean but also the posterior variance are easily calculated we use this additional information to two ends: active data selection is performed by either querying at points of high estimated posterior variance or at points that minimize the estimated posterior variance averaged over the input distribution of interest or (in a transductive manner) averaged over the test set. Test point rejection is performed using the estimated posterior variance as a confidence measure. We find that, for both a two-dimensional toy problem and a real-world benchmark problem, the variance is a reasonable criterion for both active data selection and test point rejection.",
The 2-SAT problem of regular signed CNF formulas,"Signed conjunctive normal form (signed CNF) is a classical conjunctive clause form using a generalized notion of literal, called signed atom. A signed atom is an expression of the form S:p, where p is a classical atom and S, its sign, is a subset of a domain N. The informal meaning is ""p rakes one of the values in S "" Applications for deduction in signed logics derive from those of annotated logic programming (e.g., mediated deductive databases), constraint programming (e.g., scheduling), and many-valued logics (e.g., natural language processing). The central role of signed CNF justifies a detailed study of its subclasses, including algorithms for and complexities of associated SAT problems. Continuing previous work (1999), in this paper we present new results on the complexity of the signed 2-SAT problem; i.e., the case in which all clauses of a signed CNF formula have at most two literals.",
Aggregate TCP congestion control using multiple network probing,"Extensive research in TCP's congestion control mechanism has resulted in an effective algorithm that gives fairly precise estimates on the available bandwidth on a given network path (J. Postel, 1981; V. Jacobson, 1988). However, most past efforts focused on enhancing the accuracy and robustness of the path bandwidth estimation algorithms for individual TCP connections. Relatively fewer attempts have been made to further improve data transport efficiency by sharing path bandwidth information among concurrent TCP connections with the same sources and destinations. The paper proposes an aggregate TCP-based congestion control algorithm (ATCP) that allows individual TCP connections to reach their fair shares of the available network path bandwidth more quickly, while still observing TCP's congestion control semantics. In addition, the proposed algorithm is guaranteed to perform no worse than current TCP congestion control algorithm in all cases, and is designed to be implemented in a way that is completely transparent to both ends of a TCP connection. ATCP is particularly useful for TCP connections that are short-lived and yet have a long round-trip delay, such as Web page transfers using HTTP 1.0. Our trace-driven simulation study shows that the aggregate congestion control algorithm can reduce the normalized transaction latency by a factor of up to 2, compared to standard TCP.",
Algorithm performance contest,"This contest involved the running and evaluation of computer vision and pattern recognition techniques on different data sets with known groundwidth. The contest included three areas; binary shape recognition, symbol recognition and image flow estimation. A package was made available for each area. Each package contained either real images with manual groundtruth or programs to generate data sets of ideal as well as noisy images with known groundtruth. They also contained programs to evaluate the results of an algorithm according to the given groundtruth. These evaluation criteria included the generation of confusion matrices, computation of the misdetection and false alarm rates and other performance measures suitable for the problems. The paper summarizes the data generation for each area and experimental results for a total of six participating algorithms.",
RT-level interconnect optimization in DSM regime,"We propose global-net clustering based RT-level datapath design methodology. Static timing analysis identifies critical nets and critical primary input/output paths. Net clustering (based shared macro-cells and criticality) yields clusters wherein each cluster has strongly interdependent nets. Clusters and nets within every cluster are prioritized based on number of critical nets, number of nets, and the total macro-cell area. We propose two approaches to generate layouts at RTL: constructive (cluster growth) approach and iterative improvement based (simulated annealing) approach. For datapaths implemented in 0.35 /spl mu/m technology, for both approaches, we achieved an average decrease of 54% in longest wirelength and 53% in overall wirelength.",
An asymmetric watermarking scheme based on visual cryptography,"Digital watermarking is a very active research area for copyright protection of electronic documents and media. A visual cryptographic approach is used to generate two random shares of a watermark: one is embedded into the cover-image and another is kept as a secret key for the later watermark extraction. The watermark can be extracted by simply superimposing the key share over the stego-image. This asymmetric digital watermark is specially designed and is not easily changed or removed. But, it is very convenient to be extracted. The embedded digital watermark by this approach seems robust after several attacks are performed on the stego-image.",
Dynamical evolution of coherent structures in intermittent two-dimensional MHD turbulence,"Recent satellite observations indicate that the Earth's magnetotail is generally in a state of intermittent turbulence. A model of sporadic localized merging of coherent structures has recently been proposed by Chang to describe the dynamics of the Earth's magnetotail. The authors report the results of MHD simulations regarding the development and merging of two dimensional coherent structures. With a magnetic shear, such coherent structures are generated in alignment with the imposed current sheet. The calculated fluctuation spectra suggest long-ranged correlations with power-law characteristics.",
Fully complex backpropagation for constant envelope signal processing,"One of the challenges in designing a neural network to process complex-valued signals is finding a suitable nonlinear complex activation function. The main reason for this difficulty is the conflict between the boundedness and the differentiability of complex functions in the entire complex plane, stated by Louiville's theorem. To avoid this difficulty, splitting, i.e., using two separate real nonlinear activation functions for the real and imaginary signal components has been the traditional approach. We introduce a feedforward neural network (FNN) architecture employing hyperbolic tangent tanh(z) function defined in the entire complex domain, and compare its performance with the FNN that uses a split complex structure. Since tanh(z) is analytic and bounded almost everywhere in the complex plane, when trained by backpropagation, it can easily outperform the non-analytic split complex activation function in convergence speed and achievable minimum squared error when the domain is bounded around the unit circle. We demonstrate this property by an equalization example, equalization of multi-phase shift keying (MPSK) signals corrupted by a multipath channel. The properties of tanh(z) and future directions to combat nonlinear distortions in complex transmission schemes are discussed.",
Integrating transportation in a multi-site scheduling environment,"Multi-site scheduling deals with the scheduling problems of an enterprise with several distributed production sites, where sites are using the intermediate products of other sites to manufacture the products of the enterprise. Therefore the transportation of the raw materials and the intermediate products to the plants is an important task within the whole process of manufacturing. Scheduling problems are treated on two levels. On the global level, a global schedule is generated including the requirements for the local level schedulers which then have to transform the global schedule into a local schedule for manufacturing. Since transportation is a vital task in the multi-site scenario, we view it as a scheduling task on the local level as well. Besides the ""classical"" objectives of transportation tasks such as finding shortest paths or minimizing costs, the temporal restrictions of meeting the delivery dates here are the most important goals. We describe how transportation tasks can be modeled as a scheduling problem and which kind of solution strategies are appropriate.",
Super-linear time-space tradeoff lower bounds for randomized computation,"We prove the first time-space lower bound tradeoffs for randomized computation of decision problems. The bounds hold even in the case that the computation is allowed to have arbitrary probability of error on a small fraction of inputs. Our techniques are an extension of those used by M. Ajtai (1999) in his time-space tradeoffs for deterministic RAM algorithms computing element distinctness and for deterministic Boolean branching programs computing an explicit function based on quadratic forms over GF(2). Our results also give a quantitative improvement over those given by Ajtai. Ajtai shows, for certain specific functions, that any branching program using space S=o(n) requires time T that is superlinear. The functional form of the superlinear bound is not given in his paper, but optimizing the parameters in his arguments gives T= /spl Omega/(n log log n/log log log n) for S=0(n/sup 1-/spl epsiv//). For the same functions considered by Ajtai, we prove a time-space tradeoff of the form T=/spl Omega/(n/spl radic/(log(n/S)/log log(n/S))). In particular for space 0(n/sup 1-/spl epsiv//), this improves the lower bound on time to /spl Omega/(n/spl radic/(log n/log log n)).",
Dimension in complexity classes,"A theory of resource-bounded dimension is developed using gales, which are natural generalizations of martin-gales. When the resource bound /spl Delta/(a parameter of the theory) is unrestricted, the resulting dimension is precisely the classical Haludolff dimension (sometimes called ""fractal dimension""). Other choices of the parameter /spl Delta/ yield internal dimension theories in E, E/sub 2/, ESPACE, and other complexity classes, and in the class of all decidable problems. In general, if C is such a class, then every set X of languages has a dimension in C, which is a real number dim(X|C)/spl isin/[0, 1]. Along with the elements of this theory two preliminary applications are presented: 1. For every real number 0/spl les//spl alpha//spl les/ 1/2 , the set FREQ(/spl ap//spl alpha/), consisting of all languages that asymptotically contain at most /spl alpha/ of all strings, has dimension /spl Hscr/(/spl alpha/)-the binary entropy of /spl alpha/-in E and in E/sub 2/. 2. For every real number 0/spl les//spl alpha//spl les/1, the set SIZE(/spl alpha/2/sup n//n), consisting of all languages decidable by Boolean circuits of at most /spl alpha/2/sup n//n gates, has dimension /spl alpha/ in ESPACE.",
Design and analysis of 10-transistor full adders using novel XOR-XNOR gates,"Full adders are important elements in applications such as DSP architectures and microprocessors. We propose a technique to build a total of 41 new 10-transistor full adders using novel XOR and XNOR gates in combination with existing ones. We have done over 10000 HSPICE simulation runs of all the different adders in different input patterns, frequencies, and load capacitances. Almost all those new adders consume less power in high frequencies, while three new adders consistently consume on average 10% less power and have higher speed compared with the previous 10-transistor full adder and the conventional 28-transistor CMOS adder.",
Hop integrity in computer networks,"A computer network is said to provide hop integrity iff when any router p in the network receives a message m supposedly from an adjacent router q, then p can check that m was indeed sent by q, was not modified after it was sent, and was not a replay of an old message sent from q to p. We describe three protocols that can be added to the routers in a computer network so that the network can provide hop integrity. These three protocols are a secret exchange protocol, a weak integrity protocol, and a strong integrity protocol. All three protocols are stateless, require small overhead, and do not constrain the network protocol in the routers in any way.",
Diffusion follows structure - a network model of the software market,"By conducting simulations, we show that the structure of personal networks significantly influences the diffusion processes in network-effect markets like the software market. Varying connectivity (the number of link to other participants), the heterogeneity of preferences and the price, we analyse networks with two different topologies. The first topology is determined by linking consumers with their closest neighbors (""close topology""). For the second topology, the link as determined randomly (""random topology""). For both topologies, the concentration of market share strongly correlates with the connectivity. The ""random topology"" leads to total market concentration, even with very small connectivity. In contrast, diffusion within networks with close topologies results in a larger diversity of products; in this case, we find total market concentration only for very high connectivity.",
Interpreting key issues in IS/IT benefits management,"This paper examines those contextual issues that determine the success of a computer-based information system (IS). It shows that a narrow, technology-focused orientation in systems development is a limiting factor in realizing benefits. In doing so, it argues that real benefits reside not within the IT domain but instead in the changes in the organizational activities that the IT system has enabled. The paper reviews such benefits management aspects through two IS implementation case studies.",
A continuous Chinese sign language recognition system,"We describe a system for recognizing both the isolated and continuous Chinese sign language (CSL) using two cybergloves and two 3SAPCE-position trackers as gesture input devices. To get robust gesture features, each joint-angle collected by cybergloves is normalized. The relative position and orientation of the left hand to those of the right hand are proposed as the signer position-independent features. To speed up the recognition process, fast match and frame prediction techniques are proposed. To tackle the epenthesis movement problem, context-dependent models are obtained by the dynamic programming (DP) technique. HMM are utilized to model basic word units. Then we describe training techniques of the bigram language model and the search algorithm used in our baseline system. The baseline system converts sentence level gestures into synthesis speech and gestures of a 3D virtual human synchronously. Experiments show that these techniques are efficient both in recognition speed and recognition performance.",
Routing protocols overview and design issues for self-organized network,"A self-organized network is a kind of wireless network that can be deployed instantly and provide easy network communication without the support of pre-established network infrastructures, such as base stations. This network architecture brings promise of much better mobility and communication capacity. Various routing protocols have been presented. This paper gives a detailed study of whether each of them works and whether there is a superior one that can function successfully under all kinds of situations. We conclude that there is no superior protocol for all situations and look at the main issues to be considered when designing routing protocols for a self-organized network.",
Model structure and load balancing in optimistic parallel discrete event simulation,The concept of strong groups is introduced to describe the structure of simulation models. It is shown that logical processes within strong groups process at approximately the same rate and that different strong groups can progress at different rates. An algorithm based on the rates of the strong groups is presented to balance the load among the physical processors and for flow control.,
Pattern recognition using genetic algorithm,"Genetic algorithms have been proved to be quite effective in solving certain optimization and artificial intelligence (AI) problems. They have been used in many application areas, including pattern recognition. However, the applications of genetic algorithms in pattern recognition have concentrated primarily on training neural networks for pattern recognition (Montana 1989, Whitley 1992, Kitano 1994). The research in this paper is aimed at using a genetic algorithm to perform pattern matching directly. The basic idea is to use a genetic algorithm to find the best match between nodes of the two patterns. An objective function can be defined in terms of the total difference in the magnitudes of angles between the corresponding edges of the two patterns. Experiments designed to evaluate the algorithm have shown very promising results with high accuracy in classifying the input patterns.",
Parallel execution of a sequential network simulator,"Parallel discrete event simulation (PDES) techniques have not yet made a substantial impact on the network simulation community because of the need to recast the simulation models using a new set of tools. To address this problem, we present a case study in transparently parallelizing a widely used network simulator, called ns. The use of this parallel ns does not require the modeler to learn any new tools or complex PDES techniques. The paper describes our approach and design choices to build the parallel ns and presents preliminary performance results, which are very encouraging.",
Individual 3D face synthesis based on orthogonal photos and speech-driven facial animation,"In the paper, a methodology for individual face synthesis using given orthogonal photos is proposed. An integrated speech-driven facial animation system is presented. Firstly, in order to capture a given subject's personal facial configuration, a novel coarse-to-fine strategy based on facial texture and deformable template is proposed to localize some facial feature points in the image of frontal view. The corresponding feature points in the profile are extracted by using the polygonal approximation. Secondly, all these feature points are aligned to fit the generic 3D face model to a specialized one to reflect the given person's facial configuration. Then a multi-direction texture-mapping technique is presented to synthesize a lifelike personal face. Finally, muscle-based expression and lip-motion models are built up. All the above technologies are integrated into a speech-driven face animation system. We are aiming at a MPEG-4 compatible video-driven face animation system.",
Modular pattern classifiers: a brief survey,"While solving a complex pattern classification problem, it is often difficult to design a monolithic classifier. One approach is to divide the problem into smaller ones, and solve each subproblem using a simpler classifier. This kind of divide and conquer policy has motivated the researchers to substitute a modular classifier for the single monolithic classifier. The paper reviews the advantages, issues and various techniques available for designing the modular classifiers.",
Adaptive agent integration architectures for heterogeneous team members,"With the proliferation of software agents and smart hardware devices there is a growing realization that large-scale problems can be addressed by integration of such standalone systems. This has led to an increasing interest in integration architectures that enable a heterogeneous variety of agents and humans to work together. These agents and humans differ in their capabilities, preferences, the level of autonomy they are willing to grant the integration architecture and their information requirements and performance. The challenge in coordinating such a diverse agent set is that potentially a large number of domain-specific and agent specific coordination plans may be required. We present a novel two-tiered approach to address this coordination problem. We first provide the integration architecture with general purpose teamwork coordination capabilities, but then enable adaptation of such capabilities for the needs or requirements of specific individuals. A key novel aspect of this adaptation is that it takes place in the context of other heterogeneous team members. We are realizing this approach in an implemented distributed agent integration architecture called Teamcore. Experimental results from two different domains are presented.","Humans,
Teamwork,
Computer architecture,
Software agents,
Collaboration,
Computer science,
Large scale integration,
Monitoring,
Costs,
Encoding"
Analysis of high-level address code transformations for programmable processors,"Memory intensive applications require considerable arithmetic for the computation and selection of the different memory access pointers. These memory address calculations often involve complex (non) linear arithmetic expressions which have to be calculated during program execution under tight timing constraints, this becoming a critical bottleneck in the overall system performance. This paper explores applicability and effectiveness of source-level optimisations (as opposed to instruction-level) for address computations in the context of multimedia. We propose and evaluate two processor-target independent source-level optimisation techniques, namely, global scope operation cost minimisation complemented with loop-invariant code hoisting, and nonlinear operator strength reduction. The transformations attempt to achieve minimal code execution within loops and reduced operator strengths. The effectiveness of the transformations is demonstrated with two real-life multimedia application kernels by comparing the improvements in the number of execution cycles, before and after applying the systematic source-level optimisations. Using state-of-the-art C compilers on several popular RISC platforms.",
Analyzing xfig using the Rigi tool suite,"An experiment is conducted on how well expert users of program comprehension tools are able to perform specific program understanding and maintenance tasks on the xfig drawing program using these tools. The paper reports on the experiences of the users of the Rigi reverse engineering tool suite. Rigi is an interactive, visual tool designed to help developers better understand and redocument their software. Rigi includes parsers to read the source code of the subject software and produce a graph of extracted artifacts such as procedures, variables, calls, and data accesses. To manage the complexity of the graph, an editor allows the software engineer to automatically or manually collapse related artifacts into subsystems. These subsystems typically represent concepts such as abstract data types or personnel assignments. The created hierarchy can be navigated, analyzed, and presented using various automatic or user-guided graphical layouts.",
Calibration of light sources,"We present a methodology for calibrating multiple light source locations in 3D from images. The procedure involves the use of a novel calibration object that consists of either 2 or 3 spheres at known relative positions. There are two variants of the process: one which uses range and intensity imaging to find the positions of the light sources, and one that uses only the intensity image to locate the illuminants. We conducted experiments using both variations of the technique to locate light sources in 51 different positions in a laboratory setting. Our data shows that the vector from a point in the scene to a light source can be measured to within 3/spl deg/(6%) of its tote direction and within 0.13 m (9%) of its true magnitude compared to empirically measured ground truth. Finally, we demonstrate how light source information can be applied to burn scar color correction and color segmentation.",
Face detection by facets: combined bottom-up and top-down search using compound templates,"New techniques are needed to effectively search the image and feature space of larger and more complex domains. One such technique uses subfeatures and spatial models to represent a compound object, such as a face. From these compound models, hypothesis based search then combines bottom-up and top-down search processes to localize the search within the image and feature space. Detected subfeatures become evidence for facial hypotheses, which then guide local searches for the remaining subfeatures, based upon the expected facial configuration. We describe this compound technique and present a comparison of the compound templates technique with a single template technique in a mug shot style face domain. Attention is paid to performance, including both efficiency and accuracy. The results are complex, and the strengths, weaknesses, and various trade-offs of the two techniques are detailed.",
Viable systems: the control paradigm for software architecture revisited,"An emerging class of software applications are identified as ""complex"" systems. They are complex in that they must adapt to a changing environment. This motivates us to revisit the ""control paradigm"" for software architecture. In this paper, we go beyond that approach and introduce the concept of viability as the overall characteristic of the behaviour desired in such systems. We present an architecture to guide the software engineering of this class of complex system. The architecture is based on a cybernetic model called the ""viable system model"". As an application of the approach, we are developing a ""smart lecture room"". We report on our efforts in employing the architecture to develop this application.",
Feature learning for recognition with Bayesian networks,"Many realistic visual recognition tasks are ""open"" in the sense that the number and nature of the categories to be learned are not initially known, and there is no closed set of training images available to the system. We argue that open recognition tasks require incremental learning methods, and feature sets that are capable of expressing distinctions at any level of specificity or generality. We describe progress toward such a system that is based on an infinite combinatorial feature space. Feature primitives can be composed into increasing complex and specific compound features. Distinctive features are learned incrementally, and are incorporated into dynamically updated Bayesian network classifiers. Experimental results illustrate the applicability and potential of our approach.",
Improving appearance-based object recognition in cluttered backgrounds,"Appearance-based object recognition systems are currently the most successful approach for dealing with 3D recognition of arbitrary objects in the presence of clutter and occlusion. However, no current system seems directly scalable to human performance levels in this domain. We describe a series of experiments on a previously described object recognition system that try to see, if any, which design axes of such systems hold the greatest potential for improving performance. We look at the potential effect of different design modifications, and conclude that the greatest leverage lies at the level of intermediate feature construction.",
A region extraction method using multiple active contour models,"A new method for extracting an object region in a complex scene image is proposed. For improving the accuracy and the robustness of region extraction, to the region of a single object, the proposed method applies multiple active contour models (ACMs) controlled by the statistical characteristics of image data. Around the object boundary, these ACMs compete with each other and each ACM extracts a subregion of uniform image properties. As a result of this competition, the entire region of an object is extracted as a set of several subregions. For the proposed method it is necessary to set many initial contours. To lighten this load, a procedure for making multiple initial contours from a single initial curve is also proposed. In this procedure, a few initial curves are set in an image, each initial curve is divided into segments at the optimal loci, and initial contours are made by dilating these segments.",
Software architecture analysis based on statechart semantics,"High assurance architecture-based and component-based software development relies fundamentally on the quality of the components of which a system is composed and their configuration. Analysis over those components and their integration as a system plays a key role in the software development process. This paper describes an approach to develop and assess architecture and component-based systems based on specifying software architecture augmented by statecharts representing component behavioral specifications. The approach is applied for the C2 style and associated ADL and is supported within a quality-focussed environment, called Argus-I, which assists specification-based analysis and testing at both the component and architecture levels.",
Online learning of the sensors fuzzy membership functions in autonomous mobile robots,"We describe a technique which enables a fuzzy-logic based robot control system to automatically determine the membership functions (MF) of the input sensors online and in a short time interval. There is a necessity for such online self-calibration for fast changing and dynamic environments such as agricultural environments and difficult or inaccessible environments, such as nuclear reactors, underwater and space environments. In these media the robot has to learn the appropriate MF with no human intervention taking into account the difference in sensor characteristics in the different environments and changes in production requirements and repairing or otherwise upgrading robots. So there is a necessity to find a fast converging algorithm that can calibrate the MF online in real time with no need for human intervention or simulation. our work reports on an approach based on the use of a modified genetic algorithm to evolve the fuzzy MF of the individual behaviours. The MF of four behaviours were learnt online in an average time of 4 minutes for each behaviour in an outdoor environment. These learnt behaviours were then co-ordinated and tested in complex and dynamic environments in which the robot gave a very good response.",
Interference mitigation in frequency-hopped spread-spectrum systems,In this paper we explore the error probability of different coding schemes in a frequency-hopped spread-spectrum communication systems subject to partial-band interference and a system subject to Rayleigh fading. The interplay between block length of the code and channel memory is quantified. We show that there is an optimal memory length that maximizes performance. At low signal-to-noise ratios (or close to capacity) large memory is better while at large signal-to-noise ratio smaller memory is optimum.,
To gesture or not to gesture: what is the question?,"Computer synthesized characters are expected to make appropriate face, limb, and body gestures during communicative acts. We focus on non-facial movements and try to elucidate what is intended with the notions of ""gesture"" and ""naturalness"". We argue that looking only at the psychological notion of gesture and gesture type is insufficient to capture movement qualities needed by an animated character. Movement observation science, specifically Laban Movement Analysis and its Effort and Shape components with motion phrasing provide essential gesture components (I. Bartenieff and D. Lewis, 1980). We assert that the expression of movement qualities from the Effort dimensions are needed to make a gesture naturally crystallize out of abstract movements. Finally, we point out that nonfacial gestures must involve the rest of the body to appear natural and convincing. A system called EMOTE has been implemented which applies parameterized Effort and Shape qualities to movements and thereby forms improved synthetic gestures.",
A recurrent neural network for solving nonlinear projection equations,"In this paper, we are concerned with the nonlinear projection equations of the following form P/sub /spl chi//(u-F(u))=u. Despite the particular structure of the feasible set /spl chi/, the problem is still a very general problem in mathematics programming. Moreover, there are a number of important applications which lead to this special class of variational inequalities such as equilibrium models arising in fields of economics and transportation science, etc. Various numerical solution procedures for the problem have been investigated over decades. Because of the nature of digital computers, conventional algorithms are time-consuming for large-scale optimization problems. It is well-known that one promising approach to optimization problems in real time is to employ artificial neural networks implemented in hardware. Recurrent neural networks for solving optimization problems are readily hardware-implementable. Thus, neural networks are a top choice of real-time solvers for optimization problems. Since the seminal work of Hopfield and Tank (1985), the neural network approach to optimization has been investigated and many neural networks for optimization problems have been proposed.",
A hybrid architecture for hierarchical reinforcement learning,"Autonomous robot systems operating in the real world have to be able to learn new tasks and environmental conditions without the need for an outside teacher. While reinforcement learning represents a good formalism to achieve this, its long learning times and need for extensive exploration often make it impracticable for online learning on complex systems. The hybrid architecture presented in this paper addresses this issue by applying reinforcement learning on top of an automatically derived abstract discrete event dynamic system (DEDS) supervisor. This reduces the problem of policy acquisition within this approach to learning to coordinate a set of closed-loop control strategies in order to perform a given task. Besides dramatically reducing the complexity of the learning task this framework also permits the incorporation of a priori knowledge and facilitates the inclusion of learned policies as actions in order to transfer skills to new task domains. To demonstrate the applicability of this approach, the architecture is used to learn locomotion gaits on a four-legged robot platform.",
Combining inductive and deductive inference in knowledge management tasks,"This paper indicates how different logic programming technologies can underpin an architecture for distributed knowledge management in which higher throughput in information supply is achieved by a (semi-)automated solution to the more challenging problem of knowledge creation. The paper first proposes working definitions of the notions of data, knowledge and information in purely logical terms, and then shows how existing technologies can be combined into an inference engine, referred to as a knowledge, information and data engine (KIDE), integrating inductive and deductive capabilities. The paper then briefly introduces the notion of virtual organizations and uses the set-up stage of virtual organizations to exemplify the value-adding potential of KIDEs in knowledge management contexts.",
Petri-net based performance-evaluation of distributed homogeneous task systems,"TG (task graphs) are used to describe the execution of several tasks under some precedence constraints. Direct evaluation of TG provides an average completion time of the overall job, assuming no limits exist in the number of processing units and with no regard for allocation schemes. This paper presents a systematic approach for evaluating TG of jobs executed under predetermined allocation constraints. This extension of TG relies on GSPN (generalized stochastic Petri nets). A systematic mapping of a TG into a GSPN model is discussed. This GSPN model is extended to incorporate information about the static allocation of the set of tasks in the TG. An algorithm is implemented to evaluate static allocation schemes with or without task replication. However, for task replication, a homogeneous system is assumed because the execution time of those tasks does not change when allocated to various processing units. Also, under this assumption, task execution rates are modified by adding communication costs involved in sending data required by the next task, in turn, to execute. Thus, using a single model, TG are evaluated with constraints not only on where replicated and nonreplicated tasks are to be executed but on the number of processing units available, task allocation constraints, and the communication costs involved when they are remotely located.",
Perspectives on learning in a capstone design course,"A new course learning model was developed for our capstone design course in computer engineering, ECE 482-Capstone: Computer System Design. It has been delivered during six semesters, each providing a set of new experiences and an array of lessons learned. Students, employers and educational researchers have recognized the benefits of the course. Despite the positive outcomes, key questions and obstacles remain that impact sustainable reform in the capstone design experience. We chronicle the experiences and lessons from the perspectives of the faculty and students involved with the course. We focus on the learning model, including its implementation, adaptation, impact and perception.",
Biologically inspired computing,"Computing systems inspired by biological systems (biocomputation) are one possible alternative currently being investigated. Whether it will impact information technology as defined today is still unclear. The field of biocomputation has a twofold definition: the use of biology or biological processes as metaphor, inspiration, or enabler in developing new computing technologies and new areas of computer science; and conversely, the use of information science concepts and tools to explore biology from a different theoretical perspective. In addition to its potential applications, such as DNA computation, nanofabrication, storage devices, sensing, and health care, biocomputation also has implications for basic scientific research. It can provide biologists, for example, with an IT-oriented paradigm for looking at how cells compute or process information, or help computer scientists construct algorithms based on natural systems, such as evolutionary and genetic algorithms. Biocomputing has the potential to be a very powerful tool.",
Optimization of spare capacity in self-healing multicast ATM network using genetic algorithm,"The objective of this paper is to optimize the amount of spare capacity reserved for the backup virtual paths (BVPs) in multicast asynchronous transfer mode networks. In this paper, we study the capacity and routing assignment problem arising in the design of self-healing networks using the VP concept. A major contribution of this work is to apply a genetic algorithm (GA) to the backup path search process instead of the trivial exhaustive search method. Experimental results indicate that both approaches have very comparable results in finding the multicast backup paths. Further, it also indicated that using a GA approach has a number of advantages over the exhaustive search approach, such as the computational requirement for a GA in finding good BVPs is small when compared to the exhaustive search method. A major drawback of our approach is that we cannot guarantee the finding of global optimum in real time.",
A Wrapper Generator for Wrapping High Performance Legacy Codes as Java/CORBA Components,"This paper describes a Wrapper Generator for wrapping high performance legacy codes as Java/CORBA components for use in a distributed component-based problem- solving environment. Using the Wrapper Generator we ave automatically wrapped an MPI-based legacycode as a single CORBA object, and implemented a problem- solving environment for molecular dynamic simulations. Performance comparisons between runs of the CORBA object and the original legacy code on a cluster of workstations and on a parallel computer are also presented.",
Model-based halftoning for color image segmentation,"Grouping algorithms based on histograms over measured image features have very successfully been applied to textured image segmentation. However, the competing goals of statistical estimation significance demanding few quantization levels versus the necessary richness in representation often prevent a successful application for the color cue, since quantization may result in contouring. We combine a halftoning technique called spatial quantization with distribution-based grouping algorithms to synthesize a powerful color image segmentation technique. The spatial quantization simultaneously determines color palette and halftoning by optimization of a joint cost function. It therefore allows for a highly adapted image representation with a smooth transition of color distributions for non-constant image surfaces.",
A dual-hybrid adaptive routing strategy for wireless ad-hoc networks,"Recent debate in the research community has focused around the following question: should the design of routing algorithms for ad-hoc networks be predicated upon the minimization of routing overhead, or the optimization of network paths? The answer to this question depends upon the mobility and traffic characteristics present in the network. It is doubtful that any one approach by itself can be optimal, or even sufficient when there are temporal and spatial changes in the network dynamics. Most proposed solutions have adopted a fixed approach. To address this shortcoming we propose a dynamic cluster based routing strategy that shifts the characteristics of its routing approach according to localized network dynamics. This is achieved by dividing routing into intra-cluster and inter-cluster components utilizing a strategy that is hybrid in terms of both its route acquisition and path computation approaches. By adapting the cluster organization to node mobility the strategy senses the network and balances optimality and overhead. The reason for doing this is to achieve the best possible performance subject to a wide range of operating environments.",
Application-specific file prefetching for multimedia programs,"This paper describes the design, implementation, and evaluation of an automatic application-specific file prefetching mechanism that is designed to improve the I/O performance of multimedia programs with complicated access patterns. The key idea of the proposed approach is to convert an application into two threads: a computation thread, which is the original program containing both computation and disk I/O, and a prefetch thread, which contains all the instructions in the original program that are related to disk accesses. At run time, the prefetch thread is scheduled to run far ahead of the computation thread, so that disk blocks can be prefetched and put in the file system buffer cache before the computation thread needs them. A source-to-source translator is developed to automatically generate the prefetch and computation thread from a given application program without any user intervention. We have successfully implemented a prototype of this automatic application-specific file prefetching mechanism under Linux. The prototype is shown to provide as much as 54% overall performance improvement for real-world multimedia applications.",
On L/sub p/ (l/sub p/) performance with global internal stability for linear systems with actuators subject to amplitude and rate saturations,"It has been shown that for critically unstable systems with saturating actuators one can not simultaneously achieve the global internal stabilization and the global finite-gain L/sub p/ (l/sub p/) performance whenever the external input (disturbance) is not input-additive. However, one can achieve the global internal stabilization and global L/sub p/ (l/sub p/) stabilization (without finite-gain) for any p/spl isin/[1,/spl infin/). In this paper we show that for null controllable systems with actuators subject to amplitude and rate saturations, the L/sub p/ (l/sub p/) performance (without finite gain) with global internal stability can be achieved.",
Self-directed learning in an ASL course,"Every study of engineering education and the skills required of practicing engineers lists life-long learning (LLL) as a necessary ingredient. And yet, there has been little developed in the way of formal preparation for engineering students so that they will become life-long learners. In fact, the loaded curriculum presented in most engineering programs works against developing the learning skills and love of learning required to be a successful LLL. Students do not have time to reflect on what they are learning or to explore personal interests through elective courses while in college. The seeds of effective LLL must be sown at the beginning of the program if the college experience is going to support this type of development in engineering students. Previously, a paper describing activities being introduced in the DTeC course at Binghamton University (BU) and the engineering science program at Broome Community College (BCC) to start students on the path toward becoming self-directed learners (SDL) was presented (S.B. Fellows et al., 2000). The paper describes developments during the past year (2000-2001) in implementing the SDL program at the two institutions.",
The LANL Neutron-Science-Center time-of-flight/position-sensitive-detect module: status and progress,"This paper describes the progress and current status of a joint collaboration between Argonne National Laboratory (ANL) and the Los Alamos National Laboratory (LANL) Manuel Lujan Neutron Science Center (MLNSC) to develop and implement a Time-of-flight (TOF)/Position-Sensitive-Detector (PSD) VXI-based C-size neutron-event data-collection module. The LANL module, based on the ANL-developed hardware which uses field-programmable gate arrays (FPGAs) and analog-input-signal conditioning modules for a flexible topology capable of accepting either eight or 16 input channels, has been programmed and modified to incorporate more LANL-specific features such as improved peak detection, 24-bit time stamps, and 16-bit channel identification numbers which are all part of a 64-bit event record (2 by 32-bits wide). Using a backplane 10-MHz clock, timing resolution is /spl plusmn/50 ns. The module uses two, frame first-in-first-out buffers (FIFOs), each 2-kwords deep, to accumulate event data at up to 330 kEvents/sec during a frame until the host computer can read it out. One FIFO is read while the other is being filled. The module does not use the ANL token-passing configuration for accessing data. Rather, it uses direct logical-address and register-based addressing modes. To interface with analog signals from the neutron detectors, the module incorporates eight 72-pin single-inline-memory-module-size plug-in boards, called SIMs, which contain differential receivers, analog threshold comparators, and 8-bit analog-to-digital converters. A total of 16-analog channels are available if used in TOF mode, or eight channels if used in PSD mode.",
Hardware versus hybrid data prefetching in multimedia processors: a case study,"Data prefetching is a promising technique for hiding the penalties due to compulsory cache misses. In this paper we present a case study on two types of data prefetching in the context of multimedia processing: a purely hardware-based technique and a more low-cost hybrid hardware/software technique. Moreover, we also propose a technique for increasing the so-called prefetch distance in hardware prefetching and a scheme to reduce trashing in the data cache. Our results demonstrate that the low-cost hybrid prefetching scheme slightly outperforms hardware-based prefetching for the code segments for which both solutions have been applied, while hardware prefetching potentially allows more code to benefit from the prefetching.",
The evaluating system of human skin surface condition by image processing,"We propose an automatic evaluation system of human skin surface condition based on subjective evaluation provided by cosmeticians. In the proposed system, image features extracted on the skin image and subjective evaluation by cosmeticians are flexibly connected by using a backpropagation neural network, so that it can automatically estimate human skin surface condition based on subjective evaluation from various skin images. We show some experimental results. Using the trained neural network, human skin surface condition based on subjective evaluation is estimated for unlearned skin images. Then subjective evaluation by this system was compared with that by cosmeticians. Since the proposed system can successfully estimate human skin surface condition like cosmeticians, the effect of the system is demonstrated.",
CAPSL integrated protocol environment,"CAPSL, a Common Authentication Protocol Specification Language, is a high-level language to support security analysis of cryptographic authentication and key distribution protocols. It is translated to CIL, an intermediate language expressing state transitions with term-rewriting rules. Connectors are being written to adapt CIL to supply input to different security analysis tools, including PVS for inductive verification and Maude for model-checking.",
Is there a role for programming in non-major computer science courses?,"Should non-computer science (CS) majors learn to program? While the 1999 National Academy of Sciences report ""Being Fluent with Information Technology"" advocates teaching programming as part of the CS-0 experience, we challenge the assumptions upon which this recommendation rests. Our extensive review of the NECC (National Educational Computing Conference) and SIGCSE (Special Interest Group on Computer Science Education) conference proceedings from 1979 to 1998 clearly shows a decline in the number of articles in which programming is taught in CS-0 courses. Furthermore, based upon learning theory literature, we argue that conceptual understanding of computing can be acquired without learning to program. Finally, we describe our criterion-referenced, mastery-model course that prepares 1800 students per semester for a computing future that is constantly changing.",
The NA48 event-building PC farm,"The NA48 experiment at the CERN SPS aims to measure the parameter Re(/spl epsiv/'//spl epsiv/) of direct CP violation in the neutral kaon system with an accuracy of 2x10/sup -4/. Based on the requirements of: high event rates (up to 10 kHz) with negligible dead time; support for a variety of detectors with very wide variation in the number of readout channels; data rates of up to 150 MByte/s sustained over the beam burst; level-3 filtering and remote data logging in the CERN computer center; the collaboration has designed and built a modular pipelined data flow system with 40 MHz sampling rate. The architecture combines custom-designed components with commercially available hardware for cost effectiveness and flexibility. To increase the available data bandwidth and to add filtering and monitoring capabilities, the original custom-built event builder hardware has been replaced by a farm of 24 Intel PentiumII based PCs running the Linux operating system during the shutdown between the 1997 and 1998 data taking periods. During the data taking period 1998 the system has been successfully operated taking ca. 70 Terabyte of data.",
Real-time digital signal processing of component-oriented phased array radars,"With the advance of hardware and software technology, modern phased array radars are now built with commercial off-the-shelf (COTS) components, and it opens up a new era in real-time resource scheduling of digital signal processing. This paper targets the essential issues in building a component-oriented signal processor (SP), which is one of the two major modules in modern phased array radars. We propose a simple but effective task allocation policy and a real-time scheduling algorithm to address the design objectives of SPs. We are able to bound the number of processing units needed for a component-oriented SP in the design time, while everything was done empirically in the past. A series of experiments were done to demonstrate the strength of our methodology.",
Automatic keyword extraction with relational clustering and Levenshtein distances,"Alternating cluster estimation (ACE) is a generalized clustering model. Relational ACE is a modification of ACE that can be used to cluster data which do not possess a clear numerical representation, but for which a meaningful relation matrix can be defined. For text data sets we define (pairwise) relation matrices based on the Levenshtein string distance (1966). Relational ACE with Levenshtein distances is applied to four different texts. The cluster centers represent typical words in the texts, so this algorithm can be used to automatically determine keywords.",
Fast multipattern search algorithms for intrusion detection,"Presents new search algorithms to detect the occurrences of any pattern from a given pattern set in a text, allowing in the occurrences a limited number of spurious text characters among those of the pattern. This is a common requirement in intrusion detection applications. Our algorithms exploit the ability to represent the search state of one or more patterns in the bits of a single machine word and to update all the search states in a single operation. We show analytically and experimentally that the algorithms are able of rapidly searching large sets of patterns, allowing a large number of spurious characters, yielding about a 75-fold improvement over the classical algorithm.",
Distance-spectrum formulas on the largest minimum distance of block codes,"A general formula for the asymptotic largest minimum distance (in block length) of deterministic block codes under generalized distance functions (not necessarily additive, symmetric, and bounded) is presented. As revealed in the formula, the largest minimum distance can be fully determined by the ultimate statistical characteristics of the normalized distance function evaluated under a properly chosen random-code generating distribution. Interestingly, the new formula has an analogous form to the general information-spectrum expressions of the channel capacity and the optimistic channel capacity, respectively derived by Verdu and Han (1994) and Chen and Alajaji (1998, 1999). As a result, a minor class of distance functions for which the largest minimum distance can be derived is characterized. A general Varshamov-Gilbert lower bound is next addressed. Some discussions on the tightness of the general Varshamov-Gilbert bound are also provided. Finally, lower bounds on the largest minimum distances for several specific block coding schemes are rederived in terms of the new formulas, followed by comparisons with the known results devoted to the same codes.",
Multicovering bounds from relative covering radii,"The concept of multicovering radius was introduced by A. Klapper (see IEEE Trans. Inform. Theory, vol.43, p.1372-7, 1997) in the context of studying the security of stream ciphers. The purpose of this paper is to derive new bounds by relating the multicovering radii of a code to a relative notion of covering radius. For generality, we define this notion for multicovering radii, although we only use the ordinary covering radius version.",
A 3D filtered back-projection algorithm for SPECT using a multi-segment rotating slant-hole collimator,"The purpose of this study is to develop, implement and evaluate a 3D filtered back-projection (FBP) algorithm for SPECT using a multi-segment rotating slant-hole (RSH) collimator. Previously, the authors derived a 3D frequency-domain filter function used in a back-projection filtering (BPF) algorithm for multi-segment RSH SPECT. The 3D orientation of the projection direction from each segment, each collimator rotation angle and each collimator-camera position defines a 2D central section through the center of the 3D BPF reconstruction filter. For FBP, these central sections are determined analytically from the mathematical expression of 3D BPF filter, and are re-scaled and rotated before filtering the corresponding projections to avoid interpolation. The FBP algorithm was implemented to reconstruct simulated and experimentally acquired SPECT data from a four-segment RSH collimator. The reconstructed images were compared to those from BPF. The results of FBP are comparable to BPF, and with reduced low-intensity image artifacts. It is concluded that FBP is a superior alternative to BPF for RSH SPECT. It provides accurate reconstruction in relatively short computing time. In addition, it requires less computer memory, permits on-the-fly image reconstruction during data acquisition, and may facilitate additional processing during reconstruction to reduce truncation artifacts.",
An application based differentiated service model,"In this paper we propose a flexible differentiated service model, which is based on the application layer so that applications can indicate their network service requirements directly. We classify Internet applications into four categories and provide different schemes for each of the categories to indicate its request. The model hides the complexity of the network services from the end-users but can still make efficient use of the negotiated network resources. We implement our model in the ns2 network simulator and show that the performance meets our design goal.",
"Using product, process, and execution metrics to predict fault-prone software modules with classification trees","Software-quality classification models can make predictions to guide improvement efforts to those modules that need it the most. Based on software metrics, a model can predict which modules will be considered fault-prone, or not. We consider a module fault-prone if any faults were discovered by customers. Useful predictions are contingent on the availability of candidate predictors that are actually related to faults discovered by customers. With a diverse set of candidate predictors in hand, classification-tree modeling is a robust technique for building such software quality models. This paper presents an empirical case study of four releases of a very large telecommunications system. The case study used the regression-tree algorithm in the S-Plus package and then applied our general decision rule to classify modules. Results showed that in addition to product metrics, process metrics and execution metrics were significant predictors of faults discovered by customers.",
"Fast, fully-automated generation of control skeletons for use in animation","This paper describes an algorithm for automatically generating a control skeleton for use in animating a polygonal data model. The algorithm has several steps. First, the figure is voxelized, and a Euclidean distance map is computed for voxels interior to the figure. A path creation algorithm then generates a tree-structured set of voxel paths spanning the object's interior. Simplification of these paths leads to a control skeleton for the object. The voxelization is used again to anchor the vertices of the polygonal model to the skeleton. Afterwards, new joint angles may be specified for the skeleton, and once the skeleton has been updated, vertices of the model can be repositioned accordingly. Unlike previous methods, the algorithm is fully automated, requiring very little user input. Also, it can be applied successfully to a wide variety of objects. Furthermore, the algorithm is fast-it can produce a useful control skeleton for most objects in under two minutes on a low-end PC.",
Exploiting planned disconnections in mobile environments,"We present the notion of a distributed database made up entirely of mobile components. Since disconnections will be frequent in such an environment, we develop a disconnection and reconnection procedure to allow normal processing on the connected components. We briefly discuss a protocol based on epidemic communication to support such a system while ensuring one-copy serializability.",
Multicasting in a class of multicast-capable WDM networks,"Multicast is the ability to transmit information from a single source node to multiple destination nodes. Multicast can be supported more efficiently in an optical domain by utilizing the inherent light splitting capacity of optical switches than by copying data in an electronic domain. In this paper, we study multicast communication in a class of multicast-capable WDM networks (i.e., the networks that have light splitting switches) with regular topologies under some commonly used routing algorithms. Upper and lower bounds on the minimum number of wavelengths required are determined for a network to be rearrangeable for an arbitrary multicast assignment, and compared with those WDM networks without light splitting switches.",
Modeling shared-memory ATM switches with fluid-flow models,Shared-memory switches are developing as the principal switch architecture in ATM networks. This has created demand for efficient and accurate methods of analysis. But the increase in memory sizes used in switches make existing forms of analysis too computationally intensive. Fluid flow models do not include the buffer content in their states and therefore the number of states is buffer independent. This paper critically reviews existing fluid-flow models for shared-memory switches. The advantages of each model are used in developing a model that accurately and efficiently provides cell loss and delay values. The performance of the proposed model is compared with results obtained via simulation and other fluid-flow models. The number of states and the computational time of the fluid-flow models are significantly less than traditional discrete queuing models.,
Processing color and complex data using mathematical morphology,"Mathematical morphology has been used extensively to process binary and grayscale images. The use of mathematical morphology to process color images, however, is quite limited due to technical problems with ordering vector quantities. This paper describes three morphological techniques for processing color images that have appeared in the literature, in addition to presenting a new technique based on vector projections. The usefulness of these techniques is then illustrated on the task of extracting an object from a color image. Techniques for morphologically processing color images can be also applied to images and signals containing complex data.",
Verification of reactive system specifications with outer event conditional formula,"We introduce an efficient tableau-based satisfiability checking procedure for a specification which consists of several modules. This method extracts reduced constraints from each module and verifies a property with them. We also show that this method is applicable to the decision procedure for strong satisfiability and stepwise satisfiability. Finally, we show the experimental results of the method.",
The case for continued Cobol education,"Many educators are reexamining their curricula's content and scope to more successfully market graduates from their information systems programs. Meanwhile, business and industry IS managers have similar concerns about the continued need to maintain the large inventory of legacy code while also developing new systems. Cobol applications and mainframe computing continue to dominate a large segment of the business community in which conventional data- and transaction-processing requirements still drive major applications. The requirement to maintain existing Cobol applications supports continued demand for new Cobol programmers. Furthermore, e-commerce applications will require linking existing mainframe applications to the Internet, occupying much of the IS activity for the foreseeable future. In accessing the IS manager's view and the academic's view of Cobol's future, the survey presented found that 95% of its academic respondents and 90% of the IS managers polled want the IS curricula to continue offering Cobol instruction. Also, nearly 90% of IS managers indicate that Cobol instruction in colleges should cover both Cobol's OO and Web based features.",
The test vector problem and limitations to evolving digital circuits,"How do we know the correctness of an evolved circuit? While Evolutionary Hardware is exhibiting its effectiveness, we argue that it is very difficult to design a large-scale digital circuit by conventional evolutionary techniques alone, if we are using a subset of the entire truth table for fitness evaluation. The test vector generation problem for testing VLSI (Very Large Scale Integration) suggests that there is no efficient way to determine a training set which assures full correctness of an evolved circuit.",
On Connectedness: A Solution Based on Oscillatory Correlation,"A long-standing problem in neural computation has been the problem of connectedness, first identified by Minsky and Papert (1969). This problem served as the cornerstone for them to establish analytically that perceptrons are fundamentally limited in computing geometrical (topological) properties. A solution to this problem is offered by a different class of neural networks: oscillator networks. To solve the problem, the representation of oscillatory correlation is employed, whereby one pattern is represented as a synchronized block of oscillators and different patterns are represented by distinct blocks that desynchronize from each other. Oscillatory correlation emerges from LEGION (locally excitatory globally inhibitory oscillator network), whose architecture consists of local excitation and global inhibition among neural oscillators. It is further shown that these oscillator networks exhibit sensitivity to topological structure, which may lay a neurocomputational foundation for explaining the psychophysical phenomenon of topological perception.",
Is this spreadsheet a tax evader? How HM Customs and Excise test spreadsheet applications,"Spreadsheet models are commonly used by UK taxpayers to calculate their liabilities. The risk of error from spreadsheets has been exhaustively documented, and applications in this domain are no less at risk of error than those in any other. Officers of HM Customs and Excise in the UK have been performing field audits of taxpayers' spreadsheet applications since 1985. Building on the experience gained, HM Customs and Excise computer auditors have developed a testing methodology and audit support software for use by generalised tax inspectors. This paper briefly summarises the audit experience, describes the methodology and outlines the results to date of a campaign of spreadsheet testing that started in July 1999.",
"Computation: evolutionary, neural, molecular","A confluence of factors emanating from computer science, biology, and technology have brought self-organizing approaches back to the fore. Neural networks in particular provide high evolvability platforms for variation-selection search strategies. The neuron doctrine and the fundamental nature of computing come into question. Is a neuron an atom of the brain or is it itself a complex information processing system whose interior molecular dynamics can be elicited and exploited through the evolution process? We argue the latter point of view, illustrating how high evolvability dynamics can be achieved with artificial neuromolecular computer designs and how such designs might in due course be implemented using molecular computing devices. A tabletop enzyme-driven prototype recently implemented in our laboratory is briefly described; it can be thought of as a sort of artificial neuron in which the context sensitivity of enzyme recognition is used to transform injected signal patterns into output activity.",
Implementing software process improvement: two cases of technology transfer,"This paper addresses the issue of technology transfer in software development organizations. Common problems for the software industry are still software failures, project overruns, and unfinished projects. To remedy these, knowledge-intensive technologies like quality management and software process improvement (SPI) have been promoted. The organizational implementation of such approaches is an important and problematic matter. Here, two cases of implementing SPI are reported. A framework integrating theories of general innovation with theories on adoption of information technologies is used to present and interpret the cases. The framework consists of three perspectives: an individualist, a structuralist, and an interaction process perspective. The latter comprises the first two and emphasizes the content, context and process of implementation. The framework turned out to be well suited and provided a rich understanding of the interplay of the different elements influencing the implementation process in the two cases. As such, it might be a useful guide for future SPI implementation in organizations.",
"Computer network-based scientific collaboration in the energy research community, 1973-1977: a memoir","In 1973, four national laboratory groups and three university energy research groups started an investigation of whether scientific collaboration could be improved with computer networks. Four years later, the seven groups were piggybacked on to the Arpanet, and more than 60 case histories of applications had been documented. This article reviews the early collaboration and tells how the energy research community was introduced to modern computer networking.",
A fractionally spaced DFE with subband decorrelation,"We proposed a modification of the ""classic"" fractionally spaced decision feedback equaliser (FS-DFE) in order to increase the slow convergence rate of the oversampled feed-forward section for LMS type algorithms. This is performed by employing a subband structure for the latter part of the FS-DFE, which results in a decorrelation of the input. We motivate why an oversampled subband decomposition is beneficial and comment on the selection of the filter banks. As an additional benefit, computational savings arise if judicious implementation of the filter banks is combined with the internal decimation and expansion operations in the FS-DFE. Simulations for a severe multipath environment are presented.",
Join tensors: On 3D-to-3D alignment of dynamic sets,"Introduces a family of 4/spl times/4/spl times/4 tensors, referred to as ""join tensors"" or Jtensors for short, which perform ""3D to 3D"" alignment between coordinate systems of sets of dynamic 3D points. 3D configurations of points are obtained by a 3D measuring device (such as a structured light or laser range sensor, or a stereo rig) at times t/sub 1/, t/sub 2/, t/sub 3/ from different viewing positions in addition to the motion of the sensor the points are also allowed to move in space; each point can move along an arbitrary straight-line path-we refer to this situation as ""dynamic"". The problem is to recover the motion of the sensor given the 3D correspondences of the points over time. We introduce Jtensors to capture the problem described above. Three observations P, P', P'' of a point measured at three time instants contribute a linear measurement to the Jtensor, regardless of whether the point has moved in space or has remained stationary while the sensor has changed position.",
Incorporating VHDL into the digital curriculum,"Summary form only given. Traditional courses in digital design, especially in engineering technology, have focused on and used TTL logic integrated circuits in lecture and lab. It is still difficult to find a textbook suitable for a technology program that is not based on these chips. However, TTL is very rarely used in industry today. This work in progress report discusses the planned implementation of advanced programmable logic devices (PLDs) and associated software into the digital curriculum, including introductory courses.",
A new class of median based impulse rejecting filters,"This paper proposes a novel adaptive filter based on the impulse rejecting mechanism, where detected impulses are filtered and noise-free pixels are left unaltered. Previous impulse detection strategies based on thresholding operations tend to work well for large, fixed-valued impulses but poorly for random-valued impulse noise, or vice versa. The objective of this work is to utilize the center weighted median (CWM) filters with variable center weights to define a more general operator, which forms estimates according to the differences defined between the outputs of CWM filters and the current pixel in consideration. As compared with existing schemes, the proposed filter consistently performs well in suppressing both types of impulse noise while still employing a simple structure. Better performance has been achieved by the new filter in restoring a variety of images corrupted with different noise ratios.",
Agent-oriented software engineering,,
A blind signature scheme based on ElGamal signature,"The use of public key cryptosystems has received considerable attention. They are beneficial in encryption as well as signing which plays an essential role in electronic banking and financial transactions. This paper presents a new generalized blind signature scheme based on ElGamal (1985) signatures. This new scheme has a valuable property that assures that if a message is signed multiple times, the corresponding signatures will be different. This adds to the anonymity of the blind signatures. The new scheme uses number theory operations and modular arithmetic techniques to achieve the desired goal. The current research introduces a generalized signature scheme that could be used to generate blind signatures as well as ordinary ElGamal signatures. The new scheme is found to be comparable to the RSA blinding. Moreover, the new scheme has the advantage of having less computational complexity and is faster than RSA in the blinding procedure.",
Impact of resource reservation on the distributed multi-path quality of service routing scheme,"This paper studies the impact of resource reservation on the multipath quality-of-service (QoS) routing schemes that use the global network state to make routing decisions. Incorporating resource reservation into multipath QoS routing algorithms greatly changes the communication characteristics in a network system and affects the performance of the routing algorithms. In this paper, we develop a QoS routing protocol that combines resource reservation with the ticket-based distributed multipath QoS routing scheme, evaluate the new routing protocol through extensive simulation, and study the impact of other network components, such as the network size and the link state update mechanisms, on the performance of multipath QoS routing schemes with resource reservation.",
Approximating the statistical distribution of color histogram for content-based image retrieval,"With the rapid development of multimedia technologies, the problem of how to retrieve a specified image from a large amount of image databases becomes an important issue. We propose a new method by approximating the statistical distribution of color histogram for image indexing and comparison. It becomes simple to compute the distance measure of two images by the proposed method. Also, our proposed method tolerates the generally considered problems when retrieving images such as transition, scaling and rotation. The represented features of the approximated distribution of the color histogram can also be used as the indices for the image database. Experimental results show that our proposed method is quite effective not only for the performance but also better results for image indexing and retrieval.",
Pulse mode multilayer neural network based on floating point number representation,"This paper proposes a new type of pulse mode multilayer neural network (MNN) that uses floating point number system for synapse weight value. Combined with pulse mode operation, the floating point operation is implemented without a multiplier. The proposed neuron, synapse multiplier and experimental MNN are implemented on field programmable gate arrays (FPGA) and various experiments are conducted to test the feasibility of the system. These experiments prove that the proposed MNN architecture can be used for applications which require high precision in their calculation.",
Using decision theory to formalize emotions in multi-agent systems,"We use the formalism of decision theory to develop principled definitions of emotional states of a rational agent. We postulate that these notions are useful for rational agent design. First, they can serve as internal states controlling the allocation of computations and time devoted to cognitive tasks under external pressures. Second, they provide a well defined implementation-independent vocabulary the agents can use to communicate their internal states to each other. Finally, they are essential during interactions with human agents in open multi-agent environments. Our approach provides a formal bridge between the rich bodies of work in cognitive science and the high-end AI architectures for designing rational artificial agents.",
A greedy approach to rule reduction in fuzzy models,"The characteristics of a fuzzy model are frequently determined by the manner in which the rules are constructed. Rules obtained by a heuristic assessment of a system generally are linguistically interpretable and have large granularity. The generation of rules via learning algorithms that analyse training data produces precise models consisting of multiple rules of small grannularity. In this paper, a greedy algorithm is presented that combines rule learning with a region merging strategy to reduce the number of rules. This approach differs from standard rule reduction techniques in that the latter are employed after the rule base has been completed while the learn-and-merge strategy generates a rule simultaneously with expanding its region of applicability. The objective of the algorithm is to produce fuzzy models with both a small number of interpretable rules and high precision.",
Test-case generator TCG-2 for nonlinear parameter optimisation,"Experimental results reported in many papers suggest that making an appropriate a priori choice of an evolutionary method for a nonlinear parameter optimisation problem remains an open question. It seems that the most promising approach at this stage of research is experimental, involving a design of a scalable test suite of constrained optimisation problems, in which many features could be easily tuned. Then it would be possible to evaluate merits and drawbacks of the available methods as well as test new methods efficiently. We discuss a new test-case generator for constrained parameter optimisation techniques, which deals with deficiencies of generators proposed earlier. This generator TCG-2 is capable of creating various test problems with different characteristics, including the dimensionality of the problem, number of local optima, number of active constraints at the optimum, topology of the feasible search space, etc. Such a test-case generator is very useful for analysing and comparing different constraint-handling techniques and different nonlinear parameter optimisation techniques.",
Tiling and adaptive image compression,"We investigate the task of compressing an image by using different probability models for compressing different regions of the image. In this task, using a larger number of regions would result in better compression, but would also require more bits for describing the regions and the probability models used in the regions. We discuss using quadtree methods for performing the compression. We introduce a class of probability models for images, the k-rectangular tilings of an image, that is formed by partitioning the image into k rectangular regions and generating the coefficients within each region by using a probability model selected from a finite class of N probability models. For an image of size n/spl times/n, we give a sequential probability assignment algorithm that codes the image with a code length which is within O(k log(Nn/k) of the code length produced by the best probability model in the class. The algorithm has a computational complexity of O(Nn/sup 3/). An interesting subclass of the class of k-rectangular tilings is the class of tilings using rectangles whose widths are powers of two. This class is far more flexible than quadtrees and yet has a sequential probability assignment algorithm that produces a code length that is within O(k log(Nn/k) of the best model in the class with a computational complexity of O(Nn/sup 2/logn) (similar to the computational complexity of sequential probability assignment using quadtrees). We also consider progressive transmission of the coefficients of the image.",
Performance evaluation of image registration,"From the medical point of view, the only meaningful measure of system performance is correct diagnosis. In ventilation and perfusion (V/Q) lung scan analysis, the goal is to identify the degree of match or mismatch, which is dependent on V/Q ratios. High registration performance is a critical factor for computation of the ratios. Thus, the method used to evaluate system performance is important. Here, the authors present an exclusive-or (XOR) function to evaluate registration error improvement and investigate several commonly used evaluation methods to compare registration results. Three measurements are compared with respect to accuracy, reliability, and computational cost. Both direct and indirect methods are studied to assess registration performance. The direct method requires direct user interaction such as manual identification of corresponding points or expert judgment. The indirect method is an automated approach with a minimum amount of user interaction. Examples include the correlation coefficient function and signal-to-noise ratio (SNR) approach.",
Reduction optimization in heterogeneous cluster environments,"Network of workstation (NOW) is a cost-effective alternative to massively parallel supercomputers. As commercially available off-the-shelf processors become cheaper and faster, it is now possible to build a cluster that provides high computing power within a limited budget. However, a cluster may consist of different types of processors and this heterogeneity complicates the design of efficient collective communication protocols. For example, it is a very hard combinatorial problem to find an optimal reduction schedule for such heterogeneous clusters. Nevertheless, we show that a simple technique called slowest-node-first (SNF) is very effective in designing efficient reduction protocols for heterogeneous clusters. First, we show that SNF is actually an approximation algorithm with competitive ratio two. In addition, we show that SNF does give the optimal reduction time when the cluster consists of two types of processors, anal the ratio of communication speed between them is at least two.",
Interoperability with distributed objects through Java wrapper,"The major hurdle in developing distributed systems is implementing the interoperability between systems. Currently, most of the interoperability techniques require that the data or services be tightly coupled to a particular server. Furthermore, as most programmers are trained in designing stand-alone application, developing a distributed system proves to be time-consuming and difficult. The paper address the issues by creating an interface wrapper model that allows developers the feature of treating distributed objects as local objects. A tool was developed to generate a Java interface wrapper from a specification language called the Prototyping System Description Language.",
Performance analysis and visualization of parallel systems using SimOS and Rivet: a case study,"Presents an evolving system for the analysis and visualization of parallel application performance on shared memory multiprocessors. Our system couples SimOS, a complete machine simulator, with Rivet, a powerful visualization environment. This system demonstrates how visualization is necessary to realize the full power of simulation for performance analysis. We identify several features required of the visualization system, including flexibility, exploratory interaction techniques and data aggregation schemes. We demonstrate the effectiveness of this parallel analysis and visualization system with a case study. We developed two visualizations within Rivet to study the Argus parallel rendering library, focusing on the memory system and process scheduling activity of Argus, respectively. Using these visualizations, we uncovered several unexpected interactions between Argus and the underlying operating system. The results of the analysis led to changes that greatly improved its performance and scalability. Argus had previously been unable to scale beyond 26 processors; after analysis and modification, it achieved linear speedup up to 45 processors.",
Study of unstable limit cycles in power system models,"Unstable limit cycles (ULCs) provide a mechanism for nonlinear oscillatory phenomena in power systems. When ULC is located on the stability boundary, transient stability is determined by whether the post-fault initial condition is inside or outside the stable manifold of the ULC. Therefore, depending on its size, ULC can have a crippling effect on transient stability properties. In this paper, we present examples of ULCs in several power system models including a large system example.",
A symbolic representation for 3D object feature detection,"We define a spatial symbolic model that can be used to describe classes of 3D objects (anatomical and man-made) and a method for finding correspondences between the features of the symbolic models and point sets of 3D mesh data. An abstract symbolic model is used to describe spatial object classes in terms of parts, boundaries, and spatial associations. A working model is a mechanism to link the symbolic model to geometric information found in a sensed instance of the class, represented by a 3D mesh data set. Matching is performed in a three-step procedure that first finds working sets of points in the mesh, then fits constructed features to these sets, and finally selects a subset of these constructed features that best correspond to the features of the working model.",
"AVR theory, techniques and application","AVR (from actual reality to virtual reality) is a development of VR technique. In AVR, the objects in real world are virtualized directly from their shape information based on computer graphics and computer vision techniques, which provides the virtual environment with a higher level realism and preciseness. The basic theory and core techniques of AVR have been expounded in this paper. In order to achieve the integration of computer vision and computer graphics, superquadric-based model (SQ-based model) as the universal description of virtual objects is chosen and PAMVISION model providing a general framework for vision recognition and reconstruction in AVR modeling is developed. As the core techniques of AVR, two approaches for virtualizing the real objects from either 2D images or 3D data are implemented.",
Structuring reactive systems in B AMN,"B has been widely used for high-integrity systems development, for example in the railway industry. However, there are few published guidelines on how to structure B specifications for particular types of system, such as reactive control systems. In this paper, we describe a method to support the graphical design of systems using the B abstract machine notation (AMN), and we develop guidelines for expressing the structuring requirements of reactive systems in B.",
LACE frameworks and technique-identifying the legacy status of a business information system from the perspectives of its causes and effects,"This paper first presents a definition of the concept 'legacy status' with a three-dimensional model. It then discusses LACE frameworks and techniques, which can be used to assess legacy status from the cause and effects perspectives. A method of applying the LACE frameworks is shown and a technique with a mathematical model and metric so that the legacy status of a system can be calculated. This paper describes a novel and practical way to identify legacy status of a system, and has pointed out a new direction for research in this area.",
Navigation-dependent visualization of distributed Internet structures,"One of the main aspects of the Internet is that it is a distributed, multimedia database offering a fast growing amount of valuable information. Searching this database can be done by posing questions to search machines like Alta Vista. On the other hand, it is possible to browse through the space of connected information objects. When browsing, the problem arises that the user usually has no special knowledge of the structure of the visited hypertext area he is looking at. It may happen that he loses orientation and gets 'lost in hyperspace'. This article presents a software architecture that provides a visitor to a limited, distributed Internet area with additional structure information depending on his individual navigation path. Knowledge about the local hypermedia area is made available by a server program and is passed on to a visualization tool working in a frame of the client's Internet browser. This tool is loaded automatically from the server as a Java applet. By communication with the server, the visualization tool remains active throughout the navigation process and can be updated automatically.",
Analyzing reconfigurable algorithms for managing replicated data with strict consistency requirements: a case study,"We address reconfigurable algorithms for managing replicated data with strict consistency requirements, that is, whenever the user performs an update operation, the update is applied to all reachable copies as part of the update protocol. A key issue of designing such algorithms is to determine how often one should detect and react to failure conditions so that reorganization operations can be performed at the appropriate time to improve the availability of replicated data. We use dynamic voting as a case study to illustrate how often such failure detection and reconfiguration activities should be performed so as to maximize the data availability. We show that there exists an optimal period at which the failure detection and reconfiguration activities should be performed to optimize the system availability. Stochastic Petri nets (SPNs) are used as a tool to facilitate our analysis.",
Development of computer system of the collecting and processing of the technical and economic information,"Discusses the development of a computer system for collecting and processing technical and economic information. The components of the system, the functions of its separate parts, its areas of application, and the necessities that led to the creation of the system are considered. The universality of the computer system is shown. Its integrated function chart is given. The question of forecasting is considered and a schematic of the forecasting process is given.",
MPEG-4 support to multiuser virtual environments,"The few existing virtual reality distributed systems integrated to multimedia suffer from technical limitations on the number of users supported and on the presentation quality of the scenes. These limitations are due, among other things, to the slow progress of the VRML language to deal with multimedia integration. Today, the MPEG-4 standard is one of the most attractive alternative technologies to the development of multiuser virtual environments integrated to multimedia. The paper evaluates the current status of multimedia integration to virtual environments in the WWW and proposes a tool called SVRT-MM (Shared Virtual Reality Tool Integrated with Multimedia) that extends an existing MPEG-4 player to support multiuser. The article intends to be a contribution to the ongoing developments of the MPEG-4 standard related to multiuser support in 3D environments.",
Socratenon-a Web-based training system with an intellect,"The main goal of the project called Socratenon was to build a new Web-based training environment that would go beyond traditional ones. In the open literature, there are several solutions trying to accomplish the same to a certain degree. Some of them are nothing more but plain virtual textbooks that only flip pages on mouse-clicks. More sophisticated techniques include user modeling in order to personalize the content for the user, adaptive interfaces, intelligent agents for improved assistance and search, neural networks and case-based reasoning for building intelligent back-ends, etc. In general, many existing learning environments lack interaction, full utilization of Web resources is scarce, while solutions utilizing a combination of all above are practically non-existent or in development. This paper tries to merge the potential of the new Internet technologies and the latest developments in cognitive sciences, on the one hand, with the comfort of learning at the most suitable time and in the most suitable place, on the other hand. The project has been finalized, the package works, and its performance has been tested both objectively and subjectively; it demonstrates superiority over similar solutions from the open literature. Its complexity is such that it can fit even the widespread PC platforms, although it demonstrates the best performance on state-of-the-art corporate platforms.",
A Web-based computer architecture course database,"A Web-based database of course materials in computer architecture is being developed. Its goal is to allow instructors at different institutions to share independently developed materials and to develop new materials jointly. This database comprises problems and lectures downloaded (with permission) from the Websites of courses in computer architecture at universities around the world. The site is searchable by classification or fulltext string for problems on particular topics. Instructors can obtain accounts that will allow them to browse the database content and loan their questions directly. A Java application is being developed to further automate the process. This project has been developed in conjunction with the WebAssign project for on-line homework submission and grading. Where the format permits, homework and test problems can be automatically graded. Although this prototype is specific to the field of computer architecture, the software for building the database is usable for constructing databases in almost any academic field.","Computer architecture,
Databases,
Joining materials,
Permission,
Educational institutions,
Java,
Application software,
Automatic testing,
Software prototyping,
Buildings"
Register allocation for common subexpressions in DSP data paths,"This paper presents a new code optimization technique for DSPs with irregular data path structures. We consider the problem of generating machine code for data flow graphs with common subexpressions (CSEs). While in previous work CSEs are supposed to be strictly stored in memory, the technique proposed in this paper also permits the allocation of special purpose registers for temporarily storing CSEs. As a result, both the code size and the number of memory accesses are reduced. The optimization is controlled by a simulated annealing algorithm. We demonstrate its effectiveness for several DSP applications and a widespread DSP processor.",
Motion detection from temporally integrated images,"Motion blur arises when motion is fast relative to the shutter time of a camera. Unlike most work on motion blur, which considers the streaks due to motion blur to be noisy artifacts. In this paper we introduce a new method to extract motion information from these streaks. Previous methods with similar goals first extract an optic flow field from local information in the motion streaks and then infer global motion parameters. On the contrary, we adopt a more direct feature-based approach and extract global motion parameters from the motion streaks. We first extract edges in the motion blurred images, which we then group to determine the foci of expansion, the center of rotation, or motion parallel to the image plane. Furthermore, we determine the direction of motion. We present results on real images from a mobile robot in cluttered environments.",
Barriers to actualizing organizational memories lessons from industry,"Organizational memories have been routed recently for their potential to enhance decision-making processes within organizations. Several organizations have acknowledged the importance of these knowledge structures and developed new organizational roles to manage them. Though a number of companies have been successful in creating and maintaining functional organizational memory, data collected from four organizations suggests that there are inherent barriers within organizations that constrain both the development and maintenance of these repositories. This paper focuses on identifying the concepts underlying the adoption process and potential barriers that might have a negative short term impact on usability or a long term effect on the institutionalization of Organizational Memory Systems.",
Framework of integrating 2D points and curves for tracking of 3D non-rigid motion and structure,"We present a method for 3D non-rigid motion tracking and structure reconstruction from 2D points and curve segments from a sequence of perspective images. The 3D locations of features in the first frame are known. The 3D affine motion model is used to describe the nonrigid motion. The results from synthetic and real data are presented. The applications include: lip tracking, MPEG4 face player, and burn scar assessment. The results show that: 1) curve segments are more robust under noise (observed from synthetic data with different Gaussian noise level); and 2) using both feature yields a significant performance gain in real data.",
A general framework for symbol and rule extraction in neural networks,"We split the rule extraction task into a subsymbolic and a symbolic phase and present a set of neural networks for filling the former. Under the two general commitments of: (i) having a learning algorithm that is sensitive to feedback signals coming from the latter phase, and (ii) extracting Boolean variables whose meaning is determined by the further symbolic processing, we introduce three unsupervised learning algorithms and show related numerical examples for a multilayer perceptron, recurrent neural networks, and a specially devised vector quantizer.","Intelligent networks,
Neural networks,
Biological neural networks,
Data mining,
Artificial intelligence,
Artificial neural networks,
Computer science,
Mathematics,
Educational institutions,
Filling"
Verification of a wireless ATM medium-access protocol,"We report on a model checking case study of an industrial medium access protocol for wireless ATM. Since the protocol is too large to be verified by any of the existing checkers as a whole, the verification exploits the layered and modular structure of the protocol's SDL specification and proceeds in a bottom-up, compositional way. The compositional arguments are used in combination with abstraction techniques to further reduce the state space of the system. The verification is primarily aimed at debugging the system. After correcting the specification step by step and validating various untimed and time-dependent properties, a model of the whole control component of the medium-access protocol is built and verified. The significance of the case study is in demonstrating that verification tools can handle complex properties of a model as large as shown.",
On the global attractivity of a class of switching systems,"We investigate the stability properties of a class of switching systems of the form x/spl dot/=A/sub i/x, A/sub i//spl isin/IR/sup n/spl times/n/, A/sub i//spl isin//spl Ascr//spl Delta/=A{A/sub 1/, ..., A/sub m/}. We consider sets of matrices /spl Ascr/, where no single matrix T exists that simultaneously transforms each A/sub i//spl isin/ /spl Ascr/ to upper triangular form, but where a set of nonsingular matrices T/sub ij/ exist such that the matrices T/sub ij/A/sub i/T/sub ij//sup -1/, T/sub ij/A/sub j/T/sub ij//sup -1/, are upper triangular. We show that, for a special class of such systems, the origin of the switching system is globally attractive.",
Coating and dicing of wafer applied underfills for low-cost flip chip processing,The following topics were dealt with: teamwork; system prototyping; technical communication education; distance teaching; user advocates; online document editing; Internet and Web design; globalization; professional communication and learning; process integration; process management; testing; online-based instruction; design collaboration; wordcraft; literate programming; documentation tools; and information migration,
Interactive stereoscopic rendering of voxel-based terrain,"Presents an interactive stereoscopic rendering algorithm of voxel-based terrain. It provides unambiguous depth information of a terrain scene by generating perspective images for a pair of eyes with a horizontal parallax. The left-eye image is generated using a fast ray-casting algorithm accelerated by exploiting a specific ray coherence method in the voxel-based terrain scene. The right-eye image is obtained by exploiting the frame coherence between the two views. Most of the pixel values are directly obtained from the left image by re-projection. The remaining pixels are computed by ray casting, which is further accelerated with ray coherence. An A-buffer is employed to reduce the image error caused by re-projection to non-integer pixel locations. Image-based task partitioning schemes are explored to effectively parallelize our algorithm on a multiprocessor.",
Hysteresis loops and transition shapes during recording,"Hysteresis loop shapes corresponding to particular recorded transition shapes are calculated self-consistently. It is found that the closer a ramp function comes to the shape of the transition, the squarer the hysteresis loop that is needed.",
Neural network face recognition using statistical feature extraction,"Recognition method of human face using statistical analysis feature extraction and a neural network algorithm is proposed. In the preprocessing step we detect the edges of the face image by using the Sobel algorithm. Then we propose a new method to transform the two-dimension black and white image to a one-dimension vector. Finally, based on the statistical analysis, we extract seven features. In the recognition step we use the fast backpropagation (FBP) algorithm. Computer simulation results with 100 test images of 10 persons (the images of each person in a various pauses, facial expression, and facial details) show that the proposed method yields a high recognition rate.",
Resolution of quadratic assignment problems using an evolutionary algorithm,"This investigation presents evolution strategies to solve quadratic assignment problems. The proposed algorithm applies family competition and clustering to enhance the solution quality. Several problems obtained from QAPLIB (Burkard et al., 1991) were tested and experimental results were compared with other approaches in the literature. Experimental results also show that the proposed algorithm is promising.",
Study of Hopf bifurcations in a simple power system model,"This paper studies the occurrence of Hopf bifurcations in a simple single-machine-infinite-bus power system model. Our interest is primarily on the subcritical or supercritical nature of the Hopf bifurcations under variations in operating parameters. We observe that the Hopf bifurcations are mostly subcritical. When the exciter control is a fast high gain control and when the Thevenin equivalent transmission line impedance is high, the Hopf bifurcation can become supercritical leading to stable limit cycles. Two degenerate bifurcations, namely, one related to double zero eigenvalues, and the other related to Hopf normal form efficient becoming zero, give rise to homoclinic orbits and nested limit cycles respectively.",
Networked virtual environments for the Web: the WebTalk-I and WebTalk-II architectures,"Networked virtual environments (Net-VEs) are a class of applications which allow for cooperation between multiple users within a 3D representation of an environment. We have developed a general platform-webtalk-i-which allows the creation of any of these environments with easy accessibility via the Web, and fully provides for third-party authoring. After several experiments with different applications deployed using this technique (among these, the Italian National Science Museum), we analyzed user response and usage data, in order to design a new architecture-WebTalk-II-for an improved class of Net-VEs. WebTalk-II is able to generate 3D worlds dynamically based on a database with all major 3D formats, defines different sets of cooperation metaphors within users, and is entirely written in Java. We describe both architectures, assessing their flaws and advantages.",
An efficient technique for summarizing videos using visual contents,"We can summarize a video using its 'important' and/or 'interesting' scenes. The scenes selected, however, depend on the purpose of the summary. A good summarizing technique should be able to support various summarization needs. In this paper, we introduce a technique which allows the user to choose a few scene as important according to the application. Based on these selections, our algorithm automatically uncovers the remaining important scenes in the video. In this method, each scene as represented by a couple of numerical values. Since the processing is based on these numbers, it is highly efficient. Our experimental results indicate that this approach performs accurately, and is suitable for large video archives.",
WebSail: from on-line learning to Web search,"We investigate the applicability of on-line learning algorithms to the real-world problem of Web search. Consider that Web documents are indexed using n Boolean features. We first present a practically efficient online learning algorithm TW2 to search for Web documents represented by a disjunction of at most k relevant features. We then design and implement WebSail, a real-time adaptive Web search learner, with TW2 as its learning component. WebSail learns from the user's relevance feedback in real-time and helps the user to search for the desired Web documents. The architecture and performance of WebSail are also discussed.",
Memory test time reduction by interconnecting test items,"The idea is to interconnect test items to reuse memory states left from the previous test item for saving initialization and verification sequences. Meanwhile, signal settling time of the tester between two consecutive test items being applied can also be minimized since all test items are connected together into a continuous one. The interconnection problem is transformed to the Rural Chinese Postman (RCP) problem. The RCP problem is a famous NP-hard problem, one way to solve the RCP problem is by modeling as an integer linear programming (ILP) model. However, in the worst case, it will incur an exponential number of constraints; therefore, it is not suitable for practical usage. Instead of putting all constraints at once, we generate and solve a number of successive ILP models with the smaller number of constraints. The total numbers of iterations and constraints applied to solve ILP models are analyzed and compared.",
A normalized color difference edge detector based on quaternion representation,"This paper presents the quaternion color difference edge detector, a new approach to detection of edges in color images. Based on a new type of convolution, the color difference subspace and the proposed edge detector are expressed analytically by using the algebra of the quaternion. The proposed color image edge detector generates edges only where sharp changes of color occur in the original image. The experimental results have verified the improved performance of the new edge detector compared to some well known methods.",
On supporting weakly-connected browsing in a mobile Web environment,"A mobile environment is weakly-connected, characterized by low communication bandwidth and poor connectivity. The conventional paradigm for surfing mobile Web documents is ineffective since portions of a document could be corrupted during transmission and it is expensive to re-transmit the whole document. It is important that the high content-bearing portions should be transmitted successfully so that a mobile client could at least obtain a high level content and determine if the corrupted portions need to be retransmitted. We have proposed a multi-resolution transmission paradigm which allows higher content-bearing portions of a Web document to be transmitted, by partitioning it into multiple organizational units and associating an information content with each unit. The client can explore the higher content-bearing portion earlier and terminate browsing an irrelevant document sooner. We extend our previous work and propose a fault-tolerant multi-resolution transmission scheme which allows units of higher information content to be recovered from transmission error. The client can obtain an overall content of a Web document and either terminate the transmission of the remaining portions or decide if the corrupted portions need to be retransmitted. We demonstrate its feasibility with a prototype and with simulation results.",
Freshman year learning communities in a computer engineering program,"Learning Communities, a growing initiative at Iowa State University, aid freshmen in the transition to college life as students live in the same residence hall and attend a common block of classes. By combining learning communities with the concept of student-centered active learning, students will gain control of and adjust more quickly to their new environment, experience increased achievement, and persist in the program. First year computer engineering students involved in the learning community participated in two new courses during the 1999/2000 academic year. The new courses were framed within the context of active learning to better prepare students for continuation in computer engineering by increasing their skills in group work and providing essential life-long learning skills. Students completed their freshman year with a greater awareness of complete, engineering, knowledge and skills for successful teamwork, and experience a quicker and more satisfying acclimation to the university and college life. The goal of project SUCCESS is 'to provide every student interested in Computer Engineering an opportunity, to succeed Iowa State University and to prepare him or her for their future careers."" Engineering students take an academically challenging program of study beginning with rigorous courses in calculus, chemistry, and physics during their freshman year. While these courses are essential for providing a foundation upon which all engineering programs are based, many students find the courses difficult and elect not to continue their study of engineering. Project SUCCESS is an effort to help students survive the demands placed upon them as freshmen by providing a collaborative environment in which they will learn to seek assistance from and provide support to peers.",
3D simulations of interacting particles,"At the Institute for Science Education, a simulation program named xyZET has been developed and combined with an introductory course in mechanics to demonstrate its usefulness in teaching and learning physics. One of its distinct features enables the visualization of the movement of interacting particles in 3D. It adds value to teaching and learning complex processes, where visualization can help reduce cognitive load. The program can be used during lectures, where a simulation can often speak for itself in place of verbal explanation. The program also supports the development of exercise material for individualized learning. Such material in the form of Web pages can be combined with xyZET simulations, controlled by applets. Its basic features are discussed.",
Back and forth between guarded and modal logics,"Guarded fixed point logic /spl mu/GF extends the guarded fragment by means of least and greatest fixed points, and thus plays the same role within the domain of guarded logics as the modal /spl mu/-calculus plays within the modal domain. We provide a semantic characterisation of /spl mu/GF within an appropriate fragment of second-order logic, in terms of invariance under guarded bisimulation. The corresponding characterisation of the modal /spl mu/-calculus, due to D. Janin and I. Walukiewicz (1999), is lifted from the modal to the guarded domain by means of model theoretic translations. At the methodological level, these translations make the intuitive analogy between modal and guarded logics available as a tool in the analysis of the guarded domain.",
Web interface-driven cooperative exception handling in ADOME workflow management system,"Exception handling in workflow management systems (WFMSs) is a very important problem since it is not possible to specify all possible outcomes and alternatives. On the other hand, cooperative support for user-driven computer supported resolution of unexpected exceptions and workflow evolution at run-time is vital for an adaptive WFMS. We have been developing ADOME-WFMS as a comprehensive framework in which the problem of workflow exception handling can be adequately addressed. We present an adaptive exception manager and its Web-based interface for ADOME-WFMS with procedures for supporting the following: effective management of problem solving agents, cooperative exception handling, user-driven computer supported resolution of unexpected exceptions, and workflow evolution.",
A Model for Fast Analog Computation Based on Unreliable Synapses,"We investigate through theoretical analysis and computer simulations the consequences of unreliable synapses for fast analog computations in networks of spiking neurons, with analog variables encoded by the current firing activities of pools of spiking neurons. Our results suggest a possible functional role for the well-established unreliability of synaptic transmission on the network level. We also investigate computations on time series and Hebbian learning in this context of space-rate coding in networks of spiking neurons with unreliable synapses.",
Superposing connectors,"The ability to construct architectural connectors in a systematic and controlled way has been argued to promote reuse and incremental development, e.g., as a way of superposing, a la carte, services like security, over a given communication protocol. Towards this goal, we present a notion of high-order connector, i.e., a connector that takes connectors as parameters, for superposing coordination mechanisms over the interactions that are handled by the connectors that are passed as actual arguments. The notion is developed over the language COMMUNITY that we have been using for formalising aspects of architectural design, and illustrated with examples inspired by the case study.",
A framework of automated reconstruction of microbial metabolic pathways,"Describes a framework for the automated reconstruction of metabolic pathways using information about orthologous and homologous gene groups derived by the automated comparison of whole genomes archived in GenBank. The method integrates automatically derived orthologs, orthologous and homologous gene groups (), the biochemical pathway template available in the Kegg database (), and enzyme information derived from the SwissProt enzyme database () and the Ligand database (). The technique is useful to identify refined metabolic pathways based on operons, and to derive the non-enzymatic genes within a group of enzymes. The technique has been illustrated by a comparison between the E. coli and B. subtilis genomes.",
Partitioned separable paraboloidal surrogate coordinate ascent algorithm for image restoration,"We introduce a new fast converging parallelizable algorithm for image restoration. This algorithm is based on paraboloidal surrogate functions to simplify the optimization problem and a concavity technique developed by De Pierro (1995) to simultaneously update a set of pixels. To obtain large step sizes which affect the convergence rate, we choose the paraboloidal surrogate functions that have small curvatures. The concavity technique is applied to separate pixels into partitioned sets so that parallel processors can be assigned to each set. The partitioned separable paraboloidal surrogates are maximized by using coordinate ascent (CA) algorithms. Unlike other existing algorithms such EM and CA algorithms, the proposed algorithm not only requires less time per iteration to converge, but is guaranteed to monotonically increase the objective function and intrinsically accommodates nonnegativity constraints as well.",
Acceleration of thresholding and labeling operations through geometric processing of gray-level images,Explores the utilization of adaptive triangular meshes representing gray-level images to accelerate basic image processing operations. In particular two algorithms for thresholding and labeling gray-level images represented with adaptive triangular meshes are described and evaluated. Experimental results with real gray-level images show that it is possible to perform faster by working in the 3D geometric domain than by sequentially processing all the pixels contained in the original images.,
Building hybrid knowledge representations from text,"A significant obstacle to the development of intelligent natural language processing systems is the lack of rich knowledge bases containing representations of world knowledge. For experimental systems it is common practice to construct small knowledge bases by hand; however, this approach does not scale well to large systems. An alternative is to attempt to extract the desired information from existing knowledge sources intended for human consumption; however, attempts to construct broad-coverage knowledge bases using in-depth analysis have met with limited success. In this paper we present some preliminary work on an alternative approach that involves using shallow processing techniques to build a hybrid knowledge representation that stores information in a partially analysed form.",
Cobol in an object-oriented world: a learning perspective,"Although recent Internet, Java, and OO trends threaten Cobol's dominance, industry will continue to need the language and its programmers for development as well as maintenance-especially once OO Cobol becomes an official standard. Thus, Cobol training remains a priority. By leveraging their existing knowledge, Cobol programmers can make a smooth transition to object oriented development and Java programming.",
Sequential order selection for real time signal processing,"In this paper, we address the problem of order selection in signal processing applications for the general case where the observations can have correlations in time and require sequential processing. We consider the finite normal mixtures (FNM) model as an example for which correct order selection is very important. We derive penalized partial likelihood as the information theoretic criterion for order selection for the general case where observations can have correlations. We propose a sequential order selection procedure and investigate its properties and give a number of examples in channel equalization showing the effectiveness of the approach.",
Three improvements to the BLASTP search of genome databases,"The BLASTP program is a search tool for databases of protein sequences that is widely used by biologists as a first step in investigating new genome sequences. BLASTP finds high-scoring local alignments (q/sub i/q/sub i+1/...q/sub i+k//spl par/s/sub j/s/sub j+1/...s/sub j+k/) without gaps between a query sequence q and sequences s in the database. The score of an alignment is the sum of the scores of individual alignments q/sub i+t//spl par/s/sub j+t/ between amino acids that make up the protein. These individual scores come from a scoring matrix modeling the rate of evolutionary mutation. Here we provide a detailed description of the original program and three separate optimisations to it. BLASTP consists of three steps, that we call neighbourhood construction, hit detection, and hit extension. The three optimisations target hit extension since it accounts for 93% of the execution time. The first optimisation alters the data representation of the query sequence and the related code for indexing the scoring matrix. The second optimisation performs extensions in step-sizes of two rather than one. The third optimisation forstalls the calling of the hit extension step in cases that are unlikely to lead to a high-scoring alignment. Individually the three optimisations show speed ups of 15%, 48%, and 63% respectively.",
Evolving rules from neural networks trained on continuous data,"Artificial neural networks (ANNs) are used extensively involving continuous data. However, their application in many domains is hampered because it is not clear how they partition continuous data for classification. The extraction of rules, therefore, from ANNs trained on continuous data is of great importance. The system described in this paper uses a genetic algorithm to generate input patterns which are presented to the network, and the output from the ANN is then used to calculate the fitness function for the algorithm. These patterns can contain null characters which represent a zero input to the ANN, and this allows the genetic algorithm to find patterns which can be converted into additive rules with few antecedent clauses. These antecedents provide information as to where and how the neural network has partitioned the continuous data and can be combined together to make rules. These rules compare favourably with the results of those generated by See5 (a decision tree-based data mining tool) when executed on a data set consisting of continuous attributes.",
Using technology and innovation to simulate daily life,"""The Sims"" is a synthetic doll's house embodied in a computer game. Besides topping sales charts and generating endless discussions among computer gamers, The Sims has received attention from various prestigious publications. Far more than a doll's house, The Sims evokes comparisons to a Greek myth in which you can play the deity, manipulating the lives of unaware humans. The trouble with this model-and what makes the game so intriguing - is that, just like mortals in Greek mythology, your ""Sims"" (simulated people) often frustrate your plans with their own autonomy. Moreover, the game demonstrates how game development can influence computer science. The Sims achieves its success through both advanced computing technology and an understanding of its users. It brings players into the game through a combination of great technology, a believable simulation environment, a superb user interface and a fun game design, which combine to make a computer game of everyday life that is more compelling than the reality it models.",
Efficient integration of compiler-directed cache coherence and data prefetching,"Cache coherence enforcement and memory latency reduction and hiding are very important and challenging problems in the design of large-scale distributed shared-memory (DSM) multiprocessors. We propose an integrated framework to solve these problems through a compiler-directed cache coherence scheme called the Cache Coherence with Data Prefetching (CCDP) scheme. The CCDP scheme enforces cache coherence by prefetching the potentially stale references in a parallel program. It also prefetches the nonstale references to hide their memory latencies. To optimize the performance of the CCDP scheme, some prefetch hardware support is provided to efficiently handle these two forms of data prefetching operations. We also developed the compiler techniques utilized by the CCDP scheme for stale reference detection, prefetch target analysis and prefetch scheduling. We evaluated the performance of the CCDP scheme via execution-driven simulations of several applications from the SPEC CFP95 and the Perfect benchmark suites. The simulation results show that the CCDP scheme provides significant performance improvements for the applications studied.","Prefetching,
Hardware,
Delay,
Random access memory,
Computer science,
Data engineering,
Design engineering,
Read only memory,
Large-scale systems,
Computer architecture"
Photonic band-gap (PBG) versus effective refractive index: a case study of dielectric nanocavities,"This paper answers the fundamental computational question: can one use the effective refractive index concept and analyze the performance of PBG structures? It is clarified that (a) when the lattice constant is of the order of the dielectric wavelength and the PBG structure operates inside the band-gap, the structure cannot be modeled using the effective refractive index and (b) for the frequencies outside the band-gap and where the lattice constant is small compared to the wavelength, the effective refractive index can almost model the PBG structure. In addition, it is shown that the PBG structure can properly localize the EM waves in a defect region, and a high Q nanocavity structure can be designed.",
Teaching hardware/software system codesign using CAD tools: a case study in image synthesis,"The teaching of electronic systems architecture covers a large domain from software to hardware aspects. Producing engineers who are experts in both areas seems impossible, but giving the students a second expertise is obviously important. The subject matter of this paper is to report a course developed for students in computer science to give them an expertise in digital electronic problems through the use of a heterogeneous architecture. The application supporting this teaching is image synthesis, partially programmed on transputer microprocessors and partially synthesized with two FPGAs. Thanks to this example, students use a large set of CAD tools and understand the advantages and drawbacks of cabled solutions versus programmed solutions. The developed application is finally validated on a board designed by others students. Beyond technical achievements, students also gain experience working in teams on a project with a module partitioning.",
