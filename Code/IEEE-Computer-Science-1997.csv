Title,Abstract,Keywords
Multimodality image registration by maximization of mutual information,"A new approach to the problem of multimodality medical image registration is proposed, using a basic concept from information theory, mutual information (MI), or relative entropy, as a new matching criterion. The method presented in this paper applies MI to measure the statistical dependence or information redundancy between the image intensities of corresponding voxels in both images, which is assumed to be maximal if the images are geometrically aligned. Maximization of MI is a very general and powerful criterion, because no assumptions are made regarding the nature of this dependence and no limiting constraints are imposed on the image content of the modalities involved. The accuracy of the MI criterion is validated for rigid body registration of computed tomography (CT), magnetic resonance (MR), and photon emission tomography (PET) images by comparison with the stereotactic registration solution, while robustness is evaluated with respect to implementation issues, such as interpolation and optimization, and image content, including partial overlap and image degradation. Our results demonstrate that subvoxel accuracy with respect to the stereotactic reference solution can be achieved completely automatically and without any prior segmentation, feature extraction, or other preprocessing steps which makes this method very well suited for clinical applications.",
Face recognition by elastic bunch graph matching,"We present a system for recognizing human faces from single images out of a large database containing one image per person. Faces are represented by labeled graphs, based on a Gabor wavelet transform. Image graphs of new faces are extracted by an elastic graph matching process and can be compared by a simple similarity function. The system differs from the preceding one (Lades et al., 1993) in three respects. Phase information is used for accurate node positioning. Object-adapted graphs are used to handle large rotations in depth. Image graph extraction is based on a novel data structure, the bunch graph, which is constructed from a small get of sample image graphs.",
A Fast Fixed-Point Algorithm for Independent Component Analysis,"We introduce a novel fast algorithm for independent component analysis, which can be used for blind source separation and feature extraction. We show how a neural network learning rule can be transformed into a fixedpoint iteration, which provides an algorithm that is very simple, does not depend on any user-defined parameters, and is fast to converge to the most accurate solution allowed by the data. The algorithm finds, one at a time, all nongaussian independent components, regardless of their probability distributions. The computations can be performed in either batch mode or a semiadaptive manner. The convergence of the algorithm is rigorously proved, and the convergence speed is shown to be cubic. Some comparisons to gradient-based algorithms are made, showing that the new algorithm is usually 10 to 100 times faster, sometimes giving the solution in just a few iterations.",
Evolutionary computation: comments on the history and current state,"Evolutionary computation has started to receive significant attention during the last decade, although the origins can be traced back to the late 1950's. This article surveys the history as well as the current state of this rapidly growing field. We describe the purpose, the general structure, and the working principles of different approaches, including genetic algorithms (GA) (with links to genetic programming (GP) and classifier systems (CS)), evolution strategies (ES), and evolutionary programming (EP) by analysis and comparison of their most important constituents (i.e. representations, variation operators, reproduction, and selection mechanism). Finally, we give a brief overview on the manifold of application domains, although this necessarily must remain incomplete.","Evolutionary computation,
History,
Genetic programming,
Genetic algorithms,
Algorithm design and analysis,
Computer science,
Power generation economics,
Performance gain,
Robustness,
Problem-solving"
A comparison of mechanisms for improving TCP performance over wireless links,"Reliable transport protocols such as TCP are tuned to perform well in traditional networks where packet losses occur mostly because of congestion. However, networks with wireless and other lossy links also suffer from significant losses due to bit errors and handoffs. TCP responds to all losses by invoking congestion control and avoidance algorithms, resulting in degraded end-to end performance in wireless and lossy systems. We compare several schemes designed to improve the performance of TCP in such networks. We classify these schemes into three broad categories: end-to-end protocols, where loss recovery is performed by the sender; link-layer protocols that provide local reliability; and split-connection protocols that break the end-to-end connection into two parts at the base station. We present the results of several experiments performed in both LAN and WAN environments, using throughput and goodput as the metrics for comparison. Our results show that a reliable link-layer protocol that is TCP-aware provides very good performance. Furthermore, it is possible to achieve good performance without splitting the end-to-end connection at the base station. We also demonstrate that selective acknowledgments and explicit loss notifications result in significant performance improvements.","Performance loss,
Throughput,
Computer network reliability,
Transport protocols,
Degradation,
Base stations,
Wireless networks,
Computer science,
Delay estimation,
Control systems"
"Context-based, adaptive, lossless image coding","We propose a context-based, adaptive, lossless image codec (CALIC). The codec obtains higher lossless compression of continuous-tone images than other lossless image coding techniques in the literature. This high coding efficiency is accomplished with relatively low time and space complexities. The CALIC puts heavy emphasis on image data modeling. A unique feature of the CALIC is the use of a large number of modeling contexts (states) to condition a nonlinear predictor and adapt the predictor to varying source statistics. The nonlinear predictor can correct itself via an error feedback mechanism by learning from its mistakes under a given context in the past. In this learning process, the CALIC estimates only the expectation of prediction errors conditioned on a large number of different contexts rather than estimating a large number of conditional error probabilities. The former estimation technique can afford a large number of modeling contexts without suffering from the context dilution problem of insufficient counting statistics as in the latter approach, nor from excessive memory use. The low time and space complexities are also attributed to efficient techniques for forming and quantizing modeling contexts.","Image coding,
Bit rate,
Context modeling,
Proposals,
Codecs,
Statistics,
Biomedical imaging,
Computer science,
ISO standards,
IEC standards"
Exact and approximate rebinning algorithms for 3-D PET data,"This paper presents two new rebinning algorithms for the reconstruction of three-dimensional (3-D) positron emission tomography (PET) data. A rebinning algorithm is one that first sorts the 3-D data into an ordinary two-dimensional (2-D) data set containing one sinogram for each transaxial slice to be reconstructed; the 3-D image is then recovered by applying to each slice a 2-D reconstruction method such as filtered-backprojection. This approach allows a significant speedup of 3-D reconstruction, which is particularly useful for applications involving dynamic acquisitions or whole-body imaging. The first new algorithm is obtained by discretizing an exact analytical inversion formula. The second algorithm, called the Fourier rebinning algorithm (FORE), is approximate but allows an efficient implementation based on taking 2-D Fourier transforms of the data. This second algorithm was implemented and applied to data acquired with the new generation of PET systems and also to simulated data for a scanner with an 18/spl deg/ axial aperture. The reconstructed images were compared to those obtained with the 3-D reprojection algorithm (3DRP) which is the standard ""exact"" 3-D filtered-backprojection method. Results demonstrate that FORE provides a reliable alternative to 3DRP, while at the same time achieving an order of magnitude reduction in processing time.",
MicroPET: a high resolution PET scanner for imaging small animals,"MicroPET is a high resolution positron emission tomography (PET) scanner designed for imaging small laboratory animals. It consists of a ring of 30 position-sensitive scintillation detectors, each with an 8/spl times/8 array of small lutetium oxyorthosilicate (LSO) crystals coupled via optical fibers to a multi-channel photomultiplier tube. The detectors have an intrinsic resolution averaging 1.68 mm, an energy resolution between 15 and 25% and 2.4 ns timing resolution at 511 keV. The detector ring diameter of microPET is 17.2 cm with an imaging field of view of 112 mm transaxially by 18 mm axially. The scanner has no septa and operates exclusively in 3D mode. Reconstructed image resolution 1 cm from the center of the scanner is 2.0 mm and virtually isotropic, yielding a volume resolution of 8 mm/sup 3/. For comparison, the volume resolution of state-of-the-art clinical PET systems is in the range of 50-75 mm/sup 3/. Initial images of phantoms have been acquired and are reported. A computer controlled bed is under construction and will incorporate a small wobble motion to improve spatial sampling. This is projected to further enhance spatial resolution. MicroPET is the first PET scanner to incorporate the new scintillator LSO and to our knowledge is the highest resolution multi-ring PET scanner currently in existence.","Image resolution,
Positron emission tomography,
High-resolution imaging,
Animals,
Optical imaging,
Energy resolution,
Spatial resolution,
Optical arrays,
Solid scintillation detectors,
Laboratories"
Registration of head volume images using implantable fiducial markers,"Describes an extrinsic-point-based, interactive image-guided neurosurgical system designed at Vanderbilt University, Nashville, TN, as part of a collaborative effort among the Departments of Neurological Surgery, Computer Science, and Biomedical Engineering. Multimodal image-to-image (II) and image-to-physical (IP) registration is accomplished using implantable markers. Physical space tracking is accomplished with optical triangulation. The authors investigate the theoretical accuracy of point-based registration using numerical simulations, the experimental accuracy of their system using data obtained with a phantom, and the clinical accuracy of their system using data acquired in a prospective clinical trial by 6 neurosurgeons at 4 medical centers from 158 patients undergoing craniotomies to respect cerebral lesions. The authors can determine the position of their markers with an error of approximately 0.4 mm in X-ray computed tomography (CT) and magnetic resonance (MR) images and 0.3 mm in physical space. The theoretical registration error using 4 such markers distributed around the head in a configuration that is clinically practical is approximately 0.5-0.6 mm. The mean CT-physical registration error for the: phantom experiments is 0.5 mm and for the clinical data obtained with rigid head fixation during scanning is 0.7 mm. The mean CT-MR registration error for the clinical data obtained without rigid head fixation during scanning is 1.4 mm, which is the highest mean error that the authors observed. These theoretical and experimental findings indicate that this system is an accurate navigational aid that can provide real-time feedback to the surgeon about anatomical structures encountered in the surgical field.",
Shape indexing using approximate nearest-neighbour search in high-dimensional spaces,"Shape indexing is a way of making rapid associations between features detected in an image and object models that could have produced them. When model databases are large, the use of high-dimensional features is critical, due to the improved level of discrimination they can provide. Unfortunately, finding the nearest neighbour to a query point rapidly becomes inefficient as the dimensionality of the feature space increases. Past indexing methods have used hash tables for hypothesis recovery, but only in low-dimensional situations. In this paper we show that a new variant of the k-d tree search algorithm makes indexing in higher-dimensional spaces practical. This Best Bin First, or BBF search is an approximate algorithm which finds the nearest neighbour for a large fraction of the queries, and a very close neighbour in the remaining cases. The technique has been integrated into a fully developed recognition system, which is able to detect complex objects in real, cluttered scenes in just a few seconds.","Shape,
Indexing,
Spatial databases,
Object detection,
Image databases,
Neural networks,
Tree data structures,
Computer science,
Computer vision,
Layout"
Statistical approach to segmentation of single-channel cerebral MR images,"A statistical model is presented that represents the distributions of major tissue classes in single-channel magnetic resonance (MR) cerebral images. Using the model, cerebral images are segmented into gray matter, white matter, and cerebrospinal fluid (CSF). The model accounts for random noise, magnetic field inhomogeneities, and biological variations of the tissues. Intensity measurements are modeled by a finite Gaussian mixture. Smoothness and piecewise contiguous nature of the tissue regions are modeled by a three-dimensional (3-D) Markov random field (MRF). A segmentation algorithm, based on the statistical model, approximately finds the maximum a posteriori (MAP) estimation of the segmentation and estimates the model parameters from the image data. The proposed scheme for segmentation is based on the iterative conditional modes (ICM) algorithm in which measurement model parameters are estimated using local information at each site, and the prior model parameters are estimated using the segmentation after each cycle of iterations. Application of the algorithm to a sample of clinical MR brain scans, comparisons of the algorithm with other statistical methods, and a validation study with a phantom are presented. The algorithm constitutes a significant step toward a complete data driven unsupervised approach to segmentation of MR images in the presence of the random noise and intensity inhomogeneities.","Image segmentation,
Biological system modeling,
Iterative algorithms,
Magnetic field measurement,
Parameter estimation,
Magnetic resonance,
Magnetic noise,
Biological tissues,
Markov random fields,
Statistical analysis"
A methodology for evaluation of boundary detection algorithms on medical images,"Image segmentation is the partition of an image into a set of nonoverlapping regions whose union is the entire image. The image is decomposed into meaningful parts which are uniform with respect to certain characteristics, such as gray level or texture. In this paper, we propose a methodology for evaluating medical image segmentation algorithms wherein the only information available is boundaries outlined by multiple expert observers. In this case, the results of the segmentation algorithm can be evaluated against the multiple observers' outlines. We have derived statistics to enable us to find whether the computer-generated boundaries agree with the observers' hand-outlined boundaries as much as the different observers agree with each other. We illustrate the use of this methodology by evaluating image segmentation algorithms on two different applications in ultrasound imaging. In the first application, we attempt to find the epicardial and endocardial boundaries from cardiac ultrasound images, and in the second application, our goal is to find the fetal skull and abdomen boundaries from prenatal ultrasound images.","Detection algorithms,
Biomedical imaging,
Image segmentation,
Ultrasonic imaging,
Gold,
Protocols,
Statistics,
Application software,
Partitioning algorithms,
Skull"
Harmonic broadcasting for video-on-demand service,"Using conventional broadcasting, if we want to support a 120-minute popular movie every 10 minutes, we need 12 video channels. Assuming the set-top box at the client end can buffer portions of the playing video on a disk, pyramid broadcasting schemes can reduce the bandwidth requirements to 5.7 channels. We present a new scheme which only needs 3.2 channels. For a movie with length D minutes, if we want to reduce the viewer waiting time to D/N minutes, we only need to allocate H(N) video channels to broadcast the movie periodically, where H(N) is the harmonic number of N, H(N)=1+1/2+...+1/N. In order to support video-on-demand service for a popular movie, the new scheme greatly reduces the bandwidth requirements.","Motion pictures,
Bandwidth,
Channel allocation,
Digital video broadcasting,
Computer science,
Tin"
Content-based image retrieval with relevance feedback in MARS,"Technology advances in the areas of image processing (IP) and information retrieval (IR) have evolved separately for a long time. However, successful content-based image retrieval systems require the integration of the two. There is an urgent need to develop integration mechanisms to link the image retrieval model to text retrieval model, such that the well established text retrieval techniques can be utilized. Approaches of converting image feature vectors (IF domain) to weighted-term vectors (IR domain) are proposed in this paper. Furthermore, the relevance feedback technique from the IR domain is used in content-based image retrieval to demonstrate the effectiveness of this conversion. Experimental results show that the image retrieval precision increases considerably by using the proposed integration approach.",
Grouping Web page references into transactions for mining World Wide Web browsing patterns,"Web-based organizations often generate and collect large volumes of data in their daily operations. Analyzing such data involves the discovery of meaningful relationships from a large collection of primarily unstructured data, often stored in Web server access logs. While traditional domains for data mining, such as point of sale databases, have naturally defined transactions, there is no convenient method of clustering web references into transactions. This paper identifies a model of user browsing behavior that separates web page references into those made for navigation purposes and those for information content purposes. A transaction identification method based on the browsing model is defined and successfully tested against other methods, such as the maximal forward reference algorithm proposed in (Chen et al., 1996). Transactions identified by the proposed methods are used to discover association rules from real world data using the WEBMINER system.","Web pages,
Web sites,
Data mining,
Web server,
Navigation,
Testing,
Association rules,
Data analysis,
Computer science,
Data engineering"
Web mining: information and pattern discovery on the World Wide Web,"Application of data mining techniques to the World Wide Web, referred to as Web mining, has been the focus of several recent research projects and papers. However, there is no established vocabulary, leading to confusion when comparing research efforts. The term Web mining has been used in two distinct ways. The first, called Web content mining in this paper, is the process of information discovery from sources across the World Wide Web. The second, called Web usage mining, is the process of mining for user browsing and access patterns. We define Web mining and present an overview of the various research issues, techniques, and development efforts. We briefly describe WEBMINER, a system for Web usage mining, and conclude the paper by listing research issues.","Web mining,
Web sites,
Taxonomy,
Intelligent agent,
Data mining,
Information resources,
Information analysis,
Search engines,
Computer science,
Data engineering"
Catadioptric omnidirectional camera,"Conventional video cameras have limited fields of view that make them restrictive in a variety of vision applications. There are several ways to enhance the field of view of an imaging system. However, the entire imaging system must have a single effective viewpoint to enable the generation of pure perspective images from a sensed image. A new camera with a hemispherical field of view is presented. Two such cameras can be placed back-to-back, without violating the single viewpoint constraint, to arrive at a truly omnidirectional sensor. Results are presented on the software generation of pure perspective images from an omnidirectional image, given any user-selected viewing direction and magnification. The paper concludes with a discussion on the spatial resolution of the proposed camera.",
A geometric snake model for segmentation of medical imagery,"We employ the new geometric active contour models, previously formulated, for edge detection and segmentation of magnetic resonance imaging (MRI), computed tomography (CT), and ultrasound medical imagery. Our method is based on defining feature-based metrics on a given image which in turn leads to a novel snake paradigm in which the feature of interest may be considered to lie at the bottom of a potential well. Thus, the snake is attracted very quickly and efficiently to the desired feature.","Solid modeling,
Image segmentation,
Biomedical imaging,
Active contours,
Image edge detection,
Magnetic resonance imaging,
Computed tomography,
Ultrasonic imaging,
Shape,
Heart"
Volumetric transformation of brain anatomy,"Presents diffeomorphic transformations of three-dimensional (3-D) anatomical image data of the macaque occipital lobe and whole brain cryosection imagery and of deep brain structures in human brains as imaged via magnetic resonance imagery. These transformations are generated in a hierarchical manner, accommodating both global and local anatomical detail. The initial low-dimensional registration is accomplished by constraining the transformation to be in a low-dimensional basis. The basis is defined by the Green's function of the elasticity operator placed at predefined locations in the anatomy and the eigenfunctions of the elasticity operator. The high-dimensional large deformations are vector fields generated via the mismatch between the template and target-image volumes constrained to be the solution of a Navier-Stokes fluid model. As part of this procedure, the Jacobian of the transformation is tracked, insuring the generation of diffeomorphisms. It is shown that transformations constrained by quadratic regularization methods such as the Laplacian, biharmonic, and linear elasticity models, do not ensure that the transformation maintains topology and, therefore, must only be used for coarse global registration.","Brain,
Anatomy,
Elasticity,
Humans,
Magnetic resonance,
Green's function methods,
Eigenvalues and eigenfunctions,
Deformable models,
Jacobian matrices,
Target tracking"
Markov random field segmentation of brain MR images,"Describes a fully-automatic three-dimensional (3-D)-segmentation technique for brain magnetic resonance (MR) images. By means of Markov random fields (MRF's) the segmentation algorithm captures three features that are of special importance for MR images, i.e., nonparametric distributions of tissue intensities, neighborhood correlations, and signal inhomogeneities. Detailed simulations and real MR images demonstrate the performance of the segmentation algorithm. In particular, the impact of noise, inhomogeneity, smoothing, and structure thickness are analyzed quantitatively. Even single-echo MR images are well classified into gray matter, white matter, cerebrospinal fluid, scalp-bone, and background. A simulated annealing and an iterated conditional modes implementation are presented.",
Geometric hashing: an overview,"Geometric hashing, a technique originally developed in computer vision for matching geometric features against a database of such features, finds use in a number of other areas. Matching is possible even when the recognizable database objects have undergone transformations or when only partial information is present. The technique is highly efficient and of low polynomial complexity.","Layout,
Spatial databases,
Robot sensing systems,
Object recognition,
Image recognition,
Image databases,
Robot vision systems,
Computer vision,
Cameras,
Indexing"
Aggregation operators for linguistic weighted information,"The aim of this paper is to model the processes of the aggregation of weighted information in a linguistic framework. Three aggregation operators of weighted linguistic information are presented: linguistic weighted disjunction operator, linguistic weighted conjunction operator, and linguistic weighted averaging operator. A study of their axiomatics is presented to demonstrate their rational aggregation.",
Normalized cuts and image segmentation,"We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images and found results very encouraging.","Image segmentation,
Brightness,
Clustering algorithms,
Computer science,
Data mining,
Eigenvalues and eigenfunctions,
Humans,
Clouds,
Layout,
Pixel"
Baring it all to software: Raw machines,"The most radical of the architectures that appear in this issue are Raw processors-highly parallel architectures with hundreds of very simple processors coupled to a small portion of the on-chip memory. Each processor, or tile, also contains a small bank of configurable logic, allowing synthesis of complex operations directly in configurable hardware. Unlike the others, this architecture does not use a traditional instruction set architecture. Instead, programs are compiled directly onto the Raw hardware, with all units told explicitly what to do by the compiler. The compiler even schedules most of the intertile communication. The real limitation to this architecture is the efficacy of the compiler. The authors demonstrate impressive speedups for simple algorithms that lend themselves well to this architectural model, but whether this architecture will be effective for future workloads is an open question.","Switches,
Registers,
Logic,
Computer architecture,
Hardware,
Tiles,
Application software,
Microprocessors,
Laboratories,
Computer science"
A measurement-based admission control algorithm for integrated service packet networks,"Many designs for integrated services networks offer a bounded delay packet delivery service to support real-time applications. To provide a bounded delay service, networks must use admission control to regulate their load. Previous work on admission control mainly focused on algorithms that compute the worst case theoretical queueing delay to guarantee an absolute delay bound for all packets. In this paper, we describe a measurement-based admission control algorithm (ACA) for predictive service, which allows occasional delay violations. We have tested our algorithm through simulations on a wide variety of network topologies and driven with various source models, including some that exhibit long-range dependence, both in themselves and in their aggregation. Our simulation results suggest that measurement-based approach combined with the relaxed service commitment of predictive service enables us to achieve a high level of network utilization while still reliably meeting the delay bound.","Admission control,
Intserv networks,
Traffic control,
Delay,
Communication system traffic control,
Telecommunication traffic,
Computer science,
Bandwidth,
Algorithm design and analysis,
Queueing analysis"
Adaptive evolutionary planner/navigator for mobile robots,"Based on evolutionary computation (EC) concepts, we developed an adaptive evolutionary planner/navigator (EP/N) as a novel approach to path planning and navigation. The EP/N is characterized by generality, flexibility, and adaptability. It unifies off-line planning and online planning/navigation processes in the same evolutionary algorithm which 1) accommodates different optimization criteria and changes in these criteria, 2) incorporates various types of problem-specific domain knowledge, and 3) enables good tradeoffs among near-optimality of paths, high planning efficiency, and effective handling of unknown obstacles. More importantly, the EP/N can self-tune its performance for different task environments and changes in such environments, mostly through adapting probabilities of its operators and adjusting paths constantly, even during a robot's motion toward the goal.","Navigation,
Mobile robots,
Evolutionary computation,
Path planning,
Computer science,
Process planning,
Uncertainty,
Associate members,
Motion planning"
The boundary shift integral: an accurate and robust measure of cerebral volume changes from registered repeat MRI,"We propose the boundary shift integral (BSI) as a measure of cerebral volume changes derived from registered repeat three-dimensional (3-D) magnetic resonance (MR) [3D MR] scans. The BSI determines the total volume through which the boundaries of a given cerebral structure have moved and, hence, the volume change, directly from voxel intensities. We found brain and ventricular BSI's correlated tightly (r=1.000 and r=0.999) with simulated volumes of change. Applied to 21 control scan pairs and 11 scan pairs from Alzheimer's disease (AD) patients (mean interval 386 days) the BSI yielded mean brain volume loss of 1.8 cc (controls) and 34.7 cc (AD); the control group was tightly bunched (SD=3.8 cc) and there was wide group separation, the group means differing by 8.7 control group standard deviations (SDs). A measure based on the same segmentation used by the BSI yielded similar group means, but wide spread in the control group (SD=13.4 cc) and group overlap, the group means differing by 2.8 control group SDs. The BSI yielded mean ventricular volume losses of 0.4 cc (controls) and 10.1 cc (AD). Good linear correlation (r=0.997) was obtained between the ventricular BSI and the difference in their segmented volumes. We conclude the BSI is an accurate and robust measure of regional and global cerebral volume changes.","Integral equations,
Robustness,
Volume measurement,
Alzheimer's disease,
Magnetic resonance imaging,
Magnetic resonance,
Loss measurement,
Drugs,
Dementia,
Hospitals"
The design of an asynchronous MIPS R3000 microprocessor,"The design of an asynchronous clone of a MIPS R3000 microprocessor is presented. In 0.6 /spl mu/m CMOS, we expect performance close to 280 MIPS, for a power consumption of 7 W. The paper describes the structure of a high-performance asynchronous pipeline, in particular precise exceptions, pipelined caches, arithmetic, and registers, and the circuit techniques developed to achieve high throughput.","Microprocessors,
Delay,
Asynchronous circuits,
Pipelines,
Throughput,
Robustness,
Energy consumption,
Low voltage,
Computer science,
CMOS technology"
A resource allocation model for QoS management,"Quality of service (QoS) has been receiving wide attention in many research communities including networking, multimedia systems, real-time systems and distributed systems. In large distributed systems such as those used in defense systems, on-demand service and inter-networked systems, applications contending for system resources must satisfy timing, reliability and security constraints as well as application-specific quality requirements. Allocating sufficient resources to different applications in order to satisfy various requirements is a fundamental problem in these situations. A basic yet flexible model for performance-driven resource allocations can therefore be useful in making appropriate tradeoffs. We present an analytical model for QoS management in systems which must satisfy application needs along multiple dimensions such as timeliness, reliable delivery schemes, cryptographic security and data quality. We refer to this model as Q-RAM (QoS-based Resource Allocation Model). The model assumes a system with multiple concurrent applications, each of which can operate at different levels of quality based on the system resources available to it. The goal of the model is to be able to allocate resources to the various applications such that the overall system utility is maximized under the constraint that each application can meet its minimum needs. We identify resource profiles of applications which allow such decisions to be made efficiently and in real-time. We also identify application utility functions along different dimensions which are composable to form unique application requirement profiles. We use a video-conferencing system to illustrate the model.","Resource management,
Quality of service,
Real time systems,
Radar tracking,
Bandwidth,
Cryptography,
Control systems,
High definition video,
Delay,
Computer science"
Multilevel Hypergraph Partitioning: Application In Vlsi Domain,,
Estimating the bias field of MR images,"The authors propose a modification of Wells et al. (ibid., vol. 15, no. 4, p. 429-42, 1996) technique for bias field estimation and segmentation of magnetic resonance (MR) images. They show that replacing the class other, which includes all tissue not modeled explicitly by Gaussians with small variance, by a uniform probability density, and amending the expectation-maximization (EM) algorithm appropriately, gives significantly better results. The authors next consider the estimation and filtering of high-frequency information in MR images, comprising noise, intertissue boundaries, and within tissue microstructures. The authors conclude that post-filtering is preferable to the prefiltering that has been proposed previously. The authors observe that the performance of any segmentation algorithm, in particular that of Wells et al. (and the authors' refinements of it) is affected substantially by the number and selection of the tissue classes that are modeled explicitly, the corresponding defining parameters and, critically, the spatial distribution of tissues in the image. The authors present an initial exploration to choose automatically the number of classes and the associated parameters that give the best output. This requires the authors to define what is meant by ""best output"" and for this they propose the application of minimum entropy. The methods developed have been implemented and are illustrated throughout on simulated and real data (brain and breast MR).","Image segmentation,
Magnetic resonance,
Gaussian processes,
Information filtering,
Information filters,
Magnetic separation,
Microstructure,
Entropy,
Brain modeling,
Breast"
MAX-MIN Ant System and local search for the traveling salesman problem,"Ant System is a general purpose algorithm inspired by the study of the behavior of ant colonies. It is based on a cooperative search paradigm that is applicable to the solution of combinatorial optimization problems. We introduce MAX-MIN Ant System, an improved version of basic Ant System, and report our results for its application to symmetric and asymmetric instances of the well known traveling salesman problem. We show how MAX-MIN Ant System can be significantly improved, extending it with local search heuristics. Our results clearly show that MAX-MIN Ant System has the property of effectively guiding the local search heuristics towards promising regions of the search space by generating good initial tours.","Cities and towns,
Traveling salesman problems,
Ant colony optimization,
Computer science,
Space exploration,
Shortest path problem,
Automatic testing,
System testing,
Probability distribution"
Scheduling real-time applications in an open environment,"This paper extends the two-level hierarchical scheme in (Deng et al., 1997) for scheduling independently developed real-time applications with non-real-time applications in an open environment. The environment allows the schedulability of each real-time application to be validated independently of other applications in the system. The extended scheme removes the following two restrictions of the scheme: real-time applications that are scheduled preemptively must consist solely of periodic tasks; and applications must not share global resources (i.e., resources used by more than one application). Consequently, the extended scheme can accommodate a much broader spectrum of real-time applications.","Processor scheduling,
Real time systems,
Application software,
Scheduling algorithm,
Timing,
Open systems,
Computer science,
Runtime,
NASA,
System testing"
Multiple sclerosis lesion quantification using fuzzy-connectedness principles,"Multiple sclerosis (MS) is a disease of the white matter. Magnetic resonance imaging (MRI) is proven to be a sensitive method of monitoring the progression of this disease and of its changes due to treatment protocols. Quantification of the severity of the disease through estimation of MS lesion volume via MR imaging is vital for understanding and monitoring the disease and its treatment. This paper presents a novel methodology and a system that can be routinely used for segmenting and estimating the volume of MS lesions via dual-echo fast spin-echo MR imagery. A recently developed concept of fuzzy objects forms the basis of this methodology. An operator indicates a few points in the images by pointing to the white matter, the grey matter, and the cerebrospinal fluid (CSF). Each of these objects is then detected as a fuzzy connected set. The holes in the union of these objects correspond to potential lesion sites which are utilized to detect each potential lesion as a three-dimensional (3-D) fuzzy connected object. These objects are presented to the operator who indicates acceptance/rejection through the click of a mouse button. The number and volume of accepted lesions is then computed and output. Based on several evaluation studies, the authors conclude that the methodology is highly reliable and consistent, with a coefficient of variation (due to subjective operator actions) of 0.9% (based on 20 patient studies, three operators, and two trials) for volume and a mean false-negative volume fraction of 1.3%, with a 95% confidence interval of 0%-2.8% (based on ten patient studies).","Multiple sclerosis,
Lesions,
Diseases,
Magnetic resonance imaging,
Monitoring,
Object detection,
Protocols,
Image segmentation,
Fuzzy sets,
Mice"
Creating connected representations of cortical gray matter for functional MRI visualization,"Describes a system that is being used to segment gray matter from magnetic resonance imaging (MRI) and to create connected cortical representations for functional MRI visualization (fMRI). The method exploits knowledge of the anatomy of the cortex and incorporates structural constraints into the segmentation. First, the white matter and cerebral spinal fluid (CSF) regions in the MR volume are segmented using a novel techniques of posterior anisotropic diffusion. Then, the user selects the cortical white matter component of interest, and its structure is verified by checking for cavities and handles. After this, a connected representation of the gray matter is created by a constrained growing-out from the white matter boundary. Because the connectivity is computed, the segmentation can be used as input to several methods of visualizing the spatial pattern of cortical activity within gray matter. In the authors' case, the connected representation of gray matter is used to create a flattened representation of the cortex. Then, fMRI measurements are overlaid on the flattened representation, yielding a representation of the volumetric data within a single image. The software is freely available to the research community.","Magnetic resonance imaging,
Image segmentation,
Humans,
Neurons,
Data visualization,
Neuroscience,
Nerve fibers,
Anatomy,
Anisotropic magnetoresistance,
Volume measurement"
Statistics of chaotic binary sequences,"Statistical properties of binary sequences generated by a class of ergodic maps with some symmetric properties are discussed on the basis of an ensemble-average technique. We give a simple sufficient condition for such a class of maps to produce a fair Bernoulli sequence, that is, a sequence of independent and identically distributed (i.i.d.) binary random variables. This condition is expressed in terms of binary function, which is a generalized version of the Rademacher function for the dyadic map.","Statistics,
Chaos,
Binary sequences,
Chaotic communication,
Random variables,
Sufficient conditions,
Application software,
Computer science,
Digital communication,
Extraterrestrial measurements"
Modeling the manifolds of images of handwritten digits,"This paper describes two new methods for modeling the manifolds of digitized images of handwritten digits. The models allow a priori information about the structure of the manifolds to be combined with empirical data. Accurate modeling of the manifolds allows digits to be discriminated using the relative probability densities under the alternative models. One of the methods is grounded in principal components analysis, the other in factor analysis. Both methods are based on locally linear low-dimensional approximations to the underlying data manifold. Links with other methods that model the manifold are discussed.",
Automated segmentation and classification of multispectral magnetic resonance images of brain using artificial neural networks,"Presents a fully automated process for segmentation and classification of multispectral magnetic resonance (MR) images. This hybrid neural network method uses a Kohonen self-organizing neural network for segmentation and a multilayer backpropagation neural network for classification. To separate different tissue types, this process uses the standard T1-, T2-, and PD-weighted MR images acquired in clinical examinations. Volumetric measurements of brain structures, relative to intracranial volume, were calculated for an index transverse section in 14 normal subjects (median age 25 years; 7 male, 7 female). This index slice was at the level of the basal ganglia, included both genu and splenium of the corpus callosum, and generally, showed the putamen and lateral ventricle. An intraclass correlation of this automated segmentation and classification of tissues with the accepted standard of radiologist identification for the index slice in the 14 volunteers demonstrated coefficients (r/sub i/) of 0.91, 0.95, and 0.98 for white matter, gray matter, and ventricular cerebrospinal fluid (CSF), respectively. An analysis of variance for estimates of brain parenchyma volumes in 5 volunteers imaged 5 times each demonstrated high intrasubject reproducibility with a significance of at least p<0.05 for white matter, gray matter, and white/gray partial volumes. The population variation, across 14 volunteers, demonstrated little deviation from the averages for gray and white matter, while partial volume classes exhibited a slightly higher degree of variability. This fully automated technique produces reliable and reproducible MR image segmentation and classification while eliminating intra- and interobserver variability.","Image segmentation,
Magnetic resonance,
Artificial neural networks,
Multi-layer neural network,
Biological neural networks,
Neural networks,
Magnetic multilayers,
Backpropagation,
Volume measurement,
Brain"
Automatic target recognition by matching oriented edge pixels,"This paper describes techniques to perform efficient and accurate target recognition in difficult domains. In order to accurately model small, irregularly shaped targets, the target objects and images are represented by their edge maps, with a local orientation associated with each edge pixel. Three dimensional objects are modeled by a set of two-dimensional (2-D) views of the object. Translation, rotation, and scaling of the views are allowed to approximate full three-dimensional (3-D) motion of the object. A version of the Hausdorff measure that incorporates both location and orientation information is used to determine which positions of each object model are reported as possible target locations. These positions are determined efficiently through the examination of a hierarchical cell decomposition of the transformation space. This allows large volumes of the space to be pruned quickly. Additional techniques are used to decrease the computation time required by the method when matching is performed against a catalog of object models. The probability that this measure will yield a false alarm and efficient methods for estimating this probability at run time are considered in detail. This information can be used to maintain a low false alarm rate or to rank competing hypotheses based on their likelihood of being a false alarm. Finally, results of the system recognizing objects in infrared and intensity images are given.",
Surface interpolation with radial basis functions for medical imaging,"Radial basis functions are presented as a practical solution to the problem of interpolating incomplete surfaces derived from three-dimensional (3-D) medical graphics. The specific application considered is the design of cranial implants for the repair of defects, usually holes, in the skull. Radial basis functions impose few restrictions on the geometry of the interpolation centers and are suited to problems where the Interpolation centers do not form a regular grid. However, their high computational requirements have previously limited their use to problems where the number of interpolation centers is small (<300). Recently developed fast evaluation techniques have overcome these limitations and made radial basis interpolation a practical approach for larger data sets. In this paper radial basis functions are fitted to depth-maps of the skull's surface, obtained from X-ray computed tomography (CT) data using ray-tracing techniques. They are used to smoothly interpolate the surface of the skull across defect regions. The resulting mathematical description of the skull's surface can be evaluated at any desired resolution to be rendered on a graphics workstation or to generate instructions for operating a computer numerically controlled (CNC) mill.",
Surface-based labeling of cortical anatomy using a deformable atlas,"The authors describe a computerized method to automatically find and label the cortical surface in three-dimensional (3-D) magnetic resonance (MR) brain images. The approach the authors take is to model a prelabeled brain atlas as a physical object and give it elastic properties, allowing it to warp itself onto regions in a preprocessed image. Preprocessing consists of boundary-finding and a morphological procedure which automatically extracts the brain and sulci from an MR image and provides a smoothed representation of the brain surface to which the deformable model can rapidly converge. The authors' deformable models are energy-minimizing elastic surfaces that can accurately locate image features. The models are parameterized with 3-D bicubic B-spline surfaces. The authors design the energy function such that cortical fissure (sulci) points on the model are attracted to fissure points on the image and the remaining model points are attracted to the brain surface. A conjugate gradient method minimizes the energy function, allowing the model to automatically converge to the smoothed brain surface. Finally, labels are propagated from the deformed atlas onto the high-resolution brain surface.",
Measures of acutance and shape for classification of breast tumors,"Most benign breast tumors possess well-defined, sharp boundaries that delineate them from surrounding tissues, as opposed to malignant tumors. Computer techniques proposed to date for tumor analysis have concentrated on shape factors of tumor regions and texture measures. While shape measures based on contours of tumor regions can indicate differences in shape complexities between circumscribed and spiculated tumors, they are not designed to characterize the density variations across the boundary of a tumor. Here, the authors propose a region-based measure of image edge profile acutance which characterizes the transition in density of a region of interest (ROI) along normals to the ROI at every boundary pixel. The authors investigate the potential of acutance in quantifying the sharpness of the boundaries of tumors, and propose its application to discriminate between benign and malignant mammographic tumors. In addition, they study the complementary use of various shape factors based upon the shape of the ROI, such as compactness. Fourier descriptors, moments, and chord-length statistics to distinguish between circumscribed and spiculated tumors. Thirty-nine images from the Mammographic Image Analysis Society (MIAS) database and an additional set of 15 local cases were selected for this study. The cases included 16 circumscribed benign, 7 circumscribed malignant, 12 spiculated benign, and 19 spiculated malignant lesions. All diagnoses were proven by pathologic examinations of resected tissue. The contours of the lesions were first marked by an expert radiologist using X-Paint and X-Windows on a SUN-SPARCstation 2 Workstation. For computation of acutance, the ROI boundaries were iteratively approximated using a split/merge and end-point adjustment technique to obtain the best-fitting polygonal approximation. The jackknife method using the Mahalanobis distance measure in the BMDP (Biomedical Programs) package was used for classification of the lesions using acutance and the shape factors as features in various combinations. Acutance alone resulted in a benign/malignant classification accuracy of 95% the MIAS cases. Compactness alone gave a circumscribed/spiculated classification rate of 92.3% with the MIAS cases. Acutance in combination with a moment-based shape measure and a Fourier descriptor-based measure gave four-group classification rate of 95% with the MIAS cases. The results indicate the importance of including lesion edge definition with shape information for classification of tumors, and that the proposed measure of acutance fills this need.","Shape measurement,
Breast tumors,
Biomedical measurements,
Cancer,
Lesions,
Breast neoplasms,
Density measurement,
Malignant tumors,
Pixel,
Statistics"
Method for segmenting chest CT image data using an anatomical model: preliminary results,"Presents an automated, knowledge-based method for segmenting chest computed tomography (CT) datasets. Anatomical knowledge including expected volume, shape, relative position, and X-ray attenuation of organs provides feature constraints that guide the segmentation process. Knowledge is represented at a high level using an explicit anatomical model. The model is stored in a frame-based semantic network and anatomical variability is incorporated using fuzzy sets. A blackboard architecture permits the data representation and processing algorithms in the model domain to be independent of those in the image domain. Knowledge-constrained segmentation routines extract contiguous three-dimensional (3-D) sets of voxels, and their feature-space representations are posted on the blackboard. An inference engine uses fuzzy logic to match image to model objects based on the feature constraints. Strict separation of model and image domains allows for systematic extension of the knowledge base. In preliminary experiments, the method has been applied to a small number of thoracic CT datasets. Based on subjective visual assessment by experienced thoracic radiologists, basic anatomic structures such as the lungs, central tracheobronchial tree, chest wall, and mediastinum were successfully segmented. To demonstrate the extensibility of the system, knowledge was added to represent the more complex anatomy of lung lesions in contact with vessels or the chest wall. Visual inspection of these segmented lesions was also favorable. These preliminary results suggest that use of expert knowledge provides an increased level of automation compared with low-level segmentation techniques. Moreover, the knowledge-based approach may better discriminate between structures of similar attenuation and anatomic contiguity. Further validation is required.","Image segmentation,
Computed tomography,
X-ray imaging,
Attenuation,
Lungs,
Lesions,
Shape,
Fuzzy sets,
Inference algorithms,
Data mining"
Feature selection via discretization,"Discretization can turn numeric attributes into discrete ones. Feature selection can eliminate some irrelevant and/or redundant attributes. Chi2 is a simple and general algorithm that uses the /spl chi//sup 2/ statistic to discretize numeric attributes repeatedly until some inconsistencies are found in the data. It achieves feature selection via discretization. It can handle mixed attributes, work with multiclass data, and remove irrelevant and redundant attributes.","Classification algorithms,
Statistics,
Pattern classification,
Accuracy,
Remuneration,
Training data,
Merging,
Information systems,
Computer science"
A concrete security treatment of symmetric encryption,"We study notions and schemes for symmetric (ie. private key) encryption in a concrete security framework. We give four different notions of security against chosen plaintext attack and analyze the concrete complexity of reductions among them, providing both upper and lower bounds, and obtaining tight relations. In this way we classify notions (even though polynomially reducible to each other) as stronger or weaker in terms of concrete security. Next we provide concrete security analyses of methods to encrypt using a block cipher, including the most popular encryption method, CBC. We establish tight bounds (meaning matching upper bounds and attacks) on the success of adversaries as a function of their resources.","Concrete,
Cryptography,
Security,
Polynomials,
Computer science,
Uniform resource locators,
Engineering profession,
Drives,
Marine vehicles,
Upper bound"
A physics-based coordinate transformation for 3-D image matching,"Many image matching schemes are based on mapping coordinate locations, such as the locations of landmarks, in one image to corresponding locations in a second image. A new approach to this mapping (coordinate transformation), called the elastic body spline (EBS), is described. The spline is based on a physical model of a homogeneous, isotropic three-dimensional (3-D) elastic body. The model can approximate the way that some physical objects deform. The EBS as well as the affine transformation, the thin plate spline and the volume spline are used to match 3-D magnetic resonance images (MRI's) of the breast that are used in the diagnosis and evaluation of breast cancer. These coordinate transformations are evaluated with different types of deformations and different numbers of corresponding (paired) coordinate locations. In all but one of the cases considered, using the EBS yields more similar images than the other methods.",
New Methods for Competitive Coevolution,"We consider competitive coevolution, in which fitness is based on direct competition among individuals selected from two independently evolving populations of hosts and parasites. Competitive coevolution can lead to an arms race, in which the two populations reciprocally drive one another to increasing levels of performance and complexity. We use the games of Nim and 3-D Tic-Tac-Toe as test problems to explore three new techniques in competitive coevolution. Competitive fitness sharing changes the way fitness is measured; shared sampling provides a method for selecting a strong, diverse set of parasites; and the hall of fame encourages arms races by saving good individuals from prior generations. We provide several different motivations for these methods and mathematical insights into their use. Experimental comparisons are done, and a detailed analysis of these experiments is presented in terms of testing issues, diversity, extinction, arms race progress measurements, and drift.","fitness sharing,
Genetic algorithms,
host/parasite,
game learning"
Grouped-coordinate ascent algorithms for penalized-likelihood transmission image reconstruction,"Presents a new class of algorithms for penalized-likelihood reconstruction of attenuation maps from low-count transmission scans. We derive the algorithms by applying to the transmission log-likelihood a version of the convexity technique developed by De Pierro for emission tomography. The new class includes the single-coordinate ascent (SCA) algorithm and Lange's convex algorithm for transmission tomography as special cases. The new grouped-coordinate ascent (GCA) algorithms in the class overcome several limitations associated with previous algorithms. (1) Fewer exponentiations are required than in the transmission maximum likelihood-expectation maximization (ML-EM) algorithm or in the SCA algorithm. (2) The algorithms intrinsically accommodate nonnegativity constraints, unlike many gradient-based methods. (3) The algorithms are easily parallelizable, unlike the SCA algorithm and perhaps line-search algorithms. We show that the GCA algorithms converge faster than the SCA algorithm, even on conventional workstations. An example from a low-count positron emission tomography (PET) transmission scan illustrates the method.","Image reconstruction,
Attenuation,
Iterative algorithms,
Positron emission tomography,
Statistical analysis,
Single photon emission computed tomography,
Whole-body PET,
Workstations,
Nuclear imaging,
Gaussian processes"
"Replication is not needed: single database, computationally-private information retrieval","We establish the following, quite unexpected, result: replication of data for the computational private information retrieval problem is not necessary. More specifically, based on the quadratic residuosity assumption, we present a single database, computationally private information retrieval scheme with O(n/sup /spl epsiv//) communication complexity for any /spl epsiv/>0.","Databases,
Information retrieval,
Complexity theory,
Data privacy,
Upper bound,
History,
Indexes,
Computer science,
Postal services,
Polynomials"
A hierarchy of authentication specifications,"Many security protocols have the aim of authenticating one agent to another. Yet there is no clear consensus in the academic literature about precisely what ""authentication"" means. We suggest that the appropriate authentication requirement will depend upon the use to which the protocol is put, and identify several possible definitions of ""authentication"". We formalize each definition using the process algebra CSP, use this formalism to study their relative strengths, and show how the model checker FDR can be used to test whether a system running the protocol meets such a specification.","Authentication,
Protocols,
Algebra,
Mathematics,
Computer science,
Computer security,
System testing,
Control systems,
Stress"
A generalized model for passively Q-switched lasers including excited state absorption in the saturable absorber,"A generalized model of a passively Q-switched laser is presented. It enables performance optimization including cases in which the saturable absorber exhibits both ground and excited state absorption (ESA) at the laser wavelength. The model accounts for the properties of the lasing material, the saturable absorber, and the resonator. The procedure for using this model to determine resonator and Q-switch parameters which optimize the laser's performance is described and the model is applied to reported systems to demonstrate its use.","Laser modes,
Laser excitation,
Absorption,
Optical resonators,
Optical materials,
Stationary state,
Laser theory,
Optical saturation,
Computer science education,
Physics education"
Efficient utilization of scratch-pad memory in embedded processor applications,"Efficient utilization of on-chip memory space is extremely important in modern embedded system applications based on microprocessor cores. In addition to a data cache that interfaces with slower off-chip memory, a fast on-chip SRAM, called Scratch-Pad memory, is often used in several applications. We present a technique for efficiently exploiting on-chip Scratch-Pad memory by partitioning the application's scalar and array variables into off-chip DRAM and on-chip Scratch-Pad SRAM, with the goal of minimizing the total execution time of embedded applications. Our experiments on code kernels from typical applications show that our technique results in significant performance improvements.","Random access memory,
Microprocessors,
Application software,
System-on-a-chip,
Embedded system,
Large scale integration,
Reduced instruction set computing,
Data buses,
Computer science,
Kernel"
A mixed c-means clustering model,"We justify the need for computing both membership and typicality values when clustering unlabeled data. Then we propose a new model called fuzzy-possibilistic c-means (FPCM). Unlike the fuzzy and possibilistic c-means (FCM/PCM) models, FPCM simultaneously produces both memberships and possibilities, along with the usual point prototypes or cluster centers for each cluster We show that FPCM solves the noise sensitivity defect of FCM, and also overcomes the coincident clusters problem of PCM. Then we derive first order necessary conditions for extrema of the PFCM objective function, and use them as the basis for a standard alternating optimization approach to finding local minima. Three numerical examples are given that compare FCM to FPCM. Our calculations show that FPCM compares favorably to FCM.","Phase change materials,
Prototypes,
Clustering algorithms,
Machine intelligence,
Computer science,
Fuzzy logic,
Equations,
Fuzzy sets"
Motion correction of PET images using multiple acquisition frames,"Positron emission tomography (PET) is a relatively lengthy brain imaging method. Because it is difficult for the subject to stay still during the data acquisition, head motion during scans is a source of image degradation. A simple data acquisition technique to reduce the effect of this problem is described. The technique associates the incoming data with the real-space position of the head. During the PET scan, the head position is constantly monitored with two video cameras and compared to its initial position. Every time the displacement for a region within the field of view (FOV) is larger than a specified threshold displacement, the PET data acquisition system starts to save the PET data in a new frame. The total number of frames required for a complete study depends on the magnitude of the head motion during the study and on the threshold displacement. At the end of the study, all the acquired frames are reconstructed independently and each image is rotated and translated to coincide with the initial position. When these images are summed, they produce a final image with fewer motion artefacts.","Positron emission tomography,
Head,
Monitoring,
Data acquisition,
Biomedical imaging,
Degradation,
Image reconstruction,
Physics,
Motion detection,
Brain"
Comparison of different methods of classification in subband coding of images,"This paper investigates various classification techniques, applied to subband coding of images, as a way of exploiting the nonstationary nature of image subbands. The advantages of subband classification are characterized in a rate-distortion framework in terms of ""classification gain"" and overall ""subband classification gain."" Two algorithms, maximum classification gain and equal mean-normalized standard deviation classification, which allow unequal number of blocks in each class, are presented. The dependence between the classification maps from different subbands is exploited either directly while encoding the classification maps or indirectly by constraining the classification maps. The trade-off between the classification gain and the amount of side information is explored. Coding results for a subband image coder based on classification are presented. The simulation results demonstrate the value of classification in subband coding.","Image coding,
Quantization,
Discrete wavelet transforms,
Rate-distortion,
Computer science,
Filters,
Classification algorithms,
Arithmetic,
International collaboration,
Image processing"
Dimension Reduction by Local Principal Component Analysis,"Reducing or eliminating statistical redundancy between the components of high-dimensional vector data enables a lower-dimensional representation without significant loss of information. Recognizing the limitations of principal component analysis (PCA), researchers in the statistics and neural network communities have developed nonlinear extensions of PCA. This article develops a local linear approach to dimension reduction that provides accurate representations and is fast to compute. We exercise the algorithms on speech and image data, and compare performance with PCA and with neural network implementations of nonlinear PCA. We find that both nonlinear techniques can provide more accurate representations than PCA and show that the local linear techniques outperform neural network implementations.",
A comparison of sender-initiated and receiver-initiated reliable multicast protocols,"Sender-initiated reliable multicast protocols based on the use of positive acknowledgments (ACKs) can suffer performance degradation as the number of receivers increases. This degradation is due to the fact that the sender must bear much of the complexity associated with reliable data transfer (e.g., maintaining state information and timers for each of the receivers and responding to receivers' ACKs). A potential solution to this problem is to shift the burden of providing reliable data transfer to the receivers-thus resulting in receiver-initiated multicast error control protocols based on the use of negative acknowledgments (NAKs). We determine the maximum throughputs for generic sender-initiated and receiver-initiated protocols for two classes of applications: (1) one-many applications where one participant sends data to a set of receivers and (2) many-many applications where all participants simultaneously send and receive data to/from each other. We show that a receiver-initiated error control protocol which requires receivers to transmit NAKs point-to-point to the sender provides higher throughput than a sender-initiated counterpart for both classes of applications. We further demonstrate that, in the case of a one many application, replacing point-to-point transfer of NAKs with multicasting of NAKs coupled with a random backoff procedure provides a substantial additional increase in the throughput of a receiver-initiated error control protocol over a sender-initiated protocol. We also find, however, that such a modification leads to a throughput degradation in the case of many-many applications.","Multicast protocols,
Throughput,
Degradation,
Error correction,
Maintenance,
Automatic repeat request,
Wide area networks,
Video sharing,
Computational modeling,
Computer science"
The NEURON Simulation Environment,The moment-to-moment processing of information by the nervous system involves the propagation and interaction of electrical and chemical signals that are distributed in space and time. Biologically realistic modeling is needed to test hypotheses about the mechanisms that govern these signals and how nervous system function emerges from the operation of these mechanisms. The NEURON simulation program provides a powerful and flexible environment for implementing such models of individual neurons and small networks of neurons. It is particularly useful when membrane potential is nonuniform and membrane currents are complex. We present the basic ideas that would help informed users make the most efficient use of NEURON.,
Fingerprint matching using transformation parameter clustering,"Fingerprint matching is used in many noncriminal identification applications. Flash, a similarity-searching algorithm akin to geometric hashing, proves suitable for one-to-many matching of fingerprints on large-scale databases.","Fingerprint recognition,
Spatial databases,
Fingers,
Filtering,
Large-scale systems,
Error analysis,
Size measurement,
Frequency estimation,
Indexes,
Indexing"
Building diverse computer systems,"Diversity is an important source of robustness in biological systems. Computers, by contrast, are notable for their lack of diversity. Although homogeneous systems have many advantages, the beneficial effects of diversity in computing systems have been overlooked, specifically in the area of computer security. Several methods of achieving software diversity are discussed based on randomizations that respect the specified behavior of the program. Such randomization could potentially increase the robustness of software systems with minimal impact on convenience, usability, and efficiency. Randomization of the amount of memory allocated on a stack frame is shown to disrupt a simple buffer overflow attack.","Biology computing,
Computer security,
Robustness,
Computer science,
Operating systems,
Usability,
Ecosystems,
Diseases,
Immune system,
Hardware"
MEG-based imaging of focal neuronal current sources,"The authors describe a new approach to imaging neural current sources from measurements of the magnetoencephalogram (MEG) associated with sensory, motor, or cognitive brain activation. Many previous approaches to this problem have concentrated on the use of weighted minimum norm (WMN) inverse methods. While these methods ensure a unique solution, they do not introduce information specific to the MEG inverse problem, often producing overly smoothed solutions and exhibiting severe sensitivity to noise. The authors describe a Bayesian formulation of the inverse problem in which a Gibbs prior is constructed to reflect the sparse focal nature of neural current sources associated with evoked response data. The authors demonstrate the method with simulated and experimental phantom data, comparing its performance with several WMN methods.","Inverse problems,
Brain modeling,
Superconducting magnets,
Bayesian methods,
Magnetic field measurement,
Current measurement,
SQUIDs,
Signal processing,
Image processing,
Superconducting device noise"
Panoramic mosaics by manifold projection,"As the field of view of a picture is much smaller than our own visual field of view, it is common to paste together several pictures to create a panoramic mosaic having a larger field of view. Images with a wider field of view can be generated by using fish-eye lens, or panoramic mosaics can be created by special devices which rotate around the camera's optical center (Quicktime VR, Surround Video), or by aligning, and pasting, frames in a video sequence to a single reference frame. Existing mosaicing methods have strong limitations on imaging conditions, and distortions are common. Manifold projection enables the creation of panoramic mosaics from video sequences under more general conditions, and in particular the unrestricted motion of a hand-held camera. The panoramic mosaic is a projection of the scene into a virtual manifold whose structure depends on the camera's motion. This manifold is more general than the customary projections onto a single image plane or onto a cylinder. In addition to being more general than traditional mosaics, manifold projection is also computationally efficient, as the only image deformations used are image plane translations and rotations. Real-time, software only, implementation on a Pentium-PC, proves the superior quality and speed of this approach.","Cameras,
Optical distortion,
Video sequences,
Layout,
Lenses,
Optical devices,
USA Councils,
Satellites,
Computer science,
Virtual reality"
Server selection using dynamic path characterization in wide-area networks,"Replication is a commonly proposed solution to problems of scale associated with distributed services. However, when a service is replicated, each client must be assigned a server. Prior work has generally assumed the assignment to be static. In contrast, we propose a dynamic server selection, and show that it enables application-level congestion avoidance. Using tools to measure the available bandwidth and round trip latency (RTT), we demonstrate the dynamic server selection and compare it to previous static approaches. We show that because of the variability of paths in the Internet, dynamic server selection consistently outperforms static policies, reducing response times by as much as 50%. However, we also must adopt a systems perspective and consider the impact of the measurement method on the network. Therefore, we look at alternative low-cost approximations and find that the careful measurements provided by our tools can be closely approximated by much lighter-weight measurements. We propose a protocol using this method which is limited to at most a 1% increase in network traffic but which often costs much less in practice.","Network servers,
Intelligent networks,
Web server,
Delay,
Bandwidth,
Internet,
Costs,
Protocols,
Time measurement,
Computer science"
Automatic correction of motion artifacts in magnetic resonance images using an entropy focus criterion,"Presents the use of an entropy focus criterion to enable automatic focusing of motion corrupted magnetic resonance images. The authors demonstrate the principle using illustrative examples from cooperative volunteers. Their technique can determine unknown patient motion or use knowledge of motion from other measures as a starting estimate. The motion estimate is used to compensate the acquired data and is iteratively refined using the image entropy. Entropy focuses the whole image principally by favoring the removal of motion induced ghosts and blurring from otherwise dark regions of the image. Using only the image data, and no special hardware or pulse sequences, the authors demonstrate correction for arbitrary rigid-body translational motion in the imaging plane and for a single rotation. Extension to three-dimensional (3-D) and more general motion should be possible. The algorithm is able to determine volunteer motion well. The mean absolute deviation between algorithm and navigator-echo-determined motion is comparable to the displacement step size used in the algorithm. Local deviations from the recorded motion or navigator-determined motion are explained and the authors indicate how enhanced focus criteria may be derived. In all cases they were able to compensate images for patient motion, reducing blurring and ghosting.","Magnetic resonance,
Entropy,
Focusing,
Navigation,
Hardware,
Hospitals,
Motion estimation,
Magnetic resonance imaging,
Image resolution,
Image processing"
A logical language for expressing authorizations,"A major drawback of existing access control systems is that they have all been developed with a specific access control policy in mind. This means that all protection requirements (i.e. accesses to be allowed or denied) must be specified in terms of the policy enforced by the system. While this may be trivial for some requirements, specification of other requirements may become quite complex or even impossible. The reason for this is that a single policy simply cannot capture the different protection requirements that users may need to enforce on different data. In this paper, we take a first step towards a model that is able to support different access control policies. We propose a logical language for the specification of authorizations on which such a model can be based. The Authorization Specification Language (ASL) allows users to specify, together with the authorizations, the policy according to which access control decisions are to be made. Policies are expressed by means of rules which enforce the derivation of authorizations, conflict resolution, access control and integrity constraint checking. We illustrate the power of our language by showing how different constraints that are sometimes required, but very seldom supported by existing access control systems, can be represented in our language.","Authorization,
Access control,
Protection,
Information systems,
Computer science,
Educational institutions,
National security,
Contracts,
Laboratories"
Prefetching Using Markov Predictors,,"Prefetching,
Permission,
Computer science,
Delay,
Modems,
Process design,
Data structures,
Design for experiments,
Performance analysis,
Hardware"
Casper: a compiler for the analysis of security protocols,"In recent years, a method for analyzing security protocols using the process algebra CSP (C.A.R. Hoare, 1985) and its model checker FDR (A.W Roscoe, 1994) has been developed. This technique has proved successful, and has been used to discover a number of attacks upon protocols. However the technique has required producing a CSP description of the protocol by hand; this has proved tedious and error prone. We describe Casper, a program that automatically produces the CSP description from a more abstract description, thus greatly simplifying the modelling and analysis process.","Protocols,
Algebra,
Mathematics,
Computer science,
Authentication,
Control systems,
Information security,
System testing,
State-space methods,
Performance evaluation"
On path selection for traffic with bandwidth guarantees,"Transmission of multimedia streams imposes a minimum-bandwidth requirement on the path being used to ensure end-to-end Quality-of-Service (QoS) guarantees. While any shortest-path algorithm can be used to select a feasible path, additional constraints that limit resource consumption and balance the network load are needed to achieve efficient resource utilization. We present a systematic evaluation of four routing algorithms that offer different tradeoffs between limiting the path hop count and balancing the network load. Our evaluation considers not only the call blocking rate but also the fairness to requests for different bandwidths, robustness to inaccurate routing information, and sensitivity to the routing information update frequency. It evaluates not only the performance of these algorithms for the sessions with bandwidth guarantees, but also their impact on the lower priority best-effort sessions. Our results show that a routing algorithm that gives preference to limiting the hop count performs better when the network load is heavy, while an algorithm that gives preference to balancing the network load performs slightly better when the network load is light. We also show that the performance of using pre-computed paths with a few discrete bandwidth requests is comparable to that of computing paths on-demand, which implies feasibility of class-based routing. We observe that the routing information update interval can be set reasonably large to reduce routing overhead without sacrificing the overall performance, although an increased number of sessions can be misrouted.","Bandwidth,
Routing,
Streaming media,
Resource management,
Robustness,
Frequency,
Telecommunication traffic,
Computer science,
Quality of service,
Joining processes"
Simulating the behavior of MEMS devices: computational methods and needs,"Technologies for fabricating a variety of MEMS devices have developed rapidly, but computational tools that allow engineers to quickly design and optimize these micromachines have not kept pace. Inadequate simulation tools force MEMS designers to resort to physical prototyping. To realistically simulate the behavior of complete micromachines, algorithmic innovation is necessary in several areas.",
Clustering association rules,"The authors consider the problem of clustering two-dimensional association rules in large databases. They present a geometric-based algorithm, BitOp, for performing the clustering, embedded within an association rule clustering system, ARCS. Association rule clustering is useful when the user desires to segment the data. They measure the quality of the segmentation generated by ARCS using the minimum description length (MDL) principle of encoding the clusters on several databases including noise and errors. Scale-up experiments show that ARCS, using the BitOp algorithm, scales linearly with the amount of data.","Association rules,
Transaction databases,
Clustering algorithms,
Data mining,
Demography,
Visual databases,
Dairy products,
Computer science,
Spatial databases,
Length measurement"
Quantitative coronary angiography with deformable spline models,"Although current edge-following schemes can be very efficient in determining coronary boundaries, they may fail when the feature to be followed is disconnected (and the scheme is unable to bridge the discontinuity) or branch points exist where the best path to follow is indeterminate. Here, the authors present new deformable spline algorithms for determining vessel boundaries, and enhancing their centerline features. A bank of even and odd S-Gabor filter pairs of different orientations are convolved with vascular images in order to create an external snake energy field. Each fitter pair will give maximum response to the segment of vessel having the same orientation as the filters. The resulting responses across filters of different orientations are combined to create an external energy field for snake optimization. Vessels are represented by B-Spline snakes, and are optimized on filter outputs with dynamic programming. The points of minimal constriction and the percent-diameter stenosis are determined from a computed vessel centerline. The system has been statistically validated using fixed stenosis and flexible-tube phantoms. It has also been validated on 20 coronary lesions with two independent operators, and has been tested for interoperator and intraoperator variability and reproducibility. The system has been found to be specially robust in complex images involving vessel branchings and incomplete contrast filling.","Angiography,
Spline,
Deformable models,
Filters,
Bridges,
Image segmentation,
Dynamic programming,
Imaging phantoms,
Lesions,
Testing"
Fractal modeling and segmentation for the enhancement of microcalcifications in digital mammograms,"The objective of this research is to model the mammographic parenchymal and ductal patterns and enhance the microcalcifications using a deterministic fractal approach. According to the theory of deterministic fractal geometry, images can be modeled by deterministic fractal objects which are attractors of sets of two-dimensional (2-D) affine transformations. The iterated functions systems and the collage theorem are the mathematical foundations of fractal image modeling. Here, a methodology based on fractal image modeling is developed to analyze and model breast background structures. The authors show that general mammographic parenchymal and ductal patterns can be well modeled by a set of parameters of affine transformations. Therefore, microcalcifications can be enhanced by taking the difference between the original image and the modeled image. The authors' results are compared with those of the partial wavelet reconstruction and morphological operation approaches. The results demonstrate that the fractal modeling method is an effective way to enhance microcalcifications. It may also be able to improve the detection and classification of microcalcifications in a computer-aided diagnosis system.","Fractals,
Image segmentation,
Geometry,
Solid modeling,
Two dimensional displays,
Mathematical model,
Image analysis,
Breast,
Image reconstruction,
Morphological operations"
The software bookshelf,"Legacy software systems are typically complex, geriatric, and difficult to change, having evolved over decades and having passed through many developers. Nevertheless, these systems are mature, heavily used, and constitute massive corporate assets. Migrating such systems to modern platforms is a significant challenge due to the loss of information over time. As a result, we embarked on a research project to design and implement an environment to support software migration. In particular, we focused on migrating legacy PL/I source code to C++, with an initial phase of looking at redocumentation strategies. Recent technologies such as reverse engineering tools and World Wide Web standards now make it possible to build tools that greatly simplify the process of redocumenting a legacy software system. In this paper we introduce the concept of a software bookshelf as a means to capture, organize, and manage information about a legacy software system. We distinguish three roles directly involved in the construction, population, and use of such a bookshelf: the builder, the librarian, and the patron. From these perspectives, we describe requirements for the bookshelf, as well as a generic architecture and a prototype implementation. We also discuss various parsing and analysis tools that were developed and integrated to assist in the recovery of useful information about a legacy system. In addition, we illustrate how a software bookshelf is populated with the information of a given software project and how the bookshelf can be used in a program-understanding scenario. Reported results are based on a pilot project that developed a prototype bookshelf for a software system consisting of approximately 300K lines of code written in a PL/I dialect.",
The farthest point strategy for progressive image sampling,"A new method of farthest point strategy (FPS) for progressive image acquisition-an acquisition process that enables an approximation of the whole image at each sampling stage-is presented. Its main advantage is in retaining its uniformity with the increased density, providing efficient means for sparse image sampling and display. In contrast to previously presented stochastic approaches, the FPS guarantees the uniformity in a deterministic min-max sense. Within this uniformity criterion, the sampling points are irregularly spaced, exhibiting anti-aliasing properties comparable to those characteristic of the best available method (Poisson disk). A straightforward modification of the FPS yields an image-dependent adaptive sampling scheme. An efficient O(N log N) algorithm for both versions is introduced, and several applications of the FPS are discussed.",
STR: a simple and efficient algorithm for R-tree packing,"Presents the results from an extensive comparison study of three R-tree packing algorithms: the Hilbert and nearest-X packing algorithms, and an algorithm which is very simple to implement, called the STR (Sort-Tile-Recursive) algorithm. The algorithms are evaluated using both synthetic and actual data from various application domains including VLSI design, GIS (Tiger files), and computational fluid dynamics. Our studies also consider the impact that various degrees of buffering have on query performance. Experimental results indicate that none of the algorithms as best for all types of data. In general, our new algorithm requires up to 50% fewer disk accesses than the best previously proposed algorithm for point and region queries on uniformly distributed or mildly skewed point and region data, and approximately the same for highly skewed point and region data.","Geographic Information Systems,
Spatial databases,
Application software,
Very large scale integration,
Indexing,
Mathematics,
Computer science,
Computer applications,
Computer vision,
Machine vision"
Computer-aided breast cancer detection and diagnosis of masses using difference of Gaussians and derivative-based feature saliency,"A new model-based vision (MBV) algorithm is developed to find regions of interest (ROI's) corresponding to masses in digitized mammograms and to classify the masses as malignant/benign. The MBV algorithm is comprised of 5 modules to structurally identify suspicious ROI's, eliminate false positives, and classify the remaining as malignant or benign. The focus of attention module uses a difference of Gaussians (DoG) filter to highlight suspicious regions in the mammogram. The index module uses tests to reduce the number of nonmalignant regions from 8.39 to 2.36 per full breast image. Size, shape, contrast, and Laws texture features are used to develop the prediction module's mass models. Derivative-based feature saliency techniques are used to determine the best features for classification. Nine features are chosen to define the malignant/benign models. The feature extraction module obtains these features from all suspicious ROI's. The matching module classifies the regions using a multilayer perceptron neural network architecture to obtain an overall classification accuracy of 100% for the segmented malignant masses with a false-positive rate of 1.8 per full breast image. This system has a sensitivity of 92% for locating malignant ROI's. The database contains 272 images (12 b, 100 /spl mu/m) with 36 malignant and 53 benign mass images. The results demonstrate that the MBV approach provides a structured order of integrating complex stages into a system for radiologists.","Breast cancer,
Cancer detection,
Gaussian processes,
Focusing,
Filters,
Testing,
Shape,
Predictive models,
Feature extraction,
Multilayer perceptrons"
Analytical energy dissipation models for low power caches,"We present detailed analytical models for estimating the energy dissipation in conventional caches as well as low energy cache architectures. The analytical models use the run time statistics such as hit/miss counts, fraction of read/write requests and assume stochastical distributions for signal values. These models are validated by comparing the power estimated using these models against the power estimated using a detailed simulator called CAPE (CAache Power Estimator). The analytical models for conventional caches are found to be accurate to within 2% error. However, these analytical models over-predict the dissipations of low-power caches by as much as 30%. The inaccuracies can be attributed to correlated signal values and locality of reference, both of which are exploited in making some cache organizations energy efficient.","Energy dissipation,
Analytical models,
Random access memory,
Microprocessors,
Permission,
Energy efficiency,
Read-write memory,
Computer science,
State estimation,
Computer architecture"
Optical components for WDM lightwave networks,"Recently, there has been growing interest in developing optical fiber networks to support the increasing bandwidth demands of multimedia applications, such as video conferencing and World Wide Web browsing. One technique for accessing the huge bandwidth available in an optical fiber is wavelength-division multiplexing (WDM). Under WDM, the optical fiber bandwidth is divided into a number of nonoverlapping wavelength bands, each of which may be accessed at peak electronic rates by an end user. By utilizing WDM in optical networks, we can achieve link capacities on the order of 50 THz. The success of WDM networks depends heavily on the available optical device technology. This paper is intended as a tutorial on some of the optical device issues in WDM networks. It discusses the basic principles of optical transmission in fiber and reviews the current state of the art in optical device technology. It introduces some of the basic components in WDM networks, discusses various implementations of these components, and provides insights into their capabilities and limitations. Then, this paper demonstrates how various optical components can be incorporated into WDM optical networks for both local and wide-area applications. Finally, the paper provides a brief review of experimental WDM networks that have been implemented.","Optical devices,
Wavelength division multiplexing,
WDM networks,
Bandwidth,
Optical fiber networks,
Optical fibers,
Optical transmitters,
Optical network units,
Internet,
Computer science"
Accurate measurement of intrathoracic airways,"Airway geometry measurements can provide information regarding pulmonary physiology and pathophysiology. There has been considerable interest in measuring intrathoracic airways in two-dimensional (2-D) slices from volumetric X-ray computed tomography (CT). Such measurements can be used to evaluate and track the progression of diseases affecting the airways. A popular airway measurement method uses the ""half-max"" criteria, in which the gray level at the airway wall is estimated to be halfway between the minimum and maximum gray levels along a ray crossing the edge. However, because the scanning process introduces blurring, the half-max approach may not be applicable across all airway sizes. The authors propose a new measurement method based on a model of the scanning process. In their approach, they examine the gray-level profile of a ray crossing the airway wall and use a maximum-likelihood method to estimate the airway inner and outer radius. Using CT scans of a physical phantom, the authors present results showing that the new approach is more accurate than the half-max method at estimating wall location for thin-walled airways.","Computed tomography,
Information geometry,
Physiology,
Volume measurement,
Two dimensional displays,
X-ray imaging,
Diseases,
Maximum likelihood estimation,
Imaging phantoms,
Thin wall structures"
Comparison of measurement-based admission control algorithms for controlled-load service,"We compare the performance of four admission control algorithms-one parameter-based and three measurement-based-for controlled-load service. The parameter-based admission control ensures that the sum of reserved resources is bounded by the capacity. The three measurement-based algorithms are based on measured bandwidth, acceptance region and equivalent bandwidth. We use simulation on several network scenarios to evaluate the link utilization and adherence to service commitment achieved by these four algorithms.","Admission control,
Bandwidth,
Quality of service,
Delay,
Costs,
Algorithm design and analysis,
Context modeling,
Traffic control,
Computer science,
IP networks"
Image Segmentation Based on Oscillatory Correlation,"We study image segmentation on the basis of locally excitatory, globally inhibitory oscillator networks (LEGION), whereby the phases of oscillators encode the binding of pixels. We introduce a lateral potential for each oscillator so that only oscillators with strong connections from their neighborhood can develop high potentials. Based on the concept of the lateral potential, a solution to remove noisy regions in an image is proposed for LEGION, so that it suppresses the oscillators corresponding to noisy regions but without affecting those corresponding to major regions. We show that the resulting oscillator network separates an image into several major regions, plus a background consisting of all noisy regions, and we illustrate network properties by computer simulation. The network exhibits a natural capacity in segmenting images. The oscillatory dynamics leads to a computer algorithm, which is applied successfully to segmenting real gray-level images. A number of issues regarding biological plausibility and perceptual organization are discussed. We argue that LEGION provides a novel and effective framework for image segmentation and figure-ground segregation.",
Staircase data broadcasting and receiving scheme for hot video service,"For a 120-minute popular movie, with 4 video channels, current digital video broadcasting systems can support the movie every 30 minutes. Suppose the set-top box at client end can buffer portions of the playing video on disk. Pyramid broadcasting schemes can support the movie every 19 minutes. We present a new data broadcasting and receiving scheme, which can service the movie every 8 minutes. For a given bandwidth allocation, the new scheme greatly reduces the viewer waiting time. The disk space and transfer rate requirements of the set-top box are also reduced to be feasible.",
H3: laying out large directed graphs in 3D hyperbolic space,"We present the H3 layout technique for drawing large directed graphs as node-link diagrams in 3D hyperbolic space. We can lay out much larger structures than can be handled using traditional techniques for drawing general graphs because we assume a hierarchical nature of the data. We impose a hierarchy on the graph by using domain-specific knowledge to find an appropriate spanning tree. Links which are not part of the spanning tree do not influence the layout but can be selectively drawn by user request. The volume of hyperbolic 3-space increases exponentially, as opposed to the familiar geometric increase of euclidean 3-space. We exploit this exponential amount of room by computing the layout according to the hyperbolic metric. We optimize the cone tree layout algorithm for 3D hyperbolic space by placing children on a hemisphere around the cone mouth instead of on its perimeter. Hyperbolic navigation affords a Focus+Context view of the structure with minimal visual clutter. We have successfully laid out hierarchies of over 20,000 nodes. Our implementation accommodates navigation through graphs too large to be rendered interactively by allowing the user to explicitly prune or expand subtrees.","Tree graphs,
Visualization,
Navigation,
Mouth,
Information systems,
Computer science,
File systems,
Web sites,
Web page design,
History"
Toward accurate attenuation correction in SPECT without transmission measurements,"The current trend in attenuation correction for single photon emission computed tomography (SPECT) is to measure and reconstruct the attenuation coefficient map using a transmission scan, performed either sequentially or simultaneously with the emission scan. This approach requires dedicated hardware and increases the cost (and in some cases the scanning time) required to produce a clinical SPECT image. Furthermore, if short focal-length fan-beam collimators are used for transmission imaging, the projection data may be truncated, leading to errors in the attenuation coefficient map. Our goal is to obtain information about the attenuation distribution from only the measured emission data by exploiting the fact that only certain attenuation distributions are consistent with a given emission dataset. Ultimately this consistency information will either be used directly to compensate for attenuation or combined with the incomplete information from fan-beam transmission measurements to produce a more accurate attenuation coefficient map. In this manuscript the consistency conditions (which relate the measured SPECT data to the sinogram of the attenuation distribution) are used to find the uniform elliptical attenuation object which is most consistent with the measured emission data. This object is then used for attenuation correction during the reconstruction of the emission data. The method is tested using both simulated and experimentally acquired data from uniformly and nonuniformly attenuating objects. The results show that, for uniform elliptical attenuators, the consistency conditions of the SPECT data can be used to produce an accurate estimate of the attenuation map without performing any transmission measurements. The results also show that, in certain circumstances, the consistency conditions can prove useful for attenuation compensation with nonuniform attenuators.","Attenuation measurement,
Image reconstruction,
Performance evaluation,
Attenuators,
Single photon emission computed tomography,
Current measurement,
Hardware,
Costs,
Collimators,
Testing"
Expectation maximization reconstruction of positron emission tomography images using anatomical magnetic resonance information,"Using statistical methods the reconstruction of positron emission tomography (PET) images can be improved by high-resolution anatomical information obtained from magnetic resonance (MR) images. The authors implemented two approaches that utilize MR data for PET reconstruction. The anatomical MR information is modeled as a priori distribution of the PET image and combined with the distribution of the measured PET data to generate the a posteriori function from which the expectation maximization (EM)-type algorithm with a maximum a posteriori (MAP) estimator is derived. One algorithm (Markov-GEM) uses a Gibbs function to model interactions between neighboring pixels within the anatomical regions. The other (Gauss-EM) applies a Gauss function with the same mean for all pixels in a given anatomical region. A basic assumption of these methods is that the radioactivity is homogeneously distributed inside anatomical regions. Simulated and phantom data are investigated under the following aspects: count density, object size, missing anatomical information, and misregistration of the anatomical information. Compared with the maximum likelihood-expectation maximization (ML-EM) algorithm the results of both algorithms show a large reduction of noise with a better delineation of borders. Of the two algorithms tested, the Gauss-EM method is superior in noise reduction (up to 50%). Regarding incorrect a priori information the Gauss-EM algorithm is very sensitive, whereas the Markov-GEM algorithm proved to be stable with a small change of recovery coefficients between 0.5 and 3%.","Image reconstruction,
Positron emission tomography,
Magnetic resonance imaging,
Gaussian processes,
Biomedical imaging,
Image resolution,
Magnetic resonance,
Noise reduction,
Statistical analysis,
Imaging phantoms"
Motion strategies for maintaining visibility of a moving target,"We introduce the problem of computing robot motion strategies that maintain visibility of a moving target in a cluttered workspace. Both motion constraints (as considered in standard motion planning) and visibility constraints (as considered in visual tracking) must be satisfied. Additional criteria, such as the total distance traveled, can be optimized. The general problem is divided into two categories, on the basis of whether the target is predictable. For the predictable case, an algorithm that computes optimal, numerical solutions is presented. For the more challenging case of a partially-predictable target, two online algorithms are presented that each attempt to maintain future visibility with limited prediction. One strategy maximizes the probability that the target will remain in view in a subsequent time step, and the other maximizes the minimum time in which the target could escape the visibility region. We additionally discuss issues resulting from our implementation and experiments on a mobile robot system.","Target tracking,
Robotics and automation,
Motion planning,
Machine vision,
Biomedical monitoring,
Computerized monitoring,
Cameras,
Robot vision systems,
Trajectory,
Computer science"
Improving video-on-demand server efficiency through stream tapping,"Efficiency is essential for video-on-demand (VOD) to be successful. Conventional VOD servers are inefficient, they dedicate a disk stream for each client, quickly using up all available streams. However, several systems have been proposed that allow clients to share streams. We present a new system called stream tapping that allows a client to greedily ""tap"" data from any stream on the VOD server containing video data the client can use. This is accomplished through the use of a small buffer on the client set-top box and requires less than 20% of the disk bandwidth used by conventional systems for popular videos. We present a description and analysis of the stream tapping system as well as comparisons between it and other efficiency-improving systems.","Displays,
Streaming media,
US Department of Transportation,
Delay,
TV,
Hardware,
Network servers,
Bandwidth,
Buffer storage,
Computer science"
Execution monitoring of security-critical programs in distributed systems: a specification-based approach,"We describe a specification-based approach to detect exploitations of vulnerabilities in security-critical programs. The approach utilizes security specifications that describe the intended behavior of programs and scans audit trails for operations that are in violation of the specifications. We developed a formal framework for specifying the security-relevant behavior of programs, on which we based the design and implementation of a real-time intrusion detection system for a distributed system. Also, we wrote security specifications for 15 Unix setuid root programs. Our system detects attacks caused by monitored programs, including security violations caused by improper synchronization in distributed programs. Our approach encompasses attacks that exploit previously unknown vulnerabilities in security-critical programs.","Monitoring,
Intrusion detection,
Information systems,
Information security,
Real time systems,
Testing,
Computer science,
Specification languages,
Computer networks,
Distributed computing"
Multishot rosette trajectories for spectrally selective MR imaging,"In nuclear magnetic resonance, different spectral components often correspond to different chemical species and as such, spectral selectivity can be a valuable tool for diagnostic imaging. In the work presented here, a multishot image acquisition method based upon rosette k-space trajectories has been developed and implemented for spectrally selective magnetic resonance imaging (MRI). Parametric forms for the gradient waveforms and design constraints are derived, and an example multishot gradient design is presented. The spectral behaviour for this imaging method is analyzed in a simulation model. For frequencies that are near to the resonant frequency, this method results in a lower intensity, but undistorted image, while for frequencies that are off-resonance by a large amount, the object is incoherently dephased into noise. A method by which acquisitions are delayed by small amounts is introduced to further reduce the residual intensity for off-resonant signals. An image reconstruction method based on convolution gridding, including a correction method for small amounts of magnetic field inhomogeneity, is implemented. Finally, the spectral selectivity is demonstrated in vivo in a study in which both water and lipid images are generated from a single imaging data set.","Magnetic resonance imaging,
Nuclear magnetic resonance,
Chemicals,
Image analysis,
Magnetic analysis,
Analytical models,
Resonant frequency,
Magnetic noise,
Delay,
Image reconstruction"
Improved operation of power transformer protection using artificial neural network,"This paper suggests the possibility of improving digital power transformer protection. The establishment of inrush in power transformers is becoming unreliable in existing numerical protection. An artificial neural network (ANN) was applied to inrush detection. The saturation of protective current transformers (CT) cannot be totally eliminated despite proper dimensioning. ANN was used for the reconstruction of distorted secondary CT currents due to saturation. In both cases, an ANN was included in the protection algorithm as an extension of the existing methods, which improved the reliability of the protection operation. The paper presents the digital protection algorithm completed in this way and the laboratory equipment by means of which experimental results were obtained. The results confirm faster and more reliable recognition of transformer inrush, as well as satisfactory reconstruction of the distorted secondary CT currents.","Power transformers,
Artificial neural networks,
Current transformers,
Circuit faults,
Surge protection,
Power harmonic filters,
Power system relaying,
Mathematical model,
Associate members,
Computer science"
A rapid and automatic image registration algorithm with subpixel accuracy,"A number of digital imaging techniques in medicine require the combination of multiple images. Using these techniques, it is essential that the images be adequately aligned and registered prior to addition, subtraction, or any other combination of the images. This paper describes an alignment routine developed to register an image of a fixed object containing a global offset error, rotation error, and magnification error relative to a second image. The described routine uses sparsely sampled regional correlation in a novel way to reduce computation time and avoid the use of markers and human interaction. The result is a fast, robust, and automatic alignment algorithm, with accuracy better than about 0.2 pixel in a test with clinical computed radiography images.","Image registration,
Chromium,
Biomedical imaging,
Digital images,
Pixel,
Radiography,
Radiology,
Computed tomography,
Registers,
Humans"
Noncontiguous processor allocation algorithms for mesh-connected multicomputers,"Current processor allocation techniques for highly parallel systems are typically restricted to contiguous allocation strategies for which performance suffers significantly due to the inherent problem of fragmentation. As a result, message-passing systems have yet to achieve the high utilization levels exhibited by traditional vector supercomputers. We are investigating processor allocation algorithms which lift the restriction on contiguity of processors in order to address the problem of fragmentation. Three noncontiguous processor allocation strategies-paging allocation, random allocation, and the Multiple Buddy Strategy (MBS)-are proposed and studied in this paper. Simulations compare the performance of the noncontiguous strategies with that of several well-known contiguous algorithms. We show that noncontiguous allocation algorithms perform better overall than the contiguous ones, even when message-passing contention is considered. We also present the results of experiments on an Intel Paragon XP/S-15 with 208 nodes that show noncontiguous allocation is feasible with current technologies.","Supercomputers,
Processor scheduling,
Computer applications,
Throughput,
Parallel machines,
Hypercubes,
Runtime,
Fault tolerance,
Information science,
Silicon"
On the power of quantum finite state automata,"In this paper, we introduce 1-way and 2-way quantum finite state automata (1qfa's and 2qfa's), which are the quantum analogues of deterministic, nondeterministic and probabilistic 1-way and 2-way finite state automata. We prove the following facts regarding 2qfa's. 1. For any /spl epsiv/>0, there is a 2qfa M which recognizes the non-regular language L={a/sup m/b/sup m/|m/spl ges/1} with (one-sided) error bounded by E, and which halts in linear time. Specifically, M accepts any string in L with probability 1 and rejects any string not in L with probability at least 1-/spl epsiv/. 2. For every regular language L, there is a reversible (and hence quantum) 2-way finite state automaton which recognizes L and which runs in linear time. In fact, it is possible to define 2qfar's which recognize the non-context-free language {a/sup m/b/sup m/c/sup m/|m/spl ges/1}, based on the same technique used for 1. Consequently, the class of languages recognized by linear time, bounded error 2qfa's properly includes the regular languages. Since it is known that 2-way deterministic, nondeterministic and polynomial expected time, bounded error probabilistic finite automata can recognize only regular languages, it follows that 2qfa's are strictly more powerful than these ""classical"" models. In the case of 1-way automata, the situation is reversed. We prove that the class of languages recognizable by bounded error 1qfa's is properly contained in the class of regular languages.","Automata,
Quantum computing,
Polynomials,
Computer science,
Physics computing,
Turing machines,
Circuits,
Computational modeling,
Error probability,
Automatic control"
Adaptive mammographic image enhancement using first derivative and local statistics,"This paper proposes an adaptive image enhancement method for mammographic images, which is based on the first derivative and the local statistics. The adaptive enhancement method consists of three processing steps. The first step is to remove the film artifacts which may be misread as microcalcifications. The second step is to compute the gradient images by using the first derivative operators. The third step is to enhance the important features of the mammographic image by adding the adaptively weighted gradient images. Local statistics of the image are utilized for adaptive realization of the enhancement, so that image details can be enhanced and image noises can be suppressed. The objective performances of the proposed method were compared with those by the conventional image enhancement methods for a simulated image and the seven mammographic images containing real microcalcifications. The performance of the proposed method was also evaluated by means of the receiver operating characteristics (ROC) analysis for 78 real mammographic images with and without microcalcifications.","Image enhancement,
Statistics,
Performance evaluation,
Image analysis,
Performance analysis,
Breast cancer,
Shape,
Biomedical engineering,
Biomedical imaging,
Medical diagnostic imaging"
Chaining: a generalized batching technique for video-on-demand systems,"Although the bandwidth of the storage I/O typically dictates the performance of a conventional DBMS, network-I/O bandwidth limitation is the main operating constraint of most multimedia database systems. In spite of the fact that the throughput of a public network (e.g. ATM) can be huge, the network-I/O bottleneck limits the number of client stations a media server can support simultaneously. A possible solution to this problem is to batch requests for the same video and multicast the data to these requesters to save the network I/O bandwidth. A disadvantage of this scheme is that it unfairly forces requests arriving early in a batch to wait for the latecomers. As a result, the reneging rate can be high in a system which employs this technique. To reduce the long access latency, we examine in this paper a new batching mechanism called chaining. This approach allows the server to serve a ""chain"" of client stations using a single data stream. The idea is to pipeline the data stream through the chain of stations. Requests arriving early in a chain (virtual batch), therefore, do not have to experience long delays as in conventional batching. Our simulation results based on an ATM networking environment indicate that very significant performance improvement over batching can be obtained.","Streaming media,
Bandwidth,
Network servers,
Delay,
Video sharing,
Buffer storage,
Computer science,
Multimedia databases,
Throughput,
Pipelines"
Rotation scheduling: a loop pipelining algorithm,"We consider the resource-constrained scheduling of loops with interiteration dependencies. A loop is modeled as a data flow graph (DFG), where edges are labeled with the number of iterations between dependencies. We design a novel and flexible technique, called rotation scheduling, for scheduling cyclic DFGs using loop pipelining. The rotation technique repeatedly transforms a schedule to a more compact schedule. We provide a theoretical basis for the operations based on retiming. We propose two heuristics to perform rotation scheduling and give experimental results showing that they have very good performance.","Pipeline processing,
Scheduling algorithm,
Processor scheduling,
Delay,
Chaos,
Flow graphs,
High level synthesis,
Computer science,
Signal processing algorithms,
Real time systems"
A wavelet-based multiresolution regularized least squares reconstruction approach for optical tomography,"The authors present a wavelet-based multigrid approach to solve the perturbation equation encountered in optical tomography. With this scheme, the unknown image, the data, as well as the weight matrix are all represented by wavelet expansions, thus yielding a multiresolution representation of the original perturbation equation in the wavelet domain. This transformed equation is then solved using a multigrid scheme, by which an increasing portion of wavelet coefficients of the unknown image are solved in successive approximations. One can also quickly identify regions of interest (ROI's) from a coarse level reconstruction and restrict the reconstruction in the following fine resolutions to those regions. At each resolution level a regularized least squares solution is obtained using the conjugate gradient descent method. This approach has been applied to continuous wave data calculated based on the diffusion approximation of several two-dimensional (2-D) test media. Compared to a previously reported one grid algorithm, the multigrid method requires substantially shorter computation time under the same reconstruction quality criterion.","Least squares methods,
Tomography,
Image reconstruction,
Equations,
Wavelet domain,
Least squares approximation,
Image resolution,
Wavelet coefficients,
Two dimensional displays,
Testing"
Adaptation in evolutionary computation: a survey,"Adaptation of parameters and operators is one of the most important and promising areas of research in evolutionary computation; it tunes the algorithm to the problem while solving the problem. In this paper we develop a classification of adaptation on the basis of the mechanisms used, and the level at which adaptation operates within the evolutionary algorithm. The classification covers all forms of adaptation in evolutionary computation and suggests further research.","Evolutionary computation,
Computer science,
Computer errors,
Cost function,
State feedback,
Biological cells,
Genetic mutations,
Encoding"
On accommodating mobile hosts in an integrated services packet network,"This paper considers the support of real-time applications to mobile hosts in an integrated services packet network. We have proposed a service model for mobile hosts that can support adaptive applications which can withstand a wide range of available bandwidth, as well as applications which require mobility independent service guarantees. We describe an admission control scheme and a reservation protocol for implementing this service model. Our admission control scheme achieves high utilization of network resources.","Intelligent networks,
Intserv networks,
Mobile computing,
Admission control,
Protocols,
Delay,
Traffic control,
Quality of service,
Scheduling algorithm,
Computer science"
Shape-based tracking of left ventricular wall motion,"An approach for tracking and quantifying the nonrigid, nonuniform motion of the left ventricular (LV) endocardial wall from two-dimensional (2-D) cardiac image sequences, on a point-by-point basis over the entire cardiac cycle, is presented. Given a set of boundaries, motion computation involves first matching local segments on one contour to segments on the next contour in the sequence using a shape-based strategy. Results from the match process are incorporated with a smoothness term into an optimization functional. The global minimum of this functional is found, resulting in a smooth flow field that is consistent with the match data. The computation is performed for all pairs of frames in the temporal sequence and equally sampled points on one contour are tracked throughout the sequence, resulting in a composite flow field over the entire sequence. Two perspectives on characterizing the optimization functional are presented which result in a tradeoff resolved by the confidence in the initial boundary segmentation. Experimental results for contours derived from diagnostic image sequences of three different imaging modalities are presented. A comparison of trajectory estimates with trajectories of gold-standard markers implanted in the LV wall are presented for validation. The results of this comparison confirm that although cardiac motion is a three-dimensional (3-D) problem, two-dimensional (2-D) analysis provides a rich testing ground for algorithm development.","Tracking,
Magnetic resonance imaging,
Image sequences,
Image segmentation,
Image motion analysis,
Computed tomography,
Heart,
Two dimensional displays,
Optical imaging,
X-ray imaging"
Optimal suffix tree construction with large alphabets,"The suffix tree of a string is the fundamental data structure of combinatorial pattern matching. Weiner (1973), who introduced the data structure, gave an O(n)-time algorithm for building the suffix tree of an n-character string drawn from a constant size alphabet. In the comparison model, there is a trivial /spl Omega/(n log n)-time lower bound based on sorting, and Weiner's algorithm matches this bound trivially. For integer alphabets, a substantial gap remains between the known upper and lower bounds, and closing this gap is the main open question in the construction of suffix trees. There is no super-linear lower bound, and the fastest known algorithm was the O(n log n) time comparison based algorithm. We settle this open problem by closing the gap: we build suffix trees in linear time for integer alphabet.","Sorting,
Tree data structures,
Pattern matching,
Buildings,
Data structures,
Computer science,
World Wide Web,
Engineering profession,
Labeling,
Upper bound"
VERIFY: evaluation of reliability using VHDL-models with embedded fault descriptions,"A new technique for reliability evaluation of digital systems will be presented by demonstrating the functionality and usage of the simulation based fault injector VERIFY (VHDL-based Evaluation of Reliability by Injecting Faults efficientlY). This software tool introduces a new way for describing the behavior of hardware components in case of faults by extending the VHDL language with fault injection signals together with their rate of occurrence. The accuracy of the results is obtained by using the same VHDL-models which have been developed during conventional phases of hardware design. For demonstrating the capabilities of VERIFY, a VHDL-model of a simple 32-bit processor (DP32) will be used as an example to illustrate the several steps of reliability evaluation.","Hardware,
Digital systems,
Software tools,
Computer science,
Computational modeling,
Computer simulation,
System testing,
Observability,
Controllability,
Pins"
Automated analysis of cryptographic protocols using Mur/spl phi/,"A methodology is presented for using a general-purpose state enumeration tool, Mur/spl phi/, to analyze cryptographic and security-related protocols. We illustrate the feasibility of the approach by analyzing the Needham-Schroeder (1978) protocol, finding a known bug in a few seconds of computation time, and analyzing variants of Kerberos and the faulty TMN protocol used in another comparative study. The efficiency of Mur/spl phi/ also allows us to examine multiple terms of relatively short protocols, giving us the ability to detect replay attacks, or errors resulting from confusion between independent execution of a protocol by independent parties.","Cryptographic protocols,
Cryptography,
NASA,
Computer science,
Public key,
Authentication,
Failure analysis,
Contracts,
US Government,
Documentation"
Computerized tumor boundary detection using a Hopfield neural network,"The authors present a new approach for detection of brain tumor boundaries in medical images using a Hopfield neural network. The boundary detection problem is formulated as an optimization process that seeks the boundary points to minimize an energy functional based on an active contour model. A modified Hopfield network is constructed to solve the optimization problem. Taking advantage of the collective computational ability and energy convergence capability of the Hopfield network, the authors' method produces the results comparable to those of standard ""snakes""-based algorithms, but it requires less computing time. With the parallel processing potential of the Hopfield network, the proposed boundary detection can be implemented for real time processing. Experiments on different magnetic resonance imaging (MRI) data sets show the effectiveness of the authors' approach.","Computer networks,
Neoplasms,
Hopfield neural networks,
Magnetic resonance imaging,
Active contours,
Convergence,
Biological neural networks,
Biomedical imaging,
Data mining,
Computed tomography"
Multilevel Circuit Partitioning,,"Circuits,
Iterative algorithms,
Permission,
Clustering algorithms,
Partitioning algorithms,
Iterative methods,
Design automation,
Computer science,
Laboratories,
Technological innovation"
Improving performance of TCP over wireless networks,"Transmission Control Protocol (TCP) assumes a relatively reliable underlying network where most packet losses are due to congestion. In a wireless network, however, packet losses will occur more often due to unreliable wireless links than due to congestion. When using TCP over wireless links, each packet loss on the wireless link results in congestion control measures being invoked at the source. This causes severe performance degradation. In this paper, we study the effect of: burst errors on wireless links; packet size variation on the wired network; local error recovery by the base station; and explicit feedback by the base station, on the performance of TCP over wireless networks. It is shown that the performance of TCP is sensitive to the packet size, and that significant performance improvements are obtained if a good packet size is used. While local recovery by the base station using link-level retransmissions is found to improve performance, timeouts can still occur at the source, causing redundant packet retransmissions. We propose an explicit feedback mechanism, to prevent these timeouts during local recovery. Results indicate significant performance improvements when explicit feedback from the base station is used. A major advantage of our approaches over existing proposals is that no state maintenance is required at any intermediate host. Experiments are performed using the Network Simulator (NS) from Lawrence Berkeley Labs. The simulator has been extended to incorporate wireless link characteristics.","Wireless networks,
Base stations,
Internet,
Degradation,
Feedback,
Computer science,
Electronic mail,
Protocols,
Computer network reliability,
Propagation losses"
Fully Bayesian estimation of Gibbs hyperparameters for emission computed tomography data,"In recent years, many investigators have proposed Gibbs prior models to regularize images reconstructed from emission computed tomography data. Unfortunately, hyperparameters used to specify Gibbs priors can greatly influence the degree of regularity imposed by such priors and, as a result, numerous procedures have been proposed to estimate hyperparameter values, from observed image data. Many of these, procedures attempt to maximize the joint posterior distribution on the image scene. To implement these methods, approximations to the joint posterior densities are required, because the dependence of the Gibbs partition function on the hyperparameter values is unknown. Here, the authors use recent results in Markov chain Monte Carlo (MCMC) sampling to estimate the relative values of Gibbs partition functions and using these values, sample from joint posterior distributions on image scenes. This allows for a fully Bayesian procedure which does not fix the hyperparameters at some estimated or specified value, but enables uncertainty about these values to be propagated through to the estimated intensities. The authors utilize realizations from the posterior distribution for determining credible regions for the intensity of the emission source. The authors consider two different Markov random field (MRF) models-the power model and a line-site model. As applications they estimate the posterior distribution of source intensities from computer simulated data as well as data collected from a physical single photon emission computed tomography (SPECT) phantom.","Bayesian methods,
Computed tomography,
Layout,
Image reconstruction,
Monte Carlo methods,
Image sampling,
Uncertainty,
Markov random fields,
Application software,
Distributed computing"
Body plans,"This paper describes a representation for people and animals, called a body plan, which is adapted to segmentation and to recognition in complex environments. The representation is an organized collection of grouping hints obtained from a combination of constraints on color and texture and constraints on geometric properties such as the structure of individual parts and the relationships between parts. Body plans can be learned from image data, using established statistical learning techniques. The approach is illustrated with two examples of programs that successfully use body plans for recognition: one example involves determining whether a picture contains a scantily clad human, using a body plan built by hand; the other involves determining whether a picture contains a horse, using a body plan learned from image data. In both cases, the system demonstrates excellent performance on large, uncontrolled test sets and very large and diverse control sets.","Image segmentation,
Animals,
Content based retrieval,
Assembly,
Computer science,
Horses,
Object recognition,
Computer vision,
Information retrieval,
Image retrieval"
Aided and automatic target recognition based upon sensory inputs from image forming systems,"This paper systematically reviews 10 years of research that several Army Laboratories conducted in object recognition algorithms, processors, and evaluation techniques. In the military, object recognition is applied to the discrimination of military targets, ranging from human-aided to autonomous operations, and is called automatic target recognition (ATR). The research described here has been concentrated in human-aided target recognition applications, but some attention has been paid to automatic processes. Definitions and performance metrics that have been developed are described along with performance data showing the present state-of-the-art. The effects of signal-to-noise and clutter parameters are indicated in the data. Multisensor fusion and model-based algorithms are discussed as the latest techniques under consideration by the military research community. The results demonstrate that useful performance can be achieved, and tools are evolving to understand and improve the performance under real-world conditions. The referenced research strongly indicates the need for the development of image science, as described in the paper, to support the theoretical underpinnings of ATR.","Target recognition,
Humans,
Signal processing algorithms,
High-resolution imaging,
Optical imaging,
Image sensors,
Optical sensors,
Biomedical imaging,
Sensor arrays,
Object recognition"
"Effects Of Communication Latency, Overhead, And Bandwidth In A Cluster Architecture",,"Delay,
Bandwidth,
Computer architecture,
Application software,
Permission,
Communication system control,
Computer science,
Workstations,
Network interfaces,
Sun"
Storing and retrieving software components: a refinement based system,"Software libraries are repositories which contain software components; as such, they represent a precious resource for the software engineer. As software libraries grow in size, it becomes increasingly difficult to maintain adequate precision and recall with informal retrieval algorithms. In this paper, we discuss the design and implementation of a storage and retrieval structure for software components that is based on formal specifications and on the refinement ordering between specifications.","Software libraries,
Formal specifications,
Software algorithms,
Information science,
Application software,
Information retrieval,
Software maintenance,
Humans,
Maintenance engineering,
Computer languages"
Towards a mathematical operational semantics,"We present a categorical theory of 'well-behaved' operational semantics which aims at complementing the established theory of domains and denotational semantics to form a coherent whole. It is shown that, if the operational rules of a programming language can be modelled as a natural transformation of a suitable general form, depending on functorial notions of syntax and behaviour, then one gets the following for free: an operational model satisfying the rules and a canonical, internally fully abstract denotational model which satisfies the operational rules. The theory is based on distributive laws and bialgebras; it specialises to the known classes of well-behaved rules for structural operational semantics, such as GSOS.","Algebra,
Computer science,
Computer languages,
Mathematical model,
Laboratories,
Equations"
Broadcast on demand: efficient and timely dissemination of data in mobile environments,"The demand for efficient scalable and cost effective mobile information access systems is rapidly growing. Radiofrequency broadcast plays a major role in mobile computing, and there is a need to provide service models for broadcasting information according to mobile users' needs. The authors present a model called broadcast on demand (BoD), which provides timely broadcasts according to requests from users. Compared to static broadcast, this approach has a different emphasis: it is based on a demand driven framework, aimed at satisfying the temporal constraints of the requests, and uses scheduling techniques at the server side to utilize the limited bandwidth dynamically and efficiently. Several broadcast transmission scheduling policies for BoD are examined. The study indicates that EDF-based policies combined with batching of requests achieve good performance. The results show that BoD is successful in satisfying the temporal constraints of the requests and is a viable service model for wireless broadcast stations.","Board of Directors,
Mobile computing,
Radio broadcasting,
Job shop scheduling,
Bandwidth,
Costs,
Power system reliability,
Wireless communication,
Radio frequency,
Computer science"
Novel view synthesis in tensor space,We present a new method for synthesizing novel views of a 3D scene from few model images in full correspondence. The core of this work is the derivation of a tensorial operator that describes the transformation from a given tensor of three views to a novel tensor of a new configuration of three views. By repeated application of the operator on a seed tensor with a sequence of desired virtual camera positions we obtain a chain of warping functions (tensors) from the set of model images to create the desired virtual views.,"Tensile stress,
Cameras,
Interpolation,
Layout,
Rendering (computer graphics),
Computer science,
Image motion analysis,
Engines,
Image generation,
Extrapolation"
Alternating-time temporal logic,"Temporal logic comes in two varieties: linear-time temporal logic assumes implicit universal quantification over all paths that are generated by system moves; branching-time temporal logic allows explicit existential and universal quantification over all paths. We introduce a third, more general variety of temporal logic: alternating-time temporal logic offers selective quantification over those paths that are possible outcomes of games, such as the game in which the system and the environment alternate moves. While linear-time and branching-time logics are natural specification languages for closed systems, alternating-time logics are natural specification languages for open systems. For example, by preceding the temporal operator ""eventually"" with a selective path quantifier, we can specify that in the game between the system and the environment, the system has a strategy to reach a certain state. Also the problems of receptiveness, realizability, and controllability can be formulated as model-checking problems for alternating-time formulas.","Logic,
Open systems,
Power system modeling,
Specification languages,
Controllability,
Costs,
Engineering profession,
Contracts,
Information science,
Debugging"
A scheme for scheduling hard real-time applications in open system environment,"The paper focuses on the problem of providing run-time support to real-time applications and non-real-time applications in an open system. It describes a two-level hierarchical priority-driven scheme for scheduling independently developed applications. The scheme allows the developer of each real-time application to validate the schedulability of the application independently of other applications. Once a real-time application is created and accepted by the open system, its schedulability is guaranteed regardless of the behaviors of other applications that execute concurrently in the system.",
Generating ROC curves for artificial neural networks,"Receiver operating characteristic (ROC) analysis is an established method of measuring diagnostic performance in medical imaging studies. Traditionally, artificial neural networks (ANN's) have been applied as a classifier to find one ""best"" detection rate. Recently researchers have begun to report ROC curve results for ANN classifiers. The current standard method of generating ROC curves for an ANN is to vary the output node threshold for classification. Here, the authors propose a different technique for generating ROC curves for a two class ANN classifier. They show that this new technique generates better ROC curves in the sense of having greater area under the ROC curve (AUC), and in the sense of being composed of a better distribution of operating points.","Artificial neural networks,
Biomedical imaging,
Image analysis,
Performance analysis,
Computer science,
Neural networks,
Power engineering and energy,
Character generation"
Texture fusion and feature selection applied to SAR imagery,"The discrimination ability of four different methods for texture computation in ERS SAR imagery is examined and compared. Feature selection methodology and discriminant analysis are applied to find the optimal combination of texture features. By combining features derived from different texture models, the classification accuracy increased significantly.","Image texture analysis,
Pixel,
Backscatter,
Frequency estimation,
Synthetic aperture radar,
Image texture,
Statistics,
Fractals,
Councils,
Computer science"
A literature survey on traffic dispersion,"Aggregation of resources is a means to improve performance and efficiency in statistically shared systems in general, and communication networks in particular. One approach to this is traffic dispersion, which means that the traffic from a source is spread over multiple paths and transmitted in parallel through the network. Traffic dispersion may help in utilizing network resources to their full potential, while providing quality-of-service guarantees. It is a topic gaining interest, and much work has been done in the field. The results are, however, difficult to find, since the technique appears under many different labels. This article is therefore an attempt to gather and report on the work done on traffic dispersion in communication networks. It looks at a specific instance of this general method. The processes are communication processes, and the resource is the link capacity in a packet switched network.","Telecommunication traffic,
Central Processing Unit,
Computer science,
Communication networks,
Random access memory,
Physics,
Production,
Power generation economics,
Packet switching,
Probability"
Speciation as automatic categorical modularization,"Many natural and artificial systems use a modular approach to reduce the complexity of a set of subtasks while solving the overall problem satisfactorily. There are two distinct ways to do this. In functional modularization, the components perform very different tasks, such as subroutines of a large software project. In categorical modularization, the components perform different versions of basically the same task, such as antibodies in the immune system. This second aspect is the more natural for acquiring strategies in games of conflict, An evolutionary learning system is presented which follows this second approach to automatically create a repertoire of specialist strategies for a game-playing system. This relieves the human effort of deciding how to divide and specialize. The genetic algorithm speciation method used is one based on fitness sharing. The learning task is to play the iterated prisoner's dilemma. The learning system outperforms the tit-for-tat strategy against unseen test opponents. It learns using a ""black box"" simulation, with minimal prior knowledge of the learning task.","Humans,
Learning systems,
Immune system,
Genetic algorithms,
Computer science,
Associate members,
Software performance,
System testing,
Machine learning,
Artificial neural networks"
Automatic tracking of the aorta in cardiovascular MR images using deformable models,"Presents a new algorithm for the robust and accurate tracking of the aorta in cardiovascular magnetic resonance (MR) images. First, a rough estimate of the location and diameter of the aorta is obtained by applying a multiscale medial-response function using the available a priori knowledge. Then, this estimate is refined using an energy-minimizing deformable model which the authors define in a Markov-random-field (MRF) framework. In this context, the authors propose a global minimization technique based on stochastic relaxation. Simulated annealing (SA), which is shown to be superior to other minimization techniques, for minimizing the energy of the deformable model. The authors have evaluated the performance and robustness of the algorithm on clinical compliance studies in cardiovascular MR images. The segmentation and tracking has been successfully tested in spin-echo MR images of the aorta. The results show the ability of the algorithm to produce not only accurate, but also very reliable results in clinical routine applications.","Biomedical imaging,
Cardiology,
Deformable models,
Image segmentation,
Robustness,
Magnetic resonance,
Simulated annealing,
Volume measurement,
Optimization methods,
Stochastic processes"
Path-based next trace prediction,"The trace cache is proposed as a mechanism for providing increased fetch bandwidth by allowing the processor to fetch across multiple branches in a single cycle. But to date predicting multiple branches per cycle has meant paying a penalty in prediction accuracy. We propose a next trace predictor that treats the traces as basic units and explicitly predicts sequences of traces. The predictor collects histories of trace sequences (paths) and makes predictions based on these histories. The basic predictor is enhanced to a hybrid configuration that reduces performance losses due to cold starts and aliasing in the prediction table. The Return History Stack is introduced to increase predictor performance by saving path history information across procedure call/returns. Overall, the predictor yields about a 26% reduction in misprediction rates when compared with the most aggressive previously proposed, multiple branch prediction methods.","History,
Jacobian matrices,
Computer science,
Bandwidth,
Accuracy,
Performance loss,
Prediction methods,
Parallel processing,
Proposals,
Assembly"
Multidimensional detective,"The display of multivariate datasets in parallel coordinates, transforms the search for relations among the variables into a 2-D pattern recognition problem. This is the basis for the application to visual data mining. The knowledge discovery process together with some general guidelines are illustrated on a dataset from the production of a VLSI chip. The special strength of parallel coordinates is in modeling relations. As an example, a simplified economic model is constructed with data from various economic sectors of a real country. The visual model shows the interelationship and dependencies between the sectors, circumstances where there is competition for the same resource, and feasible economic policies. Interactively, the model can be used to do trade-off analyses, discover sensitivities, do approximate optimization, monitor (as in a process) and provide decision support.","Multidimensional systems,
Data mining,
Guidelines,
Computer science,
Computer displays,
Pattern recognition,
Application software,
Production,
Very large scale integration,
Monitoring"
Active mobile robot localization by entropy minimization,"Localization is the problem of determining the position of a mobile robot from sensor data. Most existing localization approaches are passive, i.e., they do not exploit the opportunity to control the robot's effecters during localization. This paper proposes an active localization approach. The approach provides rational criteria for (1) setting the robot's motion direction (exploration), and (2) determining the pointing direction of the sensors so as to most efficiently localize the robot. Furthermore, it is able to deal with noisy sensors and approximate world models. The appropriateness of our approach is demonstrated empirically using a mobile robot in a structured office environment.","Mobile robots,
Entropy,
Robot sensing systems,
Robot control,
Computer science,
Actuators,
Navigation,
Learning,
Orbital robotics,
Working environment noise"
Analysis of skin erythema using true-color images,"This article presents a new method for analyzing the spreading of skin erythemas. These occur as a result of the cutaneous vascular axon reflex which can be evoked by a noxious stimulation of the skin. Series of true-color images of the observed skin patch were recorded using a video camera. The images were digitized and stored on computer disk. The delineation of the reddening was segmented for every image of the sequence by a newly developed image processing method. Each image taken after the noxious stimulation was compared with the baseline before the stimulation and each image point was classified as: ""unchanged"" or ""changed skin color."" To improve the classification the CIE L/sup */a/sup */b/sup */ color space was used. The boundaries of the erythema were extracted from the resulting binary images. Every image of a sequence was analyzed in the same way in order to follow the time course of the flare response. The erythema reaction could be determined in an objective way using this methods. The automatically detected flare sizes were independent of human observers and had a high spatial and temporal resolution. It was used for a crossover study to assess the power of new drugs which modify the blood flow of the skin induced by an intradermal histamine application.","Image analysis,
Skin,
Nerve fibers,
Cameras,
Image segmentation,
Image processing,
Image sequence analysis,
Humans,
Spatial resolution,
Drugs"
A corpus for the evaluation of lossless compression algorithms,"A number of authors have used the Calgary corpus of texts to provide empirical results for lossless compression algorithms. This corpus was collected in 1987, although it was not published until 1990. The advances with compression algorithms have been achieving relatively small improvements in compression, measured using the Calgary corpus. There is a concern that algorithms are being fine-tuned to this corpus, and that small improvements measured in this way may not apply to other files. Furthermore, the corpus is almost ten years old, and over this period there have been changes in the kinds of files that are compressed, particularly with the development of the Internet, and the rapid growth of high-capacity secondary storage for personal computers. We explore the issues raised above, and develop a principled technique for collecting a corpus of test data for compression methods. A corpus, called the Canterbury corpus, is developed using this technique, and we report the performance of a collection of compression methods using the new corpus.","Compression algorithms,
Entropy,
Decoding,
Computer science,
Convergence,
Encoding,
Internet,
Microcomputers,
Testing"
The T experiments: errors in scientific software,"Extensive tests showed that many software codes widely used in science and engineering are not as accurate as we would like to think. It is argued that better software engineering practices would help solve this problem, but realizing that the problem exists is an important first step.","Software measurement,
Aerodynamics,
Software testing,
Laboratories,
Computer industry,
Area measurement,
Application software,
Radio access networks,
Floating-point arithmetic,
Fault detection"
Charge collection from ion tracks in simple EPI diodes,"Charge collection from ion tracks in epi diodes is investigated. As previously noted by others, the collected charge can exceed the charge liberated in the epi layer. Fundamental concepts are discussed and illustrated by computer simulation results. It is found that the n/sup +/-p-p/sup +/ diode displays a funneling regime and a diffusion regime (as previously noted by others), but the p/sup +/-n-n/sup +/ diode does not display separate regimes. Simple quantitative models are provided which agree fairly well with computer simulation results. Experimental data were compared to one of the models, and good agreement was found.","Diodes,
Carbon capture and storage,
Computer simulation,
Doping,
Computer displays,
Propulsion,
Space technology,
Semiconductor process modeling,
Scattering,
Charge carrier processes"
Automatic detection of the mid-sagittal plane in 3-D brain images,"This article presents a detailed description of an algorithm for the automatic detection of the mid-sagittal plane in three-dimensional (3-D) brain images. The algorithm seeks the plane with respect to which the image exhibits maximum symmetry. For a given plane, symmetry is measured by the cross-correlation between the image sections lying on either side. The search for the plane of maximum symmetry is performed by using a multiresolution approach which substantially decreases computational time. The choice of the starting plane was found to be an important issue in optimization. A method for selecting the initial plane is presented. The algorithm has been tested on brain images from various imaging modalities in both humans and animals. Results were evaluated by visual inspection by neuroradiologists and were judged to be consistently correct.","Brain,
Biomedical imaging,
Positron emission tomography,
Image edge detection,
Image registration,
Radiology,
Nuclear medicine,
Blood vessels,
Spatial resolution,
Testing"
Buy-at-bulk network design,"The essence of the simplest buy-at-bulk network design problem is buying network capacity ""wholesale"" to guarantee connectivity from all network nodes to a certain central network switch. Capacity is sold with ""volume discount"": the more capacity is bought, the cheaper is the price per unit of bandwidth. We provide O(log/sup 2/n) randomized approximation algorithm for the problem. This solves the open problem in Salman et al. (1997). The only previously known solutions were restricted to special cases (Euclidean graphs). We solve additional natural variations of the problem, such as multi-sink network design, as well as selective network design. These problems can be viewed as generalizations of the the Generalized Steiner Connectivity and Prize-collecting salesman (K-MST) problems. In the selective network design problem, some subset of /spl kappa/ wells must be connected to the (single) refinery, so that the total cost is minimized.","Pricing,
Petroleum,
Contracts,
Bandwidth,
Refining,
Cost function,
Computer science,
Switches,
Economies of scale,
Capacity planning"
Titan: a high-performance remote-sensing database,"There are two major challenges for a high performance remote sensing database. First, it must provide low latency retrieval of very large volumes of spatio temporal data. This requires effective declustering and placement of a multidimensional dataset onto a large disk farm. Second, the order of magnitude reduction in data size due to post processing makes it imperative, from a performance perspective, that the post processing be done on the machine that holds the data. This requires careful coordination of computation and data retrieval. The paper describes the design, implementation and evaluation of Titan, a parallel shared nothing database designed for handling remote sensing data. The computational platform for Titan is a 16 processor IBM SP-2 with four fast disks attached to each processor. Titan is currently operational and contains about 24 GB of AVHRR data from the NOAA-7 satellite. The experimental results show that Titan provides good performance for global queries and interactive response times for local queries.","Remote sensing,
Databases,
Information retrieval,
Delay,
Satellites,
Earth,
Pixel,
Moon,
Electric shock,
Computer science"
Wound status evaluation using color image processing,"An accurate diagnosis of burns and pressure ulcers in the early stages can be made by computerized image processing. This study describes a critical assessment of potential methodologies for noninvasive wound evaluation using a color imaging system. The authors also developed a method for quantifying histological readings and applied these techniques to a porcine animal model of wound formation. Differences in calibrated hue between injured and noninjured skin provided a repeatable differentiation of wound severity for situations when the time of injury was known. This color analysis allowed statistically significant differentiation of mild, moderate, and severe injuries within 30 minutes after the application of the injury. It was more difficult to distinguish wound severity one to four days later, however the correlation re-emerged when the wounds were five to seven days old. This technique could be adapted for assessing and tracking wound severity in humans in a clinical setting.","Wounds,
Injuries,
Skin,
Image color analysis,
Biomedical engineering,
Humans,
Medical services,
Physiology,
Dermis,
Mechanical engineering"
A simple method for automatically locating the nipple on mammograms,"This paper outlines a simple, fast, and accurate method for automatically locating the nipple on digitized mammograms that have been segmented to reveal the skin-air interface. If the average gradient of the intensity is computed in the direction normal to the interface and directed inside the breast, it is found that there is a sudden and distinct change in this parameter close to the nipple. A nipple in profile is located between two successive maxima of this parameter; otherwise, it is near the global maximum. Specifically, the nipple is located midway between a successive maximum and minimum of the derivative of the average intensity gradient; these being local turning points for a nipple in profile and global otherwise. The method has been tested on 24 images, including both oblique and cranio-caudal views, from two digital mammogram databases. For 23 of the images (96%), the rms error was less than 1 mm at image resolutions of 400 /spl mu/m and 420 /spl mu/m per pixel. Because of its simplicity, and because it is based both on the observed behavior of mammographic tissue intensities and on geometry, this method has the potential to become a generic method for locating the nipple on mammograms.","Breast,
Biomedical engineering,
Testing,
Image databases,
Cancer detection,
Australia,
Intelligent systems,
Information processing,
Computer interfaces,
Turning"
Multi-path routing combined with resource reservation,"In high-speed networks it is desirable to interleave routing and resource (such as bandwidth) reservation. The PNNI standard for private ATM networks is an example of an algorithm that does this using a sequential crank-back mechanism. We suggest the implementation of resource reservation along several routes in parallel. We present an analytical model that demonstrates that when there are several routes to the destination it pays to attempt reservation along more than a single route. Following this analytic observation, we present a family of algorithms that route and reserve resources along parallel subroutes. The algorithms of the family represent different trade-offs between the speed and the quality of the established route. The presented algorithms are simulated against several legacy algorithms, including the PNNI crank-back, and exhibit higher network utilization and faster connection set-up time.","Routing,
Sun,
Electronic mail,
High-speed networks,
Bandwidth,
B-ISDN,
Availability,
Read only memory,
Computer science,
Analytical models"
The Rocky 7 rover: a Mars sciencecraft prototype,"This paper describes the design and implementation at the Jet Propulsion Laboratory of a small rover for future Mars missions requiring long traverses and rover-based science experiments. The small rover prototype, called Rocky 7, is capable of long traverses, autonomous navigation, and science instrument control. This rover carries three science instruments, and can be commanded from any computer platform from any location using the World Wide Web. In this paper we describe the mobility system, the sampling system, the sensor suite, navigation and control, onboard science instruments, and the ground command and control system. We also present key accomplishments of a recent field test of Rocky 7 in the Mojave Desert in California.","Mars,
Prototypes,
Instruments,
Propulsion,
Laboratories,
Aircraft navigation,
Web sites,
Sampling methods,
Sensor phenomena and characterization,
Sensor systems"
Value profiling,"Identifying variables as invariant or constant at compile-time allows the compiler to perform optimizations including constant folding, code specialization, and partial evaluation. Some variables, which cannot be labeled as constants, may exhibit semi-invariant behavior. A semi-invariant variable is one that cannot be identified as a constant at compile-time, but has a high degree of invariant behavior at run-time. If run-time information was available to identify these variables as semi-invariant, they could then benefit from invariant-based compiler optimizations. We examine the invariance found from profiling instruction values, and show that many instructions have semi-invariant values even across different inputs. We also investigate the ability to estimate the invariance for all instructions in a program from only profiling load instructions. In addition, we propose a new type of profiling called convergent profiling. Estimating the invariance from loads and convergent profiling are used to reduce the profiling time needed to generate an accurate value profile. The value profile can then be used to automatically guide code generation for dynamic compilation, adaptive execution, code specialization, partial evaluation and other compiler optimizations.","Optimizing compilers,
Runtime,
Hardware,
Computer science,
Dynamic compiler,
Feedback,
Program processors,
Steady-state"
Path coupling: A technique for proving rapid mixing in Markov chains,"The main technique used in algorithm design for approximating #P-hard counting problems is the Markov chain Monte Carlo method. At the heart of the method is the study of the convergence (mixing) rates of particular Markov chains of interest. In this paper we illustrate a new approach to the coupling technique, which we call path coupling, for bounding mixing rates. Previous applications of coupling have required detailed insights into the combinatorics of the problem at hand, and this complexity can make the technique extremely difficult to apply successfully. Path coupling helps to minimize the combinatorial difficulty and in all cases provides simpler convergence proofs than does the standard coupling method. However the true power of the method is that the simplification obtained may allow coupling proofs which were previously unknown, or provide significantly better bounds than those obtained using the standard method. We apply the path coupling method to several hard combinatorial problems, obtaining new or improved results. We examine combinatorial problems such as graph colouring and TWICE-SAT, and problems from statistical physics, such as the antiferromagnetic Potts model and the hard-core lattice gas model. In each case we provide either a proof of rapid mixing where none was known previously, or substantial simplification of existing proofs with consequent gains in the performance of the resulting algorithms.","Physics,
Combinatorial mathematics,
Convergence,
Algorithm design and analysis,
Heart,
Lattices,
Computer science,
Polynomials,
Stress"
Automatic registration and alignment on a template of cardiac stress and rest reoriented SPECT images,"Single photon emission computed tomography (SPECT) imaging with /sup 201/Tl or /sup 99m/Tc agent is used to assess the location or the extent of myocardial infarction or ischemia. A method is proposed to decrease the effect of operator variability in the visual or quantitative interpretation of scintigraphic myocardial perfusion studies. To effect this, the patient's myocardial images (target cases) are registered automatically over a template image, utilizing a nonrigid transformation. The intermediate steps are: 1) Extraction of feature points in both stress and rest three-dimensional (3-D) images. The images are resampled in a polar geometry to detect edge points, which in turn are filtered by the use of a priori constraints. The remaining feature points are assumed to be points on the edges of the left ventricular myocardium. 2) Registration of stress and rest images with a global affine transformation. The matching method is an adaptation of the iterative closest point algorithm. 3) Registration and morphological matching of both stress and rest images on a template using a nonrigid local spline transformation following a global affine transformation. 4) Resampling of both stress and rest images in the geometry of the template. Optimization of the method was performed on a database of 40 pairs of stress and rest images selected to obtain a wide variation of images and abnormalities. Further testing was performed on 250 cases selected from the same database on the basis of the availability of angiographic results and patient stratification.","Stress,
Myocardium,
Image edge detection,
Geometry,
Image databases,
Spatial databases,
Single photon emission computed tomography,
Ischemic pain,
Feature extraction,
Iterative closest point algorithm"
Lane boundary detection using a multiresolution Hough transform,"Lane boundary detection is the problem of estimating the geometric structure of the lane boundaries of a road based on the images grabbed by a camera on board a vehicle. We use the Hough transform to detect lane boundaries with a parabolic model under a variety of road pavement types, lane structures and weather conditions. In the three-dimensional Hough space, a parabolic curve is represented as a straight line. To simplify the computation, the parametric space can be divided into (i) a two-dimensional space measured by the parameters which are shared by all the lane edges, and (ii) a one-dimensional space of the parameter which makes a distinction among different edges in an image. A multiresolution strategy is used to improve both the speed and accuracy of the Hough transform. Experimental results show that the proposed method is relatively less prone to the image noise and is computationally tractable.","Vehicle detection,
Cameras,
Road vehicles"
The design of an animal PET: flexible geometry for achieving optimal spatial resolution or high sensitivity,"Presents the design of a positron emission tomograph (PET) with flexible geometry dedicated to in vivo studies of small animals (TierPET). The scanner uses two pairs of detectors. Each detector consists of 400 small individual yttrium aluminum perovskite (YAP) scintillator crystals of dimensions 2/spl times/2/spl times/15 mm/sup 3/, optically isolated and glued together, which are coupled to position-sensitive photomultiplier tubes (PSPMTs). The detector modules can be moved in a radial direction so that the detector-to-detector spacing can be varied. Special hardware has been built for coincidence detection, position detection, and real-time data acquisition, which is performed by a PC. The single-event data are transferred to workstations where the radioactivity distribution is reconstructed. The dimensions of the crystals and the detector layout are the result of extensive simulations which are described in this report, taking into account sensitivity, spatial resolution and additional parameters like parallax error or scatter effects. For the three-dimensional (3-D) reconstruction a genuine 3-D expectation-maximization (EM)-algorithm which can include the characteristics of the detector system has been implemented. The reconstruction software is flexible and matches the different detector configurations. The main advantage of the proposed animal PET scanner is its high flexibility, allowing the realization of various detector-system configurations. By changing the detector-to-detector spacing, the system is capable of either providing good spatial resolution or high sensitivity for dynamic studies of pharmacokinetics.","Animals,
Positron emission tomography,
Geometry,
Spatial resolution,
Detectors,
Crystals,
Optical scattering,
Radioactive decay,
Geometrical optics,
In vivo"
An optimal rotator for iterative reconstruction,"For implementations of iterative reconstruction algorithms that rotate the image matrix, the characteristics of the rotator may affect the reconstruction quality. Desirable qualities for the rotator include: (1) preservation of global and local image counts; (2) accurate count positioning; (3) a uniform and predictable amount of blurring due to the rotation. A new rotation method for iterative reconstruction is proposed which employs Gaussian interpolation. This method was compared to standard rotation techniques and is shown to be superior to standard techniques when measured by these qualities. The computational cost was demonstrated to be only slightly more than bilinear interpolation.","Interpolation,
Pixel,
Image reconstruction,
Biomedical imaging,
Measurement standards,
Computational efficiency,
Radiology,
Reconstruction algorithms,
Iterative methods,
Rotation measurement"
Finding an unpredictable target in a workspace with obstacles,"This paper introduces a visibility-based motion planning problem in which the task is to coordinate the motions of one or more robots that have omnidirectional vision sensors, to eventually ""see"" a target that is unpredictable, has unknown initial position, and is capable of moving arbitrarily feast. A visibility region is associated with each robot, and the goal is to guarantee that the target will ultimately lie in at least one visibility region. Both a formal characterization of the general problem and several interesting problem instances are presented. A complete algorithm for computing the motion strategy of the robots is also presented, and is based on searching a finite cell complex that is constructed on the basis of critical information changes. A few computed solution strategies are shown. Several bounds on the minimum number of needed robots are also discussed.","Robot sensing systems,
Robot kinematics,
Robot vision systems,
Cameras,
Robotics and automation,
Military computing,
Computer science,
Buildings,
Security,
Mobile robots"
Sensus: a security-conscious electronic polling system for the Internet,"Presents the design and implementation of Sensus, a practical, secure and private system for polling (conducting surveys and elections) over computer networks. Expanding on the work of Fujioka, Okamoto and Ohta (1993), Sensus uses blind signatures to ensure that only registered voters can vote and that each registered voter only votes once, while at the same time maintaining voters' privacy. Sensus allows voters to verify independently that their votes were counted correctly and to anonymously challenge the results should their votes be miscounted. We outline seven desirable properties of voting systems and show that Sensus satisfies these properties well, in some cases better than traditional voting systems.","Nominations and elections,
Privacy,
Computer networks,
Security,
Internet,
Postal services,
Electronic voting,
Protocols,
Public policy,
Computer science"
Intelligent RAM (IRAM): chips that remember and compute,"It is time to reconsider unifying logic and memory. Since most of the transistors on this merged chip will be devoted to memory, it is called 'intelligent RAM'. IRAM is attractive because the gigabit DRAM chip has enough transistors for both a powerful processor and a memory big enough to contain whole programs and data sets. It contains 1024 memory blocks each 1kb wide. It needs more metal layers to accelerate the long lines of 600mm/sup 2/ chips. It may require faster transistors for the high-speed interface of synchronous DRAM. Potential advantages of IRAM include lower memory latency, higher memory bandwidth, lower system power, adjustable memory width and size, and less board space. Challenges for IRAM include high chip yield given processors have not been repairable via redundancy, high memory retention rates given processors usually need higher power than DRAMs, and a fast processor given logic is slower in a DRAM process.","Random access memory,
Microprocessors,
Delay,
Bandwidth,
Switches,
Logic,
Vector processors,
Read-write memory,
Computer science,
Electronics industry"
Adapting hidden Markov models for ASL recognition by using three-dimensional computer vision methods,"We present an approach to continuous American sign language (ASL) recognition, which uses as input 3D data of arm motions. We use computer vision methods for 3D object shape and motion parameter extraction and an ascension technologies 'Flock of Birds' interchangeably to obtain accurate 3D movement parameters of ASL sentences, selected from a 53-sign vocabulary and a widely varied sentence structure. These parameters are used as features for hidden Markov models (HMMs). To address coarticulation effects and improve our recognition results, we experimented with two different approaches. The first consists of training context-dependent HMMs and is inspired by speech recognition systems. The second consists of modeling transient movements between signs and is inspired by the characteristics of ASL phonology. Our experiments verified that the second approach yields better recognition results.","Hidden Markov models,
Computer vision,
Handicapped aids,
Computational Intelligence Society,
Shape,
Parameter extraction,
Birds,
Speech recognition,
Deafness,
Information science"
A segmentation-based lossless image coding method for high-resolution medical image compression,"Lossless compression techniques are essential in archival and communication of medical images. Here, a new segmentation-based lossless image coding (SLIC) method is proposed, which is based on a simple but efficient region growing procedure. The embedded region growing procedure produces an adaptive scanning pattern for the image with the help of a very-few-bits-needed discontinuity index map. Along with this scanning pattern, an error image data part with a very small dynamic range is generated. Both the error image data and the discontinuity index map data parts are then encoded by the Joint Bi-level Image Experts Group (JBIG) method. The SLIC method resulted in, on the average, lossless compression to about 1.6 b/pixel from 8 b, and to about 2.9 b/pixel from 10 b with a database of ten high-resolution digitized chest and breast images. In comparison with direct coding by JBIG, Joint Photographic Experts Group (JPEG), hierarchical interpolation (HINT), and two-dimensional Burg prediction plus Huffman error coding methods, the SLIC method performed better by 4% to 28% on the database used.","Image coding,
Image segmentation,
Biomedical imaging,
X-ray imaging,
Medical diagnostic imaging,
Computed tomography,
Magnetic resonance imaging,
Pixel,
Image databases,
Breast"
AMULET2e: an asynchronous embedded controller,"AMULETI demonstrated the feasibility of building an asynchronous implementation of the ARM microprocessor. Although functional, this first asynchronous ARM microprocessor did not fully exploit the potential of the asynchronous design style to deliver improved performance and power consumption. This paper describes AMULET2e, an embedded system chip incorporating an enhanced asynchronous ARM core (AMULET2), a 4 Kbyte pipelined cache, a flexible memory interface and assorted programmable control functions. AMULET2e silicon demonstrates competitive performance and power-efficiency, ease of design, and innovative features that exploit its asynchronous operation to advantage in power-sensitive applications.","Read-write memory,
Microprocessors,
Silicon,
Testing,
Pipelines,
Prefetching,
Computer science,
Buildings,
Energy consumption,
Embedded system"
On the implementation of minimum redundancy prefix codes,"Minimum redundancy coding (also known as Huffman coding) is one of the enduring techniques of data compression. Many efforts have been made to improve the efficiency of minimum redundancy coding, the majority based on the use of improved representations for explicit Huffman trees. In this paper, we examine how minimum redundancy coding can be implemented efficiently by divorcing coding from a code tree, with emphasis on the situation when n is large, perhaps on the order of 10/sup 6/. We review techniques for devising minimum redundancy codes, and consider in detail how encoding and decoding should be accomplished. In particular, we describe a modified decoding method that allows improved decoding speed, requiring just a few machine operations per output symbol (rather than for each decoded bit), and uses just a few hundred bytes of memory above and beyond the space required to store an enumeration of the source alphabet.","Decoding,
Data compression,
Huffman coding,
Encoding,
Throughput,
Communications Society,
Australia Council,
Collaborative work,
Information technology,
Computer science"
Multimodality Bayesian algorithm for image reconstruction in positron emission tomography: a tissue composition model,"The use of anatomical information to improve the quality of reconstructed images in positron emission tomography (PET) has been extensively studied. A common strategy has been to include spatial smoothing within boundaries defined from the anatomical data. The authors present an alternative method for the incorporation of anatomical information into PET image reconstruction, in which they use segmented magnetic resonance (MR) images to assign tissue composition to PET image pixels. The authors model the image as a sum of activities for each tissue type, weighted by the assigned tissue composition. The reconstruction is performed as a maximum a posteriori (MAP) estimation of the activities of each tissue type. Two prior functions, defined for tissue-type activities, are considered. The algorithm is tested in realistic simulations employing a full physical model of the PET scanner.","Bayesian methods,
Image reconstruction,
Positron emission tomography,
Pixel,
Reconstruction algorithms,
Image segmentation,
Testing,
Brain modeling,
Smoothing methods,
Magnetic resonance"
Collaborative applications experience with the Bauhaus coordination language,"Though extremely promising in the abstract, collaborative computing systems have not yet lived up to their full potential. Besides problems pointed out by other researchers, we blame a lack of integration that pervades the field. The collaborative computing landscape consists largely of a collection of specific groupware tools, each designed and built for a specific purpose. For the most part, applications remain isolated from, and incompatible with, each other and with a user's normal work environment. The infrastructures and toolkits that do exist for building groupware systems represent a promising approach, but tend to focus on one kind of system (e.g. synchronous or asynchronous) to the exclusion of others. To ameliorate such problems, we propose taking a coordination language-based approach to groupware construction. Our new coordination language, Bauhaus, has been used to successfully construct a variety of different kinds of groupware systems. We briefly describe the Bauhaus language and our Bauhaus system prototype. We then discuss three collaborative systems that we have built using Bauhaus: a multi-user discussion system, a meeting scheduler and a multi-user dungeon (MUD).","Collaboration,
Collaborative work,
Collaborative software,
Application software,
Humans,
Computer science,
Collaborative tools,
Buildings,
Scheduling,
Multiuser detection"
Eliminating covert flows with minimum typings,"A type system is given that eliminates two kinds of covert flows in an imperative programming language. The first kind arises from nontermination and the other from partial operations that can raise exceptions. The key idea is to limit the source of nontermination in the language to constructs with minimum typings, and to evaluate partial operations within expressions of try commands which also have minimum typings. A mutual progress theorem is proved that basically states that no two executions of a well-typed program can be distinguished on the basis of nontermination versus abnormal termination due to a partial operation. The proof uses a new style of programming language semantics which we call a natural transition semantics.","Yarn,
Computer science,
Computer languages,
Java,
Information security,
Sockets,
Arithmetic"
A reliable multicast data distribution protocol based on software FEC techniques,"Applications requiring the reliable distribution of data to groups of clients would be supported perfectly by reliable multicast protocols. In many cases, the problem of congestion control (a major research issue otherwise) does not exist because downlink bandwidth is ""owned"" or can be preallocated to a particular server by independent means, but the problems of insuring reliable data delivery to larcre groups, and adaptability to heterogeneous clients, still remain. These problems can be solved at once with the use of FEC techniques. In this paper we show the design of a Reliable Multicast data Distribution Protocol (RMDP) that we have built using these techniques, and discuss the implementation tradeoffs. Experimental results show that, albeit somewhat expensive, doing encoding/decoding in software is affordable for speeds in the Mbit/s range even on low-end PCs. Slower machines can still receive at high speed, thus optimizing network usage, by taking advantage of the fact that decoding needs not to be done in real time. Finally, our RMDP can work even without any feedback from the receivers, thus making it well suited to mobile/wireless systems.","Multicast protocols,
Computer network reliability,
Telecommunication network reliability,
Encoding,
Decoding,
Feedback,
Scalability,
Computer science,
Educational institutions,
Downlink"
Weighted least-squares reconstruction methods for positron emission tomography,"We present unpenalized and penalized weighted least-squares (WLS) reconstruction methods for positron emission tomography (PET), where the weights are based on the covariance of a model error and depend on the unknown parameters. The penalty function for the latter method is chosen so that certain a priori information is incorporated. The algorithms used to minimize the WLS objective functions guarantee nonnegative estimates and, experimentally, they converged faster than the maximum likelihood expectation-maximization (ML-EM) algorithm and produced images that had significantly better resolution and contrast. Although simulations suggest that the proposed algorithms are globally convergent, a proof of convergence has not yet been found. Nevertheless, we are able to show that the unpenalized method produces estimates that decrease the objective function monotonically with increasing iterations.","Reconstruction algorithms,
Positron emission tomography,
Detectors,
Random variables,
Image converters,
Mathematical model,
Maximum likelihood detection,
Maximum likelihood estimation,
Image resolution,
Convergence"
Name-It: association of face and name in video,"This paper proposes a novel approach to extract meaningful content information from video by collaborative integration of image understanding and natural language processing. As an actual example, we developed a system that associates faces and names in videos, called Name-It, which is given news videos as a knowledge source, then automatically extracts face and name association as content information. The system can infer the name of a given unknown face image, or guess faces which are likely to have the name given to the system. This paper explains the method with several successful matching results which reveal effectiveness in integrating heterogeneous techniques as well as the importance of real content information extraction from video, especially face-name association.","Data mining,
Video compression,
Face detection,
Image sequences,
Natural languages,
Indexing,
Image retrieval,
Image color analysis,
Histograms,
Computer science"
Extracting Rules from Neural Networks by Pruning and Hidden-Unit Splitting,"An algorithm for extracting rules from a standard three-layer feedforward neural network is proposed. The trained network is first pruned not only to remove redundant connections in the network but, more important, to detect the relevant inputs. The algorithm generates rules from the pruned network by considering only a small number of activation values at the hidden units. If the number of inputs connected to a hidden unit is sufficiently small, then rules that describe how each of its activation values is obtained can be readily generated. Otherwise the hidden unit will be split and treated as output units, with each output unit corresponding to an activation value. A hidden layer is inserted and a new subnetwork is formed, trained, and pruned. This process is repeated until every hidden unit in the network has a relatively small number of input units connected to it. Examples on how the proposed algorithm works are shown using real-world data arising from molecular biology and signal processing. Our results show that for these complex problems, the algorithm can extract reasonably compact rule sets that have high predictive accuracy rates.",
Optimal-resilience proactive public-key cryptosystems,"We introduce new efficient techniques for sharing cryptographic functions in a distributed dynamic fashion. These techniques dynamically and securely transform a distributed function (or secret sharing) representation between t-out-of-l (polynomial sharing) and t-out-of-t (additive sharing). We call the techniques poly-to-sum and sum-to-poly, respectively. Employing these techniques, we solve a number of open problems in the area of cryptographic function sharing. We design a threshold function sharing scheme with proactive security for general functions with a ""homomorphic property"" (a class which includes all RSA variants and Discrete logarithm variants). The sharing has ""optimal resilience"" (server redundancy) and enables computation of the function by the servers assuring high availability, security and efficiency. Proactive security enables function sharing among servers while tolerating an adversary which is mobile and which dynamically corrupts and abandons servers (and perhaps visits all of them over the lifetime of the system, as long as the number of corruptions (faults) is bounded within a time period). Optimal resilience assures that the adversary can corrupt any minority of servers at any time-period.","Public key cryptography,
Public key,
Protection,
Robustness,
Availability,
Resilience,
Polynomials,
Additives,
Redundancy,
Protocols"
Regularized reconstruction in electrical impedance tomography using a variance uniformization constraint,"This paper describes a new approach to reconstruction of the conductivity field in electrical impedance tomography. Our goal is to improve the tradeoff between the quality of the images and the numerical complexity of the reconstruction method. In order to reduce the computational load, we adopt a linearized approximation to the forward problem that describes the relationship between the unknown conductivity and the measurements. In this framework, we focus on finding a proper way to cope with the ill-posed nature of the problem, mainly caused by strong attenuation phenomena; this is done by devising regularization techniques well suited to this particular problem. First, we propose a solution which is based on Tikhonov regularization of the problem. Second, we introduce an original regularized reconstruction method in which the regularization matrix is determined by space-uniformization of the variance of the reconstructed conductivities. Both methods are nonsupervised, i.e., all tuning parameters are automatically determined from the measured data. Tests performed on simulated and real data indicate that Tikhonov regularization provides results similar to those obtained with iterative methods, but with a much smaller amount of computations. Regularization using a variance uniformization constraint yields further improvements, particularly in the central region of the unknown object where attenuation is most severe. We anticipate that the variance uniformization approach could be adapted to iterative methods that preserve the nonlinearity of the forward problem. More generally, it appears as a useful tool for solving other severely ill-posed reconstruction problems such as eddy current tomography.","Impedance,
Tomography,
Image reconstruction,
Reconstruction algorithms,
Attenuation,
Iterative methods,
Linear approximation,
Conductivity measurement,
Testing,
Performance evaluation"
How do program understanding tools affect how programmers understand programs?,"We explore the question of whether program understanding tools enhance or change the way that programmers understand programs. The strategies that programmers use to comprehend programs vary widely. Program understanding tools should enhance or ease the programmer's preferred strategies, rather than impose a fixed strategy that may not always be suitable. We present observations from a user study that compares three tools for browsing program source code and exploring software structures. In this study, 30 participants used these tools to solve several high level program understanding tasks. These tasks required a broad range of comprehension strategies. We describe how these tools supported or hindered the diverse comprehension strategies used.","Programming profession,
Switches,
Software tools,
Software maintenance,
Reverse engineering,
Displays,
Visualization,
Computer science,
Software systems,
Animation"
Forming Neural Networks Through Efficient and Adaptive Coevolution,"This article demonstrates the advantages of a cooperative, coevolutionary search in difficult control problems. The symbiotic adaptive neuroevolution (SANE) system coevolves a population of neurons that cooperate to form a functioning neural network. In this process, neurons assume different but overlapping roles, resulting in a robust encoding of control behavior. SANE is shown to be more efficient and more adaptive and to maintain higher levels of diversity than the more common network-based population approaches. Further empirical studies illustrate the emergent neuron specializations and the different roles the neurons assume in the population.",
Cognitive design elements to support the construction of a mental model during software visualization,"The scope of software visualization tools which exist for the navigation, analysis and presentation of software information varies widely. One class of tools, which we refer to as software exploration tools, provide graphical representations of software structures linked to textual views of the program source code and documentation. This paper describes a hierarchy of cognitive issues which should be considered during the design of a software exploration tool. The hierarchy of cognitive design elements is derived through the examination of program comprehension cognitive models. Examples of how existing tools address each of these issues are provided.",
Eye-tracking for detection of driver fatigue,"In this paper, we describe a system that locates and tracks the eyes of a driver. The purpose of such a system is to perform detection of driver fatigue. By mounting a small camera inside the car, we can monitor the face of the driver and look for eye-movements which indicate that the driver is no longer in condition to drive. In such a case, a warning signal should be issued. This paper describes how to find and track the eyes. We also describe a method that can determine if the eyes are open or closed. The primary criterion for the successful implementation of this system is that it must be highly nonintrusive. The system should start when the ignition is turned on without having the driver initiate the system. Nor should the driver be responsible for providing any feedback to the system. The system must also operate regardless of the texture and the color of the face. It must also be able to handle diverse conditions, such as changes in light, shadows, reflections, etc.",
Database summarization using fuzzy ISA hierarchies,"Summary discovery is one of the major components of knowledge discovery in databases, which provides the user with comprehensive information for grasping the essence from a large amount of information in a database. We propose an interactive top down summary discovery process which utilizes fuzzy ISA hierarchies as domain knowledge. We define a generalized tuple as a representational form of a database summary including fuzzy concepts. By virtue of fuzzy ISA hierarchies where fuzzy ISA relationships common in actual domains are naturally expressed, the discovery process comes up with more accurate database summaries. We also present an informativeness measure for distinguishing generalized tuples that delivers much information to users, based on C. Shannon's (1948) information theory.",
Cryptographically resilient functions,"This correspondence studies resilient functions which have applications in fault-tolerant distributed computing, quantum cryptographic key distribution, and random sequence generation for stream ciphers. We present a number of new methods for synthesizing resilient functions. An interesting aspect of these methods is that they are applicable both to linear and nonlinear resilient functions. Our second major contribution is to show that every linear resilient function can be transformed into a large number of nonlinear resilient functions with the same parameters. As a result, we obtain resilient functions that are highly nonlinear and have a high algebraic degree.",
A structure-oriented approach to assembly sequence planning,"An assembly sequence planner is a system which, given a description of an assembly to be manufactured, identifies subassemblies and generates high-level plans for the construction of the each subassembly, including the final assembly. Previous research has shown that feasible plans can be efficiently produced for large assemblies, but the generation of good plans has met with less success. Not only are there difficulties in defining ""good plans,"" but all published methods for finding good plans have computational complexities that make them unlikely to be practical for large assemblies. This paper shows that representing an assembly as a hierarchy of assembly structures can overcome many of these difficulties. A planner is described which uses the structure hierarchy both as a framework for structure-dependent definitions of a good plan, and as a tool for finding good plans more rapidly by using high-level expert advice, by reusing subplans for repeated substructures, and by not fully optimizing the plan. Analytical and experimental results are presented to demonstrate the effectiveness of the approach.",
Improving Superscalar Instruction Dispatch And Issue By Exploiting Dynamic Code Sequences,,
A head motion measurement system suitable for emission computed tomography,"Subject motion during brain imaging studies can adversely affect the images through loss of resolution and other artifacts related to movement. The authors have developed and tested a device to measure head motion externally in real-time during emission computed tomographic (ECT) brain imaging studies, to be used eventually to correct ECT data for that motion. The system is based on optical triangulation of three miniature lights affixed to the patient's head and viewed by two position-sensitive detectors. The computer-controlled device converts the three sets of lamp positions into rotational and translational coordinates every 0.7 seconds. When compared against a mechanical test fixture, the optical system was found to be linear and accurate with minimal crosstalk between the coordinates. In a study of two subjects, comparing the angular motions measured by the optical device and a commercially available electromagnetic motion detector, the two systems agreed well, with an root mean square (rms) difference of less than 0.6/spl deg/ for all rotations.",
Feedback control mechanisms for real-time multipoint video services,"While existing research shows that reactive congestion control mechanisms are capable of providing high video quality and channel utilization for point-to-point real-time video, there has been relatively little study of the reactive congestion control of point-to-multipoint video, especially in ATM networks. Problems complicating the provision of multipoint, feedback-based real-time video service include: (1) implosion of feedback returning to the source as the number of multicast destinations increases and (2) variance in the amount of available bandwidth on different branches in the multipoint connection. A new service architecture is proposed for real-time multicast video, and two multipoint feedback mechanisms to support this service are introduced and studied. The mechanisms support a minimum bandwidth guarantee and the best effort support of video traffic exceeding the minimum rate. They both rely on adaptive, multilayered coding at the video source and closed-loop feedback from the network in order to control both the high and low priority video generation rates of the video encoder. Simulation results show that the studied feedback mechanisms provide, at the minimum, a quality of video comparable to a constant bit rate (CBR) connection reserving the same amount of bandwidth. When unutilized network bandwidth becomes available, the mechanisms are capable of exploiting it to dynamically improve video quality beyond the minimum guaranteed level.",
Dimensionality reduction of unsupervised data,"Dimensionality reduction is an important problem for efficient handling of large databases. Many feature selection methods exist for supervised data having class information. Little work has been done for dimensionality reduction of unsupervised data in which class information is not available. Principal component analysis (PCA) is often used. However, PCA creates new features. It is difficult to obtain intuitive understanding of the data using the new features only. We are concerned with the problem of determining and choosing the important original features for unsupervised data. Our method is based on the observation that removing an irrelevant feature from the feature set may not change the underlying concept of the data, but not so otherwise. We propose an entropy measure for ranking features, and conduct extensive experiments to show that our method is able to find the important features. Also it compares well with a similar feature ranking method (Relief) that requires class information unlike our method.",
Conditional distribution learning with neural networks and its application to channel equalization,"We present a conditional distribution learning formulation for real-time signal processing with neural networks based on an extension of maximum likelihood theory-partial likelihood (PL) estimation-which allows for (i) dependent observations and (ii) sequential processing. For a general neural network conditional distribution model, we establish a fundamental information-theoretic connection, the equivalence of maximum PL estimation, and accumulated relative entropy (ARE) minimization, and obtain large sample properties of PL for the general case of dependent observations. As an example, the binary case with the sigmoidal perceptron as the probability model is presented. It is shown that the single and multilayer perceptron (MLP) models satisfy conditions for the equivalence of the two cost functions: ARE and negative log partial likelihood. The practical issue of their gradient descent minimization is then studied within the well-formed cost functions framework. It is shown that these are well-formed cost functions for networks without hidden units; hence, their gradient descent minimization is guaranteed to converge to a solution if one exists on such networks. The formulation is applied to adaptive channel equalization, and simulation results are presented to show the ability of the least relative entropy equalizer to realize complex decision boundaries and to recover during training from convergence at the wrong extreme in cases where the mean square error-based MLP equalizer cannot.",
Image reconstruction for dynamic PET based on low-order approximation and restoration of the sinogram,"Many image-reconstruction methods have been proposed to improve the spatial resolution of positron emission tomography (PET) images and, thus, to produce better quantification. However, these techniques, which are designed for static images, may be inadequate for good reconstruction from dynamic data. The authors present a simple, but effective, reconstruction approach intended specifically for dynamic studies. First, the level of noise in dynamic PET data is reduced by smoothing along the time axis using a low-order approximation. Next, the denoised sinograms are restored spatially by the method of projections onto convex sets. Finally, images are reconstructed from the restored sinograms by ordinary filtered backprojection. The authors present experimental results that demonstrate substantial improvements in region-of-interest quantification in actual and simulated dopamine D-2 neuroreceptor-imaging studies of a monkey brain.",
Measurement of mode field profiles and bending and transition losses in curved optical channel waveguides,"Curved, single-mode, silicon oxynitride optical ridge channel waveguides have been characterized by measuring mode field profile alteration, bending loss, and transition loss. The results were compared to theoretical calculations. Near field imaging of the channel mode field profiles showed the effect of bend radius on mode shape. Good agreement was obtained between the measurements and profiles obtained from two-dimensional (2-D) beam propagation method calculations. The exponential dependence of bending loss on bend radius and the variation of transition loss on the lateral offset between channels having different bend radius were successfully modeled by two simple theories.",
Number-theoretic constructions of efficient pseudo-random functions,"We describe efficient constructions for various cryptographic primitives (both in private-key and in public-key cryptography). We show these constructions to be at least as secure as the decisional version of the Diffie-Hellman assumption or as the assumption that factoring is hard. Our major result is a new construction of pseudo-random functions such that computing their value at any given point involves two multiple products. This is much more efficient than previous proposals. Furthermore, these functions have the advantage of being in TC/sup 0/ (the class of functions computable by constant depth circuits consisting of a polynomial number of threshold gates) which has several interesting applications. The simple algebraic structure of the functions implies additional features. In particular, we show a zero-knowledge proof for statements of the form ""y=f/sub s/(x)"" and ""y/spl ne/f(x)"" given a commitment to a key s of a pseudo-random function f/sub s/.",
Genetic approach to radio network optimization for mobile systems,"Size and complexity of future UMTS radio networks make their planning very difficult. This paper focuses on the radio coverage problem, that is, to cover a maximum surface of a given geographical region at an optimal cost. This combinatorial optimization problem is solved with a bioinspired genetic algorithm. A first prototype runs in parallel on a network of workstations. The obtained results are shown and discussed.",
"Reduce, reuse, recycle: an approach to building large Internet caches","New demands brought by the continuing growth of the Internet will be met in part by more effective use of caching in the Web and other services. We have developed CRISP, a distributed Internet object cache targeted to the needs of the organizations that aggregate the end users of Internet services, particularly the commercial Internet Service Providers (ISPs) where much of the new growth occurs. A CRISP cache consists of a group of cooperating caching servers sharing a central directory of cached objects. This simple and obvious strategy is easily overlooked due to the well known drawbacks of a centralized structure. However, we show that these drawbacks are easily overcome for well configured CRISP caches. We outline the rationale behind the CRISP design, and report on early studies of CRISP caches in actual use and under synthetic load. While our experience with CRISP to date is at the scale of hundreds or thousands of clients, CRISP caches could be deployed to maximize capacity at any level of a regional or global cache hierarchy.",
A Penalty-Function Approach for Pruning Feedforward Neural Networks,"This article proposes the use of a penalty function for pruning feedforward neural network by weight elimination. The penalty function proposed consists of two terms. The first term is to discourage the use of unnecessary connections, and the second term is to prevent the weights of the connections from taking excessively large values. Simple criteria for eliminating weights from the network are also given. The effectiveness of this penalty function is tested on three well-known problems: the contiguity problem, the parity problems, and the monks problems. The resulting pruned networks obtained for many of these problems have fewer connections than previously reported in the literature.",
The swappable logic unit: a paradigm for virtual hardware,"Swappable Logic Units (SLUs) were introduced by the author previously (1996) to play a role in virtual hardware subsystems that is analogous to the role of pages or segments in virtual memory subsystems. The intention is that a conventional operating system can be extended to manage SLU circuitry implemented using FPGA real estate. In order to minimise operating system overheads, two particular SLU-based virtual hardware models were deemed practical: a ""sea of accelerators"" model and a ""parallel harness"" model. This paper looks in some detail at how SLUs will fit within the overall environment of a fairly conventional hardware/software system. First, there is a discussion of the FPGA-based hardware environment for SLUs, followed by a discussion of the software environment from which SLUs might be used. After this, there is a description of the operational properties that SLUs can have, and how these fit in with the two virtual hardware models. Finally, proposals for standard interfaces between SLUs and their environment are discussed. These interfaces can be regarded as constraints on the designers of SLU circuitry or, more positively, as suppliers of an enriched context within which such circuitry operates. The overall impact of the work presented in the paper is to show that it is feasible to incorporate configurable hardware within traditional computer systems that use high-level language programs and computer operating systems. That is, it should not always be necessary to devise special-purpose hardware/software systems to realise custom computing.",
Improved approximation algorithms for unsplittable flow problems,"In the single-source unsplittable flow problem we are given a graph G, a source vertex s and a set of sinks t/sub 1/, ..., t/sub k/ with associated demands. We seek a single s-t/sub i/ flow path for each commodity i so that the demands are satisfied and the total flow routed across any edge e is bounded by its capacity c/sub e/. The problem is an NP-hard variant of max flow and a generalization of single-source edge-disjoint paths with applications to scheduling, load balancing and virtual-circuit routing problems. In a significant development, Kleinberg gave recently constant-factor approximation algorithms for several natural optimization versions of the problem. In this paper we give a generic framework, that yields simpler algorithms and significant improvements upon the constant factors. Our framework, with appropriate subroutines applies to all optimization versions previously considered and treats in a unified manner directed and undirected graphs.",
Finding consistent global checkpoints in a distributed computation,"Consistent global checkpoints have many uses in distributed computations. A central question in applications that use consistent global checkpoints is to determine whether a consistent global checkpoint that includes a given set of local checkpoints can exist. Netzer and Xu (1995) presented the necessary and sufficient conditions under which such a consistent global checkpoint can exist, but they did not explore what checkpoints could be constructed. In this paper, we prove exactly which local checkpoints can be used for constructing such consistent global checkpoints. We illustrate the use of our results with a simple and elegant algorithm to enumerate all such consistent global checkpoints.",
Recognizing reverberant speech with RASTA-PLP,"The performance of the PLP (perceptual linear predictive), log-RASTA-PLP, and J-RASTA-PLP front ends for recognition of highly reverberant speech is measured and compared with the performance of humans and the performance of an experimental RASTA-like front end on reverberant speech, and with the performance of a PLP-based recognizer trained on reverberant speech. While humans are able to reliably recognize the reverberant test set, achieving a 6.1% word error rate, the best RASTA-PLP-based recognizer has a word error rate of 68.7% on the same test set, and the PLP-based recognizer trained on reverberant speech has a 50.3% word error rate. Our experimental variant on RASTA processing provides a statistically significant improvement in performance on the reverberant speech, with a best word error rate of 64.1%.",
Fast Sigmoidal Networks via Spiking Neurons,"We show that networks of relatively realistic mathematical models for biological neurons in principle can simulate arbitrary feedforward sigmoidal neural nets in a way that has previously not been considered. This new approach is based on temporal coding by single spikes (respectively by the timing of synchronous firing in pools of neurons) rather than on the traditional interpretation of analog variables in terms of firing rates. The resulting new simulation is substantially faster and hence more consistent with experimental results about the maximal speed of information processing in cortical neural systems. As a consequence we can show that networks of noisy spiking neurons are universal approximators in the sense that they can approximate with regard to temporal coding any given continuous function of several variables. This result holds for a fairly large class of schemes for coding analog variables by firing times of spiking neurons. This new proposal for the possible organization of computations in networks of spiking neurons systems has some interesting consequences for the type of learning rules that would be needed to explain the self-organization of such networks. Finally, the fast and noise-robust implementation of sigmoidal neural nets by temporal coding points to possible new ways of implementing feedforward and recurrent sigmoidal neural nets with pulse stream VLSI.",
Formal methods: promises and problems,Successfully applying formal methods to software development promises to move us closer to a true engineering discipline. The authors offer suggestions for overcoming the problems that have hindered the use of formal methods thus far.,
An evaluation of bipartitioning techniques,"Logic partitioning is an important issue in VLSI CAD, and has been an area of active research for at least the last 25 years. Numerous approaches have been developed and many different techniques have been combined for a wide range of applications. In this paper, we examine many of the existing techniques for logic bipartitioning and present a methodology for determining the best mix of approaches The result is a novel bipartitioning algorithm that includes both new and preexisting techniques. Our algorithm produces results that are at least 16% better than the state of the art while also being efficient in run time.",
An efficient message authentication scheme for link state routing,"We study methods for reducing the cost of secure link state routing. In secure link state routing, routers may need to verify the authenticity of many routing updates, and some routers such as border routers may need to sign many routing updates. Previous work such as public-key based schemes are very expensive computationally or have certain limitations. This paper presents an efficient solution, based on a detection-diagnosis-recovery approach, for the link state routing update authentication problem. Our scheme is scalable to handle large networks, applicable to routing protocols that use multiple-valued cost metrics, and applicable even, when link states change frequently.",
Semi-automatic wrapper generation for Internet information sources,"To simplify the task of obtaining information from the vast number of information sources that are available on the World Wide Web (WWW), the authors are building information mediators for extracting and integrating data from multiple Web sources. In a mediator based approach, wrappers are built around individual information sources to translate between the mediator query language and the individual sources. They present an approach for semi-automatically generating wrappers for structured Internet sources. The key idea is to exploit formatting information in Web pages to hypothesize the underlying structure of a page. From this structure the system generates a wrapper that facilitates querying of a source and possibly integrating it with other sources. They demonstrate the ease with which they are able to build wrappers for a number of Web sources using their implemented wrapper generation toolkit.",
Part orientation with programmable vector fields: two stable equilibria for most parts,"Part manipulation is an important but also time-consuming operation in industrial automation. Recent work explores alternative solutions to the mechanical parts feeders which have been traditionally used to sort and orient parts for assembly. One of the proposed alternatives is the use of programmable vector fields. The fields are realized on a plane on which the part is placed. The forces exerted on the part's contact surface translate and rotate the part to an equilibrium orientation. Certain vector fields can be implemented in the microscale with actuator arrays and in the macroscale with transversely vibrating plates. Although current technology is still limited, the dexterity that programmable vector fields offer has prompted researchers to further explore their capabilities. This paper presents a vector field that can simultaneously orient and pose most parts into two stable equilibrium configurations. The equilibrium configurations are easily computed a priori given the part to be oriented. Our analysis makes no assumptions about the shape of the part or its connectivity except that it moves as a rigid body. The proposed vector field offers the great advantage of stability of the equilibrium configurations under small perturbations of the part which is key for the orientation of toleranced parts.",
The Energy Efficiency Of Iram Architectures,,
Efficient admission control for EDF schedulers,"We present algorithms for flow admission control at an EDF link scheduler when the flows are characterized by peak rate, average rate and burst size. We show that the algorithms have very low computational complexity and are easily applicable in practice. The complexity can be further decreased by introducing the notion of flex classes. We evaluate the penalty in efficiency that the classes incur to the EDF scheduler. We find that this efficiency degradation can be made arbitrarily small and is acceptable even for a small number of classes.",
4.5 tesla magnetic field reduces range of high-energy positrons-potential implications for positron emission tomography,"We have theoretically and experimentally investigated the extent to which homogeneous magnetic fields up to 7 Tesla reduce the spatial distance positrons travel before annihilation (positron range). Computer simulations of a noncoincident detector design using a Monte Carlo algorithm calculated the positron range as a function of positron energy and magnetic field strength. The simulation predicted improvements in resolution, defined as full-width at half-maximum (FWBM) of the line-spread function (LSP) for a magnetic field strength up to 7 Tesla: negligible for F-18, from 3.35 mm to 2.73 mm for Ga-68 and from 3.66 mm to 2.68 mm for Rb-82. Also a substantial noise suppression was observed, described by the full-width at tenth-maximum (FWTM) for higher positron energies. The experimental approach confirmed an improvement in resolution for Ga-68 from 3.54 mm at 0 Tesla to 2.99 mm FWHM at 4.5 Tesla and practically no improvement for F-18 (2.97 mm at 0 Tesla and 2.95 mm at 4.5 Tesla). It is concluded that the simulation model is appropriate and that a homogeneous static magnetic field of 4.5 Tesla reduces the range of high-energy positrons to an extent that may improve spatial resolution in positron emission tomography.",
The DEVS environment for high-performance modeling and simulation,"DEVS-C++, a high-performance environment for modeling large-scale systems at high resolution, uses the DEVS (Discrete-EVent system Specification) formalism to represent both continuous and discrete processes. A prototype suggests that the DEVS formalism can be combined with genetic algorithms running in parallel to serve as the basis of a very general, very fast class of simulation environments.",
Collaborations: closing the industry-academia gap,"When it comes to software engineering education, there is a gap between what industry needs and what universities offer. To close this gap, the authors propose a comprehensive collaboration between academic software engineering programs and industry. They offer a model for this collaboration and highlight three real-world ventures.","Collaboration,
Software engineering,
Collaborative software,
Computer industry,
Educational programs,
Industrial training,
Computer science education,
Collaborative work,
Management training,
Data systems"
An Efficient Implementation Of Reactivity For Modeling Hardware In The Scenic Design Environment,,"Hardware design languages,
Permission,
Digital systems,
Computer science,
Runtime library,
Kernel,
Signal synthesis,
Signal processing,
Context modeling,
Delay"
A similarity measure for stereo feature matching,"An approach to stereo feature matching is presented with the introduction of a similarity measure for evaluating and confirming a stereo match. The contributions of this study are reflected in (1) the development of a similarity measure which evaluates a stereo match based on feature locality and gray-level gradient associated with the feature; and (2) the use of a matching procedure that integrates local and global matching strategies based on matching first those features with the highest similarity measure among the set of all highest similarities found locally under confined search spaces, ensuring that each feature is matched with a high degree of certainty. A left-to-right and right-to-left consistency check is used for each feature to comply with the uniqueness constraint and to confirm if a potential match can be declared a correct match.",
Object recognition using appearance-based parts and relations,"The recognition of general three-dimensional objects in cluttered scenes is a challenging problem. In particular, the design of a good representation suitable to model large numbers of generic objects that is also robust to occlusion has been a stumbling block in achieving success. In this paper, we propose a representation using appearance-based parts and relations to overcome these problems. Appearance-based parts and relations are defined in terms of closed regions and the union of these regions, respectively. The regions are segmented using the MDL principle, and their appearance is obtained from collection of images and compactly represented by parametric manifolds in the two eigenspaces spanned by the parts and the relations.","Object recognition,
Image segmentation,
Layout,
Robustness,
Image recognition,
Computer science,
Image databases,
Spatial databases,
Solid modeling,
Polynomials"
TITAC-2: an asynchronous 32-bit microprocessor based on scalable-delay-insensitive model,"Asynchronous design has a potential of solving many difficulties, such as clock skew and power consumption, which synchronous counterpart suffers with current and future VLSI technologies. This paper proposes a new delay model, the scalable-delay-insensitive (SDI) model, for dependable and high-performance asynchronous VLSI system design. Then, based on the SDI model, the paper presents the design, chip implementation, and evaluation results of a 32-bit asynchronous microprocessor TITAC-2 whose instruction set is based on the MIPS R2000. The measured performance of TITAC-2 is 52.3 MIPS using the Dhrystone V2.1 benchmark.","Microprocessors,
CMOS technology,
Very large scale integration,
Wire,
Circuits,
Wiring,
Clocks,
Delay systems,
Delay effects,
Information science"
Fully automatic identification of AC and PC landmarks on brain MRI using scene analysis,"Describes a method for identification of brain structures from MRI data sets. The bulk of the paper concerns an automatic system for finding the anterior and posterior commissures [(AC) and (PC)] in the midsagittal plane. These landmarks are key for the definition of the Talairach space, commonly used in stereotactic neurosurgery, in the definition of common coordinate systems for the pooling of functional positron emission tomography (PET) images and for neuroanatomy studies. The process works according to a step-by-step procedure: it first analyzes the skull limits. A grey-level histogram is then calculated and allows an automated selection of thresholds. Then, the interhemispheric plane is detected. Following an advanced scene analysis in the midsagittal plane for anatomical structures, the AC and the PC are identified. Experimentally, with a set of 200 patients, the process never failed. Its performances and limits are comparable to that of neuroanatomy experts. Those results are due to a high degree of robustness at each step of the program.",
Arithmetic crosscorrelations of feedback with carry shift register sequences,"An arithmetic version of the crosscorrelation of two sequences is defined, generalizing Mandelbaum's (1967) arithmetic autocorrelations. Large families of sequences are constructed with ideal (vanishing) arithmetic crosscorrelations. These sequences are decimations of the 2-adic expansions of rational numbers p/q such that 2 is a primitive root module q.","Arithmetic,
Feedback,
Shift registers,
Autocorrelation,
Binary sequences,
Synchronization,
Associate members,
Mathematics,
Computer science,
Clocks"
Image replacement through texture synthesis,"Photographs and images often have regions which are in some sense flawed. Often, there may be a stain or an undesired feature covering a significant portion of the image, and an algorithm that can ""fix"" the image is desired. We propose a technique based on Heeger and Bergen's texture synthesis algorithm (1995). By integrating a composition step into the aforementioned algorithm one portion of the image can be substituted with a synthetic texture derived from another portion of the image. The goal of this synthetic texture is to create a plausible patch without visible seams or repetitive features. We compare this technique, which works on areas that are ""stochastic"" in nature, with other variations of image replacement which utilize a combination of image composition and texture synthesis.","White noise,
Computer science,
Wire,
Layout,
Tiles,
Statistics,
Nonlinear filters,
Matched filters,
Humans,
Stochastic processes"
Multiresolution tetrahedral framework for visualizing regular volume data,"The authors present a multiresolution framework, called Multi-Tetra framework, that approximates volume data with different levels-of-detail tetrahedra. The framework is generated through a recursive subdivision of the volume data and is represented by binary trees. Instead of using a certain level of the Multi-Tetra framework for approximation, an error-based model (EBM) is generated by recursively fusing a sequence of tetrahedra from different levels of the Multi-Tetra framework. The EBM significantly reduces the number of voxels required to model an object, while preserving the original topology. The approach provides continuous distribution of rendered intensity or generated isosurfaces along boundaries of different levels-of-detail thus solving the crack problem. The model supports typical rendering approaches, such as marching cubes, direct volume projection, and splatting. Experimental results demonstrate the strengths of the approach.","Data visualization,
Rendering (computer graphics),
Computer graphics,
Isosurfaces,
Data mining,
Displays,
Image resolution,
Computer science,
Binary trees,
Topology"
Hardware software partitioning using genetic algorithm,"Hardware software co-design is gaining importance with the advent of CAD for embedded systems. A key phase in such designs is partitioning the specification into hardware and software implementation sets. The problem being combinatorically explosive, several greedy search algorithms have been proposed for hardware software partitioning. In this paper, we model the hardware software partitioning problem as a Constraint Satisfaction Problem (CSP), and present a genetic algorithm based approach to solve the CSP in order to obtain the partitioning solution.","Hardware,
Genetic algorithms,
Costs,
Software algorithms,
Embedded software,
Design automation,
Embedded system,
Partitioning algorithms,
Algorithm design and analysis,
Computer science"
"The gambler's ruin problem, genetic algorithms, and the sizing of populations",The paper presents a model for predicting the convergence quality of genetic algorithms. The model incorporates previous knowledge about decision making in genetic algorithms and the initial supply of building blocks in a novel way. The result is an equation that accurately predicts the quality of the solution found by a GA using a given population size. Adjustments for different selection intensities are considered and computational experiments demonstrate the effectiveness of the model.,"Genetic algorithms,
Convergence,
Laboratories,
Predictive models,
Decision making,
Equations,
Genetic engineering,
Computer science"
A field guide to boxology: preliminary classification of architectural styles for software systems,"Software architects use a number of commonly-recognized ""styles"" to guide their design of system structures. Each of these is appropriate for some classes of problems, but none is suitable for all problems. How, then, does a software designer choose an architecture suitable for the problem at hand? Two kinds of information are required: (1) careful discrimination among the candidate architectures and (2) design guidance on how to make appropriate choices. In this paper, we support careful discrimination with a preliminary classification of styles. We use a 2D classification strategy with control and data issues as the dominant organizing axes. We position the major styles within this space and use finer-grained discriminations to elaborate variations on the styles. This provides a framework for organizing design guidance, which we partially flesh out with rules of thumb.",
The complex Householder transform,"The Householder (1968) transform is very useful in matrix computations and signal processing. A straightforward derivation for a complex Householder transform is given. It needs fewer complex operations when compared with the previous results by Venkaiah et al. (1993) and Xia and Suter (see Digital Signal Process., vol.5, p.116-17, 1995). We also investigate applying our result to the derivation of a hyperbolic Householder transform.",
An elliptical head tracker,"A simple algorithm for tracking a person's head is presented. A two-dimensional model, namely an ellipse, is used to approximate the head's contour. When a new image becomes available, a local search determines the position and size of the best ellipse by maximizing the normalized sum of the image gradient magnitude around the perimeter of the ellipse. The local search begins from a predicted position, using the head's velocity which eliminates the tracker's dependence upon maximum velocity. The tracker operates at 30 Hz and actively controls camera pan and tilt in order to track a person moving in a real environment. The algorithm tolerates full 360-degree rotation of the body as well as moderate amounts of occlusion, and it performs reacquisition of the subject.",
Full abstraction for functional languages with control,"This paper considers the consequences of relaxing the bracketing condition on 'dialogue games', showing that this leads to a category of games which can be 'factorized' into a well-bracketed substructure, and a set of classically typed morphisms. These are shown to be sound denotations for control operators, allowing the factorization to be used to extend the definability result for PCF to one for PCF with control operators at atomic types. Thus we define a fully abstract and effectively presentable model of a functional language with non-local control as part of a modular approach to modelling non-functional features using games.",
Computational steering. Software systems and strategies,"With today's large and complex applications, scientists have increasing difficulty analyzing and visualizing vast amounts of data. Computational steering is an emerging technology that addresses this problem, providing a mechanism for integrating simulation, data analysis, visualization, and postprocessing.",
Automatic discovery of self-replicating structures in cellular automata,"Previous computational models of self-replication using cellular automata (CA) have been manually designed, a difficult and time-consuming process. We show here how genetic algorithms can be applied to automatically discover rules governing self-replicating structures. The main difficulty in this problem lies in the choice of the fitness evaluation technique. The solution we present is based on a multiobjective fitness function consisting of three independent measures: growth in number of components, relative positioning of components, and the multiplicity of replicants. We introduce a new paradigm for CA models with weak rotational symmetry, called orientation-insensitive input, and hypothesize that it facilitates discovery of self-replicating structures by reducing search-space sizes. Experimental yields of self-replicating structures discovered using our technique are shown to be statistically significant. The discovered self-replicating structures compare favorably in terms of simplicity with those generated manually in the past, but differ in unexpected ways. These results suggest that further exploration in the space of possible self-replicating structures will yield additional new structures. Furthermore, this research sheds light on the process of creating self-replicating structures, opening the door to future studies on the discovery of novel self-replicating molecules and self-replicating assemblers in nanotechnology.",
"Linear-time, incremental hierarchy inference for compression","Data compression and learning are, in some sense, two sides of the same coin. If we paraphrase Occam's razor by saying that a small theory is better than a larger theory with the same explanatory power, we can characterize data compression as a preoccupation with small, and learning as a preoccupation with better. Nevill-Manning et al. (see Proc. Data Compression Conference, Los Alamitos, CA, p.244-253, 1994) presented an algorithm, since dubbed SEQUITUR, that presents both faces of the compression/learning coin. Its performance as a data compression scheme outstrips other dictionary schemes, and the structures that it learns from sequences as diverse as DNA and music are intuitively compelling. We present three new results that characterize SEQUITUR's computational and compression performance. First, we prove that SEQUITUR operates in time linear in n, the length of the input sequence, despite its ability to build a hierarchy as deep as log(n). Second, we show that a sequence can be compressed incrementally, improving on the non-incremental algorithm that was described by Nevill-Manning et al., and making on-line compression feasible. Third, we present an intriguing result that emerged during benchmarking; whereas PPMC outperforms SEQUITUR on most files in the Calgary corpus, SEQUITUR regains the lead when tested on multimegabyte sequences. We make some tentative conclusions about the underlying reasons for this phenomenon, and about the nature of current compression benchmarking.",
Deformable 2-D template matching using orthogonal curves,"A new formulation of the two-dimensional (2-D) deformable template matching problem is proposed. It uses a lower-dimensional search space than conventional methods by precomputing extensions of the deformable template along orthogonal curves. The reduction in search space allows the use of dynamic programming to obtain globally optimal solutions and reduces the sensitivity of the algorithm to initial placement of the template. Further, the technique guarantees that the result is a curve which does not collapse to a point in the absence of strong image gradients and is always nonself intersecting. Examples of the use of the technique on real-world images and in simulations at low signal-to-noise ratios (SNRs) are also provided.",
Genetic-based search for error-correcting graph isomorphism,"Error-correcting graph isomorphism has been found useful in numerous pattern recognition applications. This paper presents a genetic-based search approach that adopts genetic algorithms as the searching criteria to solve the problem of error-correcting graph isomorphism. By applying genetic algorithms, some local search strategies are amalgamated to improve convergence speed. Besides, a selection operator is proposed to prevent premature convergence. The proposed approach has been implemented to verify its validity. Experimental results reveal the superiority of this new technique than several other well-known algorithms.",
Multiresolution statistical analysis of high-resolution digital mammograms,"A multiresolution statistical method for identifying clinically normal tissue in digitized mammograms is used to construct an algorithm for separating normal regions from potentially abnormal regions; that is, small regions that may contain isolated calcifications. This is the initial phase of the development of a general method for the automatic recognition of normal mammograms. The first step is to decompose the image with a wavelet expansion that yields a sum of independent images, each containing different levels of image detail. When calcifications are present, there is strong empirical evidence that only some of the image components are necessary for the purpose of detecting a deviation from normal. The underlying statistic for each of the selected expansion components can be modeled with a simple parametric probability distribution function. This function serves as an instrument for the development of a statistical test that allows for the recognition of normal tissue regions. The distribution function depends on only one parameter, and this parameter itself has an underlying statistical distribution. The values of this parameter define a summary statistic that can be used to set detection error rates. Once the summary statistic is determined, spatial filters that are matched to resolution are applied independently to each selected expansion image. Regions of the image that correlate with the normal statistical model are discarded and regions in disagreement (suspicious areas) are flagged. These results are combined to produce a detection output image consisting only of suspicious areas. This type of detection output is amenable to further processing that may ultimately lead to a fully automated algorithm for the identification of normal mammograms.",
Two novel multiway circuit partitioning algorithms using relaxed locking,"All the previous Kernighan-Lin-based (KL-based) circuit partitioning algorithms employ the locking mechanism, which enforces each cell to move exactly once per pass. In this paper, we propose two novel approaches for multiway circuit partitioning to overcome this limitation. Our approaches allow each cell to move more than once. Our first approach still uses the locking mechanism but in a relaxed way. It introduces the phase concept such that each pass can include more than one phase, and a phase can include at most one move of each cell. Our second approach does not use the locking mechanism at all. It introduces the mobility concept such that each cell can move as freely as allowed by its mobility. Each approach leads to KL-based generic algorithms whose parameters can be set to obtain algorithms with different performance characteristics. We generated three versions of each generic algorithm and evaluated them on a subset of common benchmark circuits in comparison with Sanchis' algorithm (FMS) and the simulated annealing algorithm (SA). Experimental results show that our algorithms are efficient, they outperform FMS significantly, and they perform comparably to SA. Our algorithms perform relatively better as the number of parts in the partition increases as well as the density of the circuit decreases. This paper also provides guidelines for good parameter settings for the generic algorithms.",
Interval volume tetrahedrization,"The interval volume is a generalization of the isosurface commonly associated with the marching cubes algorithm. Based upon samples at the locations of a 3D rectilinear grid, the algorithm produces a triangular approximation to the surface defined by F(x,y,z)=c. The interval volume is defined by /spl alpha//spl les/F(x,y,z)/spl les//spl beta/. The authors describe an algorithm for computing a tetrahedrization of a polyhedral approximation to the interval volume.",
Lackwit: A Program Understanding Tool Based on Type Inference,,
Anatomically constrained electrical impedance tomography for three-dimensional anisotropic bodies,"As shown previously for two-dimensional geometries, anisotropy effects should not be ignored in electrical impedance tomography (EIT) and structural information is important for the reconstruction of anisotropic conductivities. Here, we describe the static reconstruction of an anisotropic conductivity distribution for the more realistic three-dimensional (3-D) case. Boundaries between different conductivity regions are anatomically constrained using magnetic resonance imaging (MRI) data. The values of the conductivities are then determined using gradient-type-algorithms in a nonlinear-indirect approach. At each iteration, the forward problem is solved by the finite element method. The approach is used to reconstruct the 3-D conductivity profile of a canine torso. Both computational performance and simulated reconstruction results are presented together with a detailed study on the sensitivity of the prediction error with respect to different parameters. In particular, the use of an intracavity catheter to better extract interior conductivities is demonstrated.",
"An overview of CAFE specification environment-an algebraic approach for creating, verifying, and maintaining formal specifications over networks","CAFE is the name of a network based environment now under development for supporting systematic creation, checking, verification, and maintenance of formal specifications. CAFE has an algebraic specification language called CafeOBJ as its main specification language, and adopts an algebraic specification paradigm as its foundation. CafeOBJ is a successor of the OBJ language, and has important new features for concurrency and behavioral specifications. Concurrency and behavior are specified based on rewriting logic and behavioral (hidden sorted) algebra respectively. These new features make it possible to provide powerful language constructs for formal object oriented specifications. CAFE is designed to be a network based environment. For sharing specification documents systematically over networks, a new document formatting language called Forsdonnet (FORmal Specification Document ON NETwork) is designed by extending HTML. Forsdonnet includes constructs for formal and informal specifications, commands for executing (prototyping) and cheeking/verifying CafeOBJ specifications, etc. Forsdonnet is designed to be based on already established standard network infrastructure components like HTML and Netscape Navigator. The paper gives an overview and design considerations of the CAFE environment, featuring mainly CafeOBJ and Forsdonnet languages.",
Coherence Controller Architectures For Smp-based Cc-numa Multiprocessors,,
Empirical Investigation of Multiparent Recombination Operators in Evolution Strategies,"An extension of evolution strategies to multiparent recombination involving a variable number  of parents to create an offspring individual is proposed. The extension is experimentally evaluated on a test suite of functions differing in their modality and separability and the regular/irregular arrangement of their local optima. Multiparent diagonal crossover and uniform scanning crossover and a multiparent version of intermediary recombination are considered in the experiments. The performance of the algorithm is observed to depend on the particular combination of recombination operator and objective function. In most of the cases a significant increase in performance is observed as the number of parents increases. However, there might also be no significant impact of recombination at all, and for one of the unimodal objective functions, the performance is observed to deteriorate over the course of evolution for certain choices of the recombination operator and the number of parents. Additional experiments with a skewed initialization of the population clarify that intermediary recombination does not cause a search bias toward the origin of the coordinate system in the case of domains of variables that are symmetric around zero.",
Implementation of a model-based nonuniform scatter correction scheme for SPECT,"Four scatter-compensation schemes are considered. The 4 schemes are all based on a previously developed two-dimensional (2-D) scatter model. Reconstruction is achieved using the iterative expectation-maximization maximum-likelihood (EM-ML) algorithm. The schemes consist of: (1) including the model in both the forward and back projector; (2) just including the model in the forward projector; (3) and (4) implementing the model in a subtraction and addition scheme, respectively. Monte Carlo simulated projection data are used to test the accuracy, convergence properties, and noise properties of the 4 scatter-compensation schemes. Data are simulated for both uniformly and nonuniformly attenuating objects. The results show that all 4 correction schemes yield images which are similar in terms of accuracy to that obtained from reconstructing scatter-free data. The subtraction scheme is shown to converge faster than the other compensation schemes, both in terms of iterations and actual time required for reconstruction. The scheme in which the model is only used in the forward-projector and the scatter-addition scheme both performs slightly better, in terms of signal-to-noise ratio (SNR), than the subtraction scheme. However, the forward projector scheme requires significantly more CPU time for reconstruction. The correction scheme in which the scatter model was included in both the forward and backprojectors is shown to produce accurate images with SNR's higher than even a perfect scatter rejection scheme. While the scatter correction scheme with the model in both the forward projector and backprojector has superior noise properties to the other algorithms, the results suggest that the faster subtraction/addition schemes will probably prove most useful for routine clinical scatter compensation.",
Physical database design for data warehouses,"Data warehouses collect copies of information from remote sources into a single database. Since the remote data is cached at the warehouse, it appears as local relations to the users of the warehouse. To improve query response time, the warehouse administrator will often materialize views defined on the local relations to support common or complicated queries. Unfortunately, the requirement to keep the views consistent with the local relations creates additional overhead when the remote sources change. The warehouse is often kept only loosely consistent with the sources: it is periodically refreshed with changes sent from the source. When this happens, the warehouse is taken off-line until the local relations and materialized views can be updated. Clearly, the users would prefer as little down time as possible. Often the down time can be reduced by adding carefully selected materialized views or indexes to the physical schema. This paper studies how to select the sets of supporting views and of indexes to materialize to minimize the down time. We call this the view index selection (VIS) problem. We present an A* search based solution to the problem as well as rules of thumb. We also perform additional experiments to understand the space-time tradeoff as it applies to data warehouses.",
A multi-sexual genetic algorithm for multiobjective optimization,"In this paper a new method for solving multicriteria optimization problems by Genetic Algorithms is proposed. Standard Genetic Algorithms use a population, where each individual has the same sex (or has no sex) and any two individuals can be crossed over. In the proposed Multisexual Genetic Algorithm (MSGA), individuals have an additional feature, their sex or gender and one individual from each sex is used in the recombination process. In our multicriteria optimization application there are as many sexes as optimization criteria and each individual is evaluated according to the optimization criterion related to its sex. Furthermore, a multi-parent crossover is applied to generate offspring of parents belonging to all different sexes, so the offspring represents intermediate solutions not totally optimal with respect to any single criterion. During the execution of the algorithm the set of nondominated solutions is updated and this set is presented as the output of MSGA at the end.","Genetic algorithms,
Optimization methods,
Biomedical engineering,
Computer science,
Decision making,
Sorting,
Constraint optimization,
Design optimization,
Decision feedback equalizers"
Nondeterministic polynomial time versus nondeterministic logarithmic space: time-space tradeoffs for satisfiability,"We give the first nontrivial model-independent time-space tradeoffs for satisfiability. Namely, we show that SAT cannot be solved simultaneously in n/sup 1+0(1)/ time and n/sup 1-/spl epsiv// space for any /spl epsiv/>0 on general random-access nondeterministic Turing machines. We also give several other related results. Our proof uses two basic ideas. First we show that if SAT can be solved nondeterministically with a small amount of space then we can collapse a nonconstant number of levels of the polynomial-time hierarchy. Next we extend work of V. Nepomnjascii (1970) to show that a nondeterministic computation of super linear time and sublinear space can be simulated in alternating linear time. Combining these facts with simple diagonalization yields our main result. We discuss how these bounds lead to a new approach to separating the complexity classes NL and NP. We give some possibilities and limitations of this approach.",
Wavelet representations for monitoring changes in teeth imaged with digital imaging fiber-optic transillumination,"Digital imaging fiber-optic transillumination (DIFOTI) is a novel method to detect and monitor dental caries, using light, a charge-coupled device (CCD) camera, and computer-controlled image acquisition. The advantages of DIFOTI over radiography include: no ionizing radiation, no film, real-time diagnosis, and higher sensitivity in detection of early lesions not apparent to X-ray, as demonstrated in vitro. Here, we present a method of processing DIFOTI images, acquired at different times, for monitoring changes. Of central importance to this application is pattern matching of image frames that is invariant to translation and rotation of a tooth, relative to the field of view of the imaging camera, and that is robust to changes in illumination source intensity. Our method employs: (1) wavelet modulus maxima representations for segmentation of teeth images; (2) first and second moments of gray level representations of DIFOTI images in the spatial domain, to estimate tooth location and orientation; and (3) multiresolution wavelet magnitude representations for quantitative monitoring. Even with illumination source intensity variation, it is demonstrated in vitro that such wavelet representations can facilitate detection of simulated clinical changes in light transmission that cannot be detected in the spatial domain.",
Private simultaneous messages protocols with applications,"We study the Private Simultaneous Messages (PSM) model which is a variant of the model proposed in Feige et. al., (1994). In the PSM model there are n players P/sub 1/, ..., P/sub n/, each player P/sub i/ holding a secret input x/sub i/ (say, a bit), and all having access to a common random string. Each player sends a single message to a special player, Carol, depending an its own input and the random string (and independently of all other messages). Based on these messages, Carol should be able to compute f(x/sub 1/, ..., x/sub n/) (for some predetermined function f) but should learn no additional information on the values of x/sub 1/, ..., x/sub n/. Our results go in two directions. First, we present efficient PSM protocols, which improve the efficiency of previous solutions, and extend their scope to several function classes for which no such solutions were known before. These classes include most of the important linear algebraic functions; as a result, we get efficient constant-round private protocols (in the standard model) for these classes. Second, we present reductions that allow transforming PSM protocols into solutions for some other problems, thereby demonstrating the power of this model. An interesting reduction of this sort shows how to construct, based on a standard (/sub 1//sup 2/)-OT (Oblivious Transfer) primitive, generalized-OT (GOT) primitives that, we believe, might be useful for the design of cryptographic protocols.",
Digital watermarking for video,"An MPEG-based technique for embedding digital ""watermarks"" into digital video is proposed. The watermarking technique has been proposed as a method of hiding secret information in the signals so as to discourage unauthorized copying or attest the origin of the media. In our proposed method, we take advantage of prediction types of the MPEG standard to watermark both intraframe and non-intraframe blocks with different residual masks. The experimental results show that the proposed watermarking technique results in an almost invisible difference between the watermarked frames and the original frames, and is also robust to clipping operations and MPEG compression.",
Model-Checking of Real-Time Systems: A Telecommunications Application Experience Report,,
A dynamic individualized location management algorithm,"The challenge of supporting rapidly growing numbers of mobile subscribers, while constrained by limited radio spectrum, is being met through increasingly smaller radio cells. This, however, results in increased signalling for location management procedures, which reduces the bandwidth available for user traffic. Location areas in current systems, such as GSM, consist of static and arbitrarily-defined collections of cells, which do not take into account individual subscriber mobility patterns, either in space or time. A location management algorithm is proposed which uses the mobility history of individual subscribers to dynamically create individualized location areas, based on previous movements from cell to cell. The average duration spent in each visited cell is also maintained and is used to define paging which are most likely to contain the subscriber. An activity-based mobility model was developed to test the proposed algorithm. Overall, the dynamic algorithm incurred significantly lower location management costs, in terms of signalling messages generated, for all parameters examined.",
PARDIS: A parallel approach to CORBA,"This paper describes PARDIS, a system containing explicit support for interoperability of PARallel DIStributed applications. PARDIS is based on the Common Object Request Broker Architecture (CORBA). Like CORBA, it provides interoperability between heterogeneous components by specifying their interfaces in a meta-language, the CORBA IDL, which call be translated into the language of interacting components. However, PARDIS extends the CORBA object model by introducing SPMD objects representing data-parallel computations. SPMD objects allow the request broker to interact directly with the distributed resources of a parallel application. This capability ensures request delivery to all the computing threads of a parallel application and allows the request broker to transfer distributed arguments directly between the computing threads of the client and the server. To support this kind of argument transfer, PARDIS defines a distributed argument type-distributed sequence-a generalization of CORBA sequence representing distributed data structures of parallel applications. In this paper we will give a brief description of basic component interaction in PARDIS and give an account of the rationale and support for SPMD objects and distributed sequences. We will then describe two ways of implementing argument transfer in invocations on SPMD objects and evaluate and compare their performance.",
A data model and semantics of objects with dynamic roles,"Although the concept of roles is becoming a popular research issue in object oriented databases and has been proven to be useful for dynamic and evolving applications, it has only been described conceptually in most of the previous work. Moreover, the important issues such as the semantics of roles (e.g., message passing) are seldom discussed. Furthermore, none of the previous work has investigated the idea of role player qualification, which models the fact that not every object is qualified to play a particular role. We present a data model and the semantics of roles. We discuss each of the above issues and illustrate the ideas with examples. From these examples, we can easily see that the problems we discussed are fundamental and indeed exist in many complex applications.",
Exploiting locality for data management in systems of limited bandwidth,"This paper deals with data management in computer systems in which the computing nodes are connected by a relatively sparse network. We consider the problem of placing and accessing a set of shared objects that are read and written from the nodes in the network. These objects are, e.g., global variables in a parallel program, pages or cache lines in a virtual shared memory system, shared files in a distributed file system, or pages in the World Wide Web. A data management strategy consists of a placement strategy that maps the objects (possibly dynamically and with redundancy) to the nodes, and an access strategy that describes how reads and writes are handled by the system (including the routing). We investigate static and dynamic data management strategies.",
Formalizing and Integrating the Dynamic Model within OMT,,
Does parallel repetition lower the error in computationally sound protocols?,"Whether or not parallel repetition lowers the error has been a fundamental question in the theory of protocols, with applications in many different areas. It is well known that parallel repetition reduces the error at an exponential rate in interactive proofs and Arthur-Merlin games. It seems to have been taken for granted that the same is true in arguments, or other proofs where the soundness only holds with respect to computationally bounded parties. We show that this is not the case. Surprisingly, parallel repetition can actually fail in this setting. We present four-round protocols whose error does not decrease under parallel repetition. This holds for any (polynomial) number of repetitions. These protocols exploit non-malleable encryption and can be based on any trapdoor permutation. On the other hand we show that for three-round protocols the error does go down exponentially fast. The question of parallel error reduction is particularly important when the protocol is used in cryptographic settings like identification, and the error represents the probability that an intruder succeeds.",
Stability and statistical properties of second-order bidirectional associative memory,"In this paper, a bidirectional associative memory (BAM) model with second-order connections, namely second-order bidirectional associative memory (SOBAM), is first reviewed. The stability and statistical properties of the SOBAM are then examined. We use an example to illustrate that the stability of the SOBAM is not guaranteed. For this result, we cannot use the conventional energy approach to estimate its memory capacity. Thus, we develop the statistical dynamics of the SOBAM. Given that a small number of errors appear in the initial input, the dynamics shows how the number of errors varies during recall. We use the dynamics to estimate the memory capacity, the attraction basin, and the number of errors in the retrieved items. Extension of the results to higher-order bidirectional associative memories is also discussed.",
Image-based prediction of landmark features for mobile robot navigation,"We have been developing an architecture for vision-based navigation which relies on continuous feedback from visual ""landmarks"" to control robot motion, In this approach, landmarks are consistently located and acquired as they come into view. To make this process efficient and robust, it is important that the image locations of these features can be predicted from available image information. In this article, we discuss methods for direct image-based prediction of point and line features for a mobile system operating on a planar surface. Preliminary experimental results suggest that image-based prediction con be performed efficiently and with sufficient accuracy to ensure robust acquisition of navigational landmarks.",
Software organization for dynamic and adaptable wearable systems,"There is a growing interest in a class of systems having dynamic and adaptable properties. In this paper we discuss our work on one subclass of such systems, that of wearable computers. In particular, our interest is in the software organization necessary to build wearable computing systems. We will examine some of the key properties of such a software organization such as the ability to rapidly and dynamically reconfigure software to meet both physical changes and information changes of the wearable system. This has lead us to study a middleware layer for a wearable system that supports dynamic reconfiguration. The middleware approach is studied in the context of the NETMAN-a network maintenance assistant. Current results from the system evaluations and the final system requirements and open issues are presented.",
Performance-based design of distributed real-time systems,"The paper presents a design method for distributed systems with statistical, end-to-end real-time constraints, and with underlying stochastic resource requirements. A system is modeled as a set of chains, where each chain is a distributed pipeline of tasks, and a task can represent any activity requiring nonzero load from some CPU or network resource. Every chain has two end-to-end performance requirements: its delay constraint denotes the maximum amount time a computation can take to flow through the pipeline, from input to output. A chain's quality constraint mandates a minimum allowable success rate for outputs that meet their delay constraints. The design method solves this problem by deriving (1) a fixed proportion of resource load to give each task; and (2) a deterministic processing rate for every chain, an which the objective is to optimize the output success rate (as determined by an analytical approximation). They demonstrate their technique on an example system, and compare the estimated success rates with those derived via simulated on-line behavior.",
Static scheduling of pipelined periodic tasks in distributed real-time systems,"Many distributed real time applications involve periodic activities with end to end timing constraints that are larger than the periods. That is, a new instance of a periodic activity will come into existence before the previous instance has been completed. Also, such activities typically involve communicating modules in a distributed system where some modules may be replicated for resilience. For such activities, pipelined execution allows us to meet the various resource and timing constraints imposed on them. We discuss an approach to dealing with the pipelined execution of a set of periodic activities that have the above characteristics. It can be called a meta algorithm since it works in conjunction with another scheduling algorithm-one that creases the actual schedules. The idea is to exploit the existence of many such scheduling algorithms, which, however typically work with activities whose deadlines are equal to or less than their periods. Our meta algorithm invokes such a scheduling algorithm, perhaps multiple times, to generate a pipelined execution for the tasks. The effectiveness of the approach is shown via simulation studies.",
Evolution in polarimetric signatures of thin saline ice under constant growth,"An experiment is carried out to measure polarimetric backscatter signatures at C band together with physical characteristics of thin saline ice grown at a constant rate under quiescent conditions. The objectives are to investigate the electromagnetic scattering mechanism in saline ice, to relate the polarimetric backscatter to ice physical characteristics, and to assess the inversion of ice thickness from backscatter data. Controlled laboratory conditions are utilized to avoid complicated variations in interrelated characteristics of saline ice and the environment. The ice sheet was grown in a refrigerated facility at the U.S. Army Cold Regions Research and Engineering Laboratory. Growth conditions, thickness and growth rate, temperatures and salinities, and internal and interfacial structures of the ice sheet were monitored. Measurements indicate that the laboratory saline ice has characteristics similar to thin sea ice in the Arctic. A strong increase of 610 dB is observed in backscatter as the ice grows from 3 to 11.2 cm in thickness. Ice characteristics and processes suggest that the large enhancement in backscatter relates to the interconnection and increase in the size of brine inclusions during the desalination process. Polarimetric signatures calculated with a physically based sea ice model agree with backscatter data at incident angles from 20 to 35 over the thickness range of the ice growth. Furthermore, backscattering coefficients of the saline ice sheet are shown to be similar to airborne radar measurements of thin sea ice growing in a newly opened lead in the Beaufort Sea. For the inversion the large increase in backscatter indicates that the ice thickness is retrievable for thin ice grown under the conditions in this experiment. More complicated conditions should be considered in future experiments to study their effects on the retrieval of sea ice parameters.",
Automatic knowledge acquisition for spatial document interpretation,"In this paper, a qualitative representation for the layout of structured documents as well as classes of documents is presented, which is established by means of supervised learning from a labeled training set of documents. For this formal representation, an inference algorithm has been developed, adopted from error-tolerant subgraph isomorphism, which assigns logic labels to the layout objects of a test document.",
Learning noisy perceptrons by a perceptron in polynomial time,"Learning perceptrons (linear threshold functions) from labeled examples is an important problem in machine learning. We consider the problem where labels are subjected to random classification noise. The problem was known to be PAC learnable via a hypothesis that consists of a polynomial number of linear thresholds (due to A. Blum, A. Frieze, R. Kannan, and S. Vempala (1996)). The question of whether a hypothesis that is itself a perceptron (a single threshold function) can be found in polynomial time was open. We show that indeed, noisy perceptrons are PAC learnable with a hypothesis that is a perceptron.",
Multiscale Bayesian estimation of Poisson intensities,"Many important phenomena in science and engineering are well modeled as Poisson processes. In some applications, photon imaging, for example, it is of great interest to accurately estimate the intensities underlying the observed Poisson data. We present a novel multiscale Bayesian approach to this problem. We show that Bayesian estimation in a multiresolution framework provides a very natural and powerful method for estimating the underlying intensity. Within this framework, we devise Bayesian priors suitable for a wide class of real-world processes. The resulting Bayes-optimal estimators have a simple and elegant form that leads to an efficient implementation.",
Topographic Receptive Fields and Patterned Lateral Interaction in a Self-Organizing Model of the Primary Visual Cortex,"This article presents a self-organizing neural network model for the simultaneous and cooperative development of topographic receptive fields and lateral interactions in cortical maps. Both afferent and lateral connections adapt by the same Hebbian mechanism in a purely local and unsupervised learning process. Afferent input weights of each neuron self organize into hill-shaped profiles, receptive fields organize topographically across the network, and unique lateral interaction profiles develop for each neuron. The model demonstrates how patterned lateral connections develop based on correlated activity and explains why lateral connection patterns closely follow receptive field properties such as ocular dominance.",
Towards standard for experiments in program comprehension,"Program comprehension can make a unique contribution to the field of software engineering because it is feasible to validate its claims with inexpensive experiments. To fully realize this unique position, program comprehension researchers need to develop standards that will guide them in designing experiments and allow them to judge the strength of an experiment in supporting a claim. To begin the discussion leading to such standards, we propose that program comprehension experiments always measure and interpret the following dependent variables: accuracy, accurate response time, and inaccurate response time.",
On a programmable approach to introducing digital design,"Introductory approaches to digital design have traditionally centered on small-scale integration as the implementation technology. An approach which centers on programmable logic devices is now much more appropriate given their widespread adoption by industry. This can support all standard concepts in combinational and sequential design with the advantage of introducing a technology now standard for implementation. Programmable array logic (PAL) technology appears most suited to an introductory approach since it is low-cost with clear extension to advanced programmable logic techniques; there are many PAL specification (programming) systems and many PAL devices available. There is no ideal ""educational"" PAL since commercial design is a compromise of great complexity. A review of several systems which can be used to specify PAL interconnect (covering PALASM, PLPL, PLACE, ABEL-SE, and MAX+PLUS II) shows that all can support an introductory course, that there is limited standardization, and that there are a number of advantages offered by the more sophisticated packages. Using a programmable approach implies that the academic curriculum can be revised for a new implementation. Such revision is not new in a subject which has demonstrated awesome progress in its short history. The time is now right for programmable logic to give a new interpretation into the appropriate concepts for introducing students to digital design.",
Modifying min-cut for hardware and software functional partitioning,"The Kernighan/Lin heuristic, also known as min-cut, has been extended very successfully for circuit partitioning over several decades. Those extensions customized the heuristic and its associated data structure to rapidly compute the minimum-cut metric required during circuit partitioning thus, those extensions are not applicable to problems requiring other metrics. The author extends the heuristic for functional partitioning in a manner applicable to the codesign problem of hardware/software partitioning as well as to hardware/hardware partitioning. The extension customizes the heuristic and data structure to rapidly compute execution-time and communication metrics, crucial to hardware and software partitioning, and leads to near-linear time-complexity and excellent results. The experiments demonstrate extremely fast execution times (just a few seconds) with results matched only by the much slower simulated annealing heuristic, meaning that the extended Kernighan/Lin heuristic will likely prove hard to beat for hardware and software functional partitioning.",
Implementing multiply-accumulate operation in multiplication time,"Multiply-accumulate is an important and expensive operation. It is frequently used in digital signal processing and video/graphics applications. As a result, any improvement in the delay for performing this operation could have a positive impact on clock speed, instruction time and processor performance. In this paper, we show how, by extending our view of a parallel multiplier, we can apply recent innovations in parallel multiplier design to multiply-accumulators. This application results in multiply-accumulators that are as fast as multipliers of the same size (these multipliers have been shown to result in provably optimal delays faster than current designs). This allows a single (optimal multiply-accumulate) circuit to be used for both operations without delay penalty. As a result, multiply-accumulate can be efficiently and effectively implemented as an instruction in RISC CPUs. Additionally, the circuit design reduces the number of devices needed over current fast multiplier designs, so that real estate and power savings also result.",
Improved on-line handwriting recognition using context dependent hidden Markov models,"The paper presents the introduction of context dependent hidden Markov models for cursive, unconstrained handwriting recognition with large vocabularies. Since context dependent models were successfully introduced to speech recognition (R. Bahl et al., 1980; R. Schwartz et al., 1984; K. Lee, 1990), it seems obvious, that the use of trigraphs could also lead to improved online handwriting recognition systems (A. Kosmala et al., 1997). In analogy to triphones in speech recognition, trigraphs are context dependent sub word units representing a single written character in its left and right context. The tests were conducted on a writer dependent system with three different writers and two different vocabulary sizes (1000 words and 30000 words). The results we obtained with the trigaph based system compared to the monograph system, are very encouraging: a mean relative error reduction of 46% for the 1000 word handwriting recognition system and a mean relative error reduction of 37% for the same system with the 30000 word vocabulary. We believe that this represents one of the first systematic investigations of the influence of context dependent models and parameter reduction methods for a difficult large vocabulary handwriting recognition task.",
A genetic tuning algorithm of PID parameters,"PID control schemes have been widely used in most of process control systems represented by chemical process for a long time. However, it is still a very important problem how to determine or tune the PID parameters, because these parameters have a great influence on the stability and the performance of the control system. On the other hand, in the last twenty years, a genetic algorithm is attracted as one method which gives us optimal answers for search, optimization and machine learning problems. In this paper, we propose a new genetic tuning algorithm of PID parameters, in which the search area of PID parameters is reduced sharply by considering an effective parameters' area from the viewpoint of the control performance.",
Assessment and evaluation in problem-based learning,"While problem-based learning (PBL) is frequently a culture shock to new students faced with its alien paradigm, one might say the same thing about the shock to educators who try to use this technique in their classes. Besides the fundamental challenge of creating a good problem, educators are faced with the task of deciding how to evaluate the technique's effectiveness and how to assess whether students have met the overall learning objectives for the course. It is the authors' contention that traditional assessment techniques such as the familiar multiple-choice and true-false examinations do little to truly assess a student's understanding and far-transfer of the PBL learning experience. Likewise evaluating the success of PBL as compared to more traditional lecture-based classes requires more complex techniques.",
Nonprehensile manipulation for orienting parts in the plane,"The authors previously (1996) presented a model of nonprehensile manipulation, using two one-degree-of-freedom palms. Under the assumptions of low friction and quasistatic motion, we developed a planning method for part reorientation with our model, starting from a known initial state. Our method finds feasible paths through the space of equivalent state configurations of the object in the palms, without requiring that the palms maintain stable support of the object over the entire path. We have shown that such a device can reliably orient parts in the plane. In this paper we extend our method to the case of reorienting a part to a desired goal from an unknown initial state. In addition to the all sliding contacts case which the model is based upon, we look at extensions to rolling contacts. We include the results of tests with example plans.",
Deniable password snatching: on the possibility of evasive electronic espionage,"Cryptovirology has recently been introduced as a means of mounting active viral attacks using public key cryptography. It has been shown to be a tool for extortion attacks and ""electronic warfare"", where attacks are mounted against information resources. The natural question to ask is whether Cryptovirology is also useful in the area of spying via malware. We demonstrate that Cryptovirology does help in ""electronic espionage"" and allows the spy to conceal his or her identity (as well as past collected information). Specifically, we present an attack that can be mounted by a cryptotrojan that allows the attacker to gather information (passwords) from a system in such a way that the attacker cannot be proven guilty beyond reasonable doubt. That is, even if the attacker is under surveillance on the local machine from when he first attacks the target machine, to when he obtains the passwords, and even if the leaked information is made available to the attacker exclusively, he still cannot be caught. The threat is made possible by the combination of public key cryptography, probabilistic encryption, and the use of public information (I/O or communication) channels which together form a ""secure receiver-anonymous channel"". The machine can be standalone or networked. What we learn from the attack is extracted as general tools and basic principles for ""espionage attacks"".",
Selecting behaviors using fuzzy logic,"Behavior-based systems, i.e. systems that use behaviors as a way of decomposing the control policy needed for accomplishing a task, are very useful in making robots cope with the dynamics of real-world environments. However, these systems still need to be extended to design robots having multiple and possibly conflicting goals, requiring planning, and getting more out of their behaviors. One way of doing that is to allow behaviors to be selected dynamically and to blend their actions in order to get more complex behaviors. This paper addresses these issues by presenting a control architecture that, when using fuzzy logic, allows behaviors to be efficiently selected by different sources. A simulated world for mobile robots is used to illustrate these ideas.",
Learning force sensory patterns and skills front human demonstration,"The motivation behind this work is to transfer force-based assembly skills to robots by using human demonstration. For this purpose, we model the skills as a sequence of contact formations (which describe how a workpiece touches its environment) and desired transitions between contact formations. In this paper, we present a method of identifying single-ended contact formations from force sensor patterns. Instead of using geometric models of the workpieces, fuzzy logic is used to learn and model the patterns in the force signals. Membership functions are generated automatically from training data and then used by the fuzzy classifier. This classification scheme is used to learn desired sequences of contact formations which comprise a force-based skill. Experimental results are presented which use the technique to extract skill information from human demonstration data.",
A parallel hybrid genetic algorithm simulated annealing approach to finding most probable explanations on Bayesian belief networks,"Bayesian belief networks are an important knowledge structure for reasoning under uncertainty. In the most probable explanation (MPE) problem, also known as the maximum a-posteriori (MAP) assignment problem, the objective is to assign truth values to network variables in a way that will maximize their joint probability conditioned on the evidence to be explained. This problem has recently been shown to be NP-hard for general belief networks and for large networks, exact solution methods are not practical. In this paper, we present a parallel processing technique, particularly suitable for loosely-coupled multicomputers which combines genetic algorithms with simulated annealing. This method is applied to the MPE problem on Bayesian belief network and is found to be superior on the MPE problem to either genetic algorithms or simulated annealing separately.",
On the forms of locality over finite models,"Most proofs showing limitations of expressive power of first-order logic rely on Ehrenfeucht-Fraisse games. Playing the game often involves a nontrivial combinatorial argument, so it was proposed to find easier tools for proving expressivity bounds. Most of those known for first-order logic are based on its ""locality"", that is defined in different ways. In this paper we characterize the relationship between those notions of locality. We note that Gaifman's locality theorem gives rise to two notions: one deals with sentences and one with open formulae. We prove that the former implies Hauf's notion of locality, which in turn implies Gaifman's locality for open formulae. Each of these implies the bounded degree property, which is one of the easiest tools for proving expressivity bounds. These results apply beyond the first-order case. We use them to derive expressivity bounds for first-order logic with unary quantifiers and counting. Finally, we apply these results to relational database languages with aggregate functions, and prove that purely relational queries defined in such languages satisfy Gaifman's notion of locality. From this we derive a number of expressivity bounds for languages with aggregates.",
Recognition of printed Arabic text using neural networks,"The main theme of the paper is the automatic recognition of Arabic printed text using artificial neural networks in addition to conventional techniques. This approach has a number of advantages: it combines rule based (structural) and classification tests; feature extraction is inexpensive; and execution time is independent of character font and size. The technique can be divided into three major steps: The first step is preprocessing in which the original image is transformed into a binary image utilizing a 300 dpi scanner and then forming the connected component. Second, global features of the input Arabic word are then extracted such as number of subwords, number of peaks within the subword, number and position of the complementary character, etc. Finally, an artificial neural network is used for character classification. The algorithm was implemented on a powerful MS-DOS microcomputer and written in C.",
An investigation of the use of trigraphs for large vocabulary cursive handwriting recognition,"This paper presents an extensive investigation of the use of trigraphs for online cursive handwriting recognition based on hidden Markov models (HMMs). Trigraphs are context dependent HMMs representing a single written character in its left and right context, similar to triphones in speech recognition. Looking at the great success of triphones in continuous speech recognition, it was always a challenging and open question, if the introduction of trigraphs could lead to substantially improved handwriting recognition systems. The results of this investigation are indeed extremely encouraging: the introduction of suitable trigraphs led to a 50% relative error reduction for a writer dependent 1000 word handwriting recognition system, and to a 35% relative error reduction for the same system with an extended 30000 word vocabulary for cursive handwriting recognition.",
KGPMIN: an efficient multilevel multioutput AND-OR-XOR minimizer,"In the domain of combinational logic synthesis, logic minimization plays a vital role in determining the area and performance of the synthesized circuit. Logic minimization based on AND-OR decomposition of functions is a well studied area. However, minimization based on AND-XOR decomposition has received relatively lesser attention. Since many real-life combinational functions are XOR dominated, a logic minimizer producing efficient AND-XOR decomposition can lead to more efficient realization of such circuits. The computer-aided design tool KGPXORMIN presented in this paper is a multilevel AND-XOR minimizer which outperforms the scheme reported by Saul (1991) by 45.77% in the literal count metric. In general, most of the real-life and benchmark circuits are a combination of OR and XOR logic. In order to have area efficient realization, we need to have an efficient minimizer capable of judicious use of OR and XOR gates. An integrated tool KGPMIN has been developed which combines the AND-XOR minimizer KGPXORMIN and well-known AND-OR minimizer MISII. Depending on the measure of dominance of OR and XOR logic, it switches from one minimizer to the other during the decomposition phase. By judicious switching from one minimizer to the other, on the average, KGPMIN outperforms MISII by 64.08% in literal count and 45.16% in absolute gate area for the MCNC combinational logic benchmarks. It also outperforms KGPXORMIN by 17.46% in literal count and 34.32% in gate area. The number of levels of the circuits synthesized with KGPMIN can be found to be comparable with the figures arrived at from the application of MISII.",
Continuation models are universal for /spl lambda//sub /spl mu//-calculus,"We show that a certain simple call-by-name continuation semantics of Parigot's /spl lambda//sub /spl mu//-calculus (1992) is complete. More precisely, for every /spl lambda//spl mu/-theory we construct a cartesian closed category such that the ensuing continuation-style interpretation of /spl lambda//sub /spl mu//, which maps terms to functions sending abstract continuations to responses, is full and faithful. Thus, any /spl lambda//sub /spl mu//-category in the sense of is isomorphic to a continuation model derived from a cartesian-closed category of continuations.",
How to integrate mobile agents into Web servers,"Mobile agents are a new paradigm for communication and cooperation in distributed computing. To combine the new paradigm with the promising World Wide Web platform, we integrated mobile agents into Web Servers. In this way, mobile agents may travel from Web server to Web server to access their local data. The paper describes how we integrated mobile agents support into a Web server. We present the state of implementation, and we give an outlook on our future work.",
Justifying public participation in technical decision making,"The need for expert knowledge and expert knowledge-based action in an advanced technoscientific society poses a fundamental challenge to any attempt to involve the public in the specialized basis of this society, i.e. in technical decision-making. The concern is not so much with how to meet this challenge at the practical level; it is with how to ground or rationally defend the will to do so-and to provide a brief topology of discussions that are deepening this understanding. So the author sketches out reasons for participation-as prolegomena to undertaking the work of actually creating or institutionalizing such participation. The task is more theoretical than applied, but, like the mathematics and physics that commonly prepare the way for the engineering sciences and the practice of engineering design, the task is not without practical importance.",
A hierarchical processor scheduling policy for distributed-memory multicomputer systems,"Processor scheduling policies for distributed memory systems can be divided into space sharing or time sharing policies. In space sharing, the set of processors in the system is partitioned and each partition is assigned for the exclusive use of a job. In time sharing policies, on the other hand, none of the processors is given exclusively to jobs; instead, several jobs share the processors (for example, in a round robin fashion). There are advantages and disadvantages associated with each type of policy. Typically, space sharing policies are good at low to moderate system loads and when job parallelism does not vary much. However, at high system loads and widely varying job parallelism, time sharing policies provide a better performance. We propose a new policy that is based on a hierarchical organization that incorporates the merits of these two types of policies. The new policy is a hybrid policy that uses both space sharing as well as time sharing to achieve better performance. We demonstrate that, at most system loads of interest, the proposed policy outperforms both space sharing and time sharing policies by a wide margin.",
Efficient distributed selection with bounded messages,"We consider the problem of selecting the Kth smallest element of a set distributed among the sites of a communication network when the size of messages is bounded; that is, each message is a packet which contains at most c bits, where c/spl ges/1 is a constant. A general selection algorithm using packets is presented and its packet complexity is analyzed. Its complexity is shown to be a significant improvement for a large range of packet sizes over the existing bounds. The proposed technique is then instanciated for specific classes of network topologies; the resulting bounds either match or improve the ones of existing solutions for a large range of values of the packet size. Furthermore, it is bit optimal in star networks.",
MRE: a flexible approach to multi-resolution modeling,"Multi-resolution representation of simulated entities is considered essential for a growing portion of distributed simulations. Heretofore, modelers have represented entities at just one level of resolution, or have represented concurrent representations in an inconsistent manner. The authors address the question of the cost of maintaining multiple, concurrent representations. They present a brief overview of the concept of a multiple resolution entity (MRE) and attribute dependency graph (ADG) both originally described elsewhere, and then compare simulation and consistency costs of some approaches, including our own MRE/ADG-based approach, to multi-resolution modeling. The cost analysis presented is the first known analysis of its type, and will provide a basis for simulation designers to determine the best, and most cost-effective approach to supporting simulation of entities at different levels of resolution concurrently.",
On the cost effectiveness of logarithmic arithmetic for backpropagation training on SIMD processors,We show that backpropagation on a SIMD processor with logarithmic arithmetic uses less memory and may be up to 3.2 times more cost effective than on one with fixed point arithmetic when synthesized in the same technology.,
Relative performance of preemption-safe locking and non-blocking synchronization on multiprogrammed shared memory multiprocessors,"Most multiprocessors are multiprogrammed to achieve acceptable response time. Unfortunately inopportune preemption may significantly degrade the performance of synchronized parallel applications. To address this problem, researchers have developed two principal strategies for concurrent, atomic update of shared data structures: (1) preemption safe locking and (2) non blocking (lock free) algorithms. Preemption safe locking requires kernel support. Non blocking algorithms generally require a universal atomic primitive, and are widely regarded as inefficient. We present a comparison of the two alternative strategies, focusing on four simple but important concurrent data structures-stacks, FIFO queues, priority queues and counters-in microbenchmarks and real applications on a 12 processor SGI Challenge multiprocessor. Our results indicate that data structure specific non blocking algorithms, which exist for stacks, FIFO queues and counters, can work extremely well: not only do they outperform preemption safe lock based algorithms on multiprogrammed machines, they also out perform ordinary locks on dedicated machines. At the same time, since general purpose nonblocking techniques do not yet appear to be practical, preemption safe locks remain the preferred alternative for complex data structures: they outperform conventional locks by significant margins on multiprogrammed systems.",
A formal method for building concurrent real-time software,"Developing concurrent real-time programs is one of computer science's greatest challenges. Not only is such software expensive to manufacture, but its role in safety-critical systems demands that it be correct. Formal methods of program specification and refinement could strengthen the mathematical precision used to develop such software. Nevertheless, formalisms that embrace both real-time and concurrency requirements are only just emerging. The Quartz method treats time and functional behavior with equal importance in the development process. The authors argue that by modeling program development in a unified framework, we can increase our confidence in the correctness of real-time concurrent code.",
"Teaching software engineering by means of a ""virtual enterprise""","In this experience report we present our recently introduced software engineering curriculum, which focuses on multi-semester, interlaced projects. Our curriculum aims at a very application-oriented education in order to bridge the gap between studentship and professional employeeship. We explain our structure of the curriculum and its support of real, external projects. Sample projects are discussed and our conclusions are presented showing how software engineering as a science and as a profession can benefit from teaching it by real-life projects. One interesting aspect might be the comparison of our Austrian (and European) situation and results with the situation in Canada and the U.S.A.","Education,
Software engineering,
Virtual enterprises,
Educational products,
Computer science,
Computer industry,
Bridges,
Mathematics,
Current measurement,
Software measurement"
An entropy measure for power estimation of Boolean functions,"In this paper, we present a study on the relationship between entropy and the average power consumption, of circuits generated from Boolean functions. Based on a general-delay model, an entropy-based formulation for power estimation is derived from a large set of experimental data. The study shows that the entropy measure provides an effective power estimate for single-output and fully-correlated multiple-output functions. The study also shows that if entropy is used as a power measure, the internal structure of a circuit must be considered in order to achieve accurate power estimates for non-correlated multiple-output functions. Experiments on a set of benchmarks demonstrate that combining entropy-based power measures with input-output correlation analyses of logic functions leads to a viable measure for high-level power estimation.",
On the capability of delay tests to detect bridges and opens,"Recent empirical and simulation studies show that adding at-speed testing to the test suite helps in detecting defective ICs missed by slow-speed and I/sub DDQ/ testing. At-speed testing attempts to detect ICs with defects, like bridges and opens, which cause faulty dynamic logic behavior. Path delay tests and transition tests are two popular tests used during at-speed testing. We show that these tests often fail to detect many bridges and opens which cause faulty dynamic behavior. Computing at speed tests is therefore fundamentally different from computing delay tests for parametric testing and new techniques need to be developed.",
Necessary and sufficient conditions for deadlock in manufacturing systems,Scheduling resources to avoid deadlock in a manufacturing system has been studied extensively over the past decade. Previous work developed sufficient conditions to avoid deadlocked states. What distinguishes the different approaches is the number of nondeadlocked states that are allowed by the application of the method. This is an important consideration since in all the methods these non-deadlocked states are those with high resource allocation. This paper presents both sufficient and necessary conditions to determine deadlocked states in a manufacturing system. Several examples showing the applications of the theory are presented and are compared with other methods.,
Translating Object-Z specifications to object-oriented test oracles,This paper describes the translation of Object-Z specifications of container classes to C++ test oracle classes. It presents a three-stage translation process and describes how the derived test oracles are integrated into the ClassBench testing framework. The method caters for object-oriented features such as inheritance and aggregation. Translation issues and the limitations of the method are discussed. Our approach is illustrated with an example based on an integer set class.,
Line removal and restoration of handwritten characters on the form documents,"The off-line handwritten characters recorded on prescribed form documents may be overwritten by the lines of the form documents. Overwritten characters should be isolated in order to be recognized more effectively. However, removal of the lines causes breaks in the overwritten characters. Consequently, a character restoration process is necessary. In this paper, the shape types of overwritten characters are analyzed and a method of restoring characters that have been broken by line removal is proposed. A 97% correct restoration ratio was obtained through this method.",
Understanding organizational implications of change processes: a multimedia simulation approach,"Executive information systems (EIS), and other types of computer based and communication systems are increasingly used in companies to support major change processes leading to the redesign of work processes, information flows, responsibilities for resource allocation, and decision making. However, the high failure rate in implementing such systems is an indication of the resistance to change normally encountered in organizations and the limited skills of IS managers in the domain of change management. The ""EIS Simulation"", a multimedia business simulation, has been successfully used to increase managerial awareness of the dynamics and the problems arising when implementing information systems which have important implications for work processes and power redistribution within companies. The paper illustrates the innovative design of this multimedia simulation and the broader pedagogical value of such an experimental learning approach.",
Measuring asymmetries of skin lesions,"Since 1994, a clinical study has been established to digitize melanocytic lesions from patients who are referred to the Colored Pigment Lesion Clinic in the University of British Columbia. In the past, we have been using circularity as the main feature to reflect the asymmetrical aspect of skin lesions. However, its significance often depends on the accuracy of image segmentation while the borders of many lesions are often fuzzy and irregular. We investigate the use of symmetry distance (SD) to improve the measurement of the asymmetries of skin lesions. Two SDs, including the basic SD and the fuzzy SD, and the simple circularity are calculated based on the new set of color images which are digitized under the controlled environment.",
On the power of circular splicing systems and DNA computability,"From a biological motivation of the interactions between linear and circular DNA sequences, we propose a new type of splicing model called ""circular H systems"" and show that they have the same computational power as Turing machines. It is also shown that there effectively exists a universal circular H system which can simulate any circular H system with the same terminal alphabet, which strongly suggests a feasible design for a DNA computer based on circular splicing.",
The Semiconductor Simulation Hub: a network-based microelectronics simulation laboratory,"This paper reports on the Semiconductor Simulation Hub, a working, network-based virtual laboratory that allows geographically distributed users to share and run existing tools via standard web browsers (""http://www.ecn.purdue.edu/labs/punch""). The paper describes the capabilities of the Hub, briefly discusses the associated software infrastructure, and elaborates on the Hub's applications in education, research, and industry. Finally, directions of further development are mentioned.",
Adding memory to the Evolutionary Planner/Navigator,"The integration of evolutionary approaches with adaptive memory processes is emerging as a promising new area for research and practical applications. In this paper, we report our study on adding memory to the Evolutionary Planner/Navigator (EP/N), which is an adaptive planning/navigation system for mobile robots based on evolutionary computation. Preliminary results from our experiments demonstrate the potential of such extension to EP/N in improving planning effectiveness in partially-known environments.",
Verbmobil: the combination of deep and shallow processing for spontaneous speech translation,"Verbmobil is a speech-to-speech translation system for spontaneously spoken negotiation dialogs. The actual system translates 74.2% of spontaneously spoken German input. We give an overview of the Verbmobil system. After the introduction of the Verbmobil scenario and the unique constraints of the project, we describe the underlying system architecture and its realization. The progress that was achieved on the end-to-end translation rate owes much to the increase of the word recognition rate from 45% in 1993 to 87% in 1996. In order to achieve the envisaged coverage on the uncertain speech recognizer output, deep and shallow approaches to the analysis and transfer problem had to be combined.",
Data compression using text encryption,"Summary form only given. We discuss the use of a new algorithm to preprocess text in order to improve the compression ratio of textual documents, in particular online documents such as web pages on the World Wide Web. The algorithm was first introduced in an earlier paper, and in this paper we discuss the applicability of our algorithm in Internet and Intranet environments, and present additional performance measurements regarding compression ratios, memory requirements and run time. Our results show that our preprocessing algorithm usually leads to a significantly improved compression ratio. Our algorithm requires a static dictionary shared by the compressor and the decompressor. The basic idea of the algorithm is to define a unique encryption or signature for each word in the dictionary, and to replace each word in the input text by its signature. Each signature consists mostly of the special character '*' plus as many alphabetic characters as necessary to make the signature unique among all words of the same length in the dictionary. In the resulting cryptic text the most frequently used character is typically the '*' character, and standard compression algorithms like LZW applied to the cryptic text can exploit this redundancy in order to achieve better compression ratios. We compared the performance of our algorithm to other text compression algorithms, including standard compression algorithms such as gzip, Unix 'compress' and PPM, and to one text compression algorithm which uses a static dictionary.",
An overview of legacy information system migration,"Legacy information systems typically form the backbone of the information flow within an organisation and are the main vehicle for consolidating information about the business. These systems also pose considerable problems: brittleness, inflexibility, isolation, nonextensibility, lack of openness etc., the so called legacy system problem which opens up a new research topic, legacy system migration. This paper provides a brief review of the main issues involved in legacy information systems migration.",
Evolving sorting networks using genetic programming and the rapidly reconfigurable Xilinx 6216 field-programmable gate array,This paper describes how the massive parallelism of the rapidly reconfigurable Xilinx XC6216 FPCA (in conjunction with Virtual Computing Corporation's HOT Works board) can be exploited to accelerate the computationally burdensome fitness measurement task of genetic algorithms and genetic programming. This acceleration is accomplished by embodying each individual of the evolving population into hardware in order to perform this time-consuming fitness measurement task. A 16-step sorting network for seven items was evolved that has two fewer steps than the sorting network described in the 1962 O'Connor and Nelson patent on sorting networks (and the same number of steps as a 7-sorter that was devised by Floyd and Knuth (1973) subsequent to the patent and that is now known to be minimal).,
An efficient fully parallel thinning algorithm,"The paper addresses an efficient parallel thinning algorithm based on weight-values. The weight-value of a black pixel is calculated by observing neighboring pixels, and it gives one an efficient way to decide whether the pixel is deleted or not. Owing to weight-values, the proposed algorithm uses only 3/spl times/3 templates. Furthermore, it examines only the elimination conditions corresponding the weight-value of boundary pixels, and all elimination conditions will not be searched as most other parallel iterative thinning algorithms. Thus, the execution time can be reduced a lot compared to that of previous approaches. The weight-value also allow one to deal with typical troublesome patterns efficiently. Without smoothing before thinning, the algorithm produces robust thinned images even in the presence of two pixel-width noises. The authors obtain encouraging results from extensive experiments.",
Aspects of controlling a multifingered gripper,"Controlling multifingered robot hands makes high demands on the control algorithms and the speed of the control computer. The nonlinear friction, the impact problem and other plant uncertainties require a special kind of control and tuning of the controller. Some simple linear and nonlinear controllers for the Karlsruhe Dexterous Hand are presented and the results and advantages of the controllers are shown. Also, a new adaptive fuzzy controller is presented to overcome the time consuming process of fine-tuning the membership functions. Finally, a short glance at the hardware platform is taken in order to show the control system architecture.",
An adaptable and reliable authentication protocol for communication networks,"We propose a new authentication and key distribution protocol which is adaptable and reliable for communication networks. The secrets for authentication, which are chosen from a relatively small space by common users, are easy to guess. Our protocol gives a solution to protect the weak secrets from guessing attacks. Compared with other related work, our protocol is more reliable because it is resistant to various kinds of attacks including guessing attacks, and more adaptable because it reduces several overheads which make the existing protocols more expensive. We show how to apply our protocol to the Q.931 calling sequences and to the World Wide Web model.",
"Effective coding for fast redundant adders using the radix-2 digit set {0,1,2,3}","We describe a redundant radix-2 representation with digit set {0,1,2,3} and an encoding using three bits per digit, instead of the minimum of two. This representation is then used to implement several adders, having different number of redundant and conventional operands. We show that the resulting adders are faster than those using carry-save representation. The evaluations are done for two libraries of standard cells. These adders have applications where redundant adders (with limited carry propagation) are used. This includes sequential and combinational accumulators and multipliers, CORDIC units, and digit-recurrences for operations such as division and square root. We also evaluate the effect of the proposed adders on the delay and size of a 54-bit tree multiplier.",
MinISO: a minimal independent system operator [electric power industry],"The independent system operator or ISO is the lead actor in the various proposals for a deregulated, competitive electric power industry. The ISO has three possible objectives: security maintenance; service quality assurance; and promotion of economic efficiency and equity. To achieve these objectives the ISO may be authorized to set the rules for transactions between suppliers and consumers, scheduling and dispatch of generators, loads and network: services, and energy markets. Proposals differ in their specification of the ISO's objectives and authority. Two ISO structures are contrasted. MaxISO, based on the UK-Poolco model, has ambitious objectives and much regulatory authority. Its scientific merit derives from an optimal power flow dispatch model. MinISO's objective is restricted to security, and its regulatory authority is correspondingly modest. MinISO seeks to provide direct consumer access. Its scientific merit is based on the coordinated multilateral trades model.",
Path planning in a 2-D known space using neural networks and skeletonization,"A neural network and a skeletonization based path planning in a 2D known space is presented. For the neural network path planning approach a Kohonen self-organizing net has been chosen, while for the skeletonization Kwok's method (1988) was used. The output of the network represents a reduced representation of the free space available for robotic movement in a 2D known environment.",
Selective focus as a means of improving geographically distributed embedded system co-simulation,"When dealing with communication-intensive systems, hardware/software co-simulation usually either requires the communication to be simulated with a uniformly lour level of detail or it performs poorly. This problem manifests itself even more strongly when considering geographically distributed co-simulation, where designers take advantage of proprietary component simulation models that are made available over the Internet. In such systems, much of the communication can potentially occur over the Internet, with even, more expensive and slower communication primitives. This paper presents a technique, called ""selective focus"", and a Java based tool which allow communication to be represented at various levels of abstraction thus giving the designer the ability to dynamically optimize inter-module communications and improve the performance of the cosimulation.",
A fuzzy Petri nets based mechanism for fuzzy rules reasoning,"The paper presents a mechanism for normalizing fuzzy rules of a rule-based system, transforming the rules into a fuzzy Petri net, and answering queries based on the net with imprecise information. In addition, the authors present an algorithm utilizing the formalism of Petri nets to compute the degree of truth of the resulting answers to the queries. The degree of truth is computed in algebraic form based on the state equation which can be implemented in matrix computation in Petri nets.",
Applying the CDIF standard in the construction of CASE design tools,"This paper proposes a new, integrated development approach to the construction of CASE design tools using the CDIF standard for repository data interchange. The research is based on the authors' practical experience of constructing CASE tools using both RDBMS and meta-CASE technologies. An example object-based, graphical design method and notation is used to illustrate these different approaches to CASE tool construction, and CASE tools implemented with these technologies are described. The proposed integrated approach combines meta-CASE with RDBMS technologies within a unified development environment using CDIF tools to exchange repository data. Results from prototype CDIF tools are presented, and directions for future research in this area are suggested.",
Compression of silhouette-like images based on WFA,Summary form only given. The authors present the design a lossy fractal compression method for silhouette-like bi-level images that has an excellent quality to compression rate ratio. Their approach is based on weighted finite automata (WFA). We reduce the problem of the encoding of a silhouette-like bi-level image to the encoding of two one-variable functions describing the boundary (-ies) of the black and white regions of the given image. One advantage is that the automata encoding different bitplanes can share states.,
Assemblability analysis with adjusting cost evaluation based on tolerance and adjustability,"Tolerances affect the assemblability of a product, which in turn affects the cost of the product because of the scrap cost, and wasted time and energy. In this paper, we propose a method to analyze the product in terms of assemblability based on tolerances and adjustability of the parts. More specifically, the product is considered assembled if the real parts can be fit regardless of the deviations the real parts may have from their nominal, because some parts can be adjusted in position after it is assembled due to clearances between the parts. Moreover, there is a cost associated with adjusting the parts because of the adjusting time, fixturing costs, and additional operations. A method is proposed to evaluated the adjusting cost by counting the minimum number of parts that must be moved or adjusted in order to complete the assembly.",
A relational account of call-by-value sequentiality,"We construct a model for FPC, a purely functional, sequential, call-by-value language. The model is built from partial continuous functions, in the style of Plotkin, further constrained to be uniform with respect to a class of logical relations. We prove that the model is fully abstract.",
On optimizing BIST-architecture by using OBDD-based approaches and genetic algorithms,"We introduce a two-staged Genetic Algorithm for optimizing weighted random pattern testing in a Built-in-Self-Test (BIST) environment. The first stage includes the OBDD-based optimization of input probabilities with regard to the expected test length. The optimization itself is constrained to discrete weight values which can directly be integrated in a BIST environment. During the second stage, the hardware-design of the actual BIST-structure is optimized. Experimental results are given to demonstrate the quality of our approach.",
The Flux OS Toolkit: reusable components for OS implementation,"To an unappreciated degree, research both in operating systems (OSs) and their programming languages has been severely hampered by the lack of cleanly reusable code providing mundane low-level OS infrastructure such as bootstrap code and device drivers. The Flux OS Toolkit solves this problem by providing a set of clean, well-documented components. These components can be used as basic building blocks both for operating systems and for booting language run-time systems directly on the hardware. The toolkit's implementation itself embodies reuse techniques by incorporating components such as device drivers, file systems and networking code, unchanged, from other sources. We believe the kit also makes feasible the production of highly assured embedded and operating systems: by enabling reuse of low-level code, the high cost of detailed verification of that code can be amortized over many systems for critical environments. The OS toolkit is already heavily used in several different OS and programming language projects, and has already catalyzed research and development that would otherwise never have been attempted.",
"Representation inheritance: a safe form of ""White box"" code inheritance","There are two approaches to using code inheritance for defining new component implementations in terms of existing implementations. Black box code inheritance allows subclasses to reuse superclass implementations as-is, without direct access to their internals. Alternatively, white box code inheritance allows subclasses to have direct access to superclass implementation details, which may be necessary for the efficiency of some subclass operations and to prevent unnecessary duplication of code. Unfortunately, white box code inheritance violates the protection that encapsulation affords superclasses, opening up the possibility of a subclass interfering with the correct operation of its superclass methods. Representation inheritance is proposed as a restricted form of white box code inheritance where subclasses have direct access to superclass implementation details, but are required to respect the representation invariant(s) and abstraction relation(s) of their ancestor(s). This preserves the protection that encapsulation provides, while allowing the freedom of access that component implementers sometimes desire.",
A new synthesis procedure for linear-phase paraunitary digital filter banks,"In this paper, a new design algorithm is presented for a family of linear phase paraunitary filter banks with generalized filter length and symmetric polarity. A number of new constraints on the distributions of filter length and symmetry polarity among the channels are derived. In the algorithm, the lengths of the filters are gradually reduced through a cascade of lattice structures. The derivations for filter banks with even and odd number of channels are formulated in a unified form.",
A language-based approach to construct structured and efficient object-based distributed systems,"Classical object properties such as encapsulation ease the construction of distributed systems. The object paradigm supports modeling of real-world problems in a natural way and delivers units of distribution to the resource management level. To enhance the performance of distributed systems, more detailed application-specific information like potential communication dependencies should be exploited. To fulfil this requirement, we propose a top-down driven, language-based approach to construct structured distributed systems. We introduce the object-based distributed programming language INSEL (INtegration and SEparation Language), that supports advanced structuring concepts. Structural dependencies between passive and active objects are determined at the application level and exploited by the resource management system to transform INSEL programs into efficient executables.",
Progress in dynamic programming search for LVCSR,"This paper gives an overview of recent improvements in dynamic programming search methods for large-vocabulary continuous speech recognition (LVCSR): searching using lexical trees, time-conditioned searching and word graph construction.",
A parallel algorithm for optimal task assignment in distributed systems,"An efficient assignment of tasks to the processors is imperative for achieving fast job turnaround time in a parallel or distributed environment. The assignment problem is well known to be NP-complete, except in a few special cases. Thus heuristics are used to obtain suboptimal solutions in reasonable amount of time. While a plethora of such heuristics have been documented in the literature, in this paper we aim to develop techniques for finding optimal solutions under the most relaxed assumptions. We propose a best-first search based parallel algorithm that generates optimal solution for assigning an arbitrary task graph to an arbitrary network of homogeneous or heterogeneous processors. The parallel algorithm running on the Intel Paragon gives optimal assignments for problems of medium to large sizes. We believe our algorithms to be novel in solving an indispensable problem in parallel and distributed computing.",
Beyond thresholds: an alternative method for extracting information from network measurements,"There is an increasing demand for higher levels of network availability and reliability. Effective network monitoring is necessary to meet this demand. Whereas most of the network monitoring research to date has been focused on combining the information collected in a meaningful way, in this research we focus on processing the information collected before it is combined. We propose a change detection methodology for each measurement variable, where we can detect changes from the variable's usual behavior. We provide a method for learning and adapting the variable behavior to keep pace with changes occurring in the network.",
Disparity component matching for visual correspondence,"We present a method for computing dense visual correspondence based on general assumptions about scene geometry. Our algorithm does not rely on correlation, and uses a variable region of support. We assume that images consist of a number of connected sets of pixels with the same disparity, which we call disparity components. Using maximum likelihood arguments, at each pixel we compute a small set of plausible disparities. A pixel is assigned a disparity d based on connected components of pixels, where each pixel in a component considers d to be plausible. Our implementation chooses the largest plausible disparity component; however, global contextual constraints can also be applied. While the algorithm was originally designed for visual correspondence, it can also be used for other early vision problems such as image restoration. It runs in a few seconds on traditional benchmark images with standard parameter settings, and gives quite promising results.",
Chaotic behavior of learning automata in multi-level games under delayed information,"Distributed decision makers are modeled as players in a game with two levels. High level decisions concern the game environment and determine the willingness of the players to form a coalition (or group). Low level decisions involve the actions to be implemented within the chosen environment. Decisions are made using probability distributions which are updated using a learning automaton scheme. A player has knowledge of another player's likelihood of making a particular decision but this information is delayed, perhaps due to network broadcasts or other environmental influences. These delays create the potential for instabilities in the decision making process and particular parameter settings can lead to period-doubling and the onset of chaos.",
Equivalence checking using abstract BDDs,"We introduce a new equivalence checking method based on abstract BDDs (aBDDs). The basic idea is the following: given an abstraction function, aBDDs reduce the size of BDDs by merging nodes that have the same abstract value. An aBDD has bounded size and can be constructed without constructing the original BDD. We show that this method of equivalence checking is always sound. It is complete for an important class of arithmetic circuits that includes integer multiplication. We also suggest heuristics for findings suitable abstraction functions based on the structure of the circuit. The efficiency of this technique is illustrated by experiments on ISCAS'85 benchmark circuits.",
A low-cost processor group membership protocol for a hard real-time distributed system,"Processor group membership protocols implement a service that allow processors to agree on which processors are operational. Implementations of group membership for hard real-time systems have concentrated on either reducing failure detection latency or minimizing message complexity. Instead, we present a protocol that uses shared resources-processor time and network bandwidth-as a small, bounded tax imposed on existing broadcast message traffic. In doing so, the group membership protocol can easily be taken into account by any schedulability analysis.",
Low-cost prevention of error-propagation for data compression with dynamic dictionaries,"In earlier work we presented the k-error protocol, a technique for protecting a dynamic dictionary method from error propagation as the result of any k errors on the communication channel or compressed file. Here we further develop this approach and provide experimental evidence that this approach is highly effective in practice against a noisy channel or faulty storage medium. That is, for LZ2-based methods that ""blow up"" as a result of a single error, with the protocol in place, high error rates (with far more than the k errors for which the protocol was previously designed) can be sustained with no error propagation (the only corrupted bytes decoded are those that are part of the string represented by a pointer that was corrupted).",
Hierarchical graphics databases in sort-first,"Sort-first architectures for parallel rendering use the natural frame-to-frame coherence of primitives on the screen to increase performance by grouping primitives that are close together on the screen onto the same processor. This technique reduces processor-to-processor communications overhead. As primitives move on the screen, they may also be moved to other processors. This migration poses many problems for the editing and traversal of hierarchical graphics databases (HGDs) such as PHIGS. The author describes the major problems associated with implementing an HGD on a sort-first system. He discusses the basics of HGDs and how they extend to parallel graphics systems. He shows why bookkeeping for HGDs is a more complex issue for sort-first than for other parallel architectures. He then examines two possible solution branches for this issue and delve into the design choices and implementation issues that arise with either method. The methods differ in the amount of bookkeeping that they do versus the amount of primitive cull-testing that must be performed.",
An argument in favor of the presumed commit protocol,"The authors argue in favor of the presumed commit protocol by proposing two new presumed commit variants that significantly reduce the cost of logging activities associated with the original presumed commit protocol. Furthermore, for read-only transactions, they apply their unsolicited update-vote optimization and show that the cost associated with this type of transactions is the same in both presumed commit and presumed abort protocols, thus, nullifying the basis for the argument that favors the presumed abort protocol. This is especially important for modern distributed environments which are characterized by high reliability and high probability of transactions being committed rather than aborted.",
Good processor management=fast allocation+efficient scheduling,"Fast and efficient processor allocation and job scheduling algorithms are essential components of a multi-user multicomputer operating system. In this paper we propose two novel processor management schemes which meet such demands for mesh-connected multicomputers. A stack-based allocation algorithm that can locate a free sub-mesh for a job very quickly using simple coordinate calculation and spatial subtraction is proposed. Simulation results show that the stack-based allocation algorithm outperforms all the existing allocation policies in terms of allocation overhead while delivering competitive performance. Another technique, called group scheduling, schedules jobs in such a way that the jobs belonging to the same group do not block each other. The groups are scheduled in an FCFS order to prevent starvation. This simple but efficient scheduling policy reduces the response rime significantly by minimizing the queueing delay for the jobs in the same group. These two schemes, when used together can provide faster service to users with very little overhead.",
Selectivity estimation for joins using systematic sampling,"Proposes a new approach to the estimation of join selectivity. The technique, which we have called ""systematic sampling"", is a novel variant of the sampling-based approach. Systematic sampling works as follows. Given a relation R of N tuples, with a join attribute that can be accessed in ascending/descending order via an index, if n is the number of tuples to be sampled from R, select a tuple at random from the first k=[N/n] tuples of R and every kth tuple thereafter. We first develop a theoretical foundation for systematic sampling which suggests that the method gives a more representative sample than the traditional simple random sampling. Subsequent experimental analysis on a range of synthetic relations confirms that the quality of sample relations (participating in a join) yielded by systematic sampling is higher than those produced by the traditional simple random sampling. To ensure that the sample relations produced by the systematic sampling indeed assist in computation for more accurate join selectivities, we compare the systematic sampling with the most efficient simple random sampling called t-cross, using a variety of star joins and a variety of relation configurations. The results demonstrate that, with the same amount of sampling, the systematic sampling can provide considerably more accurate join selectivities than the t-cross sampling.",
Analytical estimation of transition activity for DSP architectures,"We present an analytical method to estimate the average number of transitions, T, for a given DSP architecture. First, an exact relation between the transition activity, probability, and autocorrelation for a 1-bit signal is derived. Next, we estimate word-level signal transition activity using word-level signal statistics, the signal generation model, and the signal encoding. Input signal statistics are propagated through the DSP operators in a system and then employed to estimate T. Simulations with filters result in errors in T between 1% and 12%. Also, the transpose form is shown to have fewer signal transitions than the direct form for the same input.",
Covariance Learning of Correlated Patterns in Competitive Networks,"Covariance learning is a powerful type of Hebbian learning, allowing both potentiation and depression of synaptic strength. It is used for associative memory in feedforward and recurrent neural network paradigms. This article describes a variant of covariance learning that works particularly well for correlated stimuli in feedforward networks with competitive K-of-N firing. The rule, which is nonlinear, has an intuitive mathematical interpretation, and simulations presented in this article demonstrate its utility.",
A fast VP restoration scheme using ring-shaped sharable backup VPs,"Using backup virtual paths (BVPs) has been proposed as one of the approaches to restore failed VPs in ATM networks. This scheme simplifies the restoration process but may require longer restoration time and larger number of VPIs. We propose the ring BVP (RBVP) scheme which overcomes the longer restoration time and the over-provisioning problems of the previously proposed BVP scheme. The RBVP scheme maintains a number of ring shaped BVPs for each elementary cycle (e-cycle; a shortest possible cycle containing a specific edge) and allows all the links in an e-cycle to share the corresponding RBVP for any link failures in the e-cycle. Since in the RBVP scheme the restoration process is localized only to a failed link, failed VPs can be restored almost twice faster than the previously proposed BVP scheme. Also since a RBVP can be shared by more than one VPs, a smaller number of VPIs are allocated for the VP restoration.",
Improving software project management skills using a software project simulator,"Software project management skills are becoming an important component of software engineering education. Software engineers working in teams need to carefully plan and coordinate their efforts in order to be successful. Unfortunately, most universities provide inadequate education in software project management. Most use lecture-based approaches which provide the necessary steps in software project management, but are deficient in providing the students with hands-on experience. Software project simulation provides a bridge between course-based and hands-on experience. It provides an interactive environment of repeatable exercises. It also provides a medium for measurable evaluation of student performance which can be used to customize the education process to fit the needs of individual students. Our work focuses on using system dynamics modeling for simulating software development activities because of its ability to dynamically represent relevant project attributes in the software development process. A system dynamics model of the incremental software development process has been developed and validated. This paper describes a process for utilizing this system dynamics model to create simulation environments suitable for addressing specific education objectives. Benefits and guidelines for use of a tool of this kind are provided. Our experience with using this tool in a large class is also described.",
Region-based fractal image compression with quadtree segmentation,"Fractal image coding is a novel technique for still image compression. A low bit rate region-based fractal image compression algorithm is proposed and several techniques are included as follows. First, we improve the performance of quadtree segmentation by an adaptive threshold. Then, a merging scheme is employed to the resulting quadtree segmentation that combines several similar blocks into a small number of regions. We also provide a quadtree-based segmented chain code to efficiently record the contours of the regions. The experimental results show that the proposed scheme has the lowest bit rate among the existing schemes at the same level of image quality.",
Effect of tolerancing on the relative positions of parts in an assembly,"This paper analyzes the variations of the relative positions of the parts composing an assembly when part dimensions span their specified tolerance zones. We focus on the simplified case where each part is modeled as a toleranced polygon defined as follows: each edge of this polygon is supported by a line of fixed orientation lying anywhere in a strip bounded by two extreme lines; these lines are themselves defined in a coordinate system attached to the part. The relative placements of parts in an assembly A are defined by spatial relations, such as edge e of part P is parallel to edge f of part Q, at distance d. We define the relative position of any two parts, P and Q, in an instance of A as the transform (a translation in our case) between the coordinate systems attached to these parts. Because of the possible variations in the geometry of each individual part, the relative position of P and Q is not constant over multiple instances of A. Hence, the question: what is the range of possible relative positions of any two parts in A? This paper describes an efficient algorithm to solve this problem. Experimental results obtained with an assembly sequence planner that incorporates this algorithm are also presented.",
A compositional approach for designing multifunction time-dependent protocols,"We propose a framework based on the model of timed extended finite state machines for building communication protocols which perform several functions, where each function corresponds to a component protocol. For parallel composition, we specify a conjunctive relation which requires that the execution of events in two component protocols be synchronized. We also propose a predicate strengthening technique to refine the composite protocol in a stepwise manner while preserving the invariants of the component protocols. For sequential composition, we present a set of constraints, alternating, ordering and disabling, on the actions of the component protocols, and give sufficient conditions for the composite protocol to retain the safety properties such as freedom from unspecified receptions and freedom from deadlocks. Our sufficient conditions are weaker than those given an previous works.",
A hybrid genetic algorithm for the provision of isochronous service in high speed networks,"A hybrid genetic algorithm, which combines the concepts of genetic search and hillclimbing is proposed in order to solve the problem of Isochronous Channel Reusing (ICR) in the topologies of LANs/MANs. The problem of ICR, i.e. the task to assign channels to newcoming requests with the twofold issue of obtaining the least number of employed channels while no overlapped isochronous requests/connections share the same isochronous channel(s), is an NP complete optimization problem (Nen-Fu Huang and Huey-Ing Liu, 1994). The special features of the proposed algorithm, which we call Hybrid Genetic Reuse Algorithm (HGRA), that distinguish HGRA from the Simple Genetic Algorithm (SGA) (D.E. Goldber, 89) are: (1) a binary two dimensional encoding; (2) the use of a more advanced fitness function in the phase of selection; (3) a repair strategy until a feasible solution is produced; and (4) a local improvement operator. The computational complexity is also computed. The proposed algorithm was tested through simulation on a dual bus Metropolitan Area Network (DQDB) and was compared to a graph coloring algorithm named ICRA (Nen-Fu Huang and Huey-Ing Liu, 1994), presented on a DQDB network. The experimental results indicate that a genetic algorithm, with its properties: robustness, implicit parallelism, combination and exploration capabilities hybridized with a strong local search algorithm, can be used for finding good solutions for the problem of ICR, with good scalability property, comparable to those of ICRA and even better in case of heavy network load.",
Distance learning and use of the Internet and the World Wide Web in education,"The objective of this paper is to address educational opportunities through distance learning and how to use the Internet and the World Wide Web (WWW) to provide effective education to students at remote locations. A comparison between the traditional classroom verses distance learning is presented. Due to the unprecedented technological advancements in recent years, the role of computer technology in education has changed forever. An overview of various delivery mechanisms such as corresponding courses, taped lectures, interactive videos and the Internet are discussed. The current experience in setting up the first graduate course in the power electronics on the Web at the University of Central Florida (USA) is given as a case study.",
A loop parallelization method for nested loops with non-uniform dependences,"This paper proposes an efficient method of partitioning nested loops with non-uniform dependences for maximizing parallelism. Our approach is based on convex hull theory, and it will divide the iteration space of the loop into three regions as two parallel regions where the iterations can be fully executed in parallel and one parallelizable region where the iterations are inherently serial, but possible parallelism can be exploited. And in order to maximize parallelism from the parallelizable region, an algorithm using integer programming which partitions a loop into variable size partitions is also proposed. In comparison with some works on partitioning, the proposed method is a simple and exact partitioning method, and it gives much better speedup and extracts more parallelism than them.",
Laboratory exercises using a graphical user interface to a queueing simulator,"The study of queueing systems presents mathematical barriers to the novice, although the phenomena of queues can be readily observed in everyday life. In addition, English-speaking faculty have language barriers to surmount with Japanese students. This paper describes a graphical user interface to allow quick and easy comprehension of the dynamical behavior of queues. The interface is implemented using Tcl/Tk, a high-level Window programming script language that allows for the rapid development of educational software. An outline is presented for a set of exercises for a lower division laboratory. This material is also appropriate for laboratories associated with courses in operating systems, queueing systems, and performance evaluation.",
On radix representation of rings,"This paper presents a thorough analysis of radix representations of elements from general rings. In particular, we study the questions of redundancy, completeness and mappings into such representations. After a brief description of the more usual representations of integers, a more detailed analysis of various complex number systems is performed. This includes the ""classical"" complex number systems for the Gaussian integers, as well as the Eisenstein integers.",
Approximate parametrisations for adaptive feedback linearisation,"A generalisation of the result presented by Sastry-Isidori (1989) for the adaptive stabilisation/tracking of affine systems with a well defined global relative degree and linear parametric uncertainty is developed to include bounded output disturbances. From this, we develop neurocontrol results, for linearly weights approximants with compactly supported basis functions, where the disturbance term is used to handle the approximation error. In contrast to many results in the neurocontrol literature the control is a state feedback law, taking measurements from the original coordinates not from the normal form. A local in the weights result is given, and a global result is proved under some nonlinearity constraints inherited from Sastry-Isidori's result, where the dimension of the controller varies according to the size of the initial conditions and tracking signal.",
Six hypotheses in search of a theorem,We consider the following six hypotheses: *P=NP. *SAT is truth-table reducible to a P-selective set. *SAT is truth-table reducible to a k-approximable set for some k. *FP/sub /spl par///sup NP/=FP/sup NP[log]/ *SAT is O(log n)-approximable. *Solving SAT is in P on formulae with at most one assignment. We discuss their importance and relationships among them.,
Range image segmentation using an oscillatory network,"We use a locally excitatory globally inhibitory oscillator network (LEGION) as a framework for range image segmentation. Each oscillator in the LEGION network has excitatory lateral connections to the oscillators in its neighborhood as well as a connection with a global inhibitor. The lateral connection between two oscillators is established based on the similarity between their feature vectors which consist of the surface normal and curvature at the corresponding pixel locations. The emergent behavior of the LEGION network gives rise to the segmentation result. Unlike other methods, our scheme needs no assumption about the underlying structures in image data and no prior knowledge regarding the number of regions. Experimental results for real range images are presented.",
An estimation algorithm for neuromagnetic source distribution using MRI information,"We present an estimation algorithm for the location of magnetic sources in the human brain using magnetic resonance imaging (MRI) information. MRI information is used for limiting the estimation region to the cortex and analyzing acquired results. Magnetic fields of the brain are due to neural activity. As there are many neurons on the cortex, we regard it as a prime candidate for source location. The cortex is extracted from MR images. We assume that the points on the extracted cortex are current dipoles, and all the acquired points are considered as candidate sources. We then perform the estimation by means of the multiple signal classification (MUSIC) method. This algorithm can be considered as a first step in the estimation of the parameters of magnetic sources in the human brain. The acquired results may be interpreted as candidate locations at which sources exist, We perform computer simulations and apply this algorithm to real human brain data.",
The effect of bottleneck service rate variations on the performance of the ABR flow control,"One of the main features of ABR services is the employment of a rate-based flow control mechanism. Feedback from the network switches to the end systems gives users the information necessary to adjust their transmission rates appropriately according to the current network load. This type of control has been investigated to some extent, assuming a constant transmission capacity on a bottleneck link. In this paper we develop a discrete-time analysis to study the effect of bottleneck service rate variations on the performance of the ABR flow control mechanism. Looking at real systems, such variations occur on the one hand due to the establishment and release of ATM connections and on the other hand due to the varying bandwidth demand of VBR connections. To model the rate variations, the bottleneck service rate is assumed to follow a general probability density function.",
Estimating the size of an Estelle specification for a communication protocol,"Existing software size models estimate the size of an implementation of a software system usually in terms of the number of lines of code. The main drawback of these models is that there is a wide margin of uncertainty, as the actual size depends on the type of application and the software development method adopted. To address this drawback, the authors focus their work on the formal communication protocol development, and present a size model for estimating the size of an Estelle specification of a communication protocol from its informal specification.",
Quantifying the effects of communication optimizations,"Using a specially constructed machine independent communication optimizer that allows control over optimization selection, we quantify the performance benefit of three well known communication optimizations: redundant communication removal, communication combination, and communication pipelining. The numbers are shown relative to the base performance of benchmark programs using the standard communication optimization of message vectorization. The effects on the number of calls to communication routines, both static and dynamic, are tabulated. We consider a variety of communication primitives including those found in Intel's NX library, PVM and the T3D's SHMEM library. The results show substantial improvement, with two combinations of optimizations being most effective.",
Genetic VLSI circuit partitioning with dynamic embedding,"This paper suggests a new genetic algorithm (GA) for VLSI circuit partitioning problem. In a genetic algorithm, the encoding of a solution plays an important role. The key feature of the new genetic algorithm is a technique to provide dynamically many encodings in which encodings themselves undergo evolution. Before generating every new solution, we first generate a new encoding by combining two encodings chosen from a pool containing diverse encodings. The new solution is generated by a crossover which combines two parent solutions which are temporarily encoded by the generated encoding scheme. That is, a new solution is generated by a two-layered crossover. Depending on the new solution's quality and its improvement over the parents solutions, a fitness value is assigned to the underlying encoding. The encoding is discarded or enter the pool based on the fitness. Two populations are maintained for this purpose: one for solutions and the other for diverse encodings. On experiments with the public ACM/SIGDA benchmark circuits, the new genetic algorithm significantly outperformed recently published state-of-the-art approaches.",
Robot localization - theory and practice,"We consider the first stage of the robot localization problem described as follows: a robot is at an unknown position in an indoor-environment and has to do a complete relocalization, that is, it has to enumerate all positions that it might be located at. This problem occurs if, for example, the robot ""wakes up"" after a breakdown (e.g., a power failure or maintenance works) and the possibility exists that it has been moved. An idealized version of this problem, where the robot has a range sensor, a polygonal map, and a compass (all of which are exact, i.e. without any noise), was solved by Guibas-Motwani-Raghavan (1995). In the contest of their method, we first show that the preprocessing bounds can be expressed slightly sharper in the number of reflex vertices of the map. Then we describe an approach to modifying their scheme such that it can be applied to more realistic scenarios (e.g., with uncertain sensors) as well.",
Learning heuristics for OKFDD minimization by evolutionary algorithms,"Ordered Kronecker Functional Decision Diagrams (OKFDDs) are a data structure for efficient representation and manipulation of Boolean functions. OKFDDs are very sensitive to the chosen variable ordering and the decomposition type list, i.e. the size may vary from linear to exponential. In this paper we present an Evolutionary Algorithm (EA) that learns good heuristics for OKFDD minimization starting from a given set of basic operations. The difference to other previous approaches to OKFDD minimization is that the EA does not solve the problem directly. Rather, it develops strategies for solving the problem. To demonstrate the efficiency of our approach experimental results are given. The newly developed heuristics combine high quality results with reasonable time overhead.",
Markov gated experts for time series analysis: beyond regression,"Most traditional time series models are based on local methods (in time), which means assuming that the time series can be fully and locally (in time) characterized with a finite embedding space. There are many situations in which simple regression can not help find the temporal structural in time series. In this research, a Markovian architecture, Markov gated experts, has been developed based on nonlinearly gated experts. This paper discusses the statistical framework and compares the performance of Markov gated experts to gated experts on both computer generated time series and real world data. Compared with the original method, Markov gated experts are more powerful in finding the underlying temporal structure, and are therefore a more powerful analytical and forecasting model for nonstationary and structurally changing time series.",
A compile-time partitioning strategy for non-rectangular loop nests,"The paper presents a compile-time scheme for partitioning non-rectangular loop nests which consist of inner loops whose bounds depend on the index of the outermost, parallel loop. The minimisation of load imbalance, on the basis of symbolic cost estimates, is considered the main objective; however options which may increase other sources of overhead are avoided. Experimental results on a virtual shared memory computer are also presented.",
Stability analysis of neural networks,Neural networks can only be trained with a crisp and finite data set. Therefore stability analysis seems to be impossible. We propose a new method to show how stability for neural networks can be proven. We use fuzzy input and output data for the training process. After the learning phase the fuzzy network will be defuzzified. Using special properties of fuzzy neural networks the output behaviour can be estimated. This gives us the ability of proving stability for neural networks.,
Trendy technology or a learning tool?: Using electronic journaling on WebNotes/sup TM/ for curriculum integration in the freshman program in engineering at ASU,"Lately, technology has transformed our world, with millions of users negotiating everything from purchasing goods to accessing research. The pressure to embrace this technology has grown to the point that even in the composition classroom, instructors are exploring ways to most profitably use it. Given the growth and commercialism of the World Wide Web (WWW), it is not always easy to distinguish the hype from the useful. The authors argue that, however, one such worthwhile application is WebNotes/sup TM/ a commercial, WWW-based electronic forum software product that has become a powerful journaling tool for fostering connections, delivering information, and creating an online community in and out of the classroom.",
Reducing waiting costs in user-level communication,"Describes a mechanism for reducing the cost of waiting for messages in architectures that allow user-level communication libraries. We reduce waiting costs in two ways: by reducing the cost of servicing interrupts, and by carefully controlling when the system uses interrupts and when it uses polling. We have implemented our mechanism on the SHRIMP multicomputer and integrated it with our user-level sockets library. Experiments show that a hybrid spin-then-block strategy offers good performance in a wide variety of situations, and that speeding up the interrupt path significantly improves performance.",
Domain decomposition scheme for parallel molecular dynamics simulation,"Performances of domain decomposition schemes are discussed for 3-dimensional molecular dynamics simulation on parallel computers. Three types of domain decomposition schemes are applied for the molecular dynamics simulation: plane domain, square pillar domain, and cube domain. In order to decide the optimal simulation range of each domain, we give a theoretical performance analysis of parallel simulations by taking account of communication performances of MIMD parallel computers. The optimal simulation ranges for the three domain decomposition schemes are determined and are verified on a parallel computer CM-5.",
Criticality prediction models using SDL metrics set,"This paper focuses on the experiences gained from defining design metrics for SDL and comparing three prediction models for identifying the most fault-prone entities using the defined metrics. Three sets of design complexity metrics for SDL are defined according to two design phases and SDL entity types. Two neural net based prediction models and a model using the hybrid metrics are implemented and compared by a simulation. Though the backpropagation model shows the best prediction results, the selection method in hybrid complexity order is expected to have similar performance with some supports. Also two hybrid metric forms (weighted sum and weighted multiplication) are compared and it is shown that two metric forms can be used interchangeably for ordinal purpose.",
Concurrency control for perceivedly instantaneous transactions in valid-time databases,"Although temporal databases have received considerable attention as a topic for research, little work in the area has paid attention to the concurrency control mechanisms that might be employed in temporal databases. This paper describes how the notion of the current time-also called 'now'-in valid-time databases can cause standard serialisation theory to give what are at least unintuitive results, if not actually incorrect results. The paper then describes two modifications to standard serialisation theory which correct the behaviour to give what we term perceivably instantaneous transactions: transactions where serialising T/sub 1/ and T/sub 2/ as [T/sub 1/, T/sub 2/] always implies that the current time seen by T/sub 1/ is less than or equal to the current time seen by T/sub 2/.",
No feasible interpolation for TC/sup 0/-Frege proofs,"The interpolation method has been one of the main tools for proving lower bounds for propositional proof systems. Loosely speaking, if one can prove that a particular proof system has the feasible interpolation property, then a generic reduction can (usually) be applied to prove lower bounds for the proof system, sometimes assuming a (usually modest) complexity-theoretic assumption. In this paper, we show that this method cannot be used to obtain lower bounds for Frege systems, or even for TC/sup 0/-Frege systems. More specifically, we show that unless factoring is feasible, neither Frege nor TC/sup 0/-Frege has the feasible interpolation property. In order to carry out our argument, we show how to carry out proofs of many elementary axioms/theorems of arithmetic in polynomial-size TC/sup 0/-Frege. In particular, we show how to carry out the proof for the Chinese Remainder Theorem, which may be of independent interest. As a corollary, we obtain that TC/sup 0/-Frege as well as any proof system that polynomially simulates it, is not automatizable (under a hardness assumption).",
HyperSQL: web-based query interfaces for biological databases,"HyperSQL is an interoperability layer that enables database administrators to rapidly construct browser-based query interfaces to remote Sybase databases. Current browsers (i.e., Netscape, Mosaic, Internet Explorer) do not easily interoperate with databases without extensive CGI (Common Gateway Interface) programming. HyperSQL can be used to create forms and hypertext-based database interfaces for non-computer experts (e.g., scientists, business users). Such interfaces permit the user to query databases by filling out query forms selected from menus. No knowledge of SQL is required because the interface automatically composes SQL from user input. Database results are automatically formatted as graphics and hypertext, including clickable links which can issue additional queries for browsing through related data, bring up other Web pages, or access remote search engines. Query interfaces are constructed by inserting a small set of HyperSQL descriptors and HTML formatting into text files. No compilation is necessary because commands are interpreted and carried out by the special gateway, positioned between the remote databases and the Web browser. Feedback from developers who have used the initial release of HyperSQL has been encouraging. At present, query interfaces have been successfully implemented for three major NSF-sponsored biological databases: Microbial Germplasm Database, Mycological Types Collection, and Vascular Plants Types Collection.",
Performance-power optimization of memory components for complex embedded systems,"Optimizing performance and power during the design of embedded systems for real-time constrained applications is an important problem. This paper presents a network flow optimization technique to analyze power and performance tradeoffs for memory component design of an embedded system. The optimal number of external and internal memory accesses, memory sizes, and the number of extra computations (or data regeneration) for a number of tasks is determined. This is unlike previous research, which has only discussed ad hoc suggestions for this problem. The network flow approach can be solved to a globally optimal solution in polynomial time using very fast and efficient algorithms. Results for a large, complex, real industrial application-audio compression-show that this network flow technique provides up to 3.11 and 1.44 times improvement in power and performance respectively. This research is important for industry since power, performance and cost considerations at the early stages of design are crucial for mapping high-performance applications into cost-efficient and reliable systems.",
Legacy systems migration-a method and its tool-kit framework,"The problems posed by mission-critical legacy systems: brittleness, inflexibility, isolation, non-extensibility, lack of openness etc., are well known, but practical solutions have been slow to emerge. Most approaches are ""ad hoc"" and tailored to peculiarities of individual systems. This paper presents an approach to mission-critical legacy system migration: the Butterfly methodology, its data migration engine and supporting toolkit framework. Data migration is the primary focus of the Butterfly methodology, however, it is placed in the overall context of a complete legacy system migration. The fundamental premise of the Butterfly methodology is to question the need for parallel operation of the legacy and target systems during migration. Much of the complexity of the current migration methodologies is eliminated by removing this interoperation assumption.",
"A new, studio based, multimedia dynamic systems course: does it really work?","In the Fall of 1996, a new course in dynamic systems was offered in an interactive studio environment using multimedia computer software and a variety of other interactive activities. The course results were assessed and the students' progress and scores were compared with the regular, lecture-based sections of the course. The students for the studio section were selected at random and did not have any prior knowledge about the new format. The results show an improvement of about 10% in favor of the studio type instruction. Students were also surveyed at the end of the semester and indicated that they preferred the studio-based learning environment. Although the multimedia software plays an important role in the course, the authors postulate that this is only one of the decisive factors responsible for the improvement. All modes of interactions, human-to-human as well as human-to-computer, contributed to the new experience in this studio. Computer based multimedia is only a new tool in the arsenal of old teaching and learning techniques that the educators should be incorporating into their efforts to improve a global educational experience of the students.",
A novel mobile agent search algorithm,"Intelligent agents have been shown to be a good approach to addressing the issues of limited capacity and unreliable wireless links in mobile computing. However, before the approach can be commercially viable, a set of management capabilities that support the controls of intelligent agents in a mobile environment need to be in place. Since controls can only be applied after the target agent is located, an effective agent search algorithm is an indispensable part of the management functions. In this paper, we propose a new algorithm, the highest probability first algorithm, for locating the target agent. The approach makes use of the execution time information to reduce cost and network traffic. The execution time of the agent on a server is assumed to be binomial distributed and therefore is more realistic.",
"A comprehensive, integrated three-semester software engineering sequence","At Texas Tech, USA, eight software-related courses in the computer science curriculum have evolved into an integrated software engineering component of the undergraduate curriculum. The most tightly coupled of the courses in this sequence are three junior/senior level courses: ""software engineering""; ""senior project design""; and ""senior project implementation laboratory"". A common software engineering operations manual and set of software tools (sometimes developed previously within this course sequence) are used for projects within this three-course sequence, providing the ""feel"" of a small software development company. Various views aspects of software engineering are compared contrasted within this sequence. By integrating various software engineering principles within either the eight-course segment, and in particular the three junior/senior level courses listed above, the software component of the computer science curriculum has a cohesiveness and unity that it never had previously. Students see how the various aspects of software development relate to each other, and how they relate to the overall whole. This paper provides some details of the junior/senior level course content.",
On the dynamic initialization of parallel computers,"The incremental and dynamic construction of interconnection networks from smaller components often leaves the fundamental problem of assigning addresses to processors to be contended with at power-up time. The problem-called the initialization problem-is fundamental, for virtually all parallel algorithms assume that the processors know their global coordinates within the newly created entity. Rather surprisingly, the initialization problem has not received the attention it deserves. Our main contribution is to present parallel algorithms for the initialization problem on a number of network topologies, including complete binary trees, meshes of trees, pyramids, linear arrays, rings, meshes, tori, higher-dimensional meshes and tori, hypercubes, butterflies, linear arrays with a global bus, rings with a global bus and meshes with multiple broadcasting, under various assumptions about edge labels, leader existence and a priori knowledge of the number of nodes in the network.",
System knowledge acquisition in parallel discrete event simulation,"Optimistic protocols designed for parallel discrete event simulation (PDES) rely heavily on the global virtual time(GVT) calculation. Since the simulation uses large amounts of memory, the GVT is used to synchronize processes and discard obsolete system information. In this paper we present a new algorithm, the continuously monitored global virtual time (CMGVT). System information, such as the local virtual time (LVT) of each process and information about messages in transit, is appended to simulation messages. We describe and analyze three variants of our GVT algorithm: direct, indirect and transitive knowledge. The direct knowledge algorithm maintains only the local information about outstanding messages. The indirect version is augmented with the information about the knowledge of its direct neighbors. Finally, the transitive version is the most comprehensive. It keeps track of the outstanding messages sent by all processes in the system.",
Structure of the high-order Boltzmann machine from independence maps,"In this paper we consider the determination of the structure of the high-order Boltzmann machine (HOBM), a stochastic recurrent network for approximating probability distributions. We obtain the structure of the HOBM, the hypergraph of connections, from conditional independences of the probability distribution to model. We assume that an expert provides these conditional independences and from them we build independence maps, Markov and Bayesian networks, which represent conditional independences through undirected graphs and directed acyclic graphs respectively. From these independence maps we construct the HOBM hypergraph. The central aim of this paper is to obtain a minimal hypergraph. Given that different orderings of the variables provide in general different Bayesian networks, we define their intersection hypergraph. We prove that the intersection hypergraph of all the Bayesian networks (N!) of the distribution is contained by the hypergraph of the Markov network, it is more simple, and we give a procedure to determine a subset of the Bayesian networks that verifies this property. We also prove that the Markov network graph establishes a minimum connectivity for the hypergraphs from Bayesian networks.",
On perfect codes and tilings: problems and solutions,"Although nontrivial perfect binary codes exist only for lengths n=2/sup m/-1 and n=23, many problems concerning these codes remain unsolved. We present solutions to some of these problems. In particular, we show that the smallest nonempty intersection of two perfect codes of length 2/sup m/-1 consists of two codewords, for all m/spl ges/3. We prove that a perfect code of length 2/sup m-1/-1 is embedded in a perfect code C of length 2/sup m/-1, if and only if C is not of full rank. Using this result, we determine the generalized Hamming weight hierarchy of most perfect codes. We further explore the close ties between perfect codes and tilings: we prove that full-rank tilings of F/sub 2//sup n/ exist for all n/spl ges/14, and show that the existence of full-rank tilings for other n is closely related to the existence of full-rank perfect codes with large kernels.",
Linear higher-order pre-unification,"We develop an efficient representation and a pre-unification algorithm in the style of Huet (1975) for the linear /spl lambda/-calculus /spl lambda//sup /spl rarr//spl rArr/0&T/ which includes intuitionistic functions (/spl rarr/), linear functions (/spl rArr/), additive pairing (&), and additive unit (T). Applications lie in proof scorch, logic programming, and logical frameworks based on linear type theories. We also show that, surprisingly, a similar pre-unification algorithm does not exist for certain sublanguages.",
Reduced H/sub /spl infin// load frequency controller in a deregulated electric power system environment,"The new structure of electric power systems, which include separate generation, (GENCOs), transmission (TRANSCOs) and distribution (DISCOs) companies with an open access policy, will demand novel control and operation strategies to maintain the level reliability that consumers not only have taken for granted but expected. The paper addresses the design of a reduced H/sub /spl infin// load frequency controllers for interconnected large-scale electric power systems for a possible structure in the new deregulated open-access environment. In the structure proposed the DISCOs are to be responsible for tracking the load by securing as much transmission and generation capacity as needed. A method is given to reduce the size of the controller using the singular values of the observability matrix of the full order H/sub /spl infin// controller. An example is given to demonstrate the effectiveness of the proposed methodologies.",
Hamiltonian problems for reducible flowgraphs,"We discuss Hamiltonian problems for reducible flowgraphs. The main result is finding, in linear time, the unique Hamiltonian cycle, if it exists. In order to obtain this result, two other related problems are solved: finding the Hamiltonian path starting at the source vertex and finding the Hamiltonian cycle given the Hamiltonian path.",
Application of data-centered approach to Year 2000 problem,"The data-centered approach uses variable classifications, dependence analysis, generalised program slicing and ripple effect analysis. In the Year 2000 problem, this approach can be useful. Variable classification is used to identify all input and output variables, and once the initial set of variables that are Year 2000 related is identified, dependence analysis can be used to identify all the variables that are potentially affected by the initial set of variables. The second set of variables is then examined to see if they are Year 2000 related. This process is repeated until all the Year 2000 related variables are identified. This process is essentially ripple effect analysis and uses generalized program slicing and dependence analysis. Once Year 2000 related variables are identified, generalized program slicing is performed to identify all the statements that potentially need to be changed. Once a statement is changed, it may induce additional changes. Ripple effect analysis can be used to ensure that all the parts that need to modified are examined. Finally, the changed software should be validated and regression testing can be used in this stage. Ripple effect analysis can be used in this stage by identifying the relevant test cases that needed to evaluated. This is done by maintaining traceability links between the software and its test cases.",
Adjusting the instruction of the personal software process to improve student participation,"No customer is fully satisfied unless they receive a product that does what they want, and they receive it when they want it, defect-free and at an agreed upon price. To address all four of these requirements, Watts Humphrey, at the Software Engineering Institute (SEI), developed the personal software process (PSP), which applies proven quality principles to the work of individual software engineers. PSP was initially taught to practicing software engineers in industry and to graduate students. Earlier this year, Humphrey published a new text, suitable for the beginning software engineering student. This text outlines a process for managing and producing high quality software, reducing the mathematical and statistical rigor of the original PSP, but maintaining a solid base of disciplined practices and an overriding quality philosophy. This paper describes how the PSP has been incorporated into the undergraduate computer science courses at the University of Utah, USA. It highlights how instruction has been adapted to improve student participation, retention and utilization of the material.",
An on-line handwritten note recognition method using shape metamorphosis,"We propose a novel user-dependent method for the recognition of on-line handwritten notes. The method employs as a dissimilarity measure the ""degree of morphing"" between an input curve and a template curve. A physics-based approach substantiates the ""degree of morphing"" as a deformation energy and casts the problem as an energy minimization problem. The method operates upon key segmentation points that are provided by an appropriate segmentation algorithm. The segmentation objective is not to locate letters, but instead to locate corners and some key low curvature points (an easier task). This is part of the method's strategy to see the word as a generic on-line curve. Due to this strategy, the proposed method can handle collectively both cursive words and hand-drawn line figures, the two key ingredients of handwritten notes. Most importantly, the proposed system achieves high recognition rates without ever resorting to statistical models.",
Specifying the UQ* editor user-interface with Object-Z,"The specification of a user interface describes user-perceivable functions and information structures in an implementation-independent way. In this paper, we specify part of the user interface for the UQ* (University of Queensland) editor. The UQ* editor is part of an integrated programming environment currently under development at the University of Queensland. We use the Object-Z formal specification language. We demonstrate that Object-Z can be useful for abstractly specifying user interfaces.",
Correct-schema-guided synthesis of steadfast programs,"It can be argued that for (semi-)automated software development, program schemas are indispensable, since they capture not only structured program design principles but also domain knowledge, both of which are of crucial importance for hierarchical program synthesis. Most researchers represent schemas purely syntactically (as higher-order expressions). This means that the knowledge captured by a schema is not formalised. We take a semantic approach and show that a schema can be formalised as an open (first-order) logical theory that contains an open logic program. By using a special kind of correctness for open programs, called steadfastness, we can define and reason about the correctness of schemas. We also show how to use correct schemas to synthesise steadfast programs.",
Parallel programming in multi-paradigm clusters,"An important development in cluster computing is the availability of multiprocessor workstations. These are able to provide additional computational power to the cluster without increasing network overhead and allow multiparadigm parallelism, which we define to be the simultaneous application of both distributed and shared memory parallel processing techniques to a single problem. In this paper we compare execution times and speedup of parallel programs written in a pure message-passing paradigm with those that combine message passing and shared-memory primitives in the same application. We consider three basic applications that are common building blocks for many scientific and engineering problems: numerical integration, matrix multiplication and Jacobi iteration. Our results indicate that the added complexity of combining shared- and distributed-memory programming methods in the same program does not contribute sufficiently to performance to justify the added programming complexity.",
The design and performance analysis for the multimedia function unit of the NSC-98 CPU,"Leveraging Taiwan's hardware technologies, the National Science Council in Taiwan proposed a project to design an advanced CPU, called NSC-98, compatible to the Intel MMX architecture. This paper discusses the design of the multimedia extension, called MFU (multimedia function unit), in this CPU. The MFU has the following two special features: (1) support three more instructions for permutation operations, parallel distance operations, and parallel average operations; (2) implement two MFU subunits, one without the parallel multiplier and the other without the parallel shifter and converter (for permutation). This paper also does performance analysis by estimating the performances of the MFU with and without these features, on some chosen multimedia benchmarks. These performance analysis results can justify, our MFU design.",
A progressive Ziv-Lempel algorithm for image compression,"We describe an algorithm that gives a progression of compressed versions of a single image. Each stage of the progression is a lossy compression of the image, with the distortion decreasing in each stage, until the last image is losslessly compressed. Progressive encodings are useful in applications such as Web browsing and multicast, where the best rate/distortion tradeoff often is not known in advance. With progressive encoding, the system can respond dynamically: for example, a low-quality version of an image is sufficient when a user wishes to browse quickly, or when a slow link is encountered in a multicast. Our algorithm assumes an initial vector quantization step which maps important information of an image, such as intensity values, into higher-order bits. The bit planes are then sent successively using a progressive Ziv-Lempel (1978) algorithm. We propose data structuring techniques for selectively coding only those entries in a Ziv-Lempel dictionary that are feasible matches, based on shared knowledge of the data transmitted in earlier stages. Our technique, when applied to sample images on the Web, gives significant improvements over interlaced GIF in both image quality and compression rate. Our progressive LZ algorithm runs in amortized linear time.",
Cellular automata for generating deterministic test sequences,"We propose an on-chip test pattern generator that uses a one-dimensional cellular automaton (CA) to generate either a precomputed sequence of test patterns or pairs of test patterns for path delay faults. To our knowledge, this is the first approach that guarantees successful on-chip generation of a given test pattern sequence (or a given test set for path delay faults) using a finite number of CA cells. Given a pair of columns (C/sub u/,C/sub v/) of the test matrix, the proposed method uses alternative ""linking procedures"" P/sub j/ that compute the number of extra CA cells to enable the generation of (C/sub u/,C/sub v/) by the CA. A systematic approach uses the linking procedures to minimize the total number of needed CA cells. Experimental results show that the hardware overhead is often reasonable. The performance of the scheme depends on an appropriate choice of linking procedures P/sub j/.",
Fault tolerant locomotion for walking robots,"We introduce a general method of planning fault tolerant motion for a robotic task based on the least constraint (LC) framework, which uses a set of constraints on the robot's configuration over time. A performance measure called longevity is defined which, for a given configuration and type of fault, describes the potential for future progress towards the goal. This measure examines the connectivity of the configuration space given failure. An algorithm for computing longevity based on dynamic programming is described. Using the longevity computed at discrete points, a path is computed which is optimally fault tolerant. The set of paths which maximize longevity form a contingency plan for faults occurring at each point. Using LC we specify a gait for a four-legged walking robot. A prototypical step is produced using the longevity measure, which we compare it to a straight-line motion implementation. The optimal longevity paths are shown to be significantly more fault tolerant than the straight-line motion.",
Integrating risk management into an undergraduate software engineering course,Risk management is one of the key practices of the Software Engineering Institute capability maturity model. The effective management of risk is crucial to the success of software projects. Much has recently been written concerning risk management in an industrial environment. One of the most useful documents is a risk management questionnaire developed by the Software Engineering Institute. The questionnaire consists of 194 questions that a software development team can use to identify risks in their project. Unfortunately very little has been written about the risks faced by undergraduate software development teams and how they might manage them. This paper describes the introduction of risk management in an undergraduate software engineering course. The course requires students to work in teams of 5-6 persons to develop a software application in a one-semester time frame following a systematic development process. An academic version of the Software Engineering Institute risk management questionnaire suitable for undergraduate teams is described. This questionnaire addresses the real risks that an undergraduate software development team is likely to face and is based on years of the authors' experience and that of others teaching these types of classes. The questionnaire and related risk forms and materials are described in detail as well as the authors' experience in using these materials with two classes.,
The Internet and the World Wide Web,"This tutorial shows you how to unlock the hidden resources of the Internet and the World Wide Web. After a description of its history, we demystify the Internet's terminology and procedures by exploring electronic mail, addressing, mail lists, news groups, and the World Wide Web. We illustrate Web use in two domains, personal and professional. In the personal domain we show a number of useful, time-saving resources for: travel and leisure, investments and finance; personal shopping; medicine; real estate; museums; weather; games; and finding, retrieving, and managing information on any topic whatsoever. In the professional domain we show how to access Web resources in such areas as elementary, secondary, and university-level education; computers; legal advice; libraries; news and current events; science and engineering; telecommunications, teleconferencing, telemedicine, and telecommuting. We close with a review of the ""hot topics"" of 1997: privacy and security, electronic payment systems, and low-cost and high speed Internet access.",
Representing and rendering sweep objects using volume models,"The authors describe a method for generating arbitrary sweep objects, where the object being swept may be a 2D contour or a 3D object. The path taken by the object as it is swept can include arbitrary affine transformations, for example stretching and rotating. Thus one can produce effects such as tapering and twisting with ease. The method described composites the object being swept into a volume model at a number of points along its defined path. The points at which the object is composited are determined adaptively.",
Behaviour combination through analogy,"Although people can readily use and generate analogies in everyday discourse to relate disparate domains, even most of today's end user programming languages provide no support for creating analogies. Finding ways to represent analogies that allow users to express relations between code fragments via analogy is challenging. Within Agentsheets with VisualAgentalk (AS-VAT) a ""programming by analogy"" (PBA) mechanism has been developed that allows end users to reuse code between acting agents by creating analogies between them. This mechanism harnesses the intuitive power of analogy to generate and reuse code in a way that hides complicated inheritance issues from end user programmers.",
Nonlinear shape normalization methods for gray-scale handwritten character recognition,"Proposes nonlinear shape normalization methods for gray-scale handwritten characters in order to minimize the loss of information caused by binarization and to compensate for the shape distortions of characters. A 2D linear interpolation technique has been extended to nonlinear space and the extended interpolation technique has been adopted in the proposed methods to enhance the quality of the normalized images. In order to verify the efficiency of the proposed methods, the recognition rate, the processing time and the computational complexity of the proposed algorithms have been considered. The experimental results indicate that the proposed methods are efficient not only for compensating for the shape distortions of handwritten characters but also for maintaining the information of gray-scale input characters.",
Frameworks in Catalysis: pictorial notation and formal semantics,"In OO Design, it is widely recognised that the distribution of tasks between objects and the contracts between them are key to effective design. In composing designs from reusable parts, the parts are therefore frameworks, namely descriptions of the interactive relationships between objects which participate in the interactions. Designs are then built by composing these frameworks, and any object in the final design will play (various) roles from several frameworks. Practitioners of OO Design use pictorial notations for design. However, in order to reason formally about design, we need a sound (formal) semantics for the diagrams. In this paper, we show that frameworks can be formalised as many-sorted theories, and then present a pictorial representation of such theories, developed in the Catalysis project.",
Formal description of auditory scenes,"This paper introduces the concept called auditory scenes, which is a tool for high semantic description of everyday sounds, and two grammar approaches based on this concept. It does not consider auditory scene analysis, which describes the ability of listeners to separate the acoustic events arriving from different environmental sources into separate perceptual representations (streams). The concept of auditory scenes relies on our everyday perception of sounds in daily life. It assumes various perceptual attributes such as duration, volume, position in space, and others, for each individual sound in the scene as well as the temporal, spatial and other relationships between them. Different temporal relationships and some theoretical considerations regarding these issues are presented in depth. The two grammars represent (1) the hierarchical and (2) the autonomous concept. The first grammar approach is similar to music composition and is basically a temporal composition of everyday sounds. By contrast, the second grammar approach defines the event driven non-hierarchical composition of everyday sounds. Before these grammars are discussed, an introduction to the use of sounds in man-machine interaction and the concept of auditory scenes are presented.",
Approximate strategies for learning trajectories of autonomous learning agents,"Describes a new approach to the approximate modeling of the learning dynamics of autonomous learning agents for performance improvement in supervised learning. The extracted approximate model can be used to generate target trajectories from the current performance state to the final performance goal in order to ""lead"" the learning agent through the dynamic range of the learning process. The interaction between the supervisor module and the agent can be modeled as an incentive game. Ideas introduced for the single-agent case can further be extended to include multi-agents to address the coordination problem in modular learning structures.",
Performance of standard and modified network protocols in a real-time application,"Recent advances in computer and communications technologies have made possible the interconnection of large number of real-time training simulators via local area networks. The self-healing nature of real-time networked simulation has been found to allow for a modification based on discarding old packets whenever new state updates become available. The performance benefits obtained by implementing this modification at both the application and data-link layers are presented for two medium-access network protocols: ETHERNET and GBRAM. An analysis of a phenomenon, called the greedy node problem, in distributed simulation networks is presented.",
Integration of application systems by modelling information shared among applications,"The integration of information systems has been attempted at different scales of operation: within one organisation, across multiple organisations, for an infrastructure using standards such as CORBA and DCE. It is not easy to assemble application systems using off-the-shelf CORBA objects under the prevailing infrastructure. The main problem of integration still remains that real-world objects can be modelled in many different ways. We propose to solve this multifaceted problem, by concentrating only on the information shared across information systems, while the information that concerns an individual information system only, is not considered.",
Exploiting deep parallel memory hierarchies for ray casting volume rendering,"Previous work in single-processor ray casting methods for volume rendering has concentrated on algorithmic optimizations to reduce computational work. Previous work in parallel volume rendering has concentrated on partitioning, with the goals of maximizing load balance and minimizing communication between distributed nodes. Building on our previous work at lower levels of the hierarchy, we present techniques to efficiently exploit all levels of the deep memory hierarchy of a distributed Power Challenge Array, on which we implement a logical global address space for volume blocks with caching. This focus on the optimal exploitation of the entire memory hierarchy, from the processor cache to the interconnection network between distributed nodes, allows us to efficiently render a 7.1 GByte dataset. Our results have implications for the parallel solution of other problems which, like ray casting, require a global gather operation and contain coherence. We discuss implications for the design of a parallel architecture suited to solving this class of problems.",
Security problems for statistical databases with general cell suppressions,"Studies statistical database problems for 2D tables whose regular cells, row sums, column sums and table sums may be suppressed. Using graph-theoretical techniques, we give optimal or efficient algorithms for the query system problem, the adversary problem and the minimum complementary suppression problem. These three problems are considered for a variety of data security requirements such as those of protecting linear invariants, analytic invariants, k rows (or columns) as a whole, and a table as a whole.",
Distributed learning via the World Wide Web through interactive modules,"This year, new distributed learning tools were developed and integrated into an electrical and computer engineering course at the University of Illinois. This paper discusses the use of the World Wide Web (WWW) for the teaching of ECE291 and describes the development and implementation of new interactive software modules that automatically grade on-line homeworks, record student scores, graphically display point distributions, and manage group projects.",
Automatic high-quality reengineering of database programs by temporal abstraction,"The relational database model is currently the target of choice for the conversion of legacy software that uses older models (such as indexed-sequential, hierarchical or network models). The relational model makes up for its lower efficiency by a greater expressive power and by optimization of queries, using indexes and other means. However, sophisticated analysis is required in order to take advantage of these features, since converting each database access operation separately does not use the greater expressive power of the target database and does not enable it to perform useful optimizations. By analyzing the behavior of the host program around the database access operations, it is possible to discover patterns such as filtering, joins and aggregative operations. It is then possible to remove those operations from the host program and re-implement them in the target database query language. This paper describes an automatic system, called MIDAS (MIgrator of Database Application Systems), that performs high-quality reengineering of legacy database programs in this way. The results of MIDAS were found to be superior to those of the naive one-to-one translation in terms of readability, size, speed and network data traffic.",
Toward formal TTCN-based test execution,"The formal test execution method is an important research field in formal protocol conformance testing. We propose a formal test execution approach that is based on the test notation of the TTCN's (tree and tabular combined notation) operational semantics, and describe its execution process by using the input-output transition system (IOTS). We also present a practical design of this formal approach. This formal TTCN-based test execution method is very suitable for the construction of a general protocol test system, and also as a means of automatic test suite verification.",
Creating a virtual network laboratory,"Networking courses are traditionally conceptual in nature, with little opportunity for students to apply what they have learned in order to strengthen their knowledge. To support these courses, the University of Minnesota established a hands-on network laboratory. Educational and operational concerns have surfaced throughout the years due to the physical laboratory constraints and difficulty of satisfying the high demand. To address these issues, and to overcome the shortcomings of current approaches, we are developing a virtual network laboratory environment to eliminate most of the constraints, though it is not necessarily replacing all of the physical settings at once. The kernel consists of a collection of online multimedia learning modules which integrate the key laboratory devices with instructional material. The modules are designed to allow students to carry out hands-on networking in a virtual environment or through remote access. This virtual laboratory is easily accessible over the Internet.",
Prototyping with attribute grammars and Prolog,This paper presents a rapid prototyping technique for procedural specifications of embedded distributed systems. The technique employs attribute grammars for the description of language constructs and definite clause grammars within the framework of Prolog for low-cost and straightforward translator implementation. The current version of the executable specification tool is aimed at the microcontroller application domain.,
A new interconnection network for parallel computer with low diameter,"In this paper, we propose and analyze the new interconnection network for parallel computer, called graycube. The graycube has the same number of nodes and edges as hypercube, but it's diameter is about one half of the equivalent hypercube. It has simple recursive structure, routing and broadcasting algorithms. Since hypercube can be embedded into graycube with dialation 2, algorithms developed based on hypercube are easily simulated in graycube. The basic properties, routing and broadcasting algorithms, and hypercube embedding are presented.",
MUpstart-a constructive neural network learning algorithm for multi-category pattern classification,"Constructive learning algorithms offer an approach for dynamically constructing near-minimal neural network architectures for pattern classification tasks. Several such algorithms proposed in the literature are shown to converge to zero classification errors on finite non-contradictory datasets. However, these algorithms are restricted to two-category pattern classification and (in most cases) they require the input patterns to have binary (or bipolar) valued attributes only. We present a provably correct extension of the upstart algorithm to handle multiple output classes and real-valued pattern attributes. Results of experiments with several artificial and real-world datasets demonstrate the feasibility of this approach in practical pattern classification tasks, and also suggest several interesting directions for future research.",
Coherent parallel programming in C/spl par/,"This paper presents the coherent parallel programming concept using a new parallel language called C/spl par/ (pronounced C Parallel). The C/spl par/ language is based on the standard C language with a small set of extended constructs for parallelism and process interaction. At the core of C/spl par/ is a structured construct called coherent region, which facilitates the development of coherent programs, i.e., parallel programs that are structured, determinate, terminative, and compositional. We present the basic features of C/spl par/ and show that coherent region is a versatile construct.",
TPAL: a timed-probabilistic model for concurrent processes,"We present an algebraic model for the description of concurrent systems with capabilities to express timed and probabilistic behaviours, as well as urgent interactions. This model is based on basic LOTOS, including a probabilistic choice operator, a timed prefix operator for the specification of time intervals where the involved actions are enabled, and a prefix operator for the specification of urgent interactions. An operational semantics for this model is also presented, which is based on the execution of bags. This semantics establishes that processes must always execute as many urgent actions as they can, thus complying in a great extent with the user specifications of urgent actions.",
"The ""Hardest"" natural decidable theory",We prove that any decision procedure for a modest fragment of L. Henkin's theory of pure propositional types requires time exceeding a tower of 2's of height exponential in the length of input. Until now the highest known lower bounds for natural decidable theories were at most linearly high towers of 2's and since mid-seventies it was an open problem whether natural decidable theories requiring more than that exist. We give the affirmative answer. As an application of this today's strongest lower bound we improve known and settle new lower bounds for several problems in the simply typed lambda calculus.,
A note on a high resolutional communication system using fluency theory,"When we want to encode a hand-written document and store the encoded data in a computer, and transmit it as binary image data via the Internet or public communication networks, the image data is required to be compressed into a small capacity in the network environment with limited network bandwidth. There are two important aspects concerning the encoding/decoding of binary image data. One is how to keep the fine quality of the original image data, and the other is how to reduce the capacity of data. The paper proposes a new communication system by which the original binary image can keep its fine quality even when it is enlarged or reduced, at the same time, and reconstructed from the compressed data at a high speed.",
Modelling of single mode distributions of colour data using directional statistics,"Three different statistical models of colour data for use in segmentation or tracking algorithms are proposed. The results of a performance comparison of a tracking algorithm, applied to two separate applications, using each of the three different types of underlying model of the data are presented. From these a comparison of the performance of the statistical colour models themselves is obtained.",
Disk caching in large database and timeshared systems,"We present the results of a variety of trace-driven simulations of disk cache designs using traces from a variety of mainframe timesharing and database systems in production use. We compute miss ratios, run lengths, traffic ratios, cache residency times, degree of memory pollution and other statistics for a variety of designs, varying lock size, prefetching algorithm and write algorithm. We find that for this workload, sequential prefetching produces a significant (about 20%) but still limited improvement in the miss ratio, even using a powerful technique for detecting sequentiality. Copy-back writing decreased write traffic relative to write-through by more than 50%; periodic flushing of the dirty blocks increased write traffic only slightly compared to pure write-back, and then only for large cache sizes. Write-allocate had little effect compared to no-write-allocate. Block sizes of over a track don't appear to be useful. Limiting cache occupancy by a single process or transaction appears to have little effect. This study is unique in the variety and quality of the data used in the studies.",
The WARP: wideband advanced recorder processor for the New Millennium Program EO-1,"NASA's New Millennium Program (NMP) will sponsor a series of small scientific spacecraft whose purpose is to demonstrate new technologies and methods of spacecraft design by using unique teaming arrangements between government and industry. EO-1, the first Earth Orbiter satellite in the NMP, will perform the Advanced Land Imager (ALI) mission. The ALI, combined with other instruments aboard EO-1, will output nearly 1 Gigabit per second of raw science data. An innovative data system, the Wideband Advanced Recorder Processor (WARP), is being developed to handle these very high data ingest rates. In keeping with the philosophy of the EO-1 mission, the WARP will use ""best commercial practices"" in its design guidelines. The WARP will feature extensive use of new technology including the Spaceborne Fiber Optic Data Bus (SFODB), Chip-on-Board surface mounted memory components, and stacked multi-die wafer segments of Dynamic RAM. The WARP implements a scalable architecture that can be applied to future missions requiring a Solid State Recorder (SSR) and a high performance science data processor. The WARP represents a significant advance over previous spaceflight SSRs. This paper describes the WARP and its capabilities.",
A smooth dynamical system that counts in binary,"This paper presents a smooth dynamical system that implements a toggle flip-flop. The flip-flop is described as a system of smooth, non-linear ODEs. We identify a period-2, invariant set of this system, and show that this corresponds to the discrete state transitions of a discrete model. We show that this behaviour is robust for a large class of inputs and that these toggle elements can be composed to implement a binary counter of any number of bits.",
Data visualization in the DB-Discover system,"The DB-Discover system is a research software tool for knowledge discovery from databases. Utilizing an efficient attribute oriented induction algorithm based upon the climbing tree and dropping condition methods, called multi attribute generalization, it generates many possible summaries of data from a database. We present the design of the data visualization capabilities of DB-Discover and describe how these capabilities help us manage the summaries generated.",
A tool-suite for reachability analysis of concurrent object-oriented programs,"The object-oriented paradigm provides support for modular and reusable design and is attractive for the construction of large and complex concurrent systems. Reachability analysis is an important and well-known tool for static (pre-run-time) analysis of concurrent programs. However its direct application to concurrent object-oriented programs has many problems, such as incomplete analysis for reusable classes and increased computational complexity. It also seems impossible to arrive at a single general-purpose strategy that is both safe and effective for all programs. The authors propose a tool-suite based approach for the reachability analysis of concurrent object-oriented-programs. This approach enables choice of an appropriate 'ideal' tool, for the given program and also provides the flexibility for incorporation of additional tools. They have also proposed a novel abstraction-based partitioning methodology for effective reachability analysis of concurrent object-oriented programs. Using this methodology, they have developed a variety of tools, having different degrees of safety, effectiveness and efficiency, for incorporation into the tool-suite. They have formally shown the safety of these tools for appropriate classes of programs and have evaluated their effectiveness and efficiency.",
Embodied computer vision for mobile robots,"The basic idea that the perception of actual embodied beings, be they animal or robotic, is fundamentally related to their embodiment is generally referred to as purposive or animate vision. Research in this field generally emphasises low-level vision techniques. This paper outlines a philosophical basis for embodied perception, and develops a framework for conceptual embodiment of vision-guided robots. The aim is to facilitate the use of high-level vision through an active perception framework. We argue that the classical computer vision paradigm has problems in high-level vision due to an implicit assumption that objects in the world can be objectively subdivided into categories. Further, that through conceptual embodiment, active perception offers a way forward. We present a mobile robot navigation system based on the principles of conceptual embodiment. The system uses object recognition to guide a robot around known objects. The robot's object model is embodied, and this embodiment yields specific advantages for the robot.",
Efficient calculation of data dependences in programs with pointers and structures,"We describe a method to derive safe approximations for data dependences in programs with pointers and structures. In our approach, alias information and reaching definitions' information at each program point is simultaneously covered by a single representation, called A/D graphs. We perform a single-pass data dependence analysis for a class of restricted imperative languages by solving a monotone data flow system, based on A/D graphs. The advantages of our method are improved accuracy, economical storage use, and reduced analysis time.",
Investigating approximate reasoning and fuzzy control by concepts of functional analysis,"We investigate the input-output behaviour of the inference machine of a fuzzy controller, which is interpreted as a functional operator operating on the set of all fuzzy sets defined on a given universe. The concepts of fixed point, eigen function, eigen value and linearity are suitably translated into the fuzzy approach presented. Classical fixed point theorems (Birkhoff, Zorn, Banach) are applied in order to investigate the behaviour of inference machines. The concept of linearity is studied for Mamdani interpretations.",
Managing multi-expertise in design of effective cooperative knowledge-based system,"Most branches of computer science research are already involved in working rewards collaborative problem solving. In AI, the two fields of knowledge acquisition and distributed AI play a major role. The construction of collaborative knowledge-based systems (cooperative KBSs) needs an important stage of knowledge acquisition from the experts involved in the group activity. On the other hand, distributed AI and multi-agent systems are merely concerned with cooperative KBSs since they investigate some related problem and enable distributed modeling. However, not much work has been done on modeling multi-expertise. Moreover, there is a low level of exchange between these two fields. In this paper, we propose an agent-based method for multiple-expert knowledge engineering. This method is based on a set of generic models, which serves as a template to the knowledge engineer. Experts are described as a society of interacting cognitive agents.",
Cell suppression to limit content-based disclosure,"The increasing demand for information, coupled with the increasing capability of computer systems, has compelled information providers to reassess their procedures for preventing disclosure of confidential information. General logical and numerical methods exist to determine, prior to release, if disclosure can occur-either directly or through inference. One method uses linear programming techniques applied to multi-dimensional tables of count data to determine which cells are subject to inferential disclosure. This paper develops integer programming (IP) techniques to find an optimal primary suppression set for protecting the confidentiality of sensitive data in 3D tables. An example is drawn from Federal Reserve Bank records. Data tables are randomly generated to assess the extent of inferential disclosure and the computational time/space restrictions of the IP model.",
Eigenvectors-based parallelisation of nested loops with affine dependences,"This paper is concerned with parallelising a special class of nested loops with affine dependences. The data dependences of the program are captured in a so-called dependence matrix. Based on the eigenvalues and eigenvectors of this matrix, the proposed approach can generate a greater degree of DOALL parallelism than traditional unimodular transformations.",
Modal event calculi with preconditions,"Kowalski and Sergot's (1986) event calculus (EC) is a simple temporal formalism that, given a set of event occurrences, allows the derivation of the maximal validity intervals (MVIs) over which properties initiated or terminated by those events hold. The limited expressive power of EC is notably augmented by permitting events to initiate or terminate a property only if a given set of preconditions hold at their occurrence time. We define a semantic formalization of the event calculus with preconditions. We gain further expressiveness by considering modal variants of this formalism, and show how to adapt our semantic characterization to encompass the additional operators. We discuss the complexity of MVI validation and describe examples showing that modal event calculi with preconditions can be successfully exploited to deal with real-world applications.",
Constant-space string-matching in sublinear average time,"Given two strings: pattern P of length m and text T of length n. The string-matching problem is to find all occurrences of the pattern P in the text T. We present a simple string-matching algorithm which works in average o(n) time with constant additional space for one-dimensional texts and two-dimensional arrays. This is the first attempt to the small-space string-matching problem in which sublinear time algorithms are delivered. More precisely we show that all occurrences of one- or two-dimensional patterns can be found in O(n/r) average time with constant memory, where r is the repetition size (size of the longest repeated subword) of P.",
A simple approach to 3D object metamorphosis,This paper presents a technique for smoothly blending some special categories of three-dimensional polygonal objects. Polygon blending is usually considered a two-part process: generating vertex correspondences and interpolating between corresponding vertices to create the intermediate polygons. This paper emphasises on the problem of automatic vertex correspondence determination. The proposed algorithm is based on a simple heuristic method for the metamorphosis of closed planar contours.,
Using constraint logic programming in memory synthesis for general purpose computers,"Summary form only. In modern computer systems the performance is dominated by the memory performance. Currently, there is neither a systematic design methodology nor a tool for the design of memory systems for general purpose computers. We present a first approach to CAD support for this crucial subtask of system level design. Dependencies between influencing factors and design decisions are explicitly represented by constraints, and constraint logic programming is used to make the design decisions. The memory design is optimized with respect to several objectives by iterating the (re)design cycle. Event driven simulation is used for evaluation of the intermediate results. The system is organized as an interactive design assistant.",
Grendel: a Web browser with end user extensibility,"Electronic documents, particularly those on the World Wide Web, have an inherent structure which can be utilized. However; the tools to do so have typically been oriented towards professional programmers. We present scripting language features that can be incorporated into tools that manipulate structured network documents. This set of language features allows us to build visual tools to specify transformation on such documents. Subsequently transformation scripting, is opened up to a broad class of users. This allows the tools to be easily extended by end users. World Wide Web browsers serve as a class of tools that can take advantage of this technique. We discuss our experimental browser, Grendel, which has an embedded scripting language, CrossJam, based upon transformation scripting. Grendel has a number of novel applications and a simple visual tool, Spar, to assist in scripting the browser's behavior.",
Complexity of power default reasoning,"This paper derives a new and surprisingly low complexity result for inference in a new form of Reiter's propositional default logic (1980). The problem studied here is the default inference problem whose fundamental importance was pointed out by Kraus, Lehmann, and Magidor (1980). We prove that ""normal"" default inference, in propositional logic, is a problem complete for co-NP(3), the third level of the Boolean hierarchy. Our result (by changing the underlying semantics) contrasts favorably with a similar result of Gottlob (1992), who proves that standard default inference is II/sub 2//sup P/-complete. Our inference relation also obeys all of the laws for preferential consequence relations set forth by Kraus, Lehmann, and Magidor (1990). In particular we get the property of being able to reason by cases and the law of cautious monotony. Both of these laws fail for standard propositional default logic. The key technique for our results is the use of Scott's domain theory to integrate defaults into partial model theory of the logic, instead of keeping defaults as quasiproof rules in the syntax. In particular, reasoning disjunctively entails using the Smyth powerdomain.",
Weight distribution of Justesen codes,In this note we show that a bound on the minimum distance of Justesen codes can be improved significantly in some cases.,
Characterizing images based on lines for image indexing,"Recent advances in multimedia has necessitated us to look for an effective search method of images. Most of the contemporary indexing methods are based on color distribution in the images. When the users do not remember the colors clearly, however, it has been difficult to retrieve the desired images. The paper proposes an indexing method based on shapes contained in an image. We characterize an image by the half planes that contain the objects in it. We also propose a method to characterize images by the vanishing points of parallel lines contained in an image. The vanishing points represent how three dimensional (3D) objects are projected onto the 2D image.",
A real-time system for detecting moving point target in low SNR image sequences,"Concerned with the huge amount of information and complicated combinations of trajectories, the real-time automatic detection of moving point targets in low SNR image sequences belongs to the difficult area of multidimensional image processing. The design and implementation of a real-time system for detecting point targets is presented. The algorithm is under the framework of sequential hypothesis testing based on stage integration, and a new method of stage integration is put forward, which efficiently combines summation projection with maximum value projection. The algorithm has clear hierarchies and only uses some simple operations in the whole procedure. So a real-time approach has been easily implemented on a simple pipeline-based IPM (information processing machine).",
Blind deconvolution by self-organization,"In this paper, we devise a self-organizing network to solve both the unknown system and unknown input in blind deconvolution of blurred images. We utilize a criterion function which has a similar form as the Kullback-Leibler cross information formula to adapt the network's weights to approach the unknown system function. This adaptation gradually reduces the criterion value which is a distance measure between the system output and the output of the adapted system with a reconstructed input signal. The weight matrices of the neurons in the network are shifted versions of the system function and will be aligned in the network according to their shifts during convergence. This is because the convolution operation which copes with this network scheme and the hidden topology of the shifted system functions can be aligned similarly in a 2D plane.",
Position paper on research infrastructure for reengineering,"The paper proposes a joint effort to build a research infrastructure for the area of reengineering. The main idea is to better share and build upon each other's results. The article presents some steps that the software community can take to address these issues and to leverage their efforts. Some of these are already underway, some are understood but need to be implemented, and some are themselves research questions that need to be examined. All of them need contributions from volunteer participants. These steps include increased interaction with industry, the development of a repository of research artifacts, and convergence of intermediate representations. The paper is a proposal that requires considerable discussion and consensus before it can be realized.",
Contention based consistency maintenance for client cache,"In client-server database systems, clients can maintain their own cache to reduce the overhead of accessing data objects at the server database. In this case, there should be a protocol which ensures the consistency of cached data. In this paper, we propose a cache consistency algorithm, contention based algorithm (CBA), which dynamically applies the appropriate protocol for each data object. When the contention of data objects is uniformly low or uniformly high, the proposed algorithm shows the same performance as existing algorithms, and when the contention is not uniform, the proposed algorithm shows better performance than existing algorithms.",
Building a research infrastructure for program comprehension observations,"Detailed program comprehension studies are expensive. Today, most of those studies are difficult to aggregate, as analysis methods for the data differ widely between researchers. As part of an effort to build a research infrastructure, a uniform, systematic analysis method for analyzing think-aloud protocols is proposed. This method is shown to be compatible with other analysis methods and extensible. It provides the possibility of aggregating results from experiments and leveraging results through such aggregation.",
Recognition of texts based on situations,"To understand texts, people must relate them with some specific situations. The paper, on the basis of the idea, deals with a model of understanding texts based on situations and presents a method of recognizing texts.",
Notes on Computational-Space-Based Ray-Casting for Curvilinear Volumes,"In this paper we study the computational-space-based (C-space-based) ray-casting algorithm for rendering curvilinear volumes. With a simple counter example, we demonstrate that a proposed C-space-based method may not generate rendering results as accurately as ray-casting in the physical space. We also analyze what needs to be improved and discuss several other issues related to the C-space-based approach.",
Experiences with teaching object-oriented concepts to introductory programming students using C++,"With the growth in popularity of the object-oriented paradigm, the Department of Software Development at Monash University decided to teach object-oriented programming to the introductory programming students. The first year programming subjects were completely restructured and rewritten. It was decided to introduce the object-oriented paradigm after the students have had one semester of procedural programming experience in C++. Teaching object oriented principles in C++ has presented many challenges. The students find the complicated language syntax difficult. They also show a reluctance to move from the procedural to the object-oriented paradigm. However, with support and advice from education experts we have successfully addressed these problems. Different teaching approaches and techniques have been used. Small discussion classes were introduced to provide a forum for the consolidation of ideas presented in lectures, away from the distraction of computers. Extra support was given to the students in the form of a special World Wide Web page and a help desk. A concurrent research project has enabled close monitoring of the effectiveness of our teaching programme. In spite of the difficulties we have faced, the student results for 1995 and 1996 show an improvement in student performance over previous years.",
Software configuration management tools,"Software Configuration management is concerned with the identification, organizing, and controlling the configuration of and changes to a system under parallel development environment. It should encompass all system components. However, most existing software configuration management tools have emphasized too much on version control of source files and have neglected some of the other vital functionality. In this paper, recommendations are made to the desirable features that should be incorporated in future tools to make them truly effective in the software development process.",
Global software process improvement initiatives,"Summary form only given. Dozens of software process improvement network (SPIN) chapters are currently active around the globe. SPIN is an effort sponsored by the US Department of Defense (DoD) to promote worldwide software process improvement. The DoD is one of the world's largest customers of computer software, with trillion dollar plans for purchases in 1997. Moreover, software is an integral part of the defense systems developed by the DoD, so software quality is a major concern. In order to gauge the impact that such efforts may have on worldwide software process improvement, a survey was sent to national and international SPIN chapters. The survey contained questions on SPIN objectives, membership, communication, influence and future challenges. The results suggested that the main challenge of SPIN groups worldwide is to obtain top management support for software process improvement programs. The results also showed that the SPIN chapters are performing a valuable service in furthering the worldwide software process improvement effort.","Software quality,
Computer science,
Information systems,
Collaborative software,
Software performance,
Educational programs,
Software tools,
Government,
Computer industry,
Investments"
Orientation adapted subband image compression,"A subband coding system which dynamically adapts to the local orientation of image features is investigated. In particular, the re-orientation scheme proposed by Taubman and Zakhor (1994) is used in conjunction with a separable octave-band subband decomposition. This is then followed by efficient coding techniques such as embedded zero trees, trellis coded quantization, and stack-run coding. The coded images demonstrate the superior subjective performance of the proposed methods.",
From queries to answers in visual logic programming,"Previously (J. Puigsegur et al., 1996), we presented a visual declarative programming language based on two main graphical constructs: directed acyclic graphs representing predicate application and graphical set inclusion representing logical implication. We showed that with these simple visual constructs we can cover most of the representational demands of computational logic allowing a blend of functional and relational styles of programming. We explore the advantages of directly using our visual syntax for solving queries, by presenting a method for visually asking questions about a visual program by means of query diagrams, and by defining visual inferences which operate on those diagrams. The result is an operational semantics for declarative programming which is intended to be visual, intuitive and formal. Visual, because the inference rules display graphically the transformation of query diagrams into answer diagrams. Intuitive, because it is intimately linked with the visual syntax of the declarative language. Finally, it is formal because the usual properties of formal logic (i.e. correctness and completeness) can be applied directly to it.",
Synthesis for logical initializability of synchronous finite state machines,"We present a new method for the synthesis for logical initializability of synchronous state machines. The goal is to produce a gate-level implementation that is initializable when simulated by a 3-valued (0,1,X) simulator. We build on the approach of Cheng and Agrawal (1989,92) who constrain state assignment to translate functional initializability into logic initializability. We propose an alternative method which is guaranteed safe and not as conservative. In addition, we propose necessary and sufficient conditions on 2-level and multi-level logic synthesis to insure 3-valued simulation succeeds.",
Image compression using variable blocksize vector quantization based on rate-distortion decomposition,"In this paper, we propose an optimal quadtree segmentation of an image for variable blocksize vector quantization (VBVQ) such that the total distortion of the reconstructed image is minimal and the total required bits don't exceed the bit budget. The above constraint problem is converted into an equivalent unconstrained problem by using Lagrange multiplier. We prune the full quadtree by comparing the Lagrangian costs of the parent and four child nodes. If the adjacent subblocks merge into a larger block reducing the Lagrangian cost, these subblocks will be merged. Otherwise, these subblocks will be vector quantized. From our simulation results, we see that the reconstructed image of our proposed algorithm has 1-3 db higher PSNR than the fixed blocksize VQ and conventional VBVQ algorithms.",
From concept to image with computer art graphics,"Discusses how to train computer graphics-oriented students to be able to use the computer to its full artistic potential, with application both of programming and art-related issues. Students create computer art graphics inspired by scientific concepts, to match the curricular content of other courses they are taking concurrently. The integrative visual training involves both the process and the product of the students' work. Due to the integrative instruction of art and science issues, students might learn about specific subject areas by applying visual thinking and inventing subject-related activities which involve visualizing information. Two main methods of integration were applied: (1) integrative project assignments, aimed at provoking the students to apply visual thinking and learning, and (2) art assignments, inspiring the students to create artwork inspired by scientific concepts.",
Static scheduling of MPI tasks,"The paper addresses the problem of scheduling MPI tasks on the processing nodes of an MPI environment. The authors introduce a static scheduling system, in which program requirements are gathered, dependencies among the tasks are figured out, the computing environment is characterized, and a task assignment is generated. They introduce a tool, called the sniffer which utilizes MPI to benchmark an existing computing environment. The sniffer collects information regarding the network performance, the power of the processing nodes, and the system load in general. They also present a scheduling system, which heuristically produces an allocation of MPI tasks onto processing nodes. The objective is to minimize the total completion time of the MPI program. The scheduling system uses program characterization in terms of processing, communication, synchronization, and dependencies; and the environment information collected by the sniffer to make scheduling decisions.",
A reverse engineering method and experiences for industrial COBOL system,"One of the most important things in restructuring current system is to clarify business specification which are implemented in the current system. Reverse engineering technology, which extracts reusable business specification from a system, is very effective for this purpose. We have developed DORE (Data Oriented Re-Engineering), a re-engineering methodology for system restructuring, and tools that support reverse engineering. By using these methodology and tools, it becomes possible to extract business specification which should be implemented in the new system, from the current system. In this paper, we present the re-engineering methodology and tools, and introduce a case study in Kawasaki Steel Corporation, in which those methodology and tools are used.",
Evaluating the importance of collaborative learning in ALN's,"Asynchronous learning networks (ALNs) represent a new paradigm for teaching and learning. A basic premise of the authors' research efforts has been that collaborative or group learning, which can be facilitated by anytime/anywhere access to the group communication and work space, is key to achieving superior learning outcomes in this medium. This paper describes some of the major evaluation procedures and results for a recently completed three year project that produced and delivered 26 courses in computer and information science using ALN, to over 1000 students. It focuses on some of the methodological problems and inconsistencies in results that occur in trying to assess the process and outcomes of teaching and learning in ALNs.",
An end user oriented platform for scientific visualization,"In our discussions with end users about today's visualization environments, we were led to consider new concepts for the setting up of a platform which should be better suited to their real needs. In this paper, a hierarchy of visualization objects is presented. They will be the main elements of a new platform which should be both intuitive and extensible.",
Designing cut and paste in a visual environment,"Cut and paste operations are common enough that users expect them to be provided in any application. However, some applications in which cut and paste operations are desirable have ambiguities which make it difficult to interpret the user's intent. A general method of designing this part of the system is desirable. Although current graphical environments exist which offer a simple version of cut and paste that can be easily implemented in applications, they are general in function and do not take advantage of any information available in the document. An environment that is designed to specifically support a single set of objects has the potential to provide more usable cut and paste operations, making common tasks easier and properly interpreting the user's actions.",
Neural network modeling of memory gradient in Alzheimer's disease,"Several studies have documented a temporal gradient in the memory of persons with Alzheimer's disease: patients are better able to recall more distant memories. The significance of this gradient is unclear: does the disease selectively interfere with the recall of recent memories, or does it prevent the memories from being adequately recorded? To address this question, neural networks were used to simulate learning over time. Once trained with a group of patterns, the networks were damaged to simulate the lesions associated with Alzheimer's disease. By altering the number of times a network was trained with a given pattern before additional patterns were added, and by varying the number of patterns in the training set, the direction of the temporal gradient was changed. The factors that determine the direction of the gradient are in place before the network is damaged. This suggests that the gradient associated with Alzheimer's disease is not a direct result of brain lesions that are hallmarks of the disease, but instead develops from an alteration of the learning process that begins long before dementia develops.",
Copyright in Shareware Software Distributed on the Internet - The Trumpet Winsock Case,"Since the 1980s most countries worldwide have introduced laws which extend copyright protection to computer software. In the first Australian case to consider copyright in a shareware computer program distributed on the Internet, the court held that the Internet service provider OzEmail had infringed Trumpet Software's copyright in Trumpet Winsock 2.OB by arranging for the program and a set of altered data files to be distributed with other software on diskette as a give-away inserted in copies of computer magazines. The implications of this case for software developers, distributors, and users are presented.",
Developing and accessing scientific databases with the Object-Protocol Model (OPM) data management tools,"The Object-Protocol Model (OPM) data management tools provide facilities for rapid development, documentation, and flexible exploration of scientific databases. The tools are based on OPM, an object oriented data model which is similar to the ODMG standard, but also supports extensions for modeling scientific data (L.A. Chen and V.M. Markowitz, 1995). Databases designed using OPM can be implemented using a variety of commercial relational DBMSs, using schema translation tools that generate complete DBMS database definitions from OPM schemas (L.A. Chen and V.M. Markowitz, 1996). Further OPM schemas can be retrofitted on top of existing databases defined using a variety of notations, such as the relational data model or the ASN.1 data exchange format, using OPM retrofitting tools (L.A. Chen et al., 1997).",
