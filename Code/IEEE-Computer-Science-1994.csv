Title,Abstract,Keywords
Algorithms for quantum computation: discrete logarithms and factoring,"A computer is generally considered to be a universal computational device; i.e., it is believed able to simulate any physical computational device with a cost in computation time of at most a polynomial factor: It is not clear whether this is still true when quantum mechanics is taken into consideration. Several researchers, starting with David Deutsch, have developed models for quantum mechanical computers and have investigated their computational properties. This paper gives Las Vegas algorithms for finding discrete logarithms and factoring integers on a quantum computer that take a number of steps which is polynomial in the input size, e.g., the number of digits of the integer to be factored. These two problems are generally considered hard on a classical computer and have been used as the basis of several proposed cryptosystems. We thus give the first examples of quantum cryptanalysis.","Quantum computing,
Quantum mechanics,
Polynomials,
Computational modeling,
Physics computing,
Computer simulation,
Costs,
Mechanical factors,
Cryptography,
Circuit simulation"
Genetic algorithms: a survey,"Genetic algorithms provide an alternative to traditional optimization techniques by using directed random searches to locate optimal solutions in complex landscapes. We introduce the art and science of genetic algorithms and survey current issues in GA theory and practice. We do not present a detailed study, instead, we offer a quick guide into the labyrinth of GA research. First, we draw the analogy between genetic algorithms and the search processes in nature. Then we describe the genetic algorithm that Holland introduced in 1975 and the workings of GAs. After a survey of techniques proposed as improvements to Holland's GA and of some radically different approaches, we survey the advances in GA theory related to modeling, dynamics, and deception.","Genetic algorithms,
Simulated annealing,
Genetic mutations,
Search methods,
Robustness,
Very large scale integration,
Strategic planning,
Technology planning,
Machine learning,
Heuristic algorithms"
Convergence analysis of canonical genetic algorithms,"This paper analyzes the convergence properties of the canonical genetic algorithm (CGA) with mutation, crossover and proportional reproduction applied to static optimization problems. It is proved by means of homogeneous finite Markov chain analysis that a CGA will never converge to the global optimum regardless of the initialization, crossover, operator and objective function. But variants of CGA's that always maintain the best solution in the population, either before or after selection, are shown to converge to the global optimum due to the irreducibility property of the underlying original nonconvergent CGA. These results are discussed with respect to the schema theorem.","Convergence,
Algorithm design and analysis,
Genetic algorithms,
Genetic mutations,
Biological cells,
Terminology,
Hamming distance,
Computer science"
Self-nonself discrimination in a computer,"The problem of protecting computer systems can be viewed generally as the problem of learning to distinguish self from other. The authors describe a method for change detection which is based on the generation of T cells in the immune system. Mathematical analysis reveals computational costs of the system, and preliminary experiments illustrate how the method might be applied to the problem of computer viruses.",
Disseminating active map information to mobile hosts,"The article describes an active map service (AMS) that supports context-aware computing by providing clients with information about located-objects and how those objects change over time. The authors focus on the communication issues of disseminating information from an active map server to its clients, and in particular, address how to deal with various overload situations that can occur. Simple unicast callbacks to interested clients work well enough if only a few located-objects are moving at any given time and only a few clients wish to know about any given move. However, if many people are moving about in the same region and many clients are interested in their motion, then the AMS may experience overload due to the quadratic nature of the communications involved. This overload affects both the server as well as any slow communications links being used. Mobile distributed computing enables users to interact with many different mobile and stationary computers over the course of the day. Navigating a mobile environment can be aided by active maps that describe the location and characteristics of objects within some region as they change over time.","Mobile computing,
Optical computing,
Printers,
Computer science,
Meeting planning,
Airports,
Application software,
Context-aware services,
Workstations,
Mobile communication"
A modified Hausdorff distance for object matching,"The purpose of object matching is to decide the similarity between two objects. This paper introduces 24 possible distance measures based on the Hausdorff distance between two point sets. These measures can be used to match two sets of edge points extracted from any two objects. Based on experiments on synthetic images containing various levels of noise, the authors determined that one of these distance measures, called the modified Hausdorff distance (MHD) has the best performance for object matching. The advantages of MHD ever other distances are also demonstrated on several edge snaps of objects extracted from real images.","Noise measurement,
Magnetohydrodynamics,
Power measurement,
Shape measurement,
Computer science,
Noise level,
Image segmentation,
Euclidean distance,
Performance evaluation,
Random number generation"
DSC: scheduling parallel tasks on an unbounded number of processors,"We present a low-complexity heuristic, named the dominant sequence clustering algorithm (DSC), for scheduling parallel tasks on an unbounded number of completely connected processors. The performance of DSC is on average, comparable to, or even better than, other higher-complexity algorithms. We assume no task duplication and nonzero communication overhead between processors. Finding the optimum solution for arbitrary directed acyclic task graphs (DAG's) is NP-complete. DSC finds optimal schedules for special classes of DAG's, such as fork, join, coarse-grain trees, and some fine-grain trees. It guarantees a performance within a factor of 2 of the optimum for general coarse-grain DAG's. We compare DSC with three higher-complexity general scheduling algorithms: the ETF by J.J. Hwang, Y.C. Chow, F.D. Anger, and C.Y. Lee (1989); V. Sarkar's (1989) clustering algorithm; and the MD by M.Y. Wu and D. Gajski (1990). We also give a sample of important practical applications where DSC has been found useful.","Processor scheduling,
Optimal scheduling,
Tree graphs,
Clustering algorithms,
Scheduling algorithm,
Computer science,
Parallel processing,
Multiprocessor interconnection networks,
Program processors,
Network topology"
Network intrusion detection,"Intrusion detection is a new, retrofit approach for providing a sense of security in existing computers and data networks, while allowing them to operate in their current ""open"" mode. The goal of intrusion detection is to identify unauthorized use, misuse, and abuse of computer systems by both system insiders and external penetrators. The intrusion detection problem is becoming a challenging task due to the proliferation of heterogeneous computer networks since the increased connectivity of computer systems gives greater access to outsiders and makes it easier for intruders to avoid identification. Intrusion detection systems (IDSs) are based on the beliefs that an intruder's behavior will be noticeably different from that of a legitimate user and that many unauthorized actions are detectable. Typically, IDSs employ statistical anomaly and rulebased misuse models in order to detect intrusions. A number of prototype IDSs have been developed at several institutions, and some of them have also been deployed on an experimental basis in operational systems. In the present paper, several host-based and network-based IDSs are surveyed, and the characteristics of the corresponding systems are identified. The host-based systems employ the host operating system's audit trails as the main source of input to detect intrusive activity, while most of the network-based IDSs build their detection mechanism on monitored network traffic, and some employ host audit trails as well. An outline of a statistical anomaly detection algorithm employed in a typical IDS is also included.","Intrusion detection,
Computer networks,
Protection,
Computer security,
Data security,
Computer science,
Computer crime,
Information security,
Real time systems,
Prototypes"
Optimum sequence multisets for synchronous code-division multiple-access channels,It is shown that the sum capacity of the symbol-synchronous code-division multiple-access channel with equal average-input-energy constraints is maximized precisely by those spreading sequence multisets that meet Welch's lower bound on total squared correlation. It is further shown that the symmetric capacity of the channel determined by these same sequence multisets is equal to the sum capacity.,"Multiaccess communication,
Spread spectrum communication,
Space technology,
Equations,
Computer science,
Polynomials,
Continuous wavelet transforms,
Autocorrelation,
Random sequences,
Gold"
Requirements specification for process-control systems,"The paper describes an approach to writing requirements specifications for process-control systems, a specification language that supports this approach, and an example application of the approach and the language on an industrial aircraft collision avoidance system (TCAS II). The example specification demonstrates: the practicality of writing a formal requirements specification for a complex, process-control system; and the feasibility of building a formal model of a system using a specification language that is readable and reviewable by application experts who are not computer scientists or mathematicians. Some lessons learned in the process of this work, which are applicable both to forward and reverse engineering, are also presented.","Software safety,
Writing,
Reverse engineering,
Control systems,
Costs,
Software systems,
Software prototyping,
System testing,
Computer science,
Specification languages"
Adaptive playout mechanisms for packetized audio applications in wide-area networks,"Recent interest in supporting packet-audio applications over wide area networks has been fueled by the availability of low-cost, toll-quality workstation audio and the demonstration that limited amounts of interactive audio can be supported by today's Internet. In such applications, received audio packets are buffered, and their playout delayed at the destination host in order to compensate for the variable network delays. The authors investigate the performance of four different algorithms for adaptively adjusting the playout delay of audio packets in an interactive packet-audio terminal application, in the face of such varying network delays. They evaluate the playout algorithms using experimentally-obtained delay measurements of audio traffic between several different Internet sites. Their results indicate that an adaptive algorithm which explicitly adjusts to the sharp, spike-like increases in packet delay which were observed in the traces can achieve a lower rate of lost packets for both a given average playout delay and a given maximum buffer size.",
Genetic-based new fuzzy reasoning models with application to fuzzy control,"The successful application of fuzzy reasoning models to fuzzy control systems depends on a number of parameters, such as fuzzy membership functions, that are usually decided upon subjectively. It is shown in this paper that the performance of fuzzy control systems may be improved if the fuzzy reasoning model is supplemented by a genetic-based learning mechanism. The genetic algorithm enables us to generate an optimal set of parameters for the fuzzy reasoning model based either on their initial subjective selection or on a random selection. It is shown that if knowledge of the domain is available, it is exploited by the genetic algorithm leading to an even better performance of the fuzzy controller.","Fuzzy reasoning,
Fuzzy control,
DC motors,
Genetic algorithms,
Fuzzy logic,
Fuzzy systems,
Learning systems,
Computer science,
Process control,
Humans"
Software measurement: a necessary scientific basis,"Software measurement, like measurement in any other discipline, must adhere to the science of measurement if it is to gain widespread acceptance and validity. The observation of some very simple, but fundamental, principles of measurement can have an extremely beneficial effect on the subject. Measurement theory is used to highlight both weaknesses and strengths of software metrics work, including work on metrics validation. We identify a problem with the well-known Weyuker properties (E.J. Weyuker, 1988), but also show that a criticism of these properties by J.C. Cherniavsky and C.H. Smith (1991) is invalid. We show that the search for general software complexity measures is doomed to failure. However, the theory does help us to define and validate measures of specific complexity attributes. Above all, we are able to view software measurement in a very wide perspective, rationalising and relating its many diverse activities.",
Scheduling algorithms and operating systems support for real-time systems,"This paper summarizes the state of the real-time field in the areas of scheduling and operating system kernels. Given the vast amount of work that has been done by both the operations research and computer science communities in the scheduling area, we discuss four paradigms underlying the scheduling approaches and present several exemplars of each. The four paradigms are: static table-driven scheduling, static priority preemptive scheduling, dynamic planning-based scheduling, and dynamic best effort scheduling. In the operating system context, we argue that most of the proprietary commercial kernels as well as real-time extensions to time-sharing operating system kernels do not fit the needs of predictable realtime systems. We discuss several research kernels that are currently being built to explicitly meet the needs of real-time applications.","Scheduling algorithm,
Operating systems,
Real time systems,
Processor scheduling,
Dynamic scheduling,
Kernel,
Computer science,
Timing,
Operations research,
Time sharing computer systems"
Application of a fuzzy discrimination analysis for diagnosis of valvular heart disease,"We have applied the discrimination analysis proposed by Norris, Pilsmorth, and Baldwin to the diagnosis of valvular heart diseases. They proposed the diagnosis method which uses concepts from fuzzy set theory. It consists of two independent parts: discrimination analysis and connectivity analysis. We performed the experiments in order to evaluate the effectiveness of the proposed discrimination analysis part of the method. Also, we extended the original method to handle partial manifestation of symptoms and severity of diseases by using fuzzy sets. In addition, we introduced the concept of prototypicalness of patients with a particular disease to improve the performance of the diagnosis. The results of the experiments are very promising. In the best case, we achieved a rate of true positive diagnosis of 81% while maintaining a rate of false positive diagnosis at the low level of 10%. We report the quantitative results of the experiments.",
WEKA: a machine learning workbench,"WEKA is a workbench for machine learning that is intended to aid in the application of machine learning techniques to a variety of real-world problems, in particular, those arising from agricultural and horticultural domains. Unlike other machine learning projects, the emphasis is on providing a working environment for the domain specialist rather than the machine learning expert. Lessons learned include the necessity of providing a wealth of interactive tools for data manipulation, result visualization, database linkage, and cross-validation and comparison of rule sets, to complement the basic machine learning tools.","Machine learning,
Machine learning algorithms,
Application software,
Computer science,
Expert systems,
Libraries,
User interfaces,
Data visualization,
Visual databases,
Couplings"
A comparative study of topological properties of hypercubes and star graphs,"Undertakes a comparative study of two important interconnection network topologies: the star graph and the hypercube, from the graph theory point of view. Topological properties are derived for the star graph and are compared with the corresponding properties of the hypercube. Among other results, the authors determine necessary and sufficient conditions for shortest path routing and characterize maximum-sized families of parallel paths between any two nodes of the star graph. These parallel paths are proven of minimum length within a small additive constant. They also define greedy and asymptotically balanced spanning trees to support broadcasting and personalized communication on the star graph. These results confirm the already claimed topological superiority of the star graph over the hypercube.","Hypercubes,
Tree graphs,
Routing,
Broadcasting,
Concurrent computing,
Computer science,
Multiprocessor interconnection networks,
Network topology,
Graph theory,
Additives"
SAAM: a method for analyzing the properties of software architectures,"While software architecture has become an increasingly important research topic in recent years, insufficient attention has been paid to methods for evaluation of these architectures. Evaluating architectures is difficult for two main reasons. First, there is no common language used to describe different architectures. Second, there is no clear way of understanding an architecture with respect to an organization's life cycle concerns -software quality concerns such as maintainability portability, modularity, reusability, and so forth. We address these shortcomings by describing three perspectives by which we can understand the description of a software architecture and then proposing a five-step method for analyzing software architectures called SAAM (Software Architecture Analysis Method). We illustrate the method by analyzing three separate user interface architectures with respect to the quality of modifiability.","Computer architecture,
Software architecture,
User interfaces,
Application software,
Software engineering,
Software quality,
Computer science,
Instruments,
Computer industry,
Software systems"
Real-time communication in packet-switched networks,"The dramatically increased bandwidths and processing capabilities of future high-speed networks make possible many distributed real-time applications, such as sensor-based applications and multimedia services. Since these applications will have traffic characteristics and performance requirements that differ dramatically from those of current data-oriented applications, new communication network architectures, and protocols will be required. In this paper we discuss the performance requirements and traffic characteristics of various real-time applications, survey recent developments in the areas of network architecture and protocols for supporting real-time services, and develop frameworks in which these, and future, research efforts can be considered.","Intelligent networks,
Bandwidth,
Protocols,
Computer science,
High-speed networks,
Application software,
Propagation delay,
Computer architecture,
Computer networks,
Optical fiber communication"
Imprecise computations,"The imprecise computation technique has been proposed as a way to handle transient overload and to enhance fault tolerance of real-time systems. In a system based on this technique, each time-critical task is designed in such a way that it can produce a usable, approximate result in time whenever a failure or overload prevents it from producing the desired, precise result. This paper describes ways to implement imprecise computations, models to characterize them and algorithms for scheduling them. An imprecise mechanism for the generation and use of approximate results can be integrated in a natural way with a traditional fault-tolerance mechanism. An architectural framework for this integration is described.","Timing,
Real time systems,
Time factors,
Target tracking,
Control systems,
Fault tolerant systems,
Fault tolerance,
Computer science,
Computational modeling,
Scheduling algorithm"
XmdvTool: integrating multiple methods for visualizing multivariate data,"Much of the attention in visualization research has focussed on data rooted in physical phenomena, which is generally limited to three or four dimensions. However, many sources of data do not share this dimensional restriction. A critical problem in the analysis of such data is providing researchers with tools to gain insights into characteristics of the data, such as anomalies and patterns. Several visualization methods have been developed to address this problem, and each has its strengths and weaknesses. This paper describes a system named XmdvTool which integrates several of the most common methods for projecting multivariate data onto a two-dimensional screen. This integration allows users to explore their data in a variety of formats with ease. A view enhancement mechanism called an N-dimensional brush is also described. The brush allows users to gain insights into spatial relationships over N dimensions by highlighting data which falls within a user-specified subspace.",
Real-time computing: a new discipline of computer science and engineering,This paper surveys the state of the art in real-time computing. It introduces basic concepts and identifies key issues in the design of real-time systems. Solutions proposed in literature for tackling these issues are also briefly discussed.,"Computer science,
Real time systems,
Automobiles,
Application software,
Wheels,
Processor scheduling,
Humans,
Resource management,
Environmental economics,
Computer displays"
Finding the k shortest paths,"We give algorithms for finding the k shortest paths (not required to be simple) connecting a pair of vertices in a digraph. Our algorithms output an implicit representation of these paths in a digraph with n vertices and m edges, in time O(m+n log n+k). We can also find the k shortest paths from a given source s to each vertex in the graph, in total time O(m+n log n+kn). We describe applications to dynamic programming problems including the knapsack problem, sequence alignment, and maximum inscribed polygons.","Shortest path problem,
Joining processes,
Dynamic programming,
Biology computing,
Computer science,
Robot motion,
Path planning,
Motion planning,
Road transportation,
Routing"
Efficient distance computation between non-convex objects,This paper describes an efficient algorithm for computing the distance between nonconvex objects. Objects are modeled as the union of a set of convex components. From this model we construct a hierarchical bounding representation based on spheres. The distance between objects is determined by computing the distance between pairs of convex components using preexisting techniques. The key to efficiency is a simple search routine that uses the bounding representation to ignore most of the possible pairs of components. The efficiency can further be improved by accepting a relative error in the returned result. Several empirical trials are presented to examine the performance of the algorithm.,"Robots,
Iterative algorithms,
Path planning,
Application software,
Laboratories,
Computer science,
Mathematical model,
Collision avoidance,
Object detection,
Computer graphics"
Efficient inverse kinematics for general 6R manipulators,"In this paper, we present an algorithm and implementation for efficient inverse kinematics for a general six-revolute (6R) manipulator. When stated mathematically, the problem reduces to solving a system of multivariate equations. We make use of the algebraic properties of the system and the symbolic formulation used for reducing the problem to solving a univariate polynomial. However, the polynomial is expressed as a matrix determinant and its roots are computed by reducing to an eigenvalue problem. The other roots of the multivariate system are obtained by computing eigenvectors and substitution. The algorithm involves symbolic preprocessing, matrix computations and a variety of other numerical techniques. The average running time of the algorithm, for most cases, is 11 milliseconds on an IBM RS/6000 workstation. This approach is applicable to inverse kinematics of all serial manipulators.",
Formalizing architectural connection,"As software systems become more complex the overall system structure - or software architecture - becomes a central design problem. An important step towards an engineering discipline of software is a formal basis for describing and analyzing these designs. We present a theory for one aspect of architectural description, the interactions between components. The key idea is to define architectural connectors as explicit semantic entities. These are specified as a collection of protocols that characterize each of the participant roles in an interaction and how these roles interact. We illustrate how this scheme can be used to define a variety of common architectural connectors. We provide a formal semantics and show how this lends to a sound deductive system in which architectural compatibility can be checked in a way analogous to type checking in programming languages.",
Computer as thinker/doer: problem-solving environments for computational science,"During the early 1960s, scientists began to envision problem-solving computing environments not only powerful enough to solve complex problems but also able to interact with users on human terms. While many tried to create PSEs over the next few years, by the early 1970s they had abandoned almost all of these attempts. Technology could not yet support PSEs in computational science. But the dream of the 1960s can be the reality of the 1990s: high-performance computers combined with better understanding of computing and computational science have put PSEs well within our reach. The term 'problem-solving environment' means different things to different people. A PSE is a computer system that provides all the computational facilities necessary to solve a target class of problems. These features include advanced solution methods, automatic or semi-automatic selection of solution methods, and ways to easily incorporate novel solution methods. Simple PSEs appeared early in computing without being recognized as such. Some of the capabilities of future problem-solving environments seem like science fiction, but whatever form they eventually take, their scientific and economic impact will be enormous.","Problem-solving,
Operating systems,
Computer languages,
Humans,
Power generation economics,
Environmental economics,
Hardware,
Software,
Color,
Computer graphics"
Distributed reset,"A reset subsystem is designed that can be embedded in an arbitrary distributed system in order to allow the system processes to reset the system when necessary. Our design is layered, and comprises three main components: a leader election, a spanning tree construction, and a diffusing computation. Each of these components is self-stabilizing in the following sense: if the coordination between the up-processes in the system is ever lost (due to failures or repairs of processes and channels), then each component eventually reaches a state where coordination is regained. This capability makes our reset subsystem very robust: it can tolerate fail-stop failures and repairs of processes and channels, even when a reset is in progress.",
Scheduling multithreaded computations by work stealing,"This paper studies the problem of efficiently scheduling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is ""work stealing,"" in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies. Specifically, our analysis shows that the expected time T/sub P/ to execute a fully strict computation on P processors using our work-stealing scheduler is T/sub P/=O(T/sub 1//P+T/sub /spl infin//), where T/sub 1/ is the minimum serial execution time of the multithreaded computation and T/sub /spl infin// is the minimum execution time with an infinite number of processors. Moreover, the space S/sub P/ required by the execution satisfies S/sub P//spl les/S/sub 1/P. We also show that the expected total communication of the algorithm is at most O(T/sub /spl infin//S/sub max/P), where S/sub max/ is the size of the largest activation record of any thread, thereby justifying the folk wisdom that work-stealing schedulers are more communication efficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.","Processor scheduling,
Yarn,
Concurrent computing,
Scheduling algorithm,
Algorithm design and analysis,
Laboratories,
Computer science,
Dynamic scheduling,
Load management,
Data structures"
MINT: a front end for efficient simulation of shared-memory multiprocessors,"MINT is a software package designed to ease the process of constructing event-driven memory hierarchy simulators for multiprocessors. It provides a set of simulated processors that run standard Unix executable files compiled for a MIPS R3000 based multiprocessor. These generate multiple streams of memory reference events that drive a user-provided memory system simulator. MINT uses a novel hybrid technique that exploits the best aspects of native execution and software interpretation to minimize the overhead of processor simulation. Combined with related techniques to improve performance, this approach makes simulation on uniprocessor hosts extremely efficient.","Computational modeling,
Computer simulation,
Costs,
Interleaved codes,
Computer science,
Software packages,
Discrete event simulation,
High performance computing,
Parallel machines,
Scheduling"
Test selection based on communicating nondeterministic finite-state machines using a generalized Wp-method,"Presents a method of generating test sequences for concurrent programs and communication protocols that are modeled as communicating nondeterministic finite-state machines (CNFSMs). A conformance relation, called trace-equivalence, is defined within this model, serving as a guide to test generation. A test generation method for a single nondeterministic finite-state machine (NFSM) is developed, which is an improved and generalized version of the Wp-method that generates test sequences only for deterministic finite-state machines. It is applicable to both nondeterministic and deterministic finite-state machines. When applied to deterministic finite-state machines, it yields usually smaller test suites with full fault coverage than the existing methods that also provide full fault coverage, provided that the number of states in implementation NFSMs are bounded by a known integer. For a system of CNFSMs, the test sequences are generated in the following manner: a system of CNFSMs is first reduced into a single NFSM by reachability analysis; then the test sequences are generated from the resulting NFSM using the generalized Wp-method.",
"""Eigenlips"" for robust speech recognition","We improve the performance of a hybrid connectionist speech recognition system by incorporating visual information about the corresponding lip movements. Specifically, we investigate the benefits of adding visual features in the presence of additive noise and crosstalk (cocktail party effect). Our study extends our previous experiments by using a new visual front end, and an alternative architecture for combining the visual and acoustic information. Furthermore, we have extended our recognizer to a multi-speaker, connected letters recognizer. Our results show a significant improvement for the combined architecture (acoustic and visual information) over just the acoustic system in the presence of additive noise and crosstalk.","Robustness,
Speech recognition,
Crosstalk,
Additive noise,
Image segmentation,
Deformable models,
Computer science,
Acoustic distortion,
Background noise,
Acoustic noise"
Connectionist probability estimators in HMM speech recognition,"The authors are concerned with integrating connectionist networks into a hidden Markov model (HMM) speech recognition system. This is achieved through a statistical interpretation of connectionist networks as probability estimators. They review the basis of HMM speech recognition and point out the possible benefits of incorporating connectionist networks. Issues necessary to the construction of a connectionist HMM recognition system are discussed, including choice of connectionist probability estimator. They describe the performance of such a system using a multilayer perceptron probability estimator evaluated on the speaker-independent DARPA Resource Management database. In conclusion, they show that a connectionist component improves a state-of-the-art HMM system.",
"Understanding ""why"" in software process modelling, analysis, and design","In trying to understand and redesign software processes, it is often necessary to have an understanding of the ""whys"" that underlie the ""whats"" - the motivations, intents, and rationales behind the activities and input-output flows. This paper presents a model which captures the intentional structure of a software process and its embedding organization, in terms of dependency relationships among actors. Actors depend on each other for goals to be achieved, tasks to be performed, and resources to be furnished. The model is embedded in the conceptual modeling language Telos. We outline some analytical tools to be developed for the model, and illustrate how the model can help in the systematic design of software processes. The examples used are adaptations of the ISPW-6/7 benchmark example.",
Science and substance: a challenge to software engineers,"For 25 years, software researchers have proposed improving software development and maintenance with new practices whose effectiveness is rarely, if ever, backed up by hard evidence. We suggest several ways to address the problem, and we challenge the community to invest in being more scientific.","Software engineering,
Standards development,
Software standards,
Documentation,
Technology management,
Software maintenance,
Productivity,
Object oriented programming,
Computer aided software engineering,
Process control"
Adaptive linear quadratic control using policy iteration,In this paper we present the stability and convergence results for dynamic programming-based reinforcement learning applied to linear quadratic regulation (LQR). The specific algorithm we analyze is based on Q-learning and it is proven to converge to an optimal controller provided that the underlying system is controllable and a particular signal vector is persistently excited. This is the first convergence result for DP-based reinforcement learning algorithms for a continuous problem.,"Programmable control,
Adaptive control,
Optimal control,
Cost function,
Symmetric matrices,
Control systems,
Computer science,
Vectors,
Learning,
Feedback control"
Guaranteeing synchronous message deadlines with the timed token medium access control protocol,"We study the problem of guaranteeing synchronous message deadlines in token ring networks where the timed token medium access control protocol is employed. Synchronous bandwidth, defined as the maximum time for which a node can transmit its synchronous messages every time it receives the token, is a key parameter in the control of synchronous message transmission. To ensure the transmission of synchronous messages before their deadlines, synchronous capacities must be properly allocated to individual nodes. We address the issue of appropriate allocation of the synchronous capacities. Several synchronous bandwidth allocation schemes are analyzed in terms of their ability to satisfy deadline constraints of synchronous messages. We show that an inappropriate allocation of the synchronous capacities could cause message deadlines to be missed, even if the synchronous traffic is extremely low. We propose a scheme, called the normalized proportional allocation scheme, which can guarantee the synchronous message deadlines for synchronous traffic of up to 33% of available utilization.","Media Access Protocol,
Access protocols,
Real time systems,
Bandwidth,
Channel allocation,
Computer science,
Delay,
Distributed computing,
Token networks,
FDDI"
Knowledge based (re-)clustering,"This paper presents a novel multiparadigm segmentation method based upon knowledge based clustering with reclustering. The techniques described enhance unsupervised classification and achieve pattern labeling. First domain knowledge is utilized to decide where and how a clustering algorithm is applied, then clustering is iteratively applied to focus-of-attention patterns with the knowledge of how many expected classes there are in those patterns and the prototypical patterns of a class. Examples showing clustering improvements are given from brain MRI's and satellite images.","Clustering algorithms,
Knowledge engineering,
Focusing,
Prototypes,
Satellites,
Training data,
Expert systems,
Computer science,
Labeling,
Iterative algorithms"
Illumination planning for object recognition using parametric eigenspaces,"Presents a novel approach to the problem of illumination planning for robust object recognition in structured environments. Given a set of objects, the goal is to determine the illumination for which the objects are most distinguishable in appearance from each other. Correlation is used as a measure of similarity between objects. For each object, a large number of images is automatically obtained by varying the pose and the illumination direction. Images of all objects together constitute the planning image set. The planning set is compressed using the Karhunen-Loeve transform to obtain a low-dimensional subspace, called the eigenspace. For each illumination direction, objects are represented as parametrized manifolds in the eigenspace. The minimum distance between the manifolds of two objects represents the similarity between the objects in the correlation sense. The optimal source direction is therefore the one that maximizes the shortest distance between the object manifolds. Several experiments have been conducted using real objects. The results produced by the illumination planner have been used to enhance the performance of an object recognition system.","Lighting,
Object recognition,
Brightness,
Machine vision,
Robotic assembly,
Image edge detection,
Reflectivity,
Computer science,
Robustness,
Karhunen-Loeve transforms"
Efficient Representation and Manipulation of Switching Functions Based on Ordered Kronecker Functional Decision Diagrams,"An efficient package for construction of and operation on ordered Kronecker Functional Decision Diagrams (OKFDD) is presented. OKFDDs are a generalization of OBDDs and OFDDs and as such provide a more compact representation of the functions than either of the two decision diagrams. In this paper basic properties of OKFDDs and their efficient representation and manipulation are presented. Based on the comparison of the three decision diagrams for several benchmark functions, a 25% improve ment in size over OBDDs is observed for OKFDDs.",
Selective pressure in evolutionary algorithms: a characterization of selection mechanisms,"Due to its independence of the actual search space and its impact on the exploration-exploitation tradeoff, selection is an important operator in any kind of evolutionary algorithm. All important selection operators are discussed and quantitatively compared with respect to their selective pressure. The comparison clarifies that only a few really different and useful selection operators exist: proportional selection (in combination with a scaling method), linear ranking, tournament selection, and (/spl mu/,/spl lambda/)-selection (respectively (/spl mu/+/spl lambda/)-selection). Their selective pressure increases in the order as they are listed here. The theoretical results are confirmed by an experimental investigation using a genetic algorithm with different selection methods on a simple unimodal objective function.","Evolutionary computation,
Genetic mutations,
Genetic algorithms,
Electronic switching systems,
Computer science"
Session guarantees for weakly consistent replicated data,"Four per-session guarantees are proposed to aid users and applications of weakly consistent replicated data: ""read your writes"", ""monotonic reads"", ""writes follow reads"", and ""monotonic writes"". The intent is to present individual applications with a view of the database that is consistent with their own actions, even if they read and write from various, potentially inconsistent servers. The guarantees can be layered on existing systems that employ a read-any/write-any replication scheme while retaining the principal benefits of such a scheme, namely high availability, simplicity, scalability, and support for disconnected operation. These session guarantees were developed in the context of the Bayou project at Xerox PARC in which we are designing and building a replicated storage system to support the needs of mobile computing users who may be only intermittently connected.",
Maisie: a language for the design of efficient discrete-event simulations,"Maisie is a C-based discrete-event simulation language that was designed to cleanly separate a simulation model from the underlying algorithm (sequential or parallel) used for the execution of the model. With few modifications, a Maisie program may be executed by using a sequential simulation algorithm, a parallel conservative algorithm or a parallel optimistic algorithm. The language constructs allow the run-time system to implement optimizations that reduce recomputation and state saving overheads for optimistic simulations and synchronization overheads for conservative implementations. This paper presents the Maisie simulation language, describes a set of optimizations, and illustrates the use of the language in the design of efficient parallel simulations.","Computational modeling,
Discrete event simulation,
Computer simulation,
Algorithm design and analysis,
Parallel architectures,
Protocols,
Design optimization,
Concurrent computing,
Distributed computing,
Computer science"
Incorporation of correlated structural images in PET image reconstruction,"Reports on a new method in which spatially correlated magnetic resonance (MR) or X-ray computed tomography (CT) images are employed as a source of prior information in the Bayesian reconstruction of positron emission tomography (PET) images. This new method incorporates the correlated structural images as anatomic templates which can be used for extracting information about boundaries that separate regions exhibiting different tissue characteristics. In order to avoid the possible introduction of artifacts caused by discrepancies between functional and anatomic boundaries, the authors propose a new method called the ""weighted line site"" method, in which a prior structural image is employed in a modified updating scheme for the boundary variable used in the iterative Bayesian reconstruction. This modified scheme is based on the joint probability of structural and functional boundaries. As to the structural information provided by CT or MR images, only those which have high joint probability with the corresponding PET data are used; whereas other boundary information that is not supported by the PET image is suppressed. The new method has been validated by computer simulation and phantom studies. The results of these validation studies indicate that this new method offers significant improvements in image quality when compared to other reconstruction algorithms, including the filtered backprojection method and the maximum likelihood approach, as well as the Bayesian method without the use of the prior boundary information.",
Design and engineering aspects of a high resolution positron tomograph for small animal imaging,"Describes the Sherbrooke positron emission tomograph, a very high resolution device dedicated to dynamic imaging of small laboratory animals. Its distinctive features are: small discrete scintillation detectors based on avalanche photodiodes (APD) to achieve uniform, isotropic, very high spatial resolution; parallel processing for low deadtime and high count rate capability; multispectral data acquisition hardware to improve sensitivity and scatter correction; modularity to allow design flexibility and upgradability. The system implements the ""clam-shell"" sampling scheme and a rotating rod transmission source. All acquisition parameters can be adjusted under computer control. Temperature stability at the detector site is ensured by the use of thermoelectric modules. The initial system consists of one layer of 256 modules (two rings of detectors) defining 3 image slices in a 118 mm diameter by 10.5 mm thick field. The axial field can be extended to 50 mm using 4 layers of modules (8 rings of detectors). The design constraints and engineering aspects of an APD-based PET scanner are reviewed and preliminary results are reported.","Design engineering,
Positrons,
Spatial resolution,
Detectors,
Radioactive decay,
Image resolution,
High-resolution imaging,
Laboratories,
Animals,
Scintillation counters"
An empirical evaluation of weak mutation,"Mutation testing is a fault-based technique for unit-level software testing. Weak mutation was proposed as a way to reduce the expense of mutation testing. Unfortunately, weak mutation is also expected to provide a weaker test of the software than mutation testing does. This paper presents results from an implementation of weak mutation, which we used to evaluate the effectiveness versus the efficiency of weak mutation. Additionally, we examined several options in an attempt to find the most appropriate way to implement weak mutation. Our results indicate that weak mutation can be applied in a manner that is almost as effective as mutation testing, and with significant computational savings.",
Hypercube communication delay with wormhole routing,"We present an analytical model for the performance evaluation of hypercube computers. This analysis is aimed at modeling a deadlock-free wormhole routing scheme prevalent on second generation hypercube systems. Probability of blocking and average message delay are the two performance measures discussed. We start with the communication traffic to find the probability of blocking. The traffic analysis can capture any message destination distribution. Next, we find the average message delay that consists of two parts. The first part is the actual message transfer delay between any source and destination nodes. The second part of the delay is due to blocking caused by the wormhole routing scheme. The analysis is also extended to virtual cut-through routing and random wormhole routing techniques. The validity of the model is demonstrated by comparing analytical results with those from simulation.",
Extended virtual synchrony,"We formulate a model of extended virtual synchrony that defines a group communication transport service for multicast and broadcast communication in a distributed system. The model extends the virtual synchrony model of the Isis system to support continued operation in all components of a partitioned network. The significance of extended virtual synchrony is that, during network partitioning and remerging and during process failure and recovery, it maintains a consistent relationship between the delivery of messages and the delivery of configuration changes across all processes in the system and provides well-defined self-delivery and failure atomicity properties. We describe an algorithm that implements extended virtual synchrony and construct a filter that reduces extended virtual synchrony to virtual synchrony.","Broadcasting,
Multicast protocols,
Intersymbol interference,
Multicast algorithms,
Partitioning algorithms,
Local area networks,
Hardware,
Electronic switching systems,
Computer science,
Bridges"
Using processor affinity in loop scheduling on shared-memory multiprocessors,"Loops are the single largest source of parallelism in many applications. One way to exploit this parallelism is to execute loop iterations in parallel on different processors. Previous approaches to loop scheduling attempted to achieve the minimum completion time by distributing the workload as evenly as possible while minimizing the number of synchronization operations required. The authors consider a third dimension to the problem of loop scheduling on shared-memory multiprocessors: communication overhead caused by accesses to nonlocal data. They show that traditional algorithms for loop scheduling, which ignore the location of data when assigning iterations to processors, incur a significant performance penalty on modern shared-memory multiprocessors. They propose a new loop scheduling algorithm that attempts to simultaneously balance the workload, minimize synchronization, and co-locate loop iterations with the necessary data. They compare the performance of this new algorithm to other known algorithms by using five representative kernel programs on a Silicon Graphics multiprocessor workstation, a BBN Butterfly, a Sequent Symmetry, and a KSR-1, and show that the new algorithm offers substantial performance improvements, up to a factor of 4 in some cases. The authors conclude that loop scheduling algorithms for shared-memory multiprocessors cannot afford to ignore the location of data, particularly in light of the increasing disparity between processor and memory speeds.",
New locomotion gaits,"This paper investigates new modes of robot land locomotion, in particular statically stable non-wheeled, non-tracked locomotion. These locomotion gaits are accomplished by a reconfigurable modular robot called Polypod using a control scheme combining a small number of primitive control modes for each module. The design of Polypod is first reviewed, then two and three-dimensional locomotion gaits are described along with two ""exotic"" gaits. These gaits have been implemented on Polypod or simulated on a graphic workstation.","Legged locomotion,
Couplings,
Kinematics,
Mobile robots,
Turning,
Leg,
Laboratories,
Computer science,
Tracking,
Graphics"
Checkpointing distributed applications on mobile computers,"The integration of mobile/portable computing devices within existing data networks can be expected to spawn distributed applications that execute on mobile hosts (MHs). For reliability, it is vital that the global state of such applications be checkpointed from time to time. A global checkpoint consists of a set of local checkpoints, one per participant. This paper first identifies the problems in recording a consistent global state of mobile distributed applications. The location of a MH within the static network varies with time and therefore, a MH will first need to be located (""searched"") in order to obtain its local checkpoint. Moreover, MHs often (voluntarily) disconnect from the network; a disconnected MH is not reachable from the rest of the network This means that a (disconnected) MH may not be available to provide its local checkpoint. Lastly, a MH is not equipped with stable storage; disk space at a MH is not considered stable due to vulnerability of MHs to loss, theft and physical damage. Therefore, an alternative stable repository is required to save local checkpoints of MHs. This paper presents a checkpointing algorithm for MHs that satisfies these constraints.","Checkpointing,
Application software,
Mobile computing,
Computer applications,
Distributed computing,
Personal digital assistants,
Portable computers,
Disk recording,
Algorithm design and analysis,
Computer science"
Efficiency enhancement of high power vacuum BWO's using nonuniform slow wave structures,"The Sinus-6, a high-power relativistic repetitively-pulsed electron beam accelerator, is used to drive various slow wave structures in a BWO configuration in vacuum. Peak output power of about 550 MW at 9.45 GHz was radiated in an 8-ns pulse. We describe experiments which study the relative efficiencies of microwave generation from a two-stage nonuniform amplitude slow wave structure and its variations without an initial stage. Experimental results are compared with 2.5 D particle-in-cell computer simulations. Our results suggest that prebunching the electron beam in the initial section of the nonuniform BWO results in increased microwave generation efficiency, Furthermore, simulations reveal that, in addition to the backward propagating surface harmonic of the TM/sub 01/ mode, backward and forward propagating volume harmonics with phase velocity twice that of the surface harmonic play an important role in high-power microwave generation and radiation.",
Efficient organization of large multidimensional arrays,"Large multidimensional arrays are widely used in scientific and engineering database applications. The authors present methods of organizing arrays to make their access on secondary and tertiary memory devices fast and efficient. They have developed four techniques for doing this: (1) storing the array in multidimensional ""chunks"" to minimize the number of blocks fetched, (2) reordering the chunked array to minimize seek distance between accessed blocks, (3) maintaining redundant copies of the array, each organized for a different chunk size and ordering and (4) partitioning the array onto platters of a tertiary memory device so as to minimize the number of platter switches. The measurements on real data obtained from global change scientists show that accesses on arrays organized using these techniques are often an order of magnitude faster than on the unoptimized data.","Multidimensional systems,
Data engineering,
Switches,
Image processing,
Delay,
Magnetic devices,
Computer science,
Databases,
Application software,
Organizing"
False sharing and spatial locality in multiprocessor caches,"The performance of the data cache in shared-memory multiprocessors has been shown to be different from that in uniprocessors. In particular, cache miss rates in multiprocessors do not show the sharp drop typical of uniprocessors when the size of the cache block increases. The resulting high cache miss rate is a cause of concern, since it can significantly limit the performance of multiprocessors. Some researchers have speculated that this effect is due to false sharing, the coherence transactions that result when different processors update different words of the same cache block in an interleaved fashion. While the analysis of six applications in the paper confirms that false sharing has a significant impact on the miss rate, the measurements also show that poor spatial locality among accesses to shared data has an even larger impact. To mitigate false sharing and to enhance spatial locality, we optimize the layout of shared data in cache blocks in a programmer-transparent manner. We show that this approach can reduce the number of misses on shared data by about 10% on average.","Cache memory,
Optimizing compilers,
Large-scale systems,
Hardware,
Programming profession,
Cache storage,
Delay,
Interleaved codes,
Pensions,
Computer science education"
Towards direct reconstruction from a gamma camera based on Compton scattering,"The Compton scattering camera (sometimes called the electronically collimated camera) has been shown by others to have the potential to better the photon counting statistics and the energy resolution of the Anger camera for imaging in SPECT. By using coincident detection of Compton scattering events on two detecting planes, a photon can be localized to having been sourced on the surface of a cone. New algorithms are needed to achieve fully three-dimensional reconstruction of the source distribution from such a camera. If a complete set of cone-surface projections are collected over an infinitely extending plane, it is shown that the reconstruction problem is not only analytically solvable, but also overspecified in the absence of measurement uncertainties. Two approaches to direct reconstruction are proposed, both based on the photons which travel perpendicularly between the detector planes. Results of computer simulations are presented which demonstrate the ability of the algorithms to achieve useful reconstructions in the absence of measurement uncertainties (other than those caused by quantization). The modifications likely to be required in the presence of realistic measurement uncertainties are discussed.","Cameras,
Image reconstruction,
Single photon emission computed tomography,
Measurement uncertainty,
Electromagnetic scattering,
Particle scattering,
Event detection,
Optical collimators,
Statistics,
Energy resolution"
Generating basis siphons and traps of Petri nets using the sign incidence matrix,"This paper introduces a new matrix called the sign incidence matrix for Petri nets. Using this sign incidence matrix, we present a simple algorithm for generating all basis siphons or traps without first generating all siphons or traps. Any siphon (trap) can be expressed as an union of basis siphons (basis traps). The concept of siphons and traps plays an important role in the analysis of Petri nets. In particular, criteria for liveness and reachability of some subclasses of Petri nets can be stated in terms of siphons and traps.","Petri nets,
Logic programming,
Equations,
System recovery,
Reproducibility of results,
Sufficient conditions,
Computer science,
Logic functions,
Linear algebra,
Linear matrix inequalities"
On the Convergence of Stochastic Iterative Dynamic Programming Algorithms,"Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments. These algorithms, including the TD(λ) algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP). In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem. The theorem establishes a general class of convergent algorithms to which both TD(λ) and Q-learning belong.",
Comparing ladder logic diagrams and Petri nets for sequence controller design through a discrete manufacturing system,"Design methods for sequence controllers play a very important role in advancing industrial automation. The increasing complexity and varying needs of modern discrete manufacturing systems have challenged the traditional design methods such as the use of ladder logic diagrams (LLDs) for programmable logic controllers. The methodologies based on research results in computer science have recently received growing attention by academic researchers and industrial engineers in order to design flexible, reusable, and maintainable control software. Particularly, Petri nets are emerging as a very important tool to provide an integrated solution for modeling, analysis, simulation, and control of industrial automated systems. This paper identifies certain criteria to compare LLDs and Petri nets in designing sequence controllers and responding to the changing control requirements. The comparison is performed through a practical system after introducing ""real-time Petri nets"" for discrete-event control purposes. The results reported in this paper will help: (a) further establish Petri net based techniques for discrete-event control of industrial automated systems; and (b) effectively convince industrial practitioners and researchers that it is worthy and timely to consider and promote the applications of Petri nets to their particular discrete-event control problems.","Logic,
Petri nets,
Automatic control,
Electrical equipment industry,
Industrial control,
Control systems,
Design methodology,
Manufacturing industries,
Programmable control,
Software maintenance"
Evolving space-filling curves to distribute radial basis functions over an input space,"An evolutionary neural network training algorithm is proposed for radial basis function (RBF) networks. The locations of basis function centers are not directly encoded in a genetic string, but are governed by space-filling curves whose parameters evolve genetically. This encoding causes each group of codetermined basis functions to evolve to fit a region of the input space. A network produced from this encoding is evaluated by training its output connections only. Networks produced by this evolutionary algorithm appear to have better generalization performance on the Mackey-Glass time series than corresponding networks whose centers are determined by k-means clustering.","Neural networks,
Encoding,
Evolutionary computation,
Humans,
Training data,
Genetic algorithms,
Genetic mutations,
Computer networks,
Computer science"
On the power of quantum computation,"The quantum model of computation is a probabilistic model, similar to the probabilistic Turing Machine, in which the laws of chance are those obeyed by particles on a quantum mechanical scale, rather than the rules familiar to us from the macroscopic world. We present here a problem of distinguishing between two fairly natural classes of function, which can provably be solved exponentially faster in the quantum model than in the classical probabilistic one, when the function is given as an oracle drawn equiprobably from the uniform distribution on either class. We thus offer compelling evidence that the quantum model may have significantly more complexity theoretic power than the probabilistic Turing Machine. In fact, drawing on this work, Shor (1994) has recently developed remarkable new quantum polynomial-time algorithms for the discrete logarithm and integer factoring problems.","Quantum computing,
Turing machines,
Quantum mechanics,
Computational modeling,
Polynomials,
Computer simulation,
Physics computing,
Computer errors,
Lakes,
Relativistic quantum mechanics"
"Perfect binary codes: constructions, properties, and enumeration","Properties of nonlinear perfect binary codes are investigated and several new constructions of perfect codes are derived from these properties. An upper bound on the cardinality of the intersection of two perfect codes of length n is presented, and perfect codes whose intersection attains the upper bound are constructed for all n. As an immediate consequence of the proof of the upper bound the authors obtain a simple closed-form expression for the weight distribution of a perfect code. Furthermore, they prove that the characters of a perfect code satisfy certain constraints, and provide a sufficient condition for a binary code to be perfect. The latter result is employed to derive a generalization of the construction of Phelps (1983), which is shown to give rise to some perfect codes that are nonequivalent to the perfect codes obtained from the known constructions. Moreover, for any m/spl ges/4 the authors construct full-rank perfect binary codes of length 2/sup m/-1. These codes are obviously nonequivalent to any of the previously known perfect codes. Furthermore the latter construction exhibits the existence of full-rank perfect tilings. Finally, they construct a set of 2(2/sup cn/) nonequivalent perfect codes of length n, for sufficiently large n and a constant c=0.5-/spl epsiv/. Precise enumeration of the number of codes in this set provides a slight improvement over the results reported by Phelps.","Binary codes,
Upper bound,
Closed-form solution,
Hamming distance,
Hamming weight,
Computer science"
On unifying some cryptographic protocol logics,"We present a logic for analyzing cryptographic protocols. This logic encompasses a unification of four of its predecessors in the BAN family of logics, namely those given by Li Gong et al. (1990); M. Abadi, M. Tuttle (1991); P.C. van Oorschot (1993); and BAN itself (M. Burrows et al., 1989). We also present a model-theoretic semantics with respect to which the logic is sound. The logic presented captures all of the desirable features of its predecessors and more; nonetheless, it accomplishes this with no more axioms or rules than the simplest of its predecessors.","Cryptographic protocols,
Body sensor networks,
Laboratories,
Cryptography,
Authentication,
Computer science,
Drives,
Logic design,
Computational modeling"
Tree-structured nonlinear filters in digital mammography,"A new class of nonlinear filters with more robust characteristics for noise suppression and detail preservation is proposed for processing digital mammographic images. The new algorithm consists of two major filtering blocks: (a) a multistage tree-structured filter for image enhancement that uses central weighted median filters as basic sub-filtering blocks and (b) a dispersion edge detector. The design of the algorithm also included the use of linear and curved windows to determine whether variable shape windowing could improve detail preservation. First, the noise-suppressing properties of the tree-structured filter were compared to single filters, namely the median and the central weighted median with conventional square and variable shape adaptive windows; simulated images were used for this purpose. Second, the edge detection properties of the tree-structured filter cascaded with the dispersion edge detector were compared to the performance of the dispersion edge detector alone, the Sobel operator, and the single median filter cascaded with the dispersion edge detector. Selected mammographic images with representative biopsy-proven malignancies were processed with all methods and the results were visually evaluated by an expert mammographer. In all applications, the proposed filter suggested better detail preservation, noise suppression, and edge detection than all other approaches and it may prove to be a useful tool for computer-assisted diagnosis in digital mammography.","Nonlinear filters,
Mammography,
Image edge detection,
Detectors,
Shape,
Adaptive filters,
Noise robustness,
Filtering algorithms,
Image enhancement,
Algorithm design and analysis"
Improving End-to-End Performance of TCP over Mobile Internetworks,"Reliable tronsport protocols such as TCP use end-toendflow, congestion, and error control mechanisms to provide reliable delivery over an internetwork. However, the end-to-end performance of a TCP connection con suffer significant degradation in the presence of a wireless link. We are exploring alternatives for optimizing end-to-end performance of TCP connections qcross an internetwork consisting of both fixed and mobile networks. The central idea in our opproach is to tronsparently split an end-to-end connection into two separale connections; one over the wireless link and other over the wired poth. The connection over the wireless link mny either use regulor TCP or a specialized transport protocol optimized for better perfornwtnce over o wireless link. Our approach does not require any changes to the existing protocol software on stationary hosts. Results of a systematic performonce evalustion using both our approoch and regular TCP show that our approach yields significant performance improvements.","Mobile communication,
Wireless communication,
Base stations,
Protocols,
IP networks,
Software,
Internet"
Decomposition constructions for secret-sharing schemes,"The paper describes a very powerful decomposition construction for perfect secret-sharing schemes. The author gives several applications of the construction and improves previous results by showing that for any graph G of maximum degree d, there is a perfect secret-sharing scheme for G with information rate 2/(d+1). As a corollary, the maximum information rate of secret-sharing schemes for paths on more than three vertices and for cycles on more than four vertices is shown to be 2/3.","Information rates,
Security,
Terminology,
Computer science,
Information science,
Mathematical model,
Cryptography"
Adaptation of the relaxation method for learning in bidirectional associative memory,"An iterative learning algorithm called PRLAB is described for the discrete bidirectional associative memory (BAM). Guaranteed recall of all training pairs is ensured by PRLAB. The proposed algorithm is significant in many ways. Unlike many existing iterative learning algorithms, PRLAB is not based on the gradient descent technique. It is a novel adaptation from the well-known relaxation method for solving a system of linear inequalities. The algorithm is very fast. Learning 200 random patterns in a 200-200 BAM takes only 20 epochs on the average. PRLAB is highly insensitive to learning parameters and the initial configuration of a BAM. It also offers high scalability for large applications by providing the same high performance when the number of training patterns are increased in proportion to the size of the BAM. An extensive performance analysis of the new learning algorithm is included.",
"Computer-assisted registration, segmentation, and 3D reconstruction from images of neuronal tissue sections","Neuroscientists have studied the relationship between nerve cell morphology and function for over a century. To pursue these studies, they need accurate three-dimensional models of nerve cells that facilitate detailed anatomical measurement and the identification of internal structures. Although serial transmission electron microscopy has been a source of such models since the mid 1960s, model reconstruction and analysis remain very time consuming. The authors have developed a new approach to reconstructing and visualizing 3D nerve cell models from serial microscopy. An interactive system exploits recent computer graphics and computer vision techniques to significantly reduce the time required to build such models. The key ingredients of the system are a digital ""blink comparator"" for section registration, ""snakes,"" or active deformable contours, for semiautomated cell segmentation, and voxel-based techniques for 3D reconstruction and visualization of complex cell volumes with internal structures.","Image segmentation,
Image reconstruction,
Morphology,
Data visualization,
Electron microscopy,
Computer graphics,
Computer vision,
Transmission electron microscopy,
Interactive systems,
Diseases"
Automated detection of vulnerabilities in privileged programs by execution monitoring,"Presents a method for detecting exploitations of vulnerabilities in privileged programs by monitoring their execution using audit trails, where the monitoring is with respect to specifications of the security-relevant behavior of the programs. Our work is motivated by the intrusion detection paradigm, but is an attempt to avoid ad hoc approaches to codifying misuse behavior. Our approach is based on the observation that although privileged programs can be exploited (due to errors) to cause security compromises in systems because of the privileges accorded to them, the intended behavior of privileged programs is, of course, limited and benign. The key, then, is to specify the intended behavior (i.e. the program policy) and to detect any action by a privileged program that is outside the intended behavior and that imperils security. We describe a program policy specification language, which is based on simple predicate logic and regular expressions. In addition, we present specifications of privileged programs in Unix, and a prototype execution monitor for analyzing audit trails with respect to these specifications. The program policies are surprisingly concise and clear, and in addition, capable of detecting exploitations of known vulnerabilities in these programs. Although our work has been motivated by the known vulnerabilities in Unix, we believe that by tightly restricting the behavior of all privileged programs, exploitations of unknown vulnerabilities can be detected. As a check on the specifications, work is in progress on verifying them with respect to an abstract security policy.","Computerized monitoring,
National security,
Specification languages,
Logic,
Computer security,
Contracts,
Fingers,
Computer science,
Intrusion detection,
Prototypes"
On the relationship between partition and random testing,"Weyuker and Jeng (ibid., vol. SE-17, pp. 703-711, July 1991) have investigated the conditions that affect the performance of partition testing and have compared analytically the fault-detecting ability of partition testing and random testing. This paper extends and generalizes some of their results. We give more general ways of characterizing the worst case for partition testing, along with a precise characterization of when this worst case is as good as random testing. We also find that partition testing is guaranteed to perform at least as well as random testing so long as the number of test cases selected is in proportion to the size of the subdomains.",
Roll-forward checkpointing scheme: a novel fault-tolerant architecture,"We propose a novel architecture for a fault-tolerant multiprocessor environment. It is assumed that the multiprocessor organization consists of a pool of active processing modules and either a small number of spare modules or active modules with some spare processing capacity. A fault-tolerance scheme is developed for duplex systems using checkpoints. Our scheme, unlike traditional checkpointing schemes, requires no rollbacks for recovering from single faults. The objective is to achieve performance of a triple modular redundant system using duplex system redundancy.",
Necessary conditions for asymptotic tracking in nonlinear systems,"In the literature, it has been shown that if a single-input single-output analytic nonlinear plant 1) has a well-defined relative degree and 2) is minimum-phase, it is possible to achieve asymptotic tracking for an open set of output trajectories containing the origin in C/sup N/ [0, /spl infin/), the space of N-times continuously differentiable functions taking values in R. When either of these sufficient conditions is not met, various authors have investigated approximate analytic solutions, discontinuous solutions and solutions for restricted sets of trajectories. In this paper, it is shown that conditions 1) and 2) are necessary for the existence of an analytic compensator which yields asymptotic tracking for an open set of output trajectories. Analogous results are established for multi-input multi-output systems.","Nonlinear systems,
Trajectory,
Sufficient conditions,
Differential equations,
Regulators,
Signal generators,
Open loop systems,
Servomechanisms,
Computer science,
Robots"
An algorithm for array variable clustering,"During synthesis of behavioral descriptions array variables are implemented with memory modules. In this paper we show that simple one-to-one mapping between the array variables and the memory modules lead to inefficient designs. We propose a new algorithm, MeSA, which computes for a given set of array variables, (a) the number of memory modules, (b) the size of each module (c) the number of ports on each module and (d) and the grouping of array variables assigned to each memory module. The effects of address translations are incorporated into the algorithm. While most previous research efforts have concentrated on scalar variables, the primary focus in this paper is deriving efficient storage assignment for array variables.","Clustering algorithms,
Random access memory,
Computer science,
Storage automation,
High level synthesis,
Hardware,
Costs,
Registers,
Art,
Adders"
"Low-latency, concurrent checkpointing for parallel programs","Presents the results of an implementation of several algorithms for checkpointing and restarting parallel programs on shared-memory multiprocessors. The algorithms are compared according to the metrics of overall checkpointing time, overhead imposed by the checkpointer on the target program, and amount of time during which the checkpointer interrupts the target program. The best algorithm measured achieves its efficiency through a variation of copy-on-write, which allows the most time-consuming operations of the checkpoint to be overlapped with the running of the program being checkpointed.","Checkpointing,
Fault tolerance,
Computer science,
Central Processing Unit,
Benchmark testing,
Fault tolerant systems,
Concurrent computing,
Delay,
Registers"
A performance study of software and hardware data prefetching schemes,"Prefetching, i.e., exploiting the overlap of processor computations with data accesses, is one of several approaches for tolerating memory latencies. Prefetching can be either hardware-based or software-directed or a combination of both. Hardware-based prefetching, requiring some support unit connected to the cache, can dynamically handle prefetches at run-time without compiler intervention. Software-directed approaches rely on compiler technology to insert explicit prefetch instructions. Mowry et al.'s software scheme (1991,1992) and the authors' hardware approach (1991) are two representative schemes. In this paper, the authors evaluate approximations to these two schemes in the context of a shared-memory multiprocessor environment. Their qualitative comparisons indicate that both schemes are able to reduce cache misses in the domain of linear array references. When complex data access patterns are considered, the software approach has compile-time information to perform sophisticated prefetching whereas the hardware scheme has the advantage of manipulating dynamic information. The performance results from an instruction-level simulation of four benchmarks confirm these observations. Simulations show that the hardware scheme introduces more memory traffic into the network and that the software scheme introduces a non-negligible instruction execution overhead. An approach combining software and hardware schemes is proposed; it shows promise in reducing the memory latency with least overhead.",
Interprocedural def-use associations for C systems with single level pointers,"Def-use analysis links possible value-setting statements for a variable (i.e. definitions) to potential value-fetches (i.e. uses) of that value. This paper describes the first algorithm that calculates accurate interprocedural def-use associations in C software systems. Our algorithm accounts for program-point-specific pointer-induced aliases, though it is currently limited to programs using a single level of indirection. We prove the NP-hardness of the interprocedural reaching definitions problem and describe the approximations made by our polynomial-time algorithm. Initial empirical results are also presented.","Performance analysis,
Polynomials,
Algorithm design and analysis,
Arithmetic,
Software tools,
Debugging,
System testing,
Information analysis,
Software systems,
Computer science"
Embedding of rings and meshes onto faulty hypercubes using free dimensions,"Fault tolerance in hypercubes is achieved by exploiting inherent redundancy and executing tasks on faulty hypercubes. The authors consider tasks that require linear chain, ring, mesh, and torus structure, which are quite useful in parallel and pipeline computations. They assume the number of faults is on the order of the number of dimensions of the hypercube. The techniques are based on a key concept called free dimension, which can be used to partition a cube into subcubes such that each subcube contains, at most, one faulty node. Subgraphs are embedded in each subcube and then merged to form the entire graph.","Hypercubes,
Fault tolerance,
Hardware,
Redundancy,
Pipelines,
Concurrent computing,
Costs,
Multiprocessing systems,
Computer science,
Emulation"
An economic paradigm for query processing and data migration in Mariposa,"Many new database applications require very large volumes of data. Mariposa is a database system under construction at Berkeley responding to this need. This system combines the best features of traditional distributed database systems, object-oriented DBMSs, tertiary memory file systems and distributed file systems. Mariposa objects can be stored over thousands of autonomous sites and on memory hierarchies with very large capacity. This scale of the system leads to complex query execution and storage management issues, unsolvable in practice with traditional techniques. We propose an economic paradigm as the solution. A query receives a budges which it spends to obtain the answers. Each site attempts to maximize income by buying and selling storage objects, and processing queries for locally stored objects. We present the protocols which underlie the Mariposa economy.",
How accurate is scientific software?,"This paper describes some results of what, to the authors' knowledge, is the largest N-version programming experiment ever performed. The object of this ongoing four-year study is to attempt to determine just how consistent the results of scientific computation really are, and, from this, to estimate accuracy. The experiment is being carried out in a branch of the earth sciences known as seismic data processing, where 15 or so independently developed large commercial packages that implement mathematical algorithms from the same or similar published specifications in the same programming language (Fortran) have been developed over the last 20 years. The results of processing the same input dataset, using the same user-specified parameters, for nine of these packages is reported in this paper. Finally, feedback of obvious flaws was attempted to reduce the overall disagreement. The results are deeply disturbing. Whereas scientists like to think that their code is accurate to the precision of the arithmetic used, in this study, numerical disagreement grows at around the rate of 1% in average absolute difference per 4000 fines of implemented code, and, even worse, the nature of the disagreement is nonrandom. Furthermore, the seismic data processing industry has better than average quality standards for its software development with both identifiable quality assurance functions and substantial test datasets.","Data processing,
Packaging,
Geoscience,
Computer languages,
Feedback,
Arithmetic,
Computer industry,
Software standards,
Standards development,
Programming"
"Learning, positioning, and tracking visual appearance","The problem of vision-based robot positioning and tracking is addressed. A general learning algorithm is presented for determining the mapping between robot position and object appearance. The robot is first moved through several displacements with respect to its desired position, and a large set of object images is acquired. This image set is compressed using principal component analysis to obtain a four-dimensional subspace. Variations in object images due to robot displacements are represented as a compact parametrized manifold in the subspace. While positioning or tracking, errors in end-effector coordinates are efficiently computed from a single brightness image using the parametric manifold representation. The learning component enables accurate visual control without any prior hand-eye calibration. Several experiments have been conducted to demonstrate the practical feasibility of the proposed positioning/tracking approach and its relevance to industrial applications.","Robot kinematics,
Robot sensing systems,
Manipulator dynamics,
Computer science,
Brightness,
Intelligent robots,
Robotics and automation,
Calibration,
Robot vision systems,
Image coding"
Detector response restoration in image reconstruction of high resolution positron emission tomography,"A mathematical method was studied to model the detector response of high spatial-resolution positron emission tomography systems consisting of close-packed small crystals, and to restore the resolution deteriorated due to crystal penetration and/or nonuniform sampling across the field-of-view (FOV). The simulated detector system had 600 bismuth germanate crystals of 3.14 mm width and 30 mm length packed on a single ring of 60 cm diameter. The space between crystals was filled up with lead (i.e., septa). Each crystal was in coincidence with 200 opposite crystals so that the FOV had a radius of 30 cm. The detector response was modeled based on the attenuating properties of the crystals and the septa, as well as the geometry of the detector system. The modeled detector-response function was used to restore the projections from the sinogram of the ring-detector system. The restored projections had a uniform sampling of 1.57 mm across the FOV. The crystal penetration and/or the nonuniform sampling were compensated in the projections. A penalized maximum-likelihood algorithm was employed to accomplish the restoration. The restored projections were then filtered and backprojected to reconstruct the image. A chest phantom with a few small circular ""cold"" objects (/spl ap/4 mm diameter) located at the center and near the periphery of FOV was computer generated and used to test the restoration. The reconstructed images from the restored projections demonstrated resolution improvement off the FOV center, while preserving the resolution near the center.","Detectors,
Image restoration,
Image reconstruction,
Crystals,
Nonuniform sampling,
Image resolution,
Mathematical model,
Positron emission tomography,
Spatial resolution,
Bismuth"
Maximum likelihood sequence estimation from the lattice viewpoint,"Considers the problem of data detection in multilevel lattice-type modulation systems in the presence of intersymbol interference and additive white Gaussian noise. The conventional maximum likelihood sequence estimator using the Viterbi algorithm has a time complexity of O(m/sup /spl nu/+1/) operations per symbol and a space complexity of O(/spl delta/m/sup /spl nu//) storage elements, where m is the size of input alphabet, /spl nu/ is the length of channel memory, and /spl delta/ is the truncation depth. By revising the truncation scheme and viewing the channel as a linear transform, the authors identify the problem of maximum likelihood sequence estimation with that of finding the nearest lattice point. From this lattice viewpoint, the lattice sequence estimator for PAM systems is developed, which has the following desired properties: 1) its expected time-complexity grows as /spl delta//sup 2/ as SNR/spl rarr//spl infin/; 2) its space complexity grow as /spl delta/; and 3) its error performance is effectively optimal for sufficiently large m. A tight upper bound on the symbol error probability of the new estimator is derived, and is confirmed by the simulation results of an example channel. It turns out that the estimator is effectively optimal for m/spl ges/4 and the loss in signal-to-noise ratio is less than 0.5 dB even for m=2. Finally, limitations of the proposed estimator are also discussed.","Maximum likelihood estimation,
Lattices,
Authentication,
Cryptography,
Computer science,
Equations,
Upper bound,
Codes,
IEEE Press,
Combinatorial mathematics"
Localizing in unstructured environments: dealing with the errors,"A robot navigating in an unstructured outdoor environment must determine its own location in spite of problems due to environmental conditions, sensor limitations and map inaccuracies, exact measurements are seldom known, and the combination of approximate measures can lead to large errors in self-localization. The conventional approach to this problem has been to deal with the errors either during processing or after they occur. The authors maintain that it is possible to limit the errors before they occur. The authors analyze how measurement errors affect errors in localization and propose that a simple algorithm can be used to exploit the geometric properties of landmarks in the environment in order to decrease errors in localization. The authors' goal is to choose landmarks that will provide the best localization regardless of measurement error, determine the best areas in which to identify new landmarks to be used for further localization and choose paths that will provide the least chance of ""straying"". The authors show the result of implementing this concept in experiments run in simulation with USGS 30 m DEM data for a robot statically locating, following a path and identifying new landmarks.","Global Positioning System,
Navigation,
Mobile robots,
Robot sensing systems,
Delay,
Measurement errors,
Computer science,
Ice,
Clouds,
Error analysis"
Very-high radix division with prescaling and selection by rounding,"A division algorithm in which the quotient-digit selection is performed by rounding the shifted residual in carry-save form is presented. To allow the use of this simple function, the divisor (and dividend) is prescaled to a range close to one. The implementation presented results in a fast iteration because of the use of carry-save forms and suitable recodings. The execution time is calculated and several convenient values of the radix are selected. Comparison with other dividers for radices 2/sup 9/ to 2/sup 18/ is performed using the same assumptions.","Iterative algorithms,
Digital arithmetic,
Delay,
Interpolation,
Computer science,
Approximation algorithms"
View integration: a step forward in solving structural conflicts,"Thanks to the development of the federated systems approach on the one hand and the emphasis on user involvement in database design on the other, the interest in schema integration techniques is significantly increasing. Theories, methods and tools have been proposed. Conflict resolution is the key issue. Different perceptions by schema designers may lead to different representations. A way must be found to support these different representations within a single system. Most current integration methodologies rely on modification of initial schemas to solve the conflicts. This approach needs a strong interaction with the database administrator, who has authority to modify the initial schemas. This paper presents an approach to view integration specifically intended to support the coexistence of different representations of the same real-world objects. The main characteristics of this approach are the following: automatic resolution of structural conflicts, conflict resolution performed without modification of initial views, use of a formal declarative approach for user definition of inter-view correspondences, applicability to a variety of data models, and automatic generation of structural and operational mappings between the views and the integrated schema. Allowing users' views to be kept unchanged should result in improved user satisfaction. Each user is able to define his own view of the database, without having to conform to some other user's view. Moreover, such a feature is essential in database integration if existing programs are to be preserved.",
Precision and accuracy of regional radioactivity quantitation using the maximum likelihood EM reconstruction algorithm,"The imaging characteristics of maximum likelihood (ML) reconstruction using the EM algorithm for emission tomography have been extensively evaluated. There has been less study of the precision and accuracy of ML estimates of regional radioactivity concentration. The authors developed a realistic brain slice simulation by segmenting a normal subject's MRI scan into gray matter, white matter, and CSF and produced PET sinogram data with a model that included detector resolution and efficiencies, attenuation, scatter, and randoms. Noisy realizations at different count levels were created, and ML and filtered backprojection (FBP) reconstructions were performed. The bias and variability of ROI values were determined. In addition, the effects of ML pixel size, image smoothing and region size reduction were assessed. Hit estimates at 3,000 iterations (0.6 sec per iteration on a parallel computer) for 1-cm/sup 2/ gray matter ROIs showed negative biases of 6%/spl plusmn/2% which can be reduced to 0%/spl plusmn/3% by removing the outer 1-mm rim of each ROI. FBP applied to the full-size ROIs had 15%/spl plusmn/4% negative bias with 50% less noise than hit. Shrinking the FBP regions provided partial bias compensation with noise increases to levels similar to ML. Smoothing of ML images produced biases comparable to FBP with slightly less noise. Because of its heavy computational requirements, the ML algorithm will be most useful for applications in which achieving minimum bias is important.","Noise level,
Maximum likelihood estimation,
Image reconstruction,
Brain modeling,
Smoothing methods,
Maximum likelihood detection,
Image segmentation,
Magnetic resonance imaging,
Positron emission tomography,
Detectors"
Robotica: a Mathematica package for robot analysis,Robotica is a computer aided design package for robotic manipulators developed in the Coordinated Science Laboratory at the University of Illinois at Urbana-Champaign. It encapsulates over 30 functions into a Mathematica package allowing efficient symbolic and numeric calculation of kinematic and dynamic equations for multi-degree-of-freedom manipulators. An X-Windows front end that utilizes the interprocess communication features of Mathematica 2.1 has also been created for ease of use. This paper describes the most important features of the package and how they are used.,"Robot kinematics,
Equations,
Robotics and automation,
Manipulator dynamics,
Animation,
Packaging machines,
Ellipsoids,
Arm,
Shape,
Time factors"
On the use and implementation of message logging,"We present a number of experiments showing that for compute-intensive applications executing in parallel on clusters of workstations, message logging has higher failure-free overhead than coordinated checkpointing. Message logging protocols, however, result in much shorter output latency than coordinated checkpointing. Therefore, message logging should be used for applications involving substantial interactions with the outside world, while coordinated checkpointing should be used otherwise. We also present an unorthodox message logging design that uses coordinated checkpointing with message logging, departing from the conventional approaches that use independent checkpointing. This combination of message logging and coordinated checkpointing offers several advantages, including improved failure-free performance, bounded recovery time, simplified garbage collection, and reduced complexity. Meanwhile, the new protocols retain the advantages of the conventional message logging protocols with respect to output commit. Finally, we discuss three ""lessons learned"" from an implementation of various message logging protocols.","Protocols,
Checkpointing,
Costs,
Workstations,
Computer science,
Computer applications,
Concurrent computing,
Application software,
Delay,
Batteries"
A graph-oriented object database model,"A graph-oriented object database model (GOOD) is introduced as a theoretical basis for database systems in which manipulation as well as conceptual representation of data is transparently graph-based. In the GOOD model, the scheme as well as the instance of an object database is represented by a graph, and the data manipulation is expressed by graph transformations. These graph transformations are described using five basic operations and a method construct, all with a natural semantics. The basic operations add and delete objects and edges as a function of the matchings of a pattern. The expressiveness of the model in terms of object-oriented modeling and data manipulation power is investigated.","Object oriented modeling,
Object oriented databases,
Data models,
Relational databases,
Power system modeling,
Database systems,
Database languages,
Application software,
Computer science,
Pattern matching"
Estimating the mean and variance of the target probability distribution,"Introduces a method that estimates the mean and the variance of the probability distribution of the target as a function of the input, given an assumed target error-distribution model. Through the activation of an auxiliary output unit, this method provides a measure of the uncertainty of the usual network output for each input pattern. The authors derive the cost function and weight-update equations for the example of a Gaussian target error distribution, and demonstrate the feasibility of the network on a synthetic problem where the true input-dependent noise level is known.<>","Probability distribution,
Noise level,
Feedforward systems,
Computer science,
Cognitive science,
Computer errors,
Measurement uncertainty,
Cost function,
Equations,
Error correction"
Descendant set: an efficient approach for the analysis of polling systems,"Polling systems have been used to model a large variety of applications and much research has been devoted to the derivation of efficient algorithms for computing the delay measures in these systems. Recent research efforts in this area, which have focused on the optimization of these systems, have raised the need for very efficient such algorithms. This work develops the descendant set approach as a general efficient algorithm for deriving all moments of customer delay (in particular, mean delay) in these systems. The method is applied to a very large variety of model variations, including: 1) The exhaustive and gated service policies, 2) Fractional service policies, 3) The cyclic visit order, 4) Arbitrary periodic visit orders (polling tables), and 5) Customer routing. For most of these variations the method significantly outperforms the algorithms commonly used today.","Algorithm design and analysis,
Delay effects,
Delay systems,
Routing,
Application software,
Token networks,
Transmission lines,
Communications Society,
Computer science,
Iterative methods"
Bar code waveform recognition using peak locations,"Traditionally, zero crossings of the second derivative provide edge features for the classification of blurred waveforms. The accuracy of these edge features deteriorates in the case of severely blurred images. In this paper, a new feature is presented that is more resistant to the blurring process, the image, and waveform peaks. In addition, an estimate of the standard deviation /spl sigma/ of the blurring kernel is used to perform minor deblurring of the waveform. Statistical pattern recognition is used to classify the peaks as bar code characters. The noise tolerance of this recognition algorithm is increased by using an adaptive, histogram-based technique to remove the noise. In a bar code environment that requires a misclassification rate of less than one in a million, the recognition algorithm showed a 43% performance improvement over current commercial bar code reading equipment.","Image edge detection,
Working environment noise,
Iterative algorithms,
Kernel,
Pattern recognition,
Parameter estimation,
Decoding,
Table lookup,
Research and development,
Computer science"
On multi-arm manipulation planning,"This paper considers the automatic generation of motion paths for several cooperating robot arms to manipulate a movable object between two configurations among obstacles. To avoid collisions the robots may have to change their grasp of the object, for example, by passing it from one arm to another. The case where the movable object can only be moved by two arms acting simultaneously is also considered. An approach for solving this planning problem is described and illustrated with a robot system made of three arms moving in a 3D environment. Experiments with a planner implementing this approach show that it is not only fast, but also reliable in finding collision-free paths.","Arm,
Robotics and automation,
Motion planning,
Manipulators,
Path planning,
Laboratories,
Computer science,
Robot programming,
Computational geometry"
Routing in ad hoc networks of mobile hosts,"An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any centralized administration or standard support services. In such an environment, it may be necessary for one mobile host to enlist the aid of others in forwarding a packet to its destination due to the limited propagation range of each mobile host's wireless transmissions. Some previous attempts have been made to use conventional routing protocols for routing in ad hoc networks, treating each mobile host as a router. This position paper points out a number of problems with this design and suggests a new approach based on separate route discovery and route maintenance protocols.","Intelligent networks,
Ad hoc networks,
Mobile computing,
Routing protocols,
IP networks,
Computer science,
Web and internet services,
Airports,
Internetworking,
Home automation"
A process algebraic approach to the specification and analysis of resource-bound real-time systems,"Recently, significant progress has been made in the development of timed process algebras for the specification and analysis of real-time systems. This paper describes a timed process algebra called ACSR, which supports synchronous timed actions and asynchronous instantaneous events. Timed actions are used to represent the usage of resources and to model the passage of time. Events are used to capture synchronization between processes. To be able to specify real systems accurately, ACSR supports a notion of priority that can be used to arbitrate among timed actions competing for the use of resources and among events that are ready for synchronization. The paper also includes a brief overview of other timed process algebras and discusses similarities and differences between them and ACSR.","Real time systems,
Algebra,
Timing,
Delay,
Availability,
Mathematical model,
Prototypes,
Information science,
Computer science,
Logic functions"
Lossless compression of multispectral image data,"While spatial correlations are adequately exploited by standard lossless image compression techniques, little success has been attained in exploiting spectral correlations when dealing with multispectral image data. The authors present some new lossless image compression techniques that capture spectral correlations as well as spatial correlation in a simple and elegant manner. The schemes are based on the notion of a prediction tree, which defines a noncausal prediction model for an image. The authors present a backward adaptive technique and a forward adaptive technique. They then give a computationally efficient way of approximating the backward adaptive technique. The approximation gives good results and is extremely easy to compute. Simulation results show that for high spectral resolution images, significant savings can be made by using spectral correlations in addition to spatial correlations. Furthermore, the increase in complexity incurred in order to make these gains is minimal.","Image coding,
Multispectral imaging,
Decorrelation,
Image storage,
Computer science,
Predictive models,
Computational modeling,
Spatial resolution,
Image resolution,
Data compression"
A comparison of heuristics for scheduling DAGs on multiprocessors,"Many algorithms to schedule directed acyclic graphs (DAGs) on multiprocessors have been proposed, but there has been little work done to determine their effectiveness. Since multiprocessor scheduling is an NP-hard problem, no exact tractable algorithm exists, and no baseline is available from which to compare the resulting schedules. This paper is an attempt to quantify the differences in a few of the heuristics. The empirical performance of five heuristics is compared when they are applied to ten specific DAGs which represent program dependence graphs of important applications. The comparison is made between a graph based method a list scheduling technique and three critical path methods.","Scheduling algorithm,
Processor scheduling,
Costs,
Clustering algorithms,
Tree graphs,
Computer science,
NP-hard problem,
Multiprocessing systems,
Parallel machines,
Performance analysis"
An experiment to assess different defect detection methods for software requirements inspections,"Software requirements specifications (SRS) are usually validated by inspections, in which several reviewers read all or part of the specification and search for defects. We hypothesize that different methods for conducting these searches may have significantly different rates of success. Using a controlled experiment, we show that a scenario-based detection method, in which each reviewer executes a specific procedure to discover a particular class of defects has a higher defect detection rate than either ad hoc or checklist methods. We describe the design, execution and analysis of the experiment so others may reproduce it and test our results for different kinds of software developments and different populations of software engineers.","Inspection,
Computer science,
Educational institutions,
Production,
Software testing,
Design engineering,
Hardware,
Fault detection,
Software standards,
Software performance"
Keeping track of position and orientation of moving indoor systems by correlation of range-finder scans,"One of the problems of autonomous mobile systems is the continuous tracking of position and orientation. In most cases, this problem is solved by dead reckoning, based on measurement of wheel rotations or step counts and step width. Unfortunately dead reckoning leads to accumulation of drift errors and is very sensitive to slipping. In this paper an algorithm for tracking position and orientation is presented, it is nearly independent from odometry and its problems with slipping. To achieve these results, a rotating range-finder is used, delivering scans of the environmental structure. The properties of this structure are used to match the scans from different locations in order to find their translational and rotational displacement. For this purpose derivatives of range-finder scans are calculated which can be used to find position and orientation by crosscorrelation.","Dead reckoning,
Humans,
Computer science,
Rotation measurement,
Wheels,
Navigation,
State estimation,
Vehicles,
Position measurement"
Determination of optimal angiographic viewing angles: basic principles and evaluation study,"Foreshortening of vessel segments in angiographic (biplane) projection images may cause misinterpretation of the extent and degree of coronary artery disease. The views in which the object of interest are visualized with minimum foreshortening are called optimal views. The authors present a complete approach to obtain such views with computer-assisted techniques. The object of interest is first visualized in two arbitrary views. Two landmarks of the object are manually defined in the two projection images. With complete information of the projection geometry, the vector representation of the object in the three-dimensional space is computed. This vector is perpendicular to a plane in which the views are called optimal. The user has one degree of freedom to define a set of optimal biplane views. The angle between the central beams of the imaging systems can be chosen freely. The computation of the orientation of the object and of corresponding optimal biplane views have been evaluated with a simple hardware phantom. The mean and the standard deviation of the overall errors in the calculation of the optimal angulation angles were 1.8/spl deg/ and 1.3/spl deg/, respectively, when the user defined a rotation angle.","X-ray imaging,
Image segmentation,
Image intensifiers,
Optical imaging,
Visualization,
Cardiology,
Angioplasty,
Radiology,
Nuclear medicine,
Hospitals"
Interprocessor collective communication library (InterCom),"We outline a unified approach for building a library of collective communication operations that performs well on a cross-section of problems encountered in real applications. The target architecture is a two-dimensional mesh with worm-hole routing, but the techniques also apply to higher dimensional meshes and hypercubes. We stress a general approach, addressing the need for implementations that perform well for various sized vectors and grid dimensions, including non-power-of-two grids. This requires the development of general techniques for building hybrid algorithms. Finally, the approach also supports collective communication within a group of nodes, which is required by many scalable algorithms. Results from the Intel Paragon system are included.","Libraries,
Supercomputers,
High performance computing,
Computer science,
Application software,
Hypercubes,
Stress,
Councils,
Broadcasting,
Scattering"
Efficient detection and resolution of generalized distributed deadlocks,"We present an efficient one-phase algorithm that consists of two concurrent sweeps of messages to detect generalized distributed deadlocks. In the outward sweep, the algorithm records a snapshot of a distributed wait-for-graph (WFG). In the inward sweep, the algorithm performs reduction of the recorded distributed WFG to check for a deadlock. The two sweeps can overlap in time at a process. We prove the correctness of the algorithm. The algorithm has a worst-case message complexity of 4e/spl minus/2n+2l and a time complexity of 2d hops, where e is the number of edges, n is the number of nodes, l is the number of leaf nodes, and d is the diameter of the WFG. This is a notable improvement over the existing algorithms to detect generalized deadlocks.","System recovery,
Topology,
Information science,
Phase detection,
Associate members,
Software algorithms,
Sufficient conditions,
Detection algorithms,
Algorithm design and analysis"
PREPARE: a tool for knowledge base verification,"The knowledge base is the most important component in a knowledge-based system. Because a knowledge base is often built in an incremental, piecemeal fashion, potential errors may be inadvertently brought into it. One of the critical issues in developing reliable knowledge-based systems is how to verify the correctness of a knowledge base. The paper describes an automated tool called PREPARE for detecting potential errors in a knowledge base. PREPARE is based on modeling a knowledge base by using a predicate/transition net representation. Inconsistent, redundant, subsumed, circular, and incomplete rules in a knowledge base are then defined as patterns of the predicate/transition net model, and are detected through a syntactic pattern recognition method. The research results to date have indicated that: the methodology ran be adopted in knowledge-based systems where logic is used as knowledge representation formalism; the tool can be invoked at any stage of the system's development, even without a fully functioning inference engine; the predicate/transition net model of knowledge bases is easy to implement and provides a clear and understandable display of the knowledge to be used by the system.","Knowledge based systems,
Computer science,
Pattern recognition,
Redundancy,
Knowledge engineering,
Computer errors,
Logic,
Knowledge representation,
Engines,
Displays"
Universally ideal secret-sharing schemes,"Given a set of parties {1, /spl middot//spl middot//spl middot/, n}, an access structure is a monotone collection of subsets of the parties. For a certain domain of secrets, a secret-sharing scheme for an access structure is a method for a dealer to distribute shares to the parties. These shares enable subsets in the access structure to reconstruct the secret, while subsets not in the access structure get no information about the secret. A secret-sharing scheme is ideal if the domains of the shares are the same as the domain of the secrets. An access structure is universally ideal if there exists an ideal secret-sharing scheme for it over every finite domain of secrets. An obvious necessary condition for an access structure to be universally ideal is to be ideal over the binary and ternary domains of secrets. The authors prove that this condition is also sufficient. They also show that being ideal over just one of the two domains does not suffice for universally ideal access structures. Finally, they give an exact characterization for each of these two conditions.","Cryptography,
Indium tin oxide,
Computer science,
Sufficient conditions,
Algebra"
Computation of surface geometry and segmentation using covariance techniques,"In this correspondence, the application of covariance techniques to surface representation of 3-D objects is discussed and such ways of computing surface geometry are compared with traditional methods using differential geometry. It is shown how the covariance method provides surface descriptors that are invariant to rigid motions without explicitly using surface parameterizations or derivatives. Analogous covariance operators for both the Gauss and Weingarten maps are defined and a range image segmentation technique is presented that labels pixels as jump or crease discontinuities or planar, parabolic or curved region types.","Computational geometry,
Image segmentation,
Eigenvalues and eigenfunctions,
Gaussian processes,
Shape measurement,
Covariance matrix,
Computer science,
Pixel,
Object recognition,
Noise measurement"
Adaptive deadlock- and livelock-free routing with all minimal paths in torus networks,"This paper consists of two parts. In the first part, two new algorithms for deadlock- and livelock-free wormhole routing in the torus network are presented. The first algorithm, called Channels, is for the n-dimensional torus network. This technique is fully-adaptive minimal, that is, all paths with a minimal number of hops from source to destination are available for routing, and needs only five virtual channels per bidirectional link, the lowest channel requirement known in the literature for fully-adaptive minimal worm-hole routing. In addition, this result also yields the lowest buffer requirement known in the literature for packet-switched fully-adaptive minimal routing. The second algorithm, called 4-Classes, is for the bidimensional torus network. This technique is fully-adaptive minimal and requires only eight virtual channels per bidirectional link. Also, it allows for a highly parallel implementation of its associated routing node. In the second part of this paper, four worm-hole routing techniques for the two-dimensional torus are experimentally evaluated using a dynamic message injection model and different traffic patterns and message lengths.","System recovery,
Routing,
Intelligent networks,
Computational modeling,
Computer networks,
Concurrent computing,
Computer simulation,
Multiprocessor interconnection networks,
Computer science,
Hypercubes"
Using iterative refinement to find reusable software,"Component libraries are the dominant paradigm for software reuse, but they suffer from a lack of tools that support the problem-solving process of locating relevant components. Most retrieval tools assume that retrieval is a simple matter of matching well-formed queries to a repository. But forming queries can be difficult. A designer's understanding of the problem evolves while searching for a component, and large repositories often use an esoteric vocabulary. CodeFinder is a retrieval system that combines retrieval by reformulation (which supports incremental query construction) and spreading activation (which retrieves items related to the query) to help users find information. I designed it to investigate the hypothesis that this design makes for a more effective retrieval system. My study confirmed that it was more helpful to users seeking relevant information with ill-defined tasks and vocabulary mismatches than other query systems. The study supports the hypothesis that combining techniques effectively satisfies the kind of information needs typically encountered in software design.","Software reusability,
Vocabulary,
Software systems,
Content based retrieval,
Terminology,
Software libraries,
Problem-solving,
Electronic mail,
Computer science"
"A systolic, linear-array multiplier for a class of right-shift algorithms","A very simple multiplier cell is developed for use in a linear, purely systolic array forming a digit-serial multiplier for unsigned or 2'complement operands. Each cell produces two digit-product terms and accumulates these into a previous sum of the same weight, developing the product least significant digit first. Grouping two terms per cell, the ratio of active elements to latches is low, and only upper bound [n]/2 cells are needed for a full n by n multiply. A module-multiplier is then developed by incorporating a Montgomery type of module-reduction. Two such multipliers interconnect to form a purely systolic module exponentiator, capable of performing RSA encryption at very high clock frequencies, but with a low gate count and small area. It is also shown how the multiplier, with some simple back-end connections, can compute modular inverses and perform modular division for a power of two as modulus.","Cryptography,
Application software,
Systolic arrays,
Clocks,
Frequency,
Nearest neighbor searches,
Councils,
Mathematics,
Computer science,
Digital signal processing"
A system for aiding creative concept formation,"This paper describes a system named AA1 (Articulation Aid 1) which aids human users in the formation of new concepts in the domain of engineering and science. From the viewpoint of concept formation, one main process of creation is divergent thinking in which broad alternatives are searched, and another process is convergent thinking in which a unique solution is sought. From the viewpoint of human activities, creation also includes the aspect of collaboration among people and the aspect of individual reflection, although they are interrelated. AA1, the system presented in this paper, supports divergent thinking during individual reflection. Engineers and scientists usually scrawl many notes on paper while exploring new possible concepts in the divergent thinking process. A system is needed to reflect the fragments of concepts that are not articulated yet and thereby stimulate the formation of new concepts. AA1 builds a two-dimensional space from the words the user provides. Looking at this space and other precedent spaces, the user can form new concepts little by little. The main feature of AA1 different, from existing hypermedia systems and CSCW systems is the strategy for building the space presented to the user. The system is as nonprescriptive as possible, but it gives stimulation for the user to form concepts that he could not by using only pencil and paper. Experimentation has shown that the space which AA1 displays can effectively help the user to build new concepts. The most prominent effect is that empty regions in the space automatically configured by the system often lead to new concepts.","Humans,
Automobiles,
Reflection,
Cognitive science,
Computer industry,
Automotive engineering,
Design engineering,
Collaboration,
Displays,
Artificial intelligence"
On formal requirements modeling languages: RML revisited,"Research issues related to requirements modeling are introduced and discussed through a review of the requirements modeling language RML, its peers and its successors from the time it was first proposed at the Sixth International Conference on Software Engineering (ICSE-6) to the present - ten ICSEs later. We note that the central theme of ""Capturing More World Knowledge"" in the original RML proposal is becoming increasingly important in requirements engineering. The paper highlights key ideas and research issues that have driven RML and its peers, evaluates them retrospectively in the context of experience and more recent developments, and points out significant remaining problems and directions for requirements modeling research.","Software engineering,
Proposals,
Computer science,
Knowledge engineering,
Context modeling,
Error correction,
Laboratories,
Educational institutions,
Shape,
Information analysis"
Faster checkpointing with N+1 parity,"This paper presents a way to perform fast incremental checkpointing of multicomputers and distributed systems by using N+1 parity. A basic algorithm is described that uses two extra processors for checkpointing and enables the system to tolerate any single processor failure. The algorithm's speed comes from a combination of N+1 parity, extra physical memory, and virtual memory hardware so that checkpoints need not be written to disk. This eliminates the most time-consuming portion of checkpointing. The algorithm requires each application processor to allocate a fixed amount of extra memory for checkpointing. This amount may be set statically by the programmer, and need not be equal to the site of the processor's writable address space. This alleviates a major restriction of previous checkpointing algorithms using N+1 parity. Finally, we outline how to extend our algorithm to tolerate any m processor failures with the addition of 2m extra checkpointing processors.","Checkpointing,
Writing,
Computer science,
Programming profession,
Fault tolerance,
Hardware,
Debugging,
Magnetic heads,
Nonvolatile memory,
Read-write memory"
Software requirements as negotiated win conditions,"Current processes and support systems for software requirements determination and analysis often neglect the critical needs of important classes of stakeholders, and limit themselves to the concerns of the developers, users and customers. These stakeholders can include maintainers, interfacers, testers, product line managers, and sometimes members of the general public. This paper describes the results to date in researching and prototyping a next-generation process model (NGPM) and support system (NGPSS) which directly addresses these issues. The NGPM emphasizes collaborative processes, involving all of the significant constituents with a stake in the software product. Its conceptual basis is a set of ""theory W"" (win-win) extensions to the spiral model of software development.","Collaborative software,
Spirals,
Contracts,
Programming,
Costs,
Software engineering,
Computer science,
Testing,
Software prototyping,
Prototypes"
Performance Analysis and Optimization of Schedules for Conditional and Loop-Intensive Specifications,"This paper presents a new method, based on Markov chain analysis, to evaluate the performance of schedules of behavioral specifications. The proposed performance measure is the expected number of clock cycles required by the schedule for a complete execution of the behavioral specification for any distribution of inputs. The measure considers both the repetition of operations (due to loops) and their conditional execution (due to conditional branches). We propose an efficient technique to calculate the metric. We introduce a loop-directed scheduling algorithm (LDS). The algorithm produces schedules such that the expected number of clock cycles, required by the schedule for a complete execution of the behavioral specification, is minimized. Experimental results on several conditional and loop-intensive specifications demonstrate the relevance and effectiveness of both the performancemeasure and the scheduling algorithm.","Performance analysis,
Clocks,
Scheduling algorithm,
Processor scheduling,
National electric code,
Optimization methods,
Computer science"
Closing the gap: near-optimal Steiner trees in polynomial time,"The minimum rectilinear Steiner tree (MRST) problem arises in global routing and wiring estimation, as well as in many other areas. The MRST problem is known to be NP-hard, and the best performing MRST heuristic to date is the Iterated 1-Steiner (I1S) method recently proposed by Kahng and Robins (see ibid., vol. 11, p. 893-902, 1992). In this paper, we develop a straightforward, efficient implementation of I1S, achieving a speedup factor of three orders of magnitude over previous implementations. We also give a parallel implementation that achieves near-linear speedup on multiple processors. Several performance-improving enhancements enable us to obtain Steiner trees with average cost within 0.25% of optimal, and our methods produce optimal solutions in up to 90% of the cases for typical nets. We generalize I1S and its variants to three dimensions, as well as to the case where all the pins lie on k parallel planes, which arises in, e.g., multilayer routing. Motivated by the goal of reducing the running times of our algorithms, we prove that any pointset in the Manhattan plane has a minimum spanning tree (MST) with maximum degree 4, and that in three-dimensional Manhattan space every pointset has an MST with maximum degree of 14 (the best previous upper bounds on the maximum MST degree in two and three dimensions are 6 and 26, respectively); these results are of independent theoretical interest and also settle an open problem in complexity theory.","Polynomials,
Pins,
Routing,
Wiring,
Computer science,
Cost function,
Upper bound,
Complexity theory,
Very large scale integration"
Randomized preprocessing of configuration for fast path planning,"This paper presents a new approach to path planning for robots with many degrees of freedom (DOF) operating in known static environments. The approach consists of a preprocessing and a planning stage. Preprocessing, which is done only once for a given environment, generates a network of randomly, but properly selected, collision-free configurations (nodes). Planning then connects any given initial and final configurations of the robot to two nodes of the network and computes a path through the network between these two nodes. Experiments show that after paying the preprocessing cost (on the order of hundreds of seconds), planning is extremely fast (on the order of a fraction of a second for many difficult examples involving a 10-DOF robot). The approach is particularly attractive for many-DOF robots which have to perform many successive point-to-point motions in the same environment.","Path planning,
Orbital robotics,
Robot programming,
Computer science,
Computer networks,
Costs,
Motion planning,
Workstations,
Joining processes,
Smoothing methods"
Analysis and recognition of alphanumeric handprints by parts,"In this paper, an advanced hierarchical model has been proposed to produce a more effective character recognizer based on the probability of occurrence of the patterns. New definitions such as crucial parts, efficiency ratios, degree of confusion, similar character pairs, etc. are also given to facilitate pattern analysis and character recognition. Using these definitions, computer algorithms have been developed to recognize the characters by parts, including halves, quarters, and sixths. The recognition rates have been analyzed and compared to those obtained from subjective experiments. Based on the results of both computer and human experiments, a detailed analysis of the crucial parts and the Canadian standard alphanumeric character set has been made which revealed some fundamental characteristics of these handprint models. The results should be useful to pattern analysis and recognition, character understanding, handwriting education, and human-computer communication.","Humans,
Character recognition,
Pattern recognition,
Pattern analysis,
Councils,
Partitioning algorithms,
Algorithm design and analysis,
Handwriting recognition,
Computer science education"
Near embeddings of hypercubes into Cayley graphs on the symmetric group,"Simulations of hypercube networks by certain Cayley graphs on the symmetric group are investigated. Let Q(k) be the familiar k-dimensional hypercube, and let S(n) be the star network of dimension n defined as follows. The vertices of S(n) are the elements of the symmetric group of degree n, two vertices x and y being adjacent if xo(1,i)=y for some i. That is, xy is an edge if x and y are related by a transposition involving some fixed symbol (which we take to be /spl I.bold/1). This network has nice symmetry properties, and its degree and diameter are sublogarithmic as functions of the number of vertices, making it compare favorably with the hypercube network. These advantages of S(n) motivate the study of how well it can simulate other parallel computation networks, in particular, the hypercube. The first step in such a simulation is the construction of a one-to-one map f:Q(k)/spl rarr/S(n) of dilation d, for d small. That is, one wants a map f such that images of adjacent points are at most distance d apart in S(n). An alternative approach, best applicable when one-to-one maps are difficult or impossible to find, is the construction of a one-to-many map g of dilation d, defined as follows. For each point x/spl isin/Q(k), there is an associated subset g(x)/spl sube/V(S(n)) such that for each edge xy in Q(k), every x'/spl isin/g(x) is at most distance d in S(n) from some y'/spl isin/g(y). Such one-to-many maps allow one to achieve the low interprocessor communication time desired in the usual one-to-one embedding underlying a simulation. This is done by capturing the local structure of Q(k) inside of S(n) (via the one-to-many embedding) when the global structure cannot be so captured. Our results are the following. 1) There exist the following one-to-many embeddings: a) f:Q(k)/spl rarr/S(3k+1) with dilation (f)=1; b) f:Q(11k+2)/spl rarr/S(13k+2) with dilation (f)=2. 2) There exists a one-to-one embedding f:Q(n2/sup n/spl minus/1/)/spl rarr/S(2/sup n/) with dilation (f)=3.","Hypercubes,
Computational modeling,
Mathematics,
Statistics,
Computer networks,
Concurrent computing,
Computer science,
Springs"
Synthesis of system-level bus interfaces,"Given a set of communication channels to be implemented as a single bus, the authors present a bus-generation algorithm which determines the width of a bus implementation. Tradeoffs between the width of the bus and the performance of the processes communicating over the bus are evaluated. The algorithm incorporates system level constraints such as data transfer rates and the number of pins and allows several channels that may be transferring different sizes of data to be implemented as a single bus. The authors demonstrate through a detailed example the usefulness of the algorithm in implementing system-level interfaces between modules.","Pins,
Timing,
Communication channels,
Merging,
Computer science,
Partitioning algorithms,
Control system synthesis,
Communication system control,
Costs,
Wires"
A system-design methodology: executable-specification refinement,"As methodologies and tools for chip-level design mature, design effort becomes focused on increasingly higher levels of abstraction. We present a methodology and tool for system-level specification, design and refinement that result in an executable specification for each system component. The specification for each component can then be synthesized into hardware or compiled to software. We highlight advantages of the proposed methodology compared to current practice.","Hardware,
System-level design,
Software prototyping,
Computer science,
Code standards,
Software standards,
Logic,
Protocols,
Timing,
Manuals"
A search for consensus among model parameters reported for the PUMA 560 robot,"The PUMA 560 robot is the white rat of robotics research - it has been studied and used in countless experiments over many years and in many laboratories. However, it remains a challenge to assemble the complete data needed for model-based control of the robot. This paper presents a numerical comparison of kinematic, dynamic and electrical parameters for the PUMA 560 robot which have been reported in the literature. For the first time, data from several experiments are presented in a single system of coordinates, which facilitates comparison. Differences in the data and the various methods of measurement are discussed. New data have been gathered and are presented where the record was incomplete.","Robot kinematics,
Robotic assembly,
Parameter estimation,
Servomotors,
Australia,
Computer science,
Robot control,
Visual servoing,
Pulp manufacturing,
Coordinate measuring machines"
Matching images by comparing their gradient fields,"We present a simple yet powerful method to perform point-to-point matching between two images. The method uses an evidence measure, whose value for a given displacement reflects both the similarity between two locations and the confidence in a correct match. The measure is based on the gradient fields of the images, and can be computed quickly and in parallel. Accumulating the evidence measure for different displacements allows (1) stable computation, of correspondences without smoothing across motion boundaries, and (2) detection of dominant motions. The method works well both on highly textured images and on images containing regions of uniform intensities, and can be used for a variety of applications, including stereo, motion, and object tracking.",
Volume rendering of 3D medical ultrasound data using direct feature mapping,"The authors explore the application of volume rendering in medical ultrasonic imaging. Several volume rendering methods have been developed for X-ray computed tomography (X-CT), magnetic resonance imaging (MRI) and positron emission tomography (PET). Limited research has been done on applications of volume rendering techniques in medical ultrasound imaging because of a general lack of adequate equipment for 3D acquisitions. Severe noise sources and other limitations in the imaging system make volume rendering of ultrasonic data a challenge compared to rendering of MRI and X-CT data. Rendering algorithms that rely on an initial classification of the data into different tissue categories have been developed for high quality X-CT and MR-data. So far, there is a lack of general and reliable methods for tissue classification in ultrasonic imaging. The authors focus on volume rendering methods which are not dependent on any classification into different tissue categories. Instead, features are extracted from the original 3D data-set, and projected onto the view plane. The authors found that some of these methods may give clinically useful information which is very difficult to get from ordinary 2D ultrasonic images, and in some cases renderings with very fine structural details. The authors have applied the methods to 3D ultrasound images from fetal examinations. The methods are now in use as clinical tools at the National Center of Fetal Medicine in Trondheim, Norway.","Ultrasonic imaging,
Magnetic resonance imaging,
Biomedical imaging,
X-ray imaging,
Positron emission tomography,
Optical imaging,
Computed tomography,
Magnetic noise,
Feature extraction,
Data mining"
Routing a multi-terminal critical net: Steiner tree construction in the presence of obstacles,"This paper presents a new model for VLSI routing in the presence of obstacles, that transforms any routing instance from a geometric problem into a graph problem. It is the first model that allows computation of optimal obstacle-avoiding rectilinear Steiner trees in time corresponding to the instance size (the number of terminals and obstacle border segments) rather than the size of the routing area. For the most common multi-terminal critical nets-those with three or four terminals-we observe that optimal trees can be computed as efficiently as good heuristic trees, and present algorithms that do so. For nets with five or more terminals, we present algorithms that heuristically compute obstacle-avoiding Steiner trees. Analysis and experiments demonstrate that the model and algorithms work well in both theory and practice.","Routing,
Very large scale integration,
Algorithm design and analysis,
Wires,
Polynomials,
Computer science,
Solid modeling,
Computational modeling,
Design automation,
Logic"
The relationship between test coverage and reliability,"Models the relationship between testing effort, coverage and reliability, and presents a logarithmic model that relates testing effort to test coverage: statement (or block) coverage, branch (or decision) coverage, computation use (c-use) coverage, or predicate use (p-use) coverage. The model is based on the hypothesis that the enumerables (like branches or blocks) for any coverage measure have different detectability, just like the individual defects. This model allows us to relate a test coverage measure directly to the defect coverage. Data sets for programs with real defects are used to validate the model. The results are consistent with the known inclusion relationships among block, branch and p-use coverage measures. We show how the defect density controls the time-to-next-failure. The model can eliminate variables like the test application strategy from consideration. It is suitable for high-reliability applications where automatic (or manual) test generation is used to cover enemerables which have not yet been tested.",
Queue locks on cache coherent multiprocessors,"Large-scale shared-memory multiprocessors typically have long latencies for remote data accesses. A key issue for execution performance of many common applications is the synchronization cost. The communication scalability of synchronization has been improved by the introduction of queue-based spin-locks instead of Test&(Test&Set). For architectures with long access latencies for global data, attention should also be paid to the number of global accesses that are involved in synchronization. We present a method to characterize the performance of proposed queue lock algorithms, and apply it to previously published algorithms. We also present two new queue locks, the LH lock and the M lock. We compare the locks in terms of performance, memory requirements, code size and required hardware support. The LH lock is the simplest of all the locks, yet requires only an atomic swap operation. The M lock is superior in terms of global accesses needed to perform synchronization and still competitive in all other criteria. We conclude that the M lock is the best overall queue lock for the class of architectures studied.","Hardware,
Costs,
Scalability,
Queueing analysis,
Algorithm design and analysis,
Delay,
Testing,
Silicon carbide,
Performance analysis,
Computer science"
Transformer inrush calculations using a coupled electromagnetic model,"The paper describes a method, using time domain analysis, in which the equations of mixed electrical and magnetic systems are solved not as separate but as a single system. The advantages of this method, from the viewpoint of a power system designer, are discussed and the inrush current is calculated for typical 5 kVA and 180 MVA transformers. From a transformer designer's viewpoint the distribution of fluxes within the transformer becomes more apparent so that the analysis of inrush and its implications can be carried out at the design stage. It is also shown how different parts of the transformer can be saturated to differing levels and how this can be incorporated into the analysis.","Design automation,
Transformers"
Rapid rumor ramification: approximating the minimum broadcast time,"Given an undirected graph representing a network of processors, and a source node containing a message that must be broadcast to all the nodes, find a scheme that accomplishes the broadcast in the minimum number of time steps. At each time step, any processor that has received the message is allowed to communicate the message to at most one of its neighbors in the network, i.e. can communicate via a telephone call to a neighbor. This has been termed the minimum broadcast time problem under the telephone model and is known to be NP-complete. The minimum broadcast time in a graph is closely related to the poise of the graph. The poise of a tree is defined to be the quantity (maximum degree of any node in the tree+diameter of the tree). The poise of a graph is the minimum poise of any of its spanning trees. Computing the poise of a graph is shown to be NP-hard and an algorithm for computing a spanning tree of approximately minimum poise is derived. This algorithm is then used to derive an O(log/sup 2/n/log log n)-approximation for the minimum broadcast time problem on an n-node graph. Our algorithm extends to many generalizations of the problem such as the multicast problem, a telephone model allowing conference calls, and to the closely related minimum gossip time problem.",
"Artificial Fishes: Autonomous Locomotion, Perception, Behavior, and Learning in a Simulated Physical World","This article develops artificial life patterned after animals as evolved as those in the superclass Pisces. It demonstrates a virtual marine world inhabited by realistic artificial fishes. Our algorithms emulate not only the appearance, movement, and behavior of individual animals, but also the complex group behaviors evident in many aquatic ecosystems. We model each animal holistically. An artificial fish is an autonomous agent situated in a simulated physical world. The agent has (a) a three-dimensional body with internal muscle actuators and functional fins that deforms and locomotes in accordance with biomechanic and hydrodynamic principles; (b) sensors, including eyes that can image the environment; and (c) a brain with motor, perception, behavior, and learning centers. Artificial fishes exhibit a repertoire of piscatorial behaviors that rely on their perceptual awareness of their dynamic habitat. Individual and emergent collective behaviors include caudal and pectoral locomotion, collision avoidance, foraging, preying, schooling, and mating. Furthermore, artificial fishes can learn how to locomote through practice and sensory reinforcement. Their motor learning algorithms discover muscle controllers that produce efficient hydrodynamic locomotion. The learning algorithms also enable artificial fishes to train themselves to accomplish higher level, perceptually guided motor tasks, such as maneuvering to reach a visible target.","computer graphics,
artificial life,
autonomous agents,
animats,
artificial fishes,
learning,
behavior,
perception,
locomotion,
physics-based modeling"
The Chaos router,"The Chaos router, a randomizing, nonminimal adaptive packet router is introduced. Adaptive routers allow messages to dynamically select paths, depending on network traffic, and bypass congested nodes. This flexibility contrasts with oblivious packet routers where the path of a packet is statically determined at the source node. A key advancement of the Chaos router over previous nonminimal routers is the use of randomization to eliminate the need for livelock protection. This simplifies adaptive routing to be of approximately the same complexity along the critical decision path as an oblivious router. The primary cost is that the Chaos router is probabilistically livelock free rather than being deterministically livelock free, but evidence is presented implying that these are equivalent in practice. The principal advantage is excellent performance for nonuniform traffic patterns. The Chaos router is described, it is shown to be deadlock free and probabilistically livelock free, and performance results are presented for a variety of work loads.","Chaos,
Routing,
Chaotic communication,
System recovery,
Telecommunication traffic,
Computer science,
Protection,
Costs,
Adaptive systems,
Communication networks"
Face locating and tracking for human-computer interaction,"Effective human-to-human communication involves both auditory and visual modalities, providing robustness and naturalness in realistic communication situations. Recent efforts at our lab are aimed at providing such multimodal capabilities for human-machine communication. Most of the visual modalities require a stable image of a speaker's face. We propose a connectionist face tracker that manipulates camera orientation and room, to keep a person's face located at all times. The system operates in real time and can adapt rapidly to different lighting conditions, cameras and faces, making it robust against environmental variability. Extensions and integration of the system with a multimodal interface are presented.","Face detection,
Cameras,
Object detection,
Humans,
Speech synthesis,
Lenses,
Shape,
Artificial neural networks,
Computer science,
Robustness"
Optimum classification in subband coding of images,"This paper investigates the classification technique, applied to subband coding of images, as a way of exploiting the non-stationary nature of image subbands. An algorithm for maximizing the classification gain, is presented. Each subband is optimally classified and the classification map is sent as side information. After optimum rate allocation, the classes are encoded using arithmetic and trellis coded quantization (ACTCQ) system. We compare this approach with other approaches for classification proposed in the literature. We propose a method for reducing the side rate which exploits the dependence between subbands as well as the within band dependence.","Image coding,
Encoding,
Discrete cosine transforms,
Quantization,
Nonlinear distortion,
Computer science,
Arithmetic,
Filter bank,
Wavelet packets,
Entropy coding"
Universal coding for the Slepian-Wolf data compression system and the strong converse theorem,"Universal coding for the Slepian-Wolf (1973) data compression system is considered. We shall demonstrate based on a simple observation that the error exponent given by Csiszar and Korner (1980) for the universal coding system can strictly be sharpened in general for a region of relatively higher rates. This kind of observation can be carried over also to the case of lower rates outside the Slepian-Wolf region, which establishes the strong converse along with the optimal exponent.","Data compression,
Decoding,
Statistics,
Information theory,
Tellurium,
Sun,
Stability,
Conferences,
Computer science"
Comprehension processes during large scale maintenance,We present results of observing professional maintenance engineers working with industrial code at actual maintenance tasks. Protocol analysis is used to explore how code understanding might differ for small versus large scale code. The experiment confirms that cognition processes work at all levels of abstraction simultaneously as programmers build a mental model of the code. Cognition processes emerged at three levels of aggregation representing lower and higher level strategies of understanding. They show differences in what triggers them and how they achieve their goals. Results are useful for defining core competencies which maintenance engineers need for their work and for documentation and development standards.,"Large-scale systems,
Cognition,
Protocols,
Software maintenance,
Computer science,
Programming profession,
Switches,
Guidelines,
Cognitive science,
Code standards"
Acyclic Multi-Way Partitioning of Boolean Networks,"Acyclic partitioning on combinational boolean networks has wide range of applications, from multiple FPGA chip partitioning to parallel circuit simulation. In this paper, we present two efficient algorithms for the acyclic multi-way partitioning. One is a generalized Fm-based algorithm. The other is based on the theory of maximum fanout-free cone (MFFC) decomposition. The acyclic FM-algorithm usually results in larger cut-size, as expected, compared to the undirected FM-algorithm due to the acyclic constraint. To our surprise, however, the MFFC-based acyclic partitioning algorithm consistently produces smaller (50% on average) cut-sized solutions than the conventional FM-algorithm. This result suggests that considering signal directions during the process can lead to very natural circuit decomposition and clustering, which in turn results in better partitioning solutions. We have also implemented parallel gate level simulators in Maisie and applied our partitioning algorithms to evaluate their impact on circuit simulation.","Partitioning algorithms,
Circuit simulation,
Clustering algorithms,
Signal processing,
Logic circuits,
Field programmable gate arrays,
Very large scale integration,
Iterative algorithms,
Pipeline processing,
Logic design"
"Parameter-free, predictive modeling of single event upsets due to protons, neutrons, and pions in terrestrial cosmic rays","In this paper we present a new approach and a computer software for modeling single event upsets. This model, named Soft Error Monte Carlo Model (SEMM), does not need any experimental inputs or any parameter fitting. It is intended to be a design tool for chip designers when they want to optimize their designs for soft error hardness and performance. The paper focuses on terrestrial cosmic rays that cause single event upsets. Details of the nuclear modeling and of the coupled device-circuit modeling are presented. Also presented are the comparison of SEMM predictions against measurements of single event upset rate in proton beam experiments and in computer main frame field tests performed at high ground elevations. We also present some proton-pion comparisons that are relevant to single event upsets.","Predictive models,
Single event upset,
Computer errors,
Software,
Monte Carlo methods,
Design optimization,
Cosmic rays,
Semiconductor device measurement,
Particle beams,
High performance computing"
"The thirsty traveler visits Gamont: a rejoinder to ""Comments on fuzzy sets/spl minus/what are they and why?""","Replying to the comments of W.H. Woodall and R.E. Davis (ibid., p.43) on the author's editorial (ibid., p.1), the authors present two illustrations to show that they do not have a poor opinion of probability but that they believe fuzzy membership to be more useful in some cases.","Fuzzy sets,
Liquids,
Switches,
Probability,
Optimized production technology,
Frequency,
Computer science,
Fuzzy logic,
Uncertainty,
Cities and towns"
Design of efficient balanced codes,"All words in a balanced code have equal number of ones and zeros. Denote by DC(n,k) a balanced (or dc-free) code of length n, and 2/sup k/ code words. We design an efficient DC(k+r, k) code with k=2/sup r+1//spl minus/0.8/spl radic/(r/spl minus/2). These codes are optimal up to the construction method, introduced by D.E. Knuth (1986).","Decoding,
Optical recording,
Optical fiber cables,
Computer science,
Optical fibers,
Communication cables,
Disk recording,
Digital magnetic recording,
Optical pulses,
Error correction"
Microarchitectural Synthesis of VLSI Designs with High Test Concurrency,"The testability of a VLSI design is strongly affected by its register-transfer level (RTL) structure. Since the high-level synthesis process determines the RTL structure, it is necessary to consider testability during high-level synthesis. A synthesis system composed of scheduling and binding components minimizes the number of hardware sharing conicts between tests in the test schedule. Novel test conict estimates are used to direct the synthesis process. The test conflict estimation is based on examination of the interconnect structure of the partial design state during synthesis. Test conict estimates enable our synthesis system to select design options which increase test concurrency, thereby decreasing test time. Experimental results show that designs generated by this approach are testable in a highly concurrent manner.","Microarchitecture,
Very large scale integration,
Concurrent computing,
Hardware,
Automatic testing,
Built-in self-test,
Sequential analysis,
Costs,
Production,
System testing"
A general procedure for the derivation of principal domains of higher-order spectra,"A general procedure to determine the principal domain (i.e., nonredundant region of computation) of any higher-order spectrum is presented, using the bispectrum as an example. The procedure is then applied to derive the principal domain of the trispectrum of a real-valued, stationary time series. These results are easily extended to compute the principal domains of other higher-order spectra.","Random processes,
Frequency,
Fourier transforms,
Gaussian processes,
Computer science,
Signal processing,
Systems engineering and theory,
Australia,
Signal reconstruction,
System identification"
Structuring distributed algorithms for mobile hosts,"Distributed algorithms have hitherto been designed for networks with static hosts. A mobile host (MH) can connect to the network from different locations at different times. This paper presents an operational system model for explicitly incorporating the effects of host mobility and proposes a general principle for structuring efficient distributed algorithms in this model. This principle is used to redesign two classical algorithms for distributed mutual exclusion for the mobile environment. We then consider a problem introduced solely by host mobility viz, location management for groups of MHs, and propose the concept of group location as an efficient approach to tackle the problem. Lastly, we present a framework which enables host mobility to be decoupled from the design of a distributed algorithm per se, to varying degrees.","Distributed algorithms,
Mobile computing,
Algorithm design and analysis,
Mobile communication,
Distributed computing,
Computer networks,
Portable computers,
Computer science,
Electrical capacitance tomography,
Protocols"
MDP routing in ATM networks using virtual path concept,"The virtual path (VP) concept has been proposed to simplify traffic control and resource management in future B-ISDN. In particular, call setup processing can be significantly reduced when resources are reserved on VPs. However, this advantage is offset by a decrease in statistical multiplexing gains of the networks. The focus of this paper is on how to improve bandwidth efficiency through adaptive routing when capacity is reserved on all VPs. The authors first examine two VP capacity reservation strategies. They then design and evaluate computationally feasible Markov decision process-based routing algorithms and show that the network blocking probability can be significantly reduced by MDP routing.","Routing,
Intelligent networks,
Bandwidth,
Resource management,
Asynchronous transfer mode,
Computer science,
Traffic control,
Communication system traffic control,
Contracts,
Costs"
High-order spectral-null codes-constructions and bounds,"Let /spl Sscr/(n.k) denote the set of all words of length n over the alphabet {+1,-1}, having a k th order spectral-null at zero frequency. A subset of /spl Sscr/(n,k) is a spectral-null code of length n and order k. Upper and lower bounds on the cardinality of /spl Sscr/(n,k) are derived. In particular we prove that (k-1) log/sub 2/ (n/k)/spl les/n-log/sub 2/|/spl Sscr/(n,k)|/spl les/O(2/sup k/log/sub 2/n) for infinitely many values of n. On the other hand, we show that /spl Sscr/(n.k) is empty unless n is divisible by 2/sup m/, where m=[log/sub 2/k]+1. Furthermore, bounds on the minimum Hamming distance d of /spl Sscr/(n,k) are provided, showing that 2k/spl les/d/spl les/k(k-1)+2 for infinitely many n. We also investigate the minimum number of sign changes in a word x/spl isin//spl Sscr/(n,k) and provide an equivalent definition of /spl Sscr/(n,k) in terms of the positions of these sign changes. An efficient algorithm for encoding arbitrary information sequences into a second-order spectral-null code of redundancy 3 log/sub 2/n+O(log log n) is presented. Furthermore, we prove that the first nonzero moment of any word in /spl Sscr/(n,k) is divisible by k!. This leads to an encoding scheme for spectral-null codes of length n and any fixed order k, with rate approaching unity as n/spl rarr//spl infin/.","Hamming distance,
Frequency,
Discrete Fourier transforms,
Computer science,
Fourier transforms"
Simplified understanding and efficient decoding of a class of algebraic-geometric codes,"An efficient decoding algorithm for algebraic-geometric codes is presented. For codes from a large class of irreducible plane curves, including Hermitian curves, it can correct up to [(d*-1)/2] errors, where d* is the designed minimum distance. With it we also obtain a proof of d/sub min//spl ges/d* without directly using the Riemann-Roch theorem. The algorithm consists of Gaussian elimination on a specially arranged syndrome matrix, followed by a novel majority voting scheme. A fast implementation incorporating block Hankel matrix techniques is obtained whose worst-case running time is O(mn/sup 2/), where m is the degree of the curve. Applications of our techniques to decoding other algebraic-geometric codes, to decoding BCH codes to actual minimum distance, and to two-dimensional shift register synthesis are also presented.","Decoding,
Error correction codes,
Algorithm design and analysis,
Reed-Solomon codes,
Voting,
Shift registers,
Computer science,
Computer errors"
Solving small and large scale constraint satisfaction problems using a heuristic-based microgenetic algorithm,"Microgenetic algorithms (MGAs) are genetic algorithms that use a very small population size (population size < 10). Recently, interest in MGAs has grown because, for some problems, they are able to find solutions with fewer evaluations than genetic algorithms with larger population sizes. This paper introduces two heuristic-based MGAs which quickly find solutions to constraint satisfaction problems. Both of these algorithms outperform a well-known algorithm, the iterative descent method, on most instances of the N-queens problem. We compare these three algorithm on the basis of the mean number of evaluations needed to find solutions to several instances of the N-queens problem.","Large-scale systems,
Genetic algorithms,
Iterative algorithms,
Heuristic algorithms,
Computer science,
Convergence,
Iterative methods,
Guidelines,
Genetic mutations,
Parallel processing"
A reduced order observer based controller design for H/sub /spl infin//-optimization,"In this note the H/sub /spl infin// control problem with measurement feedback is investigated. It is well-known that for this problem, in general, we need controllers of the same dynamic order as the given system. However, in the case that some entries of the measurement vector are not noise-corrupted, we show that one can find dynamic compensators of a lower dynamical order. Note that this implies that the standard assumptions on the direct feedthrough matrices, as made in most papers on H/sub /spl infin// control, are not satisfied. Our result can be derived by using reduced order observer based controllers.","Control systems,
Time domain analysis,
Riccati equations,
State feedback,
Noise measurement,
Art,
NASA,
Mathematics,
Space technology,
Computer science"
Efficient local search with conflict minimization: a case study of the n-queens problem,"Backtracking search is frequently applied to solve a constraint-based search problem, but it often suffers from exponential growth of computing time. We present an alternative to backtracking search: local search with conflict minimization. We have applied this general search framework to study a benchmark constraint-based search problem, the n-queens problem. An efficient local search algorithm for the n-queens problem was implemented. This algorithm, running in linear time, does not backtrack. It is capable of finding a solution for extremely large size n-queens problems. For example, on a workstation it can find a solution for 3000000 queens in less than 55 s.","Computer aided software engineering,
Search problems,
Optical computing,
Concurrent computing,
Application software,
Very large scale integration,
Workstations,
Councils,
Scholarships,
Computer science"
On weighted expectations,The classical 'expected value' frequently fails to represent a 'typical value' of a given multi-cluster data set. In this case the set is clustered and new quantities which replace the expected value and the standard deviation are defined. The 'weighted expected value' (WEV) is computed by solving a nonlinear equation and defines a 'most typical value' of the set. The 'weighted deviation' (WD) is then defined explicitly and provides a measurement to the 'grade of typicality' of the WEV.,"Terminology,
Computer science,
Physics,
Nonlinear equations,
Statistics,
Arithmetic,
Measurement standards,
Information retrieval"
On ternary complementary sequences,"A pair of real-valued sequences A=(a/sub 1/,a/sub 2/,...,a/sub N/) and B=(b/sub 1/,b/sub 2/,...,b/sub N/) is called complementary if the sum R(/spl middot/) of their autocorrelation functions R/sub A/(/spl middot/) and R/sub B/(/spl middot/) satisfies R(/spl tau/)=R/sub A/(/spl tau/)+R/sub B/(/spl tau/)=/spl Sigma//sub i=1//sup N$/ -/sup /spl tau//a/sub i/a/sub i+/spl tau//+/spl Sigma//sub j=1//sup N-/spl tau//b/sub j/b/sub j+/spl tau//=0, /spl forall//spl tau//spl ne/0. In this paper we introduce a new family of complementary pairs of sequences over the alphabet /spl alpha//sub 3/=+{1,-1,0}. The inclusion of zero in the alphabet, which may correspond to a pause in transmission, leads both to a better understanding of the conventional binary case, where the alphabet is /spl alpha//sub 2/={+1,-1}, and to new nontrivial constructions over the ternary alphabet /spl alpha//sub 3/. For every length N, we derive restrictions on the location of the zero elements and on the form of the member sequences of the pair. We also derive a bound on the minimum number of zeros necessary for the existence of a complementary pair of length N over /spl alpha//sub 3/. The bound is tight, as it is met by some of the proposed constructions, for infinitely many lengths.",Autocorrelation
Connection management and rerouting in ATM networks,"In ATM networks, two levels of connections are defined: virtual path connections and virtual channel connections. Virtual paths are the building blocks of the virtual channels. This hierarchy, known as ""the virtual path concept"", provides considerable advantages in the design of high-speed networks. In the paper the authors show that this concept has the advantage of providing a means of improving the survivability of virtual channels. To this end, they present a simple rerouting protocol, which can be invoked upon the failure of an intermediate link or node of a virtual path. This protocol reroutes all the affected virtual channels to an alternative virtual path. Due to the simplicity of the protocol, it can be completed rapidly, thus fulfilling an essential requirement of any rerouting scheme.","Intelligent networks,
Virtual colonoscopy,
Protocols,
Computer network management,
Routing,
Computer science,
Multiprotocol label switching,
Network interfaces,
Circuits,
Channel allocation"
A new routing algorithm for a class of rearrangeable networks,"This paper presents a routing algorithm for a class of multistage interconnection networks. Specifically, the concatenation of two Omega networks which has 2 log/sub 2/ N stages is treated. It is shown that this kind of asymmetric Omega+Omega network can be converted into a symmetric Omega/sup -1spl times/Omega network or a symmetric Omega/spl times/Omega/sup -1/ network. However, they have butterfly connections between the two center stages. A general algorithm is developed which routes a class of symmetric networks. The algorithm routes the network from center stages to outer stages at both the input and the output sides simultaneously. The algorithm presented is simpler and more flexible than the well-known looping algorithm in that it can be applied adaptively according to the structure of the network. It can be applied to routing the Omega-based networks regardless of the center-stage connection patterns, i.e., straight, skewed straight, simple butterfly or skewed butterfly as long as the networks are symmetric. The sufficient conditions for proper routing are shown and proved. In addition, an example is shown to demonstrate the algorithm.","Routing,
Multiprocessor interconnection networks,
Sufficient conditions,
Very large scale integration,
Parallel processing,
Communication switching,
Computer science,
Laboratories,
Genetic mutations"
Efficient fully adaptive wormhole routing in n-dimensional meshes,"An efficient fully adaptive wormhole routing algorithm for n-dimensional meshes is developed. The routing algorithm provides full adaptivity at a cost of one additional virtual channel per physical channel irrespective of the number of dimensions of the network. The algorithm is based on dividing the network graph into two acyclic graphs that contain all of the physical channels in the system. Virtual channels are classified as either waiting or nonwaiting channels. Busy channels that a message waits for to become available are classified as waiting channels, otherwise they are classified as nonwaiting channels. Thus, a message considers nonwaiting channels first to reach its destination. If all non-waiting channels are busy, the message considers waiting channels. Messages acquire waiting channels in two phases. In each phase, waiting channels belonging to one acyclic network graph are traversed. This 2-phase routing algorithm could be either minimal or nonminimal. However, we concentrate on minimal routing. It is demonstrated that this adaptive routing algorithm can utilize the virtual paths (channels) between any two nodes more efficiently than any of the present algorithms with the same hardware requirement.","Routing,
Hardware,
Network topology,
Algorithm design and analysis,
Hypercubes,
Computer science,
Costs,
Availability,
Throughput,
Communication networks"
Interactive visualization of Earth and space science computations,Scientists often view computer algorithms as risk-filled black boxes. The barrier between scientists and their computations can be bridged by techniques that make the internal workings of algorithms visible and that allow scientists to experiment with their computations. We describe two interactive systems developed at the University of Wisconsin-Madison Space Science and Engineering Center (SSEC) that provide these capabilities to Earth and space scientists. These visualization packages help scientists see the internal workings of their algorithms and thus understand their computations.,"Visualization,
Geoscience,
Atmospheric modeling,
Animation,
Computational modeling,
Weather forecasting,
Earth,
Oceans,
Workstations,
Computer simulation"
Compressionless Routing: a framework for adaptive and fault-tolerant routing,"Compressionless Routing (CR) is a new adaptive routing framework which provides a unified framework for efficient deadlock-free adaptive routing and fault-tolerance. CR exploits the tight-coupling between wormhole routers for flow control to detect potential deadlock situations and recover from them. Fault-tolerant Compressionless Routing (FCR) extends Compressionless Routing to support end-to-end fault-tolerant delivery. Detailed routing algorithms, implementation complexity and performance simulation results for CR and FCR are presented. CR has the following advantages: deadlock-free adaptive routing in torus networks with no virtual channels, simple router designs, order-preserving message transmission, applicability to a wide variety of network topologies, and elimination of the need for buffer allocation messages. FCR has the following advantages: tolerates transient faults while maintaining data integrity (nonstop fault-tolerance), tolerates permanent faults, can be applied to a wide variety of network topologies, and eliminates the need for software buffering and retry for reliability. These advantages of CR and FCR not only simplify hardware support for adaptive routing and fault-tolerance, they also can simplify communication software layers.","Routing,
Fault tolerance,
System recovery,
Chromium,
Network topology,
Hardware,
Delay,
Network interfaces,
Computer science,
Algorithm design and analysis"
Parting of water by magnetic fields,"We observed the phenomenon that water was parted by a magnetic field at 8 T. Two ""water-walls"" were formed and the surface of the water was parted. The bottom of the water chamber appeared when the water chamber was exposed to magnetic fields up to 8 T. This paper focuses on the mechanism of this phenomenon. The change in the lowest level of water was 22 mm when magnetic fields mere changed from 0 T to 8 T, and the decrease in water level was proportionate to B/sup 2/ at the center of the magnet. We also investigated the effect of temperature on the formation of the water-wall. We observed that the water-wall was formed at both 0/spl deg/C and 100/spl deg/C. Finally, we investigated the effect of mixed NaCl ions on the formation of the water-wall. The absolute change in water level declined as the concentration of NaCl rose.","Magnetic fields,
Superconducting magnets,
Magnetic liquids,
Magnetic levitation,
Boring,
Biomedical engineering,
Temperature,
Computer science,
Hydrodynamics,
Stress"
Detecting misrecognitions and out-of-vocabulary words,"This paper describes and evaluates a new technique for evaluating confidence in word strings produced by a speech recognition system. It detects misrecognized and out-of-vocabulary words in spontaneous spoken dialogs. The system uses multiple, diverse knowledge sources including acoustics, semantics, pragmatics and discourse to determine if a word string is misrecognized. When likely misrecognitions are detected, a series of tests distinguishes out-of-vocabulary words from other error sources. The work is part of a larger effort to automatically recognize and understand new words when spoken in a spontaneous spoken dialog. The newly developed acoustic confidence metrics output independent probabilites that a word is recognized correctly and a measure of how reliably we can tell if it is wrong. At p <.05, the acoustic methods detect 65% of the errors. The semantic/discourse module detects 98% of the errors that are semantically or contextually inappropriate, but cannot detect contextually consistent misrecognitions. Hence, we merged these two methods and ran them on a single test set to see whether the semantic/pragmatic/discourse component detected input not reliable rejected acoustically and to see how many of the semantically consistent errors could be detected with acoustic normalization methods.","Acoustic signal detection,
Acoustic measurements,
Acoustic testing,
Hidden Markov models,
Computer science,
System testing,
Error correction,
Radio access networks,
Vocabulary,
Merging"
Fault-tolerant scheduling on a hard real-time multiprocessor system,"Fault-tolerance is an important issue in hard real-time systems due to the critical nature of the supported tasks. One way of providing fault-tolerance is to schedule multiple copies of a task on different processors. If the primary copy of a task cannot be completed due to a fault, the scheduled backup copy is run and the task is completed. In this paper, we propose a new algorithm for fault-tolerant scheduling on multiprocessor systems. The algorithm guarantees the completion of a scheduled task before its deadline in the presence of processor failures. Our algorithm schedules several backup tasks overlapping one another and dynamically deallocates the backups as soon as the original tasks complete executions, thus increasing the utilization of processors. Simulation results show that our method achieves higher task schedulability compared to using a spare processor as a backup to be invoked in the event of a failure. Further, we show that the cost, in terms of schedulability, of guaranteeing fault tolerance for dynamic systems is quite low.","Fault tolerant systems,
Real time systems,
Multiprocessing systems,
Processor scheduling,
Optimal scheduling,
Scheduling algorithm,
Fault tolerance,
Timing,
Radar tracking,
Computer science"
Adaptive location policies for global scheduling,"Two important components of a global scheduling algorithm are its transfer policy and its location policy. While the transfer policy determines whether a task should be transferred, the location policy determines where it should be transferred. Based on their location policies, global scheduling algorithms can be broadly classified as receiver-initiated, sender-initiated, or symmetrically-initiated. The relative performance of these classes of algorithms has been shown to depend on the system workload. We present two adaptive location policies for global scheduling in distributed systems. These location policies are general, and can be used in conjunction with many existing transfer policies. By adapting to the system workload, the proposed policies capture the advantages of both sender-initiated and receiver-initiated policies. In addition, by adaptively directing their search activities toward the nodes that are most likely to be suitable counterparts in task transfers, the proposed policies provide short transfer latency and low overhead, and more important, high probability of finding a suitable counterpart if one exists. These properties allow these policies to deliver good performance over a very wide range of system operating conditions. The proposed policies are compared with nonadaptive policies, and are shown to considerably improve performance and to avoid causing system instability.","Scheduling algorithm,
Processor scheduling,
Distributed computing,
Information science,
High performance computing,
Delay,
Adaptive scheduling,
Adaptive algorithm,
Performance analysis"
A theory of manipulation and control for microfabricated actuator arrays,"This paper investigates manipulation tasks with arrays of microelectromechanical structures (MEMS). We develop a model for the mechanics of microactuators and a theory of sensorless, parallel manipulation, and we describe efficient algorithms for their evaluation. The theory of limit surfaces offers a purely geometric characterization of micro-scale contacts between actuator and moving object, which can be used to efficiently predict the motion of the object on an actuator array. We develop a theory of sensorless manipulation with microactuator arrays. It is shown how simple actuator control strategies can be used to uniquely align a part up to symmetry. These manipulation strategies can be computed efficiently and do not require sensor feedback. This theory is applicable to a wide range of microactuator arrays. Our actuators are oscillating structures of single-crystal silicon fabricated in a low-temperature SCREAM process. They exhibit high aspect ratios and high vertical stiffness, which is of great advantage for an effective implementation of our theory. Calculations show that arrays of these actuators can generate forces that are strong enough to levitate and move e.g. a piece of paper.","Actuators,
Microactuators,
Sensor arrays,
Fabrication,
Robot sensing systems,
Micromechanical devices,
Robot vision systems,
Nanofabrication,
Sensorless control,
Computer science"
Lower bounds for the complexity of reliable Boolean circuits with noisy gates,"Proves that the reliable computation of any Boolean function with sensitivity s requires /spl Omega/(s log s) gates if the gates fail independently with a fixed positive probability. This theorem was stated by Dobrushin and Ortyukov (1977), but their proof was found by Pippenger, Stamoulis, and Tsitsiklis (1991) to contain some errors.","Circuits,
Boolean functions"
Application of the measured equation of invariance to solve scattering problems involving a penetrable medium,"The measured equation of invariance (MEI) is a simple technique used to derive finite difference type local equations at mesh boundaries, where the conventional finite difference approach fails (Mei et al., 1992, 1994). Conventionally, finite difference or finite element meshes span from boundary to boundary, or to any surface where an absorbing boundary condition can be simulated. It is demonstrated that the MEI technique can be used to terminate meshes very close to the object boundary and still strictly preserves the sparsity of the finite difference equations. It results in dramatic savings in computing time and memory needs. In an earlier paper (Mei et al., 1992) it was shown that this new method can be applied to general boundary geometries including both convex and concave metal surfaces. In this paper we shall show that the MEI can be applied also to material discontinuities. The method of MEI is based on the postulate that the boundary equations, which govern the scattered fields, are independent of the incident fields. That postulate has been shown to be true for metal scatterers. In a penetrable medium the situation is quite different; in fact, inside the medium the separation of the incident and scattered fields is not a simple matter. Normally, the total field has to be calculated inside the medium, thus violating the postulate. Therefore we cannot use the MEI inside a penetrable medium. The space inside the medium has to be filled by finite difference or finite element meshes. This paper shows how to apply the MEI in this situation and concludes that the CPU time required for the MEI method is comparable to that of the method of moments (MOM), when MOM is based on the boundary integral method, i.e., if the scatterer is homogeneous. However, for inhomogeneous penetrable scatterer the MEI method definitely is preferable to the MOM.","Finite difference methods,
Mathematical model,
Method of moments,
Finite element analysis,
Scattering,
Dielectrics,
Optical surface waves"
Priority encoding transmission,"We introduce a novel approach for sending messages over lossy packet-based networks. The new method, called Priority Encoding Transmission, allows a user to specify a different priority on each segment of the message. Based on the priorities, the sender uses the system to encode the segments into packets for transmission. The system ensures recovery of the segments in order of their priority. The priority of a segment determines the minimum number of packets sufficient to recover the segment. We define a measure for a set of priorities, called the rate, which dictates how much information about the message must be contained in each bit of the encoding. We develop systems for implementing any set of priorities with rate equal to one. We also give an information-theoretic proof that there is no system that implements a set of priorities with rate greater than one. This work has applications to multi-media and high speed networks applications, especially in those with bursty sources and multiple receivers with heterogeneous capabilities.","Encoding,
Computer science,
Application software,
Decoding,
High-speed networks,
Error correction,
Buffer overflow,
Bandwidth,
Propagation losses,
Positron emission tomography"
The MMST computer-integrated manufacturing system framework,"Computer-integrated manufacturing (CIM) is the harmonious connection, integration, and interoperation of automation equipment within a manufacturing facility. In a semiconductor wafer fab, this includes integration of the processing equipment with all of the supporting systems for product and process specification, production planning and scheduling, and material handling and tracking. Traditionally, CIM systems have been characterized as monolithic mainframe-based systems and/or inflexible islands of automation with limited interoperability. Today's manufacturing demands fully integrated dynamic systems which directly support the concepts of lean, flexible and agile manufacturing to high quality standards. These requirements drove the design of a new CIM system which was developed for the Microelectronics Manufacturing Science and Technology (MMST) program. This paper provides an overview of the MMST CIM system framework which is based on open distributed system and object technologies. The CIM system was demonstrated in a 1000 wafer pilot production run in 1993 which achieved world record cycle time, and is now being commercialized as part of the WORKS product family from Texas Instruments.","Computer integrated manufacturing,
Manufacturing automation,
Production facilities,
Production planning,
Job shop scheduling,
Processor scheduling,
Materials handling,
Flexible manufacturing systems,
Agile manufacturing,
Microelectronics"
Field programmable gate arrays and floating point arithmetic,"We present empirical results describing the implementation of an IEEE Standard 754 compliant floating-point adder/multiplier using field programmable gate arrays. The use of FPGA's permits fast and accurate quantitative evaluation of a variety of circuit design tradeoffs for addition and multiplication. PPGA's also permit accurate assessments of the area and time costs associated with various features of the IEEE floating-point standard, including rounding and gradual underflow. These costs are analyzed, along with the effects of architectural correlation, a phenomenon that occurs when the cost of combining architectural features exceeds the sum of separate implementation. We conclude with an assessment of the strengths and weaknesses of using FPGA's for floating-point arithmetic.","Field programmable gate arrays,
Floating-point arithmetic,
Costs,
Adders,
Signal design,
Circuit synthesis,
Space technology,
Logic,
Sun,
Computer science"
Adaptive load migration systems for PVM,"Adaptive load distribution is necessary for parallel applications to co-exist effectively with other jobs in a network of shared, heterogeneous workstations. We present three methods that provide such support for PVM applications. Two of these methods, MPVM (migratable PVM) and UPVM (user-level PVM), adapt to changes in the workstation environment by transparently migrating the virtual processors (VPs) of the parallel application. A VP in MPVM is a Unix process, while UPVM defines lightweight process-like VPs. The third method, ADM (adaptive data movement), is a programming methodology for writing programs that perform adaptive load distribution through data movement. These methods are discussed and compared in terms of effectiveness, usability and performance.","Adaptive systems,
Workstations,
Computer networks,
Concurrent computing,
Computer science,
Application software,
Writing,
Usability,
Programming profession,
Distributed computing"
The geometry of graphs and some of its algorithmic applications,"We explore some implications of viewing graphs as geometric objects. This approach offers a new perspective on a number of graph-theoretic and algorithmic problems. There are several ways to model graphs geometrically and our main concern here is with geometric representations that respect the metric of the (possibly weighted) graph. Given a graph G we map its vertices to a normed space in an attempt to (i) Keep down the dimension of the host space and (ii) Guarantee a small distortion, i.e., make sure that distances between vertices in G closely match the distances between their geometric images. We develop efficient algorithms for embedding graphs low-dimensionally with a small distortion.","Geometry,
Extraterrestrial measurements,
Computer science,
Solid modeling,
Application software,
Mathematics,
Pattern recognition,
Embedded computing,
Polynomials"
The power of team exploration: two robots can learn unlabeled directed graphs,"We show that two cooperating robots can learn exactly any strongly-connected directed graph with n indistinguishable nodes in expected time polynomial in n. We introduce a new type of homing sequence for two robots which helps the robots recognize certain previously-seen nodes. We then present an algorithm in which the robots learn the graph and the homing sequence simultaneously by wandering actively through the graph. Unlike most previous learning results using homing sequences, our algorithm does not require a teacher to provide counterexamples. Furthermore, the algorithm can use efficiently any additional information available that distinguishes nodes. We also present an algorithm in which the robots learn by taking random walks. The rate at which a random walk converges to the stationary distribution is characterized by the conductance of the graph. Our random-walk algorithm learns in expected time polynomial in n and in the inverse of the conductance and is more efficient than the homing-sequence algorithm for high-conductance graphs.","Robots,
Polynomials,
Legged locomotion,
Cities and towns,
Learning automata,
Robotics and automation,
Laboratories,
Roads,
Computer science,
Radio communication"
Mariposa: a new architecture for distributed data,"We describe the design of Mariposa, an experimental distributed data management system that provides high performance in an environment of high data mobility and heterogeneous host capabilities. The Mariposa design unifies the approaches taken by distributed file systems and distributed databases. In addition, Mariposa provides a general, flexible platform for the development of new algorithms for distributed query optimization, storage management, and scalable data storage structures. This flexibility is primarily due to a unique rule-based design that permits autonomous, local-knowledge decisions to be made regarding data placement, query execution location, and storage management.","Distributed databases,
Database systems,
File systems,
Computer architecture,
Computer networks,
Computer science,
Query processing,
Memory,
Protection,
Object oriented modeling"
Perceptual benchmarks for automatic language identification,"There has been renewed interest in the field of automatic language identification over the past two years. The advent of a public-domain ten-language corpus of telephone speech has made the evaluation of different approaches to automatic language identification feasible. In an effort to provide benchmarks for evaluating machine performance, we conducted perceptual experiments on 1-, 2-, 4- and 6-second excerpts of telephone speech excised from spontaneous speech utterances in this corpus. The subject population consisted of 10 native speakers of English and 2 speakers from each of the remaining 9 languages. Statistical analyses of our results indicate that duration of the excerpt, familiarity with the language, and number of languages known are important factors affecting a subject's performance on the identification task.","Natural languages,
Speech analysis,
Telephony,
NIST,
Computer science,
Instruments,
Statistical analysis,
Search problems,
Surges,
Humans"
First-order versus second-order single-layer recurrent neural networks,"We examine the representational capabilities of first-order and second-order single-layer recurrent neural networks (SLRNN's) with hard-limiting neurons. We show that a second-order SLRNN is strictly more powerful than a first-order SLRNN. However, if the first-order SLRNN is augmented with output layers of feedforward neurons, it can implement any finite-state recognizer, but only if state-splitting is employed. When a state is split, it is divided into two equivalent states. The judicious use of state-splitting allows for efficient implementation of finite-state recognizers using augmented first-order SLRNN's.","Recurrent neural networks,
Neurons,
National electric code,
Educational institutions,
Automata,
Neural networks,
Circuits,
Latches,
Computer science,
USA Councils"
Dynamic file-access characteristics of a production parallel scientific workload,"Multiprocessors have permitted astounding increases in computational performance, but many cannot meet the intense I/O requirements of some scientific applications. An important component of any solution to this I/O bottleneck is a parallel file system that can provide high-bandwidth access to tremendous amounts of data in parallel to hundreds or thousands of processors. Most successful systems are based on a solid understanding of the expected workload, but thus far there have been no comprehensive workload characterizations of multiprocessor file systems. This paper presents the results of a three week tracing study in which all file-related activity on a massively parallel computer was recorded. Our instrumentation differs from previous efforts in that it collects information about every I/O request and about the mix of jobs running an a production environment. We also present the results of a trace-driven caching simulation and recommendations for designers of multiprocessor file systems.","File systems,
Workstations,
Concurrent computing,
Application software,
Computer science,
Educational institutions,
Solids,
Instruments,
Job production systems,
Computational modeling"
Virtual path layout design on ATM networks,"The paper examines the efficient layout of virtual paths (VPs) in an ATM network. The ATM network consists of ATM switches and their attached network end users, which may be gateways, routers, and hosts. The physical topology, the offered traffic, and call setup matrices of the network end users are assumed to be given. The problem is formulated as a flow-based optimization problem. A heuristic approach is presented which (i) establishes VPs according to physical network-specific and application-specific constraints and a cost function, (ii) provides multipaths between each source destination user pair to minimize the cell blocking probability and to increase network resilience, and (iii) uses a novel VP combining process which is guaranteed to always satisfy the switching constraints. Simulation results are presented for the proposed VP planning policy. Guidelines for the design of robust VP layouts and the efficient establishment of VCs are also presented.","Asynchronous transfer mode,
Telecommunication traffic,
Costs,
Virtual colonoscopy,
Switches,
Computer science,
Circuits,
Routing,
Network topology,
Resilience"
On the intrinsic Rent parameter and spectra-based partitioning methodologies,"The complexity of circuit designs has necessitated a top-down approach to layout synthesis. A large body of work shows that a good layout hierarchy, or partitioning tree, as measured by the associated Rent parameter, will correspond to an area-efficient layout. We define the intrinsic Rent parameter of a netlist to be the minimum possible Rent parameter of any partitioning tree for the netlist. Experimental results show that spectra-based ratio cut partitioning algorithms yield partitioning trees with the lowest observed Rent parameter over all benchmarks and over all algorithms tested. For examples where the intrinsic Rent parameter is known, spectral ratio cut partitioning yields a partitioning tree with Rent parameter essentially identical to this theoretical optimum. These results have deep implications with respect to both the choice of partitioning algorithms for top-down layout, as well as new approaches to layout area estimation. The paper concludes with directions for future research, including several promising techniques for fast estimation of the (intrinsic) Rent parameter.","Partitioning algorithms,
Circuit synthesis,
Recursive estimation,
Testing,
Very large scale integration,
Phase estimation,
Binary trees,
Design automation,
Computer science,
Marine vehicles"
Optimization of fuzzy clustering criteria using genetic algorithms,"This paper introduces a general approach based on genetic algorithms for optimizing a broad class of clustering criteria. The standard approach for optimizing these criteria has been to alternate optimizations between the variables which represent fuzzy memberships of the data to various clusters, and those prototype variables which determine the geometry of the clusters. The approach suggested here first re-parameterizes the criteria into functions of the prototype variables alone. The prototype variables are then coded as binary strings so that genetic algorithms can be applied. An overview of the approach and two simple numerical examples are given.","Genetic algorithms,
Prototypes,
Clustering algorithms,
Fuzzy sets,
Computer science,
Fuzzy logic,
Geometry,
Magnetic force microscopy,
Virtual colonoscopy,
Q measurement"
Simultaneous Functional-unit Binding And Floorplanning,,"High level synthesis,
Flow graphs,
Computer science,
Design automation,
Routing,
System performance,
Optimization,
Delay effects,
Tides"
Coarse-grain parallel genetic algorithms: categorization and new approach,"This paper describes a number of different coarse-grain GA's, including various migration strategies and connectivity schemes to address the premature convergence problem. These approaches are evaluated on a graph partitioning problem. Our experiments showed, first, that the sequential GA's used are not as effective as parallel GA's for this graph partition problem. Second, for coarse-grain GA's, the results indicate that using a large number of nodes and exchanging individuals asynchronously among them is very effective. Third, GA's that exchange solutions based on population similarity instead of a fixed connection topology get better results without any degradation in speed. Finally, we propose a new coarse-grained GA architecture, the Injection Island GA (iiGA). The preliminary results of iiGA's show them to be a promising new approach to coarse-grain GA's.","Genetic algorithms,
Convergence,
Genetic mutations,
Adaptive systems,
Circuit testing,
Application software,
Computer science,
Topology,
Optimization methods,
Image recognition"
Fuzzy constraint satisfaction,"In this paper the issue of soft constraint satisfaction is discussed from a fuzzy set theoretical point of view. A fuzzy constraint is considered as a fuzzy relation. Different possible definitions for the degree of joint satisfaction of a set of fuzzy constraints are given, covering other specific soft constraint satisfaction problem (CSP) types such as partial and hierarchical CSP. It is shown that the classical CSP solving heuristics based on variable and value evaluations can be generalised and used to guide the solution construction process for solving fuzzy CSPs, and that the heuristic search can be replaced by branch-and-bound search. The solution process is illustrated with an example from the CSP literature. Finally, research issues are discussed.","Fuzzy set theory,
Fuzzy sets,
Constraint theory,
Artificial intelligence,
Mathematics,
Computer science,
Knowledge acquisition,
Buildings"
A reconstruction algorithm using singular value decomposition of a discrete representation of the exponential radon transform using natural pixels,"An algorithm to correct for constant attenuation in SPECT is derived from the singular value decomposition (SVD) of a discrete representation of the exponential Radon transform using natural pixels. The algorithm is based on the assumption that a continuous image can be obtained by backprojecting the discrete array q, which is the least squares solution to Mq=p, where p is the array of discrete measurements, and the matrix M represents the composite operator of the backprojection operator A/sub /spl mu//* followed by the projection operator A/sub /spl mu//. A singular value decomposition of M is used to solve the equation Mq=p, and the final image is obtained by sampling the backprojection of the solution q at a discrete array of points. Analytical expressions are given to calculate the matrix elements of M that are integrals of exponential factors over the overlapped area of two projection strip functions (natural pixels). A spectral analysis of the exponential Radon transform is compared with that of the Radon transform. The condition number of the spectrum increases with increased attenuation coefficient, which correlates with the increase in statistical error propagation seen in clinical images obtained with low-energy radionuclides. Computer simulations using 32 projections sampled over 360 degrees show an improvement in the SVD reconstruction over the convolution backprojection reconstruction, especially when the projection data is corrupted with noise.","Reconstruction algorithms,
Singular value decomposition,
Discrete transforms,
Attenuation,
Matrix decomposition,
Image reconstruction,
Least squares methods,
Q measurement,
Integral equations,
Image sampling"
Constraint multiset grammars,"Constraint multiset grammars provide a general, high-level framework for the definition of visual languages. They are a new formalism based on multiset rewriting. We give a formal semantics for constraint multiset grammars, investigate the theoretical complexity of parsing with these grammars and give an incremental parsing algorithm.","Application software,
Programming profession,
Mars,
Logic programming,
Computer science,
Australia,
Constraint theory,
Hardware,
Software tools,
Organizing"
A paradigm for decentralized process modeling and its realization in the OZ environment,"We present a model for decentralized Process Centered Environments (PCEs), which support concerted efforts among geographically-dispersed teams - each local team with its own autonomous process - and emphasize flexibility in the tradeoff between collaboration vs. autonomy. We consider both decentralized process modeling and decentralized process enaction. We describe a realization in the OZ decentralized PCE, which employs a rule-based formalism, and also investigate the application to PCEs based on Petri-nets.","Collaboration,
Computer science,
Application software,
Software systems,
Personnel,
Computer languages,
Software performance,
Computerized monitoring,
Automation,
Automatic control"
Randomness-efficient oblivious sampling,"We introduce a natural notion of obliviousness of a sampling procedure, and construct a randomness-efficient oblivious sampler. Our sampler uses O(l+log /spl delta//sup -1//spl middot/log l) coins to output m=poly(/spl epsiv//sup -1/, log /spl delta//sup -1/, log l) sample points x/sub 1/, ..., x/sub m/, /spl isin/ {0, 1}/sup 1/ such that Pr[|1/m/spl Sigma//sub i=1//sup m/f(x/sub i/)-E[f]|","Sampling methods,
Electronic mail,
Tail,
Random variables,
Security,
Polynomials,
Boolean functions"
Reaching approximate agreement with mixed-mode faults,"In a fault-tolerant distributed system, different non-faulty processes may arrive at different values for a given system parameter. To resolve this disagreement, processes must exchange and vote upon their respective local values. Faulty processes may attempt to inhibit agreement by acting in a malicious or ""Byzantine"" manner. Approximate agreement defines one form of agreement in which the voted values obtained by the non-faulty processes need not be identical. Instead, they need only agree to within a predefined tolerance. Approximate agreement can be achieved by a sequence of convergent voting rounds, in which the range of values held by non-faulty processes is reduced in each round. Historically, each new convergent voting algorithm has been accompanied by ad-hoc proofs of its convergence rate and fault-tolerance, using an overly conservative fault model in which all faults exhibit worst-case Byzantine behavior. This paper presents a general method to quickly determine convergence rate and fault-tolerance for any member of a broad family of convergent voting algorithms. This method is developed under a realistic mixed-mode fault model comprised of asymmetric, symmetric, and benign fault modes. These results are employed to more accurately analyze the properties of several existing voting algorithms, to derive a sub-family of optimal mixed-mode voting algorithms, and to quickly determine the properties of proposed new voting algorithms.","Voting,
Fault tolerance,
Convergence,
Clocks,
Synchronization,
Fault tolerant systems,
Algorithm design and analysis,
Computer science,
Hardware,
Application software"
"Control of MMST RTP: repeatability, uniformity, and integration for flexible manufacturing [ICs]","A real-time multivariable strategy is used to control the uniformity and repeatability of wafer temperature in rapid thermal processing (RTP) semiconductor device manufacturing equipment. This strategy is based on a physical model of the process where the model parameters are estimated using an experimental design procedure. The internal model control (IMC) law design methodology is used to automatically compute the lamp powers to a multizone array of concentric heating zones to achieve wafer temperature uniformity. Control actions are made in response to real-time feedback information provided by temperature sensing, via pyrometry, at multiple points across the wafer. Several modules, including model-scheduling and antiovershoot, are coordinated with IMC to achieve temperature control specifications. The control strategy, originally developed for prototype equipment at Stanford University, is analyzed via the customization, integration, and performance on eight RTP reactors at Texas Instruments conducting thirteen different thermal fabrication operations of two sub-half-micron CMOS process technologies used in the the Microelectronics Manufacturing Science and Technology (MMST) program.",
Exposing I/O concurrency with informed prefetching,"Informed prefetching provides a simple mechanism for I/O-intensive, cache-ineffective applications to efficiently exploit highly-parallel I/O subsystems such as disk arrays. This mechanism, dynamic disclosure of future accesses, yields substantial benefits over sequential readahead mechanisms found in current file systems for non-sequential workloads. This paper reports the performance of the Transparent Informed Prefetching system (TIP), a minimal prototype implemented in a Mach 3.0 system with up to four disks. We measured reductions by factors of up to 1.9 and 3.7 in the execution time of two example applications: multi-file text search and scientific data visualization.","Concurrent computing,
Prefetching,
Delay,
File systems,
Throughput,
Testing,
Computer science,
Application software,
Prototypes,
Time measurement"
Adaptive RID kernels which minimize time-frequency uncertainty,"The reduced interference distribution (RID) satisfies many of the desirable properties of time-frequency distributions (TFDs), including reduced interference. Using simple rules, it is possible to define kernels which guarantee the RID properties. A primitive h(t) is the starting point for kernel design. Starting with this primitive h(t), one may evolve a RID which enjoys all of the desirable properties. In addition, RIDs designed by this means also exhibit scale invariance in addition to the time-shift invariance and frequency-shift invariance exhibited by members of Cohen's class relevant to these properties. Such RIDs also exhibit a property which we call information invariance. Information invariance is defined under Renyi's generalized information. Renyi information has been found to be a useful quantifier of resolution in TFDs wherein minimum values of Renyi information are related to maximum resolution of the signal components. In this paper, we show that h(t) may be adapted to minimize Renyi information and thus achieve very good time-frequency resolution.","Kernel,
Time frequency analysis,
Uncertainty,
Interference,
Signal resolution,
Delay effects,
Energy resolution,
Fourier transforms,
Computer science,
Spectrogram"
Using tickets to enforce the serializability of multidatabase transactions,"To enforce global serializability in a multidatabase environment the multidatabase transaction manager must take into account the indirect (transitive) conflicts between multidatabase transactions caused by local transactions. Such conflicts are difficult to resolve because the behavior or even the existence of local transactions is not known to the multidatabase system. To overcome these difficulties, we propose to incorporate additional data manipulation operations in the subtransactions of each multidatabase transaction. We show that if these operations create direct conflicts between subtransactions at each participating local database system, indirect conflicts can be resolved even if the multidatabase system is not aware of their existence. Based on this approach, we introduce optimistic and conservative multidatabase transaction management methods that require the local database systems to ensure only local serializability. The proposed methods do not violate the autonomy of the local database systems and guarantee global serializability by preventing multidatabase transactions from being serialized in different ways at the participating database systems. Refinements of these methods are also proposed for multidatabase environments where the participating database systems allow schedules that are cascadeless or transactions have analogous execution and serialization orders. In particular, we show that forced local conflicts can be eliminated in rigorous local systems, local cascadelessness simplifies the design of a global scheduler, and that local strictness offers no significant advantages over cascadelessness.","Database systems,
Transaction databases,
Scheduling,
Environmental management,
Optimization methods,
Control systems,
Distributed computing,
Laboratories,
Computer science,
Computer architecture"
An Efficient Zero-Skew Routing Algorithm,A bucket algorithm is proposed for zero-skew routing with linear time complexity on the average. Our algorithm is much simpler and more efficient than the best known algorithm which uses Delaunay triangulations for segments on Manhattan distance. Experimental results show that the linearity of our algorithm is accomplished. Our algorithm generates a zero-skew routing for 3000-pin benchmark data within 5 seconds on a 90MIPS RISC workstation.,"Routing,
Permission,
Design automation,
Distributed computing,
Machinery"
CS proofs,"This paper puts forward a computationally-based notion of proof and explores its implications to computation at large. In particular, given a random oracle or a suitable cryptographic assumption, we show that every computation possesses a short certificate vouching its correctness, and that, under a cryptographic assumption, any program for a /spl Nscr//spl Pscr/-complete problem is checkable in polynomial time. In addition, our work provides the beginnings of a theory of computational complexity that is based on ""individual inputs"" rather than languages.","Polynomials,
Cryptography,
Computational complexity,
Laboratories,
Computer science,
Complexity theory,
NP-complete problem"
UCLOCK: automated design of high-performance unclocked state machines,"A new automated method is described for the correct design of asynchronous state machines. The method builds upon recent work using ""burst-mode"" specifications, a class of specifications which allow multiple-input change fundamental-mode operation. Implementations are built as unclocked Huffman machines, requiring no latches or other sequential elements. The method has several significant advantages over both the classical methods of Unger (1969) Friedman and Menon (1968) and Mago (1971) as well as recent methods of Nowick et al. (1991), Yun et al. (1992) and Davis et al. (1993). The synthesis method is applied to several design examples.","Added delay,
Clocks,
Microwave integrated circuits,
Design methodology,
Computer science,
Feedback,
Latches,
Silicon carbide,
Timing,
Books"
Aspects of the Hartley transform,"Known in the signal processing literature mainly as a computational technique, the Hartley transform has proven to possess attributes in the physical world of experiment, especially in optics and microwaves. The Fourier transform, which has constituted the standard approach to spectral analysis, is understood both in the domain of numerical analysis and in the world of natural phenomena, gaining its central significance from the ubiquity of sinusoidal natural behavior; it also gains appeal from the convenience offered by complex variable analysis, which is a basic part of technical education. Computers, however, prefer real numbers, an attribute that first opened a niche for the Hartley transform. As applications have multiplied, and others open up, it will be helpful to understand the Hartley idea from more than one point of view. Several topics, including multidimensional transforms, the complex generalization, and microwave phase measurement by amplitude measurement only, are discussed with the intention of throwing light on the numerical and physical relationships between the Hartley and Fourier transforms.","Fourier transforms,
Optical signal processing,
Phase measurement,
Multidimensional signal processing,
Optical computing,
Physics computing,
Microwave theory and techniques,
Spectral analysis,
Numerical analysis,
Computer science education"
Coordinated checkpointing-rollback error recovery for distributed shared memory multicomputers,"Most recovery schemes that have been proposed for Distributed Shared Memory (DSM) systems require unnecessarily high checkpointing frequency and checkpoint traffic, which are sensitive to the frequency of interprocess communication in the applications. For message-passing systems, low overhead error recovery based on coordinated checkpointing allows the frequency of checkpointing to be determined only by the reliability requirements of the application. Efficient adaptation of this approach to DSM multicomputers is complicated by the absence of explicit messages in DSM systems, the presence of a shared and partially replicated address space, and the presence of a distributed coherency directory. We present solutions to these issues, and propose an error recovery scheme based on coordinated checkpointing and rollback for DSM multicomputers. Our performance evaluation based on trace-driven simulations indicates that this scheme incurs less checkpoint traffic than recovery schemes previously proposed for DSM systems.","Checkpointing,
Frequency,
Computer errors,
Computer science,
Application software,
Traffic control,
Hardware,
Protocols,
Very large scale integration,
Fault tolerance"
Compression by induction of hierarchical grammars,"The paper describes a technique that constructs models of symbol sequences in the form of small, human-readable, hierarchical grammars. The grammars are both semantically plausible and compact. The technique can induce structure from a variety of different kinds of sequence, and examples are given of models derived from English text, C source code and a sequence of terminal control codes. It explains the grammatical induction technique, demonstrates its application to three very different sequences, evaluates its compression performance, and concludes by briefly discussing its use as a method for knowledge acquisition.","Dictionaries,
Computer science,
Telephony,
Production,
Data compression,
Statistical analysis,
Frequency,
Arithmetic,
Context modeling,
Knowledge acquisition"
Rectification of Multiple Logic Design Errors in Multiple Output Circuits,"This paper presents the EXMalgorithm, which locates multiple logic design errors in a combinational circuit with multiple output. An error possibility index and a six-valued simulation method have been introduced to reduce the number of error candidates without missing real errors. Experimental results have shown that this algorithm locates all errors at high hit ratio for benchmark circuits.","Logic design,
Design automation,
Combinational circuits,
Computer errors,
Permission,
Circuit simulation,
Circuit synthesis,
Error correction,
Design engineering,
Automatic logic units"
Solving constraint satisfaction problems using genetic algorithms,"This article discusses the applicability of genetic algorithms (GAs) to solve constraint satisfaction problems (CSPs). We discuss the requirements and possibilities of defining so-called heuristic GAs (HGAs), which can be expected to be effective and efficient methods to solve CSPs since they adopt heuristics used in classical CSP solving search techniques. We present and analyse experimental results gained by testing different heuristic GAs on the N-queens problem and on the graph 3-colouring problem.","Genetic algorithms,
Constraint optimization,
Testing,
Space exploration,
Artificial intelligence,
Mathematics,
Computer science,
Search methods"
A framework for evaluating regression test selection techniques,"Regression testing is a necessary but expensive activity aimed at showing that code has not been adversely affected by changes. A selective approach to regression testing attempts to reuse tests from an existing test suite to test a modified program. This paper outlines issues relevant to selective retest approaches, and presents a framework within which such approaches can be evaluated. This framework is then used to evaluate and compare existing selective retest algorithms. The evaluation reveals strengths and weaknesses of existing methods, and highlights problems that future work in this area should address.","Testing,
Software maintenance,
Costs,
Computer science,
Production,
Performance evaluation,
Computational efficiency"
Multi-area unit commitment via sequential method and a DC power flow network model,"This paper presents a multi-area unit commitment method based on the sequential commitment procedure that resembles ""bidding"". Instead of the linear flow network representation usually used in multi-area production simulations, the proposed method employs a DC power flow model to represent the inter-area transmission network. In this paper, the proposed method is outlined and illustrated via sample application results.","Load flow,
Dynamic programming,
Decision making,
Production,
Power system dynamics,
Computer science,
Computational modeling,
DC generators,
Power generation,
Power transmission lines"
"Fully adaptive minimal deadlock-free packet routing in hypercubes, meshes, and other networks: algorithms and simulations","This paper deals with the problem of packet-switched routing in parallel machines. Several new routing algorithms for different interconnection networks are presented. While the new techniques apply to a wide variety of networks, routing algorithms will be shown for the hypercube, the two-dimensional mesh, and the shuffle-exchange. Although the new techniques are designed for packet routing, they can be used alternatively for virtual cut-through routing models. The techniques presented for hypercubes and meshes are fully-adaptive and minimal. A fully-adaptive and minimal routing is one in which all possible minimal paths between a source and a destination are of potential use at the time a message is injected into the network. Minimal paths followed by messages ultimately depend on the local congestion encountered in each node of the network. All of the new techniques are completely free of deadlock situations.",
Aggressive transmissions of short messages over redundant paths,"Fault-tolerant computer systems have redundant paths connecting their components. Given these paths, it is possible to use aggressive techniques to reduce the average value and variability of the response time for short, critical messages. One technique is to send a copy of a packet over an alternate path before it is known whether the first copy failed or was delayed. A second technique is to split a single stream of packets over multiple paths. The authors analyze both approaches and show that they can provide significant improvements over conventional, conservative mechanisms.",
Temporal specialization and generalization,"A standard relation has two dimensions: attributes and tuples. A temporal relation contains two additional orthogonal time dimensions: valid time records when facts are true in the modeled reality, and transaction time records when facts are stored in the temporal relation. Although there are no restrictions between the valid time and transaction time associated with each fact, in many practical applications the valid and transaction times exhibit restricted interrelationships that define several types of specialized temporal relations. This paper examines areas where different specialized temporal relations are present. In application systems with multiple, interconnected temporal relations, multiple time dimensions may be associated with facts as they flow from one temporal relation to another. The paper investigates several aspects of the resulting generalized temporal relations, including the ability to query a predecessor relation from a successor relation. The presented framework for generalization and specialization allows one to precisely characterize and compare temporal relations and the application systems in which they are embedded. The framework's comprehensiveness and its use in understanding temporal relations are demonstrated by placing previously proposed temporal data models within the framework. The practical relevance of the defined specializations and generalizations is illustrated by sample realistic applications in which they occur. The additional semantics of specialized relations are especially useful for improving the performance of query processing.",
Extensions to media richness theory: a test of the task-media fit hypothesis,"The richness of the communication environment and the type of task formed by dyads was contrasted in a laboratory experiment. Dyads communicated using face-to-face, videophone, telephone, and synchronous computer-mediated communication. One task was an intellective task while the other was a value-laden cognitive conflict task. For the intellective task, subjects were given different information (i.e. one subject received a directory from the Yellow Pages and the other a city map) and asked to locate the closest doctor's office listed in the Yellow Pages directory to a location marked on the map. For the value-laden task, subjects were added to allocate limited funds to one or more of six controversial social causes. The results of this study help to provide theoretical extensions to normative views of media richness theory by discussing how variations in task processes may act to mediate both perceptions and performance.","Oral communication,
Image communication,
Human factors"
A weighted Steiner tree-based global router with simultaneous length and density minimization,"We consider the problem of global routing, aiming to simultaneously minimize wire length and density through the regions. Previous global routers have attempted to achieve this goal; however, they minimized one of the two parameters as the main objective and proposed heuristics for minimizing the other parameter. We accomplish this task by introducing the concept of weighted Steiner trees. We propose an efficient and simple algorithm for obtaining a weighted (rectilinear) Steiner tree in the plane. The proposed global router at each step finds a weighted Steiner tree for a net, where weight of a region represents its ""complexity"". Weights of the regions are dynamically changing. Experimental results on master slice chips and on benchmark examples from the Physical Design Workshop are included, and they verify the effectiveness of the proposed global router and its superiority over related global routers.",
Software versus hardware shared-memory implementation: a case study,"Compares the performance of software-supported shared memory on a general-purpose network to hardware-supported shared memory on a dedicated interconnect. Up to eight processors, the results are based on the execution of a set of application programs on a SGI 4D/480 multiprocessor and on TreadMarks, a distributed shared memory system that runs on a Fore ATM LAN of DECstation-5000/240s. Since the DECstation and the 4D/480 use the same processor, primary cache, and compiler, the shared-memory implementation is the principal difference between the systems. Beyond eight processors, the results are based on execution-driven simulation. Specifically, the authors compare a software implementation on a general-purpose network of uniprocessor nodes, a hardware implementation using a directory-based protocol on a dedicated interconnect, and a combined implementation using software to provide shared memory between multiprocessor nodes with hardware implementing shared memory within a node.",
Single event upsets for Space Shuttle flights of new general purpose computer memory devices,"The replacement of the magnetic core with a well characterized semiconductor memory in the Space Shuttle orbiter general purpose computers (GPCs) has provided a wealth of on-orbit radiation effects data since 1991. The fault tolerant GPCs detect, correct, and downlink memory upset status and orbiter position information every few seconds, giving the ability to correlate 1400 upsets to date with altitude, geomagnetic latitude, and solar conditions. The predicted upset rate was computed by a modified path-length distribution method. The modification accounts for the Weibull distribution cross-section (rather than a single upset threshold) and the device sensitive volume thickness. Device thickness was estimated by the method normally used to account for edge effects at the upset cross-section discontinuity that occurs at ion changes. A Galactic cosmic ray environment model accurately models the average particle flux for each mission. The predicted and observed upset rates were found to be in good agreement for sensitive volume thicknesses consistent with the device's fabrication technology.",
Priority based real-time communication for large scale wormhole networks,"As advances are made in parallel processing technology, an increasing number of real-time applications are being developed for large-scale parallel processors. Since the wormhole network is a popular communication system used in the new generation of large-scale parallel multiprocessors, real-time communication support on wormhole networks becomes an important issue. We evaluate a priority mapping scheme, a priority adjustment scheme and a message dropping method for large-scale real-time wormhole networks. The priority mapping scheme embeds the timing property of a message into a priority for flow control decisions. The priority adjustment scheme dynamically modifies the priority of a message as the timing property of the message changes. The tardy messages, which miss their deadlines, are removed from the network by the message dropping method. Simulation studies show that the schemes outperform the conventional flow control scheme implemented in general purpose wormhole networks, and the schemes provide desirable performance when the size of the parallel system increases.",
Performance tuning with AIMS/spl minus/an Automated Instrumentation and Monitoring System for multicomputers,"Whether a researcher is designing the ""next parallel programming paradigm"", another ""scalable multiprocessor"", or investigating resource allocation algorithms for multiprocessors, a facility that enables parallel program execution to be captured and displayed is invaluable. A software toolkit that facilitates performance evaluation of parallel applications on multiprocessors, the Automated Instrumentation and Monitoring System (AIMS), is described in this paper. It has four major software components: a source-code instrumentor, which automatically inserts event recorders into the application; a run-time performance-monitoring library, which collects performance data; a trace-file animation and analysis toolkit; and a trace post-processor which compensates for the data collection overhead. We illustrate the process of performance tuning using AIMS with two examples. Currently, AIMS accepts FORTRAN and C parallel programs written for TMC's CM-5, Intel's iPSC/860, iPSC/Delta, Paragon, and HP workstations running PVM.",
Analyzing teams of cooperating mobile robots,"Donald (1993) described a manipulation task for cooperating mobile robots that can push large, heavy objects. There, the author asked whether explicit local and global communication between the agents can be removed from a family of pushing protocols. In this paper, the authors answer in the affirmative. They do so by using the general methods of Donald for analyzing information invariants. The authors discuss several measures for the information complexity of the task of pushing with cooperating mobile robots, and they present a methodology for creating new manipulation strategies out of existing ones. The authors develop and analyze synchronous and asynchronous manipulation protocols for a small team of cooperating mobile robots than can push large boxes. The protocols described have been implemented in several forms on the Cornell mobile robots in the authors' laboratory.",
Efficient evaluation of the valid-time natural join,"Joins are arguably the most important relational operators. Poor implementations are tantamount to computing the Cartesian product of the input relations. In a temporal database, the problem is more acute for two reasons. First, conventional techniques are designed for the optimization of joins with equality predicates, rather than the inequality predicates prevalent in valid-time queries. Second, the presence of temporally-varying data dramatically increases the size of the database. These factors require new techniques to efficiently evaluate valid-time joins. The authors address this need for efficient join evaluation in databases supporting valid-time. A new temporal-join algorithm based on tuple partitioning is introduced. This algorithm avoids the quadratic cost of nested-loop evaluation methods; it also avoids sorting. Performance comparisons between the partition-based algorithm and other evaluation methods are provided. While the paper focuses on the important valid-time natural join, the techniques presented are also applicable to other valid-time joins.",
"Statistical Physics, Mixtures of Distributions, and the EM Algorithm","We show that there are strong relationships between approaches to optmization and learning based on statistical physics or mixtures of experts. In particular, the EM algorithm can be interpreted as converging either to a local maximum of the mixtures model or to a saddle point solution to the statistical physics system. An advantage of the statistical physics approach is that it naturally gives rise to a heuristic continuation method, deterministic annealing, for finding good solutions.",
Layered explanations of software: a methodology for program comprehension,"In dealing with the legacy systems, one often encounters poorly documented and heavily maintained software. Lack of understandability of these systems complicates the task of software maintenance, making it time consuming and limiting the possibilities of the evolution of the system. We present a methodology that helps the programmers to understand programs. Our approach is compatible with the ""top-down theory"" of software understanding, where the programmer creates a chain of hypotheses and subsidiary hypotheses, concerning the properties of the code. Then he/she looks for evidence (beacons) in the code. Our approach shortens the process of hypotheses creation and verification, and allows recording of successful hypotheses for the future maintenance. All information needed for understanding is recorded in layers of annotations. An experiment was conducted to investigate how the proposed methodology helps in program understanding. A tool supporting the methodology, is presented.",
An automatic jigsaw puzzle solver,"A computer vision system to automatically analyze and assemble an image of the pieces of a jigsaw puzzle is presented. The system, called Automatic Puzzle Solver (APS), derives a new set of features based on the shape and color characteristics of the puzzle pieces. A combination of the shape dependent features and color cues is used to match the puzzle pieces. Matching is performed using a modified iterative labeling procedure in order to reconstruct the original picture represented by the jigsaw puzzle. Algorithms for obtaining shape description and matching are explained with experimental results.",
Problem solving using cultural algorithms,"In this paper an approach to evolutionary learning based upon principles of cultural evolution is developed. In this dual-inheritance system, there is an evolving population of trait sequences as well as an associated belief space. The belief space is derived from the behavior of individuals and is used to actively constrain the traits acquired in future populations. Shifts in the representation of the belief space and the population is supported. The approach is used to solve several versions of the BOOLE problem; F6, F11, and F20. The results are compared with other approaches and the advantages of a dual inheritance approach using cultural algorithms is discussed.",
Why is it so difficult for a robot to pass through a doorway using ultrasonic sensors?,"Complex tasks are usually described as high-level goals, leaving out the details on how to achieve them. However, to control a robot, details must be provided. Having the robot move itself to and through an unknown, and possibly narrow, doorway is an example of such a task. The authors illustrate the difficulty of such a task using actual data from a real robot. The authors show how the transformation from high-level goals to primitive commands can be performed at execution time and they propose an architecture based on reconfigurable objects that contain domain knowledge and knowledge about the sensors and actuators available. The authors then show how their approach is used in solving the illustrated task.",
Jovian: a framework for optimizing parallel I/O,"There has been a great deal of recent interest in parallel I/O. We discuss the design and implementation of the Jovian library, which is intended to optimize the I/O performance of multiprocessor architectures that include multiple disks or disk arrays. We also present preliminary performance measurements from benchmarking the Jovian I/O library on the IBM SP1 distributed memory parallel machine for two application templates.","Libraries,
Design optimization,
Computer architecture,
Kelvin,
Computer science,
Educational institutions,
Parallel machines,
Bandwidth,
Computational modeling,
Concurrent computing"
'Perception-action' behavior control of a mobile robot in uncertain environments using fuzzy logic,"This paper presents a new method for 'perception-action' behavior control of a mobile robot in uncertain environments using fuzzy logic. 'Perception-action' behavior control suffers from two significant problems: 1. The quantitative formulation of 'perception-action' behaviors; 2. The efficient coordination of conflicts and competitions among multiple behaviors. The main idea of the present study is to incorporate fuzzy logic control with behavior-based control such that: 'perception-action' behaviors are formulated by fuzzy sets and fuzzy rules; conflicts and competitions among different behaviors are coordinated by fuzzy reasoning. Simulation results show that the proposed method can be applied to efficient robot navigation in complex and unknown environments by weighting multiple behaviors, such as avoiding obstacles, following edges, and moving to a target, and so forth. In addition, this method is suitable for robot navigation by multisensor fusion and integration.","Mobile robots,
Fuzzy logic,
Robot kinematics,
Navigation,
Fuzzy sets,
Robot sensing systems,
Control systems,
Computer science,
Intelligent systems,
Intelligent robots"
On-line admission control and circuit routing for high performance computing and communication,"This paper considers the problems of admission control and virtual circuit routing in high performance computing and communication systems. Admission control and virtual circuit routing problems arise in numerous applications, including video-servers, real-lime database servers, and the provision of permanent virtual channel in large-scale communications networks. The paper describes both upper and lower bounds on the competitive ratio of algorithms for admission control and virtual circuit routing in trees, arrays, and hypercubes (the networks most commonly used in conjunction with nigh performance computing and communication). Our results include optimal algorithms for admission control and virtual circuit routing in trees, as well as the first competitive algorithms for these problems on non-tree networks. A key result of our research is the development of on-line algorithms that substantially outperform the greedy-based approaches that are used in practice.",
Platonic beasts: a new family of multilimbed robots,"Describes class of spherically symmetric, high degree of freedom robots called ""platonic beasts"". A robot in this family is kinematically equivalent to a symmetric polyhedron, such as one of the platonic solids, with identical multi-purpose limbs attached to its vertices. The symmetry and regularity of the design have several advantages including robustness to toppling, novel gaits such as the rolling gait, and fault tolerance. The authors describe the design, simulation, and construction of a prototype platonic beast robot that the authors have built in their lab. The robot has four limbs, each with three degrees of freedom, and is controlled by a network of four embedded 32-bit microcontrollers. The authors also discuss the general features of these robots, including locomotion using the rolling gait and the implications of its novel features.","Legged locomotion,
Robot sensing systems,
Mobile robots,
Joining processes,
Solids,
Leg,
Prototypes,
Kinematics,
Computer science,
Fault tolerance"
The Design of the SEER Predictive Caching System,"Supporting portable computers in a disconnected environment will require persistent caching of files without user intervention. SEER is a system that uses semantic information to predict which files the user is likely to work on, and arranges to transparently cache them on the portable platform prior to disconnection. We present the overall design of the SBBn system and the algorithms used to determine semantic relationships.","Distance measurement,
Prediction algorithms,
Libraries,
Marine vehicles,
Clustering algorithms,
Program processors,
Computers"
How to get good performance from the CM-5 data network,"Programmers of the Connection Machine CM-5 data network can improve the performance of their data movement code more than a factor of three by selectively using global barriers, by limiting the rate at which messages are injected in to the network, and by managing the order in which they are injected. Barriers eliminate target-processor congestion, and allow the programmer to schedule communications globally. Injection-reordering improves the statistical independence of the various packets in the network at any given time. Barriers and tuned injection rates provide forms of flow control. Barriers also provide a composition of performance property: if you understand the performance of parallel computations A and B, then you understand the performance of ""A; barrier; B"". Architectural support for global barriers, injection reordering, and flow control may be worthwhile for achieving good communications performance. Although our evidence comes from the CM-5, we expect these techniques to apply to most parallel machines.","Bandwidth,
Programming profession,
Communication system control,
Contracts,
Laboratories,
Computer science,
Computer network management,
Processor scheduling,
Concurrent computing,
Parallel machines"
"Using goals, rules, and methods to support reasoning in business process reengineering","One step towards a more systematic approach to the design of business processes is to develop models that provide appropriate representations of the knowledge that is needed for understanding and for reasoning about business processes. We present a modelling framework which uses goals, rules, and methods to support the systematic analysis and design of business processes. The framework consists of two main components/spl minus/an actor dependency model that describes a process organization in terms of intentional dependencies among actors, and an issue argumentation model that supports reasoning during process redesign. Formal representation of these models allows computer-based tools to be developed as extensions to, and eventually integrated with, other tools for supporting information systems development.","Business data processing,
Information systems,
System analysis and design,
Software tools,
Inference mechanisms,
Knowledge representation"
Novel finite-element analysis of space-charge modified fields,A new finite-element based method for the analysis of the space-charge modified field around unipolar transmission lines without resort to Deutsch's assumption is described. The method seeks a solution of only one second-order partial differential equation (PDE) rather than a solution of a nonlinear third-order PDE or a solution of two simultaneous second-order PDEs as documented in the literature. The effectiveness of the proposed method has been tested through the application to the coaxial cylindrical and conductor-to-plane configurations. Present results agree well with previous calculated and experimental values. The proposed method is characterised by a quick convergence to achieve the desired accuracy and simplicity in computer programming.,"Corona,
Electric fields,
Voltage,
Finite element methods,
High-voltage techniques,
Interpolation,
Partial differential equations,
Space charge,
Transmission line theory"
Image segmentation using pulse coupled neural networks,It is shown that if the parameters of a pulse coupled neural network are properly adjusted neurons corresponding to the pixels of each region can be forced to pulse together. Each synchronous burst identifies an image region.,"Image segmentation,
Neural networks,
Neurons,
Joining processes,
Digital images,
Pixel,
Pulse generation,
Signal generators,
Computer science,
Rockets"
Constrained software generation for hardware-software systems,"Mixed systems are composed of interacting hardware components such as general-purpose processors, application-specific circuits and software components that executes on the general purpose hardware. The software component consists of application-specific routines that must deliver the required system functionality and a runtime environment. Further, the software is required to deliver functionality under constraints of timing and memory storage available. We consider methods to achieve software generation under imposed constraints and demonstrate the utility of our approach by examples.",
Parallel algorithms for higher-dimensional convex hulls,"We give fast randomized and deterministic parallel methods for constructing convex hulls in R/sup d/, for any fixed d. Our methods are for the weakest shared-memory model, the EREW PRAM, and have optimal work bounds (with high probability for the randomized methods). In particular, we show that the convex hull of n points in R/sup d/ can be constructed in O(log n) time using O(n log n+n/sup [d/2]/) work, with high probability. We also show that it can be constructed deterministically in O(log/sup 2/ n) time using O(n log n) work for d=3 and in O(log n) time using O(n/sup [d/2]/ log/sup c([d/2]-[d/2]/) n) work for d/spl ges/4, where c>0 is a constant which is optimal for even d/spl ges/4. We also show how to make our 3-dimensional methods output-sensitive with only a small increase in running time. These methods can be applied to other problems as well.",
Recovering guaranteed performance service connections from single and multiple faults,"Fault recovery techniques must be reexamined in the light of the new guaranteed performance services that high-speed packet/cell switched networks will support. We investigate the rerouting of guaranteed performance service connections on the occurrence of link faults, focussing on the aspects of route selection and establishment in the network. In a previous investigation, we explored some components of rerouting in the presence of single link faults in the network. In this paper we study the behavior of our techniques in the presence of multiple link faults in the network. We also examine the technique of retries to improve the success of rerouting. Our schemes are simulated on a cross-section of network workloads, and compared using several performance criteria. We use our results to develop a new delayed retry technique, which performs well for all our performance criteria.",
Trace driven simulation using sampled traces,"Trace driven simulation is a well known method for evaluating computer architecture options and is the technique of choice in most published cache and memory studies. Ideally, a trace should contain all the necessary events generated by a program. However, this is usually impractical for all but the most trivial of programs because of trace storage and simulation time costs. As computer systems increase in performance and complexity there is a growing need to use larger and more realistic programs for trace driven simulation. This has lead to a growing interest in applying sampling techniques to reduce trace driven simulation costs. This paper reports on same experiments in trace sampling and discusses a prediction method for resolving cold-start or fill references when simulating with a sampled trace. The paper shows how a small sampled trace can capture the characteristics of a much larger trace and cache simulations results are presented using these sampled traces and the prediction method.",
Variable-length message scheduling algorithms for a WDM-based local lightwave network,"Two major difficulties in designing a single-hop multichannel local lightwave network are: relatively large transmitter/receiver tuning overhead and large ratio of propagation delay to packet transmission time. The authors propose several scheduling algorithms which can reduce the negative impact of tuning overhead and schedule variable-length messages. Thus, a long message can be scheduled with a single control packet transmission, instead of being segmented into many fixed-length packets, thereby significantly increasing the system's efficiency. Three novel scheduling algorithms are proposed, varying in the amount of global information and processing time. Two approximate analytical models are formulated to study the effect of tuning time and the effect of having a limited number of data channels. The system's performance is found to improve (1) if a simple mechanism is employed to avoid unnecessary transceiver tuning and/or (2) if a predictive transmitter tuning strategy is adopted.",
Empirical studies of predicate-based software testing,"We report the results of three empirical studies of fault detection and stability performance of the predicate-based BOR (Boolean Operator) testing strategy. BOR testing is used to develop test cases based on formal software specification, or based on the implementation code. We evaluated the BOR strategy with respect to some other strategies by using Boolean expressions and actual software. We applied it to software specification cause-effect graphs of a safety-related real-time control system, and to a set of N-version programs. We found that BOR testing is very effective at detecting faults in predicates, and that BOR-based approach has consistently better fault detection performance than branch testing, thorough (but informal) functional testing, simple state-based testing, and random testing. Our results indicate that BOR test selection strategy is practical and effective for detection of faulty predicates and is suitable for generation of safety-sensitive test-cases.",
Millimetre-wave antenna and propagation studies for indoor wireless LANs,"The combination of increased portability and increased computing power is placing new demands on interconnects with other such new computer systems and existing networks. One possible way of providing the required interconnections while maintaining portability is through wireless systems. Relatively low bit-rate systems are already available in the market place, and faster, second-generation systems are under development. This paper outlines work in progress at CSIRO on millimetre-wave antennas and propagation relating to wireless local area network (WLAN) systems operating at frequencies in the 40 to 60 GHz band for intra-office communication. The basic arrangement for such systems involves a hub antenna, located possibly in the ceiling, and a number of roamable antennas located on desks throughout the room, within a radius of about 20m of the hub. Signals transmitted to and from the antennas are subject to considerable multi-path interference and, whilst this can limit the system performance, it can be used to overcome the effect of obstructions in the direct path. It is apparent that to optimize WLAN performance it is important to understand propagation effects in rooms and to develop antennas whose characteristics match the overall requirements, including cost.",
Location management in distributed mobile environments,"Location management is an important problem in distributed mobile computing. Location management consists of location updates, searches and search-updates. An update occurs when a mobile host changes location. A search occurs when a mobile host needs to be located. A search-update occurs after a successful search. Various strategies can be designed for search, update and search-update. Static location management uses one combination of search, update and search-update strategies throughout the execution. Simulations were carried out to evaluate the performance of different static strategies for various communication and mobility patterns. Simulation results indicate that performing search-updates significantly reduces the message overhead of location management.",
Theory and practice of vector quantizers trained on small training sets,"Examines how the performance of a memoryless vector quantizer changes as a function of its training set size. Specifically, the authors study how well the training set distortion predicts test distortion when the training set is a randomly drawn subset of blocks from the test or training image(s). Using the Vapnik-Chervonenkis (VC) dimension, the authors derive formal bounds for the difference of test and training distortion of vector quantizer codebooks. The authors then describe extensive empirical simulations that test these bounds for a variety of codebook sizes and vector dimensions, and give practical suggestions for determining the training set size necessary to achieve good generalization from a codebook. The authors conclude that, by using training sets comprising only a small fraction of the available data, one can produce results that are close to the results obtainable when all available data are used.","Testing,
Image coding,
Pixel,
Computer science,
Image storage,
Gray-scale,
Computational efficiency,
Virtual colonoscopy,
Vector quantization,
Data compression"
Low-power electronics,"Plans for a government sponsored research program call for the development of a new electronics technology base for hand-size information systems. This base will enable the design and implementation of semiconductor technologies that consume two orders of magnitude less power than conventional technology would have allowed. The strategy follows two coupled tracks, one developing advanced materials technologies and one developing advanced architectural technologies that are independent of implementation. We describe the preliminary program plans.",
An end-to-end approach to schedule tasks with shared resources in multiprocessor systems,"We propose an end-to-end approach to scheduling tasks that share resources in a multiprocessor or distributed systems. In our approach, each task is mapped into a chain of subtasks, depending on its resource accesses. After each subtask is assigned a proper priority, its worst-case response time can be bounded. Consequently the worst-case response time of each task can be obtained and the schedulability of each task can be verified by comparing the worst-case response time with its relative deadline.","Multiprocessing systems,
Processor scheduling,
Access protocols,
Sun,
Computer science,
Real time systems,
Delay effects"
Distributed selective dissemination of information,"To help users cope with information overload, selective dissemination of information (SDI) will increasingly become an important tool in wide area information systems. In an SDI service, users post their long term queries, called profiles, at some SDI servers and continuously receive new, filtered documents. To scale up with the volume of information and the size of user population, we need a distributed SDI service with multiple servers. In this paper we first address the key problem of how to replicate and distribute profiles and documents among SDI servers. We draw a parallel between distributed SDT and the well-studied replica control problem, adapt quorum-based protocols for use in distributed SDI, and compare the performances of the different protocols. Next we address another important problem, that of efficient document delivery mechanisms. We present and evaluate a practical scheme, called profile grouping, which exploits the geographical locality of users to cut down network traffic generated by document delivery, Finally, we carry out a sensitivity analysis to determine the parameters that have critical impact on performance, and investigate strategies to cope with the scaling up of those parameters.",
A formal framework for ASTRAL intralevel proof obligations,"ASTRAL is a formal specification language for real-time systems. It is intended to support formal software development, and therefore has been formally defined. This paper focuses on how to formally prove the mathematical correctness of ASTRAL specifications. ASTRAL is provided with structuring mechanisms that allow one to build modularized specifications of complex systems with layering. In this paper, further details of the ASTRAL environment components and the critical requirements components, which were not fully developed in previous papers, are presented. Formal proofs in ASTRAL can be divided into two categories: interlevel proofs and intralevel proofs. The former deal with proving that the specification of level i+1 is consistent with the specification of level i, and the latter deal with proving that the specification of level i is consistent and satisfies the stated critical requirements. This paper concentrates on intralevel proofs.",
Rotation invariant texture classification using covariance,"Proposes a simple and powerful approach for texture classification using the eigenfeatures of local covariance measures. A texton encoder produces a texture code which is invariant to local and global textural rotations. The proposed method uses six statistical features obtained from two scales of this invariant encoder to result in indices for roughness, anisotropy, and other higher-order textural features. Classification results for synthetic and natural textures are presented. The authors also discuss the effect of window sizes used at local and global scales on the performance of the classifier.",
On the use of a directed acyclic graph to represent a population of computer programs,"This paper demonstrates a technique that reduces the time and space requirements of genetic programming. The population of parse trees is stored as a directed acyclic graph (DAG), rather than as a forest of trees. This saves space by not duplicating structurally identical subtrees. Also, the value computed by each subtree for each fitness case is cached, which saves computation both by not recomputing subtrees that appear more than once in a generation and by not recomputing subtrees that are copied from one generation to the next. I have implemented this technique for a number of problems and have seen a 15- to 28-fold reduction in the number of nodes extant per generation and an 11- to 30-fold reduction in the number of nodes evaluated per run (for populations of size 500).","Tree graphs,
Genetic programming,
Encoding,
Computer science,
Machine learning"
A fuzzy approach to input variable identification,"We introduce ""fuzzy curves"" and use them for input variable identification. We give several examples to show that, in contrast to previously proposed methods, our method is computationally simple and identifies significant input variables easily.",
A new efficient radix sort,"We present new improved algorithms for the sorting problem. The algorithms are not only efficient but also clear and simple. First, we introduce Forward Radix Sort which combines the advantages of traditional left-to-right and right-to-left radix sort in a simple manner. We argue that this algorithm will work very well in practice. Adding a preprocessing step, we obtain an algorithm with attractive theoretical properties. For example, n binary strings can be sorted in /spl Theta/ (n log(B/(n log n)+2)) time, where B is the minimum number of bits that have to be inspected to distinguish the strings. This is an improvement over the previously best known result by Paige and Tarjan (1987). The complexity may also be expressed in terms of H, the entropy of the input: n strings from a stationary ergodic process can be sorted in /spl Theta/ (n log(1/H+1)) time an improvement over the result recently presented by Chen and Reif (1993).",
Improving end-to-end performance of TCP over mobile internetworks,"Reliable transport protocols such as TCP use end-to-end flow, congestion and error control mechanisms to provide reliable delivery over an internetwork. However, the end-to-end performance of a TCP connection can suffer significant degradation in the presence of a wireless link. We are exploring alternatives for optimizing end-to-end performance of TCP connections across an internetwork consisting of both fixed and mobile networks. The central idea in our approach is to transparently split an end-to-end connection into two separate connections; one over the wireless link and other over the wired path. The connection over the wireless link may either use regular TCP or a specialized transport protocol optimized for better performance over a wireless link. Our approach does not require any changes to the existing protocol software on stationary hosts. Results of a systematic performance evaluation using both our approach and regular TCP show that our approach yields significant performance improvements.",
A framework for optimal scheduling of handoffs in wireless networks,"The phenomenon of handoffs in cellular networks is formulated as an optimization problem. The signal received by a mobile user is treated as a stochastic process with an associated reward that is a function of some measurable characteristics of the received signal while the handoff is associated with a switching penalty. Using dynamic programming, properties of optimal policies for hard as well as soft handoff problems are derived. Using these properties, an optimal policy is obtained, which determines the optimal base station the mobile should be associated with during each time slot. Simulation results are also included that compare the optimal policy with the commonly used threshold policy.","Optimal scheduling,
Intelligent networks,
Wireless networks,
Base stations,
Signal processing,
Land mobile radio cellular systems,
Stochastic processes,
Communication switching,
Algorithm design and analysis,
Computer science"
Research issues in scientific visualization,"As an emerging research discipline, scientific visualization is developing those trappings that demonstrate growth. New algorithms are just beginning to effectively handle the recurring scientific problem of data collected at nonuniform intervals. Volume visualization today is being extended from examining scientific data to reconstructing scattered data and representing geometrical objects without mathematically describing surfaces. Fluid dynamics visualization affects numerous scientific and engineering disciplines. It has taken its place with molecular modeling, imaging remote-sensing data, and medical imaging as a domain-specific visualization research area. Recently, much progress has come from using algorithms with roots in both computer graphics and machine vision. One important research thread has been the topological representation of important features. Volume and hybrid visualization now produce 3D animations of complex flows. However, while impressive 3D visualizations have been generated for scalar parameters associated with fluid dynamics, vector and especially tensor portrayal has proven more difficult. Seminal methods have appeared, but much remains to do. Great strides have also occurred in visualization systems. The area of automated selection of visualizations especially requires more work. Nonetheless, the situation has much improved, with these tools increasingly accessible to scientists and engineers.",
A case for networks of workstations (now),,
Off-line recognition of signatures using revolving active deformable models,"This paper presents a novel approach to the problem of signature recognition. We introduce the use of revolving active deformable models as a powerful way of capturing the unique characteristics of a signature's silhouette. Experimental evidence shows that the silhouette of a signature uniquely determines the signature in the majority of cases. The objective of our method is to recognize signatures based on the spatial properties of the signature boundaries. Our active deformable models originate from the snakes introduced to computer vision by Kass et al. (1987), but their implementation has been tailored to the task at hand. These computer-generated models interact with the virtual gravity field created by the image gradient. Ideally, the uniqueness of this interaction mirrors the uniqueness of the signature's silhouette. The proposed method obviates the use of a computationally expensive segmentation approach and yields satisfactory results regarding performance, without compromising the accuracy rate. Interestingly, the active deformable models have been implemented in such a way, that the method is potentially fully parallelizable. The experiments performed with a signature database show that the proposed method is promising.",
Computing with very weak random sources,"For any fixed /spl epsiv/>0, we show how to simulate RP algorithms in time n/sup O(log n/) using the output of a /spl delta/-source with min-entropy R(/spl epsiv/). Such a weak random source is asked once for R(/spl epsiv/) bits; it outputs an R-bit string such that any string has probability at most 2/sup -R/(/spl epsiv//). If /spl epsiv/>1-1/(k+1), our BPP simulations take time n/sup O(log(k/ n)) (log/sup (k/) is the logarithm iterated k times). We also give a polynomial-time BPP simulation using Chor-Goldreich sources of min-entropy R/sup /spl Omega/(1/), which is optimal. We present applications to time-space tradeoffs, expander constructions, and the hardness of approximation. Also of interest is our randomness-efficient Leftover Hash Lemma, found independently by Goldreich and Wigderson.",
New Techniques for Efficient Verification with Implicitly Conjoined BDDs,"In previous work, Hu and Dill identified a common cause of BDD-size blowup in high-level design verification and proposed the method of implicitly conjoined invariants to address the problem. That work, however, had some limitations: the user had to supply the property being verified as an implicit conjunction of BDDs, the heuristic used to decide which conjunctions to evaluate was rather simple, and the termination test, though fast and effective on a set of examples, was not proven to be always correct. In this work, we address those problems by proposing a new, more sophisticated heuristic to simplify and evaluate lists of implicitly conjoined BDDs and an exact termination test. We demonstrate on examples that these more complex heuristics are reasonably efficient as well as allowing verification of examples that were previously intractable.",
Program result-checking: a theory of testing meets a test of theory,"We review the field of result-checking, discussing simple checkers and self-correctors. We argue that such checkers could profitably be incorporated in software as an aid to efficient debugging and reliable functionality. We consider how to modify traditional checking methodologies to make them more appropriate for use in real-time, real-number computer systems. In particular, we suggest that checkers should be allowed to use stored randomness: i.e., that they should be allowed to generate, pre-process, and store random bits prior to run-time, and then to use this information repeatedly in a series of run-time checks. In a case study of checking a general real-number linear transformation (for example, a Fourier Transform), we present a simple checker which uses stored randomness, and a self-corrector which is particularly efficient if stored randomness is allowed.",
Performing group-by before join /spl lsqb/query processing/spl rsqb/,"Assume that we have an SQL query containing joins and a group-by. The standard way of evaluating this type of query is to first perform all the joins and then the group-by operation. However, it may be possible to perform the group-by early, that is, to push the group-by operation past one or more joins. Early grouping may reduce the query processing cost by reducing the amount of data participating in joins. We formally define the problem, adhering strictly to the semantics of NULL and duplicate elimination in SQL2, and prove necessary and sufficient conditions for deciding when this transformation is valid. In practice, it may be expensive or even impossible to test whether the conditions are satisfied. Therefore, we also present a more practical algorithm that tests a simpler, sufficient condition. This algorithm is fast and detects a large subclass of transformable queries.",
First-Order Recurrent Neural Networks and Deterministic Finite State Automata,"We examine the correspondence between first-order recurrent neural networks and deterministic finite state automata. We begin with the problem of inducing deterministic finite state automata from finite training sets, that include both positive and negative examples, an NP-hard problem (Angluin and Smith 1983). We use a neural network architecture with two recurrent layers, which we argue can approximate any discrete-time, time-invariant dynamic system, with computation of the full gradient during learning. The networks are trained to classify strings as belonging or not belonging to the grammar. The training sets used contain only short strings, and the sets are constructed in a way that does not require a priori knowledge of the grammar. After training, the networks are tested using various test sets with strings of length up to 1000, and are often able to correctly classify all the test strings. These results are comparable to those obtained with second-order networks (Giles et al. 1992; Watrous and Kuhn 1992a; Zeng et al. 1993). We observe that the networks emulate finite state automata, confirming the results of other authors, and we use a vector quantization algorithm to extract deterministic finite state automata after training and during testing of the networks, obtaining a table listing the start state, accept states, reject states, all transitions from the states, as well as some useful statistics. We examine the correspondence between finite state automata and neural networks in detail, showing two major stages in the learning process. To this end, we use a graphics module, which graphically depicts the states of the network during the learning and testing phases. We examine the networks' performance when tested on strings much longer than those in the training set, noting a measure based on clustering that is correlated to the stability of the networks. Finally, we observe that with sufficiently long training times, neural networks can become true finite state automata, due to the attractor structure of their dynamics.",
A parallel formulation of interior point algorithms,"Describes a scalable parallel formulation of interior point algorithms. Through our implementation on a 256-processor nCUBE 2 parallel computer, we show that our parallel formulation utilizes hundreds of processors efficiently and delivers much higher performance and speedups than reported earlier. These speedups are a result of our highly efficient parallel algorithm for solving a linear symmetric positive definite system using Cholesky factorization. We also evaluate a number of ordering algorithms for sparse matrix factorization in terms of their suitability for parallel Cholesky factorization.",
Learning by adapting representations in genetic programming,"Machine learning aims towards the acquisition of knowledge, based either on experience from the interaction with the external environment or by analyzing the internal problem-solving traces. Genetic programming (GP) has been effective in learning via interaction, but so far there have not been any significant tests to show that GP can take advantage of its own search traces. This paper demonstrates how an analysis of the evolution trace enables the genetic search to discover useful genetic material and to use it in order to accelerate the search process. The key idea is that of genetic material discovery which enables a restructuring of the search space so that solutions can be much more easily found.",
Logspace and logtime leaf languages,"Leaf languages were used in the context of polynomial time computation to capture complexity classes and to study machine-independent relativizations. In this paper, the expressibility of the leaf language mechanism is investigated in the contexts of logarithmic space and of logarithmic time computation.",
Knowledge model based approach in recognition of on-line Chinese characters,"A knowledge model-based OCR system is presented for the recognition of on-line connected stroke Chinese characters. In the approach, segment attributes are first extracted to characterize the segment sequence of an unknown character. Next, radical recognition based on model matching is adopted as the coarse classification to reduce the number of candidate characters before detailed matching. Finally, a deviation modeling method is proposed to recognize not only regular writing characters but also characters with stroke-order and stroke-number deviations. The effectiveness of the approach is verified by experiments on the recognition of on-line Chinese characters.",
Towards effective computer based education,"Computer based education (CBE) applications have enormous potential. Much literature has been published regarding this potential, overcoming technical difficulties and assessing effectiveness. However, a key and often ignored aspect is that CBE applications are unlikely to be effective unless learning theory is incorporated into the design phase. This paper discusses the relevant learning theory and develops a set of guidelines to assist in the design and refinement of effective CBE applications.",
From concrete forms to generalized abstractions through perspective-oriented analysis of logical relationships,"We believe concreteness, direct manipulation and responsiveness in a visual programming language increase its usefulness. However, these characteristics present a challenge in generalizing programs for reuse, especially when concrete examples are used as one way of achieving concreteness. In this paper, we present a technique to solve this problem by deriving generality automatically through the analysis of logical relationships among concrete program entities from the perspective of a particular computational goal. Use of this technique allows a fully general form-based program with reusable abstractions to be derived from one that was specified in terms of concrete examples and direct manipulation.",
Partitioning and retiming of multi-dimensional systems,"The use of massive parallelism in solving Partial Differential Equations (PDEs) has been studied for a long time. Fettweis and Nitache (1991) introduced a new method of transforming a PDE problem in a set of computational nodes represented by wave digital filters working in a multidimensional environment. Those computational nodes may not be mapped one-to-one to processor elements. After the nodes are partitioned into blocks, this paper introduces the concept of transforming such blocks to multidimensional data flow graphs, and an algorithm to obtain a final execution schedule with an optimal performance by using multidimensional retiming. The method is applicable to any uniformly represented data dependence graph and the Fettweis and Nitache method was chosen as an interesting example of its application.",
On the secret-key rate of binary random variables,"Consider a scenario in which two parties Alice and Bob as well as an opponent Eve receive the output of a binary symmetric source (e,g. installed in a satellite) over individual, not necessarily independent binary symmetric channels. Alice and Bob share no secret key initially and can only communicate over a public channel completely accessible to Eve. The authors derive a lower bound on the rate at which Alice and Bob can generate secret-key bits about which Eve has arbitrarily little information, This lower bound is strictly positive as long as Eve's binary symmetric channel is not perfect, even if Alice's and Bob's channels are by orders of magnitude less reliable than Eve's channel.",
Multiclass replicated data management: exploiting replication to improve efficiency,"Research efforts in replication-control protocols primarily use replication as a means of increasing availability in distributed systems. It is well-known, however, that replication can reduce the costs of accessing remotely-stored data in distributed systems. We contribute a classification of replicas and a replication-control protocol which introduce the availability benefits of replication and, at the same time, exploit replication to improve performance, by reducing response time. Each replica class has different consistency requirements. Metareplicas keep track of up-to-date replicas for recently-accessed objects and help exploit data-reference localities. Thus they allow many transaction operations to execute synchronously at only a single (and often local) replica. Pseudoreplicas are nonpermanent replicas that facilitate ""localized execution"" of transaction operations. True replicas are ordinary, permanent replicas as used in other replication schemes. For many commonly occurring replication scenarios, the protocol outperforms both replication-control protocols in the literature and nonreplicated systems, while offering the availability benefits of replication.",
Adaptive parallelism under Equus,"The authors describe adaptively parallel computations under Equus (T. Kindberg, 1990; A. V. Sahiner, 1991). These computations execute on a processor pool, and expand and contract as the number of processor nodes allocated to them varies over their run-time. They are based upon a hierarchical master-worker structure. The number of worker processes changes with the number of allocated nodes, and so does the number of processes that act as servers to them (such as the masters). The authors use an image-processing example to describe how workers and servers are added and withdrawn at run-time. The affected processes are synchronised, communication linkages are changed, and in some cases, state is transferred between them. Reconfigurations are transparent to worker (and other client) processes, but not all can be made transparent to servers.",
Simulating fail-stop in asynchronous distributed systems,"The fail-stop failure model appears frequently in the distributed systems literature. However, in an asynchronous distributed system, the fail-stop model cannot be implemented. In particular, it is impossible to reliably detect crash failures in an asynchronous system. In this paper, we show that it is possible to specify and implement a failure model that is indistinguishable from the fail-stop model from the point of view of any process within an asynchronous system. We give necessary conditions for a failure model to be indistinguishable from the fail-stop model, and derive lower bounds on the amount of process replication needed to implement such a failure model. We present a simple one-round protocol for implementing one such failure model, which we call simulated fail-stop.",
Using tries to eliminate pattern collisions in perfect hashing,"Many current perfect hashing algorithms suffer from the problem of pattern collisions. In this paper, a perfect hashing technique that uses array-based tries and a simple sparse matrix packing algorithm is introduced. This technique eliminates all pattern collisions, and, because of this, it can be used to form ordered minimal perfect hashing functions on extremely large word lists. This algorithm is superior to other known perfect hashing functions for large word lists in terms of function building efficiency, pattern collision avoidance, and retrieval function complexity. It has been successfully used to form an ordered minimal perfect hashing function for the entire 24481 element Unix word list without resorting to segmentation. The item lists addressed by the perfect hashing function formed can be ordered in any manner, including alphabetically, to easily allow other forms of access to the same list.","Information retrieval,
Data structures,
Databases,
CD-ROMs,
Sparse matrices,
Collision avoidance,
Computer science,
Dictionaries,
Catalogs,
Immune system"
FAST: a functional algorithm simulation testbed,"We extend the practical range of simulations of parallel executions by ""functional algorithm simulation,"" that is, simulation without actually performing most of the numerical computations involved. We achieve this by introducing a new approach for generating and collecting communication and computation characteristics for a class of parallel scientific algorithms. We describe FAST (Fast Algorithm Simulation Testbed), a prototype system that we developed to implement and test our approach. FAST overcomes some of the difficulties imposed by the very high complexity of interesting scientific algorithms, collects profile information representative of the algorithms rather than the underlying mapping strategies and hardware design choices, and allows a performance assessment of parallel machines with various sites and different interconnection schemes.","Testing,
Computational modeling,
Concurrent computing,
Parallel machines,
Computer simulation,
Character generation,
Algorithm design and analysis,
Parallel algorithms,
Computer science,
Virtual prototyping"
Storage alternatives for video service,"In the next decade, video-on-demand (VOD) services will be widely available to customers and potentially highly profitable to service providers. In order to provide access to many movies, video storage servers may contain not only magnetic disks but also high-capacity tertiary storage devices. In this paper, we study two storage device alternatives: an array of magnetic disks and an array of magnetic tapes. Magnetic disk arrays provide high-bandwidth, low-latency retrieval and moderate storage capacity at high cost. Magnetic tape arrays provide low-bandwidth, high-latency retrieval and high storage capacity at low cost. We evaluate these two storage system alternatives to determine the number of users that each can support. Our simulations show that magnetic disk arrays can support considerably more users, at a lower cost per user, than magnetic tape arrays.",
The calculation of signal stable ranges in combinational circuits,"The estimation of signal stable ranges in a combinational circuit is an important issue for determining clock time in a synchronous system. An optimal clocking period time highly depends on the accuracy of the shortest path length as well as the longest path length in a combinational circuit. In this paper, a sensitization criterion for the short path is first proposed. Based on this sensitization criterion, an accurate model for calculation of signal stable range can be created. This will allow the output stable range of a gate to be the union of its inputs when the input leads hold a controlling value, rather than to be always the intersection. Then, an LS-algorithm for calculation of signal stable ranges is presented in which both the sensitizable shortest path and the sensitizable longest path are considered. It avoids the exhaustive search by tracing the path sensitization and eliminates some conservative restriction to get more accurate results in a more efficient way, compared to the previous approaches. The speedup and the improved accuracy of the proposed LS-algorithm showed promising experimental results.",
The discrete-time H/sub /spl infin// control problem with strictly proper measurement feedback,"This paper is concerned with the discrete-time H/sub /spl infin// control problem with strictly proper measurement feedback. We derive necessary and sufficient conditions for the existence of a strictly proper compensator which achieves a given H/sub /spl infin// norm bound. Note that contrary to the continuous time, in discrete time there might exist a suitable proper compensator but no suitable strictly proper compensator. Finally, we give an explicit formula for one controller which achieves this bound.",
Complete exchange and broadcast algorithms for meshes,"Two complete exchange algorithms for meshes are given. The modified quadrant exchange algorithm is based on the quadrant exchange algorithm and it is well suited for square meshes with a power of two rows and columns. The store-and-forward complete exchange algorithm is suitable for meshes of arbitrary size. A pipelined broadcast algorithm for meshes is also presented. This new algorithm, called the double hop broadcast, can broadcast long messages at slightly lower cost than the edge-disjoint fence algorithm because it uses routing trees of lower height. This shows that there is still room for improvement in the design of pipelined broadcast algorithms for meshes.",
A reconfigurable analog fuzzy logic controller,"The paper presents a new reconfigurable analog fuzzy logic controller. The hardware implementation was realized in CMOS analog technology. The shape of the membership function and the inference rules can be changed by the user through additional pins. The required area for a 2 inputs, 13 rules, and 1 output using a 2.4 /spl mu/m CMOS technology is 3.2/spl times/3.9 mm/sup 2/.",
IP over connection-oriented networks and distributional paging,"Next generation wide area network are very likely to use connection-oriented protocols such as Asynchronous Transfer Mode (ATM). For the huge existing investment in current IP networks such as the Internet to remain useful, me must devise mechanisms to carry IP traffic over connection-oriented networks. A basic issue is to devise holding policies for virtual circuits carrying datagrams. In this paper we consider two variants of the paging problem that arise in the design of such holding policies. In the IP-paging problem the page inter-request times are chosen according to independent distributions. For this model we construct a very simple deterministic algorithm whose page fault rate is at most 5 times that of the best online algorithm (that knows the inter-request time distributions). We also show that some natural algorithms for this problem do not have constant competitive ratio. In distributional paging the inter-request time distributions may be dependent, and hence any probabilistic model of page request sequences can be represented. We construct a simple randomized algorithm whose page fault rate is at most 4 times that of the best online algorithm.",
A point of contact between computer science and molecular biology,"Molecular biology is rapidly becoming a data-rich science with extensive computational needs. The sheer volume of data poses a serious challenge in storing and retrieving biological information, and the rate of growth is exponential. Linking the heterogeneous data libraries of molecular biology, organizing its diverse and interrelated data sets, and developing effective query options for its databases are all areas for cross-fertilization between molecular biology and computer science. However, even the apparently simple task of analyzing a single sequence of DNA requires complex collaboration. For several years, we have been developing a computer toolkit for analyzing DNA sequences. The biology of gene regulation in mammals has driven the design of the sequence comparison toolkit to emphasize space-efficient algorithms with a high degree of sensitivity and has profoundly affected choice of tools and the development of algorithms. We sketch the biology of this class of problem and show how it specifically drives the software development. The main components of this toolkit are outlined.",
Compiler assisted fault detection for distributed-memory systems,"Distributed-memory systems provide the most promising performance to cost ratio for multiprocessor computers due to their scalability. However the issues of fault detection and fault tolerance are critical in such systems since the probability of having faulty components increases with the number of processors. We propose a methodology for fault detection through compiler support. More specifically, we augment the single-program multiple-data (SPMD) execution model to duplicate selected data items in such a way that during execution, whenever a value of a duplicated data is computed, the owners of the data are tested. The proposed compiler assisted fault detection technique does not require any specialized hardware and allows for a selective choice of redundancy at compile time.",
Experiments with various neural architectures for handwritten character recognition,"In this research we address the problem of recognition of isolated handwritten characters. Handwritten character recognition has been a topic of research for a long period of time. It has been argued that this problem is difficult to model using classical modeling techniques, and that neural networks offer a new perspective to approaching this problem. This paper outlines the experimental evidence we have compiled while investigating possible approaches to handwritten character recognition. It is the hypothesis of our approach that handwritten character recognition is a pattern recognition problem and that there exists a set of unique features in the data which can be used for classification.",
Parallel hash-based join algorithms for a shared-everything environment,"Analyzes the costs, and describes the implementation, of three hash-based join algorithms for a general purpose shared-memory multiprocessor. The three algorithms considered are the hashed loops, GRACE and hybrid algorithms. We also describe the results of a set of experiments that validate the cost models presented and demonstrate the relative performance of the three algorithms.",
A comparison of techniques used for mapping parallel algorithms to message-passing multiprocessors,"This paper presents a comparison study of popular clustering and mapping heuristics which are used to map task-flow graphs to message-passing multiprocessors. To this end, we use task-graphs which are representative of important scientific algorithms running on data-sets of practical interest. The annotation which assigns weights to nodes and edges of the task-graphs is realistic. It reflects current trends in processor, communication channel, and message-passing interface technology and takes into consideration hardware characteristics of state-of-the-art multiprocessors. Our experiments show that applying realistic models for task-graph annotation affects the effectiveness and functionality of clustering and mapping techniques. Therefore, new heuristics are necessary that will take into account more practical models of communication costs. We present modifications to existing clustering and mapping algorithms which improve their efficiency and running-time for the practical models adopted.",
VORTEX-PILOT: a top-down approach for AUVs mission telerobotics language,"This paper presents the complete high level control architecture with real time mission/task/sensor-based algorithms and operator interface language, useful to prepare the next generation of AUV and ROV mission programming, developed and tested on the VORTEX (Versatile and Open subsea Robot for Technical Experiments) at IFREMER Subsea Robotics Lab with INRIA (French institute for computer science and Automation) and UBO-LIMI Lab (University of Brest Fr). In this global architecture we focus highly on the PILOT online, interpreted, high level telerobotics language for mission programming and supervision. A top-down approach, conducted between IFREMER and the LIMI-UBO Lab, focuses on the development of an interpreted language for mission supervisory management and specification. This language, PILOT (Programming and Interpreted Language Of actions for Telerobotics), is a visual, imperative, interpreted and high level language. Its primitives take into account the operator interventions and the external events.",
A visual interaction system using real-time face tracking,"This paper describes a new method for pose estimation of a human face moving abruptly in the real world and its hardware implementation for real-time operation. The virtues of this method are the use of very simple calculation, the correlation among multiple model images, and the absence of any facial features such as facial organs, so that it is very robust and can be easily implemented with simple hardware.",
Plane wave scattering by dielectric elliptic cylinder coated with nonconfocal dielectric,"The scattering cross section of a dielectric elliptic cylinder coated with nonconfocal dielectric is investigated. An exact dual series solution based on the boundary value method and employing the addition theorem of Mathieu functions is derived. Both transverse magnetic and transverse electric cases are considered. The infinite series' of the solution are truncated after a finite number of terms to generate numerical data. A convergence check is built into the computer program, and the geometrical dimensions considered are within the low-frequency range. It is shown that this solution is much more general than all other solutions given before because it can handle a variety of scattering geometries.","Dielectrics,
Magnetic cores,
Geometry,
Radar applications,
Radar scattering"
Cellular automata based VLSI architecture for computing multiplication and inverses in GF(2/sup m/),"Finite fields have proved to be very useful in error correcting codes, combinatorial design and many cryptographic applications. Finding multiplication of any two elements in GF(2/sup m/) and finding the inverse of an element are most difficult and time consuming operations. In this paper, new algorithms based on cellular automata operations for performing fast multiplication and inversion in GF(2/sup m/) are presented. The new design is highly parallel, modular and well-suited for VLSI implementation.","Very large scale integration,
Computer architecture,
Galois fields,
Integrated circuit interconnections,
Algorithm design and analysis,
Error correction codes,
Pipelines,
Computer science,
Automata,
Digital circuits"
The state complexity of trellis diagrams for a class of generalized concatenated codes,"We discuss the state complexity of trellis diagrams for a class of generalized concatenated codes. The maximum number of states in the 64-section minimal trellis diagram for all the extended BCH codes of length 64 which are permuted by using the bases shown previously by Kasami et al. (1993), are the same as those obtained by Vardy and Be'ery (1993), where bit orderings were found by using DS structure and computer search. We construct several decomposable codes for which a multistage decoding up to the minimum distance can be employed. The dimensions of constructed codes are 47, 43 and 24 for length 63, and 52 and 32 for length 72. We also construct codes of length 64 as shortened codes of the codes with length 72.",
Efficient barriers for distributed shared memory computers,"Barrier algorithms are central to the performance of numerous algorithms on scalable, high-performance architectures. Numerous barrier algorithms have been suggested and studied for non-uniform memory access (NUMA) architectures, but less work has been done for cache only memory access (COMA) or attraction memory architectures such as the KSR-1. We present two new barrier algorithms that offer the best performance we have recorded on the KSR-1 distributed cache multiprocessor. We discuss the trade-offs and the performance of seven algorithms on two architectures. The new barrier algorithms adapt well to a hierarchical caching memory model and take advantage of parallel communication offered by most multiprocessor interconnection networks. Performance results are shown for a 256-processor KSR-1 and a 20-processor Sequent Symmetry.",
Fault-tolerant algorithms for fair interprocess synchronization,"The implementation of nondeterministic pairwise synchronous communication among a set of asynchronous processes is modeled as the binary interaction problem. The paper describes an algorithm for this problem that satisfies a strong fairness property that guarantees freedom from process starvation. This is the first algorithm for binary interactions with strong fairness whose message cost and response time are independent of the total number of processes in the system. The paper also describes how the fair algorithm may be extended to tolerate detectable fail-stop failures. Finally, we show how any solution to the dining philosophers problem can be embedded to design a fair algorithm for binary interactions. In particular, this embedding is used to derive a fair algorithm that can cope with undetectable fail-stop failures.",
Multiple fault detection in parity checkers,"Parity checkers are widely used in digital systems to detect errors when systems are in operation. Since parity checkers are monitoring circuits, their reliability must be guaranteed by performing a thorough testing. In this work, multiple fault detection of parity checkers is investigated. We have found that all multiple stuck-at faults occurring on a parity tree can be completely detected using test patterns provided by the identity matrix plus zero vector. The identity matrix contains 1's on the main diagonal and 0's elsewhere; while the zero vector contains 0's. The identity matrix vectors can also detect all multiple general bridging faults, if the bridgings result in a wired-AND effect. However, test patterns generated from the identity matrix and binary matrix are required to detect a majority of the multiple bridging faults which yield wired-OR connections. Note that the binary matrix contains two 1's at each column of the matrix.",
Human-computer interface design in the software lifecycle,"Because human-computer interface design is traditionally included in computer science curricula as an advanced level topic, it is usually perceived as an add-on, and not well integrated into the software lifecycle. An holistic user-centred methodology for system design, one which has proved particularly appropriate for software development based on 4GLs, is described. By treating interface design as an integral component of the analysis and design, rather than an afterthought, successful systems can be produced for the more enlightened, demanding and assertive users of today's interactive systems. The methodology has been successfully applied as both a teaching tool and in real system development environments.",
Extremal solutions of inequations over lattices with applications to supervisory control,"We study the existence and computation of extremal solutions of a system of inequations defined over lattices. Using the Knaster-Tarski fixed point theorem, we obtain sufficient conditions for the existence of supremal as well as infimal solution of a given system of inequations. Iterative techniques are presented for the computation of the extremal solutions whenever they exist, and conditions under which the termination occurs in a single iteration are provided. These results are then applied for obtaining extremal solutions of various inequations that arise in computation of maximally permissive supervisors in control of logical discrete event systems (DESs). Thus our work presents a unifying approach for computation of supervisors in a variety of situations.",
"Computational structures technology in Europe. Past, present, and future","Europe's universities and industries, computational structures technology is increasingly recognized as an indispensable tool for structural analysis and design. It is part of the store of numerical computer-algorithmic strategies that scientists and engineers use to analyze complex physical phenomena. Aerospace, civil, and mechanical engineers use this approach mainly when they are developing new methods or products, and cannot rely exclusively on experience or intuition. Computational techniques are also well established in the aerospace and automotive industries, not only in research and development but also in manufacturing. The Commission of the European Communities is promoting a wide range of computer-based industrial technologies through R & D programs, including research directed toward preserving and protecting the environment.",
Algorithmic number theory-the complexity contribution,"Though algorithmic number theory is one of man's oldest intellectual pursuits, its current vitality is perhaps unrivalled in history. This is due in part to the injection of new ideas from computational complexity. In this paper, a brief history of the symbiotic relationship between number theory and complexity theory will be presented. In addition, some of the technical aspects underlying 'modern' methods of primality testing and factoring will be described. Finally, an extensive lists of open problems in algorithmic number theory will be provided.",
Design Methodology Management Using Graph Grammars,"In this paper, we present a design methodology management system, which assists designers in selecting a suitable design process and invoking the selected sequence of tools on the correct versions of design data. We introduce a formal graph representation of design methodologies in which nodes represent either tasks or design data. Using a graph grammar, nodes representing abstract tasks are replaced by graphs of less abstract tasks and intermediate specifications. Graph grammars enable us to concisely and flexibly describe a large class of methodologies. Often there are several alternative methodologies or tools available for some subtasks, leading to different results. Our system utilizes automated control agents, in combination with user interaction, to select among methodologies and invoke tools. Multiple alternatives can easily be explored simultaneously.",
Single-link and time communicating finite state machines,"We propose two variants of the classical communicating finite state machines (CFSMs) model, single-link communicating finite state machines (SLCFSMs) and time communicating finite state machines (TCFSMs). For SLCFSMs the notion of well-formedness, which provides a necessary condition for SLCFSMs to be free of some logical errors, is proposed. For TCFSMs, it is argued that they are more suitable for modeling delay-sensitive distributed algorithms/communication protocols. Two practical communication protocols, a token ring and a sliding window protocol, are modeled using TCFSMs.",
Software metrics for object-oriented designs,"We discuss software metrics for object-oriented programs. Two kinds of metrics are defined to evaluate the design and implementation. One is class-based and that evaluates design of classes. The second type measures the class design structure of the program. Metrics are defined to evaluate key features of object-oriented design, like encapsulation, inheritance, polymorphism, specialization of classes, etc. The metrics can ""quantify"" the design, so that the programmer can evaluate the design.",
Three Domain Voting In Real-time Distributed Control Systems,,
Optical interprocessor communication protocols,"We study routing properties of an optical communication architecture for parallel computing. The building block of the system is a model called the Optical Communication Parallel Computer (OCPC). The units of this computer (processors with local memory) communicate with each other by transmitting messages. A processor can transmit a message to any other processor, and to the same processor neither transmission is successful and retransmission must occur. We also consider a 2-stage processor organization scheme, called the 2-stage OCPC, where processors are organized in a two-dimensional array whose rows and columns consist of OCPCs. The problem that motivated this work is the desire to program these architecture models using high-level, general-purpose, and user-friendly programming languages. The languages should be powerful enough to support features like concurrent memory access, virtual processors, barrier synchronization, and both automatic and explicit memory allocation. Such features are captured by the Parallel Random Access Machine and by Valiant's Bulk-Synchronous Parallel Computer model. Both of these models can be implemented using a certain communication pattern called h-relations. We discuss protocols for realizing h-relations on the OCPC and 2-stage OCPC. The protocols primarily deal with contention resolution since contention in an optical system can inhibit message transmission.",
An artist's studio: a metaphor for modularity and abstraction in a graphical diagramming environment,"The paper describes a graphic editor in a visual diagramming environment. Its management tools permit heterogeneous users to work with large graphic diagrams. Modularization is provided using the metaphor of an artist's studio, complete with a gallery of compositions, and sketches. Visibility control mechanisms are proposed as a method to provide information hiding capabilities at various levels of granularity. Abstraction mechanisms are provided by syntactic, semantic and pragmatic zooming concepts, for controlling the level of various types of detail visible to the user. Structured naming is another abstraction mechanism, by which graphical references may be made to visual objects. The studio paradigm has been implemented on a pen computer using the PenPoint operating system, as a part of the Hyperflow visual programming environment.",
McColm's conjecture [positive elementary inductions],"G. McColm (1990) conjectured that positive elementary inductions are bounded in a class K of finite structures if every (FO+LFP) formula is equivalent to a first-order formula in K. Here (FO+LFP) is the extension of first-order logic with the least fixed point operator. We disprove the conjecture. Our main results are two model-theoretic constructions, one deterministic and the other randomized, each of which refutes McColm's conjecture.",
Over-the-cell routing algorithms for industrial cell models,"The effective utilization of over-the-cell areas for routing leads to minimization or elimination of channel areas in standard cell designs. In this paper, we present two results. Firstly, we develop a new cell model called Target Based Cell (TBC) designs. Standard cells in TBC designs have terminals in the form of vertical segments in M1 layer. The exact locations for placing interconnect contacts on the targets are determined by the routing algorithms. Cell widths in TBC designs are smaller than the widths in existing cell designs and have improved over-the-cell routing flexibility. Secondly, we develop an efficient router for TBC designs which includes two key features; an optimal O(KL) algorithm (where K, and L are number of cell rows and layout width respectively) for assigning over-the-cell area to each channel, so as to minimize total layout height, and an irregular boundary channel HV-HVH-HV router for over-the-cell and channel areas between terminal rows which optimally utilizes the over-the-cell area.",
Gamma-ray spectroscopic system for remote detection and monitoring of fissile materials,"The scientific and methodical aspects of system design for radiation monitoring of nuclear weapons for treaty verification and for customs monitoring to prevent contraband of the fissile materials are considered in this paper. The /spl gamma/-ray spectroscopic systems for remote detection of nuclear weapons and for the monitoring of the number of warheads within a missile nose cone are considered, and the results of experimental test and computer simulations of this system are given.",
A functional specification language for instruction set architectures,"Application-specific programmable processing systems consist of not only a processor, but also the software that runs on it. In order to support development of such systems, a design environment must support both hardware and software development. Unfortunately, there are no specification languages for processors that are suitable for such dual use. Therefore, we have designed a functional-style language that is specifically intended for describing instruction sets; its functional nature allows it to describe the result that an instruction produces without having to specify the mechanism of operation. This property is key to allowing dual use of the specification, since it will not be biased towards either hardware or software development. We are constructing a design environment based on our language.",
Auditory stream segregation based on oscillatory correlation,"Auditory segmentation is critical for complex auditory pattern processing. We present a generic neural network framework for auditory pattern segmentation. The network is a laterally coupled two-dimensional neural oscillators with a global inhibitor. One dimension represents time and another one represents frequency. We show that this architecture can, in real-time, group auditory features into a segment by phase synchrony and segregate different segments by desynchronization. The network demonstrates the phenomenon that auditory stream segregation critically depends on the rate of presentation. The neuroplausibility and possible extensions of the model are discussed.",
Using Warp to control network contention in Mermera,"Parallel computing on a network of workstations can saturate the communication network leading to excessive message delays and consequently poor application performance. We examine empirically the consequences of integrating a flow control protocol, called Warp control, into Mermera, a software shared memory system that supports parallel computing on distributed systems. For an asynchronous iterative program that solves a system of linear equations, our measurements show that Warp succeeds in stabilizing the network's behavior even under high levels of contention. As a result, the application achieves a higher effective communication throughput, and a reduced completion time. In some cases, however, Warp control does not achieve the performance attainable by fixed size buffering when using a statically optimal buffer size. Our use of Warp to regulate the allocation of network bandwidth emphasizes the possibility for integrating it with the allocation of other resources, such as CPU cycles and disk bandwidth, so as to optimize overall system throughput, and enable fully-shared execution of parallel programs.",
A linear-time online task assignment scheme for multiprocessor systems,"A new online task assignment scheme is presented for multiprocessor systems where individual processors execute the rate-monotonic scheduling algorithm. The computational complexity of the task assignment scheme grows linearly with the number of tasks, and its performance is shown to be significantly better than previously existing schemes. The superiority of the assignment scheme is achieved by a new schedulability condition derived for the rate-monotonic scheduling discipline.",
Partial matching of 3-D objects in CAD/CAM systems,"This application presents an algorithm for model-based partial recognition of CAD/CAM objects in 3-D space. Its main goal is to serve as an industrial application in robotic aided production lines and to facilitate the programming of process robots. Hence, a special emphasis is given to its practical capabilities. The recognition algorithm is invariant to the 3-D similarity transformation: rotation (& reflection), translation and scale. The objects can be represented as wireframes, solids or surfaces. The algorithm is based on the geometric hashing method and exploits specific CAD/CAM geometric constraints.",
,,
A binding protocol for distributed shared objects,"A number of actions, collectively known as binding, prepare a reference for invocation of its target: locating the target, setting up a connection, checking access rights and concurrency control state, type-checking, instantiating a proxy, etc. Existing languages or operating systems support only a single binding policy, that cannot be tailored to object-specific semantics for the management of distribution, replication, or persistence. We propose a general binding protocol covering the above needs; the protocol is simple (a single RPC and one upcall at each end) but recursive; however the recursion can be terminated at any point, trading off simplicity and performance against completeness. This comprehensive, unified protocol is capable of supporting different languages and object models, and may be tailored to support various policies in a simple manner.",
Image segmentation by directed region subdivision,"In this paper, an image segmentation method based on directed image region partitioning is proposed. The method consists of two separate stages: a splitting phase followed by a merging phase. The splitting phase starts with an initial coarse triangulation and employs the incremental Delaunay triangulation as a directed image region splitting technique. The triangulation process is accomplished by adding points as vertices one by one into the triangulation. A top-down point selection strategy is proposed for selecting these points in the image domain of grey-value and color images. The merging phase coalesces the oversegmentation, generated by the splitting phase, into homogeneous image regions. Because images might be negatively affected by changes in intensity due to shading or surface orientation change, the authors propose homogeneity criteria which are robust to intensity changes caused by these phenomena for both grey-value and color images. Performance of the image segmentation method has been evaluated by experiments on test images.",
Simulation of heterogeneous networks,"This paper presents a simulation tool to help the network designer to identify the optimum network design parameters for a heterogeneous network environment. The tool consists of two parts: simulation modules and a user interface. The simulation modules are created using enhancements of an asynchronous, event-driven C-function library to enable the simulation of heterogeneous scenarios. In the future, this module will be distributed to speed-up the simulation runtime. The user interface helps the user to create the design of the network to be simulated. By this modular concept of distributed simulation modules and user interface the optimum network can be found out without much knowledge of the underlying simulation details.",
Finitary fairness,"Fairness is a mathematical abstraction: in a multiprogramming environment, fairness abstracts the details of admissible (""fair"") schedulers; in a distributed environment, fairness abstracts the speeds of independent processors. We argue that the standard definition of fairness often is unnecessarily weak and can be replaced by the stronger, yet still abstract, notion of finitary fairness. While standard weak fairness requires that no enabled transition is postponed forever, finitary weak fairness requires that for every run of a system there is an unknown bound k such that no enabled transition is postponed more than k consecutive times. In general, the finitary restriction fin(F) of any given fairness assumption F is the union of all w-regular safety properties that are contained in F. The adequacy of the proposed abstraction is demonstrated in two ways. Suppose that we prove a program property under the assumption of finitary fairness. In a multiprogramming environment, the program then satisfies the property for all fair finite-state schedulers. In a distributed environment, the program then satisfies the property for all choices of lower and upper bounds on the speeds (or timings) of processors.",
Experience with a project-based approach to teaching software engineering,"The paper reviews the experiences of teaching software engineering at the University of Adelaide using a group-based project over the course of one semester (15 weeks). Students are required to design, prototype, implement, maintain and document the software to satisfy the requirements defined by the lecturing staff who endeavour to simulate a customer as closely as possible. The paper focuses on the project and the benefits it offers rather than the content of the lectures.",
A possibilistic interval constraint problem: fuzzy temporal reasoning,"A fuzzy temporal algebra is defined for solving qualitative interval constraint problems as formulated by Allen (1983) and applied in temporal reasoning for applications such as planning. The problem definition is expanded as possibilities temporal reasoning to allow exploitation a partial or indefinite information. The constraints are binary relations on intervals and consist a sums of thirteen atomic relations (precedes, during, meets, etc.). Fuzzy operations of composition, union, and intersection are defined, and Allen's path consistency (3-consistency or triangle operation) algorithm for constraint propagation is generalized.",
Test generation for stuck-at and gate-delay faults in sequential circuits: a mixed functional/structural method,This paper presents a mixed approach for sequential circuit test pattern generation employing the accuracy of structural algorithms and the speed of a pattern generator working at the functional level. The new strategy selects from the State Transition Graph of a Finite State Machine the appropriate edges that allow a functional test pattern generator (FSMTest) to build test sequences covering 100% of the detectable single stuck-at and gate-delay faults. Experiments and comparisons are presented to justify the proposed test strategy.,
Design and performance evaluation of a distributed eigenvalue solver on a workstation cluster,"Clusters of high-performance workstations are emerging as promising platforms for parallel scientific computing. The paper describes an eigenvalue solver for symmetric tridiagonal matrices, as implemented on a cluster of workstations using two different interprocess communication packages, PVM and P4. The algorithm is based on the split-merge technique, which uses Laguerre's iteration and exploits the separation property of rank two splitting in order to create subtasks that can be solved independently. A performance study that compares the distributed, parallel split-merge algorithm to a parallel version of the well-known bisection algorithm, over standard matrix types, demonstrates the performance advantage of the new algorithm and its cluster implementation.",
Dark current induced in large CCD arrays by proton-induced elastic reactions and single to multiple-event spallation reactions,Computer simulations of the non-ionizing energy loss deposited in sensitive volumes as a result of proton-induced spallation reactions agree with analytic models for large sensitive volumes exposed to high fluence. They predict unique features for small volumes and low-fluence exposures which are observed in exposures of large arrays of CCD pixels. Calculations of the number of spallation reactions per pixel correlate with the recently reported relative frequency of switching dark-current states.,
On automatic filtering of multilingual texts,"An emerging requirement to sift through the increasing flood of text information has led to the rapid development of information filtering technology in the past five years. This study introduces novel approaches for filtering texts regardless of their source language. We begin with a brief description of related developments in text filtering and multilingual information retrieval. We then present three alternative approaches to selecting texts from a multilingual information stream which represent a logical evolution from existing techniques in related disciplines. Finally, a practical automated performance evaluation technique is proposed.",
Object placement in parallel object-oriented database systems,"Parallelism is a viable solution to constructing high performance object-oriented database systems. In parallel systems based on a shared-nothing architecture, the database is horizontally declustered across multiple processors, enabling the system to employ multiple processors to speedup the execution time of a query. The placement of objects across the processors has a significant impact on the performance of queries that traverse a few objects. The paper describes and evaluates a greedy algorithm for the placement of objects across the processors of a system. Moreover, it describes two alternative availability strategies and quantifies their performance tradeoff using a trace-driven simulation study.",
A case study of scientific application I/O behavior,"Characterizing the I/O demands of scientific applications is an integral part of the solution to the I/O bottleneck problem in high performance systems. Knowledge of the type, volume, and frequency of events that occur in common patterns of I/O activity can provide insight into the policies and mechanisms needed at all levels of the memory hierarchy. This paper is a detailed case study of one scientific application's dynamic I/O behavior. Results from this study show that with appropriate measurement tools, dynamic I/O behavior can be characterized and regular, recurring patterns of I/O activity can be isolated.","Computer aided software engineering,
Application software,
High performance computing,
Performance analysis,
Laboratories,
Computer science,
Supercomputers,
Data visualization,
Multiprocessor interconnection networks,
Operating systems"
Using a multi-FPGA based rapid prototyping board for system design at the undergraduate level,"The rapid prototyping system created at North Carolina State University consisting of an FPGA-based board called the AnyBoard, was revised to use commercial design entry methods rather than board specific software. The resulting environment is a promising tool for teaching system design at the undergraduate level. This paper presents our experiences to date with this new tool.",
Narrow band frame multiresolution analysis with perfect reconstruction,"We define the notion of a frame multiresolution analysis (FMRA), and analyze its associated subband coding system. An important feature of this 2-band system is that the elements of the filter bank can be narrow band, unlike the half-band nature of a classical 2-band perfect reconstruction filter bank (PRFB). In the class of multiresolution structures, it is unnecessary to introduce FMRAs for perfect reconstruction since this call be achieved by MRAs. However, for FMRAs with narrow band filters, we achieve noise suppression in the subband coding/quantization process simultaneously with reconstruction of a given narrow band signal. As with MRAs, this reconstruction is perfect if channel/quantization noise is discounted and if the signal is in one of the subspaces defining the FMRA.",
Dynamic and static packet routing on symmetric communication networks,"For dynamic routing on any vertex and edge symmetric topology we relate network contention to the packet generation rate, and the degree and average distance of the underlying topology. As a direct consequence we show that the star graph becomes saturated as the packet generating rate approaches one. Also, for graphs of approximately the same order, the average delay of a packet routed on an alternating-group graph is smaller than the corresponding delays on the hypercube and the star graph. Similar conclusions are drawn from comparisons based on various static permutation routing problems. Our theoretical results compare favorably with simulations of considerably large systems.",
Relation between surface impedance and bandwidth of hard surface,"Shows that the bandwidth of a hard surface is always inversely proportional to the diameter of the guiding structure. This seems to be a general relation that is independent of the way the hard surface has been realised. The corresponding relation for an open hard surface seems to be that the bandwidth is inversely proportional to the length of it. These two relations limit applications of the hard surfaces to structures that have small such critical dimensions. Such applications of hard surfaces with small critical dimensions have been experimentally verified and show reasonable bandwidths (10-2046). The present study has also shown that the bandwidth can be improved if materials with high wave impedances are used to realise the hard surface. Finally, it is mentioned that the hard strut can represent a useable model in order to experimentally test different realisations of hard surfaces. Theoretical searches for such surfaces can be done with computer codes that can model plane wave reflection from arbitrary single or double periodic surfaces. The paper considers the frequency characteristics of the hard surfaces when the surface is applied for horn antennas and waveguides.",
A modular approach in the simulation of high speed networks,"The advent of high speed networks such as ATM networks has led to an important role for simulation, since exact analysis is difficult in most cases. This paper describes a modular approach in the modeling and simulation of ATM networks and with other networks such as FDDI networks. The OPNET graphics simulation tool is used as the underlying platform of the ATM simulator. The various protocol layers of the ATM network and any control functions that need to be studied are simulated in well defined modules. The main benefit of this approach is that performance issues can be studied independently, and a large number of scenarios constructed and analyzed easily. The detailed simulation model for an ATM network using OPNET is presented, together with a brief/summary of some initial results.",
Design considerations of an animal PET scanner utilizing ESO scintillators and position sensitive PMTs,"Various parameters relevant to the design of a small scale PET scanner were studied both experimentally and by computer simulation. LSO (cerium-doped lutetium oxyorthosilicate) was compared with EGO, using a position sensitive PMT (PSPMT). (Compared to EGO, LSO has 8 times faster decay time, 4 times the scintillation light output, but 14% lower attenuation coefficient for 511 keV photons). The experiments with EGO and LSO involved: the crystal identification on the face of a PSPMT, measurement of the coincidence line-spread functions, and measurement of the sensitivity above the electronic noise. The resulting coincidence images clearly resolved each LSO crystal. EGS4 Monte Carlo simulations were used to estimate the efficiency, singles rate and mispositioning due to the depth of interaction for PET scanners with various sizes of LSO and EGO.",
On the robustness of functional equations,"Given a functional equation, such as /spl forall/x, y f(x)+f(y)=f(x+y), we study the following general question: When can the ""for all"" quantifiers be replaced by ""for most"" quantifiers without essentially changing the functions that are characterized by the property? When ""for most"" quantifiers are sufficient, we say that the functional equation is robust. We show conditions on functional equations of the form /spl forall/x, y F[f(x-y), f(x+y), f(x), f(y)]=0, where F is an algebraic function, that imply robustness. We then initiate a general study aimed at characterizing properties of functional equations that determine whether or not they are robust. Our results have applications to the area of self-testing/correcting programs-this paper provides results which show that the concept of self-testing/correcting has much broader applications than we previously understood. We show that self-testers and self-correctors can be found for many functions satisfying robust functional equations, including tan x, 1/1+cot x, Ax/1-Ax', cosh x.",
An optimal algorithm for maximum two planar subset problem /spl lsqb/VLSI layout/spl rsqb/,"The Two Row Maximum Planar Subset (TRMPS) problem asks for finding the maximum planar subset of nets, that can be routed between two rows of terminals an a cell row. This problem was first encountered by Gong, Liu, and Preas (1990). They declared it open, and presented an approximation algorithm for this problem. In this paper we show that TRMPS problem can be solved optimally in polynomial time, and we present an O(kn/sup 2/) algorithm to solve this problem. Our algorithm can also be extended to solve the TRMPS problem, in the presence of pre-routed nets, a chosen subset of nets, as well as for planar channel routing. We also apply our technique to obtain an improved approximation algorithm, for over the cell routing in middle terminal model standard cell layouts.",
A class of good characteristic polynomials for LFSR test pattern generators,"Linear Feedback Shift Registers (LFSRs) constitute a very efficient mechanism for generating pseudo-exhaustive or pseudo-random test sets for the built-in self-testing of digital circuits. However, a well-known problem with the use of LFSRs is the occurrence of linear dependencies in the generated patterns. In this paper, we show for the first time that the amount of linear dependencies can be controlled by selecting appropriate characteristic polynomials and reordering the LFSR cells. We identify a class of such polynomials which, by appropriate LFSR cell ordering, guarantees that a large ratio of linear dependencies cannot occur. Experimental results show significant enhancements on the fault coverage for pseudo-random testing and support the theoretical relation between minimization of linear dependencies and effective fault coverage.",
Transformable computers,The FPGA (Field Programmable Gate Array) was introduced in 1986 and was created for designers requiring a solution that bridged the gap between PALs (Programmable Array Logic) and ASICs (Application Specific Integrated Circuit). Exploitation of the FPGA's reconfigurable capabilities by independent researchers throughout the world has shown that computationally intensive software algorithms can be transposed directly into hardware design for extreme performance gain. This continuing research has spawned numerous developments in the arena of high performance supercomputing. We believe this research proves that transformable computing is the next frontier in computer science. These achievements as featured represent the building blocks of a new order of computers.,
A Bayesian framework for hierarchical relaxation,"Our aim in this paper is to develop the formal basis for hierarchical probabilistic relaxation. The adopted approach is an evidence combining one and relies on the specification of the relaxation process in terms of Bayesian probability distributions. The approach is novel and represents a considerable advance in extending the functionality of relaxation processes. In particular, since many tasks in computer vision are formulated in terms of hierarchies of increasingly abstract image representations, the technique holds out the promise of providing a framework in which constraints from different levels can be brought to bear objectively on the interpretation task.",
A Novel Audio/Video Synchronization Model and its Application in Multimedia Authoring System,,
A multilevel approach to surface response in dynamically deformable models,"Discretized representations of deformable objects, based upon simple dynamic point-mass systems, rely upon the propagation of forces between neighbouring elements to produce a global change in the shape of the surface. Attempting to make such a surface rigid produces stiff equations that are costly to evaluate with any numerical stability. This paper introduces a new multilevel approach for controlling the response of a deformable object to external forces. The user specifies the amount of flexibility or stiffness of the surface by controlling how the applied forces propagate through the levels of a multi-resolution representation of the object. A wide range of surface behaviour is possible, and rigid motion is attained without resort to special numerical methods. This technique is applied to the displacement constraints method of Gascuel and Gascuel (1992) to provide explicit graduated control of the response of a deformable object to imposed forces.",
EC/DSIM: a frontend and simulator for huge parallel systems,"This paper presents a new fast way to simulate large networks of computers. The method uses a frontend EC, which accepts a parallel C program and translates it into a program in an intermediate language for parallel system simulations. An event driven simulator for distributed shared memory systems, DSIM, uses the intermediate language to simulate and obtain efficiency results in networks of thousands of processors. In simulations of large parallel systems, enormous memory needs and long execution times are major difficulties. To minimize these problems, the new method analyzes system performance by using extracted execution step information, e.g. the number of shored writes in a code segment, to predict task completion times rather than executing the program step by step. Workstation evaluations of parallel codes running on several thousand processors are feasible using EC/DSIM.",
Application of the fuzzy min-max neural network classifier to problems with continuous and discrete attributes,"The fuzzy min-max classification network constitutes a promising pattern recognition approach that is based on hyberbox fuzzy sets and can be incrementally trained requiring only one pass through the training set. The definition and operation of the model considers only attributes assuming continuous values. Therefore, the application of the fuzzy min-max network to a problem with continuous and discrete attributes, requires the modification of its definition and operation in order to deal with the discrete dimensions. Experimental results using the modified model on a difficult pattern recognition problem establishes the strengths and weaknesses of the proposed approach.",
The D Editor: a new interactive parallel programming tool,"Fortran D and High Performance Fortran are languages designed to support efficient data-parallel programming on a variety of parallel architectures. The goal of the D Editor is to provide a tool that allows scientists to use these languages efficiently. The D Editor combines analyses for shared memory machines and compiler optimizations for distributed memory machines. By cooperating with the underlying compiler, it can provide novel information on partitioning, parallelism, and communication based on compile time analysis at the level of the original Fortran program. The D Editor uses color coding and a collection of graphical displays to help the user to zoom in on portions of the program containing sequentialized code or expensive communication. The prototype implementation is useful for interactively displaying the results of compile time analysis; however, it has a number of shortcomings that must be addressed. Future enhancements will provide additional advice and transformation capabilities. We believe the D Editor is representative of a new generation of tools that will be needed to assist scientists to fully exploit languages such as High Performance Fortran.",
Computer-supported cooperative environment for requirements elicitation,"As the number of jobs in which shared rather than individual work continues to grow, we are being forced to re-examine the way we teach people to work. This paper suggests that in today's workplace, computer programmers must be adept at both technical as well as cooperative skills. It also suggests that we must be prepared to teach students how to work in collaborative environments. Towards this end, we have developed a computer-supported cooperative problem solving environment designed to teach computer science students to elicit software requirements. We believe that requirements elicitation and cooperative skills are highly interrelated and, as such, can be taught more effectively through the use of a computer-supported cooperative environment. The environment encourages cooperative work and yet provides instructors with the ability to monitor both individual and group performance. From our study of students who used the cooperative interface, we found that subjects were able to learn requirements elicitation techniques and, at the same time, gain valuable experience in working together.",
A queueing network model of human performance of concurrent spatial and verbal tasks,"This article describes a 3-node queueing network model of human performance of concurrent spatial and verbal tasks. The model integrates the concerns of single channel, queuing theoretic models of selective attention and parallel processing, multiple resources models of divided attention, and provides a computational framework to model both the task selection and the concurrent execution aspects of multitask performance. The single channel and multiple resources concepts and their applications in engineering models are reviewed. Experimental evidence in support of the queueing network model is summarized. The potential value of queueing network methods in developing integrated computational models of human multitask performance and of human machine interaction is discussed.",
Analysis of common subexpression exploitation models in multiple-query processing,"In multiple-query processing, a subexpression that appears in more than one query is called a common subexpression (CSE). A CSE needs to he evaluated once only to produce a temporary result that can then be used to evaluate all the queries containing the CSE. Therefore, the cost of evaluating the CSE is amortized over the queries requiring its evaluation. Two queries, posed simultaneously to the optimizer, may however contain subexpression that are not equivalent but are, nevertheless related by implication (the extension of one is a proper subset of the other) or intersection (the intersection of the two extensions is a proper subset of both extensions). In order to exploit the opportunity for cost amortization offered by the two latter relationships. the optimizer must rewrite the two queries in such a way that a CSE is induced. This paper compares, empirically and analytically, the performance of the various query execution models that are implied by different approaches to query rewriting.",
Dynamic mapping and load balancing with parallel genetic algorithms,"The paper presents an approach to dynamic mapping and load balancing of parallel programs in MIMD multicomputers, based on coordinated migration of processes of a parallel program. A program graph is interpreted as a multi-agent system with locally defined goals and actions, operating in some environment. A parallel genetic algorithm (island model) is developed to work out a set of collective decisions concerning processes' migration. Presented experiments show a behavior of the algorithm.",
Meta-level control of rule execution in a parallel and distributed expert database system,"The PARADISER system is a parallel and distributed programming environment for expert databases. It extends the memory model of expert systems to disk-resident databases, and adopts parallel and distributed processing to address expert system scaling issues. The kernel rule language, PARULEL, has parallel execution semantics and a metarule facility that allows for user-specified ""programmable"" conflict resolution. Under a parallel and distributed processing model of a rule language that has inherently parallel execution semantics, issues of distributed control assume paramount importance, since database consistency and synchronization must be guaranteed. We discuss various alternative strategies for implementing metarules within the PARADISER architecture.",
Inexpensive technology lab exercises for grades 6-9,"The need for technology education in primary and secondary schools is well documented. What is needed is a way to infuse the excitement of technology and engineering into the middle and high school curriculum. This must be done at a low cost, since most school districts work with limited funding. One vehicle for introducing technology and engineering will be the Stiquito and Stiquito II robots. Stiguito is a small, simple, inexpensive ($4) hexapod robot developed by the Computer Science Department of Indiana University at Bloomington. Stiquito II is a larger, slightly more expensive ($7) hexapod robot which is easier to build than Stiquito. Even though these robots were originally designed to be used in research, they are used worldwide for educational purposes. These devices employ several concepts from electrical, mechanical, and chemical engineering. Applications of the robots would encompass computer and industrial engineering concepts as well. This paper contains motivations for introducing engineering to middle, junior high, and high school students, an introduction to the Stiquito robots, and details on lab exercises. Instructions at the end of the paper direct users to the location of these lab exercises on the Internet.",
Simulation models and group negotiation: problems of task understanding and computer support,"Computer simulation models have been proposed as aids for policy negotiations in complex domains. We studied the use of such a model by college students in mock policy negotiations in reservoir management. Comparisons between groups given differing forms of access to the model suggest that: (I) the model is of limited value in helping negotiators understand the task per se; (2) the model does provide some help in finding policies that satisfy specific constraints of the task; (3) availability of the model encourages negotiators to consider more policies; and (4) the burden of using the model directly, rather than through an assistant, is comparable to the benefit of using it. This study contributes to an understanding of prerequisites for successful use of simulation models in negotiation.",
An evaluation of the iHARP multiple instruction issue processor,"The paper evaluates the architectural features of iHARP, a VLIW (very long instruction word) processor with an instruction issue rate of four, which has been developed at the University of Hertfordshire. One of the distinctive features of iHARP is the provision of Boolean guards on all instructions. Every iHARP instruction is only executed at run time if the attached Boolean guard is true. The paper evaluates the benefits of guarded instruction execution and quantifies its performance advantage. Other architectural features considered include instruction issue rate, code size, number of data cache ports, number of register file write ports, number of branch units and addressing mechanisms. The evaluation uses RLS, a resource limited instruction scheduler, specifically developed to statically reorder code for parallel execution on iHARP.",
Studies on soft tissue pressure distribution in the arm during non-invasive blood pressure measurement,"The transmission of the pressure applied by means of an external cuff to the soft tissues of the arm was analyzed in order to understand significant causes of errors. The problem was studied with a finite element model that showed that the periarterial pressure depends upon several factors, such as the cuff size, the soft tissue elastic properties of the arm and the artery position. Experimental trials were performed with physical models to verify the finite element model and characterize the behavior of the cuff. The results indicate that the proposed model may contribute a simple new tool that is able to describe the tissue and cuff behavior with sufficient accuracy to be useful in non-invasive arterial pressure measurements.",
Performance and design choices of level-two caches,"The increasing disparity of speed between processor and its main memory makes ways for multi-level cache hierarchies in almost any of today's computer systems; specifically, the second-level (L2) caches with larger capacity but longer access time than the first-level (L1) caches have been adopted to reduce this memory gap. In this study an enhanced one-pass trace-driven simulation technique is used to evaluate the design choices of three L2 cache parameters: associativity, line size, and cache size. The traces are from large workloads of both commercial and scientific applications.",
"A new interpretation of ""polynomial residue number system""",The authors show that the polynomial residue number system can be interpreted in terms of the Chinese remainder theorem for polynomials (CRTP) over a finite ring which is useful for signal processing.,
A new thinning algorithm,"This paper introduces a new thinning algorithm which is simple yet can maintain the same advantages of other key algorithms. In this new algorithm, only a very small set of rules of criteria for deleting pixels is used. It is faster and easier to implement. Several examples are illustrated.",
Cluster-C/sup */: understanding the performance limits,"Data parallel languages are gaining interest as it becomes clear that they support a wider range of computation than previously believed. With improved network technology, it is now feasible to build data parallel supercomputers using traditional RISC-based workstations connected by a highspeed network. The paper presents an in-depth look at the communication behavior of nine C/sup */ programs (J.L. Frankel, 1991). It also compares the performance of these programs on both a cluster of 8 HP 720 workstations and a 32 node (128 Vector Unit) CM-5. The result is that under some conditions, the cluster is faster on an absolute scale, and that on a relative, per-node scale, the cluster delivers superior performance in all cases.",
Timing analysis of superscalar processor programs using ACSR,"This paper illustrates a formal technique for describing the timing properties and resource constraints of pipelined superscalar processor instructions at high level. Superscalar processors can issue and execute multiple instructions simultaneously. The degree of parallelism depends on the multiplicity of hardware functional units as well as data dependencies among instructions. Thus, the timing properties of a superscalar program is difficult. To analyze and predict. We describe how to model the instruction-level architecture of a superscalar processor using ACSR and how to derive the temporal behavior of an assembly program using the ACSR laws. The salient aspect of ACSR is that the notions of time, resources and priorities are supported directly in the algebra. Our approach is to model superscalar processor registers as ACSR resources, instructions as ACSR processes, and use ACSR priorities to achieve maximum possible instruction-level parallelism.",
Applied virtual reality in aerospace design,"A Virtual Reality (VR) applications program has been under development at the Marshall Space Flight Center (MSFC) since 1989. The objectives of the MSFC VR Applications Program are to develop, assess, validate, and utilize VR in hardware development, operations development and support, mission operations training and science training. Before VR can be used with confidence in a particular application, VR must be validated for that class of applications. For that reason, specific validation studies for selected classes of applications have been proposed and are currently underway. These include macro-ergonomic ""control-room class"" design analysis, Spacelab stowage reconfiguration training, a full-body micro-gravity functional reach simulator, a gross anatomy teaching simulator, and micro-ergonomic design analysis. This paper describes the MSFC VR Applications Program and the validation studies.",
A multiresolution probabilistic neural network for image segmentation,"A multiresolution network for segmenting textures and magnetic resonance images is proposed, based on maximum likelihood estimation. The network incorporates a probabilistic neural architecture to facilitate the generation of likelihood estimates. Further on, an iterative segmentation process is used, which refines the likelihood estimates based upon both the neighbouring estimated likelihoods and the confidence on these estimates. A multiresolution neural network structure which permits a significant reduction of the time needed to solve the segmentation problem is proposed. This is performed by an initial segmentation at lower resolution and subsequent refinement at higher resolutions.",
Software technology risk advisor,"This paper describes the Software Technology Risk Advisor (STRA), a knowledge-based software engineering tool that provides assistance in identifying and managing software technology risks. The STRA contains a knowledge base of software product and process needs, satisfying capabilities and capability maturity factors. After a user ranks the importance of relevant needs to his or her project, the STRA automatically infers risk areas by evaluating disparities between project needs and technology maturities. Identified risks are quantitatively prioritized and the user is given risk reduction advice and rationale for each conclusion. This paper presents methods used in the STRA, along with discussions of knowledge acquisition experimental results, current status, and related work.",
Automated class testing: methods and experience,"In contrast to the explosion of activity in object-oriented design and programming, little attention has been given to object testing. In our approach, a driver class and an oracle class are developed for each class-under-test (CUT). The driver class is based on a test-graph which partially models the CUT as a state machine, but with vastly fewer states and transitions. The oracle class provides essentially the same operations as the CUT, but supports only the testgraph states and transitions. Surprisingly thorough testing is achievable with simple testgraphs and oracles. The key is designing the two together, to avoid tests for which input generation and output checking are unaffordable. We summarize recent experience with the testgraphs framework, including handling of operations on pairs of objects and the application of testgraphs to test part of the testgraphs implementation.",
Microwave Annealing of Integrated Ferroelectric Films,"Ferroelectric thin films integrated with semiconductors are playing growing role as a key element of future computer memory. One of the film processing stage needs heating in order to obtain a good crystalline-structured ferroeleotric. The principle idea of proposed new method is to use significant distinction between a large microwave absorption of ferroelectrics and low absorption of semiconductors in order to apply microwave radiation energy in immediate region of film integrated with semiconductor in complex sandwich. During the annealing process the less heating of wafer the better, to prevent complicate semiconductor structures from becoming blurred. Metal-oxide electrode(which is interface between semiconductor and ferroelectric) and thin film itself are the main absorbers of microwave energy short pulses. Their power, rise time, duration and-duty can be much more carefully controlled by the modern microwave technique than any other heating processes. Moreover, the microwave-assisted processing of ferroelectric film offers unique possibility for non-electrode and non-contact measurements of film electric parameters simultaneously with the annealing or other processes.",
Improving neural network predictions of software quality using principal components analysis,"The application of statistical modeling techniques has been an intensely pursued area of research in the field of software engineering. The goal has been to model software quality and use that information to better understand the software development process. Neural network modeling methods have been applied to this field. The results reported indicate that neural network models have better predictive quality than some statistical models when predicting reliability and the number of faults. In this paper, we will explore the application of principal components analysis to neural network modeling as a way of improving the predictive quality of neural network quality models. We trained two neural nets with data collected from a large commercial software system, one with raw data, and one with principal components. Then, we compare the predictive quality of the two competing neural net models.",
To contradictions between wealth and compactness of languages,"We describe an approach to the problem of contradiction between the wealth of facilities and the compactness of a language. This approach removes this contradiction by structuring languages. The main idea is to decompose the development task into micro contexts each of which can best be supported with their own micro sub-language. The /spl pi/-schemes (the family of visual languages) and software complexes PYTHAGORAS and /spl pi/LOGO are implementation examples. The characteristic features of the software environments are orientation to graphic descriptions of algorithms, functions and syntax, animation, and using the hierarchical multilanguage technology.",
"Concatenated codes with fixed inner code, and the outer code randomly selected","A lower bound for the above given coding scheme, where the inner and outer codes are block codes, is derived. The bound, which depends on the outer code rate, and the weight distribution of the inner code, is compared to the Gilbert-Varshamov bound for block codes.",
HARP-1: a special-purpose computer for N-body simulation with the Hermite integrator,"The authors have designed and built HARP (Hermite AcceleratoR Pipeline)-1, a special-purpose computer for solving astronomical N-body problems with high accuracy using the Hermite integrator. The Hermite integrator uses analytically calculated time derivatives of the acceleration, in addition to the acceleration, to integrate orbits of particles. HARP-I has a 24-stage pipeline to perform the calculation of the acceleration and its time derivative, which is the most expensive part of the Hermite scheme. The pipeline calculates one gravitational interaction at every three clock cycles. Thus, the acceleration and its time derivative of a particle are calculated in 3N+24 clock cycles, where N is the number of particles. The peak speed of HARP-1 is 160 Mflops.",
On-line minimization of call setup time via load balancing: a stochastic approximation approach,"With the addition of new network services, it is anticipated that the processing involved in setting up a call in a circuit-switched network or a session in a packet-switched network will vary greatly for different types of services. In this paper, we address the problem of reducing the call setup time in a circuit-switched network, or equivalently the session setup time in a packet-switched network, through the balancing of load across call processors. With a view to designing algorithms to execute on-line in a system, we formulate a stochastic optimization problem and study the use of stochastic approximation techniques. Given the distributed nature of the problem, we extend previous results obtained for a single node to the case where several nodes operate simultaneously and in an asynchronous manner. Our results include a theoretical study of convergence as well as several simulation results that compare two stochastic approximation techniques.",
Investigating a new visual cue for image motion estimation,"Motion smear is an important visual cue for the human vision system (HVS) to perceive motion. However, in image analysis research, using motion smear as a valuable visual cue for motion estimation has remained largely ignored. Rather, motion smear is usually considered as a degradation on images that need to be removed. A computational model has been established to estimate image motion from motion smear information. Both theoretical derivation and experimental results are provided.",
Adjustable flow control filters and reflective memories as support for distributed real-time systems,"Distributed real-time systems are difficult to construct such that the system behaves predictably. In addition, many of these systems must operate in complex and uncertain environments. We discuss two main ideas for moving distributed real-time systems towards predictability: adjustable flow control filters and reflective memories. Both of these are hardware based solutions that should be exploited for enhancing predictability and robustness, especially as hardware becomes cheaper. Adjustable flow control filters is a new idea being proposed here, while reflective memories are available off-the-shelf.",
A fast sparse iterative method (SIM) for method of moments,"The sparse iterative method (SIM) provides a faster solution to method of moments (MoM) matrix equations than does LU-decomposition with forward and back substitution. The SIM produces a solution with computational time proportional to N/sup 2/, as opposed to the N/sup 3/ time dependence associated with LU-decomposition. The SIM is implemented in an object oriented MoM program which is functionally equivalent to NEC2. In three examples, the SIM is shown to produce results as accurate as LU-decomposition. For dipoles, a flat wire grid and a generic three dimensional missile shape, the speed increase ranged from 3-30 times the speed of LU-decomposition; greater speed increases can be expected with electrically larger problems. The SIM requires no problem formulation changes, such as segment renumbering, and despite the fact that it is demonstrated for a wire MoM based on NEC2, it is general enough to be incorporated to any MoM formulation.",
Systems modeling with xpetri,"The design of a new modeling tool, xpetri, is described, and its use in solving real computer system design problems is illustrated. The tool is based on an extension of stochastic Petri nets, and provides both a succinct model language specification and a wide-ranging collection of modeling capabilities including detailed stochastic workload representation. Although solutions of models in the specified xpetri language are outside the realm of purely analytic techniques, favorable solution times are provided via a multi-threaded simulation compiler. A technique is suggested for allowing the threads to execute transition firing outside strictly time-sequential order. An X-windows (Motif) interface offers fast and reliable generation of models in the xpetri language.",
Bidirectional convergence: A cognitive approach to generalisation,"We present a cognitive approach to generalisation, Bidirectional Convergence. This is the implementation of a cognitive process we call concept crystallisation, whereby a concept is formed gradually from initially many possibilities which converge to a single possibility under the weight of a series of learning instances shown over a period of time. Bidirectional Convergence (BDC) is a form of concept crystallisation that represents the alternative possible concepts through. Boundary versions of the concept during learning. BDC is an abstraction of Mitchell's symbolic concept learning technique (1982). We describe how BDC is evolved from Mitchell's technique into a form suitable for incorporation into neural networks, BDC is shown to provide a best-fit to given problems.",
On the double bilinear transformation and nonessential singularities of the second kind at infinity,This paper addresses the BIBO (bounded-input bounded-output) stability of a class of discrete 2-D transfer functions in the presence of nonessential singularities of the second kind (NSSK's) on the unit bidisk. Conditions under which the double bilinear transformation (DBT) preserves stability are derived. The results presented here also extend the class of systems whose stability can be predicted. Use of the inverse DBT to produce a continuous equivalent of the discrete 2-D transfer function allows easy application of a continuous-domain equivalent of a criterion developed by Dautov. The necessary and sufficient condition for stability derived in this work provides a simple check for the class of systems under consideration.,
A group structuring mechanism for a distributed object-oriented language,"Describes a structuring mechanism for grouping objects in a distributed object-oriented language. A group structuring mechanism provides a single flexible method for managing distributed applications that involve complicated communication protocols and sophisticated structure. We have added such a mechanism to the Emerald distributed object-oriented language and its run-time system. Our group structuring mechanism fits entirely within the context of object-oriented programming, so similar mechanisms could be added to other distributed object-oriented languages.",
A method for 3D surface reconstruction from range images,"A method for 3D surface reconstruction from range images is proposed. It uses the implicit algebraic shape primitive which allows for a host of possible shapes. First by using the segmentation method based on mathematical morphology, the range image is segmented into regions corresponding to the surface patches on objects. Then the algebraic surfaces are fitted to the range points in these regions by solving a generalized eigenvector problem. It is proved that this scheme is viewpoint invariant and computationally efficient, and in comparison with other implicit surface reconstruction techniques, can represent more general 3D surfaces.",
Automatic network restoration using a two-level associative memory,"Based on the principles of neural computing, a new approach in network restoration is proposed and implemented in this paper. This new approach, called NRNN (network restoration using neural networks), is a hybrid algorithm and is implemented by using a two-level associative memory. Compared to NETSPAR (a hybrid algorithm proposed by Bellcore in 1991), NRNN not only obtains 100% recovery of disrupted traffic due to a link or node failure but also requires a very small memory at each node. At a node, the memory requirement for NRNN is less than 1% of the memory requirement for NETSPAR.<>",
Mobile software agents for control of distributed systems based on principles of social insect behaviour,"Mobile software agents are computational processes that are capable of moving. The agents are based on organisational principles observed in social insects such as ants. It is argued that if these principles can be emulated, then the desirable features of natural systems such as simplicity, adaptability and robustness can be made inherent in computer control systems. We have devised an application which is capable of showing an ensemble of mobile agents controlling congestion in a circuit switched telecommunications network and illustrating the points discussed.",
Introducing connection-oriented service in MLAN to provide seamless interface to ATM networks,"ATM being a connection-oriented technology, there will be a need for connection-oriented service on LANs to be able to provide seamless internetworking between LANs and ATM broadband networks. We present a general technique to provide connection-oriented service on LANs and then show how this technique is incorporated into a recently designed multimedia LAN (MLAN) protocol. The results are presented of a performance evaluation of the MLAN protocol with connection-oriented service.",
Relations between CSCW and software process research: a position statement,"The field of CSCW is investigating computer support for arbitrary, distributed cooperative work while the field of software process is focusing on control and automation of the activities of collaborating software engineers. We look at the scope and important ideas of these fields and address the following questions: are software process and CSCW two approaches for the same research field? Are the two fields taking incompatible approaches to solve the same problem? Can each field help the other to better meet its goals? We explore the possibility of integrating our solutions. We give only preliminary answers to these questions and hope that by raising these questions we can develop better answers and a concrete plan to integrate our own efforts and those of other participants in the workshop.",
Novel Low-Loss Interdigitated Filters for Microwave and Millimeter-Wave Applications Using Micromachining Techniques,We report on the design and measurement of a new class of stripline filters for millimeter-wave applications. The novelty in this work is to suspend the interdigitated lines on a thin dielectric membrane to eliminate the dielectric losses and to render the interdigitated filter physically realizable at millimter-wave frequencies. A wide band filter with a center frequency of 13.7 GHz and a bandwidth of 45% has been fabricated. The filter exhibits a port-to-port insertion loss of 0.7dB at 13.7 GHz. This fabrication technique can be easily used at higher frequencies to result in planar low loss filters suitable for monolithic integration or surface mount devices up to 40-60 GHz.,
Exploring the software development process using a security camera in a CS2 course,"The University of Virginia is in the middle of a major restructuring of the entire undergraduate computer science program. A new philosophy was needed to define the necessary tools, skills and experience needed by the contemporary computing professional. It is expected that the new curriculum will produce computer scientists who have a clear understanding of, an appreciation for, and skills that support the engineering and comprehension of large software systems, reengineering of existing systems, use of modern tools and environments, and application of innovative techniques such as software reuse. This paper presents a brief overview of a project focused around the software development process. Students will use a security camera model as the means for exploring the software development process.",
Detecting 3D flow,"An algorithm for computing 3D motion (velocity field and motion parameters) from a sequence of range images is presented. The algorithm is based on a new integral 3D rigid motion invariant feature which can be computed locally at every point of the moving surface. This feature is the trace of a 3/spl times/3 matrix (referred to as ""feature matrix"" in the paper) which is closely related to the moment of inertia tenser in physics. This trace of the ""feature matrix"" at any point provides a quantitative measure of the local shape of the surface around that point. Since the trace feature is rigid motion invariant, it remains conserved along the trajectory of every moving point. Based on that, a 3D flow constraint equation is formulated at every point on the surface, which can be solved for the velocity at the point using optical flow techniques. Since the proposed feature is integral in nature, it is robust in terms of noise and discontinuity in the moving surface. Theoretical analysis is provided to justify these claims. Experimental results are also provided to justify the efficacy of the method.",
Generalized state space averaging approach and its application to the analysis of quasi-resonant converters,"A generalized state space averaging (GSSA) technique for the analysis of a class of periodically switched networks has been proposed in the present paper. With GSSA, under the assumption that switching frequency is much higher than the highest natural frequencies of the networks, a periodically switched network with fast time-varying input sources can be handled. The GSSA is given as the main theorem in this paper, its application to the low-frequency analysis of quasi-resonant power converters (QRCs) has been illustrated via an example of a zero-voltage switching QRC boost converter.",
A resonance correlation network with adaptive fuzzy leader clustering,"Cluster analysis is a significant area of research in pattern recognition. Determining the optimal number of clusters in any real data set remains a difficult problem. The paper develops a new neural network model with the combined advantages of self-organization and no sequential search (as in the resonance correlation network) with more stable, fewer and better clusters (as in the adaptive fuzzy leader clustering network). This new model is the Adaptive Fuzzy Leader Clustering Resonance Correlation Network (AFLCRCN). It adaptively clusters continuous-valued data into classes without a priori knowledge of the entire data set or ifs number of clusters. AFLCRCN incorporates the fuzzy K-means learning rule used in the AFLC network into the RCN control structure. It has a modular design that allows metric replacement for improved performance in a specific problem. Applications for the model include classification, feature extraction, and pattern recognition.",
The SOLAR algorithm,"We propose SOLAR (supervised one-shot learning algorithm for real number inputs). SOLAR requires only a single presentation of real number input data, so it can learn very quickly compared to the backpropagation algorithm (BP). We introduce a new similarity matrix which measures Euclidean distance of the training set. From the topology of the similarity matrix, the structure of network, learning parameters and linear threshold functions are determined. Since it uses the structural method, SOLAR is suited to the dynamic environment, e.g. addition or subtraction of training instances, input units and output units. The main contribution of this paper is that SOLAR can handle analog inputs, so it can be easily extended to learning temporal sequences and improve the ability of the generalization.<>",
The emptiness problem for intersection types,"We prove that it is undecidable whether a given intersection type is non-empty, i.e., whether there exists a closed term of this type.",
Modeling complex information systems using parameterized Petri nets (PPNs),"A new class of Petri nets (PNs) called Parameterized Petri Nets (PPNs) is proposed and applied for the modeling, analysis, and simulation of complex dynamic systems. The PPN approach uses a hierarchical, dynamically reconfigurable organization to deal with the complexity characteristic of net representations through parameterization of transitions and the use of generalized or parameterized places in higher level nets. The parameterization capability of PPNs can be used to generate behavior sequences of different levels of abstraction/detail to be mapped into more specific sequences at a lower level of the hierarchy. The PPN system model structural properties are verifiable based on the corresponding simple PN model. The PPN methodology enables multiple simultaneous ""points of view"" regarding a system.",
Stochastic search approach to object location,"Object location within an image involves establishing a location within an image for which a particular error function is minimised, and thus is essentially an optimization problem. Within the content of location via template matching, a random search approach, called a cluster search, based on an extension of mathematical methods due to Matyas has been studied to determine the most appropriate parameter /spl beta/. For well chosen /spl beta/ the cluster search has been shown to be on average significantly faster than the fastest deterministic search schemes, notably coarse fine approaches.",
Subpixel edge estimation using geometrical edge models with noise miniaturization,"The significant disadvantage for traditional contour representation is as the resolution is reduced the effort of undersampling is proportional enlarged. The goal of this study is to improve edge detection results, especially for those corner points in low resolution. This study describes a method, which is based on 4-connected pixel-wise linearization, for finding contours from low resolution video images. This allows a more accurate inspection and identification of objects from image data. In practice, geometrical models are used to manipulate this linearization. A method is employed for examining the corner points as well.",
Computer-aided engineering economics and cost estimating,"The NSF-funded project entitled ""The Integration of Economics with Design in the Engineering Science Component of the Undergraduate Curriculum"" intends to demonstrate how the design activity when fully integrated with economic principles can be effectively utilized to teach engineering science. An integrated core of engineering science courses featuring enhanced design and economics content has been developed at Georgia Tech (USA). In addition, a new course entitled ""Economics of Engineering Design"" has been developed at Virginia Tech that includes a multi-media based approach to the teaching of design economics. This paper reports on the development of support software for these courses.",
A New Approach For Factorizing FSM's,,
An investigation into the use of physical modelling for the prediction of various feature types visible from different view points,"Describes a general purpose flexible technique which uses physical modelling techniques for determining the features of a 3D object that are visible from any predefined view. Physical modelling techniques are used to determine which of many different types of features are visible from a complete set of viewpoints. The power of this technique lies in its ability to detect and parameterise object features, regardless of object complexity. Raytracing is used to simulate the physical process by which object features are visible so that surface properties (e.g. specularity, transparency) as well as object boundaries can be used in the recognition process. Using this technique occluding and non-occluding edge based features are extracted using image processing techniques and then parameterised. Features caused by specularity are also extracted and qualitative descriptions for these are defined.",
High-speed text search based on a programmable signal processor,"Document retrieval systems must execute a variety of full-text search operations in a real-time environment. When the size of the textual database exceeds the GByte level, the presence of special hardware for high-speed textual pattern matching becomes essential. A text search processor is presented that uses a programmable analog signal processing device based on acoustic charge transport technology. The processor operates at 180 MHz and is capable of performing 128 character comparisons in parallel. Based on a look-up table technique, each input character is encoded on-the-fly according to some simple rules to increase the detection efficiency of the text processor. A prototype was implemented and the initial test results indicate that a variety of text search operations can be executed at throughput rates close to 90 million characters per second.",
Process assessment with a project focus,"The author compares three process assessment techniques that measure improvement against the Software Engineering Institute's capability maturity model (CMM), particularly the Computer Science Corporation's software process audit method. The Software Engineering Institute has developed two approaches to determine an organization's process maturity: software process assessments (SPA) and software capability evaluations (SCA), both of which use the SEI's CMM as a yardstick. Computer Science Corporation's software process audit method addresses the major limitations of the SPA, ie. SPAs occur too infrequently to track progress; SPA results are reported at the organization, not project level; and SPAs direct process improvement at the organizational level. The process-audit approach at CSC focuses on projects, specifically their conformance to the organization's defined process. As such, a process audit fits between a SPA and an SCE.",
A general modeling tool for hybrid systems,"This paper describes joint work between Probots, Inc., and the Samara Division of the Mechanical Engineering Research Institute, Russian Academy of Science (SD MERI). The objective of this project is to develop techniques for modeling complex, dynamic systems that include the modeling of hybrid systems combining logical models and continuous time models. The techniques we have developed are based on automatically constructing both continuous and discrete models of dynamic systems from their description in the form of mathematical relations (algebraic, differential, logical, matrix, and heuristic) and on automating the process of specifying control functions for intelligent agents by embedding learning functions in dynamic objects.",
Application-specific array processors for binary prefix sum computation,"The main contribution of this work is to propose two application-specific bus architectures for computing the prefix sums of a binary sequence. Our architectures feature the following characteristics: all broadcasts occur on buses of length 15 or 63; we use a new technique that we call shift switching which allows switches to cyclically permute an incoming signal, dramatically improving the performance of the reconfigurable bus system. As it turns out, our special-purpose architectures improve the performance of the best algorithms known to date by a significant factor. Specifically, our solutions require no adders, are faster, and use less VLSI area than the architectures of the state of the art.",
"Training software engineering professionals in Hong Kong, and the approach of 1997","This reviews the steps being taken in Hong Kong to make the recognition of engineers independent of Britain before the return of sovereignty to China in 1997. It outlines the process needed to achieve recognition at a standard which is acceptable to the leading international bodies in the English speaking world and predicts that local independence in applying this will be achieved during 1995. Software Engineering is discussed in the same light as any other discipline within the general framework of engineering. It shows that, despite being a relative new-comer to the processes of granting professional recognition, and not having developed an extended experience of accrediting software engineering courses or post-graduate training facilities, it may take a lead in the process of gaining international recognition.",
A framework for reliability modeling of electronics,"A physics-of-failure approach and the associated CADMP (computer aided design of microelectronic packages) software assist cost effective MCM (multichip module) designs scientific consideration of reliability during the design phase; evaluation of new materials, structures, and technologies; assessment of packages designed by different manufacturers; development of science-based tests, screens, and derating methods. The tools can also be used to assess reliability, of single chip package designs, including plastic encapsulated microcircuits (PEMs). The development and validation process for CADMP software continues. At present, the software is being exercised by over forty government and industry organizations, and feedback is being used to ready the software for commercialization.",
Interactive tools for signal processing education,"The Signal Processing Instructional Facility (SPIF Lab.) is an educational laboratory featuring interactive multimedia for teaching concepts related to linear systems theory and signal processing. The goals of the SPIF lab are to augment, enhance, and interconnect sophomore, junior, and senior level courses with the common thread of linear systems and transforms by unifying the experimentation medium. In this fashion, physical phenomenon is returned to the forefront of engineering education. The major components of the new laboratory are a family of dynamic Mathematica Notebooks (a form of hypertext) and interactive applications which utilize dedicated DSP microprocessors.",
A verified integration of parallel programming paradigms in CC++,"CC++ is an object-oriented parallel programming language that uses parallel composition, atomic functions, and single-assignment variables to express concurrency. We show that this programming paradigm is equivalent to several traditional imperative communication and synchronization models, namely monitors and asynchronous channels. Furthermore, the object-oriented nature of CC++ provides an ideal framework for integrating these paradigms. We specify, implement, and formally verify a collection of libraries that integrates these traditional models with CC++.",
Complexity lower bounds for computation trees with elementary transcendental function gates,"We consider computation trees which admit as gate functions along with the usual arithmetic operations also algebraic or transcendental functions like exp, log, sin, square root (defined in the relevant domains) or much more general, Pfaffian functions. A new method for proving lower bounds on the depth of these trees is developed which allows to prove a lower bound /spl Omega/(/spl radic/(log N)) for testing membership to a convex polyhedron with N facets of all dimensions, provided that N is large enough. This method differs essentially from the previous approaches adopted for algebraic computation trees.",
Research issues for a real-time nested transaction model,"Real-time database systems have been studied to meet stringent timing and reliability constraints that were observed in many application areas. In some applications, many transactions must be not only executed correctly but also completed within their deadlines, and soma data may become invalid after a certain duration of time. For some of these advanced applications, transactions are long and complicated. A transaction model beyond the conventional single-level transaction models is needed to manage the complexity. In order to extend current real-time transactions to a nested transaction model several issues need to be explored or re-examined. We discuss two important issues: how to propagate deadlines from a transaction to its subtransactions and how to control concurrent execution of nested transactions.",
A medical expert system using object-oriented framework,"The primary goal of this paper is to present a generic framework for complex medical expert system inference. The rapid prototype of an expert system can be achieved by to three system components. They are namely the object static component, presentation components and inference components. This idea is borrowed from the Model View and Control (MVC) approach. The inter-relationship between these system components are elaborated in this paper. We use PC-Kappa as our system development environment which supports an expert system development facility as well as object-oriented programming. An extension of full object-oriented techniques that is not supported by the Kappa-PC is also presented in this paper for object-oriented expert system construction.",
Project management software-the fourth tool?,"Many people are involved in the management of small projects, but software to assist with this has been too complex to be of much use to other than full-time professional project managers. The authors contend that after word processors, spreadsheets and database managers, project management software is destined to become the fourth software tool, and should change the way that even small projects are planned, implemented, and managed. Most computing courses give token recognition to project management, but if is in the way project management is typically taught in undergraduate computing degrees at many universities that we see room for improvement. Our research has shown that most computing students draw Gantt and PERT charts only because this is required, not because they find them useful.",
Fast accurate simulation of large shared memory multiprocessors,"Fast computer simulation is an essential tool in the design of large parallel computers. We discuss the design and performance of our Fast Accurate Simulation Tool, FAST. We start by summarizing the tradeoffs made in the designs of this and other simulators. The key ideas used in this simulator involve execution driven simulation techniques that modify the object code of the application program being studied. This produces an augmented version of the code that is directly executed and performs much of the work of the simulation. We extend the previous work in execution driven simulation by introducing several new uses for code augmentation. The result of these techniques is a simulator that is one to two orders of magnitude faster than previous simulators of comparable accuracy.",
A reliable trading service in open distributed systems,"The trading function is an important function which specifies the trading service in open distributed systems. A trader is an object which provides the trading service. In the client-server model, the trader allows clients to locate servers dynamically and allows servers to advertise their services. A reliable trader is particularly important, because if the trader fails, clients could not find suitable servers and servers could not advertise their services. The primary goal of the research is to improve reliability of a trader. To achieve this goal, a distributed system model is proposed which allows traders to provide a reliable trading service. A mechanism is also developed which makes traders tolerant to process failures, site failures, and most importantly, network partitionings. When network partitionings occur, the mechanism uses the semantic information of trader's operations to maintain the consistency of trader database replicas. Besides providing high reliability, the mechanisms provide higher availability than other proposed consistency control algorithms, such as the weighted voting algorithm. They also provide failure and location transparent trading services to clients and servers.",
The design and implementation of multimedia intelligent tutoring system for Chinese characters,"The multimedia intelligent tutoring system for Chinese characters (CMITS) developed by us is a kind of microcomputer-oriented multimedia tutoring software. It has the following function: to teach the writing of Chinese characters based on the order of strokes observed in calligraphy, Chinese phonetics, word formation and explanation of the Chinese character, etc.; and to analyze and evaluate students' study status to guide students in error correction. This software is intelligent. In CMITS, pronunciation, graphs, images, text and data been integrated using the multimedia technique. CMITS provides a vivid and flexible user interface and rich studying environment through hypertext.",
A multi-temporal classification of multi-spectral images using a neural network,"The classification of remotely sensed multi-spectral data using classical statistical methods has been worked on for several decades. There have been many new developments in neural network (NN) research, and many new applications have been studied. It is well known that NN approaches have the ability to classify without assuming a distribution. The authors previously proposed an NN model to combine the spectral and spatial information of LANDSAT TM images. In this paper, the authors apply the NN approach with a normalization method to classify multi-temporal LANDSAT TM images in order to investigate the robustness of their approach. From the authors' experiments, they confirm that their approach is more effective for the classification of multi-temporal data than the original NN approach and maximum likelihood approach.",
Increase analysis in the total execution time of a parallel program,Lower bound on the finishing time of optimal schedules is used as an absolute performance measure of static scheduling heuristics. This paper presents an efficient method of computing such a bound based on estimating overlaps among the execution ranges of tasks in a given task graph and analyzing the delays of tasks on the critical paths of the graph. The computation performed by this method is shown to be of higher quality than that of other known methods. The future work and directions on this topic are also indicated.,
Fuzzy self-organizing map: mechanism and convergence,"This paper presents the learning mechanism of a model of fuzzy neural network, fuzzy self-organizing map (FSOM). The analysis on the convergence of learning mechanism will be elucidated. When the dimension of input data is one, we can prove that the convergence of the learning mechanism is almost sure. While the input data dimension is higher than one, the mechanism fulfils only the necessary condition for convergence. Simulation result will be given to illustrate the model.",
Concepts and techniques: active electronics and computers in safety-critical accelerator operation,"The RHIC requires an extensive Access Control System to protect personnel from radiation, oxygen deficiency and electrical hazards. In addition, the complicated nature of operation of the collider as part of a complex of other accelerators necessitates the use of active electronic measurement circuitry to ensure compliance with established operational safety limits. Solutions were devised which permit the use of modern computer and interconnection technology for safety-critical applications, while preserving and enhancing, tried and proven protection methods. In addition a set of Guidelines, regarding required performance for accelerator safety systems and a handbook of design criteria and rules were developed to assist future system designers and to provide a framework for internal review and regulation. This paper is offered in the hope and belief that many of the techniques developed as part of the Access and Operational Safety systems designed for RHIC (and other recently completed large research accelerators) may be directly applicable to smaller research and clinical treatment facilities. Because of the relatively short length of this paper, I will concentrate on four topics: novel features of our system design; potentially useful applications of probability theory empowered by these system concepts: a listing of configuration control and design approaches, which when followed, will enable one to design high reliability hardware and systems (to meet required functionality); software design concepts and approaches.",
Approximation Of The Bounded Plasma Model By The Plasma And The Sheath Models,,
Efficient average-case algorithms for the modular group,The modular group occupies a central position in many branches of mathematical sciences. In this paper we give average polynomial-time algorithms for the unbounded and bounded membership problems for finitely generated subgroups of the modular group. The latter result affirms a conjecture of Y. Gurevich (1990).,
A variable-size parallel regenerator for long integrated interconnections,"This paper presents a low-power and low-area variant of the recently proposed parallel regeneration technique (PRT), thus providing an improved technique for the regeneration of long integrated interconnects. Taking advantage of the particular design of the regenerator in PRT, we propose a variant (called VPRT), where the regenerators along the interconnection have a variable size. Electrical simulations involving different interconnection lengths and technological processes are carried out to show that the interconnection delay, obtained with VPRT, is smaller than with PRT. A performance analysis combining area (A), delay (T), and power dissipation (P) shows that VPRT leads to an ATP metric at least 4 times better than with PRT.",
Design assistance for user interface designers,"The paper addresses a research approach in the area of user interface development tools, namely to discover the helpful, structured, and organised ways to integrate the use of guidelines, standards, and styles into the development tools. The aim of this research approach is to assist the user interface designer during the design process with the help of a powerful design aid tool, called the user interface designer assistance, that will lead to user interfaces of high ergonomic quality, an important aspect for the end-users.",
Qualitative simulation of organizational microprocesses,"Computer-based tools for the representation and simulation of business processes have been noted for their importance in the support of reengineering activities. However, organizational processes can be extremely difficult to represent and simulate mathematically, and limitations of the knowledge-based systems common in the business domain can preclude them from effectively supporting process analysis and redesign. The research described is oriented toward overcoming such limitations of extant computer-based simulation tools. Employing the AI technology of qualitative simulation, we developed and implemented a system for the representation and simulation of organizational microprocesses. This technology possesses a number of desirable features and the microprocess represents a useful unit of analysis. The use and utility of our system is demonstrated via two simulation examples, and we provide a set of research hypotheses for continuing this line of work.",
A hybrid knowledge representation based on logical objects,"This paper presents a hybrid knowledge representation based on the framework of logical knowledge objects (LKO), which are viewed as abstractions with behaviors, state and inheritance. It accommodates the main features of various knowledge representations, such as rule, frame, semantic network and blackboard, while amalgamating logic programming and object oriented programming paradigms. Therefore, it supports such notions as inheritance, modularity, reusability, structuring and dynamic modification of knowledge, which have turned out to be extremely useful for the design and maintenance of large scale knowledge based systems and expert systems. Using the framework, we can explicitly define the hierarchy of logical objects and easily obtain the solutions. With the basic notions and the semantics of LKO introduced, we focus on how it fits for the various knowledge representations, and discuss the management of knowledge base by exploring the formal specifications of metalevel operations.",
The complementary relationship of interprocedural register allocation and inlining,"Inline expansion and interprocedural register allocation are two general approaches used for interprocedural optimization. However, there are certain situations which prevent either of them from being applied smoothly to procedure calls. This paper describes a method of integrating interprocedural register allocation and inlining in order to reduce the procedure call overhead. The method uses profile information to identify the heavy called procedures regions and optimizes the placement of the register save/restore code. This method also takes full advantage of free-use registers at each procedure cell site. The average performance improvement is 1.21 compared with the previous schemes that performed either of them independently.",
1994 Proceedings of the Twenty-Seventh Hawaii International Conference on System Sciences,,
A general approach for determining 3D motion and structure of multiple objects from image trajectories,"Presents a general approach to determine the 3D motion and structure of multiple objects undergoing arbitrary motions. We segment the scene based on 3D motion parameters. First, the general motion model is fitted to each single trajectory. For this nonlinear fitting, initial estimates are obtained by a linear multiple-motion SFM (structure from motion) algorithm using the first two frames. Next, trajectories are clustered into groups corresponding to different moving objects. In our approach, discontinuous trajectories, resulting from occlusion, are also allowed. Finally, multiple trajectory fitting is applied to each trajectory group to improve the estimates further. Our simulation results show that the proposed method is robust.",
Visual language for behavioral specifications of reactive systems,"Complex reactive systems require specifying the relationship of inputs and outputs in the time domain. Typically, such descriptions involve complex sequences of events, often, with explicit timing constraints, that combine to form the system's overall behavior. In addition, most reactive systems are highly concurrent and hierarchical. Investigating formal methods for behavior specification of reactive systems presents real challenges. A new visual language for behavioral specification of complex reactive systems is described. We call it Constraint Narrowing Grammar (CNG). CNG combines concepts from logic programming, rewriting, and lazy evaluation. The non-deterministic and nonterminating properties of CNG make it a nice formalism to model infinite reactive systems' behavior. We demonstrate how a graphical specification can be mapped to CNG rules and show step-by-step how notations of hierarchy, concurrency, and event communication are introduced into the specification. The resulting grammar is an executable behavioral specification of complex reactive systems.",
Charged particle dynamics in the acceleration gap of the PBFA II ion diode,"We are improving the understanding of pulsed-power-driven ion diodes using measurements of the charged particle distributions in the diode anode-cathode (AK) gap. We measure the time- and space-resolved electric field in the AK gap using Stark-shifted Li I 2s-2p emission. The ion density in the gap is determined from the electric field profile and the ion current density. The electron density is inferred by subtracting the net charge density, obtained from the derivative of the electric field profile, from the ion density. The measured electric field and charged particle distributions are compared with results from QUICKSILVER, a 3D particle-in-cell computer code. The comparison validates the fundamental concept of electron build-up in the AK gap. However, the PBFA II diode exhibits considerably richer physics than presently contained in the simulation, suggesting improvements both to the experiments and to our understanding of ion diode physics.",
DART1-a possible ideal building block of memory systems,"In this paper, a bidirectional associative memory dual adaptive resonance theory 1 (DART1) consisting of two simplified ART1 (SART1) networks, which share the same category layer called the conceptual layer and use only top-down weight matrices, is studied. The DART1 can meet the most important requirement of the neural networks as the memory device, that is, the ability to store arbitrary patterns in networks and retrieve them correctly. Other advantages of this model include the capability of associating one pattern with another arbitrary pattern like the MLP (multilayer perceptron) networks, eliminating the input noise as a nearest-neighbor classifier in terms of the Hamming distance, capable of handling both spatial and temporal patterns, full memory capacity like the ART1 networks, fast construction of weights, easy to update the memory system, and self-organizing ability in response to arbitrary binary patterns. In addition, the DART1 can deal with the 1-to-many and many-to-1 mappings in the domains formed by trained patterns. These features make the DART1 very attractive as the building block of memory system. Therefore, it is possible to build the neural-based database or expert systems, which are the topics of future research.",
Numerical dispersion in the vector finite element method using first-order quadrilateral elements,"Although the vector finite element method has been widely used, the errors associated with it have not been thoroughly investigated. For finite elements, one of the most significant errors arises from the inability of the polynomial basis functions to represent the fields exactly within an element. A plane wave propagating through a finite element mesh will experience numerical dispersion as a result of this error. For practical node densities, this numerical dispersion can be characterized by a cumulative phase error. The numerical dispersion of a time-harmonic plane wave propagating through an infinite, two-dimensional, first-order vector finite element mesh composed of uniform quadrilateral elements is investigated. It is shown that the phase error for the vector finite elements is dependent upon the direction of propagation through the mesh and the electrical size of the elements. In addition, a simple formula is given which is an approximation to the exact numerical dispersion.",
An available copy protocol tolerating network partitions,,
Towards automated brachytherapy film implant labeling using statistical pattern recognition,"This paper presents a new approach towards automating the identification and labeling of brachytherapy sources. Brachytherapy is a cancer treatment method in which radioactive sources (seeds) are implanted into and around cancerous tissues. A common method for determining the radiation dose distribution delivered to the patient requires analysis of two orthogonal radiographs that show the interstitial or intercavitary seed placement. First, the seeds must be identified on the radiographs, and then the corresponding seeds must be identified on both films. Once this labeling process is complete the spatial coordinates of the seeds can be determined and the 3 dimensional dose distribution can be computed. For the type of implant studied, typically the number of seeds is between 50 and 250, and the seed distributions are dense. The methodology for automatic seed labeling consists of three steps. First, Multiscale Geometric Statistical Pattern Recognition (MGSPR) is used to identify treatment relevant objects in the radiographs. Second, a priori information must be processed to verify the integrity of the object identification by MGSPR and to resolve any object identification ambiguities. Third, the identified objects are projected into 3 dimensions, completing the labeling task, and allowing computation of the dose distribution.",
Recognizing patterns in protein sequences using iteration-performing calculations in genetic programming,"Uses genetic programming with automatically defined functions (ADFs) for the dynamic creation of a pattern-recognizing computer program consisting of initially-unknown detectors, an initially-unknown iterative calculation incorporating the as-yet-undiscovered detectors, and an initially-unspecified final calculation incorporating the results of the as-yet-unspecified iteration. The program's goal is to recognize a given protein segment as being a transmembrane domain or non-transmembrane area of the protein. Genetic programming with automatic function definition is given a training set of differently-sized mouse protein segments and their correct classification. Correlation is used as the fitness measure. Automatic function definition enables genetic programming to dynamically create subroutines (detectors). A restricted form of iteration is introduced to enable genetic programming to perform calculations on the values returned by the detectors. When cross-validated, the best genetically-evolved recognizer for transmembrane domains achieves an out-of-sample correlation of 0.968 and an out-of-sample error rate of 1.6%. This error rate is better than that recently reported for five other methods.",
An Ada model for the specification of control systems,"At present, design teams which need to produce animated specifications of control systems have to write their models from scratch. Usually this is done in an ad hoc manner, rather than exploiting generic results from computer science. This article proposes a formal model, for use in practical projects, based on extensions to the 'Parnas (A7) dataflow model' of computation. The model is described using Ada generic packages, which need instantiation for the project in hand. An example of automotive throttle control is used to illustrate the method.",
An ADC test system based on the Macintosh computer,"An ADC test system based on UA1 Macintosh development system is described. The test system includes a CAMAC, 12 bit, 1 /spl mu/s ADC module, an I/O Register, an HP pulse generator, a NIM dual timer and a GPIB programmable Keithley 237 high precision voltage source. The first test program was written in Fortran which gave the test results and histograms on the screen. A new Lab-VIEW 2 test program is also described. An upgrade of the test system and the CAMAC control to VME ADC is now in progress.",
"Using a computer model to assist in teaching the interaction between land use, transport and the environment","Knowledge of the interaction between land use, transport and the environment is fundamental in understanding the impact of transport on cities' environment. Many students do not fully understand the complexities of this interaction. Traditional lectures and tutorials alone do not do justice to the level of interaction and a more interactive educational approach is required. This paper discusses an interactive learning approach built around an educational computer system called LAND. Environmental and land use modellers have made considerable steps forward in the development of theory and the incorporation of this theory into computer models. Also, recent developments in computer software provide considerable potential for creating a user friendly, interactive, dynamic, easy to learn, and visually stimulating computer environment which can assist students in interacting with computer models. LAND has brought together these developments in a form that can assist in education. This paper introduces the teaching aims and approach, briefly describes the model and then outlines experiences in using this interactive teaching tool.",
Efficient Implementation Of An Abstract Programming Model For Image Processing On Transputers,,
Design analysis of a high resolution detector block for a low cost small animal positron emission imaging system,"Computer simulations have been run to evaluate and optimize a high resolution block detector design for a low cost small animal PET system. The proposed detector module would be made using GSO crystals instead of the more commonly used scintillator, BGO. The main advantage of using GSO instead of BGO is that recent samples of GSO have been measured to produce at least twice the light output of BGO. Because of the additional light, it should be possible to develop a practical 6/spl times/14 crystal GSO detector module. This is approximately a factor of two greater than the number of crystals which are currently being decoded using BGO. Using the design the authors propose, the intrinsic spatial resolution of the detector module should be <2.5 mm FWHM in both dimensions and provide adequate sampling when assembled in a ring configuration to meet the Nyquist sampling requirements for 3 mm FWHM image resolution. The system the authors propose would have a 15.6 cm detector ring diameter, an 8 cm in plane field of view, and a 2.1 cm axial field of view. The proposed system would consist of 12 detector modules and would require only 24 dual photocathode PMTs and /spl sim/127 cc of GSO.",
Evaluation of write-back caches for multiple block-sizes,"Trace-driven simulation of cache memories usually requires enormous disk space to store CPU trace data. On-the-fly simulation avoids this problem by running the simulation while the trace data is being generated. For write-back least-recently-used (LRU) caches, we propose an on-the-fly simulation method, which is a variant of the standard one-pass stack evaluation techniques that evaluates write ratios for multiple block-sizes in a one-pass processing of trace data. It is known that for LRU cache memories, the hit ratios can be found for multiple block-sizes in one-pass trace processing; and for write-back caches, the write ratios can be found for a single block size in one-pass trace processing. Combining these two techniques, this paper proposes a simple one-pass method for write-back LRU caches that evaluates write ratios for multiple block-sizes.",
A counterpropagation neural network for determining target spacecraft orientation,"This paper describes a concept that integrates a counterpropagation neural network into a video-based vision system employed for automatic spacecraft docking. A brief overview of docking phases, the target orientation problem, and potential benefits resulting from an automated docking system is provided. Issues and challenges of automatic target recognition, as applied to automatic docking, are addressed. Following a review of the architecture, training, and desirable characteristics of the counterpropagation network, an approach for determining the relative orientation of a target spacecraft based on a counterpropagation net is presented.",
A Presentation Agent For Multilingual Services,,
Parallel training of simple recurrent neural networks,"A concurrent implementation of the method of conjugate gradients for training Elman networks is discussed. The parallelism is obtained in the computation of the error gradient and the method is therefore applicable to any gradient descent training technique for this form of network. The experimental results were obtained on a Sun Sparc Center 2000 multicomputer. The Spare 2000 is a shared memory machine well suited to coarse-grained distributed computations, but the concurrency could be extended to other architectures as well.<>",
The Army High Performance Computing Research Center,"The US Army High Performance Computing Research Center (AHPCRC) provides nationwide leadership in high performance computing (HPC) research. Its supercomputing resources are maintained and operated by the Minnesota Supercomputer Center, Inc. AHPCRC researchers are developing novel techniques and advanced algorithms and applying them to physical problems, as well as evaluating and using advanced computing platforms for HPC applications. The Center trains future scientists and engineers by sponsoring computer science educational research at the undergraduate, MS, PhD, and postdoctoral levels. It provides a framework for HPC technology transfer and infrastructure support to Army laboratories by conducting joint research projects between Army scientists and Center researchers; by jointly supervising the staff scientists placed by the Center at the Army laboratories; and by encouraging visits between Army scientists and Center researchers. Finally, the AHPCRC provides a state-of-the-art HPC environment to its academic partners, to Army and Defense Department scientists, and to industrial and international collaborators.",
An uncertain rule matching technique in an expert system,"We developed a special technique, called equivalent shifting in three dimensions (ES3D) for rule matching in a weather forecasting expert system CME. ES3D is able to describe the evolving procedures of domestic systems fully and exactly in three aspects: strength, position, and concept. ES3D is able to overcome the well-known brittleness of the first generation expert systems, then enhances the capabilities of expert systems adapting to evidences. The idea lying in ES3D can be applied to other similar task domains.",
A new combinatorial approach to optimal embeddings of rectangles,"An important problem in graph embeddings and parallel computing is to embed a rectangular grid into other graphs. We present a novel, general, combinatorial approach to (one-to-one) embedding rectangular grids into their ideal rectangular grids and optimal hypercubes. In contrast to earlier approaches of Aleliunas and Rosenberg, and Ellis (1982), our approach is based on a special kind of doubly stochastic matrix. We prove that any rectangular grid can be embedded into its ideal rectangular grid with dilation equal to the ceiling of the compression ratio, which is both optimal up to a multiplicative constant and a substantial generalization of previous work. We also show that any rectangular grid can be embedded into its nearly ideal square grid with dilation at most 3. Finally, we show that any rectangular grid can be embedded into its optimal hypercube with optimal dilation 2, a result previously obtained, after much research, through an ad hoc approach. Our results also imply optimal simulations of 2-dimensional mesh-connected parallel machines by hypercubes and mesh-connected machines, where each processor in the guest is simulated by exactly one processor in the host.",
Content-Based Hypermedia-intelligent browsing of structured media objects,"Interacting with a multimedia information system is different from interacting with a standard text-based information system. In this paper, we present the design of a system called Content-Based Hypermedia (CBH), which allows a user to utilize metadata to intelligently browse through a collection of media objects. We describe the approach we use to model data in order to make it browsable, explore browsing (data and metadata), indicate how metadata is used in the concept of similarity, present the architecture of our system, and discuss indexing techniques for similarity browsing using content-based metadata.",
Near-distance software engineering education,"The University of Wollongong has been seeking ways by which it can attract capable students from the southern side of the States capital city, Sydney, Australia. The paper describes the concept of limiting the required daily travel to the University through an amalgam of technologies, and changes to the core teaching syllabus. Progress through 1994, and plans for full introduction in 1995, is described.",
Back-propagation neural networks for the inverse control of discrete-time nonlinear plant,"A backpropagation (BP) neural networks are applied to two kinds of inverse control methods for a discrete-time nonlinear plant. Two kinds of topological structure of BP neural networks are provided. Simulation results show that the tracking performance of a discrete-time nonlinear plant has been obtained satisfactorily in two kinds of input signal. In order to improve the tracking performance of the system, a combination of a variety of mapping functions is proposed. The learning and response of two networks have been developed in the Professional II plus neural networks. The program that connected in the neural networks is written in C language.",
Explicit versus implicit set-covering for supervised learning,"It has been shown that implicit covering algorithms are effective for learning concepts from preclassified training examples. In this paper, we show that by making these covering algorithms explicit, concepts with lower error rates can be learned. Experimental results are reported for three real domains.",
Si/SiGe Heterostructure p-MOSFET with Triangular Ge Channel Profiles,"The first SiGe p-MOSFETs with triangular Ge profiles, fabricated in a Si CMOS-compatible LOCOS isolated process are reported. The feasibility of triangular profiles with peak Ge mole fractions as high as 50% is demonstrated for both CVD and MBE MOSFETs. The transconductance of 3 μm devices with 0-40% Ge profiles is 34 mS/mm, 100% higher than that of the corresponding Si p-MOSFET fabricated on the same chip.",
An analysis of edge fault tolerance in recursively decomposable regular networks,"Fault tolerance of interconnection networks is one of the major considerations in evaluating the reliability of large scale multiprocessor systems. In the paper, the reliability of a family of regular networks with respect to edge failures is investigated using four different fault tolerance measures. Two probabilistic measures, resilience and restricted resilience, are developed, used to evaluate disconnection likelihoods using two different failure models, and compared with corresponding deterministic measures. The network topologies chosen for the present study all have the recursive decomposition property, where larger networks can be decomposed into copies of smaller networks of the same topology. This family of graphs includes the k-ary n-cube, star and cube connected cycle graphs, which have optimal deterministic connectivities. The probabilistic fault tolerance measures, however, are found to depend on topological properties such as network size and degree.","Fault tolerance,
Intelligent networks,
Resilience,
Network topology,
Multiprocessor interconnection networks,
Large-scale systems,
Size measurement,
Failure analysis,
Computer science,
Character generation"
Probability-one homotopy algorithms for full and reduced order H/sup 2//H/sup /spl infin// controller synthesis,"Homotopy algorithms for both full- and reduced-order LQG controller design problems with an H/sup /spl infin// constraint on disturbance attenuation are developed. The H/sup /spl infin// constraint is enforced by replacing the covariance Lyapunov equation by a Riccati equation whose solution gives an upper bound on H/sup 2/ performance. The numerical algorithm, based on homotopy theory, solves the necessary conditions for a minimum of the upper bound on H/sup 2/ performance. The algorithms are based on a minimal parameter formulation: Ly-Bryson-Cannon's 2/spl times/2 block parametrization. Numerical experiments suggest that the combination of a globally convergent homotopy method and a minimal parameter formulation applied to the upper bound minimization gives excellent results for mixed-norm H/sup 2//H/sup /spl infin// synthesis. The nonmonotonicity of homotopy zero curves is demonstrated, proving that algorithms more sophisticated than standard continuation are necessary.",
PARTS-a temporal logic-based real-time software specification method supporting multiple-viewpoints,"Areas of computer application are being broadened rapidly due to the rapid improvement of the performance of computer hardware. This results in increased demands for computer applications that are large and have complex temporal characteristics. This paper introduces a real-time systems analysis method named PARTS. PARTS supports analyses from two viewpoints: external viewpoint, a view of the system from the user's perspective, and internal viewpoint, a view from the developer's perspective. These viewpoints are specified using formal languages, which are: Real-Time Events Trace (RTET) for the external viewpoint, and Time Enriched Statechart (TES) and PARTS Data Flow Diagram (PDFD) for the internal viewpoint. All PARTS languages are based on the Metric Temporal Logic (MTL), and consistency of the specifications made from the two different viewpoints are analyzed based on the same MTL formalism.",
Modeling of Buffered Multiple Bus Multiprocessor Systems in a Non-Uniform Traffic Environment,,
