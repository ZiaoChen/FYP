{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ieee = pd.read_csv(\"Data/IEEE-Computer-Science-2017.csv\")\n",
    "ieee[\"Combined_text\"] = ieee[\"Title\"] + ieee[\"Abstract\"] + ieee[\"Keywords\"]\n",
    "raw = ieee[\"Combined_text\"]\n",
    "raw = raw.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1483, 600)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.04080061,  0.        ],\n",
       "       [ 0.0625694 ,  0.        ,  0.        , ...,  0.04504443,\n",
       "         0.0563764 ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.05041872, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "#Tokenize the text\n",
    "def tokenize(text):\n",
    "\n",
    "    #Create Stemmer\n",
    "    #stemmer = PorterStemmer()\n",
    "    stemmer = WordNetLemmatizer()\n",
    "\n",
    "    #Remove irrelevant character\n",
    "    text = re.sub(r\"[^a-zA-Z]\", ' ', text)\n",
    "\n",
    "    #Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "\n",
    "    #Stemming\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "#Stemming Function\n",
    "def stem_tokens(t,s):\n",
    "    stemmed=[]\n",
    "    for item in t:\n",
    "        # stemmed.append(s.stem(item))\n",
    "        stemmed.append(s.lemmatize(item))\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "# tfidf_vectorizer = CountVectorizer(stop_words='english', tokenizer=tokenize, min_df=50)\n",
    "max_feature = 800\n",
    "tfidf_vectorizer_auto = TfidfVectorizer(stop_words='english', tokenizer=tokenize, max_df=500, max_features=600)\n",
    "tfidf_data = tfidf_vectorizer_auto.fit_transform(raw)\n",
    "print tfidf_data.shape\n",
    "tfidf_data.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "# stemmer = PorterStemmer()\n",
    "sentences = []\n",
    "paper_list = []\n",
    "for paper in raw:\n",
    "    word_list = []\n",
    "    for sent in sent_tokenize(paper.decode('utf-8')):\n",
    "        words = word_tokenize(sent)\n",
    "        # sentences.append([stemmer.stem(word.lower()) for word in words if word not in string.punctuation])\n",
    "        sentences.append([stemmer.lemmatize(word.lower()) for word in words if word not in string.punctuation and word not in stops and not word.isdigit()])\n",
    "        word_list += words\n",
    "    paper_list.append(word_list)\n",
    "word2vec_model = gensim.models.Word2Vec(sentences, size=400, window=6, min_count=20, workers=4, iter=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(paper_list)):\n",
    "    new_list = []\n",
    "    for word in paper_list[i]:\n",
    "        if word.lower() not in stops:\n",
    "            new_list.append(word)\n",
    "    paper_list[i] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer = tokenize)\n",
    "tfidf_data = tfidf_vectorizer.fit_transform(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ziaochen/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  import sys\n",
      "/Users/ziaochen/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "paper_vector = []\n",
    "paper_count = 0\n",
    "for paper in paper_list:\n",
    "    vector = np.zeros(400)\n",
    "    diviser = 0\n",
    "    for word in paper:\n",
    "        if word in word2vec_model and word in tfidf_vectorizer.vocabulary_:\n",
    "            tfidf_value = tfidf_data[paper_count,tfidf_vectorizer.vocabulary_[word]]\n",
    "            diviser += tfidf_value\n",
    "            vector += word2vec_model[word]*tfidf_value\n",
    "    paper_vector.append(vector / diviser)\n",
    "    paper_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1483, 400)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_word2vec = np.array(paper_vector)\n",
    "data_word2vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): Only clang++ is supported. With g++, we end up with strange g++/OSX bugs.\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "import pylab\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(n_visible, n_hidden):\n",
    "    initial_W = np.asarray(\n",
    "        np.random.uniform(\n",
    "            low=-4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
    "            high=4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
    "            size=(n_visible, n_hidden)),\n",
    "        dtype=theano.config.floatX)\n",
    "    return theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "def init_bias(n):\n",
    "    return theano.shared(value=np.zeros(n,dtype=theano.config.floatX),borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.fmatrix('x')  \n",
    "d = T.fmatrix('d')\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "\n",
    "training_epochs = 200\n",
    "learning_rate = 0.1\n",
    "batch_size = 16\n",
    "first_dimension = min(50,int(max_feature*0.7))\n",
    "# second_dimension = 50\n",
    "\n",
    "W1 = init_weights(tfidf_data.shape[1], first_dimension)\n",
    "b1 = init_bias(first_dimension)\n",
    "b1_prime = init_bias(tfidf_data.shape[1])\n",
    "W1_prime = W1.transpose() \n",
    "# W2 = init_weights(first_dimension, second_dimension)\n",
    "# b2 = init_bias(second_dimension)\n",
    "# W2_prime = W2.transpose()\n",
    "# b2_prime = init_bias(first_dimension)\n",
    "\n",
    "y1 = T.nnet.sigmoid(T.dot(x, W1) + b1)\n",
    "# y2 = T.nnet.sigmoid(T.dot(y1,W2) + b2)\n",
    "# z2 = T.nnet.sigmoid(T.dot(y2, W2_prime) + b2_prime)\n",
    "z1 = T.nnet.sigmoid(T.dot(y1, W1_prime) + b1_prime)\n",
    "cost1 = T.sum((x-z1)**2)\n",
    "\n",
    "params1 = [W1, b1, b1_prime]\n",
    "grads1 = T.grad(cost1, params1)\n",
    "updates1 = [(param1, param1 - learning_rate * grad1)\n",
    "           for param1, grad1 in zip(params1, grads1)]\n",
    "train_da1 = theano.function(inputs=[x], outputs = cost1, updates = updates1, allow_input_downcast = True)\n",
    "test = theano.function(inputs=[x], outputs = y1, allow_input_downcast = True)\n",
    "upsampling = theano.function(inputs=[y1], outputs = z1, allow_input_downcast = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dae1 ...\n",
      "2.81372889943\n",
      "0.938533686517\n",
      "0.935897322708\n",
      "0.934292089702\n",
      "0.932874402311\n",
      "0.931305809726\n",
      "0.929279202763\n",
      "0.926574815161\n",
      "0.923542339562\n",
      "0.92093263833\n",
      "0.918838786384\n",
      "0.916903161026\n",
      "0.91482881625\n",
      "0.912435098219\n",
      "0.909649132539\n",
      "0.906519033168\n",
      "0.903150249281\n",
      "0.899569825896\n",
      "0.895685512164\n",
      "0.891494068704\n",
      "0.887245851225\n",
      "0.883058538235\n",
      "0.878887300092\n",
      "0.874775939124\n",
      "0.870751638274\n",
      "0.866745154619\n",
      "0.862680453923\n",
      "0.858520306262\n",
      "0.854261541528\n",
      "0.850041971324\n",
      "0.846087405964\n",
      "0.842436761954\n",
      "0.838972522607\n",
      "0.835582436528\n",
      "0.832209233066\n",
      "0.82883555787\n",
      "0.825462372455\n",
      "0.822096953161\n",
      "0.81874764056\n",
      "0.815418283878\n",
      "0.812101128564\n",
      "0.808771857753\n",
      "0.805389413455\n",
      "0.801900476543\n",
      "0.798250639758\n",
      "0.794406312542\n"
     ]
    }
   ],
   "source": [
    "print('training dae1 ...')\n",
    "d = []\n",
    "for epoch in range(training_epochs):\n",
    "    # go through trainng set\n",
    "    c = []\n",
    "    for start, end in zip(range(0, tfidf_data.shape[0], batch_size), range(batch_size, tfidf_data.shape[0], batch_size)):\n",
    "        c.append(train_da1(tfidf_data.toarray()[start:end]))\n",
    "    d.append(np.mean(c, dtype='float64') / batch_size)\n",
    "    print(d[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.79119289  0.21104235  0.55880483 ...,  0.70685674  0.02187131\n",
      "   0.26534901]\n",
      " [ 0.52292467  0.62275458  0.40969103 ...,  0.27858279  0.01665859\n",
      "   0.67977598]\n",
      " [ 0.7399837   0.38220733  0.45604365 ...,  0.13080241  0.01343907\n",
      "   0.42152503]\n",
      " ..., \n",
      " [ 0.40882439  0.68291476  0.30476679 ...,  0.49719277  0.01499242\n",
      "   0.39519157]\n",
      " [ 0.43642496  0.44110755  0.29942032 ...,  0.44078169  0.0124258\n",
      "   0.47495906]\n",
      " [ 0.50832936  0.28121095  0.16326793 ...,  0.23035646  0.00860837\n",
      "   0.28306392]]\n"
     ]
    }
   ],
   "source": [
    "data = test(tfidf_data.toarray())\n",
    "print data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "class TopicTree:\n",
    "    def __init__(self, level=0, index=1, children=[], keywords=[], n_top_words=10):\n",
    "        self.children = children\n",
    "        self.keywords = keywords\n",
    "        self.level = level\n",
    "        self.index = index\n",
    "        self.dup_index = 0\n",
    "        self.association_index = 0\n",
    "        self.base_url = 'https://api.wordassociations.net/associations/v1.0/json/search?apikey=85618aa1-21c2-4382-9dc6-373c5b1424b8&lang=en&limit=50'\n",
    "        self.n_top_words = n_top_words\n",
    "    \n",
    "    def get_number_of_children(self):\n",
    "        return len(self.children)\n",
    "\n",
    "    def get_children(self):\n",
    "        return children\n",
    "    \n",
    "    def get_topic_content(self):\n",
    "        return self.topic_content\n",
    "\n",
    "    def compute_dup_index(self):\n",
    "        if not self.children:\n",
    "            return\n",
    "        children_keywords = []\n",
    "        for child in self.children:\n",
    "            children_keywords += child.keywords\n",
    "            child.compute_dup_index()\n",
    "        original_len = len(children_keywords)\n",
    "        children_keywords = list(set(children_keywords))\n",
    "        num_of_duplicate = original_len - len(children_keywords)\n",
    "        self.dup_index = float(num_of_duplicate) / original_len\n",
    "        \n",
    "    def get_whole_tree_dup_index(self):\n",
    "        if self.dup_index == 0:\n",
    "            self.compute_dup_index()\n",
    "        if not self.children:\n",
    "            return self.dup_index\n",
    "        children_index = 0\n",
    "        for child in self.children:\n",
    "            children_index += child.dup_index\n",
    "        children_index = children_index / len(self.children)\n",
    "        return (self.dup_index + children_index) / 2\n",
    "    \n",
    "    def compute_association_index(self):\n",
    "        base_url = self.base_url\n",
    "        word_sum = 0\n",
    "        if len(self.keywords) <= 10:\n",
    "            for word in self.keywords:\n",
    "                base_url = base_url + '&text=' + word\n",
    "            response = requests.get(base_url).json()['response']\n",
    "        else:\n",
    "            for word in self.keywords[:10]:\n",
    "                base_url = base_url + '&text=' + word\n",
    "            response = requests.get(base_url).json()['response']\n",
    "            base_url = self.base_url\n",
    "            for word in self.keywords[10:]:\n",
    "                base_url = base_url + '&text=' + word\n",
    "            response = response + requests.get(base_url).json()['response']\n",
    "        for word_json in response:\n",
    "            items = word_json['items']\n",
    "            for item in items:\n",
    "                if item['item'] in self.keywords:\n",
    "                    word_sum += int(item['weight'])\n",
    "        self.association_index = float(word_sum) / self.n_top_words / (self.n_top_words - 1)\n",
    "    \n",
    "    def get_whole_tree_association_index(self):\n",
    "        if self.association_index == 0:\n",
    "            self.compute_association_index()\n",
    "        if not self.children:\n",
    "            return self.association_index\n",
    "        children_index = 0\n",
    "        for child in self.children:\n",
    "            children_index += child.association_index\n",
    "        children_index = children_index / len(self.children)\n",
    "        return (self.dup_index + children_index) / 2\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Soft Clustering Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FuzzyClustering:\n",
    "    \n",
    "    def __init__(self,weight_vector, dictionary, epochs=10, num_topic_range=10,m=2, word2vec=False):\n",
    "        self.epochs = epochs\n",
    "        self.num_topic_range = num_topic_range\n",
    "        self.m = m\n",
    "        self.pre_cost = 100000000\n",
    "        self.weight_vector = weight_vector\n",
    "        self.dictionary = dictionary\n",
    "        self.word2vec = word2vec\n",
    "        \n",
    "    def get_silhouette_coefficient(self):\n",
    "        num_doc = self.data.shape[0]\n",
    "        num_topic = self.doc_topic_weight_mat.shape[1]\n",
    "        topic_format_data_list=[]\n",
    "        for topic_index in range(num_topic):\n",
    "            topic_format_data_list.append(((self.data.T)*self.doc_topic_weight_mat[:,topic_index]).T)  \n",
    "        sil_coe_all = 0\n",
    "        \n",
    "    \n",
    "        for doc_index in range(num_doc):\n",
    "            for topic_index in range(num_topic):\n",
    "                doc = topic_format_data_list[topic_index][doc_index]\n",
    "                a = np.mean(np.linalg.norm(topic_format_data_list[topic_index] - doc, axis=1))\n",
    "                b = np.finfo(float).max\n",
    "                sil_coe = -1\n",
    "                for topic_index2 in range(num_topic):\n",
    "                    if topic_index2 != topic_index:\n",
    "                        b = min(b, np.mean(np.linalg.norm(topic_format_data_list[topic_index2] - doc, axis=1)))\n",
    "                sil_coe = max(sil_coe, (b - a) / max(a,b))\n",
    "            sil_coe_all += sil_coe\n",
    "        return sil_coe_all / num_doc\n",
    "    \n",
    "    def get_MPC(self):\n",
    "        num_doc = self.doc_topic_weight_mat.shape[0]\n",
    "        num_topic = self.doc_topic_weight_mat.shape[1]\n",
    "        pc = np.sum(np.square(self.doc_topic_weight_mat)) / num_doc\n",
    "        if num_topic == 1:\n",
    "            mpc = 0.90\n",
    "        else:\n",
    "            mpc = 1 - float(num_topic) / (num_topic - 1) * (1 - pc)\n",
    "        return mpc\n",
    "    \n",
    "    def do_clustering(self,doc_word_mat, num_cluster=0, num_topic=10, exp=False):\n",
    "        num_doc = doc_word_mat.shape[0]\n",
    "        doc_topic_weight_mat = self.initialize_weight_mat(num_doc,num_topic)\n",
    "        centroids = self.initialize_centroid(doc_word_mat, num_topic)\n",
    "        self.pre_cost = 1000000000\n",
    "        for epoch in range(self.epochs):\n",
    "            doc_topic_weight_mat = self.update_weight_mat(doc_word_mat, doc_topic_weight_mat, centroids, exp) \n",
    "            cost = self.get_cost(doc_word_mat, doc_topic_weight_mat, centroids)\n",
    "            # print cost\n",
    "            if cost > self.pre_cost:\n",
    "                self.data = doc_word_mat\n",
    "                self.doc_topic_weight_mat = doc_topic_weight_mat\n",
    "                self.centroids = centroids\n",
    "                return doc_topic_weight_mat, centroids\n",
    "            self.pre_cost = cost\n",
    "            centroids = self.compute_centroid(doc_word_mat, doc_topic_weight_mat)\n",
    "        # doc_topic_weight_mat = (doc_topic_weight_mat.T / doc_topic_weight_mat.sum(axis=1)).T\n",
    "        self.data = doc_word_mat\n",
    "        self.doc_topic_weight_mat = doc_topic_weight_mat\n",
    "        self.centroids = centroids\n",
    "        return doc_topic_weight_mat, centroids\n",
    "                \n",
    "        \n",
    "    def compute_centroid(self, doc_word_mat, doc_topic_weight_mat):\n",
    "        num_cen = doc_topic_weight_mat.shape[1]\n",
    "        num_word = doc_word_mat.shape[1]\n",
    "        \n",
    "        centroids = np.zeros((num_cen, num_word))\n",
    "        for cen_index in range(num_cen):\n",
    "            centroids[cen_index] = np.dot(np.power(doc_topic_weight_mat.T[cen_index], self.m)*self.weight_vector, doc_word_mat) / np.sum(np.power(doc_topic_weight_mat.T[cen_index],self.m)*self.weight_vector)\n",
    "        return centroids\n",
    "    \n",
    "    def initialize_centroid(self, doc_word_mat, num_topic):\n",
    "        return doc_word_mat[np.random.randint(0,doc_word_mat.shape[0],num_topic),:] + 0.0001\n",
    "    \n",
    "    def initialize_weight_mat(self,row,col):\n",
    "        # return np.random.uniform(low=0, high=2.0/col, size=(row,col))\n",
    "        return np.full((row,col),1.0/col, dtype=float)\n",
    "    \n",
    "    def update_weight_mat(self,doc_word_mat, doc_topic_weight_mat, centroids, exp=False):\n",
    "        num_doc = doc_topic_weight_mat.shape[0]\n",
    "        num_topic = doc_topic_weight_mat.shape[1]\n",
    "        \n",
    "        if exp:\n",
    "            doc_topic_distance = np.zeros((num_doc, num_topic))\n",
    "            for topic_index in range(num_topic):\n",
    "                Fk = np.zeros((doc_word_mat.shape[1], doc_word_mat.shape[1]))\n",
    "                Fk_diviser = np.sum(doc_topic_weight_mat[:,topic_index]*self.weight_vector)\n",
    "                \n",
    "                for doc_index in range(num_doc):\n",
    "                    Fk += doc_topic_weight_mat[doc_index][topic_index]*self.weight_vector[doc_index]*np.outer(centroids[topic_index] - doc_word_mat[doc_index], centroids[topic_index] - doc_word_mat[doc_index])\n",
    "                Fk = Fk/Fk_diviser\n",
    "                Fk_determiner = np.linalg.det(Fk)**0.5\n",
    "                Fk_inverse = np.linalg.inv(Fk)\n",
    "                ak = np.sum(doc_topic_weight_mat[:,topic_index]) / np.sum(self.weight_vector)\n",
    "                for doc_index in range(num_doc):\n",
    "                    difference = centroids[topic_index] - doc_word_mat[doc_index]\n",
    "                    doc_topic_distance[doc_index][topic_index] = Fk_determiner / ak*np.exp(np.dot(np.dot(difference, Fk_inverse),difference/2))\n",
    "                    \n",
    "            \n",
    "            for doc_index in range(num_doc):\n",
    "                dist_to_all_clusters = np.sum(doc_topic_distance[doc_index]) + np.finfo(float).eps\n",
    "                for topic_index in range(num_topic):\n",
    "                    doc_topic_weight_mat[doc_index][topic_index] = np.power(doc_topic_distance[doc_index][topic_index] / dist_to_all_clusters, 2.0/(self.m-1))\n",
    "            \n",
    "        else:\n",
    "            for doc_index in range(num_doc):\n",
    "                dist_to_all_cluster = np.linalg.norm(centroids - doc_word_mat[doc_index], axis=1)\n",
    "\n",
    "                new_weight = np.zeros(num_topic)\n",
    "                for topic_index in range(num_topic):\n",
    "\n",
    "                    new_weight += np.power(dist_to_all_cluster / (np.linalg.norm(centroids[topic_index] - doc_word_mat[doc_index])+np.finfo(float).eps),2.0/(self.m-1))\n",
    "                    # new_weight += np.power(dist_to_all_cluster / (1-np.sum(centroids[topic_index] * doc_word_mat[doc_index])/ np.linalg.norm(centroids[topic_index]) / np.linalg.norm(doc_word_mat[doc_index])),2/(self.m-1))\n",
    "\n",
    "                doc_topic_weight_mat[doc_index] = 1.0 / new_weight\n",
    "           \n",
    "        \n",
    "        return doc_topic_weight_mat\n",
    "    \n",
    "    def get_cost(self, doc_word_mat, doc_topic_weight_mat, centroids):\n",
    "        cost = 0\n",
    "        num_doc = doc_topic_weight_mat.shape[0]\n",
    "        num_topic = doc_topic_weight_mat.shape[1]\n",
    "        \n",
    "        for doc_index in range(num_doc):\n",
    "            for topic_index in range(num_topic):\n",
    "                cost += doc_topic_weight_mat[doc_index, topic_index]**self.m *self.weight_vector[doc_index]* np.linalg.norm(centroids[topic_index] - doc_word_mat[doc_index])\n",
    "                # cost += doc_topic_weight_mat[doc_index, topic_index]**self.m * (1-np.sum(centroids[topic_index] * doc_word_mat[doc_index])/ np.linalg.norm(centroids[topic_index]) / np.linalg.norm(doc_word_mat[doc_index]))\n",
    "        return cost\n",
    "\n",
    "    def print_top_words(self,n_top_words, topics, feature_names):\n",
    "        topic_idx = 0\n",
    "        if self.word2vec:\n",
    "            for topic in topics:\n",
    "                topic_words = [x[0] for x in word2vec_model.wv.similar_by_vector(topic, topn=n_top_words)]\n",
    "                # topic_words.sort()\n",
    "                topic_idx += 1\n",
    "                message = \"Topic #%d: \" % topic_idx\n",
    "                message += \" \".join(topic_words)\n",
    "                print message\n",
    "        else:\n",
    "            topics = upsampling(topics)\n",
    "            for topic in topics:\n",
    "                topic_idx += 1\n",
    "                message = \"Topic #%d: \" % topic_idx\n",
    "                topic_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "                message += \" \".join(topic_words)\n",
    "                print message\n",
    "    \n",
    "    def get_top_words(self, level, n_top_words, topics, feature_names):\n",
    "        children = []\n",
    "        topic_idx = 0\n",
    "        if self.word2vec:\n",
    "            for topic in topics:\n",
    "                topic_words = [x[0] for x in word2vec_model.wv.similar_by_vector(topic, topn=n_top_words)]\n",
    "                topic_idx += 1\n",
    "                children.append(TopicTree(level=level, index=topic_idx, keywords=topic_words, n_top_words=n_top_words))\n",
    "                \n",
    "        else:\n",
    "            topics = upsampling(topics)\n",
    "            for topic in topics:\n",
    "                topic_idx += 1\n",
    "                topic_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "                children.append(TopicTree(level=level, index=topic_idx, keywords=topic_words, n_top_words=n_top_words))\n",
    "        return children\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hierarchical_Fuzzy_Clustering:\n",
    "    def __init__(self,dictionary, max_level=3,epochs=10, m=2, max_topic=5, n_top_words=10):\n",
    "        self.dictionary = dictionary\n",
    "        self.max_level = max_level\n",
    "        self.m = m\n",
    "        self.max_topic = max_topic\n",
    "        self.epochs = epochs\n",
    "        self.topic_tree = TopicTree(level=1, index=1)\n",
    "        self.current_tree = self.topic_tree\n",
    "        self.n_top_words = n_top_words\n",
    "    \n",
    "    def do_hierarchical_clustering(self,data, weight_vector, level=1,topic_index=1, word2vec=False):\n",
    "        \n",
    "        if level==1:\n",
    "            self.build_root(data, word2vec)\n",
    "            \n",
    "        if level > self.max_level:\n",
    "            return\n",
    "        \n",
    "        print \"\\nlevel %d topic %d\" %(level, topic_index)\n",
    "        model = FuzzyClustering(dictionary=self.dictionary, weight_vector=weight_vector, epochs=self.epochs, m=self.m, word2vec=word2vec)\n",
    "        target_MPC = 0\n",
    "        target_doc_topic_weight_matrix = np.array([])\n",
    "        target_topics = np.array([])\n",
    "        for num_topic in range(1,self.max_topic+1): \n",
    "            doc_topic_weight_matrix, topics = model.do_clustering(data, num_topic=num_topic)\n",
    "            MPC = model.get_MPC()\n",
    "            if MPC > target_MPC:\n",
    "                target_MPC = MPC\n",
    "                target_doc_topic_weight_matrix = doc_topic_weight_matrix\n",
    "                target_topics = topics\n",
    "            \n",
    "        num_topics = target_doc_topic_weight_matrix.shape[1]\n",
    "        print \"Detect %d topics\" %num_topics\n",
    "        model.print_top_words(self.n_top_words, target_topics, self.dictionary)\n",
    "        self.current_tree.children = model.get_top_words(level + 1, self.n_top_words, target_topics, self.dictionary)\n",
    "        \n",
    "        children = self.current_tree.children\n",
    "        if num_topics > 1:\n",
    "            for topic_index in range(num_topics):\n",
    "                self.current_tree = children[topic_index]\n",
    "                self.do_hierarchical_clustering(data, target_doc_topic_weight_matrix.T[topic_index], level=level+1, topic_index = topic_index+1, word2vec=word2vec)\n",
    "    \n",
    "\n",
    "    def build_root(self, data, word2vec=False):\n",
    "        print \"\\nlevel 0\"\n",
    "        message = \"#Topic 1: \" \n",
    "        if word2vec:\n",
    "            centroids = np.mean(data, axis=0)\n",
    "            topic_words = [x[0] for x in word2vec_model.wv.similar_by_vector(centroids, topn=20)]\n",
    "            # topic_words.sort()\n",
    "\n",
    "\n",
    "        else:\n",
    "            centroids = np.mean(data,axis=0)\n",
    "            centroids = upsampling([centroids])\n",
    "            feature_names = self.dictionary\n",
    "            topic_words = [feature_names[i]\n",
    "                                 for i in centroids[0].argsort()[:20]]\n",
    "\n",
    "        self.current_tree.keywords = topic_words\n",
    "        message += \" \".join(topic_words)   \n",
    "        print message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering on Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "level 0\n",
      "#Topic 1: concept proposes example obtain focus iot comparison traditional respectively fault effectively mimo called lower dictionary benefit relay outperforms color antenna\n",
      "\n",
      "level 1 topic 1\n",
      "Detect 8 topics\n",
      "Topic #1: optimization problem solution approach objective optimal function distributed analysis design vehicle parameter network dynamic strategy framework learning set process power\n",
      "Topic #2: image feature learning camera approach quality human extraction object training classifier task sparse database information representation imaging framework different analysis\n",
      "Topic #3: control power stability controller dynamic nonlinear state distributed analysis grid optimal vehicle time problem design output approach uncertainty attack process\n",
      "Topic #4: information task user network social traffic framework analysis human real approach large cluster learning search video propose computing modeling mobile\n",
      "Topic #5: high frequency circuit power bandwidth low measurement current mode field phase analysis structure wa modulation effect design voltage array band\n",
      "Topic #6: cloud computing application service resource design architecture server network software task management cost internet communication traffic virtual search physical memory\n",
      "Topic #7: signal parameter high analysis noise matrix process approach state measurement estimation technique sensing design array phase multiple information low modulation\n",
      "Topic #8: network wireless communication interference energy transmission receiver access cellular throughput power radio node multiple user scheme information optimal channel design\n",
      "\n",
      "level 2 topic 1\n",
      "Detect 3 topics\n",
      "Topic #1: mimo matrix design problem multiple optimization optimal solution analysis output stability function approach uncertainty source linear power parameter controller mean\n",
      "Topic #2: optimization problem objective solution approach function optimal parameter design analysis fuzzy learning distributed process set dynamic state network strategy structure\n",
      "Topic #3: problem vehicle optimization network optimal distributed solution time control traffic approach framework analysis resource dynamic topology cost management allocation strategy\n",
      "\n",
      "level 2 topic 2\n",
      "Detect 4 topics\n",
      "Topic #1: image feature quality visual imaging camera target object map information approach database framework structure human view different sparse high spatial\n",
      "Topic #2: color image user visual camera low analysis feature high approach visualization extraction target quality view object region study current tracking\n",
      "Topic #3: learning image sparse classification dictionary face feature training classifier representation information sample approach matrix structure database object analysis function problem\n",
      "Topic #4: feature recognition human learning task extraction neural classifier image training deep camera monitoring approach traffic object analysis detection network framework\n",
      "\n",
      "level 2 topic 3\n",
      "Detect 6 topics\n",
      "Topic #1: voltage control power current stability controller dc phase high output distributed nonlinear analysis grid vehicle uncertainty dynamic measurement parameter state\n",
      "Topic #2: control power frequency stability controller modulation output nonlinear dc design grid uncertainty phase voltage analysis problem current high optimal vehicle\n",
      "Topic #3: vehicle dynamic control power optimal traffic optimization cost delay application analysis nonlinear allocation distributed parameter bandwidth machine technique level communication\n",
      "Topic #4: control stability time nonlinear controller dynamic delay state optimal vehicle analysis problem event neural distributed network design approach output feedback\n",
      "Topic #5: attack power distributed measurement state control sensor grid physical controller stability dynamic smart process approach time receiver detection parameter reliability\n",
      "Topic #6: power grid control smart distributed dc energy stability state optimal analysis application load current controller design dynamic transmission management operation\n",
      "\n",
      "level 2 topic 4\n",
      "Detect 6 topics\n",
      "Topic #1: detection learning human traffic monitoring task information analysis network approach machine level framework design computer process classifier different tracking motion\n",
      "Topic #2: social user information network mobile task traffic problem real computing large framework human service device cellular modeling game event approach\n",
      "Topic #3: access detection scheme user signal large computing cloud information radio transmission multiple control throughput optimal search analysis server time software\n",
      "Topic #4: graph network topology information analysis node real problem modeling scheme framework social software traffic time different large set number multi\n",
      "Topic #5: privacy scheme information cloud image security user learning database framework social computing search server cluster task mobile efficient machine analysis\n",
      "Topic #6: video clustering task cluster information spatial framework approach large traffic analysis modeling user mobile real cloud search propose ha pattern\n",
      "\n",
      "level 2 topic 5\n",
      "Detect 9 topics\n",
      "Topic #1: fault analysis current circuit feature phase modulation detection voltage dc scheme process machine frequency mode power classification set map high\n",
      "Topic #2: sensor imaging target sensing optical high mode low d current structure device field wa design antenna circuit tracking image parameter\n",
      "Topic #3: gate device voltage low current high layer effect power channel circuit process wa modeling field dc application analysis communication measurement\n",
      "Topic #4: antenna frequency mimo band multiple array modulation channel design bandwidth high transmission signal wave radio output time receiver throughput phase\n",
      "Topic #5: circuit current power dc voltage high low architecture modeling phase memory field operation process switching energy analysis simulation bandwidth measurement\n",
      "Topic #6: antenna band measurement bandwidth frequency array field power high low mode technique transmission imaging rate target multiple sensor time filter\n",
      "Topic #7: frequency bandwidth mode high modulation structure measurement phase analysis effect filter design low optical band power wa array signal time\n",
      "Topic #8: power measurement voltage high circuit field phase low structure current bandwidth analysis switching wa process error effect simulation transmission parameter\n",
      "Topic #9: optical mode frequency bandwidth sensor signal design modulation high time transmission sensing wa network different imaging analysis s parameter single\n",
      "\n",
      "level 2 topic 6\n",
      "Detect 2 topics\n",
      "Topic #1: architecture application computing design cloud software internet memory network communication protocol server physical energy analysis traffic resource cost management security\n",
      "Topic #2: cloud service computing resource task application allocation server user management virtual network cost access mobile quality search software vehicle scheduling\n",
      "\n",
      "level 2 topic 7\n",
      "Detect 10 topics\n",
      "Topic #1: game training attack approach learning process d design dynamic communication wireless parameter channel computer device user function cell architecture field\n",
      "Topic #2: signal modulation classification sparse problem classifier analysis rate parameter information monitoring noise measurement processing phase error process sensing matrix objective\n",
      "Topic #3: d matrix object task robot process modeling structure low imaging human topology approach noise dimensional error application high target training\n",
      "Topic #4: sensor robot sensing target monitoring design tracking measurement high level error human estimation parameter filter technology field mobile map d\n",
      "Topic #5: code rate signal multiple scheme channel modulation noise information technique matrix output high source mimo phase relay array low analysis\n",
      "Topic #6: process analysis approach modeling high design processing software parameter function task field ha phase simulation low used application technique traffic\n",
      "Topic #7: optical target network time image sensor camera imaging high different view visual design tracking real framework filter sensing single analysis\n",
      "Topic #8: noise neural tracking nonlinear state high feature communication stability parameter framework modulation linear output approach process network array level technique\n",
      "Topic #9: matrix problem analysis cluster approach spatial task signal sensing process optimization multiple function number design parameter estimation sparse clustering linear\n",
      "Topic #10: estimation noise measurement state parameter high array signal multiple structure approach matrix robot filter technique framework tracking error time camera\n",
      "\n",
      "level 2 topic 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detect 3 topics\n",
      "Topic #1: network wireless energy node sensor communication routing transmission protocol topology scheme design mobile distributed throughput monitoring optimal receiver traffic problem\n",
      "Topic #2: interference receiver scheme user access channel rate multiple wireless radio transmission network throughput communication signal spectrum power allocation layer ratio\n",
      "Topic #3: d network user game cellular communication interference resource device cell framework allocation power energy multiple information strategy management wireless transmission\n"
     ]
    }
   ],
   "source": [
    "auto_model = Hierarchical_Fuzzy_Clustering(dictionary=tfidf_vectorizer_auto.get_feature_names(), m=1.01,max_level=2, max_topic=10, n_top_words=20)\n",
    "auto_model.do_hierarchical_clustering(data, weight_vector=np.zeros(data.shape[0])+1, word2vec = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37144097222222217"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_model.topic_tree.get_whole_tree_dup_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1625"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_model.topic_tree.get_whole_tree_association_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering on Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "level 0\n",
      "#Topic 1: control power system network communication energy resource architecture noise wireless load estimation performance voltage optimization computing harvesting distributed algorithm processor\n",
      "\n",
      "level 1 topic 1\n",
      "Detect 8 topics\n",
      "Topic #1: control nonlinear adaptive stability controller feedback system multiagent dynamic consensus disturbance fuzzy dynamical event-triggered uncertain time-varying synchronization optimal closed-loop distributed\n",
      "Topic #2: image feature learning recognition method dictionary color visual sparse saliency classification face training descriptor segmentation representation multimodal object reconstruction discriminative\n",
      "Topic #3: localization indoor sleep positioning monitoring fall wearable tracking health sensor navigation location activity malware healthcare tactile radar detection phone anomaly\n",
      "Topic #4: mining big set partitioning data analysis leverage semantic view city semantics literature social privacy-preserving leading variety learning clustering heuristic encryption\n",
      "Topic #5: power voltage converter circuit control system switching capacitor current load inverter frequency reactive conversion switch microgrids compensation charge operation resonant\n",
      "Topic #6: antenna waveguide laser microwave frequency graphene dielectric radiation optical resonator circuit resonant impedance metal band ghz parasitic fabricated microstrip coupling\n",
      "Topic #7: energy channel power interference transmission throughput noise relay secrecy receiver communication downlink delay optimization signal algorithm harvesting estimation rate transmit\n",
      "Topic #8: cloud service mobile computing resource network communication wireless access server user management privacy security internet energy scheduling application vehicular architecture\n",
      "\n",
      "level 2 topic 1\n",
      "Detect 5 topics\n",
      "Topic #1: graph approximate sequential then innovation construct quantum signal redundant topology markov mutual characterized closed-form modeled geometric matching ring palette covariance\n",
      "Topic #2: relay receiver eavesdropper interference selection power transmit uplink secrecy su secondary essential considering active harvesting full-duplex allocation leakage simultaneous association\n",
      "Topic #3: forecasting wind establish nn predictive short-term pairwise gene markov prediction similarity log qos bus brain basis supply output positioning major\n",
      "Topic #4: control nonlinear stability adaptive controller feedback system dynamic multiagent consensus disturbance fuzzy uncertain dynamical event-triggered time-varying topology distributed synchronization optimal\n",
      "Topic #5: negotiation arrival result salient discrete trust rank multiobjective special well-known music constant emergency interval solve first calculation smaller enhancement device-to-device\n",
      "\n",
      "level 2 topic 2\n",
      "Detect 6 topics\n",
      "Topic #1: action latent causal discriminative dictionary cnn feature gait joint movement relation subspace saliency learn player task depth intrinsic regression human\n",
      "Topic #2: query search interactive data manner recurrent local image vms engine keyword limited candidate worker malicious setting lack greatly made growing\n",
      "Topic #3: causal exploiting whose changing le basis six reveal hence efficient scalable first proper five handle ecg rely input threat update\n",
      "Topic #4: machine torque forecasting transformation resonance sign kernel prototype movement dual mapping density predictive equation compression evaluation proactive relative experimentally capacitor\n",
      "Topic #5: gene then although forecasting word symmetric transformed multimedia extreme transformation balance swarm brief similarity r sparse along ecg important active\n",
      "Topic #6: image feature learning recognition method color dictionary visual sparse saliency classification face training descriptor segmentation representation multimodal object reconstruction discriminative\n",
      "\n",
      "level 2 topic 3\n",
      "Detect 4 topics\n",
      "Topic #1: vein finger matching secondary diode purpose systematic sleep addressed metric four metal electron social preserving partition enhancing internal fingerprint neural\n",
      "Topic #2: causal negotiation first exploiting feasible solve handle basis charging calculation arrival series changing finally music update another whose common scalable\n",
      "Topic #3: sleep validation workflow radar health wearable person activity monitoring fall monitor biomedical defect human technology establish disease medical application building\n",
      "Topic #4: localization indoor radar monitoring positioning tracking sensor fall wearable location navigation detection health healthcare malware phone tactile sensing activity anomaly\n",
      "\n",
      "level 2 topic 4\n",
      "Detect 2 topics\n",
      "Topic #1: consensus controller multiagent convergence event-triggered adaptive illustrate matrix observer distributed under dynamical class desired control lossy nonlinear nash time-varying feedback\n",
      "Topic #2: fading channel user eavesdropper small transmitter transmitting radio terminal antenna uplink mmwave received characterized communication particular imperfect cellular modeled loss\n",
      "\n",
      "level 2 topic 5\n",
      "Detect 4 topics\n",
      "Topic #1: wind forecasting market predictive power uncertainty stability generation reactive output economic heating reduction discharge major qos short-term mosfet electricity transfer\n",
      "Topic #2: vein finger matching secondary diode purpose systematic sleep addressed metric four metal electron social preserving partition enhancing internal fingerprint neural\n",
      "Topic #3: power voltage converter circuit control system switching current capacitor frequency load inverter switch conversion reactive charge compensation microgrids resonant fault\n",
      "Topic #4: logic characterization management decoding cache adder nonvolatile computer context width interconnected uncertain processor integrated node shift effort complex hyperspectral harvesting\n",
      "\n",
      "level 2 topic 6\n",
      "Detect 5 topics\n",
      "Topic #1: graphene waveguide dielectric radiation antenna circuit metal microwave frequency parasitic resonator resonant impedance nitride ghz fabricated beam coupling film wideband\n",
      "Topic #2: manual evolution treatment principle material adversary performing methodology biomedical moving dataset big fiber actual mass advance contact general annotation workflow\n",
      "Topic #3: every while chip flexibility gait exploration dual consistent client hashing cyber stream population faster applying unsupervised motor patient gene designing\n",
      "Topic #4: optical laser fiber microwave modulation measurement mode frequency waveguide high-speed pulse fabrication db resonant noise fabricated experimentally electrical voltage transmission\n",
      "Topic #5: antenna band resonator mmwave frequency microstrip ghz impedance rf radiation db waveguide substrate graphene slot utilizes wave array wideband resonant\n",
      "\n",
      "level 2 topic 7\n",
      "Detect 8 topics\n",
      "Topic #1: memory flash cache heating energy lifetime processor resource overhead cpu discharge hardware percent battery execution management program latency consumption parallel\n",
      "Topic #2: energy power system consumption delay throughput communication transmission network packet topology harvesting reliability wireless management processor performance resource latency load\n",
      "Topic #3: thermal temperature circuit switch modeling renewable voltage centralized fiber electron chip music enable load migration trust operational latency ev charge\n",
      "Topic #4: code lattice decoding encoding bound frame linear binary hashing attacker derive space nash estimator generator give degree block observer neural\n",
      "Topic #5: vein finger matching secondary diode purpose systematic sleep addressed metric four metal electron social preserving partition enhancing internal fingerprint neural\n",
      "Topic #6: secrecy interference relay transmit beamforming eavesdropper downlink transmission scheme channel throughput precoding receiver uplink noma ratio receive transmitter user mimo\n",
      "Topic #7: algorithm optimization problem evolutionary bound multiobjective graph solution binary genetic swarm convergence matrix objective heuristic convex optimal weight approximate function\n",
      "Topic #8: estimation noise channel signal measurement error reconstruction covariance array matrix filter uncertainty estimate linear receiver db asymptotic sampling sensor estimated\n",
      "\n",
      "level 2 topic 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detect 10 topics\n",
      "Topic #1: trust identity social risk secure negotiation data-driven infrastructure worker cloud malicious match providing pricing program security incentive effective service resource\n",
      "Topic #2: smart software education emotion internet cloud service big computing application future architecture mobile crowdsourcing broadband data science iot technology engineering\n",
      "Topic #3: adversary data effectively electricity measured request without theoretical injection manual eavesdropper utility packet thing jamming under malicious information tradeoff mobility\n",
      "Topic #4: topic social data importance interaction issue business visual transformed blind understanding discovery activity people object biological identified perspective brain information\n",
      "Topic #5: cell required array association histogram port approximated interface semantics road create percent validation at cellular scale term volume fabrication job\n",
      "Topic #6: diagnosis fault motor disease patient driver extended microgrids assessment several defect similar ship synchronous artificial verification review addressed blood little\n",
      "Topic #7: service resource cloud computing mobile network scheduling communication energy management access wireless user server storage allocation game demand vm architecture\n",
      "Topic #8: cloud mobile privacy security computing service network social wireless access protocol communication resource server spectrum user data internet vehicular secure\n",
      "Topic #9: negotiation arrival result salient discrete trust rank multiobjective special well-known music constant emergency interval solve first calculation smaller enhancement device-to-device\n",
      "Topic #10: pricing energy welfare smart company market user utility scheduling grid service provider price home resource incentive mobile renewable electricity demand\n"
     ]
    }
   ],
   "source": [
    "word2_model = Hierarchical_Fuzzy_Clustering(dictionary=tfidf_vectorizer.get_feature_names(), m=1.01,max_level=2, max_topic=10, n_top_words=20)\n",
    "word2_model.do_hierarchical_clustering(data_word2vec, weight_vector=np.zeros(data_word2vec.shape[0])+1, word2vec = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07322916666666666"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2_model.topic_tree.get_whole_tree_dup_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028125"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2_model.topic_tree.get_whole_tree_association_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2_model.topic_tree.association_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Cluster Validity Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1.0\n",
      "0.9\n",
      "2\n",
      "0.128502600246\n",
      "0.971036468703\n",
      "3\n",
      "-0.0823058320283\n",
      "0.952725751718\n",
      "4\n",
      "-0.395483622351\n",
      "0.960163612987\n"
     ]
    }
   ],
   "source": [
    "model = FuzzyClustering(dictionary = tfidf_vectorizer.get_feature_names(), weight_vector=np.zeros(data_word2vec.shape[0])+1, epochs=50, m=1.04)\n",
    "for i in range(1,5):\n",
    "    print i\n",
    "    a, topics_word2vec = model.do_clustering(data_word2vec,num_topic=i)\n",
    "    print model.get_silhouette_coefficient()\n",
    "    print model.get_MPC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Association API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'item': u'Fuck', u'pos': u'verb', u'weight': 100}, {u'item': u'Pant', u'pos': u'noun', u'weight': 87}, {u'item': u'Clete', u'pos': u'noun', u'weight': 67}, {u'item': u'Brick', u'pos': u'noun', u'weight': 66}, {u'item': u'Bastard', u'pos': u'noun', u'weight': 65}, {u'item': u'Bastard', u'pos': u'adjective', u'weight': 62}, {u'item': u'Ass', u'pos': u'noun', u'weight': 59}, {u'item': u'Scared', u'pos': u'adjective', u'weight': 58}, {u'item': u'Eating', u'pos': u'adjective', u'weight': 55}, {u'item': u'Sitter', u'pos': u'noun', u'weight': 52}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "base_url = 'https://api.wordassociations.net/associations/v1.0/json/search?apikey=85618aa1-21c2-4382-9dc6-373c5b1424b8&text=%s&lang=en&limit=10'\n",
    "print requests.get(base_url % 'shit').json()['response'][0]['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "import pylab\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import os\n",
    "\n",
    "\n",
    "class DimensionReductionData:\n",
    "    def __init__(self,raw_data):\n",
    "        self.raw_data = raw_data,\n",
    "        self.word2vec_model = None,\n",
    "        self.word2vec_data = None,\n",
    "        self.auto_data = None,\n",
    "        self.tfidf_vectorizer_word2vec = None,\n",
    "        self.tfidf_data_word2vec = None,\n",
    "        self.tfidf_vectorizer_auto = None,\n",
    "        self.tfidf_data_auto = None,\n",
    "        self.test_auto = None,\n",
    "        self.train_auto = None,\n",
    "        self.upsampling_auto = None,\n",
    "    \n",
    "    def build_word2vec(self, n_iter=1000, window_size=5, dimension=600):\n",
    "        stops = set(stopwords.words('english'))\n",
    "        stemmer = WordNetLemmatizer()\n",
    "        # stemmer = PorterStemmer()\n",
    "        sentences = []\n",
    "        paper_list = []\n",
    "        for paper in raw:\n",
    "            word_list = []\n",
    "            for sent in sent_tokenize(paper.decode('utf-8')):\n",
    "                words = word_tokenize(sent)\n",
    "                # sentences.append([stemmer.stem(word.lower()) for word in words if word not in string.punctuation])\n",
    "                sentences.append([stemmer.lemmatize(word.lower()) for word in words if word not in string.punctuation and word not in stops and not word.isdigit()])\n",
    "                word_list += words\n",
    "            paper_list.append(word_list)\n",
    "        self.word2vec_model = gensim.models.Word2Vec(sentences, size=400, window=6, min_count=20, workers=4, iter=1000)\n",
    "        for i in range(len(paper_list)):\n",
    "            new_list = []\n",
    "            for word in paper_list[i]:\n",
    "                if word.lower() not in stops:\n",
    "                    new_list.append(word)\n",
    "            paper_list[i] = new_list\n",
    "        self.tfidf_vectorizer_word2vec = TfidfVectorizer(tokenizer = tokenize)\n",
    "        tfidf_data_word2vec = tfidf_vectorizer_word2vec.fit_transform(raw)\n",
    "        \n",
    "        paper_vector = []\n",
    "        paper_count = 0\n",
    "        for paper in paper_list:\n",
    "            vector = np.zeros(400)\n",
    "            diviser = 0\n",
    "            for word in paper:\n",
    "                if word in word2vec_model and word in tfidf_vectorizer.vocabulary_:\n",
    "                    tfidf_value = tfidf_data[paper_count,tfidf_vectorizer.vocabulary_[word]]\n",
    "                    diviser += tfidf_value\n",
    "                    vector += word2vec_model[word]*tfidf_value\n",
    "            paper_vector.append(vector / diviser)\n",
    "            paper_count += 1\n",
    "        word2vec_data = np.array(paper_vector)\n",
    "    \n",
    "    def build_auto(self, n_iter=200, dimension=600, learning_rate=0.1, batch_size=16):\n",
    "        self.tfidf_vectorizer_auto = TfidfVectorizer(stop_words='english', tokenizer=tokenize, max_df=500, max_features=dimension)\n",
    "        self.tfidf_data_auto = self.tfidf_vectorizer_auto.fit_transform(raw)\n",
    "        self.create_network(n_iter=n_iter)\n",
    "    \n",
    "        print('training dae1 ...')\n",
    "        d = []\n",
    "        for epoch in range(n_iter):\n",
    "            # go through trainng set\n",
    "            c = []\n",
    "            for start, end in zip(range(0, self.tfidf_data_auto.shape[0], batch_size), range(batch_size, self.tfidf_data_auto.shape[0], batch_size)):\n",
    "                c.append(self.train_auto(self.tfidf_data_auto.toarray()[start:end]))\n",
    "            d.append(np.mean(c, dtype='float64') / batch_size)\n",
    "            print(d[epoch])\n",
    "        self.auto_data = test(self.tfidf_data_auto.toarray())\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "\n",
    "        #Create Stemmer\n",
    "        #stemmer = PorterStemmer()\n",
    "        stemmer = WordNetLemmatizer()\n",
    "\n",
    "        #Remove irrelevant character\n",
    "        text = re.sub(r\"[^a-zA-Z]\", ' ', text)\n",
    "\n",
    "        #Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [i for i in tokens if i not in string.punctuation]\n",
    "\n",
    "        #Stemming\n",
    "        stems = self.stem_tokens(tokens, stemmer)\n",
    "        return stems\n",
    "\n",
    "    #Stemming Function\n",
    "    def stem_tokens(self, t,s):\n",
    "        stemmed=[]\n",
    "        for item in t:\n",
    "            # stemmed.append(s.stem(item))\n",
    "            stemmed.append(s.lemmatize(item))\n",
    "        return stemmed\n",
    "\n",
    "    def init_weights(self, n_visible, n_hidden):\n",
    "    initial_W = np.asarray(\n",
    "        np.random.uniform(\n",
    "            low=-4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
    "            high=4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
    "            size=(n_visible, n_hidden)),\n",
    "        dtype=theano.config.floatX)\n",
    "    return theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "    def init_bias(self, n):\n",
    "        return theano.shared(value=np.zeros(n,dtype=theano.config.floatX),borrow=True)\n",
    "    \n",
    "    def create_auto(self, n_iter):\n",
    "        x = T.fmatrix('x')  \n",
    "        d = T.fmatrix('d')\n",
    "\n",
    "        rng = np.random.RandomState(123)\n",
    "        theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "        training_epochs = n_iter\n",
    "        learning_rate = 0.1\n",
    "        batch_size = 16\n",
    "        first_dimension = min(50,int(self.tfidf_data_auto.shape[1]*0.7))\n",
    "        # second_dimension = 50\n",
    "\n",
    "        W1 = init_weights(self.tfidf_data_auto.shape[1], first_dimension)\n",
    "        b1 = init_bias(first_dimension)\n",
    "        b1_prime = init_bias(self.tfidf_data_auto.shape[1])\n",
    "        W1_prime = W1.transpose() \n",
    "        # W2 = init_weights(first_dimension, second_dimension)\n",
    "        # b2 = init_bias(second_dimension)\n",
    "        # W2_prime = W2.transpose()\n",
    "        # b2_prime = init_bias(first_dimension)\n",
    "\n",
    "        y1 = T.nnet.sigmoid(T.dot(x, W1) + b1)\n",
    "        # y2 = T.nnet.sigmoid(T.dot(y1,W2) + b2)\n",
    "        # z2 = T.nnet.sigmoid(T.dot(y2, W2_prime) + b2_prime)\n",
    "        z1 = T.nnet.sigmoid(T.dot(y1, W1_prime) + b1_prime)\n",
    "        cost1 = T.sum((x-z1)**2)\n",
    "\n",
    "        params1 = [W1, b1, b1_prime]\n",
    "        grads1 = T.grad(cost1, params1)\n",
    "        updates1 = [(param1, param1 - learning_rate * grad1)\n",
    "                   for param1, grad1 in zip(params1, grads1)]\n",
    "        self.train_auto = theano.function(inputs=[x], outputs = cost1, updates = updates1, allow_input_downcast = True)\n",
    "        self.test_auto = theano.function(inputs=[x], outputs = y1, allow_input_downcast = True)\n",
    "        self.upsampling_auto = theano.function(inputs=[y1], outputs = z1, allow_input_downcast = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
