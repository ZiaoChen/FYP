Title,Abstract,Keywords
Divergence measures based on the Shannon entropy,"A novel class of information-theoretic divergence measures based on the Shannon entropy is introduced. Unlike the well-known Kullback divergences, the new measures do not require the condition of absolute continuity to be satisfied by the probability distributions involved. More importantly, their close relationship with the variational distance and the probability of misclassification error are established in terms of bounds. These bounds are crucial in many applications of divergence measures. The measures are also well characterized by the properties of nonnegativity, finiteness, semiboundedness, and boundedness.",
Stable adaptive teleoperation,"A study is made of how the existence of transmission time delays affects the application of advanced robot control schemes to effective force-reflecting telerobotic systems. This application best exploits the presence of the human operator while making full use of available robot control technology and computing power. A physically motivated, passivity-based formalism is used to provide energy conservation and stability guarantees in the presence of transmission delays. The notion of wave variable is utilized to characterize time-delay systems and leads to a configuration for force-reflecting teleoperation. The effectiveness of the approach is demonstrated experimentally. Within the same framework, an adaptive tracking controller is incorporated for the control of the remote robotic system and can be used to simplify, transform, or enhance the remote dynamics perceived by the operator.","Application software,
Telerobotics,
Robot control,
Control systems,
Space technology,
Humans,
Cognitive robotics,
Control theory,
Cognitive science,
Computer science"
"Particle-in-cell charged-particle simulations, plus Monte Carlo collisions with neutral atoms, PIC-MCC","Many-particle charged-particle plasma simulations using spatial meshes for the electromagnetic field solutions, particle-in-cell (PIC) merged with Monte Carlo collision (MCC) calculations, are coming into wide use for application to partially ionized gases. The author emphasizes the development of PIC computer experiments since the 1950s starting with one-dimensional (1-D) charged-sheet models, the addition of the mesh, and fast direct Poisson equation solvers for 2-D and 3-D. Details are provided for adding the collisions between the charged particles and neutral atoms. The result is many-particle simulations with many of the features met in low-temperature collision plasmas; for example, with applications to plasma-assisted materials processing, but also related to warmer plasmas at the edges of magnetized fusion plasmas.","Monte Carlo methods,
Plasma simulation,
Plasma materials processing,
Computational modeling,
Plasma applications,
Plasma temperature,
Plasma measurements,
Atomic measurements,
Computer simulation,
Nuclear and plasma sciences"
Adaptive Mixtures of Local Experts,"We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.",
Fitting parameterized three-dimensional models to images,"Model-based recognition and motion tracking depend upon the ability to solve for projection and model parameters that will best fit a 3-D model to matching 2-D image features. The author extends current methods of parameter solving to handle objects with arbitrary curved surfaces and with any number of internal parameters representing articulation, variable dimensions, or surface deformations. Numerical stabilization methods are developed that take account of inherent inaccuracies in the image measurements and allow useful solutions to be determined even when there are fewer matches than unknown parameters. The Levenberg-Marquardt method is used to always ensure convergence of the solution. These techniques allow model-based vision to be used for a much wider class of problems than was possible with previous methods. Their application is demonstrated for tracking the motion of curved, parameterized objects.",
Recognition by linear combinations of models,"An approach to visual object recognition in which a 3D object is represented by the linear combination of 2D images of the object is proposed. It is shown that for objects with sharp edges as well as with smooth bounding contours, the set of possible images of a given object is embedded in a linear space spanned by a small number of views. For objects with sharp edges, the linear combination representation is exact. For objects with smooth boundaries, it is an approximation that often holds over a wide range of viewing angles. Rigid transformations (with or without scaling) can be distinguished from more general linear transformations of the object by testing certain constraints placed on the coefficients of the linear combinations. Three alternative methods of determining the transformation that matches a model to a given image are proposed.","Image recognition,
Object recognition,
Research initiatives,
Cognitive science,
Artificial intelligence,
Mathematics"
Constraint-based automatic test data generation,A novel technique for automatically generating test data is presented. The technique is based on mutation analysis and creates test data that approximate relative adequacy. It is a fault-based technique that uses algebraic constraints to describe test cases designed to find particular types of faults. A set of tools (collectively called Godzilla) that automatically generates constraints and solves them to create test cases for unit and module testing has been implemented. Godzilla has been integrated with the Mothra testing system and has been used as an effective way to generate test data that kill program mutants. The authors present an initial list of constraints and discuss some of the problems that have been solved to develop the complete implementation of the technique.,"Automatic testing,
Software testing,
System testing,
Genetic mutations,
Algorithms,
Fault detection,
Costs,
Software systems,
Software engineering,
Computer science"
Using program slicing in software maintenance,"Program slicing is applied to the software maintenance problem by extending the notion of a program slice (that originally required both a variable and line number) to a decomposition slice, one that captures all computation on a given variable, i.e., is independent of line numbers. Using the lattice of single variable decomposition slices ordered by set inclusion, it is shown how a slice-based decomposition for programs can be formed. One can then delineate the effects of a proposed change by isolating those effects in a single component of the decomposition. This gives maintainers a straightforward technique for determining those statements and variables which may be modified in a component and those which may not. Using the decomposition, a set of principles to prohibit changes which will interfere with unmodified components is provided. These semantically consistent changes can then be merged back into the original program in linear time.",
Tree-maps: a space-filling approach to the visualization of hierarchical information structures,"A method for visualizing hierarchically structured information is described. The tree-map visualization technique makes 100% use of the available display space, mapping the full hierarchy onto a rectangular region in a space-filling manner. This efficient use of space allows very large hierarchies to be displayed in their entirety and facilitates the presentation of semantic information. Tree-maps can depict both the structure and content of the hierarchy. However, the approach is best suited to hierarchies in which the content of the leaf nodes and the structure of the hierarchy are of primary importance, and the content information associated with internal nodes is largely derived from their children.","Visualization,
Feedback,
Computer displays,
Computer science,
Laboratories,
Educational institutions,
Libraries,
Humans,
Marine vehicles,
Two dimensional displays"
An efficiently computable metric for comparing polygonal shapes,"A method for comparing polygons that is a metric, invariant under translation, rotation, and change of scale, reasonably easy to compute, and intuitive is presented. The method is based on the L/sub 2/ distance between the turning functions of the two polygons. It works for both convex and nonconvex polygons and runs in time O(mn log mn), where m is the number of vertices in one polygon and n is the number of vertices in the other. Some examples showing that the method produces answers that are intuitively reasonable are presented.","Shape measurement,
Computer science,
Image recognition,
Machine vision,
Turning,
Rotation measurement,
Geometry,
Solid modeling,
Computer vision,
Cost function"
Direct computation of the focus of expansion from velocity field measurements,"A new method for computing the direction of translational motion (focus of expansion) of a moving observer (or camera) in a stationary environment is proposed. The method applied simple 1D search directly to velocity field measurements of the changing image, for cases of general motion and constrained motion. In the case of general motion, the velocity field is derotated to cancel the velocity of the observed point at the image origin. In principle, a single moving closed contour which encircles the origin is sufficient to recover the focus of expansion. In practice, additional velocity measurements in the image may be incorporated in the computation, for improved robustness in the face of image noise and velocity field inaccuracies.","Velocity measurement,
Motion measurement,
Focusing,
Cameras,
Noise measurement,
Noise robustness,
Equations,
Mathematics,
Computer science,
Working environment noise"
The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression,"Approaches to the zero-frequency problem in adaptive text compression are discussed. This problem relates to the estimation of the likelihood of a novel event occurring. Although several methods have been used, their suitability has been on empirical evaluation rather than a well-founded model. The authors propose the application of a Poisson process model of novelty. Its ability to predict novel tokens is evaluated, and it consistently outperforms existing methods. It is applied to a practical statistical coding scheme, where a slight modification is required to avoid divergence. The result is a well-founded zero-frequency model that explains observed differences in the performance of existing methods, and offers a small improvement in the coding efficiency of text compression over the best method previously known.","Probability,
Decoding,
Context modeling,
Computer science,
Data compression,
Encoding,
Computer errors,
Councils,
Drives,
Arithmetic"
Directed-graph epidemiological models of computer viruses,"The strong analogy between biological viruses and their computational counterparts has motivated the authors to adapt the techniques of mathematical epidemiology to the study of computer virus propagation. In order to allow for the most general patterns of program sharing, a standard epidemiological model is extended by placing it on a directed graph and a combination of analysis and simulation is used to study its behavior. The conditions under which epidemics are likely to occur are determined, and, in cases where they do, the dynamics of the expected number of infected individuals are examined as a function of time. It is concluded that an imperfect defense against computer viruses can still be highly effective in preventing their widespread proliferation, provided that the infection rate does not exceed a well-defined critical epidemic threshold.","Computer viruses,
Biology computing,
Laboratories,
Biological system modeling,
Pattern analysis,
Computational modeling,
Analytical models,
Biological processes,
Humans,
Computer science"
On the complexity of VLSI implementations and graph representations of Boolean functions with application to integer multiplication,"Lower-bound results on Boolean-function complexity under two different models are discussed. The first is an abstraction of tradeoffs between chip area and speed in very-large-scale-integrated (VLSI) circuits. The second is the ordered binary decision diagram (OBDD) representation used as a data structure for symbolically representing and manipulating Boolean functions. The lower bounds demonstrate the fundamental limitations of VLSI as an implementation medium, and that of the OBDD as a data structure. It is shown that the same technique used to prove that any VLSI implementation of a single output Boolean function has area-time complexity AT/sup 2/= Omega (n/sup 2/) also proves that any OBDD representation of the function has Omega (c/sup n/) vertices for some c>1 but that the converse is not true. An integer multiplier for word size n with outputs numbered 0 (least significant) through 2n-1 (most significant) is described. For the Boolean function representing either output i-1 or output 2n-i-1, where 1","Very large scale integration,
Boolean functions,
Data structures,
Logic circuits,
Integrated circuit technology,
Chip scale packaging,
Size measurement,
Computer science,
Input variables"
An information retrieval approach for automatically constructing software libraries,"A technology for automatically assembling large software libraries which promote software reuse by helping the user locate the components closest to her/his needs is described. Software libraries are automatically assembled from a set of unorganized components by using information retrieval techniques. The construction of the library is done in two steps. First, attributes are automatically extracted from natural language documentation by using an indexing scheme based on the notions of lexical affinities and quantity of information. Then a hierarchy for browsing is automatically generated using a clustering technique which draws only on the information provided by the attributes. Due to the free-text indexing scheme, tools following this approach can accept free-style natural language queries.","Information retrieval,
Software libraries,
Indexing,
Documentation,
Programming profession,
Productivity,
Assembly,
Natural languages,
Computer science,
Data mining"
Pseudospark discharges via computer simulation,"A hybrid fluid-particle (Monte Carlo) model to describe the initiation of pseudospark discharges has been developed. In this model, time-dependent fluid equations for the electrons and positive ions are solved self-consistently with Poisson's equation for the electric field in a two-dimensional, cylindrically symmetrical geometry. The Monte Carlo simulation is used to determine the ionization source term in the fluid equations. This model has been used to study the evolution of a discharge in helium at 0.5 torr, with an applied voltage of 2 kV and in a typical pseudospark geometry. From the numerical results, the authors have identified a sequence of physical events that lead to the rapid rise in current associated with the onset of the pseudospark discharge mode. They find that there is a maximum in the electron multiplication at the time which corresponds to the onset of the hollow cathode effect, and although the multiplication later decreases, it is always greater than needed for a steady-state discharge.",
Analyzing partition testing strategies,"Partition testing strategies, which divide a program's input domain into subsets with the tester selecting one or more elements from each subdomain, are analyzed. The conditions that affect the efficiency of partition testing are investigated, and comparisons of the fault detection capabilities of partition testing and random testing are made. The effects of subdomain modifications on partition testing's ability to detect faults are studied.",
The asymptotic decider: resolving the ambiguity in marching cubes,"A method for computing isovalue or contour surfaces of a trivariate function is discussed. The input data are values of the trivariate function, F/sub ijk/, at the cuberille grid points (x/sub i/, y/sub j/, z/sub k/), and the output of a collection of triangles representing the surface consisting of all points where F(x,y,z) is a constant value. The method is a modification that is intended to correct a problem with a previous method.","Isosurfaces,
Computer science,
Interpolation,
Assembly"
Robotic exploration as graph construction,"Addressed is the problem of robotic exploration of a graphlike world, where no distance or orientation metric is assumed of the world. The robot is assumed to be able to autonomously traverse graph edges, recognize when it has reached a vertex, and enumerate edges incident upon the current vertex relative to the edge via which it entered the current vertex. The robot cannot measure distances, and it does not have a compass. It is demonstrated that this exploration problem is unsolvable in general without markers, and, to solve it, the robot is equipped with one or more distinct markers that can be put down or picked up at will and that can be recognized by the robot if they are at the same vertex as the robot. An exploration algorithm is developed and proven correct. Its performance is shown on several example worlds, and heuristics for improving its performance are discussed.","Robot sensing systems,
Robot kinematics,
Navigation,
Intelligent robots,
Machine intelligence,
Computer science,
Tracking,
Computer errors,
Councils,
Information technology"
Nonblocking broadcast switching networks,"Results are presented for nonblocking multistage broadcast networks wherein a request from an idle input port to be connected to some set of idle output ports can be satisfied without any disturbance of other broadcast connections already existing in the network. Furthermore, a linear network control algorithm for realizing such a broadcast connection request is given. These results represent the best known explicit constructions with limited numbers of stages relative to both crosspoint and control algorithm complexity. Thus, these networks are highly useful for practical applications involving the movement of and collaboration with voice/video/text/graphics information that require broadcast capability. These networks are also useful for the interconnection of processor and memory units in parallel processing systems.",
Computation of 3-D velocity fields from 3-D cine CT images of a human heart,"A method of computing the three-dimensional (3-D) velocity field from 3-D cine computer tomographs (CTs) of a beating heart is proposed. Using continuum theory, the authors develop two constraints on the 3-D velocity field generated by a beating heart. With these constraints, the computation of the 3-D velocity field is formulated as an optimization problem and a solution to the optimization problem is developed using the Euler-Lagrange method. The solution is then discretized for computer implementation. The authors present the results for both simulated images and clinical cine CT images of a beating heart.",
"Tree automata, mu-calculus and determinacy","It is shown that the propositional mu-calculus is equivalent in expressive power to finite automata on infinite trees. Since complementation is trivial in the mu-calculus, the equivalence provides a radically simplified, alternative proof of M.O. Rabin's (1989) complementation lemma for tree automata, which is the heart of one of the deepest decidability results. It is also shown how mu-calculus can be used to establish determinacy of infinite games used in earlier proofs of complementation lemma, and certain games used in the theory of online algorithms.","Automata,
Game theory,
Logic,
Calculus,
Heart,
Mathematics,
Computer science,
Marine vehicles"
An effective on-chip preloading scheme to reduce data access penalty,"Conventional cache prefetching approaches can be either hardware-based, generally by using a one-block-lookahead technique, or compiler-directed, with insertions of non-blocking prefetch instructions. We introduce a new hardware scheme based on the prediction of the execution of the instruction stream and associated operand references. It consists of a reference prediction table and a look-ahead program counter and its associated logic. With this scheme, data with regular access patterns is preloaded, independently of the stride size, and preloading of data with irregular access patterns is prevented. We evaluate our design through trace driven simulation by comparing it with a pure data cache approach under three different memory access models. Our experiments show that this scheme is very effective for reducing the data access penalty for scientific programs and that is has moderate success for other applications.",
A generalization of the Berlekamp-Massey algorithm for multisequence shift-register synthesis with applications to decoding cyclic codes,"A generalization of the Berlekamp-Massey algorithm is presented for synthesizing minimum length linear feedback shift registers for generating prescribed multiple sequences. A more general problem is first considered, that of finding the smallest initial set of linearly dependent columns in a matrix over an arbitrary field, which includes the multisequence problem as a special case. A simple iterative algorithm, the fundamental iterative algorithm (FIA), is presented for solving this problem. The generalized algorithm is then derived through a refinement of the FIA. Application of this generalized algorithm to decoding cyclic codes up to the Hartmann-Tzeng (HT) bound and Roos bound making use of multiple syndrome sequences is considered. Conditions for guaranteeing that the connection polynomial of the shortest linear feedback shift register obtained by the algorithm will be the error-locator polynomial are determined with respect to decoding up to the HT bound and special cases of the Roos bound.","Iterative algorithms,
Iterative decoding,
Polynomials,
Linear feedback shift registers,
Computer science,
Senior members,
Shift registers,
Information theory"
Nonlinear refractive index in erbium doped optical fiber: Theory and experiment,"An intensity-dependent refractive index in erbium doped optical fiber has been observed for the first time. The effect has been observed by an interferometric technique using a two-core fiber. The effect is analyzed theoretically, and the important nonlinear parameters for thin material have been determined and are presented.",
Software requirements analysis for real-time process-control systems,"A set of criteria is defined to help find errors in, software requirements specifications. Only analysis criteria that examine the behavioral description of the computer are considered. The behavior of the software is described in terms of observable phenomena external to the software. Particular attention is focused on the properties of robustness and lack of ambiguity. The criteria are defined using an abstract state-machine model for generality. Using these criteria, analysis procedures can be defined for particular state-machine modeling languages to provide semantic analysis of real-time process-control software requirements.","Real time systems,
Control systems,
Process control,
Safety,
Aircraft manufacture,
Programming,
Computer science,
Software prototyping,
Prototypes,
Robustness"
Delay jitter control for real-time communication in a packet switching network,"A study is made of the feasibility of bounding the delay jitter for real-time channels in a packet-switched store-and-forward wide-area network with general topology, extending the scheme proposed in previous papers. The authors prove the correctness of the solution, and study its effectiveness by means of simulations. The results show that the scheme is capable of providing a significant reduction in delay jitter, that there is no accumulation of jitter along the path of a channel, and that jitter control reduces the buffer space required in the network significantly.","Jitter,
Communication system control,
Intelligent networks,
Packet switching,
Delay,
Switching circuits,
Communication switching,
Computer science,
Telecommunication traffic,
Bandwidth"
Gray level thresholding in badly illuminated images,"The thresholding method involves first locating objects in an image by using the intensity gradient, then noting the levels that correspond to the objects in various areas of the image, and finally using these levels as initial guesses at a threshold. This method is capable of thresholding images that have been produced in the context of variable illumination. The thresholding method, called the local intensity gradient (LIG) method, was implemented in C using a Sun4 host running UNIX. The LIG method was compared against iterative selection (IS), gray level histograms (GLHs) and two correlation based algorithms on a dozen sample images under three different illumination effects. Overall, the LIG method, while it takes significantly longer, properly thresholds a larger set of images than does any other method examined over the sample images tested.","Lighting,
Pixel,
Histograms,
Image processing,
Image segmentation,
Data mining,
Computer science,
Correlation,
Image analysis,
Machine intelligence"
A genetic algorithm for the linear transportation problem,"Genetic algorithms are adaptive procedures that find solutions to problems by an evolutionary process based on natural selection. The use of alternative genetic algorithms for solving the linear transportation problem is discussed. Using it as an example the relationship between representation structures and genetic operators is investigated for constrained problems, and the value of structures richer than bitstrings is demonstrated.","Genetic algorithms,
Transportation,
Genetic mutations,
Intelligent systems,
Statistics,
Operations research,
Mathematics,
Computer science,
Algorithm design and analysis"
New heuristic algorithms for efficient hierarchical path planning,"The authors consider one of the most popular approaches to path planning: hierarchical approximate cell decomposition. This approach consists of constructing successive decompositions of the robot's configuration space into rectangloid cells and searching the connectivity graph built at each level of decomposition for a path. Despite its conceptual simplicity, an efficient implementation of this approach raises many delicate questions that have not yet been addressed. The major contributions this work are (1) a novel approach to cell decomposition based on constraint reformulation and (2) a new algorithm for hierarchical search with a mechanism for recording failure conditions. These algorithms have been implemented in a path planner, and experiments with this planner have been carried out on various examples. These experiments show that the proposed planner is significantly (approximately 10 times) faster than previous planners based on the same general approach.","Heuristic algorithms,
Path planning,
Robotics and automation,
Motion planning,
Orbital robotics,
Robots,
Humans,
Automatic control,
Computational Intelligence Society,
Computer science"
Path sensitization in critical path problem,"Since the delay of a circuit is determined by the delay of its longest sensitizable paths (such paths are called critical paths), the problem of estimating the delay of a circuit is called critical path problem. One important aspect of the critical path problem is to decide whether a path is sensitizable. A framework which allows various previously proposed path sensitization criteria to be compared with each other in a unified way is presented. An exact path sensitization criterion and a looser path sensitization criterion based on the framework are also proposed.",
Testability of software components,The concept of domain testability of software is defined by applying the concepts of observability and controllability to software. It is shown that a domain-testable program does not exhibit any input-output inconsistencies and supports small test sets in which test outputs are easily understood. Metrics that can be used to assess the level of effort required in order to modify a program so that it is domain-testable are discussed. Assessing testability from program specifications and an experiment which shows that it takes less time to build and test a program developed from a domain-testable specification than a similar program developed from a nondomain-testable specification are also discussed.,"Software testing,
Controllability,
Observability,
Timing,
Computer displays,
Software engineering,
Hardware,
Programming,
Computer science,
Explosives"
A real-time locking protocol,"The authors examine a priority driven two-phase lock protocol called the read/write priority ceiling protocol. It is shown that this protocol leads to freedom from mutual deadlock. In addition, a high-priority transactions can be blocked by lower priority transactions for at most the duration of a single embedded transaction. These properties can be used by schedulability analysis to guarantee that a set of periodic transactions using this protocol can always meet its deadlines. Finally, the performance of this protocol is examined for randomly arriving transactions using simulation studies.","Concurrency control,
Transaction databases,
Access protocols,
Job shop scheduling,
System recovery,
Contracts,
Computer science,
Database systems,
Real time systems,
Aircraft"
A stable tracking control method for a non-holonomic mobile robot,"Proposes a stable control rule to find a reasonable target linear and rotational velocities ( nu , omega )/sup t/. The stability of the rule is proved through the use of a Lyapunov function. The rule contains three parameters, K/sub x/, K/sub y/ and K/sub theta /. Although any set of positive parameters makes the system stable, a condition on the parameters for the system being critically damped for a small disturbance is obtained through linearizing the system's differential equation. This method was successfully implemented on the autonomous mobile robot Yamabico-11. Experimental results obtained turn out to be close to the results with the velocity/acceleration limiter.",
Theoretical aspects of gray-level morphology,"After a brief discussion of the extension of mathematical morphology to complete lattices, the space of gray-level functions is considered and the concept of a threshold set is introduced. It is shown how one can use binary morphological operators and thresholding techniques to build a large class of gray-level morphological operators. Particular attention is given to the class of so-called flat operators, i.e. operators which commute with thresholding. It is also shown how to define dilations and erosions with nonflat structuring elements if the gray-level set is finite. It is reported that mere truncation yields wrong results.","Morphology,
Lattices,
Filters,
Visualization,
Mathematics,
Computer science"
Minimization of binary decision diagrams based on exchanges of variables,"The authors present a novel exact algorithm and gradual improvement methods for minimizing binary decision diagrams (BDDs). In the exact minimization algorithm, the optimum order is searched by the exchanges of variables of BDDs based on the framework of the algorithm of S.J. Friedman and K.J. Supowit (1990). The use of the BDD representation of a given function and intermediate functions makes it possible to produce pruning into the method, which drastically reduces the computation cost. The authors succeeded in minimizing a 17-variable function by the use of the BDD representation of intermediate functions and the introduction of pruning. They also propose a greedy method and a simulated annealing method based on exchanges of two arbitrary variables, and a greedy method based on exchanges of adjacent m variables for m=3 and 4.","Data structures,
Boolean functions,
Binary decision diagrams,
Minimization methods,
Computational efficiency,
Simulated annealing,
Logic testing,
Information systems,
Systems engineering and theory,
Information science"
Debugging concurrent Ada programs by deterministic execution,A language-based approach to deterministic execution debugging of concurrent Ada programs is presented. The approach is to define synchronization (SYN)-sequences of a concurrent Ada program in terms of Ada language constructs and to replay such SYN-sequences without the need for system-dependent debugging tools. It is shown how to define a SYN-sequence of a concurrent Ada program in order to provide sufficient information for deterministic execution. It is also shown how to transform a concurrent Ada program P so that the SYN-sequences of previous executions of P can be replayed. This transformation adds an Ada task to P that controls program execution by synchronizing with the original tasks in P. A brief description is given of the implementation of tools supporting deterministic execution debugging of concurrent Ada programs.,"Computer science,
Testing,
Software debugging,
Software engineering,
Mathematics"
Efficient Training of Artificial Neural Networks for Autonomous Navigation,"The ALVINN (Autonomous Land Vehicle In a Neural Network) project addresses the problem of training artificial neural networks in real time to perform difficult perception tasks. ALVINN is a backpropagation network designed to drive the CMU Navlab, a modified Chevy van. This paper describes the training techniques that allow ALVINN to learn in under 5 minutes to autonomously control the Navlab by watching the reactions of a human driver. Using these techniques, ALVINN has been trained to drive in a variety of circumstances including single-lane paved and unpaved roads, and multilane lined and unlined roads, at speeds of up to 20 miles per hour.",
Distributed constrained heuristic search,"A model of decentralized problem solving, called distributed constrained heuristic search (DCHS), that provides both structure and focus in individual agent search spaces to optimize decisions in the global space, is presented. The model achieves this by integrating decentralized constraint satisfaction and heuristic search. It is a formalism suitable for describing a large set of distributed artificial intelligence problems. The notion of textures that allow agents to operate in an asynchronous concurrent manner is introduced. The use of textures coupled with distributed asynchronous backjumping, a type of distributed dependency-directed backtracking that the authors have developed, enables agents to instantiate variables in such a way as to substantially reduce backtracking. The approach has been tested experimentally in the domain of decentralized job-shop scheduling. A formulation of distributed job-shop scheduling as a DCHS and experimental results are presented.","Problem-solving,
Constraint optimization,
Employment,
Testing,
State-space methods,
Aircraft,
Computer science,
Multiagent systems,
Scheduling"
Approximating clique is almost NP-complete,"The computational complexity of approximating omega (G), the size of the largest clique in a graph G, within a given factor is considered. It is shown that if certain approximation procedures exist, then EXPTIME=NEXPTIME and NP=P.","Approximation algorithms,
Polynomials,
Computational complexity,
Protocols,
Testing,
Upper bound,
Data mining"
High-performance clock routing based on recursive geometric matching,,"Clocks,
Routing,
Very large scale integration,
Wire,
Synchronization,
Permission,
Circuit testing,
Partitioning algorithms,
Pins,
Computer science"
Analyzing error-prone system structure,"Using measures of data interaction called data bindings, the authors quantify ratios of coupling and strength in software systems and use the ratios to identify error-prone system structures. A 148000 source line system from a prediction environment was selected for empirical analysis. Software error data were collected from high-level system design through system testing and from field operation of the system. The authors use a set of five tools to calculate the data bindings automatically and use a clustering technique to determine a hierarchical description of each of the system's 77 subsystems. A nonparametric analysis of variance model is used to characterize subsystems and individual routines that had either many or few errors or high or low error correction effort. The empirical results support the effectiveness of the data bindings clustering approach for localizing error-prone system structure.","Error analysis,
Software measurement,
Software systems,
Data analysis,
Computer errors,
Software testing,
System testing,
Error correction,
Inspection,
Computer science"
BONSAI: 3D object recognition using constrained search,"BONSAI, a model-based 3D object recognition system, is described. It identifies and localizes 3D objects in range images of one or more parts that have been designed on a computer-aided-design (CAD) system. Recognition is performed via constrained search of the interpretation tree, using unary and binary constraints (derived automatically from the CAD models) to prune the search space. Attention is focused on the recognition procedure, but the model-building, image acquisition, and segmentation procedures are also outlined. Experiments with over 200 images demonstrate that the constrained search approach to 3D object recognition has an accuracy comparable to that of previous systems.",
Robust dynamic motion estimation over time,A novel approach to incrementally estimating visual motion over a sequence of images is presented. The authors start by formulating constraints on image motion to account for the possibility of multiple motions. This is achieved by exploiting the notions of weak continuity and robust statistics in the formulation of a minimization problem. The resulting objective function is non-convex. Traditional stochastic relaxation techniques for minimizing such functions prove inappropriate for the task. A highly parallel incremental stochastic minimization algorithm is presented which has a number of advantages over previous approaches. The incremental nature of the scheme makes it dynamic and permits the detection of occlusion and disocclusion boundaries.,
Run-time parallelization and scheduling of loops,"The authors study run-time methods to automatically parallelize and schedule iterations of a do loop in certain cases where compile-time information is inadequate. The methods presented involve execution time preprocessing of the loop. At compile-time, these methods set up the framework for performing a loop dependency analysis. At run-time, wavefronts of concurrently executable loop iterations are identified. Using this wavefront information, loop iterations are reordered for increased parallelism. The authors utilize symbolic transformation rules to produce: inspector procedures that perform execution time preprocessing, and executors or transformed versions of source code loop structures. These transformed loop structures carry out the calculations planned in the inspector procedures. The authors present performance results from experiments conducted on the Encore Multimax. These results illustrate that run-time reordering of loop indexes can have a significant impact on performance.",
Fast spectral methods for ratio cut partitioning and clustering,"The ratio cut partitioning objective function successfully embodies both the traditional min-cut and equipartition goals of partitioning. Fiduccia-Mattheyses style ratio cut heuristics have achieved cost savings averaging over 39% for circuit partitioning and over 50% for hardware simulation applications. The authors show a theoretical correspondence between the optimal ratio cut partition cost and the second smallest eigenvalue of a particular netlist-derived matrix, and present fast Lanczos-based methods for computing heuristic ratio cuts from the eigenvector of this second eigenvalue. Results are better than those of previous methods, e.g. by an average of 17% for the Primary MCNC benchmarks. An efficient clustering method, also based on the second eigenvector, is very successful on the 'difficult' input classes in the CAD (computer-aided design) literature. Extensions and directions for future work are also considered.",
An implementation of interprocedural bounded regular section analysis,"Regular section analysis, which summarizes interprocedural side effects on subarrays in a form useful to dependence analysis, while avoiding the complexity of prior solutions, is shown to be a practical addition to a production compiler. Optimizing compilers should produce efficient code even in the presence of high-level language constructs. However, current programming support systems are significantly lacking in their ability to analyze procedure calls. This deficiency complicates parallel programming, because loops with calls can be a significant source of parallelism. The performance of regular section analysis is compared to two benchmarks: the LINPACK library of linear algebra subroutines and the Rice Compiler Evaluation Program Suite (RiCEPS), a set of complete application codes from a variety of scientific disciplines. The experimental results demonstrate that regular section analysis is an effective means of discovering parallelism, given programs written in an appropriately modular programming style.",
An architecture for software-controlled data prefetching,,"Prefetching,
Delay,
Hardware,
Permission,
Predictive models,
Costs,
Traffic control,
Microprocessors,
Materials science and technology,
Programming profession"
Image-based homing,"A system that allows a robot to acquire a model of its environment and to use this model to navigate is described. The system maps the environment as a set of snapshots of the world taken at target locations. The robot uses an image-based local homing algorithm to navigate between neighboring target locations. Features of the approach include an imaging system that acquires a compact, 360 degrees representation of the environment and an image-based, qualitative homing algorithm that allows the robot to navigate without explicitly inferring three-dimensional structure from the image. The results of an experiment in a typical indoor environment are described, and its argued that image-based navigation is a feasible alternative to approaches using three-dimensional models.","Navigation,
Mobile robots,
Contracts,
Indoor environments,
Cultural differences,
Marine vehicles,
Solid modeling,
Military computing,
Information science,
Data mining"
Lazy task creation: a technique for increasing the granularity of parallel programs,"When a parallel algorithm is written naturally, the resulting program often produces tasks of a finer grain than an implementation can exploit efficiently. Two solutions to the granularity problem that combine parallel tasks dynamically at runtime are discussed. The simpler load-based inlining method, in which tasks are combined based on dynamic bad level, is rejected in favor of the safer and more robust lazy task creation method, in which tasks are created only retroactively as processing results become available. The strategies grew out of work on Mul-T, an efficient parallel implementation of Scheme, but could be used with other languages as well. Mul-T implementations of lazy task creation are described for two contrasting machines, and performance statistics that show the method's effectiveness are presented. Lazy task creation is shown to allow efficient execution of naturally expressed algorithms of a substantially finer grain than possible with previous parallel Lisp systems.","Programming profession,
Costs,
Program processors,
Parallel algorithms,
Concurrent computing,
Parallel processing,
Computer science,
Robustness,
Statistics,
Partitioning algorithms"
Performance evaluation of the PC-2048: a new 15-slice encoded-crystal PET scanner for neurological studies,"Initial experience is reported with the Scanditronix PC 2048-15B, a 15-slice positron emission tomography (PET) system using multicrystal/multiphoto-multiplier modules to obtain high spatial resolution. Random and scattered events are reduced using an orbiting /sup 68/Ge rod source for transmission scans by only accepting coincidence lines which intersect the instantaneous position of the source. Scatter correction of the emission data is removed with a deconvolution kernel, random and dead-time correction by the use of observed singles rates. The peak count rates are 11.7/20.0 Kcps for the direct cross slices at concentrations of 4.5/5.1 mu Ci/cc. respectively. Over the radial range 0-9 cm from the ring center, radial transverse resolution is 4.6-6.4 mm, aid tangential transverse resolution is 4.6-5.1 mm using a Hanning filter. Over the same range, axial resolution varies from 6.1-6.2 mm in direct slices and from 5.4-7.1 mm in cross slices. This near-isotropic resolution allows collection of image volume data with no preferred direction for signal averaging errors.",
Curious model-building control systems,"A novel curious model-building control system is described which actively tries to provoke situations for which it learned to expect to learn something about the environment. Such a system has been implemented as a four-network system based on Watkins' Q-learning algorithm which can be used to maximize the expectation of the temporal derivative of the adaptive assumed reliability of future predictions. An experiment with an artificial nondeterministic environment demonstrates that the system can be superior to previous model-building control systems, which do not address the problem of modeling the reliability of the world model's predictions in uncertain environments and use ad-hoc methods (like random search) to train the world model.",
Longitudinal Control of a Platoon of Vehicles with no Communication of Lead Vehicle Information,,"Communication system control,
Road vehicles,
Vehicle dynamics,
Aerodynamics,
Vehicle driving,
Communication system traffic control,
Automatic control,
Acceleration,
Engines,
Design optimization"
Specification and refinement of probabilistic processes,"A formalism for specifying probabilistic transition systems, which constitute a basic semantic model for description and analysis of, e.g. reliability aspects of concurrent and distributed systems, is presented. The formalism itself is based on transition systems. Roughly a specification has the form of a transition system in which transitions are labeled by sets of allowed probabilities. A satisfaction relation between processes and specifications that generalizes probabilistic bisimulation equivalence is defined. It is shown that it is analogous to the extension from processes to modal transition systems given by K. Larsen and B. Thomsen (1988). Another weaker criterion views a specification as defining a set of probabilistic processes; refinement is then simply containment between sets of processes. A complete method for verifying containment between specifications, which extends methods for deciding containment between specifications, which extends methods for deciding containment between finite automata or tree acceptors, is presented.",
"Note on generalization, regularization and architecture selection in nonlinear learning systems","The author proposes a new estimate of generalization performance for nonlinear learning systems called the generalized prediction error (GPE) which is based upon the notion of the effective number of parameters p/sub eff/( lambda ). GPE does not require the use of a test set or computationally intensive cross validation and generalizes previously proposed model selection criteria (such as GCV, FPE, AIC, and PSE) in that it is formulated to include biased, nonlinear models (such as back propagation networks) which may incorporate weight decay or other regularizers. The effective number of parameters p/sub eff/( lambda ) depends upon the amount of bias and smoothness (as determined by the regularization parameter lambda ) in the model, but generally differs from the number of weights p. Construction of an optimal architecture thus requires not just finding the weights w/sub lambda /* which minimize the training function U( lambda , w) but also the lambda which minimizes GPE( lambda ).","Learning systems,
Computer architecture,
Supervised learning,
Noise generators,
Computer science,
Internet,
Computer errors,
Testing,
Computer networks,
Adaptive signal processing"
Table-lookup algorithms for elementary functions and their error analysis,"Table-lookup algorithms for calculating elementary functions offer superior speed and accuracy when compared with more traditional algorithms. It is shown that, with careful design, it is feasible to implement table-lookup algorithms in hardware. A uniform approach for carrying out a tight error analysis for such implementations is presented. The advantages of table-lookup algorithms over CORDIC and ordinary (without table-lookup) polynomial algorithms are described.","Error analysis,
Hardware,
Polynomials,
Algorithm design and analysis,
Mathematics,
Computer science,
Laboratories,
Arithmetic,
Yarn,
Runtime library"
Compile-time techniques for data distribution in distributed memory machines,"A solution to the problem of partitioning data for distributed memory machines is discussed. The solution uses a matrix notation to describe array accesses in fully parallel loops, which allows the derivation of sufficient conditions for communication-free partitioning (decomposition) of arrays. A series of examples that illustrate the effectiveness of the technique for linear references, the use of loop transformations in deriving the necessary data decompositions, and a formulation that aids in deriving heuristics for minimizing a communication when communication-free partitions are not feasible are presented.","Programming profession,
Random access memory,
Matrix decomposition,
Program processors,
Sufficient conditions,
Costs,
Data analysis,
Pattern analysis,
Hypercubes,
Information science"
Maximum likelihood SPECT in clinical computation times using mesh-connected parallel computers,"Extending the work of A.W. McCarthy et al. (1988) and M.I. Miller and B. Roysam (1991), the authors demonstrate that a fully parallel implementation of the maximum-likelihood method for single-photon emission computed tomography (SPECT) can be accomplished in clinical time frames on massively parallel systolic array processors. The authors show that for SPECT imaging on 64*64 image grids, with 96 view angles, the single-instruction, multiple data (SIMD) distributed array processor containing 64/sup 2/ processors performs the expectation-maximization (EM) algorithm with Good's smoothing at a rate of 1 iteration/1.5 s. This promises for emission tomography fully Bayesian reconstructions including regularization in clinical computation times which are on the order of 1 min/slice. The most important result of the implementations is that the scaling rules for computation times are roughly linear in the number of processors.","Concurrent computing,
Image reconstruction,
Maximum likelihood detection,
Attenuation,
Maximum likelihood estimation,
Positron emission tomography,
Computed tomography,
Iterative algorithms,
Detectors,
Reconstruction algorithms"
Computer simulation of a carbon-deposition plasma in CH/sub 4/,"A transport and reaction model of a low-pressure, high-frequency (13.56 MHz) CH/sub 4/ plasma used for diamondlike carbon (a-C:H) deposition was developed. The model includes reactions among four molecular species (CH/sub 4/, C/sub 2/H/sub 6/, C/sub 2/H/sub 4/, and H/sub 2/), five radicals and atom (CH/sub 3/, CH/sub 2/, CH, C/sub 2/H/sub 5/, and H), and four ions (CH/sub 4//sup +/, CH/sub 3//sup +/, CH/sub 5//sup +/, and C/sub 2/H/sub 5//sup +/). It also accounts for the influence of the sticking coefficient of species at the walls. Calculated values of the dissociation degree for several flow rates are in good agreement with experimental measurements made by quadrupole mass spectroscopy. A simple surface-model based on the hydrogen coverage of surface and ion flux and energy at the substrate surface was established. This model permitted the calculation of the deposition rate on the powered electrode as a function of the power applied to this electrode. Good agreement between experimental and calculated growth rates was obtained when CH/sub 3/, C/sub 2/H/sub 5/, and CH/sub 2/ were assumed to participate in film formation, and when hydrogen removal by ion bombardment with variable energy as a function of the power was included in the model.","Computer simulation,
Plasma simulation,
Plasma transport processes,
Hydrogen,
Electrodes,
Diamond-like carbon,
Atomic measurements,
Fluid flow measurement,
Mass spectroscopy,
Substrates"
Genetic generation of both the weights and architecture for a neural network,"Shows how to find both the weights and architecture for a neural network, including the number of layers, the number of processing elements per layer, and the connectivity between processing elements. This is accomplished by using a recently developed extension to the genetic algorithm which genetically breeds a population of LISP symbolic expressions of varying size and shape until the desired performance by the network is successfully evolved. The novel 'genetic programming' paradigm is applied to the problem of generating a neural network for a one-bit adder.",
Fluorescent Computer Tomography: A Model For Correction Of X-ray Absorption,,
Combining multiple knowledge bases,"Combining knowledge present in multiple knowledge base systems into a single knowledge base is discussed. A knowledge based system can be considered an extension of a deductive database in that it permits function symbols as part of the theory. Alternative knowledge bases that deal with the same subject matter are considered. The authors define the concept of combining knowledge present in a set of knowledge bases and present algorithms to maximally combine them so that the combination is consistent with respect to the integrity constraints associated with the knowledge bases. For this, the authors define the concept of maximality and prove that the algorithms presented combine the knowledge bases to generate a maximal theory. The authors also discuss the relationships between combining multiple knowledge bases and the view update problem.","Logic,
Computer science,
Lifting equipment,
Marine vehicles,
Constraint theory,
Helium,
Knowledge based systems,
Deductive databases,
Knowledge management,
Military computing"
Decomposition of convex polygonal morphological structuring elements into neighborhood subsets,"A discussion is presented of the decomposition of convex polygon-shaped structuring elements into neighborhood subsets. Such decompositions will lead to efficient implementation of corresponding morphological operations on neighborhood-processing-based parallel image computers. It is proved that all convex polygons are decomposable. Efficient decomposition algorithms are developed for different machine structures. An O(1) time algorithm, with respect to the image size, is developed for the four-neighbor-connected mesh machines; a linear time algorithm for determining the optimal decomposition is provided for the machines that can quickly perform 3*3 morphological operations.","Morphological operations,
Concurrent computing,
Image analysis,
Morphology,
Pipelines,
Shape,
Image processing,
Logic,
Computer aided instruction,
Computer science"
A retrospective view of FA/C distributed problem solving,"The functionally accurate, cooperative (FA/C) paradigm provides a model for task decomposition and agent interaction in a distributed problemsolving system. In this model, agents need not have all the necessary information locally to solve their subproblems, and agents interact through the asynchronous, co-routine exchange of partial results. This model leads to the possibility that agents may behave in an uncoordinated manner. The author traces the development of a series of increasingly sophisticated cooperative control mechanisms for coordinating agents. They include integrating data- and goal-directed control, using static metalevel information specified by an organizational structure, and using dynamic metalevel information developed in partial global planning. The framework of distributed search motivates these developments. Major themes are the importance of sophisticated local control, the interplay between local control and cooperative control, and the use of satisficing cooperative control. Ongoing and new directions for research in FA/C systems are presented.","Problem-solving,
Bandwidth,
Computer networks,
Helium,
Optical wavelength conversion,
Computer science,
Knowledge based systems,
Research initiatives,
Distributed computing,
Resource management"
On unordered codes,"By extending the results obtained by D. E. Knuth (1986), a parallel unordered coding scheme with 2/sup r/ information bits is described. Balanced codes in which each codeword contains equal amounts of zeros and ones, with r check bits and up to 2/sup r+1/-(r+2) information bits, are constructed. Unordered codes with r check bits and up to 2/sup r/+2/sup r-1/-1 information bits are designed. Codes capable of detecting 2/sup r-1/+(2/sup r//2)-1 unidirectional errors using r check bits are also described. A review of previous work is presented.","Decoding,
Binary codes,
Laser transitions,
Very large scale integration,
Fault tolerance,
Sequential circuits,
Optical fibers,
Optical design,
Computer science,
Hamming distance"
Knowledge-based segmentation of Landsat images,"A knowledge-based approach for Landsat image segmentation is proposed. The image segmentation problem is solved by extracting kernel information from the input image to provide an initial interpretation of the image and by using a knowledge-based hierarchical classifier to discriminate between major land-cover types in the study area. The proposed method is designed in such a way that a Landsat image can be segmented and interpreted without any prior image-dependent information. The general spectral land-cover knowledge is constructed from the training land-cover data, and the road information of an image is obtained through a road-detection program.",
On the suitability of non-hardened high density SRAMs for space applications,"Several non-radiation-hardened high-density static RAMs (SRAMs) were tested for susceptibility to single event upset (SEU) and latchup. Test results indicate that at present only a few such device types are suitable for use in space applications. Several additional factors such as susceptibility to multiple-bit upsets and to radiation induced permanent damage need to be taken into consideration before these device types can be recommended. One nonhardened SRAM device type has recently been used on a low-Earth-orbit satellite, enabling the upset rate measured in space to be compared to that predicted from ground-based testing.","Random access memory,
Space technology,
Single event upset,
Extraterrestrial measurements,
Automatic testing,
Computer errors,
Read-write memory,
Satellites,
Particle beams,
Costs"
A test sequence selection method for protocol testing,"A method for automated selection of test sequences from a protocol specification given in Estelle for the purpose of testing both control and data flow aspects of a protocol implementation is discussed. First, a flowgraph modeling the flow of both control and data expressed in the given specification is constructed. In the flowgraph, definitions and uses of each context variable, as well as each input and output interaction parameter employed in the specification, are identified. Based on this information, associations between each output and those inputs that influence the output are established. Test sequences are selected to cover each such association at least once. The resulting test sequences are shown to provide the capability of checking whether a protocol implementation under test establishes the desired flow of both control and data expressed in the protocol specification. The proposed method is illustrated by using the class 0 transport protocol as an example.",
Synthesis of communication protocols: survey and assessment,"Eleven methods for the synthesis of communication protocols are described. Based on particular features of the synthesis process, these methods are classified and compared. In particular, it is noted that interactive methods allow flexibility in the design process; as a result, communication patterns are not prespecified but may be constructed interactively. Methods that only consider the synchronous mode of behavior of communicating entities exclude a wide range of real-life protocols. Methods that make no reference to service requirements do not guarantee the semantic correctness of the synthesized protocol and therefore require the application of a semantic verification procedure. Most methods concentrate on the synthesis of the control part of the protocol entities, which mainly consists of the exchange of synchronization messages. The data part is not adequately treated by any of the synthesis methods. Other than the exchange of synchronization messages, some methods have been extended to deal with unreliable media by synthesizing error-recovery patterns. Some new research directions for enhancing the applicability of the synthesis approach to the design of real-life protocols are obtained.","Access protocols,
Computer science,
Communication effectiveness,
Context-aware services,
Context modeling,
Design methodology,
Performance analysis,
Error analysis,
Error correction"
Self-stabilization by local checking and correction,"The first self-stabilizing end-to-end communication protocol and the most efficient known self-stabilizing network reset protocol are introduced. A simple method of local checking and correction, by which distributed protocols can be made self-stabilizing without the use of unbounded counters, is used. The self-stabilization model distinguishes between catastrophic faults that abstract arbitrary corruption of global state, and other restricted kinds of anticipated faults. It is assumed that after the execution starts there are no further catastrophic faults, but the anticipated faults may continue to occur.",
The combinatorics of heuristic search termination for object recognition in cluttered environments,"Many current recognition systems terminate a search once an interpretation that is good enough is found. The author formally examines the combinatorics of this approach, showing that choosing correct termination procedures can dramatically reduce the search. In particular, the author provides conditions on the object model and the scene clutter such that the expected search is at most quartic. The analytic results are shown to be in agreement with empirical data for cluttered object recognition. These results imply that it is critical to use techniques that select subsets of the data likely to have come from a single object before establishing a correspondence between data and model features.","Combinatorial mathematics,
Object recognition,
Sensor phenomena and characterization,
Layout,
Working environment noise,
Testing,
Search methods,
Performance analysis,
Computer science,
Contracts"
Simulation of SEU transients in CMOS ICs,"An efficient computer simulation algorithm set, SITA, predicts the vulnerability of data stored in and processed by complex combinational logic circuits to SEU. SITA is described in detail to allow researchers to incorporate it into their error analysis packages. Required simulation algorithms are based on approximate closed-form equations modeling individual device behavior in CMOS logic units. Device-level simulation is used to estimate the probability that ion-device interactions produce erroneous signals capable of propagating to a latch (or an output node), and logic-level simulation to predict the spread of such erroneous, latched information through the IC. Simulation results are compared to those from SPICE for several circuit and logic configurations. SITA results are comparable to this established circuit-level code, and SITA can analyze circuits with state-of-the-art device densities (which SPICE cannot). At all IC complexity levels, SITA offers several factors of 10 savings in simulation time over SPICE.","Circuit simulation,
Predictive models,
SPICE,
Computational modeling,
Logic devices,
Computer simulation,
Prediction algorithms,
Combinational circuits,
Error analysis,
Packaging"
A new test generation method for sequential circuits,"A novel test generation method for synchronous sequential circuits is proposed. Among the new ideas employed are: (1) efficiently maintaining path information using an extended value system in forward time processing, and (2) efficiently enumerating cubes for state justification in backward time processing. Experimental results show that the proposed method is effective in generating high coverage tests for sequential circuits.","Circuit testing,
Sequential analysis,
Sequential circuits,
Circuit faults,
Forward contracts,
Flip-flops,
Logic arrays,
Logic testing,
Computer science,
Cities and towns"
Understanding spontaneous speech: the Phoenix system,"The author describes the design of the Phoenix speech understanding system and reports on its current status. Phoenix is a system currently being developed at Carnegie Mellon University to understand spontaneous speech. The system has been implemented for an air travel information service (ATIS) task. In the ATIS task, novice users are asked to perform a task that requires getting information from the air travel database. Users compose the questions themselves, and are allowed to phrase the queries any way they choose. No explicit grammar or lexicon is given to the subject. This task presents several problems not found in real input. Not only is the speech not fluent, but the vocabulary and grammar are open. The results of the processing of both the speech and transcript data for subjects performing the task are reported.","Context modeling,
Databases,
Acoustic noise,
Working environment noise,
Computer science,
Speech processing,
Vocabulary,
Error analysis,
Speech analysis,
Airports"
A framework of knowledge-based assembly planning,"A framework of knowledge-based assembly planning for the automatic generation of assembly plans from the CAD model of an assembly is presented. The knowledge about assembly structure, precedence constraints, and resource constraints is represented using predicate calculus, and it forms the static knowledge database. Production rules are used to generate assembly plans. A graph search mechanism is used to search for the optimal assembly plan. To test the proposed approach, a prototype system has been developed. It can read in the CAD data of a product and the resource information of an assembly cell and automatically generate an optimal assembly plan based on the selection criteria specified by the user of the system.","Assembly systems,
Calculus,
Fixtures,
Optimization methods,
Databases,
Production,
System testing,
Prototypes,
Computer science,
Intelligent manufacturing systems"
Interactive parallel programming using the ParaScope Editor,"The ParaScope Editor, an intelligent interactive editor for parallel Fortran programs, which is the centerpiece of the ParaScope project, an integrated collection of tools to help scientific programmers implement correct and efficient parallel programs, is discussed. ParaScope Editor reveals to users potential hazards of a proposed parallelization in a program. It provides a variety of powerful interactive program transformations that have been shown useful in converting programs to parallel form. ParaScope Editor supports general user editing through a hybrid text and structure editing facility that incrementally analyzes the modified program for potential hazards. It is shown that ParaScope Editor supports an exploratory programming style in which users get immediate feedback on their various strategies for parallelization.",
Formal hardware verification by symbolic ternary trajectory evaluation,,"Hardware,
Circuit simulation,
Computational modeling,
Formal verification,
Automata,
Multivalued logic,
Logic circuits,
Analytical models,
Pipeline processing,
Computer science"
Distributed program checking: a paradigm for building self-stabilizing distributed protocols,"The notion of distributed program checking as a means of making a distributed algorithm self-stabilizing is explored. A compiler that converts a deterministic synchronous protocol pi for static networks into a self-stabilizing version of pi for dynamic networks is described. If T/sub pi / is the time complexity of pi and D is a bound on the diameter of the final network, the compiled version of pi stabilizes in time O(D+T/sub pi /) and has the same space complexity as pi . The general method achieves efficient results for many specific noninteractive tasks. For instance, solutions for the shortest paths and spanning tree problems take O(D) to stabilize, an improvement over the previous best time of O(D/sup 2/).","Protocols,
Contracts,
Costs,
Computer crashes,
Laboratories,
Computer science,
Distributed algorithms,
Topology,
Fault tolerance,
Springs"
The organization and performance of a TREAT-based production system compiler,"An ensemble of techniques that compile OPS5 production system programs to executable machine code is described and an increase in the execution speed of production system programs by two orders of magnitude over the commonly used list processing (LISP)-based OPS5 system is demonstrated. The computer is based on the TREAT incremental match algorithm. A version of the TREAT algorithm, formulated in relational algebra, is presented. The compiler employs optimization techniques derived from relational database system. The combination of the TREAT algorithm and the compiling techniques reduces the proportion of time spent in the match phase below the 'greater than 90%' figure often cited by developers of other production system environments.","Production systems,
Prototypes,
Optimizing compilers,
Program processors,
Algebra,
Relational databases,
Expert systems,
Encoding,
Instruments,
Computer science"
Improved algorithms for mapping pipelined and parallel computations,"Recent work on the problem of mapping pipelined or parallel computations onto linear array, shared memory, and host-satellite systems is extended. It is shown how these problems can be solved even more efficiently when computation module execution times are bounded from below, intermodule communication times are bounded from above, and the processors satisfy certain homogeneity constraints. The improved algorithms have significantly lower time and space complexities than the more general algorithms: in one case, an O(nm/sup 3/) time algorithm for mapping m modules onto n processors is replaced with an O(nm log m) time algorithm, and the space requirements are reduced from O(nm/sup 2/) to O(m). Run-time complexity is reduced further with parallel mapping algorithms based on these improvements, which run on the architectures for which they create mappings.","Concurrent computing,
Strips,
Signal processing algorithms,
Signal processing,
Computer architecture,
Image processing,
Partial differential equations,
NASA,
Computer science,
Runtime"
Development of on-line reactivity meter for nuclear reactors,"An online reactivity meter which has been developed for continuous monitoring of reactivity in the Pakistan Research Reactor-1 (PARR-1) is described. The theory of the kinetic technique of measuring reactivity, used by the meter, is described. The meter itself consists of hardware and software for online acquisition of neutron flux signals from plant instrumentation channels and high-level Fortran-77 real-time programming for the computation of reactivity by the solution of neuron kinetic equation. The PDP-11/23 plant computer is used to monitor the plant reactor with the time sharing of its regular data logging function. For the PARR-2 reactor the reactivity meter has been implemented with an IBM PC/AT personal computer. The response of both reactivity meters is fast enough to monitor safety-related reactivity and power excursions in the two reactors. The results of various reactivity measurements are described. The proper choice and location of nuclear detectors and system calibration are discussed.",
Building an electronic scientific community,"The Community Systems project is building an electronic scientific community, by collecting 'all' the knowledge of a scientific community into a digital library and developing the systems technology to transparently manipulate this library over nationwide networks. The community members are scientists studying the nematode worm C. elegans, a model organism in molecular biology. The community system will encode their knowledge into an information space, with the goal of supporting retrieval and annotation of formal and informal data and information for any biologist with a personal computer and a NREN connection. The paper introduces community systems, the worm community and its knowledge, technology and sociology solutions, and an evolving prototype with plans for its propagation.","Space technology,
Software libraries,
Computer worms,
Biological system modeling,
Organisms,
Computational biology,
Information retrieval,
Biological information theory,
Microcomputers,
Paper technology"
The art gallery theorem for polygons with holes,"Art gallery problems which have been extensively studied over the last decade ask how to station a small (minimum) set of guards in a polygon such that every point of the polygon is watched by at least one guard. The graph-theoretic formulation and solution to the gallery problem for polygons in standard form is given. A complexity analysis is carried out, and open problems are discussed.","Subspace constraints,
Art,
Watches,
Algorithm design and analysis,
Shape,
Upper bound"
Higher-order critical pairs,"A subclass of lambda -terms, called patterns, which have unification properties resembling those of first-order terms, is introduced. Higher-order rewrite systems are defined to be rewrite systems over lambda -terms whose left-hand sides are patterns: this guarantees that the rewrite relation is easily computable. The notion of critical pair is generalized to higher-order rewrite systems, and the analog of the critical pair lemma is proved. The restricted nature of patterns is instrumental in obtaining these results. The critical pair lemma is applied to a number of lambda -calculi and some first-order logic formalized by higher-order rewrite systems.","System testing,
Equations,
Postal services,
Terminology,
Instruments,
Logic"
A partial approach to model checking,"A model-checking method for linear-time temporal logic that avoids the state explosion due to the modeling of concurrency by interleaving is presented. The method relies on the concept of the Mazurkiewicz trace as a semantic basis and uses automata-theoretic techniques, including automata that operate on words of ordinality higher than omega . In particular, automata operating on words of length omega *n, n in omega are defined. These automata are studied, and an efficient algorithm to check whether such automata are nonempty is given. It is shown that when it is viewed as an omega *n automaton, the trace automaton can be substituted for the production automaton in linear-time model checking. The efficiency of the method of P. Godefroid (Proc. Workshop on Computer Aided Verification, 1990) is thus fully available for model checking.","Automata,
Interleaved codes,
Concurrent computing,
Costs,
Probabilistic logic,
Aging,
Explosions,
Logic design"
Characterizing three-dimensional surface structures from visual images,"A new technique for computing intrinsic surface properties is presented. Intrinsic surface properties are those properties of a surface that are not affected by the choice of the coordinate system, the position of the viewer relative to the surface, and the particular parametric representation used to describe the imaged surface. Since intrinsic properties are characteristics of a surface, they are ideal for the purposes of representation and recognition. The intrinsic properties of interest are the principal curvatures, the Gaussian curvatures, and the lines of curvature. It is proposed that a structured-light sensing configuration where a grid pattern is projected to encode the imaged surfaces for analysis be adopted. At each stripe junction, the curvatures of the projected stripes on the imaged surface are computed and related to those of the normal sections that share the same tangential directional as the projected curves. The principal curvatures and their directions at the stripe junction under consideration are then recovered using Euler's theorem. Results obtained using both synthetic and real images are presented.","Surface structures,
Character recognition,
Pattern analysis,
Cameras,
Image analysis,
Shape,
Radiometry,
Layout,
Computer science,
Equations"
On the complexity of generating optimal test sequences,The authors investigate whether maximal overlapping of protocol test subsequences can be achieved in polynomial time. They review the concepts related to FSM (finite state machine)-based test sequence generation and then define the optimal test sequence generation (OTSG) problem. It is proved that the OTSG problem is NP-complete. Therefore an efficient solution to the problem should not be expected in the general case.,"Testing,
Protocols,
Tail,
Councils,
Computer science,
Polynomials"
Clustering on a hypercube multicomputer,"Squared error clustering algorithms for single-instruction multiple-data (SIMD) hypercubes are presented. The algorithms are shown to be asymptotically faster than previously known algorithms and require less memory per processing element (PE). For a clustering problem with N patterns, M features per pattern, and K clusters, the algorithms complete in O(k+log NM) steps on NM processor hypercubes. This is optimal up to a constant factor. These results are extended to the case in which NMK processors are available. Experimental results from a multiple-instruction, multiple-data (MIMD) medium-grain hypercube are also presented.","Hypercubes,
Clustering algorithms,
Pattern recognition,
Computer errors,
Pattern analysis,
Image segmentation,
Silicon carbide,
Computer science,
Partitioning algorithms,
Iterative algorithms"
Critical net routing,"A critical net has been routed traditionally by interconnecting its terminals with a minimum length rectilinear Steiner tree (MRST). A novel interconnection form, the maximum performance tree (MPT), that better approximates an interconnection with optimal circuit performance is proposed. In addition, a heuristic approach is presented that quickly generates MPTs, and the quality of these trees is compared with MRSTs for several net instances.","Routing,
Integrated circuit interconnections,
Wire,
Circuit optimization,
Steiner trees,
Computer science,
Time measurement"
An improved algorithm for 2-D translational motion artifact correction,"The quality of magnetic resonance imaging systems has improved to the point that motion is a major limitation in many examinations. Translational motion in the imaging plane causes the phase of the data to be corrupted. An algorithm using computer post-processing is proposed to correct the phase of the data, and hence remove the artifact. This algorithm has superior convergence properties to an earlier algorithm, which is achieved by incorporating additional prior information specific to the situation. The algorithm is verified using a Shepp and Logan phantom with simulated motion in the imaging plane. It is shown that the algorithm can correct both periodic and random motion, and that the algorithm is not significantly degraded when noise is present.",
A transformation of multiple-valued input two-valued output functions and its application to simplification of exclusive-or sum-of-products expressions,A transformation for p-valued input functions is presented. The number of products in minimum exclusive-or sum-of-products expressions (ESOPs) is invariant under this transformation. Algorithms for reducing the number of product terms in ESOPs using this transformation are presented for p=2 and p=4. Arithmetic functions are simplified to show the ability of this approach.,"Minimization,
Decoding,
Arithmetic,
Logic circuits,
Programmable logic arrays,
Application software,
Circuit synthesis,
Ducts,
Computer science,
Automatic logic units"
Multiprogramming on multiprocessors,"Many solutions have been proposed to the problem of multiprogramming a multiprocessor. However, each has limited applicability or fails to address an important source of overhead. In addition, there has been little experimental comparison of the various solutions in the presence of applications with varying degrees of parallelism and synchronization. The authors explore the tradeoffs between three different approaches to multiprogramming a multiprocessor: time-slicing, coscheduling, and dynamic hardware partitions. They implemented applications that vary in the degree of parallelism, and the frequency and type of synchronization. They show that in most cases coscheduling is preferable to time-slicing. They also show that although there are cases where coscheduling is beneficial, dynamic hardware partitions do no worse, and will often do better. They conclude that under most circumstances, hardware partitioning is the best strategy for multiprogramming a multiprocessor, no matter how much parallelism applications employ or how frequently synchronization occurs.","Hardware,
Parallel processing,
Frequency synchronization,
Switches,
Communication switching,
Quantum computing,
Context modeling,
Operating systems,
Time sharing computer systems,
Computer science"
Computation of probabilities for an island-driven parser,"The authors describe an effort to adapt island-driven parsers to handle stochastic context-free grammars. These grammars could be used as language models (LMs) by a language processor (LP) to computer the probability of a linguistic interpretation. As different islands may compete for growth, it is important to compute the probability that an LM generates a sentence containing islands and gaps between them. Algorithms for computing these probabilities are introduced. The complexity of these algorithms is analyzed both from theoretical and practical points of view. It is shown that the computation of probabilities in the presence of gaps of unknown length requires the impractical solution of a nonlinear system of equations, whereas the computation of probabilities for cases with gaps containing a known number of unknown words has polynomial time complexity and is practically feasible. The use of the results obtained in automatic speech understanding systems is discussed.","Stochastic processes,
Algorithm design and analysis,
Automatic speech recognition,
Nonlinear systems,
Nonlinear equations,
Polynomials,
Speech processing,
Natural languages,
Councils,
Computer science"
Performance debugging shared memory multiprocessor programs with MTOOL,,Debugging
A computer-vision technique for the acquisition and processing of 3-D profiles of dental imprints: an application in orthodontics,"The authors present a computer vision technique for the acquisition and processing of 3-D images of the profile of wax dental imprints in the automation of diagnosis in orthodontics. The acquisition of the 3-D images is based on the absorption of light by a dispersive medium and uses standard CCD (charge coupled device) cameras. The profiles of both sides of the imprint are acquired simultaneously. The 3-D image of each side of the imprint is segmented by nonlinear filtering of the 3-D data, and the interstices between the teeth are detected. Two operators are presented: one for the detection of the interstices between the teeth for incisors, canines, and premolars, and one for those between molars. A method for deciding the optimal neighborhood of application of each operator is also presented. Experimental results show that the two operators are very effective at detecting the interstices.","Charge-coupled image sensors,
Teeth,
Computer vision,
Dentistry,
Automation,
Absorption,
Dispersion,
Charge coupled devices,
Image segmentation,
Filtering"
Ambivalent data structures for dynamic 2-edge-connectivity and k smallest spanning trees,"Ambivalent data structures are presented for several problems on undirected graphs. They are used in finding the k smallest spanning trees of a weighted undirected graph in O(m log beta (m,n)+min(k/sup 3/2/, km/sup 1/2/)) time, where m is the number of edges and n the number of vertices in the graph. The techniques are extended to find the k smallest spanning trees in an embedded planar graph in O(n+k(log n)/sup 3/) time. Ambivalent data structures are also used to maintain dynamically 2-edge-connectivity information. Edges and vertices can be inserted or deleted in O(m/sup 1/2/) time, and a query as to whether two vertices are in the same 2-edge-connected component can be answered in O(log n) time, where m and n are understood to be the current number of edges and vertices, respectively. Again, the techniques are extended to maintain an embedded planar graph so that edges and vertices can be inserted or deleted in O((log n)/sup 3/) time, and a query answered in O(log n) time.","Tree data structures,
Tree graphs,
Data structures,
Computer science,
Contracts"
Policies for efficient memory utilization in a remote caching architecture,"The high performance networks in modern distributed systems enable a site to obtain cached objects from the main memory of other sites more quickly than the time needed to access local disks. In this environment, efficient mechanisms can be devised to support rapid request/response exchanges for objects that reside on remote sites. As a result, it becomes possible to use remote memory as an additional layer in the memory hierarchy between local memory and disks. The paper studies the performance potential of remote memory caching. A key issue is managing the of object replicas maintained by different sites. Although exploiting remote memory improves performance over a wide range of cache sizes as compared to a distributed client/server caching architecture, efficient use of remote memory can be difficult to achieve. The effect of several system parameters, and several cache policies are examined; their effect on both overall and local system performance is described.","Computer architecture,
Resource management,
Intelligent networks,
Computer science,
System performance,
Distributed computing,
Availability,
Delay,
Ethernet networks,
Access protocols"
Extending a Tool Integration Language,,"Kernel,
Software tools,
Computer science,
Hafnium,
Programming,
Costs,
Encapsulation,
Testing,
Joining processes"
Resolution enhancement of tomographic images using the row action projection method,"The row action projection (RAP) method is used to increase the spatial resolution of images reconstructed via the filtered back projection (FBP) algorithm. An implementation of RAP is introduced which is computationally efficient and facilitates local adaptation of the projection operators. The local mean value as well as minimum and maximum bounds are used as constraints. The method is proposed to provide zoom-in capability, which yields a high-resolution estimate of a specified region of the image. Computer simulations demonstrate the new method to be very effective in recovering high-order spectral components of designated regions of the reconstructed image.","Image resolution,
Tomography,
Image reconstruction,
Spatial resolution,
Bandwidth,
Computer errors,
Smoothing methods,
Image restoration,
Yield estimation,
Computer simulation"
Scheduling parallel machines on-line,The authors study the problem of scheduling jobs on parallel machines when the existence of a job is not known until an unknown release date and the processing requirement of a job is not known until the job is processed to completion. They demonstrate two general algorithmic techniques for converting existing polynomial-time algorithms that require complete knowledge about the input data into algorithms that need less advance knowledge. They prove information-theoretic lower bounds on the lengths of online schedules for several basic parallel machine models and then show that the algorithms construct schedules with lengths that either match or come within a constant factor of the lower bounds.,
Modeling uncertainty in databases,"Relational algebra operations were extended to produce, together with answers to queries, information regarding sources that contributed to the answers. The author's previous model is reviewed and the semantic interpretation is presented. It is shown that extended relational algebra operations are precise, that is, they produce exactly the same answers that are expected under the semantic interpretation. Algorithms for computing the reliability of answers to a query are also reviewed and their correctness under the semantic interpretation proposed is proved.","Uncertainty,
Relational databases,
Algebra,
Artificial intelligence,
Database systems,
Computer science,
Decision making,
Solids,
Prototypes,
Probability"
An experimental study comparing the effectiveness of computer graphics data versus computer tabular data,A number of studies that compare the effectiveness of data presented in graphic form versus data presented in tabular form have been performed. Results have been mixed. In a study by Dickson et al. (1986) that used business students as subjects it is suggested that task variables are paramount in the determination of such effectiveness. It is suggested that the type of data and type of subject used in the work by Dickson et al. favored tabular presentation and compromise any generalization of their conclusions. The current study finds that user training/expertise is a critical variable in determining the effectiveness of graphs versus tables. The conclusion drawn is that a user-friendly system should make both modes available so that the users have the option of selecting their preferred mode.,"Computer graphics,
Humans,
Computer displays,
Degradation,
Computer science,
Cities and towns,
Computational Intelligence Society,
History,
Performance analysis,
Data analysis"
Using type inference and induced rules to provide intensional answers,"A new approach is presented that uses knowledge induction and type inference to provide intensional answers. Machine learning techniques are used to analyze database contents and to induce a set of if-then rules. Type inference which is based on forward inference and backward inference is developed that uses database type hierarchies to derive the intensional answers for a query. It is shown that more precise intensional answers can be derived by properly merging the type inference results from multiple type hierarchies. A prototype intensional query-processing system which uses the proposed approach has been implemented. Using a ship database as a testbed, the effectiveness of the use of type interference and induced rules to derive specific intensional answers is demonstrated.","Databases,
Marine vehicles,
Contracts,
Data analysis,
Testing,
Query processing,
Computer science,
Missiles,
Underwater vehicles,
Machine learning"
High-frequency scattering by a smooth coated cylinder simulated with generalized impedance boundary conditions,"Rigorous uniform geometrical theory of diffraction (UTD) diffraction coefficients are presented for a convex coated cylinder simulated with generalized impedance boundary conditions. In particular, ray solutions are obtained which remain valid in the transition region and reduce uniformly to those in the deep lit and shadow regions. These involve new transition functions in place of the usual Fock-type integrals, characteristic to the impedance cylinder. A uniform asymptotic solution is also presented for observations in the close vicinity of the cylinder. As usual, the diffraction coefficients for the convex cylinder are obtained via a generalization of the corresponding ones for the circular cylinder.","Scattering,
Coatings,
Surface treatment,
Boundary conditions,
Optical surface waves,
Surface impedance,
Impedance"
Adaptive interfaces: modeling tasks and users,"The use of fuzzy logic is suggested as a powerful set of techniques for building user models. With this in mind, the author presents an introductory overview of several projects that are approaching user modeling from a fuzzy perspective. It is suggested that fuzzy user models and fuzzy users can provide the necessary adaptive mechanisms for intelligent human-computer interfaces.","Fuzzy logic,
Telephony,
Information retrieval,
Information systems,
Intelligent systems,
Cognitive science,
Control system synthesis,
Information resources,
System performance,
Databases"
Removal of redundant dependences in DOACROSS loops with constant dependences,"An efficient algorithm to remove redundant dependences in simple loops with constant dependences is presented. Dependences constrain the parallel execution of programs and are typically enforced by synchronization instructions. The synchronization instructions represent a significant part of the overhead in the parallel execution of a program. Some program dependences are redundant because they are covered by other dependences. It is shown that unlike with single loops, in the case of nested loops, a particular dependence may be redundant at some iterations but not redundant at others, so that the redundancy of a dependence may not be uniform over the entire iteration space. A sufficient condition for the uniformity of redundancy in a doubly nested loop is developed.",
Finding convex edge groupings in an image,"A method for identifying groups of intensity edges in an image that are likely to result from the same convex object in a scene is described. A key property of the method is that its output is no more complex than the original image. The method uses a triangulation of linear edge segments to define a local neighborhood that is scale invariant. From this local neighborhood a local convexity graph that encodes which neighboring image edges could be part of a convex group of image edges is constructed. A path in the graph corresponds to a convex polygonal chain in the image, such as a convex polygon or a spiral. Examples are presented to illustrate that the technique find intuitively salient groups.","Image segmentation,
Layout,
Spirals,
Computer science,
Image recognition,
Filtering,
Shape,
Computational complexity,
Statistical analysis,
Jacobian matrices"
Enterprise Analyzer: electronic support for group requirements elicitation,"The paper reports two cases in which groups from the USA Army used an automated meeting environment to define the functional requirements of two modules of a large integrated information system. The process used for these sessions is called Enterprise Analyzer, a combination of tools, facilities, facilitation, and processes that blends aspects of both computer aided software engineering (CASE) and the University of Arizona's GroupSystems Electronic Meeting System (EMS). The paper describes the evolution of the Enterprise Analyzer concept as it matured throughout the two elicitation and refinement sessions. These experiences led us to develop new techniques, tools and procedures for group requirements elicitation, which have been used during three later sessions and will be used in future sessions.","Management information systems,
Computer aided software engineering,
Project management,
Productivity,
Medical services,
Europe,
Drugs,
Contracts,
Standardization,
Joining processes"
