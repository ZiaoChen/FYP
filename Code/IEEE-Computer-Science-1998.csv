Title,Abstract,Keywords
Bilateral filtering for gray and color images,"Bilateral filtering smooths images while preserving edges, by means of a nonlinear combination of nearby image values. The method is noniterative, local, and simple. It combines gray levels or colors based on both their geometric closeness and their photometric similarity, and prefers near values to distant values in both domain and range. In contrast with filters that operate on the three bands of a color image separately, a bilateral filter can enforce the perceptual metric underlying the CIE-Lab color space, and smooth colors and preserve edges in a way that is tuned to human perception. Also, in contrast with standard filtering, bilateral filtering produces no phantom colors along edges in color images, and reduces phantom colors where they appear in the original image.",
Design and construction of a realistic digital brain phantom,"After conception and implementation of any new medical image processing algorithm, validation is an important step to ensure that the procedure fulfils all requirements set forth at the initial design stage. Although the algorithm must be evaluated on real data, a comprehensive validation requires the additional use of simulated data since it is impossible to establish ground truth with in vivo data. Experiments with simulated data permit controlled evaluation over a wide range of conditions (e.g., different levels of noise, contrast, intensity artefacts, or geometric distortion). Such considerations have become increasingly important with the rapid growth of neuroimaging, i.e., computational analysis of brain structure and function using brain scanning methods such as positron emission tomography and magnetic resonance imaging. Since simple objects such as ellipsoids or parallelepipedes do not reflect the complexity of natural brain anatomy, the authors present the design and creation of a realistic, high-resolution, digital, volumetric phantom of the human brain. This three-dimensional digital brain phantom is made up of ten volumetric data sets that define the spatial distribution for different tissues (e.g., grey matter, white matter, muscle, skin, etc.), where voxel intensity is proportional to the fraction of tissue within the voxel. The digital brain phantom can be used to simulate tomographic images of the head. Since the contribution of each tissue type to each voxel in the brain phantom is known, it can be used as the gold standard to test analysis algorithms such as classification procedures which seek to identify the tissue ""type"" of each image voxel. Furthermore, since the same anatomical phantom may be used to drive simulators for different modalities, it is the ideal tool to test intermodality registration algorithms. The brain phantom and simulated MR images have been made publicly available on the Internet (http://www.bic.mni.mcgill.ca/brainweb).","Imaging phantoms,
Brain modeling,
Algorithm design and analysis,
Image analysis,
Testing,
Biomedical image processing,
Medical simulation,
In vivo,
Computational modeling,
Solid modeling"
OpenMP: an industry standard API for shared-memory programming,"At its most elemental level, OpenMP is a set of compiler directives and callable runtime library routines that extend Fortran (and separately, C and C++ to express shared memory parallelism. It leaves the base language unspecified, and vendors can implement OpenMP in any Fortran compiler. Naturally, to support pointers and allocatables, Fortran 90 and Fortran 95 require the OpenMP implementation to include additional semantics over Fortran 77. OpenMP leverages many of the X3H5 concepts while extending them to support coarse grain parallelism. The standard also includes a callable runtime library with accompanying environment variables.","Message passing,
Scalability,
Hardware,
Computer architecture,
Power system modeling,
ANSI standards,
Parallel processing,
Coherence,
Software systems,
Parallel programming"
Predicting error in rigid-body point-based registration,"Guidance systems designed for neurosurgery, hip surgery, and spine surgery, and for approaches to other anatomy that is relatively rigid can use rigid-body transformations to accomplish image registration. These systems often rely on point-based registration to determine the transformation, and many such systems use attached fiducial markers to establish accurate fiducial points for the registration, the points being established by some fiducial localization process. Accuracy is important to these systems, as is knowledge of the level of that accuracy. An advantage of marker-based systems, particularly those in which the markers are bone-implanted, is that registration error depends only on the fiducial localization error (FLE) and is thus to a large extent independent of the particular object being registered. Thus, it should be possible to predict the clinical accuracy of marker-based systems on the basis of experimental measurements made with phantoms or previous patients. This paper presents two new expressions for estimating registration accuracy of such systems and points out a danger in using a traditional measure of registration accuracy. The new expressions represent fundamental theoretical results with regard to the relationship between localization error and registration error in rigid-body, point-based registration. Rigid-body, point-based registration is achieved by finding the rigid transformation that minimizes ""fiducial registration error"" (FRE), which is the root mean square distance between homologous fiducials after registration. Closed form solutions have been known since 1966. The expected value (FRE/sup 2/) depends on the number N of fiducials and expected squared value of FLE, (FLE/sup 2/), but in 1979 it was shown that (FRE/sup 2/) is approximately independent of the fiducial configuration C. The importance of this surprising result seems not yet to have been appreciated by the registration community: Poor registrations caused by poor fiducial configurations may appear to be good due to a small FRE value. A more critical and direct measure of registration error is the ""target registration error"" (TRE), which is the distance between homologous points other than the centroids of fiducials. Efforts to characterize its behavior have been made since 1989. Published numerical simulations have shown that (TRE/sup 2/) is roughly proportional to (FLE/sup 2/)/N and, unlike (FRE/sup 2/), does depend in some way on C. Thus, FRE, which is often used as feedback to the surgeon using a point-based guidance system, is in fact an unreliable indicator of registration-accuracy. In this work the authors derive approximate expressions for (TRE/sup 2/), and for the expected squared alignment error of an individual fiducial. They validate both approximations through numerical simulations. The former expression can be used to provide reliable feedback to the surgeon during surgery and to guide the placement of markers before surgery, or at least to warn the surgeon of potentially dangerous fiducial placements; the latter expression leads to a surprising conclusion: Expected registration accuracy (TRE) is worst near the fiducials that are most closely aligned! This revelation should be of particular concern to surgeons who may at present be relying on fiducial alignment as an indicator of the accuracy of their point-based guidance systems.",
Some new indexes of cluster validity,"We review two clustering algorithms (hard c-means and single linkage) and three indexes of crisp cluster validity (Hubert's statistics, the Davies-Bouldin index, and Dunn's index). We illustrate two deficiencies of Dunn's index which make it overly sensitive to noisy clusters and propose several generalizations of it that are not as brittle to outliers in the clusters. Our numerical examples show that the standard measure of interset distance (the minimum distance between points in a pair of sets) is the worst (least reliable) measure upon which to base cluster validation indexes when the clusters are expected to form volumetric clouds. Experimental results also suggest that intercluster separation plays a more important role in cluster validation than cluster diameter. Our simulations show that while Dunn's original index has operational flaws, the concept it embodies provides a rich paradigm for validation of partitions that have cloud-like clusters. Five of our generalized Dunn's indexes provide the best validation results for the simulations presented.","Clustering algorithms,
Couplings,
Statistics,
Volume measurement,
Partitioning algorithms,
Measurement standards,
Clouds,
Computer science,
Fuzzy logic,
Machine intelligence"
Collusion-secure fingerprinting for digital data,"This paper discusses methods for assigning code-words for the purpose of fingerprinting digital data, e.g., software, documents, music, and video. Fingerprinting consists of uniquely marking and registering each copy of the data. This marking allows a distributor to detect any unauthorized copy and trace it back to the user. This threat of detection will deter users from releasing unauthorized copies. A problem arises when users collude: for digital data, two different fingerprinted objects can be compared and the differences between them detected. Hence, a set of users can collude to detect the location of the fingerprint. They can then alter the fingerprint to mask their identities. We present a general fingerprinting solution which is secure in the context of collusion. In addition, we discuss methods for distributing fingerprinted data.","Fingerprint recognition,
Cryptography,
Protection,
Computer science,
Object detection,
Watermarking,
Explosives,
Motion pictures,
Terminology"
A metric for distributions with applications to image databases,"We introduce a new distance between two distributions that we call the Earth Mover's Distance (EMD), which reflects the minimal amount of work that must be performed to transform one distribution into the other by moving ""distribution mass"" around. This is a special case of the transportation problem from linear optimization, for which efficient algorithms are available. The EMD also allows for partial matching. When used to compare distributions that have the same overall mass, the EMD is a true metric, and has easy-to-compute lower bounds. In this paper we focus on applications to image databases, especially color and texture. We use the EMD to exhibit the structure of color-distribution and texture spaces by means of Multi-Dimensional Scaling displays. We also propose a novel approach to the problem of navigating through a collection of color images, which leads to a new paradigm for image database search.","Image databases,
Histograms,
Image retrieval,
Psychology,
Frequency,
Application software,
Computer science,
Geoscience,
Computer displays,
Navigation"
Elliptical head tracking using intensity gradients and color histograms,"An algorithm for tracking a person's head is presented. The head's projection onto the image plane is modeled as an ellipse whose position and size are continually updated by a local search combining the output of a module concentrating on the intensity gradient around the ellipse's perimeter with that of another module focusing on the color histogram of the ellipse's interior. Since these two modules have roughly orthogonal failure modes, they serve to complement one another. The result is a robust, real-time system that is able to track a person's head with enough accuracy to automatically control the camera's pan, tilt, and zoom in order to keep the person centered in the field of view at a desired size. Extensive experimentation shows the algorithm's robustness with respect to full 360-degree out-of-plane rotation, up to 90-degree tilting, severe but brief occlusion, arbitrary camera movement, and multiple moving people in the background.","Histograms,
Electrical capacitance tomography,
Head,
Cameras,
Videoconference,
Computer science,
Tracking,
Set theory,
Computer aided instruction,
Surveillance"
Supporting stored video: reducing rate variability and end-to-end resource requirements through optimal smoothing,"Variable-bit-rate (VBR) compressed video can exhibit significant multiple-time-scale bit-rate variability. In this paper we consider the transmission of stored video from a server to a client across a network, and explore how the client buffer space can be used most effectively toward reducing the variability of the transmitted bit rate. Two basic results are presented. First, we show how to achieve the greatest possible reduction in rate variability when sending stored video to a client with given buffer size. We formally establish the optimality of our approach and illustrate its performance over a set of long MPEG-1 encoded video traces. Second, we evaluate the impact of optimal smoothing on the network resources needed for video transport, under two network service models: deterministic guaranteed service (Chang 1994; Wrege et al. 1996) and renegotiated constant-bit-rate (RCBR) service (Grossglauser et al. 1997). Under both models, the impact of optimal smoothing is dramatic.",
Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms,"This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These test sare compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely used statistical tests are shown to have high probability of type I error in certain situations and should never be used: a test for the difference of two proportions and a paired-differences t test based on taking several random train-test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of type I error. A fourth test, McNemar's test, is shown to have low type I error. The fifth test is a new test, 5 × 2 cv, based on five iterations of twofold cross-validation. Experiments show that this test also has acceptable type I error. The article also measures the power (ability to detect algorithm differences when they do exist) of these tests. The cross-validated t test is the most powerful. The 5×2 cv test is shown to be slightly more powerful than McNemar's test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, Mc-Nemar's test is the only test with acceptable type I error. For algorithms that can be executed 10 times, the 5 × 2 cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set.",
Automatic tumor segmentation using knowledge-based techniques,"A system that automatically segments and labels glioblastoma-multiforme tumors in magnetic resonance images (MRIs) of the human brain is presented. The MRIs consist of T1-weighted, proton density, and T2-weighted feature images and are processed by a system which integrates knowledge-based (KB) techniques with multispectral analysis. Initial segmentation is performed by an unsupervised clustering algorithm. The segmented image, along with cluster centers for each class are provided to a rule-based expert system which extracts the intracranial region. Multispectral histogram analysis separates suspected tumor from the rest of the intracranial region, with region analysis used in performing the final tumor labeling. This system has been trained on three volume data sets and tested on thirteen unseen volume data sets acquired from a single MRI system. The KB tumor segmentation was compared with supervised, radiologist-labeled ""ground truth"" tumor volumes and supervised K-nearest neighbors tumor segmentations. The results of this system generally correspond well to ground truth, both on a per slice basis and more importantly in tracking total tumor volume during treatment over time.",
List-mode likelihood: EM algorithm and image quality estimation demonstrated on 2-D PET,"Using a theory of list-mode maximum-likelihood (ML) source reconstruction presented recently by Barrett et al. (1997), this paper formulates a corresponding expectation-maximization (EM) algorithm, as well as a method for estimating noise properties at the ML estimate. List-mode ML is of interest in cases where the dimensionality of the measurement space impedes a binning of the measurement data. It can be advantageous in cases where a better forward model can be obtained by including more measurement coordinates provided by a given detector. Different figures of merit for the detector performance can be computed from the Fisher information matrix (FIM). This paper uses the observed FIM, which requires a single data set, thus, avoiding costly ensemble statistics. The proposed techniques are demonstrated for an idealized two-dimensional (2-D) positron emission tomography (PET) [2-D PET] detector. The authors compute from simulation data the improved image quality obtained by including the time of flight of the coincident quanta.","Image quality,
Positron emission tomography,
Detectors,
Maximum likelihood estimation,
Two dimensional displays,
Maximum likelihood detection,
Image reconstruction,
Impedance,
Coordinate measuring machines,
Statistics"
The Visible Human Project,"The Visible Human Project data sets are designed to serve as a common reference point for the study of human anatomy, as a set of common public-domain data for testing medical imaging algorithms, and as a testbed and model for the construction of image libraries that can be accessed through networks. The data sets are being applied to a wide range of educational, diagnostic, treatment planning, virtual reality, artistic, mathematical, and industrial uses by more than 800 licensees in 27 countries. But key issues remain in the development of methods to link such image data to text-based data. Standards do not currently exist for such linkages. Basic research is needed in the description and representation of image-based structures and in the connection of image-based structural-anatomical data to text-based functional-physiological data. This is the larger, long-term goal of the Visible Human Project: to link the print library of functional-physiological knowledge with the image library of structural-anatomical knowledge transparently into one unified resource of health information.",
The distributed constraint satisfaction problem: formalization and algorithms,"We develop a formalism called a distributed constraint satisfaction problem (distributed CSP) and algorithms for solving distributed CSPs. A distributed CSP is a constraint satisfaction problem in which variables and constraints are distributed among multiple agents. Various application problems in distributed artificial intelligence can be formalized as distributed CSPs. We present our newly developed technique called asynchronous backtracking that allows agents to act asynchronously and concurrently without any global control, while guaranteeing the completeness of the algorithm. Furthermore, we describe how the asynchronous backtracking algorithm can be modified into a more efficient algorithm called an asynchronous weak-commitment search, which can revise a bad decision without exhaustive search by changing the priority order of agents dynamically. The experimental results on various example problems show that the asynchronous weak-commitment search algorithm is, by far more, efficient than the asynchronous backtracking algorithm and can solve fairly large-scale problems.",
The NEOS Server,"The Network-Enabled Optimization System (NEOS) is an Internet based optimization service. The NEOS Server introduces a novel approach for solving optimization problems. Users of the NEOS Server submit a problem and their choice of optimization solver over the Internet. The NEOS Server computes all information (for example, derivatives and sparsity patterns) required by the solver, links the optimization problem with the solver, and returns a solution.","Web server,
Constraint optimization,
Jacobian matrices,
Laboratories,
Internet,
Workstations,
Software libraries,
Algorithms,
Computer networks,
IP networks"
Fast data broadcasting and receiving scheme for popular video service,"Using multiple channels to broadcast a popular video can reduce the viewer's waiting time to approach video-on-demand service. For a given bandwidth allocation, pyramid broadcasting schemes can provide users with shorter waiting time as compared with conventional broadcasting schemes. However, for a 120-minute movie, the waiting time is only reduced to 20 minutes as we allocate 4 video channels for the movie. The harmonic broadcasting scheme can reduce the waiting time to 4 minutes but it can not support the live telecast. This paper presents a new broadcasting scheme, which can support live videos and reduce the waiting time to 8 minutes for the above case. With the same waiting requirement for a hot video, the new scheme can greatly reduce the bandwidth requirements and provide heterogeneous users' services.","Multimedia communication,
Motion pictures,
Channel allocation,
Bandwidth,
Video on demand,
Digital video broadcasting,
TV broadcasting,
Councils,
Computer science"
Tikhonov regularization and prior information in electrical impedance tomography,"The solution of impedance distribution in electrical impedance tomography is a nonlinear inverse problem that requires the use of a regularization method. The generalized Tikhonov regularization methods have been popular in the solution of many inverse problems. The regularization matrices that are usually used with the Tikhonov method are more or less ad hoc and the implicit prior assumptions are, thus, in many cases inappropriate. In this paper, the authors propose an approach to the construction of the regularization matrix that conforms to the prior assumptions on the impedance distribution. The approach is based on the construction of an approximating subspace for the expected impedance distributions. It is shown by simulations that the reconstructions obtained with the proposed method are better than with two other schemes of the same type when the prior is compatible with the true object. On the other hand, when the prior is incompatible with the true object, the method will still give reasonable estimates.",
TCP-like congestion control for layered multicast data transfer,"We present a novel congestion control algorithm suitable for use with cumulative, layered data streams in the MBone. Our algorithm behaves similarly to TCP congestion control algorithms, and shares bandwidth fairly with other instances of the protocol and with TCP flows. It is entirely receiver driven and requires no per-receiver status at the sender, in order to scale to large numbers of receivers. It relies on standard functionalities of multicast routers, and is suitable for continuous stream and reliable bulk data transfer. In the paper we illustrate the algorithm, characterize its response to losses both analytically and by simulations, and analyse its behaviour using simulations and experiments in real networks. We also show how error recovery can be dealt with independently from congestion control by using FEC techniques, so as to provide reliable bulk data transfer.","Multicast algorithms,
Multicast protocols,
Bandwidth,
Algorithm design and analysis,
Analytical models,
Streaming media,
Computer science,
Educational institutions,
Error correction,
Forward error correction"
Maximum-likelihood estimation of Rician distribution parameters,"The problem of parameter estimation from Rician distributed data (e.g., magnitude magnetic resonance images) is addressed. The properties of conventional estimation methods are discussed and compared to maximum-likelihood (ML) estimation which is known to yield optimal results asymptotically. In contrast to previously proposed methods, ML estimation is demonstrated to be unbiased for high signal-to-noise ratio (SNR) and to yield physical relevant results for low SNR.","Maximum likelihood estimation,
Rician channels,
Parameter estimation,
Signal to noise ratio,
Magnetic resonance,
Yield estimation,
Gaussian noise,
Image reconstruction,
Physics,
Magnetic resonance imaging"
Segmenting cortical gray matter for functional MRI visualization,"We describe a system that is being used to segment gray matter and create connected cortical representations from MRI. The method exploits knowledge of the anatomy of the cortex and incorporates structural constraints into the segmentation. First, the white matter and CSF regions in the MR volume are segmented using some novel techniques of posterior anisotropic diffusion. Then, the user selects the cortical white matter component of interest, and its structure is verified by checking for cavities and handles. After this, a connected representation of the gray matter is created by a constrained growing-out from the white matter boundary. Because the connectivity is computed, the segmentation can be used as input to several methods of visualizing the spatial pattern of cortical activity within gray matter. In our case, the connected representation of gray matter is used to create a representation of the flattened cortex. Then, fMRI measurements are overlaid on the flattened representation, yielding a representation of the volumetric data within a single image.","Magnetic resonance imaging,
Data visualization,
Level measurement,
Computer science,
Psychology,
Neuroscience,
Anatomy,
Anisotropic magnetoresistance,
Volume measurement,
Software measurement"
A fuzzy vessel tracking algorithm for retinal images based on fuzzy clustering,"In this paper the authors present a new unsupervised fuzzy algorithm for vessel tracking that is applied to the detection of the ocular fundus vessels. The proposed method overcomes the problems of initialization and vessel profile modeling that are encountered in the literature and automatically tracks fundus vessels using linguistic descriptions like ""vessel"" and ""nonvessel."" The main tool for determining vessel and nonvessel regions along a vessel profile is the fuzzy C-means clustering algorithm that is fed with properly preprocessed data, Additional procedures for checking the validity of the detected vessels and handling junctions and forks are also presented. The application of the proposed algorithm to fundus images and simulated vessels resulted in very good overall performance and consistent estimation of vessel parameters.",
Architecture-based runtime software evolution,"Continuous availability is a critical requirement for an important class of software systems. For these systems, runtime system evolution can mitigate the costs and risks associated with shutting down and restarting the system for an update. We present an architecture-based approach to runtime software evolution and highlight the role of software connectors in supporting runtime change. An initial implementation of a tool suite for supporting the runtime modification of software architectures, called ArchStudio, is presented.","Runtime,
Connectors,
Software systems,
Software architecture,
Costs,
Mission critical systems,
Operating systems,
Control systems,
Computer architecture,
Computer science"
GTM: The Generative Topographic Mapping,"Latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. A familiar example is factor analysis, which is based on a linear transformation between the latent space and the data space. In this article, we introduce a form of nonlinear latent variable model called the generative topographic mapping, for which the parameters of the model can be determined using the expectation-maximization algorithm. GTM provides a principled alternative to the widely used self-organizing map (SOM) of Kohonen (1982) and overcomes most of the significant limitations of the SOM. We demonstrate the performance of the GTM algorithm on a toy problem and on simulated data from flow diagnostics for a multiphase oil pipeline.",
Information distance,"While Kolmogorov (1965) complexity is the accepted absolute measure of information content in an individual finite object, a similarly absolute notion is needed for the information distance between two individual objects, for example, two pictures. We give several natural definitions of a universal information metric, based on length of shortest programs for either ordinary computations or reversible (dissipationless) computations. It turns out that these definitions are equivalent up to an additive logarithmic term. We show that the information distance is a universal cognitive similarity distance. We investigate the maximal correlation of the shortest programs involved, the maximal uncorrelation of programs (a generalization of the Slepian-Wolf theorem of classical information theory), and the density properties of the discrete metric spaces induced by the information distances. A related distance measures the amount of nonreversibility of a computation. Using the physical theory of reversible computation, we give an appropriate (universal, antisymmetric, and transitive) measure of the thermodynamic work required to transform one object in another object by the most efficient process. Information distance between individual objects is needed in pattern recognition where one wants to express effective notions of ""pattern similarity"" or ""cognitive similarity"" between individual objects and in thermodynamics of computation where one wants to analyze the energy dissipation of a computation from a particular input to a particular output.","Information theory,
Thermodynamics,
Pattern recognition,
Entropy,
Noise measurement,
Computer science,
Turing machines,
Additives,
Physics computing,
Discrete transforms"
Color- and texture-based image segmentation using EM and its application to content-based image retrieval,"Retrieving images from large and varied collections using image content as a key is a challenging and important problem. In this paper we present a new image representation which provides a transformation from the raw pixel data to a small set of image regions which are coherent in color and texture space. This so-called ""blobworld"" representation is based on segmentation using the expectation-maximization algorithm on combined color and texture features. The texture features we use for the segmentation arise from a new approach to texture description and scale selection. We describe a system that uses the blobworld representation to retrieve images. An important and unique aspect of the system is that, in the context of similarity-based querying, the user is allowed to view the internal representation of the submitted image and the query results. Similar systems do not offer the user this view into the workings of the system; consequently, the outcome of many queries on these systems can be quite inexplicable, despite the availability of knobs for adjusting the similarity metric.","Image segmentation,
Image retrieval,
Image representation,
Application software,
Content based retrieval,
Pixel,
Image databases,
Computer science,
Expectation-maximization algorithms,
Information retrieval"
From Fermi acceleration to collisionless discharge heating,"The heating of electrons by time-varying fields is fundamental to the operation of radio frequency (RF) and microwave discharges. Ohmic heating, in which the phase of the electron oscillation motion in the field is randomized locally by interparticle collisions, can dominate at high pressures. Phase randomization can also occur due to electron thermal motion in spatially inhomogeneous RF fields, even in the absence of collisions, leading to collisionless or stochastic heating, which can dominate at low pressures, Generally, electrons are heated collisionlessly by repeated interaction with fields that are localized within a sheath, skin depth layer, or resonance layer inside the discharge. This suggests the simple heating model of a ball bouncing elastically back and forth between a fixed and an oscillating wall. Such a model was proposed originally by Fermi to explain the origin of cosmic rays. In this review, Fermi acceleration is used as a paradigm to describe collisionless heating and phase randomization in capacitive, inductive, and electron cyclotron resonance (ECR) discharges. Mapping models for Fermi acceleration are introduced, and the Fokker-Planck description of the heating and the effects of phase correlations are described. The collisionless heating rates are determined in capacitive and inductive discharges and compared with self-consistent (kinetic) calculations where available. Experimental measurements and computer simulations are reviewed and compared to theoretical calculations. Recent measurements and calculations of nonlocal heating effects, such as negative electron power absorption, are described, Incomplete phase randomization and adiabatic barriers are shown to modify the heating in low pressure ECR discharges.",
Experimental models for validating technology,"Experimentation helps determine the effectiveness of proposed theories and methods. However, computer science has not developed a concise taxonomy of methods for demonstrating the validity of new techniques. Experimentation is a crucial part of attribute evaluation and can help determine whether methods used in accordance with some theory during product development will result in software being as effective as necessary. By looking at multiple examples of technology validation, the authors develop a taxonomy for software engineering experimentation that describes twelve different experimental approaches.","Testing,
Computer science,
Software safety,
Costs,
Paper technology,
Taxonomy,
Particle measurements,
Software measurement,
Time measurement,
Hardware"
Using automatic clustering to produce high-level system organizations of source code,We describe a collection of algorithms that we developed and implemented to facilitate the automatic recovery of the modular structure of a software system from its source code. We treat automatic modularization as an optimization problem. Our algorithms make use of traditional hill-climbing and genetic algorithms.,
Constructive Incremental Learning from Only Local Information,"We introduce a constructive, incremental learning system for regression problems that models data by means of spatially localized linear models. In contrast to other approaches, the size and shape of the receptive field of each locally linear model, as well as the parameters of the locally linear model itself, are learned independently, that is, without the need for competition or any other kind of communication. Independent learning is accomplished by incrementally minimizing a weighted local cross-validation error. As a result, we obtain a learning system that can allocate resources as needed while dealing with the bias-variance dilemma in a principled way. The spatial localization of the linear models increases robustness toward negative interference. Our learning system can be interpreted as a nonparametric adaptive bandwidth smoother, as a mixture of experts where the experts are trained in isolation, and as a learning system that profits from combining independent expert knowledge on the same problem. This article illustrates the potential learning capabilities of purely local learning and offers an interesting and powerful approach to learning with receptive fields.",
Faster and simpler algorithms for multicommodity flow and other fractional packing problems,"This paper considers the problem of designing fast, approximate, combinatorial algorithms for multicommodity flows and other fractional packing problems. We provide a different approach to these problems which yields faster and much simpler algorithms. Our approach also allows us to substitute shortest path computations for min-cost flow computations in computing maximum concurrent flow and min-cost multicommodity flow; this yields much faster algorithms when the number of commodities is large.","Concurrent computing,
Throughput,
Computer science,
Design engineering,
Polynomials"
A new algorithm for error-tolerant subgraph isomorphism detection,"We propose a new algorithm for error-correcting subgraph isomorphism detection from a set of model graphs to an unknown input graph. The algorithm is based on a compact representation of the model graphs. This representation is derived from the set of model graphs in an off-line preprocessing step. The main advantage of the proposed representation is that common subgraphs of different model graphs are represented only once. Therefore, at run time, given an unknown input graph, the computational effort of matching the common subgraphs for each model graph onto the input graph is done only once. Consequently, the new algorithm is only sublinearly dependent on the number of model graphs. Furthermore, the new algorithm can be combined with a future cost estimation method that greatly improves its run-time performance.",
Operation of the unified power flow controller (UPFC) under practical constraints,"The UPFC is the most versatile and complex power electronic equipment that has emerged for the control and optimization of power flow in electrical power transmission systems. It offers major potential advantages for the static and dynamic operation of transmission lines, but it brings with it major design challenges, both in the power electronics and from the perspective of the power system. As the UPFC transitions from concept to full-scale power system implementation, the control and protection of this sophisticated equipment are of primary concern. This paper describes the basic control, sequencing and protection philosophies that govern the operation of the UPFC, subject to the practical constraints encountered in an actual high power installation. The operation of the UPFC is illustrated with representative results from a TNA study, undertaken jointly by the Electric Power Research Institute (EPRI), Western Area Power Administration (WAPA) and Westinghouse Science and Technology Center (STC).","Load flow,
Inverters,
Automatic control,
Feedback,
Reactive power,
Automatic voltage control,
Control systems,
Impedance,
Voltage control,
Load flow control"
Video indexing based on mosaic representations,"Video is a rich source of information. It provides visual information about scenes. This information is implicitly buried inside the raw video data, however, and is provided with the cost of very high temporal redundancy. While the standard sequential form of video storage is adequate for viewing in a movie mode, it fails to support rapid access to information of interest that is required in many of the emerging applications of video. This paper presents an approach for efficient access, use and manipulation of video data. The video data are first transformed from their sequential and redundant frame-based representation, in which the information about the scene is distributed over many frames, to an explicit and compact scene-based representation, to which each frame can be directly related. This compact reorganization of the video data supports nonlinear browsing and efficient indexing to provide rapid access directly to information of interest. This paper describes a new set of methods for indexing into the video sequence based on the scene-based representation. These indexing methods are based on geometric and dynamic information contained in the video. These methods complement the more traditional content-based indexing methods, which utilize image appearance information (namely, color and texture properties) but are considerably simpler to achieve and are highly computationally efficient.","Indexing,
Video compression,
Layout,
Information resources,
Costs,
Video sequences,
Merging,
Graphics,
Motion pictures,
Computer science"
The Globus project: a status report,"The Globus project is a multi-institutional research effort that seeks to enable the construction of computational grids providing pervasive, dependable, and consistent access to high-performance computational resources, despite geographical distribution of both resources and users. Computational grid technology is being viewed as a critical element of future high-performance computing environments that will enable entirely new classes of computation-oriented applications, much as the World Wide Web fostered the development of new classes of information-oriented applications. The authors report on the status of the Globus project as of early 1998. They describe the progress that has been achieved to date in the development of the Globus toolkit, a set of core services for constructing grid tools and applications. They also discuss the Globus Ubiquitous Supercomputing Testbed (GUSTO) that they have constructed to enable large-scale evaluation of Globus technologies, and they review early experiences with the development of large-scale grid applications on the GUSTO testbed.","Grid computing,
Distributed computing,
Pervasive computing,
Computer networks,
Testing,
Large-scale systems,
Prototypes,
Computational modeling,
Mathematics,
Computer science"
Making use of population information in evolutionary artificial neural networks,"This paper is concerned with the simultaneous evolution of artificial neural network (ANN) architectures and weights. The current practice in evolving ANN's is to choose the best ANN in the last generation as the final result. This paper proposes a different approach to form the final result by combining all the individuals in the last generation in order to make best use of all the information contained in the whole population. This approach regards a population of ANN's as an ensemble and uses a combination method to integrate them. Although there has been some work on integrating ANN modules, little has been done in evolutionary learning to make best use of its population information. Four linear combination methods have been investigated in this paper to illustrate our ideas. Three real-world data sets have been used in our experimental studies, which show that the recursive least-square (RLS) algorithm always produces an integrated system that outperforms the best individual. The results confirm that a population contains more information than a single individual. Evolutionary learning should exploit such information to improve generalization of learned systems.","Intelligent networks,
Artificial neural networks,
Resonance light scattering,
Genetic programming,
Evolutionary computation,
Design methodology,
Australia Council,
Computational intelligence,
Computer science,
Statistics"
A virtual image cryptosystem based upon vector quantization,"We propose a new image cryptosystem to protect image data. It encrypts the original image into another virtual image. Since both original and virtual images are significant, our new cryptosystem can confuse illegal users. Besides the camouflage, this new cryptosystem has three other benefits. First, our cryptosystem is secure even if the illegal users know that our virtual image is a camouflage. Second, this cryptosystem can compress image data. Finally, our method is more efficient than a method that encrypts the entire image directly.","Cryptography,
Vector quantization,
Data security,
Information security,
Protection,
Image coding,
Humans,
Eyes,
Computer science,
Information management"
Segmentation and tracking in echocardiographic sequences: active contours guided by optical flow estimates,"This paper presents a method for segmentation and tracking of cardiac structures in ultrasound image sequences. The developed algorithm is based on the active contour framework. This approach requires initial placement of the contour close to the desired position in the image, usually an object outline. Best contour shape and position are then calculated, assuming that at this configuration a global energy function, associated with a contour, attains its minimum. Active contours can be used for tracking by selecting a solution from a previous frame as an initial position in a present frame. Such an approach, however, fails for large displacements of the object of interest. This paper presents a technique that incorporates the information on pixel velocities (optical flow) into the estimate of initial contour to enable tracking of fast-moving objects. The algorithm was tested on several ultrasound image sequences, each covering one complete cardiac cycle. The contour successfully tracked boundaries of mitral valve leaflets, aortic root and endocardial borders of the left ventricle. The algorithm-generated outlines were compared against manual tracings by expert physicians. The automated method resulted in contours that were within the boundaries of intraobserver variability.",
Should computer scientists experiment more?,"Computer scientists and practitioners defend their lack of experimentation with a wide range of arguments. Some arguments suggest that experimentation is inappropriate, too difficult, useless, and even harmful. This article discusses several such arguments to illustrate the importance of experimentation for computer science. It considers how the software industry is beginning to value experiments, because results may give a company a three- to five-year lead over the competition.","Computer science,
Testing,
Computer aided manufacturing,
Laser modes,
Laser theory,
Probability,
Humans,
Brain modeling,
Nervous system,
Immune system"
Analysis of probabilistic roadmaps for path planning,"We provide an analysis of a path planning method which uses probabilistic roadmaps. This method has proven very successful in practice, but the theoretical understanding of its performance is still limited. Assuming that a path /spl gamma/ exists between two configurations a and b of the robot, we study the dependence of the failure probability to connect a and b, on: 1) the length of /spl gamma/; 2) the distance function of /spl gamma/ from the obstacles; 3) the number of nodes N of the probabilistic roadmap constructed. Importantly, our results do not depend strongly on local irregularities of the configuration space, as was the case with previous analysis. These results are illustrated with a simple but illuminating example. In this example, we provide estimates for N, the principal parameter of the method, in order to achieve failure probability within prescribed bounds. We also compare, through this example, the different approaches to the analysis of the planning method.","Path planning,
Orbital robotics,
Robots,
Motion planning,
Computer aided manufacturing,
Robotics and automation,
Computer science,
Application software,
Biology computing,
Surgery"
Accelerating fast multipole methods for the Helmholtz equation at low frequencies,"The authors describe a diagonal form for translating far-field expansions to use in low frequency fast multipole methods. Their approach combines evanescent and propagating plane waves to reduce the computational cost of FMM implementation. More specifically, we present the analytic foundations for a new version of the fast multipole method for the scalar Helmholtz equation in the low frequency regime. The computational cost of existing FMM implementations, is dominated by the expense of translating far field partial wave expansions to local ones, requiring 189p/sup 4/ or 189p/sup 3/ operations per box, where harmonics up to order p/sup 2/ have been retained. By developing a new expansion in plane waves, we can diagonalize these translation operators. The new low frequency FMM (LF-FMM) requires 40p/sup 2/+6p/sup 2/ operations per box.","Frequency,
Electromagnetic scattering,
Laplace equations,
Electromagnetic propagation,
Acoustic propagation,
Microwave propagation,
Computational efficiency,
Acoustic waves,
Electromagnetic modeling,
Large-scale systems"
Mining association rules with weighted items,"Discovery of association rules has been found useful in many applications. In previous work, all items in a basket database are treated uniformly. We generalize this to the case where items are given weights to reflect their importance to the user. The weights may correspond to special promotions on some products, or the profitability of different items. We can mine the weighted association rules with weights. The downward closure property of the support measure in the unweighted case no longer exists and previous algorithms cannot be applied. In this paper, two new algorithms are introduced to handle this problem. In these algorithms we make use of a metric called the k-support bound in the mining process. Experimental results show the efficiency of the algorithms for large databases.",
Pipeline gating: speculation control for energy reduction,"Branch prediction has enabled microprocessors to increase instruction level parallelism (ILP) by allowing programs to speculatively execute beyond control boundaries. Although speculative execution is essential for increasing the instructions per cycle (IPC), it does come at a cost. A large amount of unnecessary work results from wrong-path instructions entering the pipeline due to branch misprediction. Results generated with the SimpleScalar tool set using a 4-way issue pipeline and various branch predictors show an instruction overhead of 16% to 105% for event instruction committed. The instruction overhead will increase in the future as processors use more aggressive speculation and wider issue widths. In this paper we present an innovative method for power reduction ,which, unlike previous work that sacrificed flexibility or performance reduces power in high-performance microprocessors without impacting performance. In particular we introduce a hardware mechanism called pipeline gating to control rampant speculation in the pipeline. We present inexpensive mechanisms for determining when a branch is likely to mispredict, and for stopping wrong-path instructions from entering the pipeline. Results show up to a 38% reduction in wrong-path instructions with a negligible performance loss (/spl ap/1%). Best of all, even in programs with a high branch prediction accuracy, performance does not noticeable degrade. Our analysis indicates that there is little risk in implementing this method in existing processors since it does not impact performance and can benefit energy reduction.","Pipelines,
Microprocessors,
Costs,
Read only memory,
Power engineering and energy,
Computer science,
Degradation,
Risk analysis,
Application software,
Packaging"
Optimized homomorphic unsharp masking for MR grayscale inhomogeneity correction,"Grayscale inhomogeneities in magnetic resonance (MR) images confound quantitative analysis of these images. Homomorphic unsharp masking and its variations have been commonly used as a post-processing method to remove inhomogeneities in MR images, However, little data is available in the literature assessing the relative effectiveness of these algorithms to remove inhomogeneities, or describing how these algorithms can affect image data. In this study, the authors address these questions quantitatively using simulated images with artificially constructed and empirically measured bias fields. The authors' results show that mean-based filtering is consistently more effective than median-based algorithms for removing inhomogeneities in MR images, and that artifacts are frequently introduced into images at the most commonly used window sizes. The authors' results demonstrate dramatic improvement in the effectiveness of the algorithms with significantly larger windows than are commonly used.","Gray-scale,
Radio frequency,
Magnetic resonance imaging,
Image analysis,
Magnetic analysis,
Magnetic field measurement,
Filtering,
Image segmentation,
Humans,
Biomedical imaging"
Mosaics of scenes with moving objects,"Image mosaics are useful for a variety of tasks in vision and computer graphics. A particularly convenient way to generate mosaics is by 'stitching' together many ordinary photographs. Existing algorithms focus on capturing static scenes. This paper presents a complete system for creating visually pleasing mosaics in the presence of moving objects. There are three primary contributions. The first component of our system is a registration method that remains unbiased by movement-the Mellin transform is extended to register images related by a projective transform. Second an efficient method for finding a globally consistent registration of all images is developed. By solving a linear system of equations, derived from many pairwise registration matrices, we find an optimal global registration. Lastly, a new method of compositing images is presented. Blurred areas due to moving objects are avoided by segmenting the mosaic into disjoint regions and sampling pixels in each region from a single source image.","Layout,
Cameras,
Computer graphics,
Transforms,
Pixel,
Computer science,
Linear systems,
Equations,
Image segmentation,
Image sampling"
Segmentation and interpretation of MR brain images. An improved active shape model,"This paper reports a novel method for fully automated segmentation that is based on description of shape and its variation using point distribution models (PDM's). An improvement of the active shape procedure introduced by Cootes and Taylor (1997) to find new examples of previously learned shapes using PDM's is presented. The new method for segmentation and interpretation of deep neuroanatomic structures such as thalamus, putamen, ventricular system, etc. incorporates a priori knowledge about shapes of the neuroanatomic structures to provide their robust segmentation and labeling in magnetic resonance (MR) brain images. The method was trained in eight MR brain images and tested in 19 brain images by comparison to observer-defined independent standards. Neuroanatomic structures in all testing images were successfully identified. Computer-identified and observer-defined neuroanatomic structures agreed well. The average labeling error was 7%/spl plusmn/3%. Border positioning errors were quite small, with the average border positioning error of 0.8/spl plusmn/0.1 pixels in 256/spl times/256 MR images. The presented method was specifically developed for segmentation of neuroanatomic structures in MR brain images. However, it is generally applicable to virtually any task involving deformable shape analysis.","Image segmentation,
Brain,
Active shape model,
Labeling,
Computer errors,
Robustness,
Magnetic resonance,
Testing,
Magnetic analysis,
Humans"
A novel approach to microcalcification detection using fuzzy logic technique,"Breast cancer continues to be a significant public health problem in the United States. Approximately, 182,000 new cases of breast cancer are diagnosed and 46,000 women die of breast cancer each year. Even more disturbing is the fact that one out of eight women in the United States will develop breast cancer at some point during her lifetime. Since the cause of breast cancer remains unknown, primary prevention becomes impossible. Computer-aided mammography is an important and challenging task in automated diagnosis. It has great potential over traditional interpretation of film-screen mammography in terms of efficiency and accuracy. Microcalcifications are the earliest sign of breast carcinomas and their detection is one of the key issues for breast cancer control. In this study, a novel approach to microcalcification detection based on fuzzy logic technique is presented. Microcalcifications are first enhanced based on their brightness and nonuniformity. Then, the irrelevant breast structures are excluded by a curve detector. Finally, microcalcifications are located using an iterative threshold selection method. The shapes of microcalcifications are reconstructed and the isolated pixels are removed by employing the mathematical morphology technique. The essential idea of the proposed approach is to apply a fuzzified image of a mammogram to locate the suspicious regions and to interact the fuzzified image with the original image to preserve fidelity. The major advantage of the proposed method is its ability to detect microcalcifications even in very dense breast mammograms. A series of clinical mammograms are employed to test the proposed algorithm and the performance is evaluated by the free-response receiver operating characteristic curve. The experiments aptly show that the microcalcifications can be accurately detected even in very dense mammograms using the proposed approach.","Fuzzy logic,
Breast cancer,
Mammography,
Public healthcare,
Cancer detection,
Automatic control,
Brightness,
Detectors,
Iterative methods,
Shape"
Investigation of intraoperative brain deformation using a 1.5-T interventional MR system: preliminary results,"All image-guided neurosurgical systems that the authors are aware of assume that the head and its contents behave as a rigid body. It is important to measure intraoperative brain deformation (brain shift) to provide some indication of the application accuracy of image-guided surgical systems, and also to provide data to develop and validate nonrigid registration algorithms to correct for such deformation. The authors are collecting data from patients undergoing neurosurgery in a high-field (1.5 T) interventional magnetic resonance (MR) scanner. High-contrast and high-resolution gradient-echo MR image volumes are collected immediately prior to surgery, during surgery, and at the end of surgery, with the patient intubated and lying on the operating table in the operative position. Here, the authors report initial results from six patients: one freehand biopsy, one stereotactic functional procedure, and four resections. The authors investigate intraoperative brain deformation by examining threshold boundary overlays and difference images and by measuring ventricular volume. They also present preliminary results obtained using a nonrigid registration algorithm to quantify deformation. They found that some cases had much greater deformation than others, and also that, regardless of the procedure, there was very little deformation of the midline, the tentorium, the hemisphere contralateral to the procedure, and ipsilateral structures except those that are within 1 cm of the lesion or are gravitationally above the surgical site.",
Computer-aided diagnosis: a neural-network-based approach to lung nodule detection,"In this work, the authors have developed a computer-aided diagnosis system, based on a two-level artificial neural network (ANN) architecture. This was trained, tested, and evaluated specifically on the problem of detecting lung cancer nodules found on digitized chest radiographs. The first ANN performs the detection of suspicious regions in a low-resolution image. The input to the second ANN are the curvature peaks computed for all pixels in each suspicious region. This comes from the fact that small tumors possess and identifiable signature in curvature-peak feature space, where curvature is the local curvature of the image data when viewed as a relief map. The output of this network is thresholded at a chosen level of significance to give a positive detection. Tests are performed using 60 radiographs taken from a routine clinic with 90 real nodules and 288 simulated nodules. The authors employed free-response receiver operating characteristics method with the mean number of false positives (FP's) and the sensitivity as performance indexes to evaluate all the simulation results. The combination of the two networks provide results of 89%-96% sensitivity and 5-7 FP's/image, depending on the size of the nodules.",
Partial-volume Bayesian classification of material mixtures in MR volume data using voxel histograms,"The authors present a new algorithm for identifying the distribution of different material types in volumetric datasets such as those produced with magnetic resonance imaging (MRI) or computed tomography (CT). Because the authors allow for mixtures of materials and treat voxels as regions, their technique reduces errors that other classification techniques can create along boundaries between materials and is particularly useful for creating accurate geometric models and renderings from volume data. It also has the potential to make volume measurements more accurately and classifies noisy, low-resolution data well. There are two unusual aspects to the authors' approach. First, they assume that, due to partial-volume effects, or blurring, voxels can contain more than one material, e.g., both muscle and fat; the authors compute the relative proportion of each material in the voxels. Second, they incorporate information from neighboring voxels into the classification process by reconstructing a continuous function, /spl rho/(x), from the samples and then looking at the distribution of values that /spl rho/(x) takes on within the region of a voxel. This distribution of values is represented by a histogram taken over the region of the voxel; the mixture of materials that those values measure is identified within the voxel using a probabilistic Bayesian approach that matches the histogram by finding the mixture of materials within each voxel most likely to have created the histogram. The size of regions that the authors classify is chosen to match the sparing of the samples because the spacing is intrinsically related to the minimum feature size that the reconstructed continuous function can represent.",
The self-reconfiguring robotic molecule,"We discuss a robotic module called a molecule. Molecules can be the basis for building self-reconfiguring robots. They support multiple modalities of locomotion and manipulation. We describe the design, functionality, and control of the molecule. We show how a set of molecules can aggregate as active three-dimensional structures that can move and change shape. Finally, we discuss our molecule experiments.","Robots,
Bonding,
Connectors,
Shape,
Computer science,
Educational institutions,
Buildings,
Aggregates,
Robustness,
Protocols"
A general environment for the treatment of discrete problems and its application to the finite element method,"A general computer aided description environment for the treatment of discrete problems, based on a concise structure for both development and application levels, is described and applied to the finite element method. Its characteristics reveal its great ability to welcome in an evolutive way a wide range of physical models and numerical methods, with any coupling of them. It is therefore adapted to various activities, such as research, collaboration, education, training and industrial studies.",
"Nearest prototype classification: clustering, genetic algorithms, or random search?","Three questions related to the nearest prototype classifier (NPC) are addressed: when is it better to construct the prototypes instead of selecting them as a subset of the given labeled data; how can we trade classification accuracy for a reduction in the number of prototypes; and how good is pure random search (RS) for selection of prototypes from the data? We compare the resubstitution performance of the NPC on the IRIS data set, where the prototypes are either extracted by replacement (R-prototypes) or by selection (S-prototypes). Results for the R-prototypes are taken from a previous study and are contrasted with S-prototype results obtained by a genetic algorithm (GA) or by RS. The best results reached by both algorithms (GA and RS), followed by resubstitution NPC, are two errors with sets of three S-prototypes. This compares favorably to the best result found with R-prototypes, viz., three errors with five R-prototypes. Based on our results, we recommend GA selection for the NPC. A by-product of this research is a counter example to minimality of a recently published minimal consistent set selection procedure.","Prototypes,
Genetic algorithms,
Iris,
Data mining,
Counting circuits,
Algorithm design and analysis,
Pattern recognition,
Error analysis,
Mathematics,
Computer science"
Zero-error information theory,"The problem of error-free transmission capacity of a noisy channel was posed by Shannon in 1956 and remains unsolved, Nevertheless, partial results for this and similar channel and source coding problems have had a considerable impact on information theory, computer science, and mathematics. We review the techniques, results, information measures, and challenges encountered in this ongoing quest.",
CSE education: An introduction to scientific programming,"The University of Utah's Department of Computer Science has offered an introductory course on scientific programming, called Engineering Computing, since 1994. Each year, approximately 300 first- and second-year science and engineering majors take the course. They learn how to use a variety of programming techniques to solve the kinds of computational science problems they will encounter in their academic and professional careers.","Integral equations,
Kernel,
Electrostatics,
Circuits"
The potential for using thread-level data speculation to facilitate automatic parallelization,"As we look to the future, and the prospect of a billion transistors on a chip, it seems inevitable that microprocessors will exploit having multiple parallel threads. To achieve the full potential of these ""single-chip multiprocessors"", however, we must find a way to parallelize non-numeric applications. Unfortunately, compilers have had little success in parallelizing non-numeric codes due to their complex access patterns. This paper explores the potential for using thread-level data speculation (TLDS) to overcome this limitation by allowing the compiler to view parallelization solely as a cost/benefit tradeoff rather than something which is likely to violate program correctness. Our experimental results demonstrate that with realistic compiler support, TLDS can offer significant program speedups. We also demonstrate that through modest hardware extensions, a generic single-chip multiprocessor could support TLDS by augmenting its cache coherence scheme to detect dependence violations, and by using the primary data caches to buffer speculative state.","Yarn,
Hip,
Application software,
Parallel processing,
Computer science,
Microprocessors,
Ores,
Integrated circuit technology,
Transistors,
Face detection"
An empirical study of the effects of minimization on the fault detection capabilities of test suites,"Test suite minimization techniques attempt to reduce the cost of saving and reusing tests during software maintenance, by eliminating redundant tests from test suites. A potential drawback of these techniques is that in minimizing a test suite, they might reduce the ability of that test suite to reveal faults in the software. A study showed that minimization can reduce test suite size without significantly reducing the fault detection capabilities of test suites. To further investigate this issue, we performed an experiment in which we compared the costs and benefits of minimizing test suites of various sizes for several programs. In contrast to the previous study, our results reveal that the fault detection capabilities of test suites can be severely compromised by minimization.","Fault detection,
Software testing,
Costs,
Software maintenance,
Computer science,
Information science,
Read only memory,
Minimization methods"
The relative performance of various mapping algorithms is independent of sizable variances in run-time predictions,"The author studies the performance of four mapping algorithms. The four algorithms include two naive ones: opportunistic load balancing (OLB), and limited best assignment (LBA), and two intelligent greedy algorithms: an O(nm) greedy algorithm, and an O(n/sup 2/m) greedy algorithm. All of these algorithms, except OLB, use expected run-times to assign jobs to machines. As expected run-times are rarely deterministic in modern networked and server based systems, he first uses experimentation to determine some plausible run-time distributions. Using these distributions, he next executes simulations to determine how the mapping algorithms perform. Performance comparisons show that the greedy algorithms produce schedules that, when executed, perform better than naive algorithms, even though the exact run-times are not available to the schedulers. He concludes that the use of intelligent mapping algorithms is beneficial, even when the expected time for completion of a job is not deterministic.",
Adaptive key frame extraction using unsupervised clustering,"Key frame extraction has been recognized as one of the important research issues in video information retrieval. Although progress has been made in key frame extraction, the existing approaches are either computationally expensive or ineffective in capturing salient visual content. We first discuss the importance of key frame selection; and then review and evaluate the existing approaches. To overcome the shortcomings of the existing approaches, we introduce a new algorithm for key frame extraction based on unsupervised clustering. The proposed algorithm is both computationally simple and able to adapt to the visual content. The efficiency and effectiveness are validated by large amount of real-world videos.","Data mining,
Streaming media,
Video compression,
Gunshot detection systems,
Computer science,
Clustering algorithms,
Indexing,
Graphics,
Costs,
Image coding"
Video OCR for digital news archive,"Video OCR is a technique that can greatly help to locate topics of interest in a large digital news video archive via the automatic extraction and reading of captions and annotations. News captions generally provide vital search information about the video being presented, the names of people and places or descriptions of objects. In this paper, two difficult problems of character recognition for videos are addressed: low resolution characters and extremely complex backgrounds. We apply an interpolation filter, multi-frame integration and a combination of four filters to solve these problems. Segmenting characters is done by a recognition-based segmentation method and intermediate character recognition results are used to improve the segmentation. The overall recognition results are good enough for use in news indexing. Performing video OCR on news video and combining its results with other video understanding techniques will improve the overall understanding of the news video content.",
Interactive tools for education in automatic control,"Experiments have shown that the time is now ripe for a new generation of interactive learning tools for control. The tools are based on objects which admit direct graphical manipulation. During manipulations, objects are updated instantaneously, so that relations between objects are maintained all the time. The tools are natural complements to traditional education, and allow students to quickly gain insight and motivation. A high degree of interactivity has been found to be a key issue in the design. Together with a high bandwidth in the man-machine interaction, this enhances learning significantly. Another nice feature is the possibility to hide minor issues and focus on the essentials. It is not easy to describe the power of these tools adequately in text. The best way to appreciate them is simply to use them. We believe that there is a strong pedagogical potential for the type of tools that we have described. We are also of the opinion that we are only at the very beginning in the development of learning tools of this type. The addition of sound and animation are interesting avenues that should be pursued.","Automatic control,
Computer science education,
Control engineering education,
Computational modeling,
Feedback,
Robust stability,
Control systems,
Laboratories,
Man machine systems,
Mathematical model"
Registration of head CT images to physical space using a weighted combination of points and surfaces [image-guided surgery],"Most previously reported registration techniques that align three-dimensional image volumes by matching geometrical features such as points or surfaces use a single type of feature. The authors recently reported a hybrid registration technique that uses a weighted combination of multiple geometrical feature shapes. In this study they use the weighted geometrical feature (WGF) algorithm to register computed tomography (CT) images of the head to physical space using the skin surface only, the bone surface only, and various weighted combinations of these surfaces and one fiducial point (centroid of a bone-implanted marker). The authors use data acquired from 12 patients that underwent temporal lobe craniotomies for the resection of cerebral lesions. The authors evaluate and compare the accuracy of the registrations obtained using these various approaches by using as a reference gold standard the registration obtained using three bone-implanted markers. The results demonstrate that a combination of geometrical features can improve the accuracy of CT-to-physical space registration. Point-based registration requires a minimum of three noncollinear points. The position of a bone-implanted marker can be determined much more accurately than that of a skin-affixed marker or an anatomic landmark. A major disadvantage of using bone-implanted markers is that an invasive procedure is required to implant each marker. By combining surface information, the WGF algorithm allows registration to be performed using only one or two such markers. One important finding is that the use of a single, very accurate point (a bone-implanted marker) allows very accurate surface-based registration to be achieved using very few surface points. Finally, the WGF algorithm, which not only allows the combination of multiple types of geometrical information but also handles point-based and surface-based registration as degenerate cases, could form the foundation of a ""flexible"" surgical navigation system that allows the surgeon to use what he considers the method most appropriate for an individual clinical situation.","Head,
Computed tomography,
Surgery,
Shape,
Registers,
Skin,
Bones,
Temporal lobe,
Lesions,
Gold"
An image-guided planning system for endosseous oral implants,"A preoperative planning system for oral implant surgery was developed which takes as input computed tomographies (CT's) of the jaws. Two-dimensional (2-D) reslices of these axial CT slices orthogonal to a curve following the jaw arch are computed and shown together with three-dimensional (3-D) surface rendered models of the bone and computer-aided design (CAD)-like implant models. A technique is developed for scanning and visualizing an eventual existing removable prosthesis together with the bone structures. Evaluation of the planning done with the system shows a difference between 2-D and 3-D planning methods. Validation studies measure the benefits of the 3-D approach by comparing plans made in 2-D mode only with those further adjusted using the full 3-D visualization capabilities of the system. The benefits of a 3-D approach are then evident where a prosthesis is involved in the planning. For the majority of the patients, clinically important adjustments and optimizations to the 2-D plans are made once the 3-D visualization is enabled, effectively resulting in a better plan. The alterations are related to bone quality and quantity (p<0.05), biomechanics (p<0.005), and esthetics (p<0.005), and are so obvious that the 3-D plan stands out clearly (p<0.005). The improvements often avoid complications such as mandibular nerve damage, sinus perforations, fenestrations, or dehiscences.","Implants,
Surgery,
Computed tomography,
Bones,
Prosthetics,
Visualization,
Biomedical imaging,
Dentistry,
Titanium,
Radiology"
Rotation invariant neural network-based face detection,"In this paper, we present a neural network-based face detection system. Unlike similar systems which are limited to detecting upright, frontal faces, this system detects faces at any degree of rotation in the image plane. The system employs multiple networks; a ""router"" network first processes each input window to determine its orientation and then uses this information to prepare the window for one or more ""detector"" networks. We present the training methods for both types of networks. We also perform sensitivity analysis on the networks, and present empirical results on a large test set. Finally, we present preliminary results for detecting faces rotated out of the image plane, such as profiles and semi-profiles.","Neural networks,
Face detection,
Detectors,
Histograms,
Pixel,
Computer science,
Data preprocessing,
Change detection algorithms,
Detection algorithms,
Skin"
Best neighborhood matching: an information loss restoration technique for block-based image coding systems,"Imperfect transmission of block-coded images often results in lost blocks. A novel error concealment method called best neighborhood matching (BNM) is presented by using a special kind of information redundancy-blockwise similarity within the image. The proposed algorithm can utilize the information of not only neighboring pixels, but also remote regions in the image. Very good restoration results are obtained by experiments.","Image restoration,
Image coding,
Video compression,
Circuits,
Computer science,
Image sequences,
Image processing,
Vector quantization,
Discrete transforms"
Territorial multi-robot task division,"This work demonstrates the application of the distributed behavior-based approach to generating a multirobot controller for a group of mobile robots performing a clean-up and collection task. The paper studies a territorial approach to the task in which the robots are assigned individual territories that can be dynamically resized if one of the robots malfunctions, permitting the completion of the task. The described controller is implemented on a group of four IS Robotics R2e mobile robots. Using a collection of experimental robot data, we empirically derive and demonstrate most effective foraging in our domain, and show the decline of performance of the space division strategy with increased group size.","Orbital robotics,
Mobile robots,
Interference,
Distributed control,
Control systems,
Multirobot systems,
Computer science,
Intelligent robots,
Artificial intelligence,
Control system synthesis"
Effect of soft and softer handoffs on CDMA system capacity,"The effect of soft and softer handoffs on code-division multiple-access (CDMA) system capacity is evaluated for unsectorized and sectorized hexagonal cells according to an average bit energy-to-interference power spectral density, which corresponds to a bit-error rate (BER) of 10/sup -3/. The effect of imperfect sectorization on sectorization efficiency is also considered. On the reverse link, there is no capacity loss as no extra channels are needed to perform soft handoff, while the macrodiversity provided by soft handoff can improve the reverse-link quality and extend the cell coverage. On the forward link, when soft handoff is employed in unsectorized cells, the capacity loss due to two traffic channels assigned to a user in the handoff zone is 0.2% or 1.1% for a voice activity factor of 3/8 or 1/2, respectively. As the forward-link capacity is higher than that of the reverse link, this small capacity loss does not affect the system capacity. For sectorized cells having three sectors per cell, there are overlapping coverage areas between sectors, where mobiles in these areas are subjected to an increase in cochannel interference. For an overlapping angle of 5/spl deg/, the sectorization efficiency is 0.96 and 0.7 for the reverse-link and forward-link systems, respectively. When soft and softer handoffs are employed, the forward-link sectorization efficiency is improved to 0.97. We find the application of soft and softer handoff improves not only the forward-link capacity, but also the signal-to-interference ratio (SIR) for mobiles near the cell and sector boundaries.","Multiaccess communication,
Mobile communication,
Bit error rate,
Interchannel interference,
Base stations,
Matched filters,
Communications technology,
Computer science"
Interactive ray tracing for isosurface rendering,"We show that it is feasible to perform interactive isosurfacing of very large rectilinear datasets with brute-force ray tracing on a conventional (distributed) shared-memory multiprocessor machine. Rather than generate geometry representing the isosurface and render with a z-buffer, for each pixel we trace a ray through a volume and do an analytic isosurface intersection computation. Although this method has a high intrinsic computational cost, its simplicity and scalability make it ideal for large datasets on current high-end systems. Incorporating simple optimizations, such as volume bricking and a shallow hierarchy, enables interactive rendering (i.e. 10 frames per second) of the 1 GByte full resolution Visible Woman dataset on an SGI Reality Monster. The graphics capabilities of the Reality Monster are used only for display of the final color image.","Ray tracing,
Isosurfaces,
Rendering (computer graphics),
Hardware,
Computer science,
Computational geometry,
Computational efficiency,
Scalability,
Computer graphics,
Displays"
Statistical analysis of functional MRI data in the wavelet domain,"The use of the wavelet transform is explored for the detection of differences between brain functional magnetic resonance images (fMRIs) acquired under two different experimental conditions. The method benefits from the fact that a smooth and spatially localized signal can be represented by a small set of localized wavelet coefficients, while the power of white noise is uniformly spread throughout the wavelet space. Hence, a statistical procedure is developed that uses the imposed decomposition orthogonality to locate wavelet-space partitions with large signal-to-noise ratio (SNR), and subsequently restricts the testing for significant wavelet coefficients to these partitions. This results in a higher SNR and a smaller number of statistical tests, yielding a lower detection threshold compared to spatial-domain testing and, thus, a higher detection sensitivity without increasing type I errors. The multiresolution approach of the wavelet method is particularly suited to applications where the signal bandwidth and/or the characteristics of an imaging modality cannot be well specified. The proposed method was applied to compare two different fMRI acquisition modalities, Differences of the respective useful signal bandwidths could be clearly demonstrated; the estimated signal, due to the smoothness of the wavelet representation, yielded more compact regions of neuroactivity than standard spatial-domain testing.","Statistical analysis,
Magnetic resonance imaging,
Wavelet analysis,
Wavelet domain,
Testing,
Signal to noise ratio,
Wavelet coefficients,
Bandwidth,
Wavelet transforms,
Magnetic resonance"
Anatomy-based registration of CT-scan and intraoperative X-ray images for guiding a surgical robot,"Describes new methods for rigid registration of a preoperative computed tomography (CT)-scan image to a set of intraoperative X-ray fluoroscopic images, for guiding a surgical robot to its trajectory planned from CT. Our goal is to perform the registration, i.e., compute a rotation and translation of one data set with respect to the other to within a prescribed accuracy, based upon bony anatomy only, without external fiducial markers. With respect to previous approaches, the following aspects are new: (1) the authors correct the geometric distortion in fluoroscopic images and calibrate them directly with respect to the robot by affixing to it a new calibration device designed as a radiolucent rod with embedded metallic markers, and by moving the device along two planes, while radiographs are being acquired at regular intervals; (2) the registration uses an algorithm for computing the best transformation between a set of lines in three space, the (intraoperative) X-ray paths, and a set of points on the surface of the bone (imaged preoperatively), in a statistically robust fashion, using the Cayley parameterization of a rotation; and (3) to find corresponding sets of points to the X-ray paths on the surfaces, the authors' new approach consists of extracting the surface apparent contours for a given viewpoint, as a set of closed three-dimensional nonplanar curves, before registering the apparent contours to X-ray paths. Aside from algorithms, there are a number of major technical difficulties associated with engineering a clinically viable system using anatomy and image-based registration. To detect and solve them, the authors have so far conducted two experiments with the surgical robot in an operating room (OR), using CT and fluoroscopic image data of a cadaver bone, and attempting to faithfully simulate clinical conditions. Such experiments indicate that intraoperative X-ray-based registration is a promising alternative to marker-based registration for clinical use with the authors' proposed method.","X-ray imaging,
Medical robotics,
Computed tomography,
Anatomy,
Bones,
Robots,
Calibration,
Algorithm design and analysis,
Radiography,
Embedded computing"
Image guidance of breast cancer surgery using 3-D ultrasound images and augmented reality visualization,"This paper describes augmented reality visualization for the guidance of breast-conservative cancer surgery using ultrasonic images acquired in the operating room just before surgical resection. By combining an optical three-dimensional (3-D) position sensor, the position and orientation of each ultrasonic cross section are precisely measured to reconstruct geometrically accurate 3-D tumor models from the acquired ultrasonic images. Similarly, the 3-D position and orientation of a video camera are obtained to integrate video and ultrasonic images in a geometrically accurate manner. Superimposing the 3-D tumor models onto live video images of the patient's breast enables the surgeon to perceive the exact 3-D position of the tumor, including irregular cancer invasions which cannot be perceived by touch, as if it were visible through the breast skin. Using the resultant visualization, the surgeon can determine the region for surgical resection in a more objective and accurate manner, thereby minimizing the risk of a relapse and maximizing breast conservation. The system was shown to be effective in experiments using phantom and clinical data.","Breast cancer,
Oncological surgery,
Ultrasonic imaging,
Augmented reality,
Visualization,
Breast neoplasms,
Optical sensors,
Skin neoplasms,
Surges,
Biomedical optical imaging"
"WebOQL: restructuring documents, databases and Webs","The widespread use of the Web has originated several new data management problems, such as extracting data from Web pages and making databases accessible from Web browsers, and has renewed the interest in problems that had appeared before in other contexts, such as querying graphs, semistructured data and structured documents. Several systems and languages have been proposed for solving each of these Web data management problems, but none of these systems addresses all the problems from a unified perspective. Many of these problems essentially amount to data restructuring: we have information represented according to a certain structure and we want to construct another representation of (part of it) using a different structure. We present the WebOQL system, which supports a general class of data restructuring operations in the context of the Web. WebOQL synthesizes ideas from query languages for the Web, for semistructured data and for Website restructuring.","Data models,
Database languages,
Uniform resource locators,
Data mining,
Computer science,
File systems,
Navigation,
Tree graphs,
Encoding,
Manipulator dynamics"
Coupled B-snake grids and constrained thin-plate splines for analysis of 2-D tissue deformations from tagged MRI,"Magnetic resonance imaging (MRI) is unique in its ability to noninvasively and selectively alter tissue magnetization and create tagged patterns within a deforming body such as the heart muscle. The resulting patterns define a time-varying curvilinear coordinate system on the tissue, which the authors track with coupled B-snake grids. B-spline bases provide local control of shape, compact representation, and parametric continuity. Efficient spline warps are proposed which warp an area in the plane such that two embedded snake grids obtained from two tagged frames are brought into registration, interpolating a dense displacement vector field. The reconstructed vector field adheres to the known displacement information at the intersections, forces corresponding snakes to be warped into one another, and for all other points in the plane, where no information is available, a C/sup 1/ continuous vector field is interpolated. The implementation proposed in this paper improves on the authors' previous variational-based implementation and generalizes warp methods to include biologically relevant contiguous open curves, in addition to standard landmark points. The methods are validated with a cardiac motion simulator, in addition to in-vivo tagging data sets.","Magnetic resonance imaging,
Shape control,
Magnetic analysis,
Magnetization,
Heart,
Muscles,
Time varying systems,
Spline,
Image reconstruction,
Biological system modeling"
Multilevel Algorithms for Multi-Constraint Graph Partitioning,"Traditional graph partitioning algorithms compute a k-way partitioning of a graph such that the number of edges that are cut by the partitioning is minimized and each partition has an equal number of vertices. The task of minimizing the edge-cut can be considered as the objective and the requirement that the partitions will be of the same size can be considered as the constraint. In this paper we extend the partitioning problem by incorporating an arbitrary number of balancing constraints. In our formulation, a vector of weights is assigned to each vertex, and the goal is to produce a k-way partitioning such that the partitioning satisfies a balancing constraint associated with each weight, while attempting to minimize the edge-cut. Applications of this multi-constraint graph partitioning problem include parallel solution of multi-physics and multi-phase computations, that underlay many existing and emerging large-scale scientific simulations. We present new multi-constraint graph partitioning algorithms that are based on the multilevel graph partitioning paradigm. Our work focuses on developing new types of heuristics for coarsening, initial partitioning, and refinement that are capable of successfully handling multiple constraints. We experimentally evaluate the effectiveness of our multi-constraint partitioners on a variety of synthetically generated problems.","Partitioning algorithms,
Computational modeling,
Concurrent computing,
Computer science,
Cities and towns,
Large-scale systems,
Military computing,
Parallel processing,
Numerical simulation,
High performance computing"
Fuzzy clustering for symbolic data,"Most of the techniques used in the literature in clustering symbolic data are based on the hierarchical methodology, which utilizes the concept of agglomerative or divisive methods as the core of the algorithm. The main contribution of this paper is to show how to apply the concept of fuzziness on a data set of symbolic objects and how to use this concept in formulating the clustering problem of symbolic objects as a partitioning problem. Finally, a fuzzy symbolic c-means algorithm is introduced as an application of applying and testing the proposed algorithm on real and synthetic data sets. The results of the application of the new algorithm show that the new technique is quite efficient and, in many respects, superior to traditional methods of hierarchical nature.","Clustering algorithms,
Partitioning algorithms,
Fuzzy sets,
Testing,
Data analysis,
Computer science,
Data structures,
Position measurement,
Area measurement"
Practical solutions for QoS-based resource allocation problems,"The QoS based Resource Allocation Model (Q-RAM) proposed by R. Rajkumar et al. (1998) presented an analytical approach for satisfying multiple quality of service dimensions in a resource constrained environment. Using this model, available system resources can be apportioned across multiple applications such that the net utility that accrues to the end users of those applications is maximized. We present several practical solutions to allocation problems that were beyond the limited scope of Q-RAM. We show that the Q-RAM problem of finding the optimal resource allocation to satisfy multiple QoS dimensions is NP hard. We then present a polynomial solution for this resource allocation problem which yields a solution within a provably fixed and short distance from the optimal allocation. Secondly, Q-RAM dealt mainly with the problem of apportioning a single resource to satisfy multiple QoS dimensions. We study the converse problem of apportioning multiple resources to satisfy a single QoS dimension. In practice, this problem becomes complicated, since a single QoS dimension perceived by the user can be satisfied using different combinations of available resources. We show that this problem can be formulated as a mixed integer programming problem that can be solved efficiently to yield an optimal resource allocation. We also present the run times of these optimizations to illustrate how these solutions can be applied in practice. A good understanding of these solutions will yield insights into the general problem of apportioning multiple resources to satisfy simultaneously multiple QoS dimensions of multiple concurrent applications.","Resource management,
Radar tracking,
Read-write memory,
Electrical capacitance tomography,
Control systems,
Delay,
Computer science,
Statistics,
Operating systems,
Polynomials"
Real-time system for respiratory-cardiac gating in positron tomography,"A Macintosh-based signal processing system has been developed to support simultaneous respiratory and cardiac gating on the ECAT EXACT HR PET scanner. Using the Lab-View real-time software environment, the system reads analog inputs from a pneumatic respiratory bellows and an ECG monitor to compute an appropriate histogram memory location for the PET data. Respiratory state is determined by the bellows signal amplitude; cardiac state is based on the time since the last R-wave. These two states are used in a 2D lookup table to determine a combined respiratory-cardiac state. A 4-bit address encoding the selected histogram is directed from the system to the ECAT scanner, which dynamically switches the destination of tomograph events as respiratory-cardiac state changes. To test the switching efficiency of the combined Macintosh/ECAT system, a rotating emission phantom was built. Acquisitions with 25 msec states while the phantom was rotating at 240 rpm demonstrate the system could effectively stop motion at this rate, with approximately 5 msec switching time between states.","Real time systems,
Positron emission tomography,
Bellows,
Histograms,
Imaging phantoms,
Signal processing,
Software systems,
Electrocardiography,
Monitoring,
Analog computers"
Faster maximum and minimum mean cycle algorithms for system-performance analysis,"Maximum and minimum mean cycle problems are important problems with many applications in performance analysis of synchronous and asynchronous digital systems including rate analysis of embedded systems, in discrete-event systems, and in graph theory. Karp's algorithm is one of the fastest and most common algorithms for these problems. We present this paper mainly in the context of the maximum mean cycle problem. We show that Karp's algorithm processes more nodes and arcs than needed to find the maximum cycle mean of a digraph. This observation motivated us to propose a new graph-unfolding scheme that remedies this deficiency and leads to two faster algorithms with different characteristics. Theoretical analysis tells us that our algorithms always run faster than Karp's algorithm and that they are among the fastest to date. Experiments on small benchmark graphs confirm this fact for most of the graphs. These algorithms have been used in building a framework for analysis of timing constraints for embedded systems.","Algorithm design and analysis,
Performance analysis,
Discrete event systems,
Embedded system,
Fires,
Computer science,
Digital systems,
Graph theory,
Timing,
Engineering profession"
Ego-motion and omnidirectional cameras,"Recent research in image sensors has produced cameras with very large fields of view. An area of computer vision research which will benefit from this technology is the computation of camera motion (ego-motion) from a sequence of images. Traditional cameras stiffer from the problem that the direction of translation may lie outside of the field of view, making the computation of camera motion sensitive to noise. In this paper, we present a method for the recovery of ego-motion using omnidirectional cameras. Noting the relationship between spherical projection and wide-angle imaging devices, we propose mapping the image velocity vectors to a sphere, using the Jacobian of the transformation between the projection model of the camera and spherical projection. Once the velocity vectors are mapped to a sphere, we show how existing ego-motion algorithms can be applied and present some experimental results. These results demonstrate the ability to compute ego-motion with omnidirectional cameras.","Cameras,
Focusing,
Navigation,
Optical computing,
Optical sensors,
Image motion analysis,
Image sensors,
Computer vision,
Optical imaging,
Computer science"
"Comparative performance evaluation of routing protocols for mobile, ad hoc networks","We evaluate several routing protocols for mobile, wireless, ad hoc networks via packet level simulations. The protocol suite includes routing protocols specifically designed for ad hoc routing, as well as more traditional protocols, such as link state and distance vector used for dynamic networks. Performance is evaluated with respect to fraction of packets delivered, end-to-end delay and routing load for a given traffic and mobility model. It is observed that the new generation of on-demand routing protocols use a much lower routing load. However the traditional link state and distance vector protocols provide, in general, better packet delivery and delay performance.","Routing protocols,
Ad hoc networks,
Delay,
Mobile computing,
Computer science,
Network topology,
Intelligent networks,
Intelligent sensors,
Packet radio networks,
Computational modeling"
On the linear complexity of Legendre sequences,We determine the linear complexity of all Legendre sequences and the (monic) feedback polynomial of the shortest linear feedback shift register that generates such a Legendre sequence. The result shows that Legendre sequences are quite good from the linear complexity viewpoint.,"Polynomials,
Cryptography,
Computer science,
Galois fields,
Linear feedback shift registers,
Encyclopedias,
Mathematics,
Binary sequences"
Image segmentation using local variation,"We present a new graph-theoretic approach to the problem of image segmentation. Our method uses local criteria and yet produces results that reflect global properties of the image. We develop a framework that provides specific definitions of what it means for an image to be under- or over-segmented. We then present an efficient algorithm for computing a segmentation that is neither under- nor over-segmented according to these definitions. Our segmentation criterion is based on intensity differences between neighboring pixels. An important characteristic of the approach is that it is able to preserve detail in low-variability regions while ignoring detail in high-variability regions, which we illustrate with several examples on both real and synthetic images.","Image segmentation,
Pixel,
Computer science,
Computer vision,
Psychology,
Humans,
Visual perception,
Greedy algorithms,
Partitioning algorithms,
Statistics"
Traveling-wave tube devices with nonlinear dielectric elements,"The performance of traveling-wave tube (TWT) amplifiers incorporating nonlinear dielectric elements is studied via computer simulation. Two different situations are investigated: (1) the use of nonlinear dielectric elements to reduce intermodulation distortion and (2) the use of voltage-controlled dielectrics to provide rapid modification of the dispersion characteristics of the slow-wave structure. In the first case, the use of dielectrics with negative second-order susceptibilities is studied as a means of reducing phase and intermodulation distortion. Use of these dielectrics along with dynamic velocity taper to reduce amplitude modulation distortion results in marked reduction of predicted intermodulation distortion. In the second case, the goal is to design an amplifying structure whose gain as a function of frequency can be varied electrically. Preliminary design studies show that relatively large changes in the center frequency of the amplification band can be achieved with relatively modest changes in the dielectric constant of helix support structure.","Dielectric devices,
Nonlinear distortion,
Intermodulation distortion,
Frequency,
Predistortion,
Phase distortion,
Amplitude modulation,
Laboratories,
Signal processing,
Computer simulation"
TCP behavior of a busy Internet server: analysis and improvements,"We analyze the way in which Web browsers use TCP connections based on extensive traffic traces obtained from a busy Web server (the official Web server of the 1996 Atlanta Olympic games). At the time of operation, this Web server was one of the busiest on the Internet. We first describe the techniques used to gather these traces and reconstruct the behavior of the TCP on the server. We then present a detailed analysis of the TCP's loss recovery and congestion control behavior from the recorded transfers. Our two most important results are: (1) short Web transfers lead to poor loss recovery performance for TCPs, and (2) concurrent connections are overly aggressive users of the network. We then discuss techniques designed to solve these problems. To improve the data-driven loss recovery performance of short transfers, we present a new enhancement to the TCP's loss recovery. To improve the congestion control and loss recovery performance of parallel TCP connections, we present a new integrated approach to congestion control and loss recovery that works across the set of concurrent connections. Simulations and trace analysis show that our enhanced loss recovery scheme could have eliminated 25% of all timeout events, and that our integrated approach provides greater fairness and improved startup performance for concurrent connections.","Web server,
Telecommunication traffic,
Internet,
Performance loss,
Web sites,
Network servers,
Spine,
Transport protocols,
Computer science,
Application software"
Calibration of tracking systems in a surgical environment,"The purpose of this paper was to assess to what extent an optical tracking system (OTS) used for position determination in computer-aided surgery (CAS) can be enhanced by combining it with a direct current (DC) driven electromagnetic tracking system (EMTS). The main advantage of the EMTS is the fact that it is not dependent on a free line-of-sight. Unfortunately, the accuracy of the EMTS is highly affected by nearby ferromagnetic materials. The authors have explored to what extent the influence of the metallic equipment in the operating room (OR) can be compensated by collecting precise information on the nonlinear local error in the EMTS by using the OTS for setting up a calibration look-up table. After calibration of the EMTS and registration of the sensor systems in the OR the authors have found the average euclidean deviation in position readings between the DC tracker and the OTS reduced from 2.9/spl plusmn/1.0 mm to 2.1/spl plusmn/0.8 mm within a half-sphere of 530-mm radius around the magnetic field emitter. Furthermore the authors have found the calibration to be stable after re-registration of the sensors under varying conditions such as different heights of the OR table and varying positions of the OR equipment over a longer time interval. These results encourage the further development of a hybrid magnetooptical tracker for computer-aided surgery where the electromagnetic tracker acts as an auxiliary source of position information for the optical system. Strategies for enhancing the reliability of the proposed hybrid magnetooptic tracker by detecting artifacts induced by mobile ferromagnetic objects such as surgical tools are discussed.","Calibration,
Surgery,
Biomedical optical imaging,
Nonlinear optics,
Optical sensors,
Computer vision,
Optical computing,
Magnetic sensors,
Magnetooptic devices,
Content addressable storage"
Reconciling system requirements and runtime behavior,"This paper considers the problem of system deviations from requirements specifications. Such deviations may arise from lack of anticipation of possible behaviors of environment agents at specification time, or from evoking conditions in this environment. We discuss an architecture and a development process for monitoring system requirements at runtime to reconcile the requirements and the system's behavior. This process is deployed on three scenarios of requirements-execution reconciliation for the Meeting Scheduler system. The work builds on our previous work on goal-driven requirements engineering and on runtime requirements monitoring.","Runtime,
Robustness,
Condition monitoring,
Permission,
Feathers,
Computer science,
Engineering management,
Humans,
System analysis and design,
Time factors"
Tracking and segmenting people in varying lighting conditions using colour,"Colour cues were used to obtain robust detection and tracking of people in relatively unconstrained dynamic scenes. Gaussian mixture models were used to estimate probability densities of colour for skin, clothing and background. These models were used to detect, track and segment people, faces and hands. A technique for dynamically updating the models to accommodate changes in apparent colour due to varying lighting conditions was used. Two applications are highlighted: (1) actor segmentation for virtual studios, and (2) focus of attention for face and gesture recognition systems. A system implemented on a 200 MHz PC tracks multiple objects in real time.","Electrical capacitance tomography,
Histograms,
Machine vision,
Face recognition,
Face detection,
Laboratories,
Computer science,
Educational institutions,
Layout,
Clothing"
Efficient retrieval of similar time sequences under time warping,"Fast similarity searching in large time sequence databases has typically used Euclidean distance as a dissimilarity metric. However, for several applications, including matching of voice, audio and medical signals (e.g., electrocardiograms), one is required to permit local accelerations and decelerations in the rate of sequences, leading to a popular, field tested dissimilarity metric called the ""time warping"" distance. From the indexing viewpoint, this metric presents two major challenges: (a) it does not lead to any natural indexable ""features"", and (b) comparing two sequences requires time quadratic in the sequence length. To address each problem, we propose to use: (a) a modification of the so called ""FastMap"", to map sequences into points, with little compromise of ""recall"" (typically zero); and (b) a fast linear test, to help us discard quickly many of the false alarms that FastMap will typically introduce. Using both ideas in cascade, our proposed method achieved up to an order of magnitude speed-up over sequential scanning on both real and synthetic datasets.","Databases,
Indexing,
Pattern matching,
Speech recognition,
Discrete Fourier transforms,
Computer science,
Educational institutions,
Application software,
Acceleration,
Testing"
Supporting ranked Boolean similarity queries in MARS,"To address the emerging needs of applications that require access to and retrieval of multimedia objects, we are developing the Multimedia Analysis and Retrieval System (MARS). In this paper, we concentrate on the retrieval subsystem of MARS and its support for content-based queries over image databases. Content-based retrieval techniques have been extensively studied for textual documents in the area of automatic information retrieval. This paper describes how these techniques can be adapted for ranked retrieval over image databases. Specifically, we discuss the ranking and retrieval algorithms developed in MARS based on the Boolean retrieval model and describe the results of our experiments that demonstrate the effectiveness of the developed model for image retrieval.","Mars,
Information retrieval,
Image retrieval,
Content based retrieval,
Multimedia databases,
Image databases,
Feature extraction,
Multimedia systems,
Histograms,
Computer science"
Feature-adaptive motion tracking of ultrasound image sequences using a deformable mesh,"By exploiting the correlation of ultrasound speckle patterns that result from scattering by underlying tissue elements, two-dimensional tissue motion theoretically can be recovered by tracking the apparent movement of the associated speckle patterns. Speckle tracking, however, is an ill-posed inverse problem because of temporal decorrelation of the speckle patterns and the inherent low signal-to-noise ratio of medical ultrasonic images. This paper investigates the use of an adaptive deformable mesh for nonrigid tissue motion recovery from ultrasound images. The nodes connecting the mesh elements are allocated adaptively to stable speckle patterns that are less susceptible to temporal decorrelation. The authors use the approach of finite clement analysis in manipulating the irregular mesh elements. A novel deformable block matching algorithm, making use of a Lagrange element for higher-order description of local motion, is proposed to estimate a nonrigid motion vector at each node. In order to ensure that the motion estimates are admissible to a physically plausible solution, the nodal displacements are regularized by minimizing the strain energy associated with the mesh deformations. Experiments based on ultrasound images of a tissue mimicking phantom and a muscle undergoing contraction, and on computer simulations, have shown that the proposed algorithm can successfully track nonrigid displacement fields.","Tracking,
Ultrasonic imaging,
Image sequences,
Speckle,
Decorrelation,
Motion estimation,
Scattering,
Inverse problems,
Signal to noise ratio,
Biomedical imaging"
Efficient broadcasting protocols for video on demand,"Broadcasting protocols can improve the efficiency of video on demand services by reducing the bandwidth required to transmit videos that are simultaneously watched by many viewers. We present two broadcasting protocols that achieve nearly the same low bandwidth as the best extant broadcasting protocol while guaranteeing a lower maximum access time. Our first protocol, cautious harmonic broadcasting, requires somewhat more bandwidth than our second protocol, quasi-harmonic broadcasting, but is also much simpler to implement.","Multimedia communication,
Video on demand,
Access protocols,
Computer science,
Bandwidth,
Costs,
Casting,
TV broadcasting,
Cable TV,
Proposals"
Benchmarking the task graph scheduling algorithms,"The problem of scheduling a weighted directed acyclic graph (DAG) to a set of homogeneous processors to minimize the completion time has been extensively studied. The NP-completeness of the problem has instigated researchers to propose a myriad of heuristic algorithms. While these algorithms are individually reported to be efficient, it is not clear how effective they are and how well they compare against each other. A comprehensive performance evaluation and comparison of these algorithms entails addressing a number of difficult issues. One of the issues is that a large number of scheduling algorithms are based upon radically different assumptions, making their comparison on a unified basis a rather intricate task. Another issue is that there is no standard set of benchmarks that can be used to evaluate and compare these algorithms. Furthermore, most algorithms are evaluated using small problem sizes, and it is not clear how their performance scales with the problem size. The authors first provide a taxonomy for classifying various algorithms into different categories according to their assumptions and functionalities. They then propose a set of benchmarks which are of diverse structures without being biased towards a particular scheduling technique and still allow variations in important parameters. They have evaluated 15 scheduling algorithms, and compared them using the proposed benchmarks. Based upon the design philosophies and principles behind these algorithms, they interpret the results and discuss why some algorithms perform better than the others.","Scheduling algorithm,
Processor scheduling,
Parallel processing,
Concurrent computing,
Heuristic algorithms,
Laboratories,
Computer science,
Taxonomy,
Algorithm design and analysis,
Scalability"
Depth discontinuities by pixel-to-pixel stereo,"An algorithm to detect depth discontinuities from a stereo pair of images is presented. The algorithm matches individual pixels in corresponding scanline pairs while allowing occluded pixels to remain unmatched, then propagates the information between scanlines by means of a fast postprocessor. The algorithm handles large untextured regions, uses a measure of pixel dissimilarity that is insensitive to image sampling, and prunes bad search nodes to increase the speed of dynamic programming. The computation is relatively fast, taking about 1.5 microseconds per pixel per disparity on a workstation. Approximate disparity maps and precise depth discontinuities (along both horizontal and vertical boundaries) are shown for five stereo images containing textured, untextured, fronto-parallel, and slanted objects.","Pixel,
Image sampling,
Dynamic programming,
Workstations,
Velocity measurement,
Paints,
Humans,
Stereo vision,
Computer science,
Cost function"
T Spaces,"With the creation of computer networks in the 1970s came the birth of distributed network applications. Since then, there have been many applications that spanned multiple machines, but in the last 20 years no one created a serviceable network middleware package for developing highly effective distributed applications, that is, until now. This paper describes the design and architecture of T Spaces, a project at the IBM Almaden Research Center that fills the network middleware void. T Spaces embodies the three main characteristics of a useful mechanism for network programs, namely, data management, computation, and communication. Since it has the potential to connect any program to any other program on a computing network, T Spaces is an ideal platform on which to build a global computing services platform where any program or system service is available to any other program or service. In addition, its small footprint and Java™ implementation make T Spaces an ideal platform for writing distributed applications for embedded and palm-top computers, thus forging a needed gateway from the emerging embedded and palm-top computers to established desktop and server computers.",
NetSTAT: a network-based intrusion detection approach,"Network-based attacks have become common and sophisticated. For this reason, intrusion detection systems are now shifting their focus from the hosts and their operating systems to the network itself. Network-based intrusion detection is challenging because network auditing produces large amounts of data, and different events related to a single intrusion may be visible in different places on the network. This paper presents NetSTAT, a new approach to network intrusion detection. By using a formal model of both the network and the attacks, NetSTAT is able to determine which network events have to be monitored and where they can be monitored.","Intrusion detection,
Monitoring,
Operating systems,
Computer networks,
Computer network reliability,
Computer science,
Read only memory,
Programmable logic arrays,
Protection,
IP networks"
In-parameter-order: a test generation strategy for pairwise testing,"Pairwise testing (or 2-way testing) is a specification-based testing criterion, which requires that for each pair of input parameters of a system, every combination of valid values of these two parameters be covered by at least one test case. Empirical results show that pairwise testing is practical and effective for various types of software systems. We show that the problem of generating a minimum test set for pairwise testing is NP-complete. We propose a test generation strategy, called in-parameter-order (or IPO), for pairwise testing. For a system with two or more input parameters, the IPO strategy generates a pairwise test set for the first two parameters, extends the test set to generate a pairwise test set for the first three parameters, and continues to do so for each additional parameter. The IPO strategy allows the use of local optimization techniques for test generation and the reuse of existing tests when a system is extended with new parameters or new values of existing parameters. We present practical, IPO-based test generation algorithms. We describe the implementation of an IPO-based test generation tool and show some empirical results.","System testing,
Computer science,
Reactive power,
Independent component analysis,
Software testing,
Software systems"
Confidence estimation for speculation control,"Modern processors improve instruction level parallelism by speculation. The outcome of data and control decisions is predicted, and the operations are speculatively executed and only committed if the original predictions were correct. There are a number of other ways that processor resources could be used, such as threading or eager execution. As the use of speculation increases, we believe more processors will need some form of speculation control to balance the benefits of speculation against other possible activities. Confidence estimation is one technique that can be exploited by architects for speculation control. In this paper, we introduce performance metrics to compare confidence estimation mechanisms, and argue that these metrics are appropriate for speculation control. We compare a number of confidence estimation mechanisms, focusing on mechanisms that have a small implementation cost and gain benefit by exploiting characteristics of branch predictors, such as clustering of mispredicted branches. We compare the performance of the different confidence estimation methods using detailed pipeline simulations. Using these simulations, we show how to improve some confidence estimators, providing better insight for future investigations comparing and applying confidence estimators.","Testing,
Diseases,
Parallel processing,
Pipelines,
Computational modeling,
State estimation,
Terminology,
Computer science,
Measurement,
Computer simulation"
A layered approach to stereo reconstruction,"We propose a framework for extracting structure from stereo which represents the scene as a collection of approximately planar layers. Each layer consists of an explicit 3D plane equation, a colored image with per-pixel opacity (a sprite), and a per-pixel depth offset relative to the plane. Initial estimates of the layers are recovered using techniques taken from parametric motion estimation. These initial estimates are then refined using a re-synthesis algorithm which takes into account both occlusions and mixed pixels. Reasoning about such effects allows the recovery of depth and color information with high accuracy even in partially occluded regions. Another important benefit of our framework is that the output consists of a collection of approximately planar regions, a representation which is far more appropriate than a dense depth map for many applications such as rendering and video parsing.","Layout,
Image reconstruction,
Pixel,
Motion estimation,
Cameras,
Stereo image processing,
Computer science,
Read only memory,
Equations,
Sprites (computer)"
Utilization and predictability in scheduling the IBM SP2 with backfilling,"Scheduling jobs on the IBM SP2 system is usually done by giving each job a partition of the machine for its exclusive use. Allocating such partitions in the order that the jobs arrive (FCFS scheduling) is fair and predictable, but suffers from severe fragmentation, leading to low utilization. An alternative is to use the EASY scheduler, which uses aggressive backfilling: small jobs are moved ahead to fill in holes in the schedule, provided they do not delay the first job in the queue. The authors show that a more conservative approach, in which small jobs move ahead only if they do not delay any job in the queue, produces essentially the same benefits in terms of utilization. The conservative scheme has the added advantage that queueing times can be predicted in advance, whereas in EASY the queueing time is unbounded.","Processor scheduling,
Delay,
Supercomputers,
Runtime,
Partitioning algorithms,
Computer science,
Dynamic scheduling,
Production,
Writing,
Uniform resource locators"
Multiple-prototype classifier design,"Five methods that generate multiple prototypes from labeled data are reviewed. Then we introduce a new sixth approach, which is a modification of Chang's (1974) method. We compare the six methods with two standard classifier designs: the 1-nearest prototype (1-np) and 1-nearest neighbor (1-nn) rules. The standard of comparison is the resubstitution error rate; the data used are the Iris data. Our modified Chang's method produces the best consistent (zero-error) design. One of the competitive learning models produces the best minimal prototypes design (five prototypes that yield three resubstitution errors).","Prototypes,
Error analysis,
Testing,
Iris,
Nearest neighbor searches,
Pattern recognition,
Hypercubes,
Marine vehicles,
Stock markets,
Computer science"
Statistical rate monotonic scheduling,"Statistical rate monotonic scheduling (SRMS) is a generalization of the classical RMS results of C. Liu and J. Layland (1973) for periodic tasks with highly variable execution times and statistical QoS requirements. The main tenet of SRMS is that the variability in task resource requirements could be smoothed through aggregation to yield guaranteed QoS. This aggregation is done over time for a given task and across multiple tasks for a given period of time. Similar to RMS, SRMS has two components: a feasibility test and a scheduling algorithm. SRMS feasibility test ensures that it is possible for a given periodic task set to share a given resource without violating any of the statistical QoS constraints imposed on each task in the set. The SRMS scheduling algorithm consists of two parts: a job admission controller and a scheduler. The SRMS scheduler is a simple, preemptive, fixed priority scheduler. The SRMS job admission controller manages the QoS delivered to the various tasks through admit/reject and priority assignment decisions. In particular it ensures the important property of task isolation, whereby tasks do not infringe on each other.","Testing,
Scheduling algorithm,
Statistics,
Processor scheduling,
Internet,
Computer science,
Reactive power,
Statistical analysis,
Resource management,
Real time systems"
The energy complexity of register files,"Register files (RF) represent a substantial portion of the energy budget in modern processors, and are growing rapidly with the trend towards wider instruction issue. The actual access energy costs depend greatly on the register file circuitry used. This paper compares various RF circuitry techniques for their energy efficiencies, as a function of architectural parameters such as the number of registers and the number of ports. The port priority selection technique was found to be the most energy efficient. The dependence of register file access energy upon technology scaling is also studied. However, as this paper shows, it appears that none of these will be enough to prevent centralized register files from becoming the dominant power component of next-generation superscalar computers, and alternative methods for inter-instruction communication need to be developed. Split register file architecture is analyzed as a possible alternative.","Registers,
Radio frequency,
Logic,
Circuits,
Power system modeling,
Permission,
Buffer storage,
Computer science,
Modems,
Costs"
Face pose discrimination using support vector machines (SVM),"This paper describes an approach for the problem of face pose discrimination using support vector machines (SVM). Face pose discrimination means that one can label the face image as one of several known poses. Face images are drawn from the standard FERET database. The training set consists of 150 images equally distributed among frontal, approximately 33.75/spl deg/ rotated left and right poses, respectively, and the test set consists of 450 images again equally distributed among the three different types of poses. SVM achieved perfect accuracy-100%-discriminating between the three possible face poses on unseen test data, using either polynomials of degree 3 or radial basis functions (RBF) as kernel approximation functions.","Support vector machines,
Face detection,
Kernel,
Face recognition,
Testing,
Polynomials,
Computer science,
Pattern analysis,
Layout,
Lighting"
Towards flexible teamwork in persistent teams,"Teamwork is a critical capability in multi-agent environments. Many such environments mandate that the agents and agent-teams mast be persistent i.e., exist over long periods of time. Agents in such persistent teams are bound together by their long-term common interests and goals. This paper focuses on flexible teamwork in such persistent teams. Unfortunately, while previous work has investigated flexible teamwork, persistent teams remain unexplored. For flexible teamwork, one promising approach that has emerged is model-based, i.e., providing agents with general models of teamwork that explicitly specify their commitments in teamwork. Such models enable agents to autonomously reason about coordination. Unfortunately, for persistent teams, such models may lead to coordination and communication actions that while locally optimal, are highly problematic for the team's long-term goals. We present a decision-theoretic technique to enable persistent teams to overcome such limitations of the model-based approach. In particular, agents reason about expected team utilities of future team states that are projected to result from actions recommended by the teamwork model, as well as lower-cost (or higher-cost) variations on these actions. To accommodate real-time constraints, this reasoning is done in an any-time fashion. Implemented examples from an analytic search tree and some real-world domains are presented.","Teamwork,
Robot kinematics,
Orbital robotics,
Space exploration,
Reconnaissance,
Resource management,
Computer science,
Educational robots,
Internet,
Virtual environment"
Surface-based registration of CT images to physical space for image-guided surgery of the spine: a sensitivity study,"This paper presents a method designed to register preoperative computed tomography (CT) images to vertebral surface points acquired intraoperatively from ultrasound (US) images or via a tracked probe. It also presents a comparison of the registration accuracy achievable with surface points acquired from the entire posterior surface of the vertebra to the accuracy achievable with points acquired only from the spinous process and central laminar regions. Using a marker-based method as a reference, this work shows that submillimetric registration accuracy can be obtained even when a small portion of the posterior vertebral surface is used for registration. It also shows that when selected surface patches are used, CT slice thickness is not a critical parameter in the registration process. Furthermore, the paper includes qualitative results of registering vertebral surface points in US images to multiple CT slices. The method has been tested with US points and physical points on a plastic spine phantom and with simulated data on a patient CT scan.","Computed tomography,
Surgery,
Fasteners,
Biomedical engineering,
Spine,
Anatomy,
Design methodology,
Registers,
Ultrasonic imaging,
Probes"
Exploring large graphs in 3D hyperbolic space,"Drawing graphs as nodes connected by links is visually compelling but computationally difficult. Hyperbolic space and spanning trees can reduce visual clutter, speed up layout, and provide fluid interaction. This article briefly describes a software system that explicitly attempts to handle much larger graphs than previous systems and support dynamic exploration rather than final presentation. It then discusses the applicability of this system to goals beyond simple exploration. A software system that supports graph exploration should include both a layout and an interactive drawing component. I have developed new algorithms for both layout and drawing (H3 and H3Viewer). The H3Viewer drawing algorithm remains under development, so this article presents preliminary results. I have implemented a software library that uses these algorithms. It can handle graphs of more than 100,000 edges by using a spanning tree as the backbone for the layout and drawing algorithms.","Tree graphs,
Visualization,
Cognitive science,
Spine,
Libraries,
Uniform resource locators,
Runtime,
Bipartite graph,
Robustness,
Layout"
Active Pages: a computation model for intelligent memory,"Microprocessors and memory systems suffer from a growing gap in performance. We introduce Active Pages, a computation model which addresses this gap by shifting data-intensive computations to the memory system. An Active Page consists of a page of data and a set of associated functions which can operate upon that data. We describe an implementation of Active Pages on RXDram (Reconfigurable Architecture DRAM), a memory system based upon the integration of DRAM and reconfigurable logic. Results from the SimpleScalar simulator demonstrate up to 1000X speedups on several applications using the RADram system versus conventional memory systems. We also explore the sensitivity of our results to implementations in other memory technologies.","Computational modeling,
Logic,
Microprocessors,
Computer architecture,
Random access memory,
Fabrication,
Integrated circuit technology,
Computer applications,
Programming profession,
Computer science"
Decision trees can initialize radial-basis function networks,"Successful implementations of radial-basis function (RBF) networks for classification tasks must deal with architectural issues, the burden of irrelevant attributes, scaling, and some other problems. This paper addresses these issues by initializing RBF networks with decision trees that define relatively pure regions in the instance space; each of these regions then determines one basis function. The resulting network is compact, easy to induce, and has favorable classification accuracy.","Decision trees,
Neurons,
Radial basis function networks,
Neural networks,
Pattern recognition,
Learning systems,
Concrete,
Transforms,
Equations,
Computer science"
Video scene segmentation via continuous video coherence,"In extended video sequences, individual frames are grouped into shots which are defined as a sequence taken by a single camera, and related shots are grouped into scenes which are defined as a single dramatic event taken by a small number of related cameras. This hierarchical structure is deliberately constructed, dictated by the limitations and preferences of the human visual and memory systems. We present three novel high-level segmentation results derived from these considerations, some of which are analogous to those involved in the perception of the structure of music. First and primarily, we derive and demonstrate a method for measuring probable scene boundaries, by calculating a short term memory-based model of shot-to-shot ""coherence"". The detection of local minima in this continuous measure permits robust and flexible segmentation of the video into scenes, without the necessity for first aggregating shots into clusters. Second, and independently of the first, we then derive and demonstrate a one-pass on-the-fly shot clustering algorithm. Third, we demonstrate partially successful results on the application of these two new methods to the next higher, ""theme"", level of video structure.","Layout,
Cameras,
Humans,
Gunshot detection systems,
Ear,
Computer science,
Laboratories,
Educational institutions,
Robustness,
Clustering algorithms"
Scheduling for large-scale on-demand data broadcasting,"Advances in telecommunications have enabled the deployment of broadcast-based wide-area information services that provide on-demand data access to very large client populations. In order to effectively utilize a broadcast medium for such a service, it is necessary to have efficient, on-line scheduling algorithms that can balance individual and overall performance and can scale in terms of data set sizes, client populations, and broadcast bandwidth. In this study we introduce a parameterized algorithm that provides good performance across all of these criteria and can be tuned to emphasize either average or worst case waiting time. Unlike previous work on low overhead scheduling, the algorithm is not based on estimates of the access probabilities of items, but rather, it makes scheduling decisions based on the current queue state, allowing it to easily adapt to changes in the intensity and distribution of the workload. We examine the performance of the algorithm using a simulation model.","Large-scale systems,
Satellite broadcasting,
TV broadcasting,
Scheduling algorithm,
Bandwidth,
Unicast,
Delay,
Databases,
Processor scheduling,
Computer science"
Gender and ethnic classification of face images,"The paper considers hybrid classification architectures for gender and ethnic classification of human faces and shows their feasibility using a collection of 3006 face images corresponding to 1009 subjects from the FERET database. The hybrid approach consists of an ensemble of RBF networks and inductive decision trees (DT). Experimental cross validation (CV) results yield on average accuracy rate of (a) 96% on the gender classification task and (b) 94% on the ethnic classification task. The benefits of the hybrid architecture include (i) robustness via query by consensus provided by the ensembles of RBF networks, and (ii) flexible and adaptive thresholds as opposed to ad hoc and hard thresholds provided by using only DT.","Face,
Humans,
Image databases,
Psychology,
Testing,
Computer science,
Computer architecture,
Decision trees,
Robustness,
Radial basis function networks"
Verification and validation of simulation models,"This paper discusses verification and validation of simulation models. The different approaches to deciding model validity are presented; how model verification and validation relate to the model development process are discussed; various validation techniques are defined; conceptual model validity, model verification, operational validity, and data validity are described; ways to document results are given; and a recommended procedure is presented.","Accreditation,
Computational modeling,
Random variables,
Cost function,
Computer simulation,
Educational institutions,
Computer science,
Problem-solving,
Decision making,
Application software"
Improved low-density parity-check codes using irregular graphs and belief propagation,"We construct new families of low-density parity-check codes, which we call irregular codes. When decoded using belief propagation, our codes can correct more errors than previously known low-density codes. Our improved performance comes from using codes based on irregular random bipartite graphs, based on the work of Luby et al. (1997). Previously studied low-density codes have been derived from regular bipartite graphs. Initial experimental results for our irregular codes suggest that, with improvements, irregular codes may be able to match turbo code performance.","Parity check codes,
Belief propagation,
Digital systems,
Computer errors,
Bipartite graph,
Turbo codes,
Decoding,
Terminology,
Error correction,
Computer science"
A class of end-to-end congestion control algorithms for the Internet,"We formulate end-to-end congestion control as a global optimization problem. Based on this formulation, a class of minimum cost flow control (MCFC) algorithms for adjusting session rates or window sizes is proposed. Significantly, we show that these algorithms can be implemented at the transport layer of an IP network and can provide certain fairness properties and user priority options without requiring non-FIFO switches. Two algorithm versions are discussed. A coarse version is geared towards implementation in the current Internet, relying on the end-to-end packet loss observations as an indication of congestion. A more complete version anticipates an Internet where sessions can solicit explicit congestion information through a concise probing mechanism. We show that TCP congestion control, after some modification, may be treated as a special case of the MCFC algorithms.","Traffic control,
Web and internet services,
IP networks,
Communication system traffic control,
Switches,
Computer science,
Costs,
Gears,
Observability,
Controllability"
Global state routing: a new routing scheme for ad-hoc wireless networks,"In an ad-hoc environment with no wired communication infrastructure, it is necessary that mobile hosts operate as routers in order to maintain the information about connectivity. However with the presence of high mobility and low signal/interference ratio (SIR), traditional routing schemes for wired networks are not appropriate, as they either lack the ability to quickly reflect the changing topology, or may cause excessive overhead, which degrades network performance. Considering these restrictions, we propose a new scheme especially designed for routing in an ad-hoc wireless environments. We call this scheme ""global state routing"" (GSR), where nodes exchange vectors of link states among their neighbors during routing information exchange. Based on the link state vectors, nodes maintain a global knowledge of the network topology and optimize their routing decisions locally. The performance of the algorithm, studied in this paper through a series of simulations, reveals that this scheme provides a better solution than existing approaches in a truly mobile, ad-hoc environment.","Wireless networks,
Network topology,
Floods,
Bandwidth,
Routing protocols,
Convergence,
Computer science,
Mobile communication,
Mobile computing,
Degradation"
Autocorrelation values of generalized cyclotomic sequences of order two,The generalized cyclotomic sequence of order two has several good randomness properties and behaves like the Legendre sequence in several aspects. We calculate the autocorrelation values of the generalized cyclotomic sequence of order two. Our result shows that this sequence could have very good autocorrelation property and pattern distributions of length two if the two primes are chosen properly.,"Autocorrelation,
Application software,
Random sequences,
Software testing,
Radar applications,
Spread spectrum radar,
Communication systems,
Information systems,
Computer science"
Robust parameter estimation of intensity distributions for brain magnetic resonance images,"Presents two new methods for robust parameter estimation of mixtures in the context of magnetic resonance (MR) data segmentation. The head is constituted of different types of tissue that can be modeled by a finite mixture of multivariate Gaussian distributions. The authors' goal is to estimate accurately the statistics of desired tissues in presence of other ones of lesser interest. These latter can be considered as outliers and can severly bias the estimates of the former. For this purpose, the authors introduce a first method, which is an extension of the expectation-maximization (EM) algorithm, that estimates parameters of Gaussian mixtures but incorporates an outlier rejection scheme which allows to compute the properties of the desired tissues in presence of atypical data. The second method is based on genetic algorithms and is well suited for estimating the parameters of mixtures of different kind of distributions. The authors use this property by adding a uniform distribution to the Gaussian mixture for modeling the outliers. The proposed genetic algorithm can efficiently estimate the parameters of this extended mixture for various initial settings. Also, by changing the minimization criterion, estimates of the parameters can be obtained by histogram fitting which considerably reduces the computational cost. Experiments on synthetic and real MR data show that accurate estimates of the gray and white matters parameters are computed.","Robustness,
Parameter estimation,
Magnetic resonance,
Genetic algorithms,
Image segmentation,
Magnetic heads,
Gaussian distribution,
Statistical distributions,
Histograms,
Computational efficiency"
GI tract unraveling with curved cross sections,"Gastrointestinal (GI) tract examination with spiral/helical computed tomography (CT) is currently performed by slice-based inspection of axial images. CT colography is a recent advance which allows an intraluminal visualization of the colon, similar to endoscopy. Various rendering algorithms have been developed with promising results, however navigation through the complex, tortuous anatomy of the colon can be time consuming in practice. In this paper, the authors propose an electrical-field-based method to unravel the convoluted colon, that is, to digitally straighten it with curved cross sections and flatten it over a plane. In the authors' method, electrical charges are simulated along the central colon path. Curved cross sections are defined by the electrical force lines, and lead to consistent unraveling. It is demonstrated with image volumes of two patients that this technique produces a global planar view of complicated colon features with a potential for detection of polyps.","Gastrointestinal tract,
Colon,
Computed tomography,
Colonography,
Spirals,
Inspection,
Visualization,
Endoscopes,
Rendering (computer graphics),
Navigation"
Darwin: customizable resource management for value-added network services,"The Internet is rapidly changing from a set of wires and switches that carry packets into a sophisticated infrastructure that delivers a set of complex value-added services to end users. Services can range from bit transport all the way up to distributed value-added services like video teleconferencing, data mining, and distributed interactive simulations. Before such services can be supported in a general and dynamic manner we have to develop appropriate resource management mechanisms. These resource management mechanisms must make it possible to identify and allocate resources that meet service or application requirements, support both isolation and controlled dynamic sharing of resources across organizations sharing physical resources, and be customizable so services and applications can tailor resource usage to optimize their performance. The Darwin project is developing a set of customizable resource management mechanisms that support value-added services, In this paper we present these mechanisms, describe their implementation in a prototype system, and describe the results of a series of proof-of-concept experiments.","Resource management,
Computer networks,
Web and internet services,
Read only memory,
Electrical capacitance tomography,
Identity-based encryption,
Computer science,
Tellurium,
Ores,
Tail"
Modular domain specific languages and tools,"A domain specific language (DSL) allows one to develop software for a particular application domain quickly and effectively, yielding programs that are easy to understand, reason about, and maintain. On the other hand, there may be a significant overhead in creating the infrastructure needed to support a DSL. To solve this problem, a methodology is described for building domain specific embedded languages (DSELs), in which a DSL is designed within an existing, higher-order and typed, programming language such as Haskell or ML. In addition, techniques are described for building modular interpreters and tools for DSELs. The resulting methodology facilitates reuse of syntax semantics, implementation code, software tools, as well as look-and-feel.","Domain specific languages,
DSL,
Application software,
Computer languages,
Costs,
Programming,
Buildings,
Hardware,
Computer science,
Software maintenance"
Integrating global position estimation and position tracking for mobile robots: the dynamic Markov localization approach,"Localization is one of the fundamental problems of mobile robots. In order to efficiently perform useful tasks such as office delivery, mobile robots must know their position in their environment. Existing approaches can be distinguished according to the type of localization problem they are designed to solve. Tracking techniques aim at monitoring the robot's position. They assume that the position is initially known and cannot recover from situations in which they lost track of the robot's position. Global localization techniques on the other hand, are able to estimate the robot's position under complete uncertainty. We present the dynamic Markov localization technique as a uniform approach to position estimation, which is able (1) to globally estimate the position of the robot, (2) to efficiently track its position whenever the robot's certainty is high, and (3) to detect and recover from localization failures. The approach has been implemented and intensively tested in real-world environments. We present several experiments illustrating the strength of our method.","Mobile robots,
Orbital robotics,
State-space methods,
Uncertainty,
State estimation,
Computer science,
Testing,
Robustness,
Condition monitoring,
Humans"
A fault detection service for wide area distributed computations,"The potential for faults in distributed computing systems is a significant complicating factor for application developers. While a variety of techniques exist for detecting and correcting faults, the implementation of these techniques in a particular context can be difficult. Hence, we propose a fault detection service designed to be incorporated, in a modular fashion, into distributed computing systems, tools, or applications. This service uses well-known techniques based on unreliable fault detectors to detect and report component failure, while allowing the user to tradeoff timeliness of reporting against false positive rates. We describe the architecture of this service, report on experimental results that quantify its cost and accuracy, and describe its use in two applications, monitoring the status of system components of the GUSTO computational grid testbed and as part of the NetSolve network-enabled numerical solver.","Fault detection,
Distributed computing,
Grid computing,
Computer networks,
Resource management,
Application software,
Costs,
Mathematics,
Computer science,
Laboratories"
Registration of real and CT-derived virtual bronchoscopic images to assist transbronchial biopsy,"This paper describes research work motivated by an innovative medical application: computer-assisted transbronchial biopsy. This project involves the registration, with no external localization device, of a preoperative three-dimensional (3-D) computed tomography (CT) scan of the thoracic cavity (showing a tumor that requires a needle biopsy), and an intraoperative endoscopic two-dimensional (2-D) image sequence, in order to provide assistance in transbronchial puncture of the tumor. Because of the specific difficulties resulting from the data being processed, a multilevel strategy was introduced. For each analysis level, the relevant information to process and the corresponding algorithms were defined. This multilevel strategy, thus, provides the best possible accuracy. Original image processing methods were elaborated, dealing with segmentation, registration and 3-D reconstruction of the bronchoscopic images. In particular, these methods involve adapted mathematical morphology tools, a ""daemon-based"" registration algorithm, and a model-based shape-from-shading algorithm. This pilot study presents the application of these algorithms to recorded bronchoscopic video sequences for five patients. The preliminary results presented here demonstrate that it is possible to precisely localize the endoscopic camera within the CT data coordinate system. The computer can thus synthesize in near real-time the CT-derived virtual view that corresponds to the actual endoscopic view.","Biopsy,
Computed tomography,
Neoplasms,
Medical services,
Biomedical equipment,
Computer applications,
Needles,
Two dimensional displays,
Image sequences,
Information analysis"
A Grid-Enabled MPI: Message Passing in Heterogeneous Distributed Computing Systems,"Application development for high-performance distributed computing systems, or computational grids as they are sometimes called, requires ``grid-enabled'' tools that hide mundane aspects of the heterogeneous grid environment without compromising performance. As part of an investigation of these issues, we have developed MPICH-G, a grid-enabled implementation of the Message Passing Interface (MPI) that allows a user to run MPI programs across multiple computers at different sites using the same commands that would be used on a parallel computer. This library extends the Argonne MPICH implementation of MPI to use services provided by the Globus grid toolkit. In this paper, we describe the MPICH-G implementation and present preliminary performance results.","Message passing,
Distributed computing,
Grid computing,
Computer interfaces,
High performance computing,
Concurrent computing,
Computer networks,
Computer science,
Application software,
Libraries"
A prescription for the multilevel Helmholtz FMM,"The authors describe a multilevel Helmholtz FMM (fast multipole method) as a way to compute the field caused by a collection of source points at an arbitrary set of field points. Their description focuses on the algorithm's mathematical basics, so that it can be applied to a variety of applications.","Electromagnetic scattering,
Integral equations,
Acoustic scattering,
Electromagnetic fields,
Impedance,
Surface waves,
Distributed computing,
Filtering,
Power harmonic filters"
Representing and querying changes in semistructured data,"Semistructured data may be irregular and incomplete and does not necessarily conform to a fixed schema. As with structured data, it is often desirable to maintain a history of changes to data, and to query over both the data and the changes. Representing and querying changes in semistructured data is more difficult than in structured data due to the irregularity and lack of schema. We present a model for representing changes in semistructured data and a language for querying over these changes. An important feature of our approach is that we represent and query changes directly as annotations on the affected data, instead of indirectly as the difference between database states. We describe the implementation of our model and query language. We also describe the design and implementation of a query subscription service that permits users to subscribe to changes in semistructured information sources.","Identity-based encryption,
Web pages,
Computer science,
Subscriptions,
National electric code,
HTML,
Database languages,
Electrical capacitance tomography,
Plasma welding,
Laboratories"
Automated atlas integration and interactive three-dimensional visualization tools for planning and guidance in functional neurosurgery,"Many critical functionally distinct subcortical structures are not distinguishable on anatomical magnetic resonance imaging (MRI) scans. In order to provide the neurosurgeon with this missing information, a deformable volumetric atlas of the basal ganglia and thalamus has been created from the Schaltenbrand and Wahren atlas of cryogenic slices. The volumetric atlas can be automatically deformed to an individual patient's MRI. To facilitate the clinical use of the atlas, a visualization platform has been developed for preoperative and intraoperative use which permits manipulation of the merged atlas and MRI data sets in two- and three-dimensional views. The platform includes graphical tools which allow the visualization of projections of a leukotome and other surgical tools with respect to the atlas data, as well as preregistered images from any other imaging modality. In addition, a graphical interface has been designed to create custom virtual lesions using computer models of neurosurgical tools for intraoperative planning. To date this system has been employed as an adjunct to over 30 functional neurosurgical cases including surgery for movement disorders.","Visualization,
Neurosurgery,
Brain,
Magnetic resonance imaging,
Surgery,
Parkinson's disease,
Hospitals,
Lesions,
Nervous system,
Basal ganglia"
Two nonparametric models for fusing heterogeneous fuzzy data,"Two models are discussed that integrate heterogeneous fuzzy data of three types: real numbers, real intervals, and real fuzzy sets. The architecture comprises three modules: 1) an encoder that converts the mixed data into a uniform internal representation; 2) a numerical processing core that uses the internal representation to solve a specified task; and 3) a decoder that transforms the internal representation back to an interpretable output format. The core used in this study is fuzzy clustering, but there are many other operations that are facilitated by the models. Two schemes for encoding the data and decoding it after clustering are presented. One method uses possibility and necessity measures for encoding and several variants of a center of gravity defuzzification method for decoding. The second approach uses piecewise linear splines to encode the data and decode the clustering results. Both procedures are illustrated using two small sets of heterogeneous fuzzy data.","Fuzzy sets,
Decoding,
Encoding,
Piecewise linear techniques,
Computer science,
Gravity,
Pattern recognition,
Road vehicles,
Road transportation,
Velocity measurement"
Edge detection in medical images using a genetic algorithm,"An algorithm is developed that detects well-localized, unfragmented, thin edges in medical images based on optimization of edge configurations using a genetic algorithm (GA). Several enhancements were added to improve the performance of the algorithm over a traditional GA. The edge map is split into connected subregions to reduce the solution space and simplify the problem. The edge-map is then optimized in parallel using incorporated genetic operators that perform transforms on edge structures. Adaptation is used to control operator probabilities based on their participation. The GA was compared to the simulated annealing (SA) approach using ideal and actual medical images from different modalities including magnetic resonance imaging (MRI), computed tomography (CT), and ultrasound. Quantitative comparisons were provided based on the Pratt figure of merit and on the cost-function minimization. The detected edges were thin, continuous, and well localized. Most of the basic edge features were detected, Results for different medical image modalities are promising and encourage further investigation to improve the accuracy and experiment with different cost functions and genetic operators.","Image edge detection,
Biomedical imaging,
Genetic algorithms,
Magnetic resonance imaging,
Computed tomography,
Computational modeling,
Medical simulation,
Simulated annealing,
Ultrasonic imaging,
Computer vision"
Finding all solutions of piecewise-linear resistive circuits using linear programming,"An efficient algorithm is proposed for finding all solutions of piecewise-linear resistive circuits. This algorithm is based on a new test for nonexistence of a solution to a system of piecewise-linear equations f/sub i/(x)=0(i=1.2,/spl middot//spl middot//spl middot/,n) in a super-region. Unlike the conventional sign test, which checks whether the solution surfaces of the single piecewise-linear equations exist or not in a super-region, the new test checks whether they intersect or not in the super-region. Such a test can be performed by using linear programming. It is shown that the simplex method can be performed very efficiently by exploiting the adjacency of super-regions in each step. The proposed algorithm is much more efficient than the conventional sign test algorithms and can find all solutions of large scale circuits very efficiently. Moreover, it can find all characteristic curves of piecewise-linear resistive circuits.","Piecewise linear techniques,
Linear programming,
Circuit testing,
Equations,
Large-scale systems,
Resistors,
Performance evaluation,
Computer science,
Vectors,
System testing"
Dextrous manipulation by rolling and finger gaiting,"Many practical dextrous manipulation tasks involve large-scale motion of the grasped object while maintaining a stable grasp. To plan such task, one must control both the motion of the object and the contact locations, while also adhering to the workspace constraints typical of multi-fingered hands. We integrate the relevant theories of contact kinematics, nonholonomic motion planning, coordinated object manipulation, grasp stability and finger gaits to develop a general framework for dextrous manipulation planning. To illustrate our approach, the framework is applied to the problem of manipulating a sphere with three hemi-spherical fingertips. The simulation results are presented.","Fingers,
Large-scale systems,
Robot kinematics,
Computer science,
Motion control,
Strain control,
Stability,
Planing,
Heart,
Computational geometry"
Use of sequencing constraints for specification-based testing of concurrent programs,"This paper presents and evaluates a specification-based methodology for testing concurrent programs. This methodology requires sequencing constraints, which specify restrictions on the allowed sequences of synchronization events. Sequencing constraints for a concurrent program can be derived from the program's formal or informal specification. Details of the proposed testing methodology based on the use of Constraints on Succeeding and Preceding Events (CSPE) are given. How to achieve coverage and detect violations of CSPE constraints for a concurrent program, according to deterministic and nondeterministic testing of this program, are described. A coverage criterion for CSPE-based testing is defined and analyzed. The results of empirical studies of CSPE-based testing for four concurrent problems are reported. These results indicate that the use of sequencing constraints for specification-based testing of concurrent programs is a promising approach.","Software testing,
Automata,
Explosions,
Computer science,
Sequential analysis,
Fault detection,
Protocols,
Event detection"
Validation of the coupling dependency metric as a predictor of run-time failures and maintenance measures,The coupling dependency metric (CDM) is a successful design quality metric. Here we apply it to four case studies: run-time failure data for a COBOL registration system; maintenance data for a C text-processing utility; maintenance data for a C++ patient collaborative care system; and maintenance data for a Java electronic file transfer facility. CDM outperformed a wide variety of competing metrics in predicting run-time failures and a number of different maintenance measures. These results imply that coupling metrics may be good predictors of levels of interaction within a software product.,"Runtime,
Position measurement,
Collaboration,
Computer science,
Java,
Product design,
Testing,
Size measurement,
Network-on-a-chip"
Experimental assessment of workstation failures and their impact on checkpointing systems,"In the past twenty years, there has been a wealth of theoretical research on minimizing the expected running time of a program in the presence of failures by employing checkpointing and rollback recovery. In the same time period, there has been little experimental research to corroborate these results. We study three separate projects that monitor failure in workstation networks. Our goals are twofold. The first is to see how these results correlate with the theoretical results, and the second is to assess their impact on strategies for checkpointing long-running computations on workstations and networks of workstations. A significant result of our work is that although the base assumptions of the theoretical research do not hold, many of the results are still applicable.","Workstations,
Checkpointing,
Computer networks,
Condition monitoring,
Failure analysis,
Probability distribution,
Equations,
Computer science,
Supercomputers,
Utility programs"
Exploiting spatial locality in data caches using spatial footprints,"Modern cache designs exploit spatial locality by fetching large blocks of data called cache lines on a cache miss. Subsequent references to words within the same cache line result in cache hits. Although this approach benefits from spatial locality, less than half of the data brought into the cache gets used before eviction. The unused portion of the cache line negatively impacts performance by wasting bandwidth and polluting the cache by replacing potentially useful data that would otherwise remain in the cache. This paper describes an alternative approach to exploit spatial locality available in data caches. On a cache miss, our mechanism, called Spatial Footprint Predictor (SFP), predicts which portions of a cache block will get used before getting evicted. The high accuracy of the predictor allows us to exploit spatial locality exhibited in larger blocks of data yielding better miss ratios without significantly impacting the memory access latencies. Our evaluation of this mechanism shows that the miss rate of the cache is improved, on average, by 18% in addition to a significant reduction in the bandwidth requirement.","Delay,
Prefetching,
Microcomputers,
Bandwidth,
Computer science,
Read only memory,
Electronic switching systems,
Page description languages,
Pollution,
Hardware"
SpecSyn: an environment supporting the specify-explore-refine paradigm for hardware/software system design,"System-level design issues are gaining increasing attention, as behavioral synthesis tools and methodologies mature. We present the SpecSyn system-level design environment, which supports the new specify-explore-refine (SER) design paradigm. This three-step approach to design includes precise specification of system functionality, rapid exploration of numerous system-level design options, and refinement of the specification into one reflecting the chosen option. A system-level design option consists of an allocation of system components, such as standard and custom processors, memories, and buses, and a partitioning of functionality among those components. After refinement, the functionality assigned to each component can then he synthesized to hardware or compiled to software. We describe the issues and approaches for each part of the SpecSyn environment. The new paradigm and environment are expected to lead to a more than ten times reduction in design time, and our experiments support this expectation.","Hardware,
Software systems,
System-level design,
Application specific integrated circuits,
Computer science,
Software design,
Embedded system,
Embedded software,
Integrated circuit interconnections"
Finding landmarks for mobile robot navigation,"Localization addresses the problem of determining the position of a mobile robot from sensor data. This paper presents an algorithm, called BaLL, which enables a mobile robot to learn a set of landmarks used in localization and to learn how to recognize them using artificial neural networks. BaLL is based on a statistical localization approach. It is applicable to a large variety of sensors and environments. Experiments with a mobile robot equipped with sonar sensors and a camera illustrate that BaLL identifies highly useful landmarks.","Mobile robots,
Navigation,
Robot sensing systems,
Robot kinematics,
Humans,
Artificial neural networks,
Computer science,
Sonar,
Large-scale systems,
Books"
Spiral CT image deblurring for cochlear implantation,"Cochlear implantation is the standard treatment for profound hearing loss, Preimplantation and postimplantation spiral computed tomography (CT) is essential in several key clinical and research aspects. The maximum image resolution with commercial spiral CT scanners is insufficient to define clearly anatomical features and implant electrode positions in the inner ear, In this paper, the authors develop an expectation maximization (EM)-like iterative deblurring algorithm to achieve spiral CT image super-resolution for cochlear implantation, assuming a spatially invariant linear spiral CT system with a three-dimensional (3-D) separable Gaussian point spread function (PSF). The authors experimentally validate the 3-D Gaussian blurring model via phantom measurement and profile fitting. The imaging process is further expressed as convolution of an isotropic 3-D Gaussian PSF and a blurred underlying volumetric image. Under practical conditions, an oblique reconstructed section is approximated as convolution of an isotropic two dimensional (2-D) Gaussian PSF and the corresponding actual cross section. The spiral CT image deblurring algorithm is formulated with sieve and resolution kernels for suppressing noise and edge artifacts. A typical cochlear cross section is used for evaluation, demonstrating a resolution gain up to 30%-40% according to the correlation criterion. Physical phantoms, preimplantation and postimplantation patients are reconstructed into volumes of 0.1-mm cubic voxels. The patient images are digitally unwrapped along the central axis of the cochlea and the implanted electrode array respectively, then oblique sections orthogonal to the central axis formed. After deblurring, representation of structural features is substantially improved in all the cases.","Spirals,
Computed tomography,
Image restoration,
Image resolution,
Electrodes,
Imaging phantoms,
Convolution,
Image reconstruction,
Deafness,
Auditory implants"
The accuracy of fast multipole methods for Maxwell's equations,"The multilevel fast multipole method can provide fast, accurate solutions to electromagnetic scattering problems, provided its users select the FMM degree and FMM cube size appropriately. The article discusses errors associated with truncating multipole expansions and methods for selecting an appropriate set of parameters.","Maxwell equations,
Electromagnetic scattering,
Error correction,
Moment methods,
Acceleration,
Scattering parameters,
Libraries,
Electromagnetic measurements,
Algorithm design and analysis,
Tree data structures"
An automated framework for structural test-data generation,"Structural testing criteria are mandated in many software development standards and guidelines. The process of generating test data to achieve 100% coverage of a given structural coverage metric is labour-intensive and expensive. This paper presents an approach to automate the generation of such test data. The test-data generation is based on the application of a dynamic optimisation-based search for the required test data. The same approach can be generalised to solve other test-data generation problems. Three such applications are discussed-boundary value analysis, assertion/run-time exception testing, and component re-use testing. A prototype tool-set has been developed to facilitate the automatic generation of test data for these structural testing problems. The results of preliminary experiments using this technique and the prototype tool-set are presented and show the efficiency and effectiveness of this approach.","Automatic testing,
Software testing,
Costs,
Application software,
Automation,
Simulated annealing,
Computer science,
Programming,
Software standards,
Standards development"
A low bandwidth broadcasting protocol for video on demand,Broadcasting protocols can improve the efficiency of video on demand services by reducing the bandwidth required to transmit videos that are simultaneously watched by many viewers. We present a polyharmonic broadcasting protocol that requires less bandwidth than the best extant protocols to achieve the same low maximum waiting time. We also show how to modify the protocol to accommodate very long videos without increasing the buffering capacity of the set-top box.,"Bandwidth,
Multimedia communication,
Broadcasting,
Protocols,
Video on demand,
Streaming media,
US Department of Transportation,
Computer science,
Costs,
Video sharing"
Stereo with mirrors,"In this paper, we propose the use of mirrors and a single camera for computational stereo. When compared to conventional stereo systems that use two cameras, our method has a number of significant advantages such as wide field of view, single viewpoint projection, identical camera parameters and ease of calibration. We propose four stereo systems that use a single camera pointed towards planar, ellipsoidal, hyperboloidal, and paraboloidal mirrors. In each case, we present a derivation of the epipolar constraints. Next, we attempt to understand what can be seen by each system, and formalize the notion of field of view. We conclude with two experiments to obtain 3-D structure. In the first we use a pair of planar mirrors, and in the second a pair of paraboloidal mirrors. The results of our experiments demonstrate the viability of stereo using mirrors.","Mirrors,
Cameras,
Layout,
Lenses,
Calibration,
Three dimensional displays,
Optical refraction,
Computer science,
Optical imaging,
Rendering (computer graphics)"
Enhanced harmonic data broadcasting and receiving scheme for popular video service,"For a popular movie, in order to minimize the service delay with a given bandwidth, currently the harmonic broadcasting scheme can provide users with the shortest waiting time to approach near video-on-demand service. This paper presents an enhanced scheme, which can improve the bandwidth utilization and reduce further the viewers' waiting time.","Multimedia communication,
Motion pictures,
Bandwidth,
Delay effects,
Digital video broadcasting,
Uncertainty,
Councils,
Computer science,
Niobium,
Consumer electronics"
Shock graphs and shape matching,"We have been developing a theory for the generic representation of 2-D shape, where structural descriptions are derived from the shocks (singularities) of a curve evolution process, acting on bounding contours. We now apply the theory to the problem of shape matching. The shocks are organized into a directed, acyclic shock graph, and complexity is managed by attending to the most significant (central) shape components first. The space of all such graphs is highly structured and can be characterized by the rules of a shock graph grammar. The grammar permits a reduction of a shockgraph to a unique rooted shock tree. We introduce a novel tree matching algorithm which finds the best set of corresponding nodes between two shock trees in polynomial time. Using a diverse database of shapes, we demonstrate our system's performance under articulation, occlusion, and changes in viewpoint.","Electric shock,
Equations,
Tree graphs,
Shape control,
Computer vision,
Computer science,
Databases,
Heart,
Object recognition,
Neck"
A feedback based scheme for improving TCP performance in ad-hoc wireless networks,"Ad-hoc networks are completely wireless networks of mobile hosts, in which the topology rapidly changes due to the movement of mobile hosts. This frequent topology may lead to sudden packet losses and delays. Transport protocols like TCP have been built mainly for reliable, fixed networks. Hence, when used in ad-hoc networks, TCP misinterprets this loss as congestion and invokes congestion control. This leads to unnecessary retransmissions and loss of throughput. To overcome this problem, a feedback scheme is proposed, so that the source can distinguish between route failure and network congestion. When a route is disrupted, the source is sent a route failure notification (RFN) packet, allowing it to freeze its timers and stop sending packets. When the route is re-established, the source is informed through a route re-establishment notification (RRN) packet, upon which it resumes by unfreezing timers and continuing packet transmissions. The simulated performance of TCP on ad-hoc networks with and without feedback is compared and reported. It is observed that in the event of route failures, as the route re-establishment time increases, the use of feedback provides significant gains in throughput as well as savings in unnecessary packet transmissions. Several further enhancements and directions for future work are also sketched.","Feedback,
Wireless networks,
Ad hoc networks,
Network topology,
Throughput,
Mobile computing,
Computer science,
Electronic mail,
Delay,
Transport protocols"
WebOS: operating system services for wide area applications,"Demonstrates the power of providing a common set of operating system services to wide-area applications, including mechanisms for naming, persistent storage, remote process execution, resource management, authentication and security. On a single machine, application developers can rely on the local operating system to provide these abstractions. In the wide area, however, application developers are forced to build these abstractions themselves or to do without. This ad-hoc approach often results in individual programmers implementing non-optimal solutions, wasting both programmer effort and system resources. To address these problems, we are building a system, WebOS, that provides the basic operating systems services needed to build applications that are geographically distributed, highly available, incrementally scalable and dynamically reconfigurable. Experience with a number of applications developed under WebOS indicates that it simplifies system development and improves resource utilization. In particular, we use WebOS to implement Rent-A-Server to provide dynamic replication of overloaded Web services across the wide area in response to client demands.","Operating systems,
Web and internet services,
Resource management,
Computer science,
Secure storage,
Authentication,
Programming profession,
Availability,
Web server,
Power system security"
Fast Monte-Carlo algorithms for finding low-rank approximations,"In several applications, the data consists of an m/spl times/n matrix A and it is of interest to find an approximation D of a specified rank k to A where, k is much smaller than m and n. Traditional methods like the Singular Value Decomposition (SVD) help us find the ""best"" such approximation. However, these methods take time polynomial in m, n which is often too prohibitive. In this paper, we develop an algorithm which is qualitatively faster provided we may sample the entries of the matrix according to a natural probability distribution. Indeed, in the applications such sampling is possible. Our main result is that we can find the description of a matrix D* of rank at most k so that /spl par/A-D*/spl par//sub F//spl les/min/D,rank(D)/spl les/k/spl par/A-D/spl par//sub F/+/spl epsiv//spl par/A/spl par//sub F/ holds with probability at least 1-/spl delta/. (For any matrix M, /spl par/M/spl par//sub F//sup 2/ denotes the sum of the squares of all the entries of M.) The algorithm takes time polynomial in k, 1//spl epsiv/, log(1//spl delta/) only, independent of m, n.","Computer science,
Matrix decomposition,
Mathematics,
Laboratories,
Application software,
Sampling methods,
Singular value decomposition,
Polynomials,
Electrical capacitance tomography,
Numerical analysis"
X-ray fluorescent computer tomography with synchrotron radiation,"This paper describes the possibility of a quantitative calculation of the distribution of a nonradioactive element within a selected cross section with nondestructive methods with the help of X-ray fluorescent tomography (XFCT). In order to increase measurement sensitivity, the use of a lamellar collimator was avoided. One of the main problems for the quantitative determination of concentration was absorption of the stimulating synchrotron ray as well as re-absorption of the emitted fluorescent light. The absorption coefficients required for a consideration of the absorption processes have been determined with two absorption tomograms. The algebraic reconstruction technique (ART) and the maximum likelihood method with expectation maximization (MLEM) were used for the reconstruction of the chemical element to be classified, with close consideration of the absorption phenomenon. The experiments were undertaken at the bending-magnet beamline, CEMO, at the laboratory for synchrotron radiation in Hamburg, HASYLAB (4.5 GeV) (100 mA). The photon intensity flux was approximately 10/sup 9/ photons/mm/sup 2//s. The concentration of iodine was calculated with phantoms and an untreated, dissected human thyroid gland with the help of a calibration curve. The total error related to the reconstructed mean value amounts to 20%. One can find at least an iodide concentration of 0.6 mmol/l in this experimental setup.","Fluorescence,
Tomography,
Synchrotron radiation,
Electromagnetic wave absorption,
Collimators,
Subspace constraints,
Chemical elements,
Laboratories,
Imaging phantoms,
Humans"
Fuzzy homogeneity approach to multilevel thresholding,"The spatial ambiguity among pixels has inherent vagueness rather than randomness, therefore, the conventional methods might not work well. We propose fuzzy homogeneity vectors to handle the greyness and spatial uncertainties among pixels, and to perform multilevel thresholding. The experimental results prove that the proposed approach works better than the histogram-based algorithms.","Pixel,
Entropy,
Fuzzy set theory,
Uncertainty,
Image processing,
Set theory,
Histograms,
Computer science"
Ovarian ultrasound image analysis: follicle segmentation,"Ovarian ultrasound is an effective tool in infertility treatment. Repeated measurements of the size and shape of follicles over several days are the primary means of evaluation by physicians. Currently, follicle wall segmentation is achieved by manual tracing which is time consuming and susceptible to inter-operator variation. An automated method for follicle wall segmentation is reported that uses a four-step process based on watershed segmentation and a knowledge-based graph search algorithm which utilizes a priori information about follicle structure for inner and outer wall detection. The automated technique was tested on 36 ultrasonographic images of women's ovaries. Validation against manually traced borders has shown good correlation of manually defined and computer-determined area measurements (R/sup 2/=0.85-0.96). The border positioning errors were small: 0.63/spl plusmn/0.36 mm for inner border and 0.67/spl plusmn/0.41 mm for outer border detection. The use of watershed segmentation and graph search methods facilitates fast, accurate inner and outer border detection with minimal user-interaction.","Ultrasonic imaging,
Image analysis,
Image segmentation,
Shape measurement,
Size measurement,
Ultrasonic variables measurement,
Automatic testing,
Area measurement,
Computer errors,
Search methods"
Background modeling for segmentation of video-rate stereo sequences,"Stereo sequences promise to be a powerful method for segmenting images for applications such as tracking human figures. We present a method of statistical background modeling for stereo sequences that improves the reliability and sensitivity of segmentation in the presence of object clutter. The dynamic version of the method, called gated background adaptation, can reliably learn background statistics in the presence of corrupting foreground motion. The method has been used with a simple head discriminator to detect and track people using a stereo head mounted on a pan/tilt platform. It runs at video rates using standard PC hardware.","Electrical capacitance tomography,
Image segmentation,
Head,
Statistics,
Layout,
Tracking,
Computer science,
Artificial intelligence,
Application software,
Humans"
The network effects of prefetching,"Prefetching has been shown to be an effective technique for reducing user perceived latency in distributed systems. In this paper we show that even when prefetching adds no extra traffic to the network, it can have serious negative performance effects. Straightforward approaches to prefetching increase the burstiness of individual sources, leading to increased average queue sizes in network switches. However, we also show that applications can avoid the undesirable queueing effects of prefetching. In fact, we show that applications employing prefetching can significantly improve network performance, to a level much better than that obtained without any prefetching at all. This is because prefetching offers increased opportunities for traffic shaping that are not available in the absence of prefetching. Using a simple transport rate control mechanism, a prefetching application can modify its behavior from a distinctly ON/OFF entity to one whose data transfer rate changes less abruptly, while still delivering all data in advance of the user's actual requests.","Prefetching,
Traffic control,
Telecommunication traffic,
Delay,
Communication system traffic control,
Web sites,
Costs,
Computer science,
Switches,
Distributed information systems"
A framework for efficient minimum distance computations,"We present a framework for minimum distance computations that allows efficient solution of minimum distance queries on a variety of surface representations, including sculptured surfaces. The framework depends on geometric reasoning rather than numerical methods and can be implemented straightforwardly. We demonstrate performance that compares favorably to other polygonal methods and is faster than reported results for other methods on sculptured surfaces.","Solid modeling,
Parametric statistics,
Robots,
Computer graphics,
Haptic interfaces,
Computational geometry,
Differential equations,
Computer science,
Cities and towns,
Time factors"
Applying NetSolve's network-enabled server,"The scientific community has long used the Internet for communication of e-mail, software, and papers. Until recently, there has been little use of the network for actual computations. This solution is changing rapidly and will have an enormous impact on the future. The NetSolve system described here has a significant role to play in these developments. The NetSolve project lets users access computational resources, both hardware and software, distributed across the network. Thanks to a variety of interfaces, users can easily perform scientific computing tasks without having any computing resources installed on their computers. Research issues involved in the NetSolve system include fault tolerance, load balancing, user interface design, computational servers, virtual libraries, and network based computing. As the project matures, several promising extensions and applications of NetSolve will emerge. We describe the project and examine some of the extensions being developed for NetSolve: an interface to the Condor system, an interface to the ScaLapack parallel library, a bridge with the Ninf system, and an integration of NetSolve and ImageVision.","Network servers,
Computer networks,
Computer interfaces,
Web server,
Internet,
Electronic mail,
Distributed computing,
Hardware,
Scientific computing,
Fault tolerant systems"
Segmentation by grouping junctions,"We propose a method for segmenting gray-value images. By segmentation, we mean a map from the set of pixels to a small set of levels such that each connected component of the set of pixels with the same level forms a relatively large and ""meaningful"" region. The method finds a set of levels with associated gray values by first finding junctions in the image and then seeking a minimum set of threshold values that preserves the junctions. Then it finds a segmentation map that maps each pixel to the level with the closest gray value to the pixel data, within a smoothness constraint. For a convex smoothing penalty, we show the global optimal solution for an energy function that fits the data can be obtained in a polynomial time, by a novel use of the maximum-flow algorithm. Our approach is in contrast to a view in computer vision where segmentation is driven by intensity, gradient, usually not yielding closed boundaries.","Image segmentation,
Polynomials,
Computer vision,
Partitioning algorithms,
Computer science,
Smoothing methods,
Prototypes,
Image edge detection,
Engineering profession,
Motion segmentation"
Fixed-alternate routing and wavelength conversion in wavelength-routed optical networks,"This paper considers optical networks which employ wavelength-routing switches that enable the establishment of wavelength-division-multiplexed (WDM) connections between node-pairs. Alternate routing improves the blocking performance of such networks by providing multiple possible paths between node-pairs. Wavelength conversion improves the blocking performance of such networks by allowing a connection to use different wavelengths along its route. This paper proposes an approximate computational model that incorporates alternate routing and sparse wavelength conversion. The model is shown to give reasonably good estimates of different network parameters. Empirical studies based on discrete-event simulation, illustrate the importance of alternate routing in improving the blocking performance of a wavelength-routed optical network.","Wavelength routing,
Optical wavelength conversion,
Intelligent networks,
Optical fiber networks,
Optical switches,
Optical network units,
Wavelength division multiplexing,
Wavelength conversion,
Wavelength assignment,
Computer science"
Trust in electronic commerce: definition and theoretical considerations,"Successful electronic commerce sites allow businesses to create low-cost or more efficient channels for product sales or to create new business opportunities. The success and acceptance of most online businesses depend on several factors, both technological and social. In this paper, we examine the role of trust in the successful use and adoption of electronic commerce applications. We define trust as a belief in the system characteristics, specifically belief in the competence, dependability and security of the system, under conditions of risk. Based on this definition, we develop a theoretical model that identifies the factors that effect users' trust in electronic commerce. The theoretical model presented in this paper serves as the basis for future empirical studies that will aim to measure the impact of these factors on the development of trust in electronic commerce.","Electronic commerce,
Internet,
IP networks,
Computer networks,
Business,
Electrical capacitance tomography,
ARPANET,
Intelligent networks,
Postal services,
Electronic mail"
Sequential prediction of individual sequences under general loss functions,"We consider adaptive sequential prediction of arbitrary binary sequences when the performance is evaluated using a general loss function. The goal is to predict on each individual sequence nearly as well as the best prediction strategy in a given comparison class of (possibly adaptive) prediction strategies, called experts. By using a general loss function, we generalize previous work on universal prediction, forecasting, and data compression. However, here we restrict ourselves to the case when the comparison class is finite. For a given sequence, we define the regret as the total loss on the entire sequence suffered by the adaptive sequential predictor, minus the total loss suffered by the predictor in the comparison class that performs best on that particular sequence. We show that for a large class of loss functions, the minimax regret is either /spl theta/(log N) or /spl Omega/(/spl radic//spl Lscr/log N), depending on the loss function, where N is the number of predictors in the comparison class and/spl Lscr/ is the length of the sequence to be predicted. The former case was shown previously by Vovk (1990); we give a simplified analysis with an explicit closed form for the constant in the minimax regret formula, and give a probabilistic argument that shows this constant is the best possible. Some weak regularity conditions are imposed on the loss function in obtaining these results. We also extend our analysis to the case of predicting arbitrary sequences that take real values in the interval [0,1].","Binary sequences,
Minimax techniques,
Computer science,
Performance loss,
Data compression,
US Department of Energy,
Writing,
Adaptive coding,
Pattern recognition,
Adaptive algorithm"
Detecting disruptive routers: a distributed network monitoring approach,"An attractive target for a computer system attacker is the router. An attacker in control of a router can disrupt communication by dropping or misrouting packets passing through the router. We present a protocol called WATCHERS that detects and reacts to routers that drop or misroute packets. WATCHERS is based on the principle of conservation of flow in a network: all data bytes sent into a node, and not destined for that node, are expected to exit the node. WATCHERS tracks this flow, and detects routers that violate the conservation principle. We show that WATCHERS has several advantages over existing network monitoring techniques. We argue that WATCHERS' impact on router performance and WATCHERS' memory requirements are reasonable for many environments. We demonstrate that in ideal conditions WATCHERS makes no false-positive diagnoses. We also describe how WATCHERS can be tuned to perform nearly as well in realistic conditions.","Protocols,
Kirk field collapse effect,
National security,
Condition monitoring,
Fault detection,
Computerized monitoring,
Computer science,
Communication system control,
Internet,
Gain control"
Dynamic sensor planning in visual servoing,"We present an approach to dynamic sensor planning problems in visual servoing. Specifically, one of the main problems an image-based visual servoing is to plan the camera trajectory in order to avoid undesired configurations (e.g., features out of view, collision with obstacles, etc.). Our approach uses the robot redundancy and employs a control scheme based on the task function approach. It combines the regulation of the selected vision-based task with the minimization of a secondary cost function, which reflects given constraints on the manipulator trajectory. We describe how this methodology is applied to common problems in robotic vision: occlusion avoidance, field of view constraint and obstacle avoidance. We demonstrate the validity of this approach with various experiments.","Visual servoing,
Robot vision systems,
Cameras,
Cost function,
Robot sensing systems,
Manipulators,
Robotics and automation,
Computer science,
Computed tomography,
Trajectory"
Breaking abstractions and unstructuring data structures,"To ensure platform independence, mobile programs are distributed in forms that are isomorphic to the original source code. Such codes are easy to decompile, and hence they increase the risk of malicious reverse engineering attacks. Code obfuscation is one of several techniques which has been proposed to alleviate this situation. An obfuscator is a tool which-through the application of code transformations-converts a program into an equivalent one that is more difficult to reverse engineer. In a previous paper (Collberg et al., 1998) we have described the design of a control flow obfuscator for Java. In this paper we extend the design with transformations that obfuscate data structures and abstractions. In particular we show how to obfuscate classes, arrays, procedural abstractions and built-in data types like strings, integers and booleans.","Data structures,
Reverse engineering,
Protection,
Application software,
Java,
Cryptography,
Computer science,
Digital signatures,
Hardware,
Virtual machining"
An active transcoding proxy to support mobile web access,"In this paper, we present a proxy based system (MOWSER) to support web browsing from mobile clients over wireless networks. Mowser is a proxy agent between the mobile host and the web server, which performs active transcoding of data on both upstream and downstream traffic to present web information to the mobile user according to the QoS parameters set by the user. Active transcoding is defined as modifying the HTTP stream in situ, and it is entirely transparent to the user. Further, our system does not pose any additional requirements on the mobile user. This is an improvement over other proxy based systems, which only transcode images on the downstream and are mostly not configurable. While developed for mobile users, such a system can actually be useful in any low bandwidth scenario.","Transcoding,
Mobile computing,
Web server,
Bandwidth,
Wireless networks,
Hardware,
Computer networks,
Web pages,
Middleware,
Computer science"
Fault detection and identification in a mobile robot using multiple-model estimation,This paper introduces a method to detect and identify faults in wheeled mobile robots. The idea behind the method is to use adaptive estimation to predict (in parallel) the outcome of several faults. Models of the system behavior under each type of fault are embedded in the various parallel estimators (each of which is a Kalman filter). Each filter is thus tuned to a particular fault. Using its embedded model each filter predicts values for the sensor readings. The residual (the difference between the predicted and actual sensor reading) is an indicator of how well the filter is performing. A fault detection and identification module is responsible for processing the residual to decide which fault has occurred. As an example the method is implemented successfully on a Pioneer I robot. The paper concludes with a discussion of future work.,"Fault detection,
Fault diagnosis,
Mobile robots,
Filters,
Intelligent robots,
Parallel robots,
Robot sensing systems,
Actuators,
Filtering,
Computer science"
A fully abstract game semantics for general references,A games model of a programming language with higher-order store in the style of ML-references is introduced. The category used for the model is obtained by relaxing certain behavioural conditions on a category of games previously used to provide fully abstract models of pure functional languages. The model is shown to be fully abstract by means of factorization arguments which reduce the question of definability for the language with higher-order store to that for its purely functional fragment.,"Computer languages,
Educational institutions,
Prototypes,
Joining processes,
Calculus"
Parallax correction in PET using depth of interaction information,"The authors investigate the restoration of radial resolution for transaxially off-center sources in a PET ring detector configuration using directly measured depth of interaction (DOI) information. Lutetium oxyorthosilicate (LSO) crystals are coupled to a photodetector along the radial dimension of the PET ring configuration. Four millimeter wide crystals are segmented into 4 mm and 10 mm lengths in the radial dimension and optically isolated so that scintillation light is localized to the radial extent of the segment, giving 4 mm and 10 mm DOI resolution. The authors also segmented 2 mm wide crystals into 10 mm lengths. This DOI technique is unambiguous but not practical for a multi-ring PET scanner. The objective here is to quantify the intrinsic radial resolution as a function of position within the FOV and DOI resolution. The degradation in radial resolution across 75% of the field of view was reduced by 45% (50%) with DOI resolution equal to one-half (one-third) the crystal length. Improved resolution via narrower crystals was overshadowed by DOI effects at /spl ges/50% off-center positions in 30 mm deep crystals with 10 mm of FOI resolution. Computer simulated images that incorporate 10 mm and 15 mm DOI in 30 mm crystals show improved resolution uniformity over a system without DOI.","Positron emission tomography,
Crystals,
Image restoration,
Solid scintillation detectors,
Couplings,
Photodetectors,
Degradation,
Computational modeling,
Computer simulation,
Image resolution"
"Complete, safe information flow with decentralized labels","The growing use of mobile code in downloaded applications and servlets has increased interest in robust mechanisms for ensuring privacy and secrecy. Information flow control is intended to directly address privacy and secrecy concerns, but most information flow models are too restrictive to be widely used. The decentralized label model is a new information flow model that extends traditional models with per-principal information flow policies and also permits a safe form of declassification. This paper extends this new model further, making it more flexible and expressive. We define a new formal semantics for decentralized labels and a corresponding new rule for relabeling data that is both sound and complete. We also show that these extensions preserve the ability to statically check information flow.","Privacy,
Laboratories,
Information analysis,
Programming profession,
Computer science,
Mobile computing,
Application software,
Protection,
Contracts,
Monitoring"
Generalized linear least squares method for fast generation of myocardial blood flow parametric images with N-13 ammonia PET,"In this paper, the authors developed and tested strategies for estimating myocardial blood flow (MBF) and generating MBF parametric images using positron emission tomography (PET), N-13 ammonia, and the generalized linear least square (GLLS) method. GLLS was generalized to the general linear compartment model, modified for the correction of spillover, validated using simulated N-13 ammonia data, and examined using PET data from several patient studies. In comparison to the standard model-fitting procedure, the GLLS method provided similar accuracy and superior computational speed.","Least squares methods,
Myocardium,
Blood flow,
Positron emission tomography,
Biomedical imaging,
Medical diagnostic imaging,
Biomedical measurements,
Image generation,
Least squares approximation,
Kinetic theory"
Nanorobotic assembly of two-dimensional structures,"Precise control of the structure of matter at the nanometer scale will have revolutionary implications for science and technology. Nanoelectromechanical systems (NEMS) will be extremely small and fast, and have applications that range from cell repair to ultrastrong materials. This paper describes the first steps towards the construction of NEMS by assembling nanometer-scale objects using a scanning probe microscope as a robot. Our research takes an interdisciplinary approach that combines knowledge of macrorobotics and computer science with the chemistry and physics of phenomena at the nanoscale. We present experimental results that show how to construct arbitrary patterns of gold nanoparticles on a mica or silicon substrate, and describe the underlying technology. We also discuss the next steps in our research, which are aimed at producing connected structures in the plane, and eventually three-dimensional nanostructures.","Nanoelectromechanical systems,
Robotic assembly,
Materials science and technology,
Application software,
Probes,
Microscopy,
Computer science,
Chemistry,
Physics,
Gold"
Threaded multiple path execution,"This paper presents Threaded Multi-Path Execution (TME), which exploits existing hardware on a Simultaneous Multithreading (SMT) processor to speculatively execute multiple paths of execution. When there are fewer threads in an SMT processor than hardware contexts, threaded multi-path execution uses spare contexts to fetch and execute code along the less likely path of hard-to-predict branches. This paper describes the hardware mechanisms needed to enable an SMT processor to efficiently spawn speculative threads for threaded multi-path execution. The Mapping Synchronization Bus is described which enables the spawning of these multiple paths. Policies are examined for deciding which branches to fork, and for managing competition between primary and alternate path threads for critical resources. Our results show that TME increases the single program performance of an SMT with eight thread contexts by 14%-23% on average, depending on the misprediction penalty, for programs with a high misprediction rate.",
A multidisciplinary cooperative problem-based learning approach to embedded systems design,"A subject introducing embedded systems design to second-gear undergraduate students is described. The subject provides units introducing microprocessors and CAD tools for electronic circuit design and integrates these units into a single cohesive subject by means of a group project. The subject is developed as a multidisciplinary cooperative problem-based learning program with the base groups structured to comprise members from different degree programs offered by the School of Engineering at James Cook University, Australia. Initial results show that cooperative problem-based learning can be used to develop problem-solving, teamwork and lifelong learning skills as well as producing a level of technical knowledge beyond that of individual achievement.",
Location dependent data and its management in mobile databases,"In traditional ways of managing data, the relationship between the data and the geographical location of the organization it represents, is usually ignored. In wireless computing this property of ""location transparency"" is in fact often replaced by a ""location dependency"" property. Furthermore, the mode of issuing queries (the geographical location where the queries originate, the way they are issued, etc.) on such data determines the outcome. Location dependent data is data whose value depends on its location. The objective of the paper is to introduce this topic and spawn further related research.",
Improving the performance of distributed applications using active networks,"An active network allows applications to inject customized programs into network nodes. This enables faster protocol innovation by making it easier to deploy new network protocols, even over the wide area. We argue that the ability to introduce active protocols offers important opportunities for end-to-end performance improvements of distributed applications. We begin by describing several active protocols that provide novel network services and discussing the potential impact of these kinds of services on end-to-end application performance. We then present and analyze the performance of an active networking protocol that uses caching within the network backbone to reduce load on both servers and backbone routers.",
A robust numerical solution to reconstruct a globally relative shear modulus distribution from strain measurements,"To noninvasively quantify tissue elasticity for differentiating malignancy of soft tissue, the authors previously proposed a two-dimensional (2-D) mechanical inverse problem in which simultaneous partial differential equations (PDE's) represented the target distribution globally of relative shear moduli with respect to reference shear moduli such that the relative values could be determined from strain distributions obtained by conventional ultrasound (US) or nuclear magnetic resonance (NMR) imaging-based analysis. Here, the authors further consider the analytic solution in the region of interest, subsequently demonstrating that the problem is inevitably ill-conditioned in real-world applications, i.e., noise in measurement data and improper configurations of mechanical sources/reference regions make it impossible to guarantee the existence of a stable and unique target global distribution. Next, based on clarification of the inherent problematic conditions, the authors describe a newly developed numerical-based implicit-integration approach that novelly incorporates a computationally efficient regularization method designed to solve this differential inverse problem using just low-pass filtered spectra derived from strain measurements. To evaluate method effectiveness, reconstructions of the global distribution are carried out using intentionally created ill-conditioned models. The resultant reconstructions indicate the robust solution is highly suitable, while also showing it has high potential to be applied in the development of an effective yet versatile diagnostic tool for quantifying the distribution of elasticity in various soft tissues.",
Fast nearest neighbor search in high-dimensional space,"Similarity search in multimedia databases requires an efficient support of nearest neighbor search on a large set of high dimensional points as a basic operation for query processing. As recent theoretical results show, state of the art approaches to nearest neighbor search are not efficient in higher dimensions. In our new approach, we therefore precompute the result of any nearest neighbor search which corresponds to a computation of the voronoi cell of each data point. In a second step, we store the voronoi cells in an index structure efficient for high dimensional data spaces. As a result, nearest neighbor search corresponds to a simple point query on the index structure. Although our technique is based on a precomputation of the solution space, it is dynamic, i.e. it supports insertions of new data points. An extensive experimental evaluation of our technique demonstrates the high efficiency for uniformly distributed as well as real data. We obtained a significant reduction of the search time compared to nearest neighbor search in the X tree (up to a factor of 4).",
Current and future applications of virtual reality for medicine,"Virtual reality is just emerging as an accepted scientific discipline for medicine. The majority of near-term applications are in the area of surgical planning, interoperative navigation, and surgical simulations. Its use in rehabilitative medicine and psychiatry has made significant progress. The immediate future holds promise for virtual endoscopy, which may replace standard endoscopic procedures for diagnostic screening. Viewing of these virtual images may be with head-mounted displays or true suspended holograms. The most highly developed area is in surgical simulations. Current generations are approaching photorealistic representation of the anatomy, while measurement science is providing physical tissue properties and physiologic parameters. The types of simulations range from ""needle-based"" procedures, such as standard intravenous insertion, central venous placement catheter, and chest-tube insertion to more sophisticated simulations of full surgical procedures like laparoscopic cholecystectomy or hysteroscopic resection of interuterine myoma. In addition, haptic input devices are providing the sense of touch to the procedures. Soon there will be patient-specific models derived from computed tomography or magnetic resonance imaging scans that will permit a surgeon to practice a delicate surgical procedure on the patient's specific virtual anatomy before actually performing the procedure on the patient.",
Algebraic reconstruction for magnetic resonance imaging under B/sub 0/ inhomogeneity,"In magnetic resonance imaging, spatial localization is usually achieved using Fourier encoding which is realized by applying a magnetic field gradient along the dimension of interest to create a linear correspondence between the resonance frequency and spatial location following the Larmor equation. In the presence of B/sub 0/ inhomogeneities along this dimension, the linear mapping does not hold and spatial distortions arise in the acquired images. In this paper, the problem of image reconstruction under an inhomogeneous field is formulated as an inverse problem of a linear Fredholm equation of the first kind. The operators in these problems are estimated using field mapping and the k-space trajectory of the imaging sequence. Since such inverse problems are known to be ill-posed in general, robust solvers, singular value decomposition and conjugate gradient method, are employed to obtain corrected images that are optimal in the Frobenius norm sense. Based on this formulation, the choice of the imaging sequence for well-conditioned matrix operators is discussed, and it is shown that nonlinear k-space trajectories provide better results. The reconstruction technique is applied to sequences where the distortion is more severe along one of the image dimensions and the two-dimensional reconstruction problem becomes equivalent to a set of independent one-dimensional problems. Experimental results demonstrate the performance and stability of the algebraic reconstruction methods.",
An efficient RMS admission control and its application to multiprocessor scheduling,"A real-time system must execute functionally correct computations in a timely manner. In order to guarantee that all tasks accepted in the system will meet their timing requirements, an admission control algorithm must be used to only accept tasks whose requirements can be satisfied. Rate-monotonic scheduling (RMS) is arguably the best known scheduling policy for periodic real-time tasks on uniprocessors. We propose a new and efficient admission control for rate-monotonic scheduling on a uniprocessor and analyze its performance. This admission control is then modified to provide an admission control mechanism for multiprocessor systems. Experimental results indicate that this new admission control for multiprocessor systems achieves a processor utilization of up to 96% for a large number of tasks and has a low computational complexity. The proposed admission control is also used to derive a new and better multiprocessor schedulability bound for RMS with provisions for periodic servers and for RMS with tolerance to transient faults.",
MPI-SIM: using parallel simulation to evaluate MPI programs,"This paper describes the design and implementation of MPI-SIM, a library for the execution driven parallel simulation of MPI programs. MPI-LITE, a portable library that supports multithreaded MPI, is also described. MPI-SIM, built on top of MPI-LITE, can be used to predict the performance of existing MPI programs as a function of architectural characteristics, including number of processors and message communication latencies. The simulation models can be executed sequentially or in parallel. Parallel executions of MPI-SIM models are synchronized using a set of asynchronous conservative protocols. MPI-SIM reduces synchronization overheads by exploiting the communication characteristics of the program it simulates. This paper presents validation and performance results from the use of MPI-SIM to simulate applications from the NAS Parallel Benchmark suite. Using the techniques described here, we are able to reduce the number of synchronizations in the parallel simulation as compared with the synchronous quantum protocol and are able to achieve speedups ranging from 3.2-11.9 in going from sequential to parallel simulation using 16 processors on the IBM SP2.",
A framework for modeling appearance change in image sequences,"Image ""appearance"" may change over time due to a variety of causes such as: 1) object or camera motion; 2) generic photometric events including variations in illumination (e.g. shadows) and specular reflections; and 3) ""iconic changes"" which are specific to the objects being viewed and include complex occlusion events and changes in the material properties of the objects. We propose a general framework for representing and recovering these ""appearance changes"" in an image sequence as a ""mixture"" of different causes. The approach generalizes previous work on optical flow to provide a richer description of image events and more reliable estimates of image motion.",
Bounding loop iterations for timing analysis,"Static timing analyzers need to know the minimum and maximum number of iterations associated with each loop in a real time program so accurate timing predictions can be obtained. The paper describes three complementary methods to support timing analysis by bounding the number of loop iterations. First, an algorithm is presented that determines the minimum and maximum number of iterations of loops with multiple exits. Second, the loop invariant variables on which the number of loop iterations depends are identified for which the user can provide minimum and maximum values. Finally, a method is given to tightly predict the execution time of loops whose number of iterations is dependent on counter variables of outer level loops. These methods have been successfully integrated in an existing timing analyzer that predicts the performance for optimized code on a machine that exploits caching and pipelining. The result is tighter timing analysis predictions and less work for the user.",
Progressive tetrahedralizations,"The paper describes some fundamental issues for robust implementations of progressively refined tetrahedralizations generated through sequences of edge collapses. We address the definition of appropriate cost functions and explain on various tests which are necessary to preserve the consistency of the mesh when collapsing edges. Although considered a special case of progressive simplicial complexes (J. Popovic and H. Hoppe, 1997), the results of our method are of high practical importance and can be used in many different applications, such as finite element meshing, scattered data interpolation, or rendering of unstructured volume data.",
Load latency tolerance in dynamically scheduled processors,"This paper provides quantitative measurements of load latency tolerance in a dynamically scheduled processor. To determine the latency tolerance of each memory load operation, our simulations use flexible load completion policies instead of a fixed memory hierarchy that dictates the latency. Although our policies delay load completion as long as possible, they produce performance (instructions committed per cycle (IPC)) comparable to an ideal memory system where all loads complete in one cycle. Our measurements reveal that to produce IPC values within 8% of the ideal memory system, between 1% and 62% of loads need to be satisfied within a single cycle and that up to 84% can be satisfied in as many as 32 cycles, depending on the benchmark and processor configuration. Load latency tolerance is largely determined by whether an unpredictable branch is in the load's data dependence graph and the depth of the dependence graph. Our results also show that up to 36% of all loads miss in the level one cache yet have latency demands lower than second level cache access times. We also show that up to 37% of loads hit in the level one cache even though they possess enough latency tolerance to be satisfied by lower levels of the memory hierarchy.",
Isophote-based interpolation,"Standard methods for image interpolation are based on smoothly fitting the image intensity surface. Previous edge-directed interpolation methods add limited geometric information (edge maps) to build more accurate and visually appealing interpolations at key contours in the image. This paper presents a method for geometry-based interpolation that smoothly fits the isophote (intensity level curve) contours at all points in the image rather than just at selected contours. By using level set methods for curve evolution, no explicit extraction or representation of these contours is required (unlike earlier edge-directed methods). The method uses existing interpolation techniques as an initial approximation and then iteratively reconstructs the isophotes using constrained smoothing. Results show that the technique produces results that are more visually realistic than standard function-fitting methods.",
Efficient collective communication on heterogeneous networks of workstations,"Networks of Workstations (NOW) have become an attractive alternative platform for high performance computing. Due to the commodity nature of workstations and interconnects and due to the multiplicity of vendors and platforms, the NOW environments are being gradually redefined as Heterogeneous Networks of Workstations (HNOW) environments. This paper presents a new framework for implementing collective communication operations (as defined by the Message Passing Interface (MPI) standard) efficiently for the emerging HNOW environments. We first classify different types of heterogeneity in HNOW and then focus on one important characteristic: communication capabilities of workstations. Taking this characteristic into account, we propose two new approaches Speed-Partitioned Ordered Chain (SPOC) and Fastest-Node First (FNF) to implement collective communication operations with reduced latency. We also investigate methods for deriving optimal trees for broadcast and multicast operations. Generating such trees is shown to be computationally intensive. It is shown that the FNF approach, in spite of its simplicity, can deliver performance within 1% of the performance of the optimal trees. Finally, these new approaches are compared with the approach used in the MPICH implementation on experimental as well as on simulated testbeds. On a 24-node existing HNOW environment with SGI workstations and ATM interconnection our approaches reduce the latency of broadcast and multicast operations by a factor of up to 3.5 compared to the approach used in the existing MPICH implementation. On a 64-node simulated testbed, our approaches can reduce the latency of broadcast and multicast operations by a factor of up to 4.5. Thus, these results demonstrate that there is significant potential for our approaches to be applied towards designing scalable collective communication libraries for current and future generation HNOW environments.",
A coherency based rescheduling method for dynamic security,"For on-line dynamic security analysis, the preventive control or remedial action should be an integral part of the function if instability for a contingency is detected. Research done so far in on-line remedial action has been in rescheduling generation and most of the suggested methods for determining such preventive control use the sensitivities of the stability energy margin to the generator power injections. In this paper, a new coherency based sensitivity method is proposed for generation rescheduling. Different coherency indices have been defined and then compared by ranking the contingencies according to these indices as well as the energy margin index. Since the coherency indices are always functions of the rotor angles, the sensitivity trajectories of a coherency index, such as the most critical rotor angle, with respect to changes of generation can be calculated at every time step of the integration process. This paper suggests that these sensitivities calculated shortly after fault clearing be used for rescheduling the generation. The calculation of these sensitivities are obviously faster than the calculation of the energy margin sensitivities. This paper also shows, with test results using several different systems, that the rescheduling achieved by this method provides the necessary remedial action. It is also noted that this method is intuitively more direct as it uses the sensitivities of the worst affected generator angles for rescheduling.",
Efficient algorithms for the minimum shortest path Steiner arborescence problem with applications to VLSI physical design,"Given an undirected graph G=(V, E) with positive edge weights (lengths) /spl omega/: E/spl rarr//spl Rfr//sup +/, a set of terminals (sinks) N/spl sube/V, and a unique root node e/spl epsiv/N, a shortest path Steiner arborescence (hereafter an arborescence) is a Steiner tree rooted at, spanning all terminals in N such that every source-to-sink path is a shortest path in G. Given a triple (G, N, r), the minimum shortest path Steiner arborescence (MSPSA) problem seeks an arborescence with minimum weight. The MSPSA problem has various applications in the areas of physical design of very large-scale integrated circuits (VLSI), multicast network communication, and supercomputer message routing; various eases have been studied in the literature. In this paper, we propose several heuristics and exact algorithms for the MSPSA problem with applications to VLSI physical design. Experiments indicate that our heuristics generate near optimal results and achieve speedups of orders of magnitude over existing algorithms.",
On the maximum tolerable noise for reliable computation by formulas,"It is shown that if a formula is constructed from noisy 2-input NAND gates, with each gate failing independently with probability E, then reliable computation can or cannot take place according as /spl epsiv/ is less than or greater than /spl epsiv//sub 0/=(3-/spl radic/7)/4=0.08856....",
An empirical study of regression test selection techniques,"Regression testing is an expensive maintenance process directed at validating modified software. Regression test selection techniques attempt to reduce the cost of regression testing by selecting tests from a program's existing test suite. Many regression test selection techniques have been proposed. Although there have been some analytical and empirical evaluations of individual techniques, to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed. We conducted an experiment to examine the relative costs and benefits of several regression test selection techniques. The experiment examined five techniques for reusing tests, focusing on their relative abilities to reduce regression testing effort and uncover faults in modified programs. Our results highlight several differences between the techniques, and expose essential tradeoffs that should be considered when choosing a technique for practical application.",
Forwarding state reduction for sparse mode multicast communication,"Reducing forwarding state overhead of multicast routing protocols is an important issue towards a scalable global multicast solution. We propose a new approach, dynamic tunnel multicast, which utilizes dynamically established tunnels on unbranched links of a multicast distribution tree to eliminate unnecessary multicast forwarding states. Analysis and simulation results show promising reduction in the state overhead of sparse mode multicast routing protocols.",
Characterization of neuropathological shape deformations,"We present a framework for analyzing the shape deformation of structures within the human brain. A mathematical model is developed describing the deformation of any brain structure whose shape is affected by both gross and detailed physical processes. Using our technique, the total shape deformation is decomposed into analytic modes of variation obtained from finite element modeling, and statistical modes of variation obtained from sample data. Our method is general, and can be applied to many problems where the goal is to separate out important from unimportant shape variation across a class of objects. In this paper, we focus on the analysis of diseases that affect the shape of brain structures. Because the shape of these structures is affected not only by pathology but also by overall brain shape, disease discrimination is difficult. By modeling the brain's elastic properties, we are able to compensate for some of the nonpathological modes of shape variation. This allows us to experimentally characterize modes of variation that are indicative of disease processes. We apply our technique to magnetic resonance images of the brains of individuals with schizophrenia, Alzheimer's disease, and normal-pressure hydrocephalus, as well as to healthy volunteers. Classification results are presented.",
Image and video segmentation: the normalized cut framework,"We propose a segmentation system based on the normalized cut framework proposed by Shi and Malik (see Proc. IEEE Conf. Computer Vision and Pattern Recognition, San Juan, Puerto Rico, p.731-7, 1997). The goal is to partition the image from a big picture point of view. Perceptually significant groups are detected first while small variations and details are treated later. Different image features-intensity, color, texture, contour continuity, motion and stereo disparity are treated in one uniform framework.",
A cubist approach to object recognition,"We describe an appearance-based object recognition system using a keyed, multi-level contest representation reminiscent of certain aspects of cubist art. Specifically, we utilize distinctive intermediate-level features in this case automatically extracted 2-D boundary fragments, as keys, which are then verified within a local contest, and assembled within a loose global contest to evoke an overall percept. This system demonstrates good recognition of a variety of 3-D shapes, ranging from sports cars and fighter planes to snakes and lizards with full orthographic invariance. We report the results of large-scale tests, involving over 2000 separate test images, that evaluate performance with increasing number of items in the database, in the presence of clutter, background change, and occlusion, and also the results of some generic classification experiments where the system is tested on objects never previously seen or modeled. To our knowledge, the results we report are the best in the literature for full-sphere tests of general shapes with occlusion and clutter resistance.",
Adaptive normalization of handwritten characters using global/local affine transformation,"This paper introduces an adaptive or category-dependent normalization method that normalizes an input pattern against each reference pattern using global/local affine transformation (GAT/LAT) in a hierarchical manner as a general deformation model. Also, the normalization criterion is clearly defined as minimization of the mean of nearest-neighbor interpoint distances between each reference pattern and a normalized input pattern. Optimal GAT/LAT is determined by iterative application of weighted least-squares fitting techniques. Experiments using input patterns of 3,171 character categories, including Kanji, Kana, and alphanumerics, written by 36 people in the cursive style against square-style reference patterns show that the proposed method not only can absorb a fairly large amount of handwriting fluctuation within the same category, but the discrimination ability is greatly improved by the suppression of excessive normalization against similarly shaped but different categories. Furthermore, comparative results obtained by the conventional shape normalization method for preprocessing are presented.",
Specifying and compiling applications for RaPiD,"Efficient, deeply pipelined implementations exist for a wide variety of important computation-intensive applications, and many special-purpose hardware machines have been built that take advantage of these pipelined computation structures. While these implementations achieve high performance, this comes at the expense of flexibility. On the other hand, flexible architectures proposed thus far have not been very efficient. RaPiD is a reconfigurable pipelined datapath architecture designed to provide a combination of performance and flexibility for a variety of applications. It uses a combination of static and dynamic control to efficiently implement pipelined computations. This control, however, is very complicated; specifying a computation's control circuitry directly would be prohibitively difficult. This paper describes how specifications of a pipelined computation in a suitably high-level language are compiled into the control required to implement that computation in the RaPiD architecture. The compiler extracts a statically configured datapath from this description, identifies the dynamic control signals required to execute the computation, and then produces the control program and decoding structure that generates these dynamic control signals.",
Schedulability analysis for mode changes in flexible real-time systems,"One important requirement of many real-time systems is the ability to undergo several mutually exclusive modes of operation. By means of a mode change the system changes its functionality over time, thus being able to adapt to changing environmental situations. In order to successfully include mode changes in real-time systems, a mode change protocol with well known real-time behaviour is necessary. The authors provide a new model and related schedulability analysis for mode changes in flexible real-time systems.",
A fast LBG codebook training algorithm for vector quantization,"A fast codebook training algorithm based on the Linde, Buzo and Gray (1980) LBG algorithm is proposed. The fundamental goal of this method is to reduce the computation cost in the codebook training process. In this method, a kind of mean-sorted partial codebook search algorithm is applied to the closest codeword search. At the same time, a generalized integral projection model is developed for the generation of test conditions, which are used to speed up the search process in finding the closest codeword for each training vector. With this proposed method, a significant time reduction can be achieved by avoiding the computation of unnecessary codewords. Our simulation results show that a significant reduction in computation cost is obtained with this proposed method. Besides, this method provides a flexible way of selecting the test conditions to accommodate the different image training sets.",
Requirements development in scenario-based design,"We describe and analyze the process of requirements development in scenario based design through consideration of a case study. In our project, a group of teachers and system developers initially set out to create a virtual physics laboratory. Our design work centered on the collaborative development of a series of scenarios describing current and future classroom activities. We observed classroom scenarios to assess needs and opportunities, and envisioned future scenarios to specify and analyze possible design moves. We employed claims analysis to evaluate design trade-offs implicit in these scenarios, to codify the specific advantages and disadvantages in achieving requirements. Through the course of this process, the nature of our project requirements has evolved, providing more information but also more kinds of information. We discuss the utility of managing requirements development through an evolving set of scenarios, and the generality of the scenario stages from this case study.",
Efficient attribute-oriented generalization for knowledge discovery from large databases,"We present GDBR (Generalize DataBase Relation) and FIGR (Fast, Incremental Generalization and Regeneralization), two enhancements of Attribute Oriented Generalization, a well known knowledge discovery from databases technique. GDBR and FIGR are both O(n) and, as such, are optimal. GDBR is an online algorithm and requires only a small, constant amount of space. FIGR also requires a constant amount of space that is generally reasonable, although under certain circumstances, may grow large. FIGR is incremental, allowing changes to the database to be reflected in the generalization results without rereading input data. FIGR also allows fast regeneralization to both higher and lower levels of generality without rereading input. We compare GDBR and FIGR to two previous algorithms, LCHR and AOI, which are O(n log n) and O(np), respectively, where n is the number of input tuples and p the number of tuples in the generalized relation. Both require O(n) space that, for large input, causes memory problems. We implemented all four algorithms and ran empirical tests, and we found that GDBR and FIGR are faster. In addition, their runtimes increase only linearly as input size increases, while the runtimes of LCHR and AOI increase greatly when input size exceeds memory limitations.",
"Electronic contracting with COSMOS-how to establish, negotiate and execute electronic contracts on the Internet","Today, the Internet gains more and more attraction even for small companies to contact business partners and to automate cooperation between each other. However, the smaller the company the higher the relative setup costs that are required if the complete process of a commercial transaction is to be supported. We propose COSMOS as an Internet-based electronic contracting service that facilitates commercial partners with offer catalogues, a brokerage service, contract negotiation and signing as well as contract execution. The COSMOS architecture supports these functions in an integrated, unified way. The design and execution of contracts integrates patterns from the CORBA Joint Business Object Facility.",
Control design for integrator hybrid systems,"The authors define controllability for hybrid systems as the existence of correct control laws that transfer the hybrid plant between predefined subsets of the hybrid state space. A methodology for analyzing controllability and synthesizing control laws for a class of hybrid systems, applicable especially in batch control, is proposed. They use a framework consisting of a hybrid plant and a hybrid controller that interact in a feedback fashion.","Control design,
Control systems,
Automatic control,
Control system synthesis,
Automata,
Controllability,
Open loop systems,
Industrial control,
Differential equations,
Computer science"
Reliability simulation of component-based software systems,"Prevalent Markovian and semi Markovian methods to predict the reliability and performance of component based heterogeneous systems suffer from several limitations: they are subject to an intractably large state space for more complex scenarios, and they cannot take into account the influence of various parameters such as reliability growth of individual components, dependencies among components, etc., in a single model. Discrete event simulation offers an alternative to analytical models as it can capture a detailed system structure, and can be used to study the influence of different factors separately as well as in a combined fashion on dependability measures. We demonstrate the flexibility offered by discrete event simulation to analyze such complex systems through two case studies, one of a terminating application, and the other of a real time application with feedback control. We simulate the failure behavior of the terminating application with instantaneous as well as explicit repair. We also study the effect of having fault tolerant configurations for some of the components on the failure behavior of the application. In the second case of the real time application, we initially simulate the failure behavior of a single version taking into account its reliability growth. We also study the failure behavior of three fault tolerant systems: DRB, NVP and NSCP which are built from the individual versions of the real time application. Results demonstrate the flexibility offered by simulation to study the influence of various factors on the failure behavior of the applications for single as well as fault tolerant configurations.","Software systems,
Read only memory,
Electrical capacitance tomography,
Software reliability,
Computational modeling,
Educational institutions,
Computer science,
Reactive power,
Feedback,
Contracts"
A viewpoint determination system for stenosis diagnosis and quantification in coronary angiographic image acquisition,"This paper describes the usefulness of computer assistance in the acquisition of ""good"" images for stenosis diagnosis and quantification in coronary angiography. The system recommends the optimal viewpoints from which stenotic lesions can be observed clearly based on images obtained from initial viewpoints. First, the viewpoint dependency of the apparent severity of a stenotic lesion is experimentally analyzed using software phantoms in order to show the seriousness of the problem. The implementation of the viewpoint determination system is then described. The system provides good user-interactive tools for the semi-automated estimation of the orientation and diameter of stenotic segments and the three-dimensional (3-D) reconstruction of vessel structures. Using these tools, viewpoints that will not give rise to foreshortening and vessel overlap can be efficiently determined. Experiments using real coronary angiograms show the system to be capable of the reliable diagnosis and quantification of stenosis.",
Combining various solution techniques for dynamic fault tree analysis of computer systems,"Fault trees provide a conceptually simple modeling framework to represent system-level reliability in terms of interactions between component reliabilities. DIFtree (Dynamic Innovative Fault trees) effectively combines the best static fault tree solution technique (binary decision diagrams) with Markov solution techniques for dynamic fault trees. DIFtree includes advanced techniques for modeling coverage; coverage modeling has been shown to be critical to the analysis of fault-tolerant computer systems. DIFtree is based on a divide-and-conquer technique for modularizing the system-level fault tree into independent sub-trees; different solution techniques can be used for sub-trees. In this paper, we extend the DIFtree analysis capability to model several different distributions of time-to-failure, including fixed probabilities (no time component), exponential (constant hazard rate), Weibull (time-varying hazard rate) and log-normal. Our approach extends both the BDD and Markov analytical approaches and incorporates simulation as well.",
On-demand multicast in mobile wireless networks,"We propose an ""on-demand"" multicast routing protocol for a wireless, mobile, multihop network. The proposed scheme has two key features: (a) it is based on the forwarding group concept (i.e., a subset of nodes is in charge of forwarding the multicast packets via scoped flooding) rather than on the conventional multicast tree scheme (b) it dynamically refreshes the forward group members using a procedure akin to on-demand routing (hence the name). ""On-demand"" multicast is well suited to operate in an on-demand routing environment where routes are selectively computed as needed between communicating node pairs instead of being maintained and updated globally by a routing ""infrastructure"" (like in distance vector or link state, for example). On-demand multicast is particularly attractive in mobile, rapidly changing networks, where the traffic overhead caused by routing updates and tree reconfigurations may become prohibitive beyond a critical speed; and in large network with sparse traffic requirements, where the traffic, processing and storage overhead of the routing infrastructure solution compromises scalability. Via simulation, we compare on-demand multicast with a traditional tree multicast scheme, DVMRP, and with a version of forwarding group multicast which uses conventional distance vector routing instead of on-demand routing. This allows us to assess the penalty of the tree and of the global routing infrastructure as a function of mobility and sparseness.",
Local divergence of Markov chains and the analysis of iterative load-balancing schemes,"We develop a general technique for the quantitative analysis of iterative distributed load balancing schemes. We illustrate the technique by studying two simple, intuitively appealing models that are prevalent in the literature: the diffusive paradigm, and periodic balancing circuits (or the dimension exchange paradigm). It is well known that such load balancing schemes can be roughly modeled by Markov chains, but also that this approximation can be quite inaccurate. Our main contribution is an effective way of characterizing the deviation between the actual loads and the distribution generated by a related Markov chain, in terms of a natural quantity which we call the local divergence. We apply this technique to obtain bounds on the number of rounds required to achieve coarse balancing in general networks, cycles and meshes in these models. For balancing circuits, we also present bounds for the stronger requirement of perfect balancing, or counting.",
Combining multiple estimators of speaking rate,"We report progress in the development of a measure of speaking rate that is computed from the acoustic signal. The newest form of our analysis incorporates multiple estimates of rate; besides the spectral moment for a full-band energy envelope that we have previously reported, we also used pointwise correlation between pairs of compressed sub-band energy envelopes. The complete measure, called mrate, has been compared to a reference syllable rate derived from a manually transcribed subset of the Switchboard database. The correlation with transcribed syllable rate is significantly higher than our earlier measure; estimates are typically within 1-2 syllables/second of the reference syllable rate. We conclude by assessing the use of mrate as a detector for rapid speech.",
An efficient multipath forwarding method,"We motivate and formally define dynamic multipath routing and present the problem of packet forwarding in the multipath routing context. We demonstrate that for multipath sets that are suffix matched, forwarding can be efficiently implemented with (1) a per packet overhead of a small, fixed-length path identifier, and (2) router space overhead linear in K, the number of alternate paths between a source and a destination. We derive multipath forwarding schemes for suffix matched path sets computed by both de-centralized (link-state) and distributed (distance-vector) routing algorithms. We also prove that (1) distributed multipath routing algorithms compute suffix matched multipath sets, and (2) for the criterion of ranked k-shortest paths, decentralized routing algorithms also yield suffix matched multipath sets.",
Efficient discovery of functional and approximate dependencies using partitions,"Discovery of functional dependencies from relations has been identified as an important database analysis technique. We present a new approach for finding functional dependencies from large databases, based on partitioning the set of rows with respect to their attribute values. The use of partitions makes the discovery of approximate functional dependencies easy and efficient, and the erroneous or exceptional rows can be identified easily. Experiments show that the new algorithm is efficient in practice. For benchmark databases the running times are improved by several orders of magnitude over previously published results. The algorithm is also applicable to much larger datasets than the previous methods.",
Proportional share scheduling of operating system services for real-time applications,"While there is currently great interest in the problem of providing real time services in general purpose operating systems, the issue of real time scheduling of internal operating system activities has received relatively little attention. Without such real time scheduling, the system is susceptible to conditions such as receive livelock-a situation in which an operating system spends all its time processing arriving network packets, and application processes, even if scheduled with a real time scheduler, are starved. We investigate the problem of scheduling operating system activities such as network protocol processing in a proportional share manner. We describe a proportional share implementation of the FreeBSD operating system and demonstrate that it solves the receive livelock problem. Packets are processed within the operating system only at the cumulative rate at which the destination applications are prepared to receive them. If packets arrive at a faster rate then they are discarded after consuming minimal system resources. In this manner the performance of ""well behaved"" applications is unaffected by ""misbehaving"" applications. We demonstrate this effect by running a set of multimedia applications under a variety of network conditions on a set of increasingly sophisticated proportional share implementations of FreeBSD and comparing their performance. This work contributes to our knowledge of the engineering of proportional share real time systems.","Operating systems,
Real time systems,
Processor scheduling,
Resource management,
Protocols,
Network interfaces,
Computer interfaces,
Computer networks,
Application software,
Computer science"
A novel test methodology for core-based system LSIs and a testing time minimization problem,"In this paper, we propose a novel test methodology for core-based system LSIs. Our test methodology aims to decrease testing time for core-based system LSIs. Considering testing time reduction, our test methodology is based on BIST and ATPG. The main contributions of this paper are summarized as follows. (i). BIST is efficiently combined with external testing to relax the limitation of the external primary inputs and outputs. (ii). External testing for one of cores and BISTs for the others are performed in parallel to reduce the total testing time. (iii). The testing time minimization problem for core-based system LSIs is formulated as a combinatorial optimization problem to select the optimal set of test vectors from given sets of test vectors for each core.",
Color imaging for multimedia,"To a significant degree, multimedia applications derive their effectiveness from the use of color graphics, images, and video. However, the requirements for accurate color reproduction and for the preservation of this information across display and print devices that have very different characteristics and may be geographically apart are often not clearly understood. This paper describes the basics of color science, color input and output devices, color management, and calibration that help in defining and meeting these requirements.",
The ramp-up problem in software projects: a case study of how software immigrants naturalize,"Joining a software development team is like moving to a new country to start employment; the immigrant has a lot to learn about the job, the local customs, and sometimes a new language. In an exploratory case study, we interviewed four software immigrants, in order to characterize their naturalization process. Seven patterns in four major categories were found. In this paper, these patterns are substantiated, and their implications discussed. The lessons learned from this study can be applied equally to improving the naturalization process, and to the formulation of further research questions.",
Choosing good distance metrics and local planners for probabilistic roadmap methods,"This paper presents a comparative evaluation of different distance metrics and local planners within the content of probabilistic roadmap methods for motion planning. Both C-space and workspace distance metrics and local planners are considered. The study concentrates on cluttered 3D workspaces, typical of mechanical designs. Our results include recommendations for selecting appropriate combinations of distance metrics and local planners for use in motion planning methods, particularly probabilistic roadmap methods. We find that each local planner makes some connections than none of the others do ndicating that better connected roadmaps will be constructed using multiple local planners. We propose a new local planning method, we call rotate-at-s, that outperforms the common straight-line in C-space method in crowded environments.",
A hybrid collision avoidance method for mobile robots,"Proposes a hybrid approach to the problem of collision avoidance for indoor mobile robots. The /spl mu/DWA (model-based dynamic window approach) integrates sensor data from various sensors with information extracted from a map of the environment, to generate collision-free motion. A novel integration rule ensures that with high likelihood, the robot avoids collisions with obstacles not detectable with its sensors, even if it is uncertain about its position. The approach was implemented and tested extensively as part of an installation, in which a mobile robot gave interactive tours to visitors of the ""Deutsches Museum Bonn."" Here our approach was essential for the success of the entire mission, because a large number of ill-shaped obstacles prohibited the use of purely sensor-based methods for collision avoidance.",
Linear and neural models for classifying breast masses,"Computational methods can be used to provide an initial screening or a second opinion in medical settings and may improve the sensitivity and specificity of diagnoses. In the current study, linear discriminant models and artificial neural networks are trained to detect breast cancer in suspicious masses using radiographic features and patient age. Results on 139 suspicious breast masses (79 malignant, 60 benign, biopsy proven) indicate that a significant probability of detecting malignancies can be achieved at the risk of a small percentage of false positives. Receiver operating characteristic (ROC) analysis favors the use of linear models, however, a new measure related to the area under the ROC curve (A/sub Z/) suggests a possible benefit from hybridizing linear and nonlinear classifiers.","Artificial neural networks,
Breast cancer,
Diagnostic radiography,
Genetic programming,
Response surface methodology,
Convergence,
Biomedical imaging,
Medical diagnostic imaging,
Sensitivity and specificity,
Cancer detection"
Conceptual clustering in information retrieval,"Clustering is used in information retrieval systems to enhance the efficiency and effectiveness of the retrieval process. Clustering is achieved by partitioning the documents in a collection into classes such that documents that are associated with each other are assigned to the same cluster. This association is generally determined by examining the index term representation of documents or by capturing user feedback on queries on the system. In cluster-oriented systems, the retrieval process can be enhanced by employing characterization of clusters. In this paper, we present the techniques to develop clusters and cluster characterizations by employing user viewpoint. The user viewpoint is elicited through a structured interview based on a knowledge acquisition technique, namely personal construct theory. It is demonstrated that the application of personal construct theory results in a cluster representation that can be used during query as well as to assign new documents to the appropriate clusters.",
Precise segmentation of the lateral ventricles and caudate nucleus in MR brain images using anatomically driven histograms,"This paper demonstrates a time-saving, automated method that helps to segment the lateral ventricles and caudate nucleus in T1-weighted coronal magnetic resonance (MR) brain images of normal control subjects. The method involves choosing intensity thresholds by using anatomical information and by locating peaks in histograms. To validate the method, the lateral ventricles and caudate nucleus were segmented in three brain scans by four experts, first using an established method involving isointensity contours and manual editing, and second using automatically generated intensity thresholds as an aid to the established method. The results demonstrate both time savings and increased reliability.",
"Verification, validation, and accreditation","This paper presents guidelines for conducting verification, validation, and accreditation (VV&A) of M&S applications. Fifteen guiding principles are introduced to help the researchers, practitioners and managers better comprehend what VV&A is all about. The VV&A activities are described in two M&S life cycles. Applicability of 77 V&V techniques is shown for the major stages of the two M&S life cycles. A methodology for accreditation of M&S applications is briefly introduced.",
Visual homing: surfing on the epipoles,"We introduce a novel method for visual homing. Using this method a robot can be sent to desired positions and orientations in 3-D space specified by single images taken from these positions. Our method determines the path of the robot on-line. The starting position of the robot is not constrained, and a 3-D model of the environment is not required. The method is based on recovering the epipolar geometry relating the current image taken by the robot and the target image. Using the epipolar geometry, most of the parameters which specify the differences in position and orientation of the camera between the two images are recovered. However, since not all of the parameters can be recovered from two images, we have developed specific methods to bypass these missing parameters and resolve the ambiguities that exist. We present two homing algorithms for two standard projection models, weak and full perspective. We have performed simulations and real experiments which demonstrate the robustness of the method and that the algorithms always converge to the target pose.",
Quantum cryptography with imperfect apparatus,"Quantum key distribution, first proposed by C.H. Bennett and G. Brassard (1984), provides a possible key distribution scheme whose security depends only on the quantum laws of physics. So far the protocol has been proved secure even under channel noise and detector faults of the receiver but is vulnerable if the photon source used is imperfect. In this paper we propose and give a concrete design for a new concept, self-checking source, which requires the manufacturer of the photon source to provide certain tests; these tests are designed such that, if passed, the source is guaranteed to be adequate for the security of the quantum key distribution protocol, even though the testing devices may not be built to the original specification. The main mathematical result is a structural theorem which states that, for any state in a Hilbert space, if certain EPR-type equations are satisfied, the state must be essentially the orthogonal sum of EPR pairs.",
Execution characteristics of desktop applications on Windows NT,"This paper examines the performance of desktop applications running on the Microsoft Windows NT operating system on Intel x86 processors, and contrasts these applications to the programs in the integer SPEC95 benchmark suite. We present measurements of basic instruction set and program characteristics, and detailed simulation results of the way these programs use the memory system and processor branch architecture. We show that the desktop applications have similar characteristics to the integer SPEC95 benchmarks for many of these metrics, However compared to the integer SPEC95 applications, desktop applications have larger instruction working sets, execute instructions in a greater number of unique functions, cross DLL boundaries frequently, and execute a greater number of indirect calls.",
Dynamic load balancing in parallel discrete event simulation for spatially explicit problems,"We present a dynamic load balancing algorithm for parallel discrete event simulation of spatially explicit problems. In our simulations, the space is discretized and divided into subareas, each of which is simulated by a logical process (LP). Load predictions are done based on the future events that are scheduled for a given LP. The information about the load of the processes is gathered and distributed during the Global Virtual Time calculation. Each LP calculates the new load distribution of the system. The load is then balanced by moving spatial data between neighboring LPs in one round of communications. In our problems, the LPs should be described as being elements of a ring from the point of view of communication. Due to the spatial characteristics, the load can be migrated only between neighboring LPs. We present an algorithm that performs the load balancing in a ring and minimizes the maximum after-balance load.",
The evolution of cybergenres,"The combination of the computer and the Internet has resulted in the emergence of cybergenre, a new class of genre. Cybergenre can be characterized by the triple , where functionality refers to the capabilities afforded by this new medium. When an existing genre initially migrates to this new medium, it is usually as a faithful reproduction of the existing genre in both content and form with little new functionality. It may then evolve into a variant cybergenre as it incorporates functionality afforded by the computer and Internet. Cybergenres also include novel genres, either not based on previously existing genres or substantially different from existing genres on the basis of increased functionality. These novel genres may have either persistent or virtual instantiations. This paper proposes a taxonomy of these cybergenres and examines the evolution of the news cybergenre and the mathematics dictionary cybergenre within the context of this taxonomy.",
Resistance of digital watermarks to collusive attacks,"In digital watermarking (also called digital fingerprinting), extra information is embedded imperceptibly into digital content (such as an audio track, a still image, or a movie). This extra information can be read by authorized parties, and other users attempting to remove the watermark cannot do so without destroying the value of the content by making perceptible changes to the content. This provides a disincentive to copying by allowing copies to be traced to their original owner. Unlike cryptography, digital watermarking provides protection to content that is in the clear. It is not easy to design watermarks that are hard to erase, especially if an attacker has access to several differently marked copies of the same base content. Cox et al. (see IEEE Trans. on Image Processing, vol.6, no.12, p.1673-87, 1997) have proposed the use of additive normally distributed values as watermarks, and have sketched an argument showing that, in a certain theoretical model, such watermarks are resistant to collusive attacks. Here, we fill in the mathematical justification for this claim.",
Optimal clock period clustering for sequential circuits with retiming,"In this paper we consider the problem of clustering sequential circuits subject to a bound on the area of each cluster, with the objective of minimizing the clock period. Current algorithms address combinational circuits only, and treat a sequential circuit as a special case, by removing all flip-flops (FF's) and clustering the combinational part of the sequential circuit. This approach breaks the signal dependencies and assumes the positions of FF's are fixed. The positions of the FF's in a sequential circuit are in fact dynamic, because of retiming. As a result, current algorithms can only consider a small portion of the whole solution space. In this paper, we present a clustering algorithm that does not segment circuits by removing FF's. In additional, it considers the effect of retiming. The algorithm can produce clustering solutions with the optimal clock period under the unit delay model. For the general delay model, it can produce clustering solutions with a clock period provably close to optimal.",
Probabilistic Interpretation of Population Codes,"We present a general encoding-decoding framework for interpreting the activity of a population of units. A standard population code interpretation method, the Poisson model, starts from a description as to how a single value of an underlying quantity can generate the activities of each unit in the population. In casting it in the encoding-decoding framework, we find that this model is too restrictive to describe fully the activities of units in population codes in higher processing areas, such as the medial temporal area. Under a more powerful model, the population activity can convey information not only about a single value of some quantity but also about its whole distribution, including its variance, and perhaps even the certainty the system has in the actual presence in the world of the entity generating this quantity. We propose a novel method for forming such probabilistic interpretations of population codes and compare it to the existing method.",
How the learning of rule weights affects the interpretability of fuzzy systems,"Neuro-fuzzy systems have recently gained a lot of interest in research and applications. These are approaches that learn fuzzy systems from data. Many of them use rule weights for this task. In this paper we discuss the influence of rule weights on the interpretability of fuzzy systems. We show how rule weights can be equivalently replaced by modifications in the membership functions of a fuzzy system. We elucidate the effects rule weights have on a fuzzy rule base. Using our neuro-fuzzy model NEFCLASS we demonstrate the problems of using rule weights in a simple example, and we show that learning in fuzzy systems can be done without them.",
Computer-aided diagnostic system for diffuse liver diseases with ultrasonography by neural networks,"The aim of the study is to establish a computer-aided diagnostic system for diffuse liver diseases such as chronic active hepatitis (CAH) and liver cirrhosis (LC). The authors introduced an artificial neural network in the classification of these diseases. In this system the neural network was trained by feature parameters extracted from B-mode ultrasonic images of normal liver (NL), CAH and LC. For input data the authors used six parameters calculated by a region of interest (ROI) and a parameter calculated by five ROIs in each image. They were variance of pixel values, coefficient of variation, annular Fourier power spectrum, longitudinal Fourier power spectrum which were calculated for the ROI, and variation of the means of the five ROIs. In addition, the authors used two more parameters calculated from a co-occurrence matrix of pixel values in the ROI. The results showed that the neural network classifier was 83.8% in sensitivity for LC, 90.0% in sensitivity for CAH and 93.6% in specificity, and the system was considered to be helpful for clinical and educational use.","Computer networks,
Liver diseases,
Ultrasonography,
Neural networks,
Artificial neural networks,
Ultrasonic imaging,
Workstations,
Pixel,
Computer displays,
Biomedical imaging"
Maximizing sets and fuzzy Markoff algorithms,"A fuzzy algorithm is an ordered set of fuzzy instructions that upon execution yield an approximate solution to a given problem. Two unrelated aspects of fuzzy algorithms are considered in this paper. The first is concerned with the problem of maximization of a reward function. It is argued that the conventional notion of a maximizing value for a function is not sufficiently informative and that a more useful notion is that of a maximizing set. Essentially, a maximizing set serves to provide information not only concerning the point or points at which a function is maximized, but also about the extent to which the values of the reward function approximate to its supremum at other points in its range. The second is concerned with the formalization of the notion of a fuzzy algorithm. In this connection, the notion of a fuzzy Markoff algorithm is introduced and illustrated by an example. It is shown that the generation of strings by a fuzzy algorithm bears a resemblance to a birth-and-death process and that the execution of the algorithm terminates when no more ""live"" strings are left.",
Graph structured views and their incremental maintenance,"Studies the problem of maintaining materialized views of graph structured data. The base data consists of records containing identifiers of other records. The data could represent traditional objects (with methods, attributes and a class hierarchy), but it could also represent a lower-level data structure. We define simple views and materialized views for such graph structured data, analyzing options for representing record identity and references in the view. We develop incremental maintenance algorithms for these views.",
Data-driven homologue matching for chromosome identification,"Karyotyping involves the visualization and classification of chromosomes into standard classes. In ""normal"" human metaphase spreads, chromosomes occur in homologous pairs for the autosomal classes 1-22, and X chromosome for females. Many existing approaches for performing automated human chromosome image analysis presuppose cell normalcy, containing 46 chromosomes within a metaphase spread with two chromosomes per class. This is an acceptable assumption for routine automated chromosome image analysis. However, many genetic abnormalities are directly linked to structural or numerical aberrations of chromosomes within the metaphase spread. Thus, two chromosomes per class cannot be assumed for anomaly analysis. This paper presents the development of image analysis techniques which are extendible to detecting numerical aberrations evolving from structural abnormalities. Specifically, an approach to identifying ""normal"" chromosomes from selected class(es) within a metaphase spread is presented. Chromosome assignment to a specific class is initially based on neural networks, followed by banding pattern and centromeric index criteria checking, and concluding with homologue matching. Experimental results are presented comparing neural networks as the sole classifier to the authors' homologue matcher for identifying class 17 within normal and abnormal metaphase spreads.",
Extracting line representations of sulcal and gyral patterns in MR images of the human brain,"This paper describes automatic procedures for extracting sulcal and gyral patterns from magnetic resonance (MR) images of the human brain. Specifically, the authors present three algorithms for the extraction of gyri, sulci, and sulcal fundi. These algorithms yield highly condensed line representations which can be used to describe the individual properties of the neocortical surface. The algorithms consist of a sequence of image analysis steps applied directly to the volumetric image data without requiring intermediate data representations such as surfaces or three-dimensional renderings. Previous studies have mostly focused on the extraction of surface representations, rather than line representations of cortical structures. The authors believe that line representations provide a valuable alternative to surface representations.",
Archetypal source code searches: a survey of software developers and maintainers,"We have conducted a survey to generate archetypes of source code searching by programmers across maintenance tasks. Using a questionnaire on a web page, we obtained 69 responses from readers of 7 newsgroups. Respondents were asked about their source code searching habits: what tools they used, why they searched, and what they searched for. The four most common search targets were function definitions, all uses of a function, variable definitions, and all uses of a variable. The most common search motivations were defect repair, code reuse, program understanding, feature addition, and impact analysis. Eleven archetypes were generated from the anecdotes and results. The implications and practical applications of these findings and method are discussed.",
Automatic text location in images and video frames,"Automatic text location (without character recognition capabilities) deals with extracting image regions that contain text only. The images of these regions can then be fed to an optical character recognition module or highlighted for users. This is very useful in a number of applications such as database indexing and converting paper documents to their electronic versions. The performance of our automatic text location algorithm is shown in several applications. Compared with some traditional text location methods, our method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy.",
Practical pushing planning for rearrangement tasks,"We address the problem of practical manipulation planning for rearrangement tasks of many movable objects. We study a special case of the rearrangement task, where the only allowed manipulation is pushing. We search for algorithms that can provide practical planning time for most common scenarios. We present a hierarchical classification of manipulation problems into several classes, each characterized by properties of the plans that can solve it. Such a classification allows one to consider each class individually, to analyze and exploit properties of each class, and to suggest individual planning methods accordingly. Following this classification, we suggest algorithms for two of the defined classes. Both items have been tested in a simulated environment, with up to 32 movable objects and 66 combined DOF. We present the simulations results as well as some experimental results using a real platform.",
Mobile code security,"Sandboxes, code signing, firewalls, and proof carrying code are all techniques that address the inherent security risks of mobile code. The article summarizes the relative merits of each. It is concluded that each of these techniques offers something different, and the best approach is probably a combination of security mechanisms. The sandbox and code signing approaches are already being hybridized. Combining these with firewalling techniques such as the playground gives an extra layer of security. The PCC approach is not yet ready for prime time, but the ability to prove safety properties of code is an important element in the arsenal available for securing mobile code. None of the techniques can do much to protect users from social engineering attacks, where a user is somehow fooled into revealing something they shouldn't reveal. For example, JavaScript can be employed in a way that fools a user into revealing passwords to a remote server. Java applets could be used to do this as well, even under the strictest security policy. User education is the only way to combat mobile code attacks that are based on social engineering.",
A direct-execution framework for fast and accurate simulation of superscalar processors,"Multiprocessor system evaluation has traditionally been based on direct-execution based Execution-Driven Simulations (EDS). In such environments, the processor component of the system is not fully modeled. With wide issue superscalar processors being the norm in today's multiprocessor nodes, there is an urgent need for modeling the processor accurately. However, using direct execution to model a superscalar processor has been considered an open problem. Hence, current approaches model the processor by interpreting the application executable. Unfortunately, this approach can be slow. In this paper, we propose a novel direct-execution framework that allows accurate simulation of wide-issue superscalar processors without the need for code interpretation. This is achieved with the aid of an Interface Window between the front-end and the architectural simulator, that buffers the necessary information. This eliminates the need for full-fledged instruction emulation. Overall, this approach enables detailed yet fast EDS of superscalar processors. Finally, we evaluate the framework and show good performance for uni- and multiprocessor configurations.","Multiprocessing systems,
Computational modeling,
Instruments,
Application software,
Interleaved codes,
Computer simulation,
Contracts,
Discrete event simulation,
Computer science,
Emulation"
Bounds on mixed binary/ternary codes,Upper and lower bounds are presented for the maximal possible size of mixed binary/ternary error-correcting codes. A table up to length 13 is included. The upper bounds are obtained by applying the linear programming bound to the product of two association schemes. The lower bounds arise from a number of different constructions.,
Efficient admission control of piecewise linear traffic envelopes at EDF schedulers,"We present algorithms for flow admission control at an earliest deadline first link scheduler when the flows are characterized by piecewise linear traffic envelopes. We show that the algorithms have very low computational complexity and, thus, practical applicability. The complexity can be further decreased by introducing the notion of discretized admission control. Through discretization, the range of positions for the end points of linear segments of the traffic envelopes is restricted to a finite set. Simulation experiments show that discretized admission control can lead to two orders of magnitude decrease in the amount of computation needed to make admission control decisions over that incurred when using exact (nondiscrete) admission control, with the additional benefit that this amount of computation no longer depends on the number of flows. We examine the relative performance degradation (in terms of the number of flows admitted) incurred by the discretization and find that it is small.","Admission control,
Piecewise linear techniques,
Delay,
Traffic control,
Protocols,
Asynchronous transfer mode,
Sufficient conditions,
Scheduling algorithm,
Processor scheduling,
Computer science"
A lightweight genetic block-matching algorithm for video coding,"A lightweight genetic search algorithm (LGSA) is proposed. Different evolution schemes are investigated, such that the control overheads are largely reduced. It is also shown that the proposed LGSA can be viewed as a novel expansion of the three-step search algorithm (TSS). It can be seen from the simulation results that the performance of LGSA is very similar to that of the full search algorithm (FSA), and the computational complexity is much lower than that of FSA and other previously proposed genetic motion estimation algorithms.",
Code churn: a measure for estimating the impact of code change,"This study presents a methodology that will produce a viable fault surrogate. The focus of the effort is on the precise measurement of software development process and product outcomes. Tools and processes for the static measurement of the source code have been installed and made operational in a large embedded software system. Source code measurements have been gathered unobtrusively for each build in the software evolution process. The measurements are synthesized to obtain the fault surrogate. The complexity of sequential builds is compared and a new measure, code churn, is calculated. This paper demonstrates the effectiveness of code complexity churn by validating it against the testing problem reports.",
On the constructions of constant-weight codes,"Two methods of constructing binary constant-weight codes from (1) codes over GF(q) and (2) constant-weight codes over GF(q) are presented. Several classes of binary optimum constant-weight codes are derived from these methods. In general, we show that binary optimum constant-weight codes, which achieve the Johnson bound, can be constructed from optimum codes over GF(q) which achieve the Plotkin bound. Finally, several classes of optimum constant-weight codes over GF(q) are constructed.",
"1-way quantum finite automata: strengths, weaknesses and generalizations","We study 1-way quantum finite automata (QFAs). First, we compare them with their classical counterparts. We show that, if an automaton is required to give the correct answer with a large probability (greater than 7/9), then any 1-way QFAs can be simulated by a 1-way reversible automaton. However, quantum automata giving the correct answer with smaller probabilities are more powerful than reversible automata. Second, we show that 1-way QFAs can be very space-efficient. We construct a 1-way QFA that is exponentially smaller than any equivalent classical (even randomized) finite automaton. We think that this construction may be useful for design of other space-efficient quantum algorithms. Third, we consider several generalizations of 1-way QFAs. Here, our goal is to find a model which is more powerful than 1-way QFAs keeping the quantum part as simple as possible.",
Quantum lower bounds by polynomials,"We examine the number T of queries that a quantum network requires to compute several Boolean functions on {0,1}/sup N/ in the black-box model. We show that, in the black-box model, the exponential quantum speed-up obtained for partial functions (i.e. problems involving a promise on the input) by Deutsch and Jozsa and by Simon cannot be obtained for any total function: if a quantum algorithm computes some total Boolean function f with bounded-error using T black-box queries then there is a classical deterministic algorithm that computes f exactly with O(T/sup 6/) queries. We also give asymptotically tight characterizations of T for all symmetric f in the exact, zero-error, and bounded-error settings. Finally, we give new precise bounds for AND, OR, and PARITY. Our results are a quantum extension of the so-called polynomial method, which has been successfully applied in classical complexity theory, and also a quantum extension of results by Nisan about a polynomial relationship between randomized and deterministic decision tree complexity.",
On model checking for non-deterministic infinite-state systems,"We demonstrate that many known algorithms for model checking infinite-state systems can be derived uniformly from a reachability procedure that generates a ""covering graph"", a generalization of the Karp-Miller graph for Petri Nets. Each node of the covering graph has an associated non-empty set of reachable states, which makes it possible to model check safety properties of the system on the covering graph. For systems with a well-quasi-ordered simulation relation, each infinite fair computation has a finite witness, which may be detected using the covering graph and combinatorial properties of the specific infinite state system. These results explain many known decidability results in a simple, uniform manner. This is a strong indication that the covering graph construction is appropriate for the analysis of infinite state systems. We also consider the new application domain of parameterized broadcast protocols, and indicate how to apply the construction in this domain. This application is illustrated on an invalidation-based cache coherency protocol, for which many safety properties can be proved fully automatically for an arbitrary number of processes.",
ScalParC: a new scalable and efficient parallel classification algorithm for mining large datasets,"We present ScalParC (Scalable Parallel Classifier), a new parallel formulation of a decision tree based classification process. Like other state-of-the-art decision tree classifiers such as SPRINT, ScalParC is suited for handling large datasets. We show that existing parallel formulation of SPRINT is unscalable, whereas ScalParC is shown to be scalable in both runtime and memory requirements. We present the experimental results of classifying up to 6.4 million records on up to 128 processors of Cray T3D, in order to demonstrate the scalable behavior of ScalParC. A key component of ScalParC is the parallel hash table. The proposed parallel hashing paradigm can be used to parallelize other algorithms that require many concurrent updates to a large hash table.",
Techniques for Speculative Run-Time Parallelization of Loops,"This paper presents a set of new run-time tests for speculative parallelization of loops that defy parallelization based on static analysis alone. It presents a novel method for speculative array privatization that is not only more efficient than previous methods when the speculation is correct, but also does not require rolling back the computation in case the variable is found not to be privatizable. We present another method for speculative parallelization which can overcome all loop-carried anti and output dependences, with even lower overhead than previous techniques which could not break such dependences. Again, in order to ameliorate the problem of paying a heavy penalty for speculatively parallelizing loops that turn out to be serial, we present a technique that enables early detection of loop-carried dependences. Our experimental results from a preliminary implementation of these tests on an IBM G30 SMP machine show a significant reduction in the penalty paid for mis-speculation, from roughly 50% to between 2% and 18% of the serial execution time. For parallel loops, we obtain about the same, and often, even better performance relative to the previous methods, making our techniques extremely attractive.",
Architecture of the dedicated short-range communications (DSRC) protocol,"This paper describes the architecture and functional aspects of the dedicated short-range communications (DSRC) protocol. The DSRC provides a communication link between vehicles and roadside beacons for road transport and traffic telematics (RTTT) applications. The DSRC technology was proposed for standardization in Europe (CEN TC 278) and ENV-standards were developed. An overview of the three DSRC communication layers is given, with a special focus on the functional aspects provided by the application layer. Finally, a presentation of the European Telematics Applications Programme research project VASCO, which deals with the validation of the DSRC ENV-standards, concludes this paper.",
Fuzzy switched hybrid systems-modeling and identification,"The combination of hybrid systems and fuzzy multiple model systems is described. Further, a hierarchical identification of the resulting fuzzy switched hybrid system is outlined. The behavior of the discrete component is identified by black box fuzzy clustering and subsequent parameter identification taking into account some prior-knowledge about the discrete states. The identification of the continuous models for each discrete state is done based on local linear fuzzy models.",
Probabilistic mapping of an environment by a mobile robot,"This paper addresses the problem of building large-scale maps of indoor environments with mobile robots. It proposes a statistical approach that describes the map building problem as a constrained maximum-likelihood estimation problem, for which it devises a practical algorithm. Experimental results in large, cyclic environments illustrate the appropriateness of the approach.",
An operator interaction framework for visualization systems,"Information visualization encounters a wide variety of different data domains. The visualization community has developed representation methods and interactive techniques. As a community, we have realized that the requirements in each domain are often dramatically different. In order to easily apply existing methods, researchers have developed a semiology of graphic representations. We have extended this research into a framework that includes operators and interactions in visualization systems, such as a visualization spreadsheet. We discuss properties of this framework and use it to characterize operations spanning a variety of different visualization techniques. The framework developed in the paper enables a new way of exploring and evaluating the design space of visualization operators, and helps end users in their analysis tasks.",
Distributed packet rewriting and its application to scalable server architectures,"To construct high performance Web servers, system builders are increasingly turning to distributed designs. An important challenge that arises in such designs is the need to direct incoming connections to individual hosts. Previous methods for connection routing (layer 4 switching) have employed a centralized node to handle all incoming requests. In contrast, we propose a distributed approach, called distributed packet rewriting (DPR), in which all hosts of the distributed system participate in connection routing. DPR promises better scalability and fault-tolerance than the current practice of using centralized special-purpose connection routers. We describe the implementation of four variants of DPR and compare their performance. We show that DPR provides performance comparable to centralized alternatives, measured in terms of throughput and delay. Also, we show that DPR enhances the scalability of Web server clusters by eliminating the performance bottleneck exhibited when centralized connection routing techniques are utilized.",
Plasma effects on electron beam focusing and microwave emission in a virtual cathode oscillator,The effect of anode and cathode plasmas on the electron beam dynamics in a virtual cathode oscillator is investigated. A cathode plasma is formed immediately after the rise of the electron beam current and is followed by an anode plasma. The anode plasma formation occurs well before beam focusing and microwave emission. Each plasma expands in the diode region with approximately the speed of 2.0 cm//spl mu/s. The electron beam current in the diode region is well characterized by the electron space-charge-limited current in bipolar flow with expanding plasmas in the anode-cathode gap. Particle-in-cell computer simulation reveals that in the presence of anode plasma the annular electron beam is focused down to small radius while oscillating between a real and a virtual cathode. These simulation results agree qualitatively with X-ray measurements of the electron beam current density profile across the anode. Such a focused beam is found to be responsible for the formation of a strong virtual cathode and microwave emission.,
Image guidance of endovascular interventions on a clinical MR scanner,"Magnetic resonance imaging (MRI) offers potential advantages over conventional X-ray techniques for guiding and evaluating vascular interventions. Image guidance of such interventions via passive catheter tracking requires real-time image processing. Commercially available MR scanners currently do not provide this functionality. This paper describes an image processing environment that allows near-real-time MR-guided vascular interventions. It demonstrates (1) that flexibility can be achieved by separating the scanner and the image processing/display system, thereby preserving the stability of the scanner and (2) that sufficiently rapid visualization can be achieved by low-cost workstations equipped with graphics hardware. The setup of the hardware and the software is described in detail. Furthermore, image processing techniques are presented for guiding the interventionalist through simple vascular anatomy. Finally, results of a phantom balloon angioplasty experiment are presented.",
Coupling metrics for object-oriented design,"We describe and evaluate some recently innovated coupling metrics for object-oriented (OO) design. The Coupling Between Objects (CBO) metric of Chidamber and Kemerer (1991) is evaluated empirically using five OO systems, and compared with an alternative OO design metric called SAS, which measures the number of associations between a class and its peers. The NAS metric is directly collectible from design documents such as the Object Model of OMT. Results from all systems studied indicate a strong relationship between CBO and NAS, suggesting that they are not orthogonal. We hypothesised that coupling would be related to understandability, the number of errors and error density. So relationships were found for any of the systems between class understandability and coupling. Only limited evidence was found to support our hypothesis linking increased coupling to increased error density. The work described in this paper is part of the 'Metrics for OO Programming Systems' (MOOPS) project, which aims to evaluate existing OO metrics, and to innovate and evaluate new OO analysis and design metrics, aimed specifically at the early stages of development.",
Selective eager execution on the PolyPath architecture,"Control-flow misprediction penalties are a major impediment to high performance in wide-issue superscalar processors. In this paper we present Selective Eager Execution (SEE), an execution model to overcome mis-speculation penalties by executing both paths after diffident branches. We present the micro-architecture of the PolyPath processor which is an extension of an aggressive superscalar out-of-order architecture. The PolyPath architecture uses a novel instruction tagging and register renaming mechanism to execute instructions from multiple paths simultaneously in the same processor pipeline, while retaining maximum resource availability for single-path code sequences. Results of our execution-driven, pipeline-level simulations show that SEE can improve performance by as much as 36% for the go benchmark, and an average of 14% on SPECint95, when compared to a normal superscalar, out-of-order speculative execution, monopath processor. Moreover our architectural model is both elegant and practical to implement, using a small amount of additional state and control logic.",
A comparison of server-based and receiver-based local recovery approaches for scalable reliable multicast,"Local recovery approaches for reliable multicast have the potential to provide significant performance gains in terms of reduced bandwidth and delay, and higher system throughput. In this paper we examine two local recovery approaches-one server-based, and the other receiver-based, and compare their performance. The server-based approach makes use of specially designated hosts, called repair servers, co-located with routers inside the network. In the receiver-based approach, only the end hosts (sender and receivers) are involved in error recovery. Using analytical models, we first show that the two local recovery approaches yield significantly higher protocol throughput and lower bandwidth usage than an approach that does not use local recovery. Next, we demonstrate that server-based local recovery yields higher protocol throughput and lower bandwidth usage than receiver-based local recovery when the repair servers have processing power slightly higher than that of a receiver and several hundred kilobytes of buffer per multicast session.",
"SableCC, an object-oriented compiler framework","In this paper, we introduce SableCC, an object-oriented framework that generates compilers (and interpreters) in the Java programming language. This framework is based on two fundamental design decisions. Firstly, the framework uses object-oriented techniques to automatically build a strictly-typed abstract syntax tree that matches the grammar of the compiled language which simplifies debugging. Secondly, the framework generates tree-walker classes using an extended version of the visitor design pattern which enables the implementation of actions on the nodes of the abstract syntax tree using inheritance. These two design decisions lead to a tool that supports a shorter development cycle for constructing compilers. To demonstrate the simplicity of the framework, we present all the steps of building an interpreter for a mini-BASIC language. This example could be easily modified to provide on embedded scripting language in an application. We also provide a brief description of larger systems that have been implemented using the SableCC tool. We conclude that the use of object-oriented techniques significantly reduces the length of the programmer written code, can shorten the development time and finally, makes the code easier to read and maintain.",
Device visualization for interventional MRI using local magnetic fields: basic theory and its application to catheter visualization,"This paper addresses one of the major problems in interventional magnetic resonance imaging (MRI): the visualization of interventional devices. For visualization locally induced magnetic fields are used, which disturb the homogeneity of the main magnetic field of the MR scanner. This results in signal loss in the vicinity of the device due to intravoxel dephasing, and leads to a disturbance of the phase image. The local fields are established by a low current in a closed copper loop along the device. This method is introduced as a means for catheter visualization. The basic theory behind this method is presented. Simulations are performed to determine the effect of intravoxel dephasing, without interfering effects like susceptibility or radio-frequency artifacts. Scanned and simulated data is used to verify the theoretical consideration. Different configurations of wire loops are discussed and two types of catheter visualization scans are proposed. Results from a pig study show that this methods holds promise for intravascular interventions under MRI guidance.",
A resource query interface for network-aware applications,"Development of portable network-aware applications demands an interface to the network that allows an application to obtain information about its execution environment. The paper motivates and describes the design of Remos, an API that allows network-aware applications to obtain relevant information. The major challenges in defining a uniform interface are network heterogeneity, diversity in traffic requirements, variability of the information, and resource sharing in the network. Remos addresses these issues with two abstraction levels, explicit management of resource sharing, and statistical measurements. The flows abstraction captures the communication between nodes, and the topologies abstraction provides a logical view of network connectivity. Remos measurements are made at network level, and therefore information to manage sharing of resources is available. Remos is designed to deliver best effort information to applications, and it explicitly adds statistical reliability and variability measures to the core information. The paper also presents preliminary results and experience with a prototype Remos implementation for a high speed IP based network testbed.",
Exploring video structure beyond the shots,"While existing shot-based video analysis approaches provide users with better access to the video than the raw data stream does, they are still not sufficient for meaningful video browsing and retrieval, since: (1) the shots in a long video are still too many to be presented to the user; and (2) shots do not capture the underlying semantic structure of the video, based on which the user may wish to browse/retrieve the video. To explore video structure at the semantic level this paper presents an effective approach for video scene structure construction, in which shots are grouped into semantic-related scenes. The output of the proposed algorithm provides a structured video that greatly facilitates user's access. Experiments based on real-world movie videos validate the effectiveness of the proposed approach.","Layout,
Gunshot detection systems,
Motion pictures,
Modular construction,
Streaming media,
Feature extraction,
Histograms,
Data engineering,
Computer science,
Information retrieval"
New sufficient conditions for absolute stability of neural networks,"The main result obtained in this paper is that for a neural network with interconnection matrix T, if -T is quasi-diagonally row-sum or column-sum dominant, then the network system is absolutely stable. The above two sufficient conditions for absolute stability are independent of the existing sufficient ones in the literature. Under either of the above two sufficient conditions for absolute stability, the vector field defined by the network system is also structurally stable.",
Distinguishing congestion losses from wireless transmission losses: a negative result,"The TCP is a popular transport protocol used in the present-day Internet. When packet losses occur the TCP assumes that the packet losses are due to congestion, and responds by reducing its congestion window. When a TCP connection traverses a wireless link, a significant fraction of packet losses may occur due to transmission errors. The TCP responds to such losses also by reducing the congestion window. This results in unnecessary degradation in the TCP performance. We define a class of functions named loss predictors which may be used by a TCP sender to guess the actual cause of a packet loss (congestion or transmission error) and take appropriate actions. These loss predictors use simple statistics on round-trip times and/or throughput, to determine the cause of a packet loss. We investigate their ability to determine the cause of a packet loss. Unfortunately, our simulation measurements suggest that the three loss predictors do not perform too well.",
Markov decision processes and regular events,Desirable properties of the infinite histories of a finite-state Markov decision process are specified in terms of a finite number of events represented as /spl omega/-regular sets. An infinite history of the process produces a reward which depends on the properties it satisfies. The authors investigate the existence of optimal policies and provide algorithms for the construction of such policies.,
Distributed QoS routing with imprecise state information,"The goal of quality-of-service (QoS) routing is to find a network path which has sufficient resources to satisfy certain constraints on delay, bandwidth and/or other metrics. The network state information maintained at every node is often imprecise in a dynamic environment because of nonnegligible propagation delay of state messages, periodic updates due to overhead concern, and hierarchical state aggregation. The information imprecision makes QoS routing difficult. The traditional shortest-path routing algorithm does not provide satisfactory performance with imprecise state information. We propose a distributed routing scheme, called ticket-based probing, which searches multiple paths in parallel for a satisfactory one. The scheme is designed to work with imprecise state information. It allows the dynamic trade-off between the routing performance and the overhead. The state information of intermediate nodes is collectively used to guide the routing messages along the most appropriate paths in order to maximize the success probability. The proposed algorithm consider not only the QoS requirements but also the cost optimality of the routing path. Extensive simulations show that our algorithm achieve high call-admission ratio and low-cost routing paths with modest overhead. The algorithm can tolerate high degree of information imprecision.",
MMPacking: a load and storage balancing algorithm for distributed multimedia servers,"In distributed multimedia servers where client requests for different video streams may have different probabilities, placement of video streams is an important parameter because it may result in unbalanced requests to the system's stations, and thus to high blocking probabilities of requests. We present a method, MMPacking, to balance traffic load and storage use in a distributed server environment. Since different video streams are requested by clients with different rates, video stream replication is used to balance the traffic patterns of the stations; thus, the requests and I/O usage of the stations are balanced, since replication allows requests for the same video stream to be routed to different stations. MMPacking achieves load balancing by producing at most N-1 replicas of video streams in a system with N servers. These replicas are distributed among the stations so that storage balancing is achieved as well, since no station stores more than two video streams more than any other station in the system.",
Performance prediction in production environments,"Accurate performance predictions are difficult to achieve for parallel applications executing on production distributed systems. Conventional point-valued performance parameters and prediction models are often inaccurate since they can only represent one point in a range of possible behaviors. The authors address this problem by allowing characteristic application and system data to be represented by a set of possible values and their probabilities, which they call stochastic values. They give a practical methodology for using stochastic values as parameters to adaptable performance prediction models. They demonstrate their usefulness for a distributed SOR application, showing stochastic values to be more effective than single (point) values in predicting the range of application behavior that can occur during execution in production environments.",
Ring-projection-wavelet-fractal signatures: a novel approach to feature extraction,"In this brief, we present a novel approach to optical character recognition that utilizes ring-projection-wavelet-fractal signatures (RP-WFS). In particular, the proposed approach reduces the dimensionality of a 2-D pattern by way of a ring-projection method and, thereafter, performs Daubechies' wavelet transformation on the derived 1-D pattern to generate a set of wavelet transformation subpatterns, namely, curves that are nonself-intersecting. Further, from the resulting nonself-intersecting curves, the divider dimensions are readily computed. These divider dimensions constitute a new feature vector for the original 2-D pattern, defined over the curves' fractal dimensions. We have conducted several experiments in which a set of printed alphanumeric symbols of varying fonts and orientation were classified, based on the formulation of our new feature vector. The results obtained from these experiments have consistently shown the character recognition approach with the proposed feature vector can yield an excellent classification rate of 100%.",
An efficient Fourier method for 3-D radon inversion in exact cone-beam CT reconstruction,"The radial derivative of the three-dimensional (3-D) radon transform of an object is an important intermediate result in many analytically exact cone-beam reconstruction algorithms. The authors briefly review Grangeat's (1991) approach for calculating radon derivative data from cone-beam projections and then present a new, efficient method for 3-D radon inversion, i.e., reconstruction of the image from the radial derivative of the 3-D radon transform, called direct Fourier inversion (DFI). The method is based directly on the 3-D Fourier slice theorem. From the 3-D radon derivative data, which is assumed to be sampled on a spherical grid, the 3-D Fourier transform of the object is calculated by performing fast Fourier transforms (FFTs) along radial lines in the radon space. Then, an interpolation is performed from the spherical to a Cartesian grid using a 3-D gridding step in the frequency domain. Finally, this 3-D Fourier transform is transformed back to the spatial domain via 3-D inverse FFT. The algorithm is computationally efficient with complexity in the order of N/sup 3/ log N. The authors have done reconstructions of simulated 3-D radon derivative data assuming sampling conditions and image quality requirements similar to those in medical computed tomography (CT).",
Multiple-description wavelet based image coding,"We consider the problem of image coding for communication systems that use diversity to overcome channel impairments. We focus on the special case in which there are two channels of equal capacity between a transmitter and a receiver. Our designs are based on a combination of techniques successfully applied to the construction of some of the most efficient wavelet based image coding algorithms, with multiple description scalar quantizers (MDSQs). For a given image, we produce two bitstreams, to be transmitted over each channel. Should one of the channels fail, each individual description guarantees a minimum image quality specified by the user. However, if both descriptions arrive at destination, they are combined to produce a higher quality image than that achievable based on individual descriptions. We formulate a discrete optimization problem, whose solution gives parameters of the proposed encoder yielding optimal performance in an operational sense. Simulation results are presented.",
Solving constraint satisfaction problems using hybrid evolutionary search,"We combine the concept of evolutionary search with the systematic search concepts of arc revision and hill climbing to form a hybrid system that quickly finds solutions to static and dynamic constraint satisfaction problems (CSPs). Furthermore, we present the results of two experiments. In the first experiment, we show that our evolutionary hybrid outperforms a well-known hill climber, the iterative descent method (IDM), on a test suite of 750 randomly generated static CSPs. These results show the existence of a ""mushy region"" which contains a phase transition between CSPs that are based on constraint networks that have one or more solutions and those based on networks that have no solution. In the second experiment, we use a test suite of 250 additional randomly generated CSPs to compare two approaches for solving CSPs. In the first method, all the constraints of a CSP are known by the hybrid at run-time. We refer to this method as the static method for solving CSPs. In the second method, only half of the constraints of a CSPs are known at run-time. Each time that our hybrid system discovers a solution that satisfies all of the constraints of the current network, one additional constraint is added. This process of incrementally adding constraints is continued until all the constraints of a CSP are known by the algorithm or until the maximum number of individuals has been created. We refer to this second method as the dynamic method for solving CSPs. Our results show hybrid evolutionary search performs exceptionally well in the presence of dynamic (incremental) constraints, then also illuminate a potential hazard with solving dynamic CSPs.",
Approximating a finite metric by a small number of tree metrics,"Y. Bartal (1996, 1998) gave a randomized polynomial time algorithm that given any n point metric G, constructs a tree T such that the expected stretch (distortion) of any edge is at most O (log n log log n). His result has found several applications and in particular has resulted in approximation algorithms for many graph optimization problems. However approximation algorithms based on his result are inherently randomized. In this paper we derandomize the use of Bartal's algorithm in the design of approximation algorithms. We give an efficient polynomial time algorithm that given a finite n point metric G, constructs O(n log n) trees and a probability distribution /spl mu/ on them such that the expected stretch of any edge of G in a tree chosen according to /spl mu/ is at most O(log n log log n). Our result establishes that finite metrics can be probabilistically approximated by a small number of tree metrics. We obtain the first deterministic approximation algorithms for buy-at-bulk network design and vehicle routing; in addition we subsume results from our earlier work on derandomization. Our main result is obtained by a novel view of probabilistic approximation of metric spaces as a deterministic optimization problem via linear programming.",
The Fast Illinois Solver Code: requirements and scaling properties,"To solve large scale computing and scattering problems and to expand knowledge about iterative solvers, the authors developed the Fast Illinois Solver Code, which uses the multilevel fast multipole algorithm (MLFMA). They discuss FISC's memory requirements and CPU time and give some empirically derived formulas and charts. The authors also plot examples they used to obtain these conclusions.",
On 3-D surface reconstruction using shape from shadows,"In this paper we discuss new results on the Shape From Darkness problem: using the motion of cast shadows to recover scene structure. Our approach is based on collecting a set of images from a fixed viewpoint as a known light source mover; ""across the sky"". Previously published solutions to this problem have performed the reconstruction only for cross sections of the scene. In this paper, we present a reconstruction algorithm and discuss the reconstruction of an entire 3-D scene under various light source trajectories. We also consider the constraints on reconstruction. We conclude with experimental results that illustrate the convergence properties of the solution process and its robustness properties.",
Measurements and simulations of VUV emissions from plasma flat panel display pixel microdischarges,"This paper reports on measurements of the principal vacuum ultra-violet emission lines from micro discharges operating with helium/xenon gas mixture used in full color plasma driven flat panel display pixels. The principal emission lines observed are the 147 and 129 nm lines from atomic xenon transitions and the relatively broad emissions from xenon dimers centered near 173 nm. We report on the changing intensities of these lines with variation in xenon concentration in the pixel gas mixtures, which affect the overall luminous efficiency of the display. A one-dimensional computer model has been used to simulate the discharge evolution. The model tracks the populations of twelve different representative quantum energy levels of the helium and xenon atoms, as well as the production and decay of the xenon dimers. The atomic physics description is sufficiently detailed to allow prediction of the relative intensities of the dominant emission lines. We find that model predicted intensities for xenon atomic and dimer emission lines agree well with experimental measurements.",
Cooperative caching of dynamic content on a distributed Web server,"We propose a new method for improving the average response time of Web servers by cooperatively caching the results of requests for dynamic content. The work is motivated by our recent study of access logs from the Alexandria Digital Library server at UCSB, which demonstrates that approximately a 30 percent decrease in average response time could be achieved by caching dynamically generated content. We have developed a distributed Web server called Swala, in which the nodes cooperatively cache the results of CGI requests, and the cache meta-data is stored in a replicated global cache directory. Our experiments show that the single-node performance of Swala without caching is comparable to the Netscape Enterprise server, that considerable speedups are obtained using caching, and that the cache hit ratio is substantially higher with cooperative cache than with stand-alone cache.","Cooperative caching,
Web server,
Delay,
Computer science,
Read only memory,
AC generators,
Web sites,
Explosives,
Load management,
Bandwidth"
Probabilistic noninterference in a concurrent language,"The authors previously give a type system that guarantees that well-typed multi-threaded programs are possibilistically noninterfering. If thread scheduling is probabilistic, however, then well-typed programs may have probabilistic timing channels. They describe how they can be eliminated without making the type system more restrictive. They show that well-typed concurrent programs are probabilistically noninterfering if every total command with a high guard executes atomically. The proof uses the concept of a probabilistic state of a computation, following the work of Kozen (1981).",
Bandwidth allocation strategies for wireless ATM networks using predictive reservation,"In future wireless networks such as the wireless ATM networks, it is important to guarantee quality-of-service (QOS) for mobile terminals. However, guaranteeing QOS while ensuring high bandwidth utilization is difficult because of the unpredictable nature of the terminal mobility pattern. We propose a new scheme to improve the QOS of wireless ATM networks by using predictive reservation of bandwidth from mobility patterns dynamically. Simulation results show that under heavy load conditions, our proposed scheme will reduce the hand-off dropping probabilities by a significant amount while resulting in only a small increase in the blocking probabilities and a minimal sacrifice in bandwidth utilization.",
Efficient construction of minimum-redundancy codes for large alphabets,"We consider the problem of calculating minimum-redundancy codes for alphabets in which there is significant repetition of symbol weights. On a sorted-by-weight alphabet of, n symbols and r distinct symbol weights we show that a minimum-redundancy prefix code can be constructed in O(r+r log(n/r)) time, and that a minimum redundancy L-bit length-limited prefix code can be constructed in O(Lr+Lrlog(n/r)) time. When r is small relative to n-which is necessarily the case for most practical coding problems on large alphabets-these bounds represent a substantial improvement upon the best previous algorithms for these two problems, which consumed O(n) time and O(nL) time, respectively. The improved algorithms are also space-efficient.",
The cascaded predictor: economical and adaptive branch target prediction,"Two-level predictors improve branch prediction accuracy by allowing predictor tables to hold multiple predictions per branch. Unfortunately, the accuracy of such predictors is impaired by two detrimental effects. Capacity misses increase since each branch may occupy many entries, depending on the number of different path histories leading up to the branch. The working set of a given program therefore increases with history length. Similarly, cold start misses increase with history length since the predictor must first store a prediction separately for each history pattern before it can predict branches with that history. We describe a new hybrid predictor architecture, cascaded branch prediction, which can alleviate both of these effects while retaining the superior accuracy of two level predictors. Cascaded predictors dynamically classify and predict easily predicted branches using an inexpensive predictor, preventing insertion of these branches into a more powerful second stage predictor. We show that for path-based indirect branch predictors, cascaded prediction obtains prediction rates equivalent to that of two-level predictors at approximately one fourth the cost. For example, a cascaded predictor with 64+1024 entries achieves the same prediction accuracy as a 4096-entry two-level predictor. Although we have evaluated cascaded prediction only on indirect branches, we believe that it could also improve conditional branch prediction and value prediction.",
Dynamic aggregation with circular visual designs,"One very effective method for managing large data sets is aggregation or binning. We consider two aggregation methods that are tightly coupled with interactive manipulation and the visual representation of the data. Through this integration we hope to provide effective support for the aggregation process, specifically by enabling: 1) automatic aggregation, 2) continuous change and control of the aggregation level, 3) spatially based aggregates, 4) context maintenance across different aggregate levels, and 5) feedback on the level of aggregation.",
Towards a completeness result for model checking of security protocols,"Model checking approaches to the analysis of security protocols have proved remarkably successful. The basic approach is to produce a model of a small system running the protocol, together with a model of the most general intruder who can interact with the protocol, and then to use a state exploration tool to search for attacks. This has led to a number of new attacks upon protocols being discovered. However if no attack is found, this only tells one that there is no attack upon the small system modelled; there may be an attack upon some larger system. This is the question considered in the paper: the author presents sufficient conditions on the protocol and its environment such that if there is no attack upon a particular small system (with one honest agent for each role of the protocol) leading to a breach of secrecy (using a fairly strong definition of secrecy), then there is no attack on any larger system leading to a breach of secrecy (using a more general definition of secrecy).",
Hidden Markov model based continuous online gesture recognition,"Presents the extension of an existing vision-based gesture recognition system using hidden Markov models(HMMs). Several improvements have been carried out in order to increase the capabilities and the functionality of the system. These improvements include position independent recognition, rejection of unknown gestures, and continuous online recognition of spontaneous gestures. We show that especially the latter requirement is highly complicated and demanding, if we allow the user to move in front of the camera without any restrictions and to perform the gestures spontaneously at any arbitrary moment. We present solutions to this problem by modifying the HMM-based decoding process and by introducing online feature extraction and evaluation methods.",
Results concerning the bandwidth of subliminal channels,"In conjunction with a six-month research program on computer security, cryptology and coding theory hosted by the Isaac Newton Institute of Mathematical Sciences, University of Cambridge, Cambridge, U.K., a Workshop on information hiding was held from May 30 through June 1, 1996. This workshop was devoted to all aspects of information hiding - other than the usual cryptographic concealment of content, including steganography, subliminal channels, fingerprinting, covert channels, etc. Two surprising results pertaining to subliminal channels were presented or grew out of presentations made at this workshop. One is of interest to the secure communications protocol designer concerned with denying the use of subliminal channels, while the other is important to the designer, or user, of subliminal channels. The first raises the question of whether the notion of a ""subliminal-free"" communication channel is an oxymoron, i.e., is it possible to force the bandwidth of the subliminal channel to be truly zero? The second forces a more precise formulation of a conjecture the author had made that the bandwidth of a subliminal channel is logarithmically limited if the transmitter is unwilling to trust the subliminal receiver unconditionally. Motivated by these results, this paper reexamines the fundamental questions of the bandwidth available for subliminal communication as a function of the trust the transmitter has in the subliminal receiver and of a logically sound interpretation of the term ""subliminal-free"".",
Agent orientated annotation in model based visual surveillance,"The paper presents an agent based surveillance system for use in monitoring scenes involving both pedestrians and vehicles. The system supplies textual descriptions for the dynamic activity occurring in the 3D world. These are derived by means of dynamic and probabilistic inference based on geometric information provided by a vision system that tracks vehicles and pedestrians. The symbolic scene annotation is given at two major levels of description: the object level and the inter-object level. At object level, each tracked pedestrian or vehicle is assigned a behaviour agent which uses a Bayesian network to infer the fundamental features of the objects' trajectory, and continuously updates its textual description. The inter-object interaction level is interpreted by a situation agent which is created dynamically when two objects are in close proximity. In the work included here the situation agent can describe a two-object interaction in terms of basic textual annotations, to summarise the dynamics of the local action.",
Computation with Infinite Neural Networks,"For neural networks with a wide class of weight priors, it can be shown that in the limit of an infinite number of hidden units, the prior over functions tends to a gaussian process. In this article, analytic forms are derived for the covariance function of the gaussian processes corresponding to networks with sigmoidal and gaussian hidden units. This allows predictions to be made efficiently using networks with an infinite number of hidden units and shows, somewhat paradoxically, that it may be easier to carry out Bayesian prediction with infinite networks rather than finite ones.",
The design and architecture of the Microsoft Cluster Service-a practical approach to high-availability and scalability,"Microsoft Cluster Service (MSCS) extends the Windows NT operating system to support high-availability services. The goal is to offer an execution environment where off-the-shelf server applications can continue to operate, even in the presence of node failures. Later versions of MSCS will provide scalability via a node and application management system which allows applications to scale to hundreds of nodes. In this paper we provide a detailed description of the MSCS architecture and the design decisions that have driven the implementation of the service. The paper also describes how some major applications use the MSCS features, and describes features added to make it easier to implement and manage fault-tolerant applications on MSCS.",
On submesh allocation for mesh multicomputers: a best-fit allocation and a virtual submesh allocation for faulty meshes,"The submesh allocation problem is to recognize and locate a free submesh that can accommodate a request for a submesh of a specified size. In this paper, we propose a new best-fit submesh allocation strategy for mesh-connected multiprocessor systems. The proposed strategy maintains and uses a free submesh list for an efficient allocation. For an allocation request, the strategy selects the best-fit submesh which causes the least amount of potential processor fragmentation. As many large free submeshes as possible are preserved for later allocations. For this purpose, we introduce a novel function quantifying the degree of potential fragmentation of submeshes. The proposed strategy has the capability of recognizing a complete submesh. We also propose an allocation strategy for faulty meshes which can maintain and allocate virtual submeshes derived from faulty submeshes. Extensive simulation is carried out to compare the proposed strategy with previous strategies. The proposed strategy has the best performance: a 6-50 percent improvement over the previous best strategy.","Multitasking,
Computer Society,
Topology,
Multiprocessing systems,
Computer architecture,
Concurrent computing,
Prototypes,
Character recognition,
Computer science"
The lane-curvature method for local obstacle avoidance,"The lane-curvature method (LCM) presented in this paper is a new local obstacle avoidance method for indoor mobile robots. The method combines curvature-velocity method (CVM) with a new directional method called the lane method. The lane method divides the environment into lanes, and then chooses the best lane to follow to optimize travel along a desired heading. A local heading is then calculated for entering and following the best lane, and CVM uses this heading to determine the optimal translational and rotational velocities, considering the heading direction, physical limitations, and environmental constraints. By combining both the directional and velocity space methods, LCM yields safe collision-free motion as well as smooth motion taking the dynamics of the robot into account.",
Time-shift scheduling-fair scheduling of flows in high-speed networks,"We present a scheduling protocol, called time-shift scheduling, to forward packets from multiple input flows to a single output channel. Each input flow is guaranteed a predetermined packet rate and an upper bound on packet delay. The protocol is an improvement over existing protocols because it satisfies the properties of rate-proportional delay, fairness, and efficiency, while existing protocols fail to satisfy at least one of these properties. In time-shift scheduling each flow is assigned an increasing timestamp, and the packet chosen for transmission is taken from the flow with the least timestamp. The protocol features the novel technique of time shifting, in which the scheduler's real-time clock is adjusted to prevent flow timestamps from increasing faster than the real-time clock. This bounds the difference between any pair of flow timestamps, thus ensuring the fair scheduling of flows.",
The fusion calculus: expressiveness and symmetry in mobile processes,"We present the fusion calculus as a significant step towards a canonical calculus of concurrency. It simplifies and extends the /spl pi/-calculus. The fusion calculus contains the polyadic /spl pi/-calculus as a proper subcalculus and thus inherits all its expressive power. The gain is that fusion contains actions akin to updating a shared state, and a scoping construct for bounding their effects. Therefore it is easier to represent computational models such as concurrent constraints formalisms. It is also easy to represent the so called strong reduction strategies in the /spl lambda/-calculus, involving reduction under abstraction. In the /spl lambda/-calculus these tasks require elaborate encodings. Our results on the fusion calculus in this paper are the following. We give a structured operational semantics in the traditional style. The novelty lies in a new kind of action, fusion actions for emulating updates of a shared state. We prove that the calculus contains the /spl pi/-calculus as a subcalculus. We define and motivate the bisimulation equivalence and prove a simple characterization of its induced congruence, which is given two versions of a complete axiomatization for finite terms. The expressive power of the calculus is demonstrated by giving a straight-forward encoding of the strong lazy /spl lambda/-calculus, which admits reduction under /spl lambda/ abstraction.",
Design of accurate and smooth filters for function and derivative reconstruction,"The correct choice of function and derivative reconstruction filters is paramount to obtaining highly accurate renderings. Most filter choices are limited to a set of commonly used functions, and the visualization practitioner has so far no way to state his preferences in a convenient fashion. Much work has been done towards the design and specification of filters using frequency based methods. However for visualization algorithms it is more natural to specify a filter in terms of the smoothness of the resulting reconstructed function and the spatial reconstruction error. Hence, the authors present a methodology for designing filters based on spatial smoothness and accuracy criteria. They first state their design criteria and then provide an example of a filter design exercise. They also use the filters so designed for volume rendering of sampled data sets and a synthetic test function. They demonstrate that their results compare favorably with existing methods.",
Cyclic allocation of two-dimensional data,"Various proposals have been made for declustering 2D tiled data on multiple I/O devices. Strictly optimal solutions only exist under very restrictive conditions on the tiling of the 2D space or for very few I/O devices. In this paper, we explore allocation methods where no strictly optimal solution exists. We propose a general class of allocation methods, referred to as cyclic allocation methods, and show that many existing methods are instances of this class. As a result, various seemingly ad hoc and unrelated methods are presented in a single framework. Furthermore, the framework is used to develop new allocation methods that give better performance than any previous method and that approach the best feasible performance.",
Enabling large-scale simulations: selective abstraction approach to the study of multicast protocols,"Due to the complexity and scale of the current Internet, large scale simulation is an increasingly important tool to evaluate network protocol design. Parallel and distributed simulation is one appropriate approach to the simulation scalability problem, but it can require expensive hardware and have high overhead. We investigate a complementary solution-simulation abstraction. Just as a custom simulator includes only details necessary for the task at hand, a general simulator can support configurable levels of detail for different simulations. We demonstrate two abstraction techniques in multicast simulations and show that they each help to gain one order of magnitude in performance. Although abstraction simulations are not identical to more detailed simulations, in many cases these differences are small and result in minimal changes in the conclusions drawn from simulations.",
Review and summary of a silicon micromachined gas chromatography system,"A miniature gas chromatography (GC) system has been designed and fabricated using silicon micromachining and integrated circuit (IC) processing techniques. The silicon micromachined gas chromatography system (SMGCS) is composed of a miniature sample injector that incorporates a 10 μl sample loop; a 0.9-m long, rectangular-shaped (300 μm width and 10 μm height) capillary column coated with a 0.2-μm thick copper phthalocyanine (CuPc) stationary-phase; and a dual-detector scheme based upon a CuPc-coated chemiresistor and a commercially available, 125-μm diameter thermal conductivity detector (TCD) bead. Silicon micromachining was employed to fabricate the interface between the sample injector and the GC column, the column itself, and the dual-detector cavity. A novel IC thin-film processing technique was developed to sublime the CuPc stationary-phase coating on the column walls that were micromachined in the host silicon wafer substrate and Pyrex cover plate, which were then electrostatically bonded together. The SMGCS can separate binary gas mixtures composed of parts-per-million (ppm) concentrations of ammonia (NH3) and nitrogen dioxide (NO2) when isothermally operated (55–80 °C). With a helium carrier gas and nitrogen diluent, a 10 μl sample volume containing ammonia and nitrogen dioxide injected at 40 psi (2.8 × 105 Pa) can be separated in less than 30 min.",
Accurate indirect branch prediction,"Indirect branch prediction is likely to become increasingly important in the fixture because indirect branches occur more frequently in object-oriented programs. With misprediction rates of around 25% on current processors, indirect branches can incur a significant fraction of branch misprediction overhead even though they remain less frequent than the more predictable conditional branches. We investigate a wide range of two-level predictors dedicated exclusively to indirect branches. Starting with predictors that useful-precision addresses and unlimited tables, we progressively introduce hardware constraints and minimize the loss of predictor performance at each step. For programs from the SPECint95 suite as well as a suite of large C++ applications, a two-level predictor achieves a misprediction rate of 9.8% with a 1 K-entry table and 7.3% with an 8 K-entry table, representing more than a threefold improvement over an ideal BTB. A hybrid predictor further reduces the misprediction rates to 8.98% (1 K) and 5.95% (8 K).",
What you see is what you test: a methodology for testing form-based visual programs,"Form-based visual programming languages, which include commercial spreadsheets and various research systems, have had a substantial impact on end-user computing. Research shows, however, that form-based visual programs often contain faults. We would like to provide at least some of the benefits of formal testing methodologies to the creators of these programs. This paper presents a testing methodology for form-based visual programs. To accommodate the evaluation model used with these programs, and the interactive process by which they are created, our methodology is validation driven and incremental. To accommodate the users of these languages, We provide an interface to the methodology that does not require an understanding of testing theory. We discuss our implementation of this methodology and empirical results achieved in its use.",
Which problems have strongly exponential complexity?,"For several NP-complete problems, there have been a progression of better but still exponential algorithms. In this paper we address the relative likelihood of sub-exponential algorithms for these problems. We introduce a generalized reduction which we call sub-exponential reduction family (SERF) that preserves sub-exponential complexity. We show that Circuit-SAT is SERF-complete for all NP-search problems, and that for any fixed k, k-SAT, k-Colorability, k-Set Cover Independent Set, Clique, Vertex Cover are SERF-complete for the class SNP of search problems expressible by second order existential formulas whose first order part is universal. In particular, sub-exponential complexity for any one of the above problems implies the same for all others. We also look at the issue of proving strongly exponential lower bounds (that is, bounds of the form 2/sup /spl Omega/(n)/) for AC/sup 0/. This problem is even open far depth-3 circuits. In fact, such a bound for depth-3 circuits with even limited (at most n/sup /spl epsiv//) fan-infer bottom-level gates would imply a nonlinear size lower bound for logarithmic depth circuits. We show that with high probability even degree 2 random GF(2) polynomials require strongly exponential site for /spl Sigma//sub 3//sup k/ circuits for k=o(loglogn). We thus exhibit a much smaller space of 2(0(/sup n2/)) functions such that almost every function in this class requires strongly exponential size /spl Sigma//sub 3//sup k/ circuits. As a corollary, we derive a pseudorandom generator (requiring O(n/sup 2/) bits of advice) that maps n bits into a larger number of bits so that computing parity on the range is hard for /spl Sigma//sub 3//sup k/ circuits. Our main technical lemma is an algorithm that, for any fixed /spl epsiv/>0, represents an arbitrary k-CNF formula as a disjunction of 2/sup /spl epsiv/n/ k-CNF formulas that are sparse, e.g., each having O(n) clauses.",
Incremental update on sequential patterns in large databases,"Mining of sequential patterns in a transactional database is time consuming due to its complexity. Maintaining present patterns is a non-trivial task after database update, since appended data sequences may invalidate old patterns and create new ones. In contrast to re-mining, the key to improve mining performance in the proposed incremental update algorithm is to effectively utilize the discovered knowledge. By counting over appended data sequences instead of the entire updated database in most cases, fast filtering of patterns found in last mining and successive reductions in candidate sequences together make efficient update on sequential patterns possible.",
Home-based SVM protocols for SMP clusters: Design and performance,"As small-scale shared memory multiprocessors proliferate in the market, it is very attractive to construct large-scale systems by connecting smaller multiprocessors together in software using efficient commodity, network interfaces and networks. Using a shared virtual memory (SVM) layer for this purpose preserves the attractive shared memory programming abstraction across nodes. In this paper: We describe home-based SVM protocols that support symmetric multiprocessor (SMP) nodes, taking advantage of the intra-node hardware cache coherence and synchronization mechanisms. Our protocols take no special advantage of the network interface and network except as a fast communication link, and as such are very portable. We present the key design tradeoffs, discuss our choices, and describe key data structures that enable us to implement these choices quite simply. We present an implementation on a network of 4-way Intel PentiumPro SMPs interconnected with Myrinet, and provide performance results. We explore the advantages of SMP nodes over uniprocessor nodes with this protocol, as well as other performance tradeoffs, through both real implementation and simulation as appropriate, since both have important roles to play. We find one approach to deliver good parallel performance on many real applications (at least at the scale we examine) and to improve performance over SVM across uniprocessor nodes.",
Circuit optimization using carry-save-adder cells,"Carry-save-adder (CSA) is the most often used type of operation in implementing a fast computation of arithmetics of register-transfer-level design in industry. This paper establishes a relationship between the properties of arithmetic computations and several optimizing transformations using CSAs to derive consistently better qualities of results than those of manual implementations. In particular, we introduce two important concepts, operation duplication and operation split, which are the main driving techniques of our algorithm for achieving an extensive utilization of CSAs. Experimental results from a set of typical arithmetic computations found in industry designs indicate that automating CSA optimization with our algorithm produces designs with up to 53% faster timing and up to 42% smaller area.",
Dynamic hammock predication for non-predicated instruction set architectures,"Conventional speculative architectures use branch prediction to evaluate the most likely execution path during program execution. However certain branches are difficult to predict. One solution to this problem is to evaluate both paths following such a conditional branch. Predicated execution can be used to implement this form of multi-path execution. Predicated architectures fetch and issue instructions that have associated predicates. These predicates indicate if the instruction should commit its result. Predicating a branch reduces the number of branches executed, eliminating the chance of branch misprediction at the cost of executing additional instructions. In this paper, we propose a restricted form of multi-path execution called Dynamic Predication for architectures with little or no support for predicated instructions in their instruction set. Dynamic predication dynamically predicates instruction sequences in the form of a branch hammock concurrently executing both paths of the branch. A branch hammock is a short forward branch that spans a few instructions in the form of an if-then or if-then-else construct we mark these and other constructs in the executable. When the decode stage detects such a sequence, it passes a predicated instruction sequence to a dynamically scheduled execution core. Our results show that dynamic predication can accrue speedups of up to 13%.","Computer architecture,
Computer science,
Microcomputers,
Costs,
Decoding,
Degradation,
Accuracy,
Dynamic scheduling,
Instruction sets"
Increasing TLB reach using superpages backed by shadow memory,"The amount of memory that can be accessed without causing a translation lookaside buffer (TLB) fault, the reach of a TLB, is failing to keep pace with the increasingly large working sets of applications. We propose to extend TLB reach via a novel Memory Controller TLB (MTLB) that lets us aggressively create superpages from non-contiguous, unaligned regions of physical memory. This flexibility increases the OS's ability to use superpages on arbitrary application data. The MTLB supports shadow pages, regions of physical address space for which the MTLB remaps accesses to ""real"" physical pages. The MTLB preserves per-base-page referenced and dirty bits, which enables the OS to swap shadow-backed superpages a page at a time, unlike conventional superpages. Simulation of five applications, including two SPECint95 benchmarks, demonstrated that a modest-sized MTLB improves performance of applications with moderate-to-high TLB miss rates by 5-20%. Simulation also showed that this mechanism can more than double the effective reach of a processor TLB with no modification to the processor MMU.",
Program slicing in understanding of large programs,"Program slicing transforms a large program into a smaller one that contains only statements relevant to the computation of a given function. It has been shown that program slicing can be useful in program understanding. Traditionally, program slices are represented in the textual form. Although slicing does narrow the size of the program, the textual representation of a slice does not provide much guidance in the understanding of large programs. In this paper we present program slicing concepts on the module level that allow for better understanding of program slices of large programs and their executions. These concepts have been developed for static and dynamic program slicing and are combined with different methods of visualization to guide programmers in the process of program understanding. The presented concepts have been implemented in the slicing tool that is used to investigate the usefulness of these concepts in the process of understanding of large programs.",
Adaptive shared tree multicast in mobile wireless networks,"Shared tree multicast is a well established concept used in several multicast protocols for wireline networks (e.g. core base tree, PIM sparse mode etc). In this paper, we extend the shared tree concept to wireless, mobile, multihop networks for applications ranging from ad hoc networking to disaster recovery and battlefield. The main challenge in wireless, mobile networks is the rapidly changing environment. We address this issue in our design by: (a) using ""soft state""; (b) assigning different roles to nodes depending on their mobility (two level mobility model); (c) proposing an adaptive scheme which combines shared tree and source tree benefits. A detailed wireless simulation model is used to evaluate the proposed schemes and compare them with source based tree (as opposed to shared tree) multicast. The results show that shared tree protocols have low overhead and are very robust to mobility.",
The NICE project: learning together in a virtual world,"This paper describes the NICE project, an immersive learning environment for children implemented in the CAVE and related multi-user virtual reality (VR) technologies. The NICE project provides an engaging setting where children construct and cultivate simple virtual ecosystems, collaborate via networks with other remotely-located children, and create stories from their interactions in the real and virtual world.",
"Testing using log file analysis: tools, methods, and issues","Large software systems often keep log files of events. Such log files can be analyzed to check whether a run of a program reveals faults in the system. We discuss how such log files can be used in software testing. We present a framework for automatically analyzing log files, and describe a language for specifying analyzer programs and an implementation of that language. The language permits compositional, compact specifications of software, which act as test oracles; we discuss the use and efficacy of these oracles for unit- and system-level testing in various settings. We explore methodological issues such as efficiency and logging policies, and the scope and limitations of the framework. We conclude that testing using log file analysis constitutes a useful methodology for software verification, somewhere between current testing practice and formal verification methodologies.",
Watermarking methods for MPEG encoded video: towards resolving rightful ownership,"Various digital watermarking techniques have been proposed in recent years as the methods to protect the copyright of multimedia data. However, the rightful ownership problem has not been properly solved. Currently, there are two proposals to solve the ownership problem. Unfortunately, one proposal lacks a formal proof and the other can be easily defeated. We present watermarking methods which will be successful in resolving rightful ownership of watermarked MPEG video. By introducing specific requirements in the watermark construction, the proposed scheme is proved to be non invertible. We show the invertibility of those schemes which do not use the original in its verification, discuss in depth various issues of the watermark construction requirements, extend the basic idea to create a non invertible scheme for image, and discuss the usage of different non invertible schemes.","Watermarking,
Streaming media,
Hip,
Multimedia systems,
Reactive power,
Protection,
Proposals,
Video compression,
Computer science,
Data security"
Communication metrics for software development,"Presents empirical evidence that metrics on communication artifacts generated by groupware tools can be used to gain significant insight into the development process that produced them. We describe a test-bed for developing and testing communication metrics, a senior-level software engineering project course at Carnegie Mellon University, in which we conducted several studies and experiments from 1991-1996 with more than 400 participants. Such a test-bed is an ideal environment for empirical software engineering, providing sufficient realism while allowing for controlled observation of important project parameters. We describe three proof-of-concept experiments to illustrate the value of communication metrics in software development projects. Finally, we propose a statistical framework based on structural equations for validating these communication metrics.","Programming,
Software testing,
Software engineering,
Equations,
Collaborative software,
Collaborative work,
Communication system control,
Software tools,
Context,
Computer Society"
Just-in-time language modelling,"Traditional approaches to language modelling have relied on a fixed corpus of text to inform the parameters of a probability distribution over word sequences. Increasing the corpus size often leads to better-performing language models, but no matter how large, the corpus is a static entity, unable to reflect information about events which postdate it. We introduce an online paradigm which interleaves the estimation and application of a language model. We present a Bayesian approach to online language modelling, in which the marginal probabilities of a static trigram model are dynamically updated to match the topic being dictated to the system. We also describe the architecture of a prototype we have implemented which uses the World Wide Web (WWW) as a source of information, and provide results from some initial proof of concept experiments.",
Automatic generation of microarchitecture simulators,"We describe the UPFAST system that automatically generates a cycle level simulator, an assembler and a disassembler from a microarchitecture specification written in a domain specific language called the Architecture Description Language (ADL). Using the UPFAST system, it is easy to retarget a simulator for an existing architecture to a modified architecture since one has to simply modify the input specification and the new simulator is generated automatically. UPFAST also allows porting of simulators to different platforms with minimal effort. We have been able to develop three simulators ranging from simple pipelined processors to complicated out-of-order issue processors over a short period of three months. While the specifications of the architectures varied from 5000 to 6000 lines of ADL code, the sizes of automatically generated software varied from 20000 to 300000 lines of C++ code. The automatically generated simulators are less than 2 times slower than hand coded simulators for similar architectures.",
Marked ancestor problems,"Consider a rooted tree whose nodes can be in two states: marked or unmarked. The marked ancestor problem is to maintain a data structure with the following operations: mark(v) marks node v: unmark(v) removes any marks from node v; firstmarked(v) returns the first marked node on the path from v to the root. We show tight upper and lower bounds for the marked ancestor problem. The lower bounds are proved in the cell probe model, the algorithms run on a unit-cost RAM. As easy corollaries we prove (often optimal) lower bounds on a number of problems. These include planar range searching, including the existential or emptiness problem, priority search trees static tree union-find, and several problems from dynamic computational geometry, including segment intersection, interval maintenance, and ray shooting in the plane. Our upper bounds improve algorithms from various fields, including coloured ancestor problems and maintenance of balanced parentheses.",
An analysis of leaky-wave dispersion phenomena in the vicinity of cutoff using complex frequency plane singularities,"In this paper we analyze characteristics of the dispersion function for leaky-wave modes in the vicinity of cutoff for several representative waveguiding structures. Our principal purpose is to demonstrate that in the vicinity of leaky-wave cutoff in open-boundary waveguides (in the spectral-gap region), dispersion behavior is controlled by the presence of branch points in the complex frequency plane. A similar situation occurs for the ordinary modes of homogeneously filled, perfefctly conducting cylindrical waveguides. These closed waveguides admit to simple analysis, leading to an explicit dispersion function which indicates frequency domain branch points. For open-boundary waveguides, the presence of frequency domain branch points is obscured by the necessity of numerically solving an implicit dispersion equation. A set of sufficient conditions is provided here which defines these branch points in a unified manner for both open and closed waveguides. Identification of these points allows for rapid determination of important and interesting regions in both the frequency and wavenumber planes and leads to increased understanding of dispersion behavior, especially in the case of dielectric loss. Examples are shown for several waveguiding geometries to demonstrate the general nature of the presented formulation.",
An introduction to the Synchronized Multimedia Integration Language,"The Web, now a multimedia environment, can handle audio, images, text and video. However, creating TV-like multimedia presentations proves difficult. It either requires complex, timer-based programming in a scripting language (such as Javascript) or an authoring tool for multimedia presentations. To allow a broader audience to author multimedia presentations for the Web, the World Wide Web Consortium developed the Synchronized Multimedia Integration Language. SMIL, an easy-to-learn HTML-like language, allows the use of a text editor to write multimedia presentations. SMIL is a W3C recommendation, and several implementations are available. This article explains how to write a multimedia presentation in SMIL.",
Interface exploration for reduced power in core-based systems,"Reducing power dissipation is becoming more important in the design of embedded systems. Core-based system design opens up the opportunity for exploring different bus interfaces in order to optimize for reduced power. We give a first approach for exploring a range of possible bus configurations, such as width and coding schemes, for a given set of communication channels. Our approach uses power estimation formulas, for fast performance. We use this approach to explore different bus interfaces for a real GPS navigation system in order to select the optimal bus interface for minimum power consumption.",
Architecture for group communication in mobile systems,"In mobile computing systems the network configuration changes due to node mobility. The paper identifies the issues a group communication service has to take into account in order to handle node mobility. These include the need to identify the location of a node, and the ability to cope with inaccuracies in the determination of a group membership. A multi level architecture for group communication in mobile systems is presented. This architecture contains a synchronous proximity layer protocol to determine the set of mobile nodes in the proximity of a given node in the network. This information is used by a three round group membership protocol for construction of groups used by mobile applications. As an example, the architecture is specialized to solve the channel allocation problem.",
A genetic algorithm for general machine scheduling problems,"This paper deals with the so-called general machine scheduling problems. In the general machine scheduling problems, job shop type jobs and open shop type jobs are scheduled together and the imposition of precedence constraints is allowed between operations belonging to either the same job or different jobs. This paper proposes a genetic algorithm to solve such general machine scheduling problems. Some experimental results are presented to show the applicability of the proposed method. The method can be used to solve traditional job shop scheduling, flow shop scheduling, and open shop scheduling as well as general machine scheduling problems.",
Caching-efficient multithreaded fast multiplication of sparse matrices,"Several fast sequential algorithms have been proposed in the past to multiply sparse matrices. These algorithms do not explicitly address the impact of caching on performance. We show that a rather simple sequential cache-efficient algorithm provides significantly better performance than existing algorithms for sparse matrix multiplication. We then describe a multithreaded implementation of this simple algorithm and show that its performance scales well with the number of threads and CPUs. For 10% sparse, 500/spl times/500 matrices, the multithreaded version running on 4-CPU systems provides more than a 41.1-fold speed increase over the well-known BLAS routine and a 14.6 fold and 44.6-fold speed increase over two other recent techniques for fast sparse matrix multiplication, both of which are relatively difficult to parallelize efficiently.",
Experimenting with error abstraction in requirements documents,"In previous experiments we showed that the Perspective-Based Reading (PBR) family of defect detection techniques was effective at detecting faults in requirements documents in some contexts. Experiences from these studies indicate that requirements faults are very difficult to define, classify and quantify. In order to address these difficulties, we present an empirical study whose main purpose is to investigate whether defect detection in requirements documents can be improved by focusing on the errors (i.e., underlying human misconceptions) in a document rather than the individual faults that they cause. In the context of a controlled experiment, we assess both benefits and costs of the process of abstracting errors from faults in requirements documents.",
Selective visualization of vortices in hydrodynamic flows,"Vortices are important features in many research and engineering fields. Visualization is an important step in gaining more understanding and control of vortices. Vortex detection criteria fall into two categories: point based scalar quantities, calculated at single points, and curve based geometric criteria, calculated for, e.g., streamlines. The first category is easy to compute, but does not work in all cases. The second category is more intuitive and should work in all cases, but currently only works in 2D (or 3D projected) flows. We show applications of both approaches in hydrodynamic flows.",
Compressed domain transcoding of MPEG,"Current video compression formats optimize for either compression or editing. For example, motion-JPEG (MJPEG) provides excellent random access and moderate overall compression, while MPEG optimizes for compression at the expense of random access. Converting from one format to another, a process called transcoding, is often desirable over the life of a video segment. The paper shows how to transcode MPEG-1 video to motion-JPEG without fully decompressing the MPEG-1 source. The described technique for compressed domain transcoding differs from previous work because it uses a new approximation approach that is optimized for software implementations. This new approach is 1.5 to 3 times faster than spatial domain transcoders and offers an additional degree of freedom: higher transcoding speeds can be obtained at the price of lower picture quality. This speed/quality trade-off is useful in many real time applications such as offline editing and video gateways.",
An optimal scheduling algorithm for parallel video processing,"We present an optimal scheduling algorithm called Periodic Write-Read-Compute (PWRC) scheduling for video processing. PWRC scheduling exploits continuity and periodicity of the video data. Therefore, it is suitable for any type of periodic data over which data independent application is to run. The target architecture is a client-server based system having point-to-point communication between the host any worker processors where SPMD type programming is assumed. PWRC requires a high level atomic write-read command for data transmission. The analysis of the cost model provides information either to form a parallel video processing environment or to predict the overall performance of an existing system. Meeting real-time requirements of video processing under PWRC scheduling is also discussed.",
A new distributed and dynamic call admission policy for mobile wireless networks with QOS guarantee,"Call admission control is one of the key elements in ensuring the QoS in mobile wireless networks supporting multimedia applications. This paper introduces a new distributed and dynamic call admission control mechanism (SDCA), whose objective is to maximize the radio channel utilization subject to a pre-determined threshold of the call dropping probability. The novelties of the proposed mechanism are that we have taken into account the effects of limited channel capacity and time dependence on the call dropping probability, and the influences from nearest and next-nearest neighboring cells. New calls are spread evenly over a control period, which leads to more effective and stable control. Simulations show that our scheme steadily satisfies the hard constraint on call dropping probability while maintaining a high channel throughput when compared with previous proposals.",
Determining fault insertion rates for evolving software systems,"In developing a software system, we would like to be able to estimate the way in which the fault content changes during its development, as well as determining the locations having the highest concentration of faults. In the phases prior to test, however, there may be very little direct information regarding the number and location of faults. This lack of direct information requires the development of a fault surrogate from which the number of faults and their location can be estimated. We develop a fault surrogate based on changes in relative complexity, a synthetic measure which has been successfully used as a fault surrogate in previous work. We show that changes in the relative complexity can be used to estimate the rates at which faults are inserted into a system between successive revisions. These rates can be used to continuously monitor the total number of faults inserted into a system, the residual fault content, and identify those portions of a system requiring the application of additional fault detection and removal resources.",
Reducing information systems costs through insourcing: experiences from the field,"Information systems (IS) sourcing continues to be a topic of great concern to both IS professionals and senior management. While a number of articles and books discuss the pros and cons of outsourcing, little has been written about companies that evaluate outsourcing but choose insourcing. Do companies actually achieve the cost savings proposed in the insourcing bid? This question prompted the study reported on in this paper. 14 in-depth case studies of companies choosing insourcing over outsourcing were studied to assess the implications of this choice. The results paint an interesting pattern of cost reductions but mixed reactions to these savings. Insourcing 'success' turns out to be a more complex notion than the literature suggests. Two conclusions emerge: the first is that the perception of success is related not solely to financial outcomes, but rather to the values and beliefs of different stakeholder groups, including senior management, business unit managers and users, and IS managers. The second is that senior management's perception of success is primarily based on IS cost-competitiveness relative to the market, rather than service excellence. We suggest future avenues of research pertaining to the identification of meaningful practices for shaping senior management's perception of IS.",
Optimal secondary segment shapes of linear reluctance motors using stochastic searching,"The authors discuss the influence of the shape of secondary segments of linear reluctance motors on the force. The magnitudes studied are the moving and the attractive force as they oscillate depending on the position of the primary. The magnetic conditions of the motor were analyzed by the finite element method, and the forces were calculated by the Maxwell stress method. The stochastic search method was used to change the shape of secondary segments.",
Where's the proof? A review of literature on effectiveness of information technology in education,"Often heard at engineering educational conferences is the plea, ""where's the proof that use of information technology really works?"" No single study can produce convincing evidence because in learning-teaching experiments there exist many confounding factors even in the best-designed study. Only sifting through the great amount of information can one find the patterns. The authors' review summarizes the research findings on computer assisted instruction over the past fifteen years. Many of the studies are themselves reviews and meta-analyses, which cover hundreds of studies, over approximately 2180 studies either directly or indirectly. Their interest is to gather hard, statistical evidence about the use of information technology for better learning, time on tasks, costs and learner/teacher attitudes. Research strongly supports the use of technology as a catalyst for improving the learning environment. Educational technology has been shown to stimulate more interactive teaching, effective grouping of students, and cooperative learning. A few studies, which estimated the cost effectiveness, reported time saving of about 30%. At first, professors can be expected to struggle with the change brought about by technology. However, they will adopt adapt, and eventually learn to use technology effortlessly and creatively.",
Threshold-based dynamic replication in large-scale video-on-demand systems,"Advances in high speed networking technologies and video compression techniques have made video-on-demand (VOD) services feasible. A large-scale VOD system imposes a large demand on bandwidth and storage resources, and therefore, parallel disks are typically used for providing VOD service. Although striping of movie data across a large number of disks can balance the utilization among these disks, such a striping technique can exhibit additional complexity, for instance, in data management, such as synchronization among disks during data delivery, as well as in supporting fault tolerant behavior. Therefore, it is more practical to limit the extent of data striping, for example, by arranging the disks in groups (or nodes) and then allowing intra-group (or intra-node) data striping only. With multiple striping groups, however, we may need to assign a movie to multiple nodes so as to satisfy the total demand of requests for that movie. Such an approach gives rise to several design issues, including: what is the right number of copies of each movie we need so as to satisfy the demand and at the same time not waste storage capacity; how to assign these movies to different nodes in the system; and what are efficient approaches to altering the number of copies of each movie (and their placement) when the need for that arises. We study an approach to dynamically reconfiguring the VOD system so as to alter the number of copies of each movie maintained on the server as the access demand for these movies fluctuates. We propose various approaches to addressing the above stated issues, which result in a VOD design that is adaptive to the changes in data access patterns. Performance evaluation is carried out to quantify the costs and the performance gains of these techniques.",
Visualising software in virtual reality,"Software visualisations of one form or another appear in numerous software maintenance tools. Visualisation is arguably one of the most profitable means of communicating information to a user. As software systems become larger and more complex we find ourselves in greater need for techniques to visualise such large information structures. This paper concentrates on visualising software systems using 3D graphics and VR technology. The main concepts of 3D software visualisation are introduced as well as a set of desirable properties which act as both guidelines for designing a visualisation and also as a framework for evaluating existing visualisations. A prototype visualisation, FileVis, is described and evaluated against these desirable properties.",
Clustering appearances of 3D objects,"We introduce a method for unsupervised clustering of images of 3D objects. Our method examines the space of all images and partitions the images into sets that form smooth and parallel surfaces in this space. It further uses sequences of images to obtain more reliable clustering. Finally, since our method relies on a non-Euclidean similarity measure we introduce algebraic techniques for estimating local properties of these surfaces without first embedding the images in a Euclidean space. We demonstrate our method by applying it to a large database of images.",
Optimized array index computation in DSP programs,"An increasing number of components in embedded systems are implemented by software running on embedded processors. This trend creates a need for compilers for embedded processors capable of generating high quality machine code. Particularly for DSPs, such compilers are hardly available, and novel DSP-specific code optimization techniques are required. In this paper we focus on efficient address computation for array accesses in loops. Based on previous work, we present a new and optimal algorithm for address register allocation and provide an experimental evaluation of different algorithms. Furthermore, an efficient and close-to-optimum heuristic is proposed for large problems.",
"Cognitive work analysis and the analysis, design, and evaluation of human-computer interactive systems","This paper provides a short conceptual and visual introduction to some of the basic principles of cognitive work analysis (CWA). CWA is an approach to the analysis, design, and evaluation of human-computer interactive systems-particularly of complex, high-technology sociotechnical systems. The paper also introduces the following five symposium papers, which provide detailed examples of CWA being used in research at the Swinburne Computer-Human Interaction Laboratory (SCHIL) at University of Illinois at Urbana-Champaign (UIUC), and at Defence Science and Technology Organisation (DSTO AMRL).",
Communication and tracking infrastructure of a mobile agent system,"Objects are reactive in the sense that they change state only when one of their methods is invoked by other objects. Reactive objects are sufficient in a client-server model of computation with a remote invocation mechanism ""a la RPC"". However active objects are much more powerful mainly because they are autonomous entities. Epidaure's objects are a particular type of active objects from which multi-agent systems can be built. In the context of mobility, migration becomes a necessity. For instance, a mobile client may disconnect from its server and reconnect to another server at a later time. In this paper we describe in a detailed manner the communication mechanism of a particular multi-agent system, Epidaure, and how the communication algorithm and mobility protocols that we developed can be applied to address problems related to mobile objects (e.g., registration, transport, tracking, ...).",
Multimedia courseware sparks interest in the industry,"The PowerLearn approach to power engineering education is under development by a core group of faculty and students at Iowa State University (ISU) and Virginia Tech (VT), under funding from the National Science Foundation (NSF) and the Electric Power Research Institute (EPRI). Four basic features characterize the approach. Courseware modularity provides that the material is organized by topic rather than by course. A Web site provides module storage and dissemination. Interactive visualization and simulation tools are heavily used in module construction. In addition, this project is integrated with efforts at both schools to develop new introductory undergraduate courses in electric power engineering. These courses serve well to illustrate the modular approach to course design.",
Self-stabilization with global rooted synchronizers,"We propose a self-stabilizing synchronization technique, called the global rooted synchronization, that synchronizes processors in a tree network. This synchronizer converts a synchronous protocol for tree networks into a self-stabilizing version. The synchronizer requires only O(1) memory (other than the memory needed to maintain the tree) at each node regardless of the size of the network, stabilizes in O(h) time, where h is the height of the tree, and does not invoice any global operations. Applications of this technique are presented.",
Compiler optimization of implicit reductions for distributed memory multiprocessors,"This paper presents reduction recognition and parallel code generation strategies for distributed-memory multiprocessors. We describe techniques to recognize a broad range of implicit reduction operations, including those involving statements at multiple loop nesting levels and intermixed with conditional control flow. We introduce two new optimizations: factoring which increases data locality for SUM and PRODUCT reductions, and index encoding which enables a single global communication to accomplish both an extreme value reduction and an extreme value location reduction. We have implemented these techniques in the dHPF compiler for High Performance Fortran (HPF). We evaluate their effectiveness experimentally by compiling several reduction benchmarks with dHPF and two commercial HPF compilers, and comparing the performance of the generated code on an IBM SP2. Our results show that our recognition techniques are more powerful and that our index encoding and factoring optimizations can improve performance by a factor of two where they apply.",
High dimensional similarity joins: algorithms and performance evaluation,"Current data repositories include a variety of data types, including audio, images and time series. State of the art techniques for indexing such data and doing query processing rely on a transformation of data elements into points in a multidimensional feature space. Indexing and query processing then take place in the feature space. We study algorithms for finding relationships among points in multidimensional feature spaces, specifically algorithms for multidimensional joins. Like joins of conventional relations, correlations between multidimensional feature spaces can offer valuable information about the data sets involved. We present several algorithmic paradigms for solving the multidimensional join problem, and we discuss their features and limitations. We propose a generalization of the Size Separation Spatial Join algorithm, named Multidimensional Spatial Join (MSJ), to solve the multidimensional join problem. We evaluate MSJ along with several other specific algorithms, comparing their performance for various dimensionalities on both real and synthetic multidimensional data sets. Our experimental results indicate that MSJ, which is based on space filling curves, consistently yields good performance across a wide range of dimensionalities.",
LOT: Logic Optimization with Testability. New transformations for logic synthesis,"A new approach to optimize multilevel logic circuits is introduced. Given a multilevel circuit, the synthesis method optimizes its area while simultaneously enhancing its random pattern testability. The method is based on structural transformations at the gate level. New transformations involving EX-OR gates as well as Reed-Muller expansions have been introduced in the synthesis of multilevel circuits. This method is augmented with transformations that specifically enhance random-pattern testability while reducing the area. Testability enhancement is an integral part of our synthesis methodology. Experimental results show that the proposed methodology not only can achieve lower area than other similar tools, but that it achieves better testability compared to available testability enhancement tools such as tstfx. Specifically for ISCAS-85 benchmark circuits, it was observed that EX-OR gate-based transformations successfully contributed toward generating smaller circuits compared to other state-of-the-art logic optimization tools.",
Learning visually guided grasping: a test case in sensorimotor learning,"We present a general scheme for learning sensorimotor tasks, which allows rapid online learning and generalization of the learned knowledge to unfamiliar objects. The scheme consists of two modules, the first generating candidate actions and the second estimating their quality. Both modules work in an alternating fashion until an action which is expected to provide satisfactory performance is generated, at which point the system executes the action. We developed a method for off-line selection of heuristic strategies and quality predicting features, based on statistical analysis. The usefulness of the scheme was demonstrated in the context of learning visually guided grasping. We consider a system that coordinates a parallel-jaw gripper and a fixed camera. The system learns to estimate grasp quality by learning a function from relevant visual features to the quality. An experimental setup using an AdeptOne manipulator was developed to test the scheme.",
Incorporating cores into system-level specification,"We describe an approach for incorporating cores into a system-level specification. The goal is to allow a designer to specify both custom behavior and pre-designed cores at the earliest design stages, and to refine both into implementations in a unified manner. The approach is based on experience with an actual application of a GPS-based navigation system. We use an object oriented language for specification, representing each core as an object. We define three specification levels, and we evaluate the appropriateness of existing inter-object communication methods for cores. The approach forms the specification basis for the Dalton project.",
Multicasting multimedia streams with active networks,"Active networks allow protocol processing code to be loaded dynamically into network nodes at run-time. This code can perform tasks specific to a stream of packets or even a single packet. In this paper we compare two active network architectures: the active node transfer system (ANTS) and the messenger system (M0). We have implemented a robust audio multicast protocol and a layered video multicast protocol with both active network systems. We discuss the differences of the two systems, evaluate architectural strengths and weaknesses, compare the runtime performance, and report practical experience and lessons learned.",
A refined fast 2-D discrete cosine transform algorithm with regular butterfly structure,"A fast computation algorithm for the two-dimensional discrete cosine transform (2-D DCT) is derived based on index permutation. As a result, only the computation of N N-point 1-D DCTs and some post-additions are required for the computation of an (N/spl times/N)-point 2-D DCT. Furthermore, as compared with the method of Cho and Lee (1992), the derivation of the refined algorithm is more succinct, and the associated post-addition stage possesses a more regular butterfly structure. The regular structure of the proposed algorithm makes it more suitable for VLSI and parallel implementations.",
A prototype model for data warehouse security based on metadata,"Gives an overview of security relevant aspects of existing OLAP/Data Warehouse solutions, an area which has seen rather little interest from product developers and is only beginning to be discussed in the research community. Following this description of the current situation, a metadata driven approach implemented as part of the WWW-EIS-DWH project is presented in detail. The prototype focuses on the technical realisation and is intended not to be open for use in different security policies.",
GENGED: a generic graphical editor for visual languages based on algebraic graph grammars,"GENGED is a generic graphical editor supporting the graphical definition of visual languages. Given an alphabet and rules of a specific visual language, GENGED generates a syntax-directed graphical editor for this language. GENGED, as well as each visual language defined using GENGED, is based on algebraic graph grammars. A sentence is given by a graphical structure consisting of a logical (abstract syntax) and a visual level (concrete syntax). Both levels are connected by layout operations. Visual language rules are defined by graph grammar rules. The underlying logical structure, however is hidden from the user but it is essential for a formal presentation and manipulation of graphical structures on both levels. The manipulations are performed by a graph transformation machine working on the logical level, whereas a graphical constraint solver manages the layout the user works with.",
Efficient all-to-all personalized exchange in multidimensional torus networks,"This paper presents new algorithms for all-to-all personalized exchange in multidimensional torus-connected multiprocessors. Unlike existing message-combining algorithms in which the number of nodes in each dimension should be power-of-two and square, the proposed algorithms accommodate non-power-of-two tori where the number of nodes in each dimension need not be power-of-two. In addition, destinations remain fixed over a larger number of steps in the proposed algorithms, thus making them amenable to optimizations. Finally, the data structures used are simple, hence making substantial saving of message-rearrangement time.",
The complexity of the approximation of the bandwidth problem,"The bandwidth problem has a long history and a number of important applications. It is the problem of enumerating the vertices of a given graph G such that the maximum difference between the numbers of adjacent vertices is minimal. We will show for any constant k/spl epsiv/N that there is no polynomial time approximation algorithm with an approximation factor of k. Furthermore, we will show that this result holds also for caterpillars, a class of restricted trees. We construct for any x,/spl epsiv//spl isin/R with x>1 and /spl epsiv/>0 a graph class for which an approximation algorithm with an approximation factor of x+/spl epsiv/ exists, but the approximation of the bandwidth problem within a factor of x-/spl epsiv/ is NP-complete. The best previously known approximation factors for the intractability of the bandwidth approximation problem were 1.5 for general graphs and 4/3 for trees.",
Use of CORBA in the ATLAS prototype DAQ,"This paper presents the experience of using a CORBA-based communication package for inter-component communication of control and status information in the ATLAS prototype DAQ project. A public domain package, called Inter-Language Unification (ILU) has been used to implement CORBA-based communication between DAQ components in a local area network (LAN) of heterogeneous computers. The selection of the CORBA standard and the ILU implementation are judged against the requirements of the DAQ system. An overview of ILU is included. Several components of the DAQ system have been designed and implemented using CORBA/ILU for which the development procedure and environment are described.",
Pattern distributions of Legendre sequences,"Legendre sequences have a number of interesting randomness properties and are closely related with quadratic residue codes. We give lower and upper bounds on the number of patterns distributed in a cycle of the Legendre sequences and establish the relationship between the weight distribution of quadratic residue codes and the pattern distribution of Legendre sequences. Our result shows that Legendre sequences have an ideal distribution of patterns of length s, when s is not large compared with log/sub 2/N, where N is the prime used to define the sequence.",
The quantum communication complexity of sampling,"Sampling is an important primitive in probabilistic and quantum algorithms. In the spirit of communication complexity, given a function f: X/spl times/Y/spl rarr/{0,1} and a probability distribution D over X/spl times/Y, we define the sampling complexity of (f,D) as the minimum number of bits Alice and Bob must communicate for Alice to pick x/spl isin/X and Bob to pick y/spl isin/Y as well as a valve z s.t. the resulting distribution of (x,y,z) is close to the distribution (D,f(D)). In this paper we initiate the study of sampling complexity, in both the classical and quantum model. We give several variants of the definition. We completely characterize some of these tasks, and give upper and lower bounds on others. In particular this allows us to establish an exponential gap between quantum and classical sampling complexity, for the set disjointness function. This is the first exponential gap for any task where the classical probabilistic algorithm is allowed to err.",
A multi-dimensional model of the software engineering curriculum,"A model is presented that describes the range of possible curricula for degree courses in software engineering and other areas of computing. This model allows distinctions to be made between the different aspects of computing that are related to software engineering, notably computer science, computer hardware engineering and information systems. It also classifies the various topics that need to be included in the curriculum for such degree programmes, and the relationships between these topics. As such it provides a basis for making decisions on the allocation of curriculum time between different topics, in order to match the aims for the overall focus of any specific degree course in this area.",
Fuzzy cluster analysis with missing values,The paper shows how methods of fuzzy cluster analysis can be extended in such a way that they are able to process datasets with missing values. We study several imputation methods and propose a new approach for reducing the influence of imputations.,
Performance implications of QoS mapping in heterogeneous networks involving ATM,"In this paper we discuss issues relating to the mapping of QoS parameters in an IP/ATM environment. We conduct preliminary experiments with constant TP packet rate, which after segmentation in AAL5, generates a jittered CBR cell stream. We use this stream to investigate the effect of parameter translation on the QoS received by the application. Results indicate that translation does have a significant impact on performance, in particular, on cell loss. There are also cost implications, as large buffers are required to achieve an acceptable level of performance.",
Optimal routing and channel assignments for hypercube communication on optical mesh-like processor arrays,"This paper considers optimal routing and channel assignment (RCA) schemes to realize hypercube communication on optical mesh-like networks. Specifically, we identify lower bounds on the number of channels required to realize hypercube communication on top of array and ring topologies and develop optimal RCA schemes that achieve the lower bounds on these two topologies. We further extend the schemes to mesh and torus topologies and obtain RCA schemes that use at most 2 more channels than the optimal for these topologies.",
Sensitivity of a ramp equalizer for series batteries,"To realize the maximum potential of large series connected battery packs, the individual battery voltages should be maintained at fairly uniform values, i.e., the voltages should be equalized. The ramp type of electronic equalizer has proven to be very effective in performing this task since it offers the advantages of simplicity and convenience. However, to achieve maximum performance, it is necessary to understand how the circuit is affected by certain parameters. This is accomplished by defining the circuit sensitivity and using a simplified model to explain the relative effects of leakage inductance, frequency, and power level.",
GRCA: a hybrid genetic algorithm for circuit ratio-cut partitioning,"A genetic algorithm for partitioning a hypergraph into two disjoint graphs of minimum ratio cut is presented. As the Fiduccia-Mattheyses graph partitioning heuristic turns out to be not effective when used in the context of a hybrid genetic algorithm, we propose a modification of the Fiduccia-Mattheyses heuristic for more effective and faster space search by introducing a number of novel features. We also provide a preprocessing heuristic for genetic encoding designed solely for hypergraphs which helps genetic algorithms exploit clustering information of input graphs. Supporting combinatorial arguments for the new preprocessing heuristic are also provided. Experimental results on industrial benchmarks circuits showed visible improvement over recently published algorithms with a lower growth rate of running time.",
The template and multiple inheritance approach into attribute grammars,"Formal methods for describing programming language semantics, such as attribute grammars, operational semantics and denotational semantics, are not widely used since they are not modular, extensible and reusable. A novel modular, extensible and reusable approach for specifying programming languages with attribute grammars is presented. The concepts from object oriented programming, i.e. templates and multiple inheritance, have been integrated with attribute grammers. A template in attribute grammar is an abstraction of a semantic rule parameterized with attribute occurrences. On the other hand, the whole attribute grammar is a subject of multiple inheritance and specialization. With the proposed approach, a language designer has the chance to design incrementally a language or reuse some fragments from other programming language specifications. Templates and multiple inheritance have been implemented in our compiler generator tool LISA version 2.0.",
The evolution of size in variable length representations,"In many cases, a program's length increases during artificial evolution. This is known as ""bloat"", ""fluff"" or ""increasing structural complexity"". We show that bloat is not specific to genetic programming and suggest it is inherent in search techniques with discrete variable-length representations using simple static evaluation functions. We investigate the bloating characteristics of three non-population-based and one population-based search technique using a novel mutation operator. An artificial ant following the Santa Fe trail problem is solved by simulated annealing, hill climbing, strict hill climbing and population-based searching using two variants of the the new subtree-based mutation operator. As predicted, bloat is observed when using unbiased mutation and is absent in simulated annealing and in both hill climbers when using the length-neutral mutation; however, bloat occurs with both mutations when using a population. We conclude that there are two causes of bloat: (1) search operators with no length bias tend to sample bigger trees, and (2) competition within populations favours longer programs, as they can usually reproduce more accurately.",
Orchestrating quartets: approximation and data correction,"Inferring evolutionary trees has long been a challenging problem both for biologists and computer scientists. In recent years research has concentrated on the quartet method paradigm for inferring evolutionary trees. Quartet methods proceed by first inferring the evolutionary history for every set of four species (resulting in a set Q of inferred quarter topologies) and then recombining these inferred quarter topologies to form an evolutionary tree. This paper presents two results on the quartet method paradigm. The first is a polynomial time approximation scheme (PTAS) for recombining the inferred quartet topologies optimally. This is an important result since, to date, there have been no polynomial time algorithms with performance guarantees for quartet methods. In fact, this is the first known PTAS for inferring evolutionary trees under any paradigm. To achieve this result the natural denseness of the set Q is exploited. The second result is a new technique, called quartet cleaning, that detects and corrects errors in the set Q with performance guarantees. This result has particular significance since quartet methods are usually very sensitive to errors in the data. It is shown how quartet cleaning can dramatically increase the accuracy of quartet methods.",
An analysis-based approach to composition of distributed embedded systems,"The growing complexity in the functionality and system architecture of embedded systems has motivated designers to raise the level of abstraction by composing the system with a mix of reusable and system-specific components. Currently, these components assume specific programming models that make them difficult to compose or retarget. The modal process model addresses the problem of control composition by separating the synchronization semantics from state unification, and by supporting automatic synthesis of control communication onto distributed architectures. By avoiding over-specifying the behavior, the components can be made more composable and the designer can more easily choose the least expensive synchronization semantics for implementing the composition. To help designers evaluate their choice, we propose a method for analyzing the properties of the composed system, including the detection of potential deadlock and livelock situations.",
Methods for providing probe position and temperature information on MR images during interventional procedures,"Interventional magnetic resonance imaging (MRI) can be defined as the use of MR images for guiding and monitoring interventional procedures (e.g., biopsy, drainage) or minimally invasive therapy (e.g., thermal ablation). This work describes the development of a prototype graphical user interface and the appropriate software methods to accurately overlay a representation of a rigid interventional device [e.g., biopsy needle, radio-frequency (RF) probe] onto an MR image given only the probe's spatial position and orientation as determined from a three-dimensional (3-D) localizer used for interactive scan plane definition. This permits 1) ""virtual tip tracking"", where the probe tip location is displayed on the image without the use of separate receiver coils or a ""road map"" image data set, and, 2) ""extending"" the probe to predict its path if it were directly moved forward toward the target tissue. Further, this paper describes the design and implementation of a method to facilitate the monitoring of thermal ablation procedures by displaying and overlaying temperature maps from temperature sensitive MR acquisitions. These methods provide rapid graphical updates of probe position and temperature changes to aid the physician during the actual interventional MRI procedures without altering the usual operation of the MR imager.",
FDTD design of a Chinese hat feed for shallow mm-wave reflector antennas,"Rotationally symmetric reflectors with hat feeds are attractive for radio link applications the mm-wave band due to their low sidelobes, simple rotationally symmetric structure and low manufacture cost. The standard hat feed has its best performance in a very deep reflector. In this paper, the V2D computer code based on the finite-difference time-domain (FDTD) method is applied to the analysis of a Chinese hat feed, which is optimized for use in a more shallow reflector. The FDTD results have been post processed to obtain the different subefficiencies characterizing spillover, polarization, illumination and phase losses.","Finite difference methods,
Time domain analysis,
Feeds,
Radio link,
Application software,
Manufacturing,
Costs,
Optimization methods,
Polarization,
Lighting"
Improving TCP performance over wireless links,"TCP is the most common transport protocol used in the Internet. It was designed primarily for wired networks. The characteristics of wireless links is very different from wired links, particularly in terms of the loss behaviour. This results in poor TCP performance over wireless links. We have proposed enhancements to TCP so that it can differentiate between a loss due to congestion, and a loss due to noise in the wireless link. With this knowledge, TCP can react differently to the two kinds of losses, leading to improved performance. The modified TCP is compatible with existing implementations.",
IEEE Transactions on Evolutionary Computation,,
Navigating in natural environments: a virtual environment training transfer study,"The ability to use virtual environments as either an abstraction of a space, similar to a map, or as a simulation of the space itself has suggested to many that it would be a useful tool in terrain familiarization of unknown environments. Up to this point, all research in this area has focused on building interiors and urban environments which are significantly different from natural environments in terms of navigation cues and useful wayfinding techniques. The experiment we present uses a virtual environment, as compared to a map only or real-world conditions on navigation tasks in a natural environment. We show that navigation ability is more important to performance than the training method, with the virtual environment being most effective for intermediate orienteers as compared to advanced or beginner orienteers.",
Percentile blobs for image similarity,"We present a new algorithm called PBSIM for computing image similarity, based upon a novel method of extracting bloblike features from images. In tests on a classification task using a data set of over 1000 images, PBSIM shows significantly higher accuracy than algorithms based upon color histograms, as well as previously reported results for another approach based upon bloblike features.",
Qualis: the quality of service component for the Globus metacomputing system,"General computing over a widely distributed set of heterogeneous machines-typically called metacomputing-offers definite advantages. The notion of quality of service (QoS) for metacomputing is very important. This paper presents Qualis, the QoS component for the Globus metacomputing system. We present the Qualis architecture, how it is integrated into the Globus architecture, and how it addresses QoS in a metacomputing environment.",
Visual electromagnetics for Mathcad(R): a computer-assisted learning tool for undergraduate electromagnetics education,"Computer computation and visualization can be powerful tools for teaching undergraduate electromagnetics. Towards this end, the electronic book: Visual electromagnetics for Mathcad(R) (Visual EM(R)) has been written to capitalize on the extraordinary performance of current mathematics software and personal computers. Here we discuss the motivation for developing such an electronic book and give an overview of its content, layout and capabilities.",
No-search approach in linguistic geometry: construction of strategies,Linguistic geometry includes mathematical models and tools for knowledge representation and reasoning about multiagent discrete pursuit-evasion games. A new approach to solving a class of search problems without search is shown on example of the aircraft war game. The paper includes the second part of the solution construction. Based on the state space chart a full set of strategies is constructed; one of them is implemented as a solution tree. It is proved that the only optimal strategy in this problem is the draw strategy; implementation of this strategy in the start state is the optimal solution.,
Criteria for building Prony models for time domain CAD,"This contribution presents several criteria for automatic creation of high quality Prony's models from time samples generated by time domain CAD tools. Guidelines for selection the model order, length of the sample trains and the desampling factor are given.",
Correcting English text using PPM models,"An essential component of many applications in natural language processing is a language modeler able to correct errors in the text being processed. For optical character recognition (OCR), poor scanning quality or extraneous pixels in the image may cause one or more characters to be mis-recognized, while for spelling correction, two characters may be transposed, or a character may be inadvertently inserted or missed out, This paper describes a method for correcting English text using a PPM model. A method that segments words in English text is introduced and is shown to be a significant improvement over previously used methods. A similar technique is also applied as a post-processing stage after pages have been recognized by a state-of-the-art commercial OCR system. We show that the accuracy of the OCR system can be increased from 96.3% to 96.9%, a decrease of about 14 errors per page.",
Multimodality exploration by an unsupervised projection pursuit neural network,"Graphical inspection of multimodality is demonstrated using unsupervised lateral-inhibition neural networks. Three projection pursuit indexes are compared on low-dimensional simulated and real-world data: principal components, Legendre polynomial, and projection pursuit network.",
Observing pose and motion through contact,"Investigates how to ""observe"" a planar object being pushed by a finger. The pushing is governed by a nonlinear system that relates through contact the object pose and motion to the finger motion. Nonlinear observability theory is employed to show that the contact information is often sufficient for the finger to determine not only the pose but also the motion of the object. Therefore a sensing strategy can be realized as an observer of the nonlinear dynamical system, which is subsequently introduced. The observer based on the result of Gauthier et al. (1992), has its ""gain"" determined by the solution of a Lyapunov-like equation. Simulations have been done to demonstrate the feasibility of the observer. A sensor has been implemented using strain gauges and mounted on an Adept robot with which preliminary experiments have been conducted from a general perspective, this work presents an approach for acquiring geometric and dynamical information about a task from a small amount of tactile data, with the application of nonlinear observability theory.",
Application of fuzzy-rough sets in modular neural networks,"In a modular neural network, the conflicting information supplied by the information sources, i.e., the outputs of the subnetworks, can be combined by applying the concept of fuzzy integral. To compute the fuzzy integral it is essential to know the importance of each subset of the information sources in a quantified form. In practice, it is very difficult to determine the level of the information sources. However, in the fuzzy integral approach the importance of a particular information source is considered to be independent of the other information sources. Therefore, determination of the importance of each information source should be based on the incomplete knowledge supplied by the source itself. This paper proposes a fuzzy-rough set theoretic approach to find the importance of each subset of the information sources from this incomplete knowledge.",
An efficient version model of software diagrams,"Various types of diagrams are used to represent the design of software systems. During the design phase, versions of a diagram may be created like other design documents and source code, and it is necessary to manage them efficiently. However, traditional configuration management systems and some object-oriented database management systems that provide object versioning are not suitable for managing versions of a diagram. In this paper, we propose an efficient version model of software diagrams. This model reflects the common characteristics and structure of software diagrams, and revisions of a diagram are managed by operation delta and object visibility. A merge model for versions of a diagram is also presented at the end of this paper.",
Enhancing the hardware design experience for computer engineers,"People become engineers because they love to build things. With the recent progress in design automation tools, it is now possible to revise the core computer engineering curriculum to include realistic design experiences in virtually all courses, even those that are not usually thought of as being laboratory courses. The use of design automation makes it possible to assign a large number of simple design exercises in low-level courses, and a few more complex projects in more advanced courses. This paper describes enhancements to three core courses: logic design; computer architecture; and digital system design. These changes are based on a common set of tools that are used throughout the curriculum. Several design exercises are described, along with a suggested sequence for the various exercises. Several laboratory manuals have been developed for use with standard textbooks. The references provide pointers to web sites where the laboratory manuals and tools can be downloaded. Suggestions for textbooks for each course are provided, although the laboratory materials should be useful with a wide variety of texts. Classroom experience and student reactions are also reported.",
Bagging in computer vision,"Previous research has shown that aggregated predictors improve the performance of non-parametric function approximation techniques. This paper presents the results of applying aggregated predictors to a computer vision problem, and shows that the method of bagging significantly improves performance. In fact, the results are better than those previously reported on other domains. This paper explains this performance in terms of the variance and bias.",
Object-oriented modelling of parallel hardware systems,Object-oriented techniques like inheritance promise great benefits for the specification and design of parallel hardware systems. The difficulties which arise from the use of inheritance in parallel hardware systems are analysed in this article. Similar difficulties are well known in concurrent object-oriented programming as the inheritance anomaly but have not yet been investigated in object-oriented hardware design. A solution to how to successfully deal with the anomaly is presented for a type based on an object-oriented extension to VHDL. Its basic idea is to separate the synchronisation code (protocol specification) and the actual behaviour of a method. Method guards which allow a method to execute if a guard expression evaluates to true are proposed to model synchronisation constraints. It is shown how to implement a suitable re-schedule mechanism for methods as part of the synchronisation code to handle the case that a guard expression is evaluated to false.,
Constructive theory refinement in knowledge based neural networks,Knowledge based artificial neural networks offer an approach for connectionist theory refinement. We present an algorithm for refining and extending the domain theory incorporated in a knowledge based neural network using constructive neural network learning algorithms. The initial domain theory comprising propositional rules is translated into a knowledge based network of threshold logic units (threshold neuron). The domain theory is modified by dynamically adding neurons to the existing network. A constructive neural network learning algorithm is used to add and train these additional neurons using a sequence of labeled examples. We propose a novel hybrid constructive learning algorithm based on the tiling and pyramid constructive learning algorithms that allows a knowledge based neural network to handle patterns with continuous valued attributes. Results of experiments on two non-trivial tasks (the ribosome binding site prediction and the financial advisor) show that our algorithm compares favorably with other algorithms for connectionist theory refinement both in terms of generalization accuracy and network size.,
Automated design of folded-cascode op-amps with sensitivity analysis,"We present a method for optimizing and automating component and transistor sizing in CMOS operational amplifiers. We observe that a wide variety of performance measures can be formulated as posynomial functions of the design variables. As a result, amplifier design problems can be expressed as geometric programs, as special type of convex problem for which very efficient global optimization methods exist. A side benefit of using convex optimization is that a sensitivity analysis is obtained with the final solution with no additional computation. This information is of great interest to analog circuit designers. The method we present can be applied to a wide variety of amplifier architectures, but in this paper we apply the method to a specific two-stage amplifier architecture.",
A custom EASE-Grid SSM/I processing system,"A task fundamental to advancing global change research is the availability of a standard reference system for direct digital comparison and interuse of remote sensing data sets on varying spatial and temporal scales. The availability of a standard gridding scheme is an essential requirement for systematic time-series studies. Such a scheme also supports the direct comparison of different remote sensing products, as well as the validation of algorithms, through digital comparison with surface station and other ancillary data sets which have been processed into the common grid. New software now allows users of Special Sensor Microwave/Imager (SSM/I) data to produce their own Equal-Area Scalable Earth Grid (EASE-Grid) image data from Temperature Data Record (TDR) format swath data. Overlapping orbit data are separately retained for the highest possible temporal resolution, and data for specific pixels can be extracted. The modular processor also simplifies testing of alternative resampling and gridding algorithms. The EASE-Grid is a standard gridding scheme now used with a variety of satellite datasets, including SSM/I data, its original application. Although the specific method used to interpolate from native sensor coordinates to a fixed Earth grid is unique to each sensor, the fundamental projection and gridding concept of the EASE-Grid provides the basis for a standard interuse gridding method. Thus, the SSM/I EASE-Grid is composed of two fundamental parts: i) a grid and projection scheme and, 2) a specific method to interpolate SSM/I data from swath space to Earth gridded coordinates. The projection and gridding scheme is independent of the satellite sensor or data type.",
Bootstrapping text recognition from stop words,Recognition of arbitrary noisy English text has been difficult because of problems in character segmentation and multi-font symbol classification. Both segmentation and recognition can be easier with more knowledge of the dominant font used in a given text page. This has led to some recent studies that show promising methods for extracting character prototypes from a text image provided that truth is given for part of the image. In this paper we investigate the feasibility of such a strategy without dependence on ground truth. We replace the needed truth by results of direct recognition of some frequently occurring words. The method makes use of the observation that over half of the words in a typical English text passage are contained in a very small lexicon.,
Tree-based state clustering using self-organizing principles for large vocabulary on-line handwriting recognition,"The introduction of trigraphs offers a powerful method for the accuracy enhancement of handwriting modeling. A trigraph is a hidden Markov model (HMM) for a special character with defined adjacent characters. Especially in large vocabulary systems, as they are investigated here, the number of unseen trigraphs for which no training samples are available, exceeds the number of seen trigraphs by far. This paper presents a novel approach, which allows a synthesis of unseen trigraphs from seen trigraphs. With the method proposed here, a mean relative error reduction of 42% was obtained on a writer dependent system, resulting in an overall word recognition rate of 94.1%.",
Compositional analysis of expected delays in networks of probabilistic I/O automata,"Probabilistic I/O automata (PIOA) constitute a model for distributed or concurrent systems that incorporates a notion of probabilistic choice. The PIOA model provides a notion of composition, for constructing a PIOA for a composite system from a collection of PIOAs representing the components. We present a method for computing completion probability and expected completion time for PIOAs. Our method is compositional, in the sense that it can be applied to a system of PIOAs, one component at a time, without ever calculating the global state space of the system (i.e. the composite PIOA). The method is based on symbolic calculations with vectors and matrices of rational functions, and it draws upon a theory of observables, which are mappings from delayed traces to real numbers that generalize the classical ""formal power series"" from algebra and combinatorics. Central to the theory is a notion of representation for an observable, which generalizes the classical notion ""linear representation"" for formal power series. As in the classical case, the representable observables coincide with an abstractly defined class of ""rational"" observables; this fact forms the foundation of our method.",
Automatic performance evaluation of parallel programs,"Traditional parallel programming forces the programmer, apart from designing the application, to analyse the performance of this recently built application. This difficult task of testing the behaviour of the program can be avoided with the use of an automatic performance analysis tool. Users are released from having to understand the enormous amount of performance information obtained from the execution of a program. The automatic analysis bases its work on the use of a predefined list of logical rules of production of performance problems. These rules form the ""knowledge base"" of the tool. When the tool analyses an application, it looks for the occurrence of an element in the list of performance problems recorded in the ""knowledge base"". When one of the problems is found (a ""match"" in the list), the tool analyses the cause of the performance problem and builds a recommendation to the user to direct the possible modifications the code of the application.",
Using Verilog LOGISCOPE to analyze student programs,"It is difficult to give an in-depth evaluation of student programs to the point of checking every line of code due to the amount of time checking would take. Solutions to this difficulty may involve only checking to see if the program executes correctly (dynamic analysis), glancing over the program to see if appropriate documentation is present (static analysis), and glancing over the code for any problems (static analysis). Automated solutions are difficult to construct. One commercial solution is Verilog LOGISCOPE which offers a limited number of licenses free to educators. LOGISCOPE is a static analysis checker capable of taking hundreds of individual measurements of a program, such as lines of code, McCabe's cyclomatic complexity, and number of operators. It also shows the control flow graph of a program which is a depiction of the statements, if structures, and looping structures in a program. LOGISCOPE enables the complexity and quality of a program to be analyzed yielding valuable feedback to both students and educators. It allows visualization of the measurements taken through the control flow graphs and Kiveat diagrams. The operation of LOGISCOPE is shown by using typical student programs taken from the introductory computing course at Texas Tech University. Then the results of analyzing several programs from the same class are given to show the diversity of results. Finally, how LOGISCOPE can be used in education to help students improve their programming and help instructors evaluate programs better is considered.",
Real time data acquisition with transputers and PowerPCs using the wavelet transform for event detection,"In this paper a new data acquisition system for the soft-X-ray diagnostics in a large fusion plasma experiment is described. The main purpose of this system is to provide real time facilities for analyzing the acquired data, to generate necessary information for the dynamical adaptation of sample rates, and to deliver triggers when certain events in the plasma are detected. The hardware is built on a network of up to 72 transputers and up to four modules, each consisting of a PowerPC and a transputer coupled via dual-ported RAM. The transputers are used for collecting the incoming data from ADCs (each sampled with 500 KHz), and for communication and synchronization within the computer network. Four PowerPCs can analyze up to eight selected channels in real time and provide the results to the other transputers in the network. The algorithm running on the PowerPCs is performing a wavelet like time-frequency transform for the detection of plasma events. With this system it was possible to observe high-n/m mode cascades following impurity accumulation. A report on this effect has been published in Physical Review Letters.",
Tracking of nonrigid motion and 3D structure from 2D image sequences without correspondences,"In this paper we present a novel method for tracking 3D nonrigid motion and 3D structure from a sequence of monocular, perspective images, where range data of the first frame is available. No a priori knowledge of point correspondences is assumed. A genneral approach and formulas for polynomial (second order) displacement functions are presented. An analysis of the number of equations versus the number of unknowns is given. Results for synthetic and real image sequences are presented.",
A system for cursive handwritten address recognition,"This paper presents a cursive handwritten address recognition system, which consists of four main modules; 1) over-segmentor, 2) dynamic zip locator, 3) zip candidates generator, and 4) city-state-zip verifier. The dynamic zip locator and city-state-zip verifier are based on a flexible matcher for matching a sequence of graphemes with a list of generalized strings. The dynamic zip locator is able to locate zip without knowing exactly where the zip starts. The zip candidates generator uses a hidden Markov model with position-dependent state transition probabilities. A scheme for utilizing prefixes is designed to reduce computation and memory requirement. Finally, the system employs a mechanism for rejection based on rank features extracted from the matching. The overall system achieves an acceptance rate of 83.5% with 3.6% error for 5-digit encoding on 805 USPS cursive address images.",
Scheduling strategies for mixed workloads in multimedia information servers,"In contrast to pure video servers, advanced applications such as digital libraries or teleteaching exhibit a mixed workload with massive access to conventional, ""discrete"" data such as text documents, images and indexes as well as requests for ""continuous data"". In addition to the service quality guarantees for continuous data requests, quality-conscious applications require that the response time of the discrete data requests stay below some user-tolerance threshold. We study the impact of different disk scheduling policies on the service quality for both continuous and discrete data. We identify a number of critical issues, present a framework for describing the various policies in terms of few parameters and finally provide experimental results, based on a detailed simulation testbed, that compare different scheduling policies.",
An automated approach for supporting software reuse via reverse engineering,"Formal approaches to software reuse rely heavily upon a specification matching criterion, where a search query using formal specifications is used to search a library of components indexed by specifications. In previous investigations, we addressed the use of formal methods and component libraries to support software reuse and construction of software based on component specifications. A difficulty for all formal approaches to software reuse is the creation of the formal indices. We have developed an approach to reverse engineering that is based on the use of formal methods to derive formal specifications of existing programs. In this paper, we present an approach for combining software reverse engineering and software reuse to support populating specification libraries for the purposes of software reuse. In addition, we discuss the results of our initial investigations into the use of tools to support an entire process of populating and using a specification library to construct a software application.",
Transitioning to the new PLI standard,"Summary form only given. This paper explores the challenges that Verilog Programming Language Interface (PLI) application developers will face when transitioning from using the older TF and ACC PLI standard (also known as ""PLI 1.0"") to the new VPI PLI standard (also known as ""PLI 2.0""). The older standard has been in use for nearly 15 years. It is well known, supported by all major Verilog simulators, and there are hundreds, possibly thousands, of commercial and proprietary applications written with the older PLI standard. The VPI library provides more capability but is harder to use and is not supported by most major Verilog simulators. So why should users transition to the new standard? How hard will the transition be? When is the right time to switch (if ever)? This paper presents the advantages and disadvantages of both the old and new PLI standards. The issues with transitioning to the new PLI standard are presented and answers are suggested on, if, and when PLI application developers should transition the new standard.",
Dynamic aspects of the InfoPriv model for information privacy,"This paper describes the dynamic aspects of the InfoPriv model for privacy. The dynamic aspects are concerned with the actual information flow as well as the change of the privacy policy over time. An information can-flow graph represents the potential information flow between entities, thereby describing only the static aspects of InfoPriv. The dynamic aspects are divided into two categories: dynamic information flow and the evolution of the static aspects. ""Dynamic information flow"" refers to the simulation of the actual information flow as it occurs over time. Information flow is, therefore, permitted or denied based on the past information flow of a system. The ""evolution of the static aspects"" refers to the change of the privacy policy over time. For instance, it is necessary to give entities access to more information during their lifetime. An algorithm is presented that extends the can-flow graph without introducing unauthorised information flow.",
Using self-organizing maps to learn geometric hash functions for model-based object recognition,"A major problem associated with geometric hashing and methods which have emerged from it is the nonuniform distribution of invariants over the hash space. In this paper, a new approach is proposed based on an elastic hash table. We proceed by distributing the hash bins over the invariants. The key idea is to associate the hash bins with the output nodes of a self-organizing feature map (SOFM) neural network which is trained using the invariants as training examples. In this way, the location of a hash bin in the space of invariants is determined by the weight vector of the node associated with the hash bin. The advantage of the proposed approach is that it is a process that adapts to the invariants through learning. Hence, it makes absolutely no assumptions about the statistical characteristics of the invariants and the geometric hash function is actually computed through learning. Furthermore, SOFM's topology preserving property ensures that the computed geometric hash function should be well behaved.",
The draw-bot: a project for teaching software engineering,"The authors present a course project which was successfully used to teach software design principles to third year computer engineering students. The goal of the project is to program a robot to trace a shortest path through a maze. The students, organized in teams of five, have to follow the classical steps of software development and prepare interface, design and testing documents. Having a project that requires controlling a device to complete a clear task generates enthusiasm in the students and helps them to understand the principles taught in the course.",
Educating industrial-strength software engineers,"A roadmap is proposed for integrating trends in software technology and product development into a graduate curriculum in software engineering. We contend that the causal relationship between building model solutions and developing products from these solutions, provides a sound basis for industrial strength software engineering. We illustrate the impact of modern software technology on the two fundamental, complementary sets of activities of domain engineering and application engineering. This is presented within a scheme for integrating design abstractions, such as architectural styles, frameworks, and patterns, with domain models and product generation. We address questions such as where in the spectrum of technology development do our current education programs fit? Is the current focus on architectures (patterns, frameworks and the like) a passing fad or a true ingredient for a mature software profession? What kind of software engineers should we be preparing for the 21st century?.",
Multi-resolution cepstral features for phoneme recognition across speech sub-bands,"Multi-resolution sub-band cepstral features strive to exploit discriminative cues in localised regions of the spectral domain by supplementing the full bandwidth cepstral features with sub-band cepstral features derived from several levels of sub-band decomposition. Multi-resolution feature vectors, formed by concatenation of the sub-band cepstral features into an extended feature vector, are shown to yield better performance than conventional MFCCs for phoneme recognition on the TIMIT database. Possible strategies for the recombination of partial recognition scores from independent multi-resolution sub-band models are explored. By exploiting the sub-band variations in signal to noise ratio for linearly weighted recombination of the log likelihood probabilities we obtained improved phoneme recognition performance in broadband noise compared to MFCC features. This is an advantage over a purely sub-band approach using non-linear recombination which is robust only to narrow band noise.",
F2ID: a personal identification system using faces and fingerprints,"A real-time automatic personal identification system should meet the conflicting dual requirements of accuracy and response time. In addition, it also should be user-friendly. We introduce a medium-size realtime automatic personal identification system, F2ID, which integrates faces and fingerprints to make a personal identification. F2ID overcomes some of the limitations of face recognition systems and fingerprint verification systems and can achieve a desirable identification accuracy with a tolerable response time. We have tested our system on a limited set of face and fingerprint images collected in a laboratory environment. Experimental results show that that our system meets both the identification accuracy as well as the speed requirements.",
Development of a real-time radon monitoring system for simultaneous measurements in multiple sites,"A real-time radon monitoring system which can simultaneously measure radon concentrations in multiple sites was developed and tested. The system consists of maximum of four radon detectors, optical fiber cables and a data acquisition personal computer. The radon detector uses a plastic scintillation counter which collects radon daughters in the chamber electrostatically. The applied voltage on the photocathode for the photomultiplier tube (PMT) acts as an electrode for radon daughters. The thickness of the plastic scintillator was thin, 50 um, so as to minimize the background counts due to the environmental gamma rays or beta particles. The energy discriminated signals from the radon detectors are fed to the data acquisition personal computer via optical fiber cables. The system made it possible to measure the radon concentrations in multiple sites simultaneously.",
Numerical solutions for optimum distributed detection of known signals in dependent t-distributed noise-the two sensor problem,We examine distributed two-sensor detection of known signals in t-distributed noise which is dependent from sensor to sensor. A Gauss-Seidel algorithm which attempts to minimize the Bayes risk is used to obtain the rules for the decision regions. The best nonrandomized fusion rules are sought. It it shown that the properties of the decision regions can be predicted based on the problem's parameters. In some specific cases the optimum distributed detection sensor rules are shown to be better than likelihood ratio tests by Monte Carlo simulations.,
Evaluating software deployment languages and schema: an experience report,"Software distribution is evolving from a physical media approach to one where it is practical and advantageous to leverage the connectivity of networks. Network distribution of software systems provides timeliness and continuity of evolution not possible with physical media distribution methods. To support network-based software distribution, companies and organizations such as Microsoft, Marimba, and the Desktop Management Task Force (DMTF) are strengthening their efforts to package software systems in a way that is conducive to network distribution and management. The result of these efforts has led to the creation of software description languages and schema such as the Open Software Description format created by Microsoft and Marimba and the Management Information Format created by DMTF. While these efforts are steps in the right direction, they do not address deployment issues in a complete and systematic fashion. The contribution of this paper is to evaluate these leading software description technologies.",
Performance optimization by gate sizing and path sensitization,"In the circuit model where outputs are latched and input vectors are successively applied at inputs, the gate resizing approach to reduce the delay of the critical path may not improve the performance. Since the clock period is determined by delays of both long and short paths in the combinational circuit, gates lying in sensitizable long and short paths can be selected for resizing. For feasible settings of the clock period, new algorithms and corresponding gate selection methods for resizing are proposed in this paper. Our algorithms are tested on ISCAS'85 benchmark circuits and experimental results show that the clock period can be optimized efficiently with our gate selection methods.",
Using Hartstone Uniprocessor Benchmark in a real-time systems course,"Demonstrates the usability of the application-oriented Hartstone Uniprocessor Benchmark (HUB) to support teaching in fixed-priority scheduling analysis. Our implementation of the benchmark in C runs on different real-time operating systems (e.g. LynxOS) and also on standard desktop systems (Windows NT). We have created some helpful tools around the Hartstone program that may be used for a performance evaluation and a rapid analysis of real-time applications. They give the possibility to understand the real-time responsiveness of applications using both real-time and standard operating systems in an uncomplicated way. We distinguish three different methods to compare performance and to check the theoretical results of scheduling analysis. The first is based on finding breakdown utilization points of process sets. The second method inspects the special overload behavior beyond the breakdown utilization point. This observation reveals a very interesting behavior of the system under overload conditions. Finally, we evaluate the performance based on a simulation of a real-world real-time application.",
Teaching systems analysis to software engineering students: experience with a structured methodology,"An undergraduate degree programme in software engineering was designed to include a systems analysis module, in which the teaching was based on a particular structured methodology. Experience is described of the conflicts that this caused within the curriculum of the degree, and of the way in which these were solved. This involved the development of a structure for the topics forming the subject of systems analysis, which is described along with the new structure for the systems analysis module that was derived from it. It is argued that this structure for systems analysis is also applicable to object oriented approaches and some experience is discussed of applying it within MSc courses as well as undergraduate ones.",
Efficient computation of terminal-pair reliability using triangle reduction in network management,"Terminal-pair reliability (TR) in network management determines the probabilistic reliability between two nodes (the source and sink) of a network, given failure probabilities of all links. It has been shown that TR can be effectively computed by means of the network reduction technique. Existing reduction axioms, unfortunately, are limited to simple rules such as valueless link removal and series-parallel link reduction. We propose a novel reduction axiom, referred to as triangle reduction. The triangle reduction axiom transforms a graph containing a triangle subgraph to that excluding the base of the triangle. The computational complexity of the transformation is as low as O(1). The paper further provides an assessment of the effectiveness of triangle reduction on partition-based TR algorithms with respect to the number of subproblems and computation time. Experimental results demonstrate that, incorporating triangle reduction, the partition-based TR algorithms yield a substantially reduced number of subproblems and computation time for all benchmarks and random networks.",
A comparison of alternative approaches to the capstone experience: case studies versus collaborative projects,"In the Computer Science and Engineering Department at Arizona State University, we have completed a multi-semester, NSF-sponsored research project in which the orientation of our traditional capstone course was changed from a lecture-oriented case study to a team-oriented collaborative project. In this paper, we describe our traditional approach to the computer systems engineering capstone course, as well as the approach adopted during the research project, in which the students were allowed to develop an interdisciplinary, team-oriented design project. In evaluating the impact of a collaborative approach on the capstone experience, we found fundamental differences in the student experience well beyond the expected organizational differences in the presentation of course material. Although both approaches were required to develop a similar set of core competencies, the manner in which students acquired those skills were quite distinct. For example, while our traditional approach develops a systems-oriented design perspective through the detailed analysis of a carefully structured case study, the collaborative approach adopted during our research program required students to use team-oriented development and the analysis of design alternatives to develop a similar systems-oriented perspective. Our paper concludes with an analysis of the strengths and shortcomings of each instructional approach and a discussion of how a traditional capstone course can be enhanced by incorporating instructional methods for collaborative, team-oriented design.",
Design and implementation of a parallel I/O runtime system for irregular applications,"In this paper we present the design, implementation and evaluation of a runtime system based on collective I/O techniques for irregular applications. We present two models, namely, ""Collective I/O"" and ""Pipelined Collective I/O"". In the first scheme, all processors participate in the I/O simultaneously making scheduling of I/O requests simpler but creating a possibility of contention at the I/O nodes. In the second approach, processors are grouped into several groups, so that only one group performs I/O simultaneously, while the next group performs communication to rearrange data, and this entire process is pipelined to reduce I/O node contention dynamically. Both models have been optimized by using software caching, chunking and on-line compression mechanisms. We demonstrate that we can obtain significantly high-performance for I/O above what has been possible so far. The performance results are presented on an Intel Paragon and on the ASCI/Red teraflops machine at Sandia National Labs.",
Controls education on the WWW: tutorials for MATLAB and SIMULINK,"Engineering in general and controls in particular have been dramatically changed by the development of fast computers and computation, simulation, and visualization software packages which run on them. An impediment to even wider use of these powerful tools is that they are frequently difficult to learn to use. The authors have developed an effective paradigm for using the World Wide Web as an education aid for tutoring students on the use of software tools. The approach provides the information that students need when and where they need it-at the computer. This paper describes the application of this method to teaching the use of MATLAB for controls and recent extensions for teaching the use of SIMULINK.",
Recognizing surfaces using curve invariants and differential properties of curves and surfaces,"A general paradigm for recognizing 3D objects is offered, and applied to some geometric primitives (spheres, cylinders, cones, and tori). The assumption is that a curve on the surface was measured with high accuracy (for instance, by a sensory robot). Differential invariants of the curve in one method and differential properties of curves and surfaces in the other are then used to recognize the surface. The motivation is twofold: the output of some devices is not surface range data, but such curves. So, surface invariants, which may be simpler in some cases, cannot always be obtained. Also, a considerable speedup is obtained by using curve data, as opposed to surface data which usually contains a much higher number of points.",
Edge preservation in volume rendering using splatting,"The paper presents a method to preserve sharp edge details in splatting for volume rendering. Conventional splatting algorithms produce fuzzy images for views close to the volume model. The lack of details in such views greatly hinders study and manipulation of data sets using virtual navigation. Our method applies a nonlinear warping to the footprints of conventional splat and builds a table of footprints for different possible edge positions and edge strengths. When rendering, we pick a footprint from the table for each splat, based on the relative position of the voxel to the closest edge. Encouraging results have been achieved both for synthetic data and medical data.",
Modelling software operational reliability via input domain-based reliability growth model,"Operational reliability of programs depends upon many factors imposed by design, testing, and operation of the programs. In the testing stage, the main imperfect factors which can affect operational reliability are faults, testing time, fault correction, testing profile and operation profile. The proposed modelling process takes these factors into account. It consists of two testing stages and one operational stage. In the testing stages, partition testing and an input domain-based reliability growth model are used to evaluate the reliability growth. Based on the profile coverage between testing and operational profiles, a method is developed to partition the input domain, so that the testing profile can follow the given operational profile of a program. In the operational stage, the proposed reliability model uses the remaining faults from testing stages as the source of unreliability. A case study is conducted using the proposed models.",
Memory space representation for heterogeneous network process migration,"A major difficulty of heterogeneous process migration is how to collect advanced dynamic data-structures, transform them into machine independent form, and restore them appropriately in a different hardware and software environment. In this study we introduce a data model, the Memory Space Representation (MSR) model, to recognize complex data structures in program address spaces. Supporting mechanisms of the MSR model are also developed for collecting program data structures and restoring them in a heterogeneous environment. The MSR design has been implemented under a prototype heterogeneous process migration environment. Pointer-intensive programs with function and recursion calls are tested. Experimental results confirm that the newly proposed design is feasible and effective for heterogeneous network process migration.",
A concurrent visual language based on Petri nets,This paper presents a concurrent visual programming language based on Petri nets. Most concurrent visual programming languages address concurrency by extending a non-concurrent paradigm and representation with additional control and synchronisation mechanisms and notation. It is argued in this paper that clearer and more concise concurrent program representations are possible if the concurrency is inherent in the paradigm. The language described demonstrates that Petri nets provide such a paradigm.,
Solving fuzzy constraint satisfaction problems with fuzzy GENET,"Constraint satisfaction is well known to be applicable in modeling AI problems. Despite their extensive literature, the framework is sometimes inflexible and the results are not very satisfactory when applied to real-life problems. With the incorporation of the theory of fuzzy sets, fuzzy constraint satisfaction problems (FCSP's) have been exploited. FCSP's model real-life problems better by allowing both full and partial satisfaction of individual constraints. GENET, which has been shown to be efficient and effective in solving certain traditional CSPs, has been extended to handle FCSPs. Through transforming FCSPs into 0-1 integer programming problems, Wong and Leung (1998) displayed the equivalence between the underlying working mechanism of fuzzy GENET and the discrete Lagrangian method. We focus on the performance of fuzzy GENET in attacking large-scale and real-life over-constrained problems. An efficient simulator of fuzzy GENET for single-processor machines is implemented. Benchmarking results confirm its feasibility, flexibility, and superb efficiency in tackling both CSPs and FCSPs.",
Lightweight transactions on networks of workstations,"Although transactions have been a valuable abstraction of atomicity, persistency, and recoverability, they have not been widely used in programming environments today, mostly because of their high overheads that have been driven by the low performance of magnetic disks. A major challenge in transaction-based systems is to remove the magnetic disk from the critical path of transaction management. We present PERSEAS, a transaction library for main memory databases that decouples the performance of transactions from the magnetic disk speed. Our system is based on a layer of reliable main memory that provides fast and recoverable storage of data. We have implemented our system as a user-level library on top of the Windows NT operating system in a network of workstations connected with the SCI interconnection network. Our experimental results suggest that PERSEAS achieves performance that is orders of magnitude better than traditional recoverable main memory systems.",
A testing assistant for object-oriented programs,"Within the last decade, the object-oriented approach to software development has become widely used. The features that make object-oriented software appealing are features that also cause complexity in the testing process. Inheritance is the characteristic that most distinguishes object-oriented languages from traditional imperative languages; however, its use makes the testing of the programs challenging. Testing the inherited features is clearly crucial; however, the testing process can easily become very complex if features in the child classes are unnecessarily tested. In this paper, we present a testing assistant, Object-Oriented Testing Assistant (OOTA), that facilitates the testing of object-oriented code by incorporating procedures to support object level testing and inheritance testing. OOTA provides a framework that helps to ensure that appropriate components and interactions are tested by generating code segments that drive the testing process. OOTA was developed and tested using the object-oriented paradigm.",
A microprogramming animation,"This paper describes a successful project using computer animation to teach the concepts of microprogramming to lower division computer science majors. The students write a simulator for the Mic-1 horizontal microcontroller described in the book Structured Computer Organization by Tanenbaum. The simulation is enhanced by the use of a graphical representation of the machine to animate the simulation. This creative use of computer animation enables the students to see the results of their simulation without having to write an extensive user interface. They can concentrate on implementing the instruction cycle, an activity that directly enhances their understanding of microprogramming and the conventional machine level. The XTango animation package is used, making the animation facility portable to any Unix system with an X display. The user interface and additional instructional material are available via the web.",
Combining neural networks and belief networks for image segmentation,"We are concerned with segmenting an image into a number of predefined classes. We show how to fuse together local predictions for the class labels with a prior model of segmentations using the scaled-likelihood method. The prior model is based on a tree-structured belief network. Both the neural network and belief network were trained on a set of training images, and then the combined system was used to make predictions on a set of test images. We show that the combined neural network/belief network classifier gives improved prediction accuracy on 9 out of the 11 classes.",
Testing in the field,"It is widely acknowledged in the HCI community that much can be gained from bringing aspects from the field into the lab, and this principle is dominating within usability groups in Danish Industry. The paper describes three such Danish usability groups and their experiments with turning the tables by using aspects from the lab in the methods applied in the field during field work projects. The context of use plays an important role for a richer understanding of the usability of particular products. As such, implications of this is not surprising, neither theoretically nor empirically. What is interesting, however, is how findings of this type are instantiated in the particular cases; how the three usability groups have used the lab approaches to aid them in working in the field and how the new methods may enhance their existing methodological toolkit. The message of the paper is that there are a variety of ways in which the theoretically driven, pre-planned, and predirected may meet the situated and open minded, both when usability work is conducted in the field and in the lab.",
"Routing of L-shaped channels, switchboxes and staircases in Manhattan-diagonal model","New techniques are presented for routing L-shaped channels, switchboxes and staircases in 2-layer Manhattan-diagonal (MD) model with tracks in horizontal, vertical and /spl plusmn/45/spl deg/ directions. First, a simple O(l.d) time algorithm is proposed which routes any L-shaped channel with length l, density d and no cyclic vertical constraints, in w (d/spl les/w/spl les/d+1) tracks. Next, an O(l.w) time greedy method for routing an L-shaped channel with cyclic vertical constraints, is described. Then, the switchbox routing problem in the MD model is solved elegantly. These techniques, easily extendible to the routing of staircase channels, yield efficient solutions to detailed routing in general floorplans. Experimental results show significantly low via-count and reduced wire length, thus establishing the superiority of MD-routing over classical strategies.",
Theory and application of multiple attractor cellular automata for fault diagnosis,This paper reports the use of a class of cellular automata for the testing and diagnosis of faults in analog circuits. The use of the scheme is explained with reference to the testing of OTA based circuits.,
View-based object matching,"We introduce a novel view-based object representation, called the saliency map graph (SMG), which captures the salient regions of an object view at multiple scales using a wavelet transform. This compact representation is highly invariant to translation, rotation (image and depth), and scaling, and offers the locality of representation required for occluded object recognition. To compare two saliency map graphs, we introduce two graph similarity algorithms. The first computes the topological similarity between two SMG's, providing a coarse-level matching of two graphs. The second computes the geometrical similarity between two SMG's, providing a fine-level matching of two graphs. We test and compare these two algorithms on a large database of model object views.",
View consistency for optimistic replication,"Optimistically replicated systems provide highly available data even when communication between data replicas is unreliable or unavailable. The high availability comes at the cost of allowing inconsistent accesses, since users can read and write old copies of data. Session guarantees have been used to reduce such inconsistencies. They preserve most of the availability benefits of optimistic systems. We generalize session guarantees to apply to persistent as well as distributed entities. We implement these guarantees, called view consistency, on Ficus an optimistically replicated file system. Our implementation enforces consistency on a per-file basis and does not require changes to individual applications. View consistency is enforced by clients accessing the data and thus requires minimal changes to the replicated data servers. We show that view consistency allows access to available and high performing data replicas and can be implemented efficiently. Experimental results show that the consistency overhead for clients ranges from 1% to 8% of application runtime for the benchmarks studied in the prototype system. The benefits of the system are an improvement in access times due to better replica selection and improved consistency guarantees over a purely optimistic system.",
A three-level user interface to multimedia digital libraries with relaxation and restriction,"This paper proposes a three-level user interface to document-rich digital libraries. Not only typical document metadata information but also document contents and personal annotations can be used to select digital documents, and afterward a two-dimension matrix can be used to relax or restrict the selections. Our approach to document retrieval called ""three-level user interface"" employs a database query language with intelligent multimedia retrieval techniques. Queries can be constituted with metadata information about documents in the first level. In the second level, queries can be constituted with document contents, the contents based on the corresponding document type definition. In the third level, for sophisticated users semantic approaches are used for queries. Users are asked for annotations or heuristics with their subjective meanings. In each level, however, a matrix based query can be used to either restrict for finitely many selections or relax for few selections. The contribution of this paper includes inventing a framework for multimedia document retrieval facility in which query conditions can be efficiently related or restricted for an appropriate number of documents.",
Software risk assessment through software measurement and modeling,"A major portion of our recent research is to refine a software process model for a major Storage Technology Corporation (STK) development program. The work is centered around the precise measurement of software development outcomes for the accurate reliability assessment of the program and subsequent projects. Three major measurement initiatives were established. First, tools and processes for the static measurement of the source code have been installed and made operational at STK. The measurement process has been automated and been made transparent to the programming staff. Second, tools and processes have been developed for the management and accurate measurement of software faults. No useful software measurement system may be developed without a precise understanding of software faults and their etiology. Finally, tools have been developed and processes initiated that will demonstrate the feasibility and the necessity of a dynamic software measurement processes. The STK program includes the development of a software profiler to obtain measures of program dynamics.",
"Are you ""pulling the plug"" or ""pushing up the daisies""? [communication patterns]","We are becoming more and more dependent upon a broad range of more or less ubiquitous communication technologies, and these are increasingly integrated with our PCs. The more connected we become, the more important it is that we pro-actively manage our communication patterns. If not, we risk being forced to either temporarily disconnecting ourselves from communicating, or spend all our time trying to catch up with the demand for communication. None of these two scenarios are desirable. The aim of this paper is to investigate how people manage communication patterns as an integral part of their daily work. Empirical investigation of complex work in a Swedish pharmaceutical company showed a variety of means for managing communication patterns. Based on the fieldwork we present SwitchIT, an application for pro-actively managing communication modes, and discuss the theoretical implications of applying the concepts of communication overflow and communication deficiency to describe communication patterns. It is concluded that there is a need for collaborative technology supporting the negotiation of communication.",
Optimal neural network algorithm for on-line string matching,"We consider an online string matching problem in which we find all the occurrences of a pattern of m characters in a text of n characters, where all the characters of the pattern are available before processing, while the characters of the text are input one after the other. We propose a space-time optimal parallel algorithm for this problem using a neural network approach, This algorithm uses m McCulloch-Pitts neurons connected as a linear array. It processes every input character of the text in one step and hence it requires at most n iteration steps.",
The architecture of secure systems,"Secure system design, verification and validation is often a daunting task, involving the merger of various protection mechanisms in conjunction with system security policy and configurations. This paper presents a generic approach to secure system development that can be readily applied to a wide range of secure systems. Use of this approach, based on separability, will greatly simplify the developer's overall design, verification and validation effort.","Kernel,
Virtual machining,
Computer architecture,
Laboratories,
Corporate acquisitions,
Protection,
Computer security,
Logic design,
Hardware,
Communication system control"
Local transformation techniques for multi-level logic circuits utilizing circuit symmetries for power reduction,"In this paper, we present several optimization techniques for power reduction utilizing circuit symmetries. There are four kinds of symmetries that we detect in a given circuit implementation. First, we propose an algorithm for detecting the four different types of symmetries in a given circuit implementation of a Boolean function. Several re-synthesis techniques utilizing such symmetries are proposed. These techniques enable us to optimize power consumption and delay with no (or very little) area overhead. We have carried out experiments on MCNC benchmark circuits to demonstrate the efficiency of the proposed techniques. The average power reduction is 14% with little or no area and/or delay overhead.",
"A state management protocol for IntServ, DiffServ and label switching","Providing quality of service (QoS) in an efficient and scalable manner in the Internet is a topic of active research. The technologies that have drawn the most attention are integrated services (IntServ), differential services (DiffServ) and label switching (MPLS). While these technologies are orthogonal in many respects and can coexist, they are all similar with respect to the fact that some or all the routers in such networks need to be programmed with state information associating data flows with QoS classes or priorities or labels. This paper describes a protocol called SSP (state setup protocol) which is designed to disseminate and manage this state information. In addition to a simple design which makes fast implementations feasible, SSP provides many features like state aggregation and third party signaling which makes SSP a suitable tool for network configuration, management and provisioning as well. A detailed discussion of SSP is presented along with numerous examples and performance measurements.",
Efficient Selection Algorithms on Distributed Memory Computers,"Consider the selection problem of determining the k th smallest element of a sequence of n elements. Under the CGM (Coarse Grained Multicomputer) model with p processors and O (n/p) local memory, we present a deterministic parallel algorithm for the selection problem that requires O(log p) communication rounds. Besides requiring a low number of communication rounds, the algorithm also attempts to minimize the total amount of data transmitted in each round (only O(p) except in the last round). The basic algorithm is then extended to solve the problem of q simultaneous selections using the same input sequence, also in O(log p) communication rounds and asymptotically same local computing time (if q = O(p) ). The simultaneous selection algorithm gives rise to a communication efficient sorting algorithm, with O(log p) communication rounds and a total of O(p2) data transmitted in each round except in the last one. In addition to showing theoretical complexities, we present very promising experimental results obtained on two parallel machines that show almost linear speedup, indicating the efficiency and scalability of the proposed algorithms. To our knowledge, this is the best deterministic CGM algorithm in the literature for the selection problem.",
New software packages and multimedia modules for electromagnetics education,"The CAEME interactive multimedia CD-ROM for engineering electromagnetics includes a variety of modules which describe a basic course in electromagnetics. Lessons range from vector algebra to transmission lines analysis. They also include conducting and dielectric materials, Maxwell's equations and wave propagation. A large number of virtual laboratories and active instruments have also been implemented to enhance interactivity. We describe the features of new simulation software packages implemented in the CD-ROM and the new multimedia modules on antennas and magnetism.",
Reverse Engineering Through Electromagnetic and Harmonic Balance Simulations,"In this communication, we address the Computer Aided Design issue by means of Harmonic Balance (HB) and Electromagnetic (EM) simulations with application to the design of a tripler with an output frequency @ 250 GHz. For this purpose, Heterostructure Barrier Varactors (HBV's) with state-of-the art performances in terms of voltage handling (10 V) and capacitance ratio (> 5:1) were fabricated starting from InP-based growth epilayers. The devices are fully dc and rf characterized up to 110 GHz by means of three kinds of test samples. On this basis, de-embedding techniques combining measurements at the intrinsic level and characterization of the diode electromagnetic environment are described in details having in mind a top-down approach.",
Embed longest rings onto star graphs with vertex faults,"The star graph has been recognized as an attractive alternative to the hypercube. Let F/sub e/ and F/sub /spl nu// be the sets of vertex faults and edge faults, respectively. Previously, Tseng et al. showed that an n-dimensional star graph can embed a ring of length n! if |F/sub e/|/spl les/n-3 (|F/sub /spl nu//|=0), and a ring of length at least n!-4|F/sub /spl nu//| if |F/sub /spl nu//|/spl les/n-3 (|F/sub e/|=0). Since an n-dimensional star graph is regular of degree n-1 and is bipartite with two partite sets of equal size, our result achieves optimality in the worst case.",
Boundary tracking in 3-D binary images to produce rhombic faces for a dodecahedral model,An algorithm is presented for tracking boundaries in three-dimensional binary images based on rhombic dodecahedral voxels. The algorithm produces a list of all the rhombic voxel faces in such a boundary.,
Power invariant vector sequence compaction,"Simulation-based power estimation is commonly used for its high accuracy, despite excessive computation times. Techniques have been proposed to speed it up by transforming a given sequence into a shorter one while preserving the power consumption characteristics of the original sequence. This work proposes a novel method to compact a given input vector sequence to improve on the existing techniques. We propose a graph model to transform the problem to the problem of finding a heaviest weighted trail in a directed graph. We also propose a heuristic based on min-cost flow algorithms, using the graph model. Furthermore, we show that generating multiple input sequences yields better solutions in terms of both accuracy and simulation time. Experiments showed that significant reduction in simulation times can be achieved with extremely accurate results. Experiments also showed that the generation of multiple sequences improved the results further both in terms of accuracy and simulation time.",
A diagnostic image system for assessing the severity of chronic liver disease,"This paper describes a new ultrasonic scoring system based on the texture characteristics of ultrasonic liver images to generate an ultrasonic disease severity score that is highly correlated with the computer morphometry score obtained from fibrosis measurement of liver biopsy specimens. Essentially, the ultrasonic disease severity has great resemblance to the computer morphometry score in the statistical presentation. Therefore, the ultrasonic disease severity is defined mathematically referring to computer morphometry score as the scoring basis. As a result, the ultrasonic disease severity can faithfully reflect the disease progression of liver fibrosis. Promising results have been obtained in experimental studies and we are currently undergoing extensive clinical experiments.",
A comparison of collaborative problem solving using face to face versus desktop video conferencing,"A small field experimental study was undertaken to ascertain the applicability of desktop video conferencing (DVC) for a University Extension Office. Subjects (county extension agents and a faculty expert in dairy nutrition) engaged in a ""ration balancing"" task for herds of dairy cows, using DVC and face-to-face collaborative meetings. Subjects preferred DVC over face-to-face collaboration, presumably because DVC allowed them more freedom to manipulate computer-generated solutions.",
Computerized biological brain phantom for evaluation of PET and SPECT reconstruction,"A digital brain phantom was created from available primate autoradiographic (AR) data for use in emission computed tomography studies. The tissue was radio-labelled with a functional analogue of the PET agent [/sup 18/F]-fluorodeoxyglucose (FDG). Following sacrifice of the animal, film records from serial 20 /spl mu/m thickness sections were digitized and calibrated to obtain ground truth 2D spatial distributions of relative radionuclide density. A 3D version was constructed by using a video subtraction method to align consecutive slices. In order to assess the effects of accurate modeling of activity, the AR data, containing cortical and basal ganglia structures, was used as a phantom in the context of a partial-volume correction method for obtaining accurate regional quantitation. A second phantom, less realistic in terms of activity assignment, was constructed and also tested. The results indicate that quantitation errors due to effects of nonuniform activity in the AR phantom are significant and comparable in magnitude to errors due to non-phantom effects.",
Event-driven verification of switch-level correctness concerns,"We propose a technique for the verification of MOS circuits, focusing on signal transitions (events) rather than signal levels. Diverse conditions, behaviors, and even delay assumptions are modeled as processes that can be coupled and compared to circuit specifications in a unified formalism. Verification is performed modularly and hierarchically by a BDD-based tool. We illustrate this technique on a self-timed RAM.",
High speed multi-channel data acquisition chip,"This paper describes the design and performance of a very high speed, low power multichannel data acquisition chip implemented using Gallium Arsenide (GaAs) technology. The mixed analog-digital circuit uses source coupled MESFET logic for analog components and GaAs merged logic for digital components. The design of n-bit flash analog-to-digital converter requires only 2/sup (n-1)/ comparators as compared to (2/sup n/-1) comparators for a standard n-bit flash converter. The data acquisition chip dissipates 185.6 milliwatts of power with conversion time of 0.85 nsec. The results validate appropriateness of design technology and techniques used for mixed analog-digital circuit design on a single chip.",
Balancing consistency and lag in transaction-based computational steering,"Computational steering, the interactive adjustment of application parameters and allocation of resources, is a promising technique for higher-productivity simulations, finer-grained optimization of dynamically varying algorithms, and greater understanding of program behavior, data sets and solution spaces. Typically, these ""steerable"" programs have been implemented as parallel or distributed programs. Tools for computational steering must provide monitoring, visualization, and interaction facilities, must address issues related to consistency, latency, and scalability at each of these phases, and must consider perturbation effects. In this paper we describe transaction-based components for a computational steering system and present an approach that guarantees consistent monitoring and displays, supports scalable monitoring, and provides the user with the ability to adjust the tradeoffs among lag, consistency and perturbation.",
On the bias of the modified output error algorithm,"In a recent work by Betser and Zeheb (1995), a modified adaptive output error algorithm for identification was proposed. In this paper, we prove that the solution is biased for most practical types of disturbance noise, including the white noise case. Bias can be avoided in certain cases, under the verification of a relation between the spectrum of the noise and the noise-free solution, specified in the form of a theorem.",
Structural and navigational analysis of hypermedia courseware,,
On team knowledge and common knowledge,"We report on an approach to defining knowledge in multi-agent systems that allows the knowledge of a structured group of agents (a team) to be more than just the knowledge from individual sub-teams. Teams are first class entities in our logic. A team may have a sub-team relationship with other teams. A team which has no sub-team relationships with other teams is considered to be an agent. We ascribe knowledge directly to teams. Relationships between team and sub-team knowledge are specified axiomatically. We show that the introduction of team knowledge in this setting yields a definition of common knowledge which is an interesting generalization of the well known definition of common knowledge provided by Halpern and Moses (1992). From a systems development perspective, the separation of knowledge from sub-team knowledge allows for both top-down and bottom-up specifications of team behavior in a single framework.",
VPPB-a visualization and performance prediction tool for multithreaded Solaris programs,"Efficient performance tuning of parallel programs is often hard. We describe an approach that uses a uni-processor execution of a multithreaded program as reference to simulate a multiprocessor execution. The speed-up is predicted, and the program behaviour is visualized as a graph, which can be used in the performance tuning process. The simulator considers scheduling as well as hardware parameters, e.g., the thread priority, no. of LWPs, and no. of CPUs. The visualization part shows the simulated execution in two graphs: one showing the threads' behaviour over time and the other the amount of parallelism over time. In the first graph it is possible to relate an event in the graph to the code line causing the event. Validation using a Sun multiprocessor with eight processors and five scientific parallel applications shows that the speed-up predictions are within +/-6% of a real execution.",
Applications of Web-based workflow,"A Web-based workflow system was developed at the Jet Propulsion Laboratory and several pilot applications were developed. The authors give an overview of the architecture and functionality of the workflow system, describe three pilot workflows that have been or are being deployed, and relate lessons learned.","Engines,
Propulsion,
Laboratories,
Workflow management software,
Mediation,
Sockets,
Databases,
User interfaces,
Read only memory,
Computer architecture"
Adaptive prefetching and storage reorganization in a log-structured storage system,"We present a storage management system that has the ability to adapt to the data access characteristics of the application that uses it based on collection and analysis of runtime statistics. This feature is especially useful in the storage management layer of database systems, where applications exhibit relatively predictable access patterns. Adaptive reorganization is performed by the storage management system in a manner that optimizes the access patterns of the system for which it is used. We enhance the log-structured storage system that naturally caters for write optimization, with the addition of a statistics collection mechanism to determine data access patterns of applications. The storage system can serve as a testbed for a variety of statistics analysis and clustering mechanisms. Higher level application-specific data clustering mechanisms can be used to override the storage system's low-level clustering mechanisms. In addition, the analysis techniques and reorganization scheme can be used in other storage systems. Performance results from our prototype show potential response time speedups of up to 83 percent over the basic log-structured file system in the best case, using a combination of storage reorganization and prefetching.",
A finite basis of the set of all monotone partial functions defined over a finite poset,"Let X be an arbitrary poset. A partial function f with n variables defined over X is said to be monotone if the following condition is satisfied: if x/sub 1//spl les/y/sub 1/,..., x/sub m//spl les/y/sub n/, and both the values f(x/sub 1/,..., x/sub n/) and f(y/sub 1/,....y/sub n/) are defined, then f(x/sub 1/,..., x/sub n/)/spl les/f(y/sub 1/,...,y/sub n/) It is shown that the set of all monotone partial functions has a finite basis.",
Effect of architecture configuration on software reliability and performance estimation,"Presents a case study that enables the early prediction of software reliability and performance at the architecture design stage. Software architecture design is a crucial stage in the software development process, especially in developing large-scale software. Early prediction of the reliability and performance of the software can be used as a basis for making design decisions. We have studied several common architectural styles, with emphasis on the pipe-filter and the batch-sequential styles, and observed the impact of different configurations on reliability and performance measurements. Moreover, several external factors that might have an influence on these measurements are studied. The results show that altering the architecture configuration to attain higher reliability and/or better performance is feasible depending on variations in the execution environment.",
Modeling cell departure for shared buffer ATM switch,"The framework of the performance analysis for a buffered asynchronous transfer mode (ATM) switch usually consists of modeling the input traffic arrivals, the switching mechanism, and the cell departure process. The overall accuracy of the performance results relies on how accurately the cell departure process, especially for shared buffer switches is modelled. Unlike output buffer switches where there are at most one cell that can leave, multiple cells may depart from the shared buffer for shared buffer switches. Modeling the cell departure process is hence more complex for shared buffer switches. It is of practical interest and is challenging to find the appropriate probabilistic model to describe the cell departure process for shared buffer switches. This paper compares and verifies the accuracy of three models, including a new one called ""Urn Model"" proposed by the authors. These models are put under test in a performance evaluation of a shared buffer ATM switch, by using a discrete-time Markov chain. The numerical results are compared to the simulation, and they show that the Urn Model is a good compromise between accuracy and efficiency. This finding is significant because it helps to speed up running an analytical model of a large network while providing satisfactory accuracy.",
An Electronic Student Observatory,"Summary form only given. AESOP (An Electronic Student Observatory Project) is a collection of computer-based data collection tools for instruction and research in computer science education. Our educational environment is one in which 5000 students study independently at a distance, off-line, using software developed for an entry-level, distance education course in computing. Each student is allocated an academic tutor whose job is to support the student by answering queries and by commenting on and grading assignments. Students and tutors interact primarily through e-mail. The aim of the project is to observe these students unobtrusively, electronically, and automatically, and to record the observations in a manner that is useful for both instruction and research. The important objectives for the recorder are that it must create a transcript short enough to be sent via e-mail (by students who pay phone charges), readable both by humans and automated analysis tools, and replayable, so that the student's session can be reproduced on an observer's computer. These sometimes conflicting objectives have led to interesting implementation issues, which are discussed in the paper, which also gives a full description of the recorder and associated replayer.",
A novel impedance-tuned monopole antenna,The insertion of coplanar stubs within printed strip monopole antennas allows for tuning of resonant frequencies and corresponding impedances. Manipulation of stub and antenna dimensions allows the creation of antennas with a single impedance-matched resonance or with two matched resonances with a specified frequency ratio.,
Nops: a conservative parallel simulation engine for TeD,"The paper describes Nops, a conservatively synchronized process oriented parallel simulation system. Nops is designed to support the programming model of the Telecommunications Description Language (TeD), with special emphasis on scaling up to very large network models. We chronicle the decisions beyond Nops' design, describe that design, assess its raw performance relative to CSIM, Maisie, and GTW, and study its ability to scale to large models.",
Selection pressure and performance in spatially distributed evolutionary algorithms,"Recent studies of spatially distributed EAs have formally characterized the selection pressure induced by various selection strategies applied to local neighborhoods of various sizes and shapes. These analyses provide us with the ability to predict the expected behavior of the local neighborhood EAs. In this paper we empirically validate these predictions using the domain of function optimization. We demonstrate the various ways selection pressure can be varied in a spatially distributed EA and show that, from a performance point of view, no optimal selection pressure can be defined since it also depends on the fitness landscape of the problem being solved. Our results suggest that it may be possible to adaptively tune selection pressure by varying a single parameter, the neighborhood radius.",
Combining optimism limiting schemes in time warp based parallel simulations,"The time warp protocol is considered to be an effective synchronization mechanism for parallel discrete event simulation (PDES). However, it is widely recognized that it suffers over-optimistic behavior on the part of the simulation processes that may be very harmful for performance. In current literature, two techniques have been used to counteract this problem: throttling of over-optimistic processes; and scheduling or load balancing. However, the study of these techniques has been primarily done in isolation. We demonstrate using a parameterized simulation model of time warp that an appropriate combination of throttling and global scheduling using LP migration can be very beneficial for performance compared to any one of these schemes acting in isolation. This study forms the basis of the design of more powerful control schemes that use a combination of multiple techniques.",
Instruction-level characterization of scientific computing applications using hardware performance counters,"The paper provides characterization methods based on empirical performance counter measurements. In particular, we provide an instruction-level characterization derived empirically in an effort to demonstrate how architectural limitations in underlying hardware will affect the performance of existing codes. Preliminary results provide promise in code characterization, and empirical/analytical modeling. These include the ability to quantify outstanding miss utilization and stall time attributable to architectural limitations in the CPU and the memory hierarchy. This work further promises insight into quantifying bounds for CPI/sub 0/ or the ideal CPI with infinite, perfect L1 cache. In general, if we can characterize workloads using parameters that are independent of architecture, such as this work, then we can more appropriately compare different architectures in an effort to direct processor/code development.",
An evaluation of factors affecting professional obsolescence of information technology professionals,"Design and development of effective information technology (IT) based systems depends upon a staff of competent information technology professionals (ITPs). Due to the rapid pace of technological innovation, diverging application of IT, and changing role responsibilities of ITPs, it is becoming increasingly difficult for ITPs to maintain up-to-date professional competency. Recent research suggests that, because of outmoded knowledge and skill deficiencies among their IT staff, some firms purposely forgo implementation of emergent technologies. Although not previously examined in IT research, professional obsolescence threats have been acknowledged and evaluated in referent research. Psychologists studying the engineering discipline have suggested individual characteristics, nature of work, and organizational climate as being important determinants of obsolescence. The purpose of this research is to evaluate the relationships between manageable work context factors and degree of professional competency, or conversely obsolescence of ITPs. Structural equation modeling is employed in evaluating the plausibility of the direct-effects model of professional competency. This field study used questionnaires to obtain 161 usable self-report responses from systems analysts. Validity of self-reports were verified using cross-reference ratings from respondents supervisors. The results of this study suggest that individual personality differences and factors of the work environment do effect professional competency levels. Overall, the research model accounted for 44% of the variance in information technologist professional competency.",
Design and implementation of the Relationlog deductive database system,"We describe the design and implementation of Relationlog, a persistent deductive database system. Unlike other related systems such as Aditi, CORAL, LDL, LOLA and Glue-Nail, Relationlog supports effective storage, efficient access and inference of large amounts of data with complex structures and provides a declarative query language that can define recursive views involving complex data and also a declarative data manipulation language to update databases.",
An improved method of synthesizing ground backscatter ionograms for spherical ionospheres,"Synthesis of ground backscatter ionograms is a useful technique for studying the structure of the ionosphere. By synthesizing backscatter ionograms from a known ionosphere, knowledge can be obtained on how variations in the ionosphere affect the shape of a backscatter ionogram. A method allowing the rapid synthesis of ground backscatter ionograms is presented. The vertical ionospheric profile is represented by a series of smoothly joined quasi-parabolic segments which can accurately reproduce both measured and model profiles. The ionosphere is, however, restricted to being spherically symmetric. The synthesis includes corrections for geometrical and time delay focusing. The speed of the technique is valuable when ionospheric state is to be determined by seeking a match between measured and synthesized backscatter ionograms. The restriction of the synthesis to spherically symmetric ionospheres means that the best choice can only be made from this class. Residual differences can be a useful indication of the significance of departures from spherical symmetry. The calculations involved in the synthesis are closely related to those necessary to establish coordinate registration in the location of targets for over-the-horizon radar.",
Self-reconfiguration scheme of 3D-mesh arrays,"Addresses a self-reconfiguration scheme of 3D-mesh arrays. The reconfiguration performance of 3D-mesh is obtained for 3D 1 1/2 . We demonstrated that the proposed HS-scheme achieves high system yield without global information. Although 2D-mesh with 8000 PEs requires much more than 90 x 90 PEs, 3D-mesh array becomes a compact size of arrays. It is seen that the HS-scheme dose not achieve high array yield for a large size ED-mesh since the number of tracks is limited. However, the HS-scheme can be widely applied to 3D-mesh consisting of more PEs, more spare PEs, and more tracks, although other schemes are only for 3D 1 1/2.",
Decreasing process memory requirements by overlapping program portions,"Most compiler optimizations focus on saving time and sometimes occur at the expense of increasing size. Yet processor speeds continue to increase at a faster rate than main memory and disk access times. Processors are now frequently being used in embedded systems that often have strict limitations on the size of programs it can execute. Also, reducing the size of a program may result in improved memory hierarchy performance. This paper describes general techniques for decreasing the memory requirements for a process by automatically overlapping portions of a program. Live range analysis, similar to the analysis used for allocating variables to registers, is used to determine which program portions conflict. Nonconflicting portions are assigned overlapping memory locations. The results show an average decrease of over 10% in process size for a variety of programs with minimal or no dynamic instruction increases.",
Measuring disagreement in groups facing limited choice problems,"A measure of the amount of disagreement, D, in a group facing a problem with limited solution choices is proposed. D is simple to calculate, meaningfully derived and provides a standard scale from 0 to 1 for the disagreement of any size group facing a number of solution choices. It also provides a related measure, d, which allows the measurement of the disagreement of each individual in the group. D essentially compares the number of differences found between pairs of individuals in the group with the number of differences theoretically possible. Extension of the measure to the case where the solution choices are represented by ranked, interval and ratio scale data shows that D is equal to twice the variance of the solution scores, although in this case the maximum value of D may be greater than 1. The properties of this measure are explored and found to be similar to what is expected of a measure of disagreement. An example application is given, illustrating how disagreement at both the individual and group levels can be meaningfully and usefully represented by d and D. The measure was used in an experiment where computer-mediated groups of five subjects, interacting only through a computer network, had to provide group solutions to multi-choice questions with four choice options.",
Inter-iteration optimization of parallel EM algorithm on message-passing multicomputers,"Estimation of the parameters of a probability distribution function is a complicated problem that is frequently encountered in many instances of real world problems. The Expectation Maximization (EM) algorithm often can be employed when there is a many-to-one mapping from all possible distribution patterns to the distribution governing the outcome. With its maximum likelihood (ML) formulation, optimal estimate can be made for the unknown variables after iterations until convergence. A variety of parallel methods have been proposed to boost its performance because of the complexity involved in the algorithm. Despite the efforts, the ML algorithm could not be easily adopted in practice primarily due to both intra- and inter-iteration data dependence problems resulting from the iterative nature of the algorithm. This research builds upon experimentation that demonstrated promising results in speeding up the algorithm in and between iterations using distributed memory message passing architecture.",
Small targets in LADAR images using fuzzy clustering,"We describe an automatic target recognition system for detecting targets in temporal sequences of intensity LADAR images. The system first finds all objects in the images using a method that finds blobs and curves. Then features of the objects are extracted. Next fuzzy c-means (FCM) is used to cluster the objects. Finally, FCM prototypes for each class of objects are relabeled with the training data and used to classify unknown objects using a nearest prototype classifier.",
Rotation Invariant Neural Network-Based Face Detection,,
Geometric aspects of polar and near polar circular orbits for the use of intersatellite links for global communication,"In this paper the polar and the near polar circular orbit constellations are examined for the use of intersatellite links. We have the following definitions: the polar orbit has an inclination angle of exactly 90 deg and the near polar orbit constellation has an inclination angle between 80 deg and 100 deg but not equal to 90 deg. We present the exact equations for the intersatellite distance, the azimuth and elevation angles between two satellites. These equations are applied to an example system, a system that is using intersatellite links and that is based on the circular polar and near polar orbit constellation. To establish an intersatellite link, the distance between two satellites should be small and the variation of the azimuth and elevation angle should also be small. It is shown how a change of the inclination angle influences the whole orbit constellation in order to provide full coverage of the Earth and it is shown how the inclination angle between two satellites on adjacent orbital planes influences the distance, the azimuth, and the elevation angle difference.",
Linear vs. branching time: a complexity-theoretic perspective,"The discussion of the relative merits of linear versus branching time frameworks goes back to early 1980s. One of the beliefs dominating this discussion has been that ""while specifying is easier in LTL (linear-temporal logic), verification is easier for CTL (branching-temporal logic)"". Indeed, the restricted syntax of CTL limits its expressive power and many important behaviours (e.g., strong fairness) can not be specified in CTL. On the other hand, while model checking for CTL can be done in time that is linear in the size of the specification, it takes time that is exponential in the specification for LTL. A closer examination of the the issue reveals, however, that the computational superiority of the branching time framework is perhaps illusory. In this talk we will compare the complexity of branching-time verification vs. Linear-time verification in many scenarios, and show that linear-time verification is not harder and often is even easier than branching-time verification. This suggests that the tradeoff between branching and linear time is not a simple tradeoff between complexity and expressiveness.",
Compression by model combination,"In the probabilistic framework for data compression, a model of the probability distribution of a data source is constructed, and the predicted probability is entropy coded. To achieve better compression, most traditional methods resort to higher order models. However, this approach is limited by memory and often suffers from the context dilution problem. In this paper, we present methods that allow us to combine a few low order models to achieve equivalent or better compression of a high order model. We show that when applying our techniques to bi-level images, we are able to achieve the state of the art compression within the probabilistic framework.",
Neural network for seismic horizon picking,"A Hopfield neural network can solve optimization problems. We use a Hopfield net for seismic horizon picking. The peak position of each seismic wavelet corresponds to one neuron. We transform the constraints for detecting local horizon patterns and the constraints for extracting one horizon each time into the system energy function. From the theory of Hopfield nets, changing the values of neurons can decrease the energy. The system will be stable until the values of neurons are not changed. One horizon is extracted by using the algorithm each time. We remove the extracted horizon from the original seismic data and extract the next horizon until the last horizon is extracted. From experimental results in a bright spot, the picked horizons can match the visual inspection.",
Dispersion managed soliton systems,Dispersion managed solitons have been discovered to have some remarkable properties which indicate an outstanding opportunity for exploitation in transmission systems. This paper will review and interpret these discoveries and discuss the potential for WDM of these solitons for both long distance systems and for the upgrade of the installed fibre base.,
Distributed query optimization using reduction filters,"The optimization of general queries in a distributed database management system is an important research topic. The difficulty is to select the database operations which will process the query and minimize costs. Traditional solutions include the use of heuristic strategies based on semijoin or join operations. We present an approach for general queries which uses reduction filters, which are based on Bloom filters, to minimize data transfers and reduce local processing costs. We discuss related work and present our algorithm-illustrating it with a simple example. We end with a brief discussion of current and future work.",
Adjusting fuzzy weights in fuzzy neural nets,"In order to train fuzzy neural nets fuzzy number weights have to be adjusted. Because fuzzy arithmetic automatically leads to monotonic increasing outputs a direct fuzzification of the backpropagation method does not work. Therefore, the focus is on other strategies like evolutionary algorithms. In this paper we suggest a backpropagation based method of adjusting the weights. Furthermore we can show that for the proposed method convergence can be guaranteed.",
Transactions for Java,"Jest is a Java VM extended to support transactions and general-purpose persistence. Jest allows Java programmers to manipulate any object using transactions and provides resilience to machine failure for these objects. Jest extends Java's current emphasis on safety and reliability to the safe and consistent management of permanent state. Our additions include syntax for transactions and run-time support for durability and atomicity. General-purpose persistence-the ability to make arbitrary kinds of objects persistent-is a key aspect of the design. We provide orthogonal persistence in which any object can be made persistent without regard to type. We do this using persistence-by-reachability, in which an object becomes persistent if it is reachable from a special persistent root. An important aim of our implementation is to explore the techniques and tradeoffs that arise when implementing persistence in a runtime system based on mark-and-compact collection. Having previously studied designs based on copying collection, this work allows us to explore additional parts of the persistence design space. Details of the implementation are provided in the paper. We have tested Jest on a debit-credit benchmark derived from TPC-B. Our system achieves a rate of 83 TPS, very close to the limits allowed by our disk and underlying logging system. Tests of the Java compiler compiling itself both with and without our extensions, suggest that, for applications that do not use transactions, our extensions result in a slowdown of about 7% compared to the original Java implementation. We suggest several possible ways of improving this result.",
A modular and less complex environment representation algorithm [for mobile robots],"The purpose of environment representation is to map the external real-world of the robot (workspace) and its evolution to an internal data structure usable by the motion planning algorithm. This operation is essential in the development of goal-attaining (complete) motion commands for an autonomous robot. In this paper, the authors present a modular environment representation which can readily be used by a hill-climbing search method to find a goal-attaining path for the robot. Capitalizing on the properties of harmonic potential functions and absorbing Markov chains, this paper presents a new method of environment representation which: (i) is able to map the robot environment to local-minima-free potential fields; (ii) is capable of handling exact geometries so that no geometric approximation is required; (iii) requires less memory for data storage than commonly used methods of environment representation; and (iv) is computationally less complex than the existing methods of representation. The process of environment representation is carried out in two stages, as described in the paper.",
Data reduction for lesion detection in dynamic positron-emission tomography (PET),"The reconstruction of dynamic positron emission tomograph (PET) images is time consuming. Previous work to speed up the dynamic image reconstruction was based on low-order approximation using Karhunen-Loeve (KL) transform. Those KL transform basis vectors corresponding to the largest K eigenvalues were retained and then used in the reconstruction process. However, with this approach the kinetic characteristic (time activity curves) of weak signals, such as non-palpable tumors, may be lost or significantly changed in the reconstructed images. In this paper, a metric is found to select a subset of vectors from the KL transform basis, which relates directly to maximizing lesion-to-background ratio (signal-to-noise ratio). The results using computer simulation PET data show that by this approach the contrast of small lesion to the background can be objectively enhanced. The characteristic of the time activity curve in the lesions can also be approximately kept.",
An efficient heuristic-based evolutionary algorithm for solving constraint satisfaction problems,"GENET and EGENET are artificial neural networks with remarkable success in solving hard constraint satisfaction problems (CSPs) such as car sequencing problems. (E)GENET uses the min-conflict heuristic in variable updating to find local minima, and then applies heuristic learning rule(s) to escape the local minima not representing solution(s). In this paper we describe a micro-genetic algorithm (MGA) which generalizes the (E)GENET approach for solving CSPs efficiently. Our proposed MGA integrates the min-conflict heuristic into mutation for reassigning allels (values) to genes (variables). In addition, we derive two methods, based on general principles from evolutionary algorithms, for escaping local minima: population based learning, and look forward. Our preliminary experimental results showed that this evolutionary approach improved on EGENET in solving certain hard instances of CSPs.",
A case analysis of system partitioning and its relationship to high-level synthesis tasks,"In this paper, we investigate the relationship between partitioning and high-level synthesis tasks, namely operation scheduling and resource allocation/binding. The interaction between partitioning and synthesis tasks is explored using IP formulations for four different design approaches representing different strategies for high-level synthesis. The results are quantified by varying three design parameters, namely the partition size bound, resource size bound and latency margin bound. Experimental results show the tradeoff between the quality of synthesis results and the computation cost for different design approaches, while simultaneous partitioning and synthesis tasks gives the best results, and the computational efficiency can be improved by separating scheduling from partitioning.",
A simulation study of packet forwarding methods over ATM: SBR evaluation,"The desire to switch ATM cells at high speed and forward data packets in a connectionless (CL) manner poses a challenging architectural difficulty that has not yet been satisfactorily resolved. This difficulty is mainly due to lack of a packet concept in ATM switches-a packet is a level-3 abstraction, totally hidden from the switch. The switch-borne router (SBR) is a proposed switch/router architecture that makes it possible to switch CL packets at very high speed using ATM technology. This paper introduces the SBR and compares its performance with other forwarding methods using a simulator. Compared to other methods, the SBR allows for a significantly smaller number of open/close VC operations per second, has less buffering requirement, and achieves higher throughput. The same results hold using a real-life Internet packet trace as well as using traffic drawn from a synthetic workload generator.",
Capturing user requirements and priorities for innovative interactive systems,"We present a new method for capturing requirements and priorities that can be used in the development of highly innovative interactive systems. We are concerned with the development of interactive systems that cannot be treated simply as incremental improvements over existing products. In such cases, it is not possible to identify user requirements on the basis of empirical techniques, as there are no instances of use of the product (or products of a similar type) from which to collect data. Consequently, the developers of innovative products must proceed by envisioning the use of the proposed product and examining hypothetical interactions with potential or surrogate users. Our proposed approach provides structure to this process of envisioning and analysing hypothetical use. The method combines techniques drawn from the soft systems methodology, scenario based design, and from Quality Function Deployment (QFD). We illustrate the approach through application to a communications tool to support distributed collaborative software development.",
A computer vision system for lumber production planning,"A computer vision-based system for lumber production planning is described. Computer axial tomography (CT or CAT) images of hardwood logs are analyzed for identification and classification of internal log defects. Individual CT image slices are analyzed for detection of 2-D defects which are correlated across CT image slices in order to establish 3-D support and identify true 3-D defects. Currently, the system is capable of 3-D reconstruction and rendering of the log and its internal defects from the individual CT image slices. It is also capable of simulation and rendering of key machining operations such as sawing and veneering on the 3-D reconstructions of the logs. From the 3-D reconstruction of the log and knowledge of its internal defects, the system can formulate sawing strategies to optimize the yield and grade of the resulting lumber. The system is intended as a decision aid for lumber production planning and an interactive training tool for novice sawyers and machinists.",
"Information engineering across the professions, a new course for students outside EE","A new course teaching the principles and practice of information engineering and technology has been developed and offered twice to date. The course was developed within electrical engineering, but is intended for all students outside of EE. This paper reports results of assessment of the first offering, resulting modifications from these results and some conclusions from the second offering which has just been completed. Course material and the ""virtual textbook"" are available on the web at http://ece.wpi.edu/infoeng.",
The Impact of Distributed Object Technology on Reengineering II: Moving from the Middle,,
Landmark-based navigation using projective invariants,"Landmark-based navigation usually relies on the identification and subsequent recognition of a number of environment objects, that are deemed adequate in describing the workspace structure. This process is inherently difficult in practice, due to the many different poses of an object that may be encountered in navigational trials. To alleviate for that, we propose an approach that employs projective invariants computed on quintuples of points as worldspace landmarks. Such quantities remain invariant under different camera positions and provide for effective description of the workspace structure. In order to identify potential corresponding quintuples in image frames, we introduce a simple test based on the covariance matrix estimate of each quintuple. With this test, we effectively by-pass the calculation of point correspondences. Since the above test indicates correspondence between quintuples, and not between their individual points, we subsequently employ a permutation projective invariant for quintuple recognition. Our approach has been extensively evaluated using synthetic as well as real environments. The results obtained verify its robustness along with its applicability in robotic navigation.",
Decoding algebraic geometric codes,"We present a new algorithm for decoding AG-codes significantly beyond the error-correction bound. Specifically, given a word y whose distance to the AG-code is at most e, where e is a parameter depending on the block length and the dimension of the code, our algorithm produces all codewords that have distance /spl les/e from y. We also discuss modifications of our general algorithm and show how to obtain similar algorithms for binary codes using concatenated codes.",
Learning the human face concept in black and white images,"Presents a learning approach for the face detection problem. The problem can be stated as follows: given an arbitrary black and white, still image, find the location and size of every human face it contains. Numerous applications of automatic face detection have attracted considerable interest in this problem, but no present face detection system is completely satisfactory from the point of view of detection rate, false alarm rate and detection time. We describe an inductive learning-based detection method that produces a maximally specific hypothesis consistent with the training data. Three different sets of features were considered for defining the concept of a human face. The performance achieved is as follows: 85% detection rate, a false alarm rate of 0.04% of the number of windows analyzed and 1 minute detection table for a 320/spl times/240 image on a Sun Ultrasparc 1.",
A recursive method for calculating error probabilities for a Reed-Solomon codeword,"A single Reed-Solomon (RS) codeword is transmitted in a channel where each transmitted symbol may experience a different interference level. Each received symbol is decoded using a hard decision mechanism, and the entire codeword is decoded using a bounded-distance (BD) decoder. Due to complexity and speed concerns, it is common practice to assume the probability of incorrect codeword decoding is negligible, and assume the decoder either correctly decodes the received codeword or fails the decoding process. However, we develop an efficient, recursive, mechanism for generating the approximation probabilities of correct decode, incorrect decode, and decoder failure. In the case where each symbol experiences the same fading level our approximation is a tight, and in the case where each symbol is faded differently we show a fast mechanism which gives a lower bound and upper bound.",
Statically checkable design level traits,"The paper is concerned with those properties of software that can be statically surmised from the source code. Many such properties have been extensively studied from the perspective of compiler construction technology. However, live variable analysis, alias analysis and the such are too low level to be of interest to the software engineer. The authors identify a family of statically checkable properties that should represent a higher level abstraction, and reach the detailed design level. Properties in this family which is defined by five precise distinguishing criteria are called traits. Some examples of traits include mutability, const correctness, ownership, and pure functions. In fact, in many ways, traits are non-standard types. They argue that traits should bring about similar benefits to these of static typing in terms of clarity, understandability, adherence to design decisions, and robustness. They further argue that traits can be used for better checking of substitutability in inheritance relationships. Having made the case for traits, they proceed to describing a taxonomy for classifying and understanding traits and show how it can be used to better understand previous work on this topic. The paper also discusses the abstract computational complexity of traits and compares previous research from that perspective.",
ISE-intelligent synthesis environment for future aerospace systems,"The Intelligent Synthesis Environment (ISE) being developed by NASA, UVA and JPL for significantly enhancing the rapid creation of innovative affordable products and missions is described. ISE uses a synergistic combination of leading-edge technologies, including high-performance computing, high-capacity communications and networking, virtual product development, knowledge-based engineering, computational intelligence, human-computer interaction, and product information management. The environment will link scientists, design teams, manufacturers, suppliers and consultants who participate in the mission synthesis, as well as in the creation and operation of the aerospace system. It will radically advance the process by which complex science missions are synthesized, and high-tech engineering systems are designed, manufactured and operated. The evolution of engineering design is described along with the shortcomings of current product development techniques. The need for ISE to create high-science payoff missions and aerospace systems at affordable costs is discussed. The five major components critical to ISE and some of their subelements are described; namely, human-ISE interaction; infrastructure for distributed collaboration; rapid synthesis and simulation tools; intelligent life-cycle system integration; and cultural change in the creative process. Related government and industry programs are outlined and future impact of ISE on complex missions and aerospace systems is discussed.",
A general framework for interconnecting annotations of software systems,"Computer-supported annotation of software systems and their documentation, including design documentation and source code, is a common and important software engineering activity. Annotated documentation is used in both formal software inspection and informal software maintenance. Viewers of annotated systems may understand the software more easily if annotations are visible not just from the annotated item itself but from other, related items. We propose a general framework for interconnecting annotatable items in software systems to achieve this visibility. We describe filtering and broadening rules that viewers can use to select the annotations they desire to see. We illustrate this framework in the context of object-oriented software system development.",
High-Performance Monte Carlo Tools [Conferences & Workshops],,
Towards a standard class framework for discrete event simulation,"The paper applies recent advances in object-oriented research to propose a class-level framework for discrete event simulations. The advances are in object-oriented analysis and design, the Unified Modeling Language, and the identification of application class frameworks as a discipline in and of itself.",
A rapid prototyping environment for teaching digital logic design,"In this paper, the authors present a rapid prototyping environment for teaching digital logic design. Their primary goal was to build an FPGA based hardware prototyping system that provides sufficient flexibility for the implementation and functional verification of various digital circuit designs. Since the system is intended to be used as an experimental and demonstration board to support digital design courses and practices for the undergraduate students, it is simple to use and completely supported by the software for automatic implementation of digital circuits. The prototyping environment consists of a PC and the programmable board composed of 3 FPGAs. One FPGA is used to implement the digital circuit, while the rest of the devices implement an interface between the digital circuit and the PC. The PC is used for designing a digital circuit, automatic generation of the interface logic, programming the FPGAs and as a test engine for functional verification of the designed circuit. The presented environment is used in the regular educational process and the sample student projects are summarized.",
A simple mechanism to deal with sequential code in dataflow architectures,The aim of this work is to propose a simple and efficient mechanism to deal with the problem of executing sequential code in a pure dataflow machine. Our results is obtained with a simulator of Wolf architecture. The implemented mechanism improved the architecture performance when executing sequential code and we expect that this improvement could be better if we use some heuristics to deal with some special groups of instructions such as branch operations. Further research will show us if this is true.,
Optimal FPGA mapping and retiming with efficient initial state computation,"For sequential circuits with given initial states, new equivalent initial states must be computed for retiming, which unfortunately is NP-hard. In this paper we propose a novel polynomial time algorithm for optimal FPGA mapping with forward retiming to minimize the clock period with guaranteed initial state computation. It enables a new methodology of separating forward retiming from backward retiming to avoid time-consuming iterations between retiming and initial state computation. Our algorithm compares very favorably with both of the conventional approaches of separate mapping followed by retiming and the recent approaches of combined mapping with retiming. It is also applicable to circuits with partial initial state assignment.",
Determination of optimal polygonal approximation using genetic algorithms,"A new polygonal approximation algorithm is presented which gives the minimum number of sides for the approximated polygon under a uniform error norm. In the proposed method, a chromosome is used to represent a polygon and is represented by a binary string. Each bit, called a gene, represents a point on the given curve. The convergence of the method is guaranteed and optimal or near-optimal solutions can be obtained. Some experimental results depict the feasibility of the proposed approach.",
A graph-based exploration strategy of indoor environments by an autonomous mobile robot,"Presents a provably complete strategy for indoor environment exploration by an autonomous mobile robot. Without prior knowledge about the environment, the strategy guarantees the construction of a grid-based map, of the entire reachable area within a bounded region. Multiple map representations are utilized including a topological grid map for guiding the exploration process, a modified occupancy grid for fusing data from multiple range sensors, and a hierarchy of grids for real-time navigation. Experiments using a Nomad 200/sup TM/ robot have shown accurate map construction while navigating at a steady speed of 0.2m/sec.",
Using GIS to visualise environmental information a case study based on digital terrain models,We present an experience of use of geographic information system (GIS) to visualise environmental information. Our experience suggests that digital terrain models may represent a common base to provide consistent information derived combining different data. Some considerations about the use of realistic visualisation (rendering) versus semantic representation based on geometric reasoning are provided.,
Real-time simulation of dynamic systems on systolic arrays,"Systolic arrays have emerged as a powerful means for solving several computational problems of practical importance. This paper discusses the applicability of systolic arrays in the real-time simulation of dynamic systems. Systolic arrays are proposed for the simulation of dynamic systems which can be represented by a set of linear or nonlinear ordinary differential equations. Efficient techniques for solving the differential equations have been chosen in these systolic implementations, so that the real-time constraints can be satisfied, while maintaining both the stability and accuracy of the simulation. The complexity issues of the systolic implementations are also discussed. Conclusions are drawn regarding the efficiency and ease of using the systolic arrays, after comparison with the earlier solutions for this problem.",
Failure handling and coordinated execution of concurrent workflows,"Workflow management systems (WFMSs) coordinate the execution of applications distributed over networks. In WFMSs, data inconsistencies can arise due to: the interaction between steps of concurrent threads within a workflow (intra-workflow coordination); the interaction between steps of concurrent workflows (inter-workflow coordination); and the presence of failures. Since these problems have not received adequate attention, this paper focuses on developing the necessary concepts and infrastructure to handle them. First, to deal with inter- and intra-workflow coordination requirements we have identified a set of high level building blocks. Secondly, to handle failures we propose a novel and pragmatic approach called opportunistic compensation and re-execution that allows a workflow designer to customize workflow recovery from correctness as well as performance perspectives. Thirdly based on these concepts we have designed a workflow specification language that expresses new requirements for workflow executions and implemented a run-time system for managing workflow executions while satisfying the new requirements. These ideas are geared towards improving the modeling and correctness properties offered by WFMSs and making them more robust and flexible.",
An abstract authorization system for the Internet,"Most of the work on Internet security focuses on cryptographic approaches. While valuable, this is not a feasible way to control access to documents. Cryptography can only control secrecy and authentication aspects, but cannot handle different types of access by different users, access to portions of documents, and other content restrictions. A higher-level approach is needed. We present here an authorization model for hypertext documents based on the access matrix. We classify different types of documents, we model these using object-oriented approaches, and we define access policies that specify access to those types of documents. Authorization restrictions can be superimposed on the document class model and on its dynamic model. These authorizations are based on a mandatory version of the access matrix, implementing role-based access control. We consider possible implementation architectures, involving servers and databases.",
A scheme for high-performance data delivery in the Web environment,"The paper describes a scheme for high performance and dependable data storage and delivery in a large scale distributed computing and communication environment such as the Web environment. The proposed scheme utilizes the parallelism of several distributed data servers storing striped data blocks to achieve high throughput. It employs coding techniques to protect the system against data unavailability and hence achieve dependable service. The performance results show that the proposed method has several advantages over traditional ones, such as data service through mirror sites. The error probability of the proposed method is orders of magnitude smaller than that of the mirroring with the same redundancy rate. The data rates for file downloading could be improved significantly by the proposed scheme.",
New approximation algorithms for longest common subsequences,"This paper focuses on finding approximations for the longest common subsequence (lcs) of two strings. Most methods which calculate an approximation for the more general problem accepting N (N/spl ges/3) input strings, give typically trivial results for the restricted case under study. Because of the small number of reliable existing heuristics, we introduce several new ones in this survey. The majority of the presented algorithms give a lower bound for the lcs. Thus they can be used, for example, as a filter to decide quickly, if a more detailed, space- and time-consuming study is needed. A lower bound can also be used to limit the search space of an exact lcs method effectively. The upper bounds complement the information about the true lcs; they form a basis to make a judgement about the reliability of a lower bound. Extensive tests have been carried out to show the strengths of the heuristics and a discussion about their role in various environments is given.",
From the editor-in-chief: Introducing computing in science & engineering,"We hope you will share our excitement as computing in science and engineering enters a new era. In January 1999, the American Institute of Physics joins with the IEEE Computer Society to publish Computing in Science and Engineering, a new bimonthly magazine. CiSE will combine the strengths of our current magazines, Computers in Physics and IEEE Computational Science & Engineering, and replace both of them in a novel joint publishing venture between two outstanding organizations.",
Proceedings. Fifth International Conference on Software Reuse (Cat. No.98TB100203),,
Analyzing the individual/combined effects of speculative and guarded execution on a superscalar architecture,"Speculative execution is a technique by which instructions are executed before the condition that controls it is evaluated. This can increase the performance if some of the idle CPU cycles are now used to execute speculated instructions. Guarded execution is a technique in which the branch instruction is eliminated and control dependencies are converted to data dependencies. This can help reduce some of the side-effects involved with branch instructions besides creating larger compilation units. However, excessive application of either one of them can result in dismal performance. Conventional approaches have used a one-time feedback metric and made all decisions based on it. We present a new way of designing feedback metrics and show how it can be used to regulate the effects of dynamic speculation and the side-effects of applying guarded execution statically. The proposed method presents 0.3 to 0.6-fold improvements over a conventional scheme using SPEC benchmarks.",
Resurrecting Ada's Rendez-Vous in Java,"Java is a programming language designed with concurrency in mind from its inception. However the synchronization mechanism provided is a restricted version of Hoare's Monitors, being too low level for most applications. The authors propose a high level synchronization mechanism for Java, based on Ada's Rendez-Vous, adapting the notation and semantics to Java. The result is a nice and readable notation to control concurrency, even cleaner than the Ada original version. The Rendez-Vous syntax adds only one statement to Java, and they developed a preprocessor to translate the new statement to pure Java, using a class library which supports messages. Their implementation is available for downloading over the Internet.",
An interpolation method using signal recovery and discrete Fourier transform,"In this work, we develop a DFT-based method for the interpolation of a real sequence that does not necessarily satisfy the Shannon-Whittaker sampling condition. Our derivation provides an insightful perspective to the interpolation problem and inspires us to formulate the interpolation as a signal recovery problem in the discrete frequency space. In combination with the downsampling operation and the Fourier shift theorem, the proposed interpolation scheme can be used to obtained regular samples of the function in question with arbitrary starting position and sampling density. All computations involved, in addition to FFT, are simple arithmetical operations; consequently, the proposed method is computationally efficient. Our computer experiments demonstrate that the proposed method can produce good interpolation results even when the functions are severely undersampled.",
Investigating maintenance processes in a framework-based environment,"The empirical study described in this paper focuses on the effectiveness of maintenance processes in an environment in which a repository of potential sources of reuse exists, e.g. a context in which applications are built using an object-oriented framework. Such a repository might contain current and previous releases of the system under maintenance, as well as other applications that are built on a similar structure or contain similar functionality. This paper presents an observational study of 15 student projects in framework-based environment. We used a mix of qualitative and quantitative methods to identify and evaluate the effectiveness of the maintenance processes.",
Competency-based engineering design courses development,"Henry Cogswell College (USA) has developed a set of competency-based engineering design curricula for technical (engineering, engineering technology and computer science) degree programs. These courses add more value to the graduates' educational experience and at the same time satisfy program and institutional accreditation requirements. At the core of these curricula is a common sophomore-level introduction to engineering design course and a capstone sequence of senior design projects. The objective of these courses is to teach students the engineering design process as well as the skills and competencies based on industry-identified skill standards. An extensive educational outcomes assessment plan is used to evaluate the effectiveness of the design curricula against their specified goals.",
Polyhedral approximation and first order segmentation of unstructured point sets,The paper is concerned with the first two steps in a surface reconstruction process. Given a set of 3D points sampled from a physical model the first problem is that of creating a polyhedral approximation of the model. For that the authors introduce an algorithm which extends Boissonnat's (1984) work. It allows the reconstruction of objects with arbitrary genus and proposes an automatic termination procedure. The next step in the process concerns the segmentation of the data points into regions for which each may be fitted by a single surface. They summarize some experiences with a region growing technique based on angle between normals criteria. Using just first order derivative estimations it is shown that the method is able to classify segments into predefined second order surface classes.,
Object oriented design of a mixed simulator,"An interactive simulation environment prototype for the study of combined models (discrete and/or continuous) has been implemented using the C++ language. Inspiring from SLAM II, a new simulation language, called TOOMS (Turbo Object-Oriented Mixed Simulator), has been designed and developed under the object-oriented approach rules. The new and most relevant aspect of this new simulation language is that it supports the execution of different models or of several submodels issued from one model.",
The Integration of HCI and Software Engineering,,
Preparing students for industrial teamwork: a seasoned software engineering curriculum,"This paper reports on a two-term workshop taught as part of a master's degree in computer science. Lectures on software engineering (SE) methods, major issues in project management and work organisation accompany the workshop. The organisational framework, SE course schedule and contents aim at meeting industrial needs with limited personnel and technical resources, while maintaining good academic standards. Teaching project management in parallel with experiencing work organisation in teamwork is a major factor in tailoring SE education towards practical needs. The authors' SE consulting and project management experiences in industry have tuned the course priorities to caution with 'latest' methods or tools, focus on experiencing roles in SE teams, work organisation at team level as well as at the individual level; and focus on the careful selection of team supervisors.",
What's ahead for design on the web,,
Analytic fuzzy RBF neural network,An analytic fuzzy neural network with a modified RBF architecture and fuzzy weights is introduced. The fuzzy weights are non-symmetric fuzzy numbers. The learning algorithm is based on a gradient technique.,
Connecting perception and action by associating symmetries in vision and language,"Symmetries are of fundamental importance in information processing. It is well understood how important visual symmetries are in the understanding of scenes. This paper explores the symmetries of languages of actions, and relates them to visual symmetries. Linguistic symmetry is explored in the context of problem solving. Algebraic techniques permit the representation and manipulation of linguistic symmetry, and the exploration of its connection to perceptual symmetry. It is shown that the invariants of the decomposition of a language of actions form a complete and independent set of perceptual features that naturally describe the search space.",
Software safety analysis using rough sets,"This paper presents a new method to evaluate software safety using rough set theory. The data about the software product and process are collected via a questionnaire. The result is the direct assessment of the software safety in terms of a single coefficient, whose goodness is then analyzed using rough sets. An automatic tool for the Windows platform has been developed to help in the interactive analysis. Results from a real experiment of software safety evaluation are discussed.",
Computing stress tests for gate-oxide shorts,Reliability screens are used to reduce infant mortality. The quality of the stress test set used during the screening process has a direct bearing on the effectiveness of the screen. We present a formal study of the problem of computing good quality stress tests for gate-oxide shorts which is the cause of much of the reliability problems. A method to compute stress test which is better than the popular method of using I/sub DDQ/ vectors is presented.,
Synaptic Runaway in Associative Networks and the Pathogenesis of Schizophrenia,"Synaptic runaway denotes the formation of erroneous synapses and premature functional decline accompanying activity-dependent learning in neural networks. This work studies synaptic runaway both analytically and numerically in binary-firing associative memory networks. It turns out that synaptic runaway is of fairly moderate magnitude in these networks under normal, baseline conditions. However, it may become extensive if the threshold for Hebbian learning is reduced. These findings are combined with recent evidence for arrested N-methyl-D-aspartate (NMDA) maturation in schizophrenics, to formulate a new hypothesis concerning the pathogenesis of schizophrenic psychotic symptoms in neural terms.",
An integrated computer architecture experience,"This paper presents a collaborative effort to combine a computer architecture lecture course with a computer topics laboratory. This integrated course provided the students with an opportunity to apply VHDL modeling to semester-long projects that illustrated many of the points learned in the lecture course. It also gave the students an opportunity to see the same material from two different perspectives, and increased teamwork skills of both the students and the faculty members involved.",
RODAIN: a highly available real-time main-memory database system,"The RODAIN database system has been designed to be used in telecommunications. It is a highly available real time object-oriented database system. The main challenge is that typical transactions are heterogeneous: short simple queries, simple updates, and long massive updates. A RODAIN node consists of a primary node and of a mirror node that are symmetrical and have the same subsystems but their functionality depends on the role. The User Request Interpreter Subsystem (URIS) accepts client requests and forwards them to the TRansaction Process (TRP). Each TRP executes one transaction at time but can handle sequential execution of multiple transactions. The Run-time Transaction Controller (RTC) allocates a TRP for an arriving transaction. TRPs execute the transactions and access the database through Object Request Dispatcher (ORD). ORD sends transaction logs to Fault-Tolerance and Recovery Subsystem (FTRS) for storing. The FTRS on Primary Node communicates with the FTRS on Mirror Node to store the logs permanently in the Secondary Storage Subsystem (SSS). Watchdog monitors the other processes and initiates node recovery actions when needed.",
