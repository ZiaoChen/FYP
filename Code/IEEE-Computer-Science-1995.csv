Title,Abstract,Keywords
"Mean shift, mode seeking, and clustering","Mean shift, a simple interactive procedure that shifts each data point to the average of data points in its neighborhood is generalized and analyzed in the paper. This generalization makes some k-means like clustering algorithms its special cases. It is shown that mean shift is a mode-seeking process on the surface constructed with a ""shadow"" kernal. For Gaussian kernels, mean shift is a gradient mapping. Convergence is studied for mean shift iterations. Cluster analysis if treated as a deterministic problem of finding a fixed point of mean shift that characterizes the data. Applications in clustering and Hough transform are demonstrated. Mean shift is also considered as an evolutionary strategy that performs multistart global optimization.","Kernel,
Clustering algorithms,
Convergence,
Iterative algorithms,
Surface treatment,
Algorithm design and analysis,
Computer science"
Human and machine recognition of faces: a survey,"The goal of this paper is to present a critical survey of existing literature on human and machine recognition of faces. Machine recognition of faces has several applications, ranging from static matching of controlled photographs as in mug shots matching and credit card verification to surveillance video images. Such applications have different constraints in terms of complexity of processing requirements and thus present a wide range of different technical challenges. Over the last 20 years researchers in psychophysics, neural sciences and engineering, image processing analysis and computer vision have investigated a number of issues related to face recognition by humans and machines. Ongoing research activities have been given a renewed emphasis over the last five years. Existing techniques and systems have been tested on different sets of images of varying complexities. But very little synergism exists between studies in psychophysics and the engineering literature. Most importantly, there exists no evaluation or benchmarking studies using large databases with the image quality that arises in commercial and law enforcement applications In this paper, we first present different applications of face recognition in commercial and law enforcement sectors. This is followed by a brief overview of the literature on face recognition in the psychophysics community. We then present a detailed overview of move than 20 years of research done in the engineering community. Techniques for segmentation/location of the face, feature extraction and recognition are reviewed. Global transform and feature based methods using statistical, structural and neural classifiers are summarized.","Humans,
Face recognition,
Psychology,
Law enforcement,
Image recognition,
Credit cards,
Surveillance,
Application software,
Image processing,
Image analysis"
Mining sequential patterns,"We are given a large database of customer transactions, where each transaction consists of customer-id, transaction time, and the items bought in the transaction. We introduce the problem of mining sequential patterns over such databases. We present three algorithms to solve this problem, and empirically evaluate their performance using synthetic data. Two of the proposed algorithms, AprioriSome and AprioriAll, have comparable performance, albeit AprioriSome performs a little better when the minimum number of customers that must support a sequential pattern is low. Scale-up experiments show that both AprioriSome and AprioriAll scale linearly with the number of customer transactions. They also have excellent scale-up properties with respect to the number of transactions per customer and the number of items in a transaction.","Itemsets,
Transaction databases,
Marketing and sales,
Computer science,
Videos"
The SPLASH-2 programs: characterization and methodological considerations,"The SPLASH-2 suite of parallel applications has recently been released to facilitate the study of centralized and distributed shared-address-space multiprocessors. In this context, the paper has two goals. One is to quantitatively characterize the SPLASH-2 programs in terms of fundamental properties and architectural interactions that are important to understand them well. The properties we study include the computational load balance, communication to computation ratio and traffic needs, important working set sizes, and issues related to spatial locality, as well as how these properties scale with problem size and the number of processors. The other, related goal is methodological: to assist people who will use the programs in architectural evaluations to prune the space of application and machine parameters in an informed and meaningful way. For example, by characterizing the working sets of the applications, we describe which operating points in terms of cache size and problem size are representative of realistic situations, which are not, and which re redundant. Using SPLASH-2 as an example, we hope to convey the importance of understanding the interplay of problem size, number of processors, and working sets in designing experiments and interpreting their results.","Permission,
Laboratories,
Context,
Delay,
Distributed computing,
Computer science,
Graphics,
Application software,
Sensitivity analysis"
On cluster validity for the fuzzy c-means model,"Many functionals have been proposed for validation of partitions of object data produced by the fuzzy c-means (FCM) clustering algorithm. We examine the role a subtle but important parameter-the weighting exponent m of the FCM model-plays in determining the validity of FCM partitions. The functionals considered are the partition coefficient and entropy indexes of Bezdek, the Xie-Beni (1991), and extended Xie-Beni indexes, and the Fukuyama-Sugeno index (1989). Limit analysis indicates, and numerical experiments confirm, that the Fukuyama-Sugeno index is sensitive to both high and low values of m and may be unreliable because of this. Of the indexes tested, the Xie-Beni index provided the best response over a wide range of choices for the number of clusters, (2-10), and for m from 1.01-7. Finally, our calculations suggest that the best choice for m is probably in the interval [1.5, 2.5], whose mean and midpoint, m=2, have often been the preferred choice for many users of FCM.","Clustering algorithms,
Fuzzy sets,
Partitioning algorithms,
Entropy,
Testing,
Unsupervised learning,
Prototypes,
Equations,
Computer science,
Fuzzy logic"
TCP Vegas: end to end congestion avoidance on a global Internet,"Vegas is an implementation of TCP that achieves between 37 and 71% better throughput on the Internet, with one-fifth to one-half the losses, as compared to the implementation of TCP in the Reno distribution of BSD Unix. This paper motivates and describes the three key techniques employed by Vegas, and presents the results of a comprehensive experimental performance study, using both simulations and measurements on the Internet, of the Vegas and Reno implementations of TCP.","Internet,
Protocols,
Throughput,
Testing,
Bandwidth,
Programmable control,
Adaptive control,
Jacobian matrices,
Computer science,
TCPIP"
An introduction to wavelets,"Wavelets were developed independently by mathematicians, quantum physicists, electrical engineers and geologists, but collaborations among these fields during the last decade have led to new and varied applications. What are wavelets, and why might they be useful to you? The fundamental idea behind wavelets is to analyze according to scale. Indeed, some researchers feel that using wavelets means adopting a whole new mind-set or perspective in processing data. Wavelets are functions that satisfy certain mathematical requirements and are used in representing data or other functions. Most of the basic wavelet theory has now been done. The mathematics have been worked out in excruciating detail, and wavelet theory is now in the refinement stage. This involves generalizing and extending wavelets, such as in extending wavelet packet techniques. The future of wavelets lies in the as-yet uncharted territory of applications. Wavelet techniques have not been thoroughly worked out in such applications as practical data analysis, where, for example, discretely sampled time-series data might need to be analyzed. Such applications offer exciting avenues for exploration.","Wavelet analysis,
Performance analysis,
Mathematics,
Prototypes,
Image coding,
Fourier series,
Geology,
Collaboration,
Signal resolution,
Signal analysis"
A scheduling model for reduced CPU energy,"The energy usage of computer systems is becoming an important consideration, especially for battery-operated systems. Various methods for reducing energy consumption have been investigated, both at the circuit level and at the operating systems level. In this paper, we propose a simple model of job scheduling aimed at capturing some key aspects of energy minimization. In this model, each job is to be executed between its arrival time and deadline by a single processor with variable speed, under the assumption that energy usage per unit time, P, is a convex function, of the processor speed s. We give an off-line algorithm that computes, for any set of jobs, a minimum-energy schedule. We then consider some on-line algorithms and their competitive performance for the power function P(s)=s/sup p/ where p/spl ges/2. It is shown that one natural heuristic, called the Average Rate heuristic, uses at most a constant times the minimum energy required. The analysis involves bounding the largest eigenvalue in matrices of a special type.","Processor scheduling,
Energy consumption,
Portable computers,
Central Processing Unit,
Computer displays,
Circuits,
Operating systems,
Scheduling algorithm,
Energy conservation,
Personal digital assistants"
Simultaneous multithreading: Maximizing on-chip parallelism,"This paper examines simultaneous multithreading, a technique permitting several independent threads to issue instructions to a superscalar's multiple functional units in a single cycle. We present several models of simultaneous multithreading and compare them with alternative organizations: a wide superscalar, a fine-grain multithreaded processor, and single-chip, multiple-issue multiprocessing architectures. Our results show that both (single-threaded) superscalar and fine-grain multithreaded architectures are limited in their ability to utilize the resources of a wide-issue processor. Simultaneous multithreading has the potential to achieve 4 times the throughput of a superscalar, and double that of fine-grain multi-threading. We evaluate several cache configurations made possible by this type of organization and evaluate tradeoffs between them. We also show that simultaneous multithreading is an attractive alternative to single-chip multiprocessors; simultaneous multithreaded processors with a variety of organizations outperform corresponding conventional multiprocessors with similar execution resources. While simultaneous multithreading has excellent potential to increase processor utilization, it can add substantial complexity to the design. We examine many of these complexities and evaluate alternative organizations in the design space.","Multithreading,
Yarn,
Samarium,
Parallel processing,
Modems,
Permission,
Delay,
Switches,
Computer science,
Throughput"
I-TCP: indirect TCP for mobile hosts,"IP based solutions to accommodate mobile hosts within existing internetworks do not address the distinctive features of wireless mobile computing. IP-based transport protocols thus suffer from poor performance when a mobile host communicates with a host on the fixed network. This is caused by frequent disruptions in network layer connectivity due to i) mobility and ii) unreliable nature of the wireless link. We describe I-TCP, which is an indirect transport layer protocol for mobile hosts. I-TCP utilizes the resources of Mobility Support Routers (MSRs) to provide transport layer communication between mobile hosts and hosts on the fixed network. With I-TCP, the problems related to mobility and unreliability of wireless link are handled entirely within the wireless link; the TCP/IP software on the fixed hosts is not modified. Using I-TCP on our testbed, the throughput between a fixed host and a mobile host improved substantially in comparison to regular TCP.","TCPIP,
Mobile computing,
Transport protocols,
Internet,
Mobile communication,
Testing,
Throughput"
A case for NOW (Networks of Workstations),"Networks of workstations are poised to become the primary computing infrastructure for science and engineering. NOWs may dramatically improve virtual memory and file system performance; achieve cheap, highly available, and scalable file storage: and provide multiple CPUs for parallel computing. Hurdles that remain include efficient communication hardware and software, global coordination of multiple workstation operating systems, and enterprise-scale network file systems. Our 100-node NOW prototype aims to demonstrate practical solutions to these challenges.","Workstations,
File systems,
Computer networks,
Design engineering,
Parallel processing,
Hardware,
Communication system software,
Operating systems,
Software prototyping,
Prototypes"
Asynchronous design methodologies: an overview,"Asynchronous design has been an active area of research since at least the mid 1950's, but has yet to achieve widespread use. We examine the benefits and problems inherent in asynchronous computations, and in some of the more notable design methodologies. These include Huffman asynchronous circuits, burst-mode circuits, micropipelines, template-based and trace theory-based delay-insensitive circuits, signal transition graphs, change diagrams, and complication-based quasi-delay-insensitive circuits.","Design methodology,
Asynchronous circuits,
Delay"
Abstractions for software architecture and tools to support them,"Architectures for software use rich abstractions and idioms to describe system components, the nature of interactions among the components, and the patterns that guide the composition of components into systems. These abstractions are higher level than the elements usually supported by programming languages and tools. They capture packaging and interaction issues as well as computational functionality. Well-established (if informal) patterns guide the architectural design of systems. We sketch a model for defining architectures and present an implementation of the basic level of that model. Our purpose is to support the abstractions used in practice by software designers. The implementation provides a testbed for experiments with a variety of system construction mechanisms. It distinguishes among different types of components and different ways these components can interact. It supports abstract interactions such as data flow and scheduling on the same footing as simple procedure call. It can express and check appropriate compatibility restrictions and configuration constraints. It accepts existing code as components, incurring no runtime overhead after initialization. It allows easy incorporation of specifications and associated analysis tools developed elsewhere. The implementation provides a base for extending the notation and validating the model.",
A discrete dynamic contour model,"A discrete dynamic model for defining contours in 2-D images is developed. The structure of this model is a set of connected vertices. With a minimum of interaction, an initial contour model can be defined, which is then automatically modified by an energy minimizing process. The internal energy of the model depends on local contour curvature, while the external energy is derived from image features. Solutions are presented to avoid undesirable deformation effects, like shrinking and vertex clustering, which are common in existing active contour models. The deformation process stops when a local minimum of the energy function is reached. The final shape of the model is a reproducible approximation of the desired contour. Results of applying the method to computer-generated images, as well as clinical images, are presented.","Deformable models,
Spline,
Reproducibility of results,
Image segmentation,
Biomedical imaging,
Active contours,
Shape,
Application software,
Mice,
Graphics"
State transition analysis: a rule-based intrusion detection approach,"The paper presents a new approach to representing and detecting computer penetrations in real time. The approach, called state transition analysis, models penetrations as a series of state changes that lead from an initial secure state to a target compromised state. State transition diagrams, the graphical representation of penetrations, identify precisely the requirements for and the compromise of a penetration and present only the critical events that must occur for the successful completion of the penetration. State transition diagrams are written to correspond to the states of an actual computer system, and these diagrams form the basis of a rule based expert system for detecting penetrations, called the state transition analysis tool (STAT). The design and implementation of a Unix specific prototype of this expert system, called USTAT, is also presented. This prototype provides a further illustration of the overall design and functionality of this intrusion detection approach. Lastly, STAT is compared to the functionality of comparable intrusion detection tools.","Intrusion detection,
Expert systems,
Data analysis,
Prototypes,
Data security,
Computer security,
Software,
Computer science,
Information analysis,
Research and development"
Efficient classification for multiclass problems using modular neural networks,"The rate of convergence of net output error is very low when training feedforward neural networks for multiclass problems using the backpropagation algorithm. While backpropagation will reduce the Euclidean distance between the actual and desired output vectors, the differences between some of the components of these vectors increase in the first iteration. Furthermore, the magnitudes of subsequent weight changes in each iteration are very small, so that many iterations are required to compensate for the increased error in some components in the initial iterations. Our approach is to use a modular network architecture, reducing a K-class problem to a set of K two-class problems, with a separately trained network for each of the simpler problems. Speedups of one order of magnitude have been obtained experimentally, and in some cases convergence was possible using the modular approach but not using a nonmodular network.","Neural networks,
Convergence,
Feedforward neural networks,
Euclidean distance,
Testing,
Computer science,
Information science,
Standards development"
Rectangle-packing-based module placement,"The first and the most critical stage in VLSI layout design is the placement, the background of which is the rectangle packing problem: Given many rectangular modules of arbitrary site, place them without overlapping on a layer in the smallest bounding rectangle. Since the variety of the packing is infinite (two-dimensionally continuous) many, the key issue for successful optimization is in the introduction of a P-admissible solution space, which is a finite set of solutions at least one of which is optimal. This paper proposes such a solution space where each packing is represented by a pair of module name sequences. Searching this space by simulated annealing, hundreds of modules could be successfully packed as demonstrated. Combining a conventional wiring method, the biggest MCNC benchmark ami49 is challenged.",
Program comprehension during software maintenance and evolution,"Code cognition models examine how programmers understand program code. The authors survey the current knowledge in this area by comparing six program comprehension models: the Letovsky (1986) model; the Shneiderman and Mayer (1979) model; the Brooks (1983) model; Soloway, Adelson and Ehrlich's (1988) top-down model; Pennington's (1987) bottom-up model; and the integrated metamodel of von Mayrhauser and Vans (1994). While these general models can foster a complete understanding of a piece of code, they may not always apply to specialized tasks that more efficiently employ strategies geared toward partial understanding. We identify open questions, particularly considering the maintenance and evolution of large-scale code. These questions relate to the scalability of existing experimental results with small programs, the validity and credibility of results based on experimental procedures, and the challenges of data availability.","Software maintenance,
Cognition,
Cognitive science,
Programming profession,
Application software,
Computer architecture,
Documentation,
Large-scale systems,
Scalability,
Round robin"
An adaptive optimal-kernel time-frequency representation,"Time-frequency representations with fixed windows or kernels figure prominently in many applications, but perform well only for limited classes of signals. Representations with signal-dependent kernels can overcome this limitation. However, while they often perform well, most existing schemes are block-oriented techniques unsuitable for on-line implementation or for tracking signal components with characteristics that change with time. The time-frequency representation developed in the present paper, based on a signal-dependent radially Gaussian kernel that adapts over time, surmounts these difficulties. The method employs a short-time ambiguity function both for kernel optimization and as an intermediate step in computing constant-time slices of the representation. Careful algorithm design provides reasonably efficient computation and allows on-line implementation. Certain enhancements, such as cone-kernel constraints and approximate retention of marginals, are easily incorporated with little additional computation. While somewhat more expensive than fixed kernel representations, this new technique often provides much better performance. Several examples illustrate its behavior on synthetic and real-world signals.","Time frequency analysis,
Kernel,
Signal processing,
Signal analysis,
Fourier transforms,
Signal design,
Optimization methods,
Algorithm design and analysis,
Computer science education,
Educational programs"
Effective hardware-based data prefetching for high-performance processors,"Memory latency and bandwidth are progressing at a much slower pace than processor performance. In this paper, we describe and evaluate the performance of three variations of a hardware function unit whose goal is to assist a data cache in prefetching data accesses so that memory latency is hidden as often as possible. The basic idea of the prefetching scheme is to keep track of data access patterns in a reference prediction table (RPT) organized as an instruction cache. The three designs differ mostly on the timing of the prefetching. In the simplest scheme (basic), prefetches can be generated one iteration ahead of actual use. The lookahead variation takes advantage of a lookahead program counter that ideally stays one memory latency time ahead of the real program counter and that is used as the control mechanism to generate the prefetches. Finally the correlated scheme uses a more sophisticated design to detect patterns across loop levels. These designs are evaluated by simulating the ten SPEC benchmarks on a cycle-by-cycle basis. The results show that 1) the three hardware prefetching schemes all yield significant reductions in the data access penalty when compared with regular caches, 2) the benefits are greater when the hardware assist augments small on-chip caches, and 3) the lookahead scheme is the preferred one cost-performance wise.","Prefetching,
Delay,
Hardware,
Bandwidth,
Counting circuits,
Computer science,
Timing,
Predictive models,
Bridges,
Coherence"
On computing three-finger force-closure grasps of polygonal objects,This paper addresses the problem of computing stable grasps of 2-D polygonal objects. We consider the case of a hand equipped with three hard fingers and assume point contact with friction. We prove new sufficient conditions for equilibrium and force closure that are linear in the unknown grasp parameters. This reduces computing the stable grasp regions in configuration space to constructing the three-dimensional projection of a five-dimensional polytope. We present an efficient projection algorithm based on linear programming and variable elimination among linear constraints. Maximal object segments where fingers can be positioned independently while ensuring force closure are found by linear optimization within the grasp regions. The approach has been implemented and several examples are presented.,"Fingers,
Friction,
Sufficient conditions,
Space technology,
Computer science,
USA Councils,
Projection algorithms,
Mechanical factors,
Robots,
Resists"
VLSI architectures for the discrete wavelet transform,"A class of VLSI architectures based on linear systolic arrays, for computing the 1-D Discrete Wavelet Transform (DWT), is presented. The various architectures of this class differ only in the design of their routing networks, which could be systolic, semisystolic, or RAM-based. These architectures compute the Recursive Pyramid Algorithm, which is a reformulation of Mallat's pyramid algorithm for the DWT. The DWT is computed in real time (running DWT), using just N/sub w/(J-1) cells of storage, where N/sub w/ is the length of the filter and J is the number of octaves. They are ideally suited for single-chip implementation due to their practical I/O rate, small storage, and regularity. The N-point 1-D DWT is computed in 2N cycles. The period can be reduced to N cycles by using N/sub w/ extra MAC's. Our architectures are shown to be optimal in both computation time and in area. A utilization of 100% is achieved for the linear array. Extensions of our architecture for computing the M-band DWT are discussed. Also, two architectures for computing the 2-D DWT (separable case) are discussed. One of these architectures, based on a combination of systolic and parallel filters, computes the N/sup 2/-point 2-D DWT, in real time, in N/sup 2/+N cycles, using 2NN/sub w/ cells of storage.","Discrete wavelet transforms,
Very large scale integration,
Computer architecture,
Systolic arrays,
Signal processing algorithms,
Computer science,
Filter bank,
Routing,
Concurrent computing,
Signal resolution"
WISE design of indoor wireless systems: practical computation and optimization,"Designing a low-power system for wireless communication within a building might seem simple. Not so-walls can affect signal strength in ways that are hard to calculate. The paper considers how AT&T's WISE software uses CAD, computational geometry, and optimization to quickly plan where to place base-station transceivers.",
The deferrable server algorithm for enhanced aperiodic responsiveness in hard real-time environments,"Most existing scheduling algorithms for hard real-time systems apply either to periodic tasks or aperiodic tasks but not to both. In practice, real-time systems require an integrated, consistent approach to scheduling that is able to simultaneously meet the timing requirements of hard deadline periodic tasks, hard deadline aperiodic (alert-class) tasks, and soft deadline aperiodic tasks. This paper introduces the Deferrable Server (DS) algorithm which will be shown to provide improved aperiodic response time performance over traditional background and polling approaches. Taking advantage of the fact that, typically, there is no benefit in early completion of the periodic tasks, the Deferrable Server (DS) algorithm assigns higher priority to the aperiodic tasks up until the point where the periodic tasks would start to miss their deadlines. Guaranteed alert-class aperiodic service and greatly reduced response times for soft deadline aperiodic tasks are important features of the DS algorithm, and both are obtained with the hard deadlines of the periodic tasks still being guaranteed. The results of a simulation study performed to evaluate the response time performance of the new algorithm against traditional background and polling approaches are presented. In all cases, the response times of aperiodic tasks are significantly reduced (often by an order of magnitude) while still maintaining guaranteed periodic task deadlines.","Delay,
Scheduling algorithm,
Real time systems,
Timing,
Time division multiplexing,
Performance evaluation,
Oceans,
Statistics,
Software engineering,
Computer science"
Symmetry as a continuous feature,"Symmetry is treated as a continuous feature and a continuous measure of distance from symmetry in shapes is defined. The symmetry distance (SD) of a shape is defined to be the minimum mean squared distance required to move points of the original shape in order to obtain a symmetrical shape. This general definition of a symmetry measure enables a comparison of the ""amount"" of symmetry of different shapes and the ""amount"" of different symmetries of a single shape. This measure is applicable to any type of symmetry in any dimension. The symmetry distance gives rise to a method of reconstructing symmetry of occluded shapes. The authors extend the method to deal with symmetries of noisy and fuzzy data. Finally, the authors consider grayscale images as 3D shapes, and use the symmetry distance to find the orientation of symmetric objects from their images, and to find locally symmetric regions in images.",
Combining multiple neural networks by fuzzy integral for robust classification,"In the area of artificial neural networks, the concept of combining multiple networks has been proposed as a new direction for the development of highly reliable neural network systems. The authors propose a method for multinetwork combination based on the fuzzy integral. This technique nonlinearly combines objective evidence, in the form of a fuzzy membership function, with subjective evaluation of the worth of the individual neural networks with respect to the decision. The experimental results with the recognition problem of on-line handwriting characters confirm the superiority of the presented method to the other voting techniques.","Neural networks,
Fuzzy neural networks,
Robustness,
Artificial neural networks,
Fuzzy systems,
Character recognition,
Computer science,
Statistics,
Handwriting recognition,
Voting"
Comparing detection methods for software requirements inspections: a replicated experiment,"Software requirements specifications (SRS) are often validated manually. One such process is inspection, in which several reviewers independently analyze all or part of the specification and search for faults. These faults are then collected at a meeting of the reviewers and author(s). Usually, reviewers use Ad Hoc or Checklist methods to uncover faults. These methods force all reviewers to rely on nonsystematic techniques to search for a wide variety of faults. We hypothesize that a Scenario-based method, in which each reviewer uses different, systematic techniques to search for different, specific classes of faults, will have a significantly higher success rate. We evaluated this hypothesis using a 3/spl times/2/sup 4/ partial factorial, randomized experimental design. Forty eight graduate students in computer science participated in the experiment. They were assembled into sixteen, three-person teams. Each team inspected two SRS using some combination of Ad Hoc, Checklist or Scenario methods. For each inspection we performed four measurements: (1) individual fault detection rate, (2) team fault detection rate, (3) percentage of faults first identified at the collection meeting (meeting gain rate), and (4) percentage of faults first identified by an individual, but never reported at the collection meeting (meeting loss rate). The experimental results are that (1) the Scenario method had a higher fault detection rate than either Ad Hoc or Checklist methods, (2) Scenario reviewers were more effective at detecting the faults their scenarios are designed to uncover, and were no less effective at detecting other faults than both Ad Hoc or Checklist reviewers, (3) Checklist reviewers were no more effective than Ad Hoc reviewers, and (4) Collection meetings produced no net improvement in the fault detection rate-meeting gains were offset by meeting losses.","Inspection,
Fault detection,
Fault diagnosis,
Design for experiments,
Computer science,
Assembly,
Performance evaluation,
Gain measurement,
Loss measurement,
Performance gain"
Private information retrieval,"We describe schemes that enable a user to access k replicated copies of a database (k/spl ges/2) and privately retrieve information stored in the database. This means that each individual database gets no information on the identity of the item retrieved by the user. For a single database, achieving this type of privacy requires communicating the whole database, or n bits (where n is the number of bits in the database). Our schemes use the replication to gain substantial saving. In particular, we have: A two database scheme with communication complexity of O(n/sup 1/3/). A scheme for a constant number, k, of databases with communication complexity O(n/sup 1/k/). A scheme for 1/3 log/sub 2/ n databases with polylogarithmic (in n) communication complexity.","Information retrieval,
Complexity theory,
Data privacy,
Protection,
Distributed databases"
Energy efficient CMOS microprocessor design,"Reduction of power dissipation in microprocessor design is becoming a key design constraint. This is motivated not only by portable electronics, in which battery weight and size is critical, but by heat dissipation issues in larger desktop and parallel machines as well. By identifying the major modes of computation of these processors and by proposing figures of merit for each of these modes, a power analysis methodology is developed. It allows the energy efficiency of various architectures to be quantified, and provides techniques for either individually optimizing or trading off throughput and energy consumption. The methodology is then used to qualify three important design principles for energy-efficient microprocessor design.","Energy efficiency,
Microprocessors,
Power dissipation,
Batteries,
Parallel machines,
Computer architecture,
Throughput,
Energy consumption"
Optimization of clustering criteria by reformulation,"Various hard, fuzzy and possibilistic clustering criteria (objective functions) are useful as bases for a variety of pattern recognition problems. At present, many of these criteria have customized individual optimization algorithms. Because of the specialized nature of these algorithms, experimentation with new and existing criteria can be very inconvenient and costly in terms of development and implementation time. This paper shows how to reformulate some clustering criteria so that specialized algorithms can be replaced by general optimization routines found in commercially available software. We prove that the original and reformulated versions of each criterion are fully equivalent. Finally, two numerical examples are given to illustrate reformulation.","Prototypes,
Clustering algorithms,
Shape,
Computer science,
Software algorithms,
Iris,
Clustering methods,
Geometry,
Fuzzy sets,
Mathematics"
A selective survey of the finite-difference time-domain literature,"The finite-difference time-domain (FDTD) method is arguably the most popular numerical method for the solution of problems in electromagnetics. Although the FDTD method has existed for nearly 30 years, its popularity continues to grow as computing costs continue to decline. Furthermore, extensions and enhancements to the method are continually being published, which further broaden its appeal. Because of the tremendous amount of FDTD-related research activity, tracking the FDTD literature can be a daunting task. We present a selective survey of FDTD publications. This survey presents some of the significant works that made the FDTD method so popular, and tracks its development up to the present-day state-of-the-art in several areas. An ""on-line"" BibT/sub E/X database, which contains bibliographic information about many FDTD publications, is also presented.",
The Mobius cubes,"The Mobius cubes are hypercube variants that give better performance with the same number of links and processors. We show that the diameter of the Mobius cubes is about one half the diameter of the equivalent hypercube, and that the average number of steps between processors for a Mobius cube is about two-thirds of the average for a hypercube. We give an efficient routing algorithm for the Mobius cubes. This routing algorithm finds a shortest path and operates in time proportional to the dimension of the cube. We also give efficient broadcast algorithms for the Mobius cubes. We show that the Mobius cubes contain ring networks and other networks. We report results of simulation studies on the dynamic message-passing performance of the hypercube, the Twisted Cube of P.A.J. Hilbers et al. (1987), and the Mobius cubes. Our results are in agreement with S. Abraham (1990), showing that the Twisted Cube has worse dynamic performance than the hypercube, but our results show that the 1-Mobius cube has dynamic performance superior to that of the hypercube. This contradicts current literature, which implies that twisted cube variants will have worse dynamic performance.","Hypercubes,
Routing,
Concurrent computing,
Multiprocessor interconnection networks,
Broadcasting,
Network topology,
Computer networks,
Parallel architectures,
Computer science"
Necessary and sufficient conditions for consistent global snapshots,"Consistent global snapshots are important in many distributed applications. We prove the exact conditions for an arbitrary checkpoint, or a set of checkpoints, to belong to a consistent global snapshot, a previously open problem. To describe the conditions, we introduce a generalization of Lamport's (1978) happened-before relation called a zigzag path.","Sufficient conditions,
Distributed computing,
Debugging,
Computational modeling,
Fault tolerance,
Concurrent computing,
Contracts,
Computer science"
Segmentation of intravascular ultrasound images: a knowledge-based approach,"Intravascular ultrasound imaging of coronary arteries provides important information about coronary lumen, wall, and plaque characteristics. Quantitative studies of coronary atherosclerosis using intravascular ultrasound and manual identification of wall and plaque borders are limited by the need for observers with substantial experience and the tedious nature of manual border detection. We have developed a method for segmentation of intravascular ultrasound images that identifies the internal and external elastic laminae and the plaque-lumen interface. The border detection algorithm was evaluated in a set of 38 intravascular ultrasound images acquired from fresh cadaveric hearts using a 30 MHz imaging catheter. To assess the performance of our border detection method we compared five quantitative measures of arterial anatomy derived from computer-detected borders with measures derived from borders manually defined by expert observers. Computer-detected and observer-defined lumen areas correlated very well (r=0.96, y=1.02x+0.52), as did plaque areas (r=0.95, y=1.07x-0.48), and percent area stenosis (r=0.93, y=0.99x-1.34.) Computer-derived segmental plaque thickness measurements were highly accurate. Our knowledge-based intravascular ultrasound segmentation method shows substantial promise for the quantitative analysis of in vivo intravascular ultrasound image data.","Image segmentation,
Ultrasonic imaging,
Ultrasonic variables measurement,
Arteries,
Atherosclerosis,
Detection algorithms,
Heart,
Catheters,
Anatomy,
Thickness measurement"
Object exchange across heterogeneous information sources,"We address the problem of providing integrated access to diverse and dynamic information sources. We explain how this problem differs from the traditional database integration problem and we focus on one aspect of the information integration problem, namely information exchange. We define an object-based information exchange model and a corresponding query language that we believe are well suited for integration of diverse information sources. We describe how, the model and language have been used to integrate heterogeneous bibliographic information sources. We also describe two general-purpose libraries we have implemented for object exchange between clients and servers.",
Lee distance and topological properties of k-ary n-cubes,"In this paper, we consider various topological properties of a k-ary n-cube (Q/sub n//sup k/) using Lee distance. We feel that Lee distance is a natural metric for defining and studying a Q/sub n//sup k/. After defining a Q/sub n//sup k/ graph using Lee distance, we show how to find all disjoint paths between any two nodes. Given a sequence of radix k numbers, a function mapping the sequence to a Gray code sequence is presented, and this function is used to generate a Hamiltonian cycle. Embedding the graph of a mesh and the graph of a binary hypercube into the graph of a Q/sub n//sup k/ is considered. Using a k-ary Gray code, we show the embedding of a k(n/sub 1/)/spl times/k(n/sub 2/)/spl times/.../spl times/k(n/sub m/)-dimensional mesh into a Q/sub n//sup k/ where n=/spl Sigma//sub i=l//sup m/n/sub i/. Then using a single digit, 4-ary reflective Gray code, we demonstrate embedding a Q/sub n/ into Q/sub [n/2]//sup 4/. We look at how Lee distance may be applied to the problem of resource placement in a Q/sub n//sup k/ by using a Lee distance error-correcting code. Although the results in this paper are only preliminary, Lee distance error-correcting codes have not been applied previously to this problem. Finally, we consider how Lee distance can be applied to message routing and single-node broadcasting in a Q/sub n//sup k/. In this section we present two single-node broadcasting algorithms that are optimal when single-port and multi-port I/O is used.","Computer science,
Reflective binary codes,
Broadcasting,
Error correction codes,
Routing,
Computer Society,
Concurrent computing,
Linear algebra,
Partial differential equations"
Cooperative multi-robot box-pushing,"This paper deals with the communication in task-sharing between two autonomous six-legged robots equipped with object and goal sensing, and a repertoire of contact and light-following behaviors. The performance of pushing an elongated box towards a goal region is difficult for a single robot and improves significantly when performed cooperatively, but requires careful coordination between the robots. We present and experimentally demonstrate an approach that utilizes cooperation at three levels: sensing, action, and control, and takes the advantage of a simple communication protocol to compensate for the robots' noisy and uncertain sensing.",
Topologically adaptable snakes,"The paper presents a typologically adaptable snakes model for image segmentation and object representation. The model is embedded in the framework of domain subdivision using simplicial decomposition. This framework extends the geometric and topological adaptability of snakes while retaining all of the features of traditional snakes, such as user interaction, and overcoming many of the limitations of traditional snakes. By superposing a simplicial grid over the image domain and using this grid to iteratively reparameterize the deforming snakes model, the model is able to flow into complex shapes, even shapes with significant protrusions or branches, and to dynamically change topology as necessitated by the data. Snakes can be created and can split into multiple parts or seamlessly merge into other snakes. The model can also be easily converted to and from the traditional parametric snakes model representation. We apply a 2D model to various synthetic and real images in order to segment objects with complicated shapes and topologies.","Shape,
Deformable models,
Topology,
Active contours,
Image segmentation,
Solid modeling,
Level set,
Parametric statistics,
Computer science,
Image converters"
Correct architecture refinement,"A method is presented for the stepwise refinement of an abstract architecture into a relatively correct lower-level architecture that is intended to implement it. A refinement step involves the application of a predefined refinement pattern that provides a routine solution to a standard architectural design problem. A pattern contains an abstract architecture schema and a more detailed schema intended to implement it. The two schemas usually contain very different architectural concepts (from different architectural styles). Once a refinement pattern is proven correct, instances of it can be used without proof in developing specific architectures. Individual refinements are compositional, permitting incremental development and local reasoning. A special correctness criterion is defined for the domain of software architecture, as well as an accompanying proof technique. A useful syntactic form of correct composition is defined. The main points are illustrated by means of familiar architectures for a compiler. A prototype implementation of the method has been used successfully in a real application.","Computer architecture,
Software architecture,
Concrete,
Integrated circuit synthesis,
Software prototyping,
Prototypes,
Application software,
Software systems,
Data engineering,
Computer science"
A nonlinear projection method based on Kohonen's topology preserving maps,"A nonlinear projection method is presented to visualize high-dimensional data as a 2D image. The proposed method is based on the topology preserving mapping algorithm of Kohonen. The topology preserving mapping algorithm is used to train a 2D network structure. Then the interpoint distances in the feature space between the units in the network are graphically displayed to show the underlying structure of the data. Furthermore, we present and discuss a new method to quantify how well a topology preserving mapping algorithm maps the high-dimensional input data onto the network structure. This is used to compare our projection method with a well-known method of Sammon (1969). Experiments indicate that the performance of the Kohonen projection method is comparable or better than Sammon's method for the purpose of classifying clustered data. Its time-complexity only depends on the resolution of the output image, and not on the size of the dataset. A disadvantage, however, is the large amount of CPU time required.","Data analysis,
Pattern recognition,
Computer science,
Network topology,
Inspection,
Thyristors,
Physics,
Computer displays,
Data visualization,
Projection algorithms"
Determination of instants of significant excitation in speech using group delay function,"A new method for determining the instants of significant excitation in speech signals is proposed. In the paper, significant excitation refers primarily to the instant of glottal closure within a pitch period in voiced speech. The method is based on the global phase characteristics of minimum phase signals. The average slope of the unwrapped phase of the short-time Fourier transform of linear prediction residual is calculated as a function of time. Instants where the phase slope function makes a positive zero-crossing are identified as significant excitations. The method is discussed in a source-filter context of speech production. The method is not sensitive to the characteristics of the filter. The influence of the type, length, and position of the analysis window is discussed. The method works well for all types of voiced speech in male as well as female speech but, in all cases, under noise-free conditions only.",
Requirements monitoring in dynamic environments,"We propose requirements monitoring to aid in the maintenance of systems that reside in dynamic environments. By requirements monitoring we mean the insertion of code into a running system to gather information from which it can he determined whether, and to what degree, that running system is meeting its requirements. Monitoring is a commonly applied technique in support of performance tuning, but the focus therein is primarily on computational performance requirements in short runs of systems. We wish to address systems that operate in a long lived, ongoing fashion in nonscientific enterprise applications. We argue that the results of requirements monitoring can be of benefit to the designers, maintainers and users of a system-alerting them when the system is being used in an environment for which it was not designed, and giving them the information they need to direct their redesign of the system. Studies of two commercial systems are used to illustrate and justify our claims.","Licenses,
Computerized monitoring,
Computer science,
Feathers,
Humans,
Information analysis,
Runtime environment"
Hierarchical cubic networks,"We introduce a new interconnection network for large-scale distributed memory multiprocessors called the hierarchical cubic network (HCN). We establish that the number of routing steps needed by several data parallel applications running on a HCN-based system and a hypercube-based system are about identical. Further, hypercube connections can be emulated on the HCN in constant time. Simulation of uniform and localized traffic patterns reveal that the normalized average internode distances in a HCN are better than in a comparable hypercube. Additionally, the HCN also has about three-fourths the diameter of a comparable hypercube, although it uses about half as many links per node-a fact that has positive ramifications on the implementation of HCN-connected systems.","Hypercubes,
Multiprocessor interconnection networks,
Computer science,
Synchronization,
Fault tolerance,
Routing,
Voting,
Distributed databases,
Computer network reliability,
Telecommunication network reliability"
Finding faces in cluttered scenes using random labeled graph matching,"An algorithm for locating quasi-frontal views of human faces in cluttered scenes is presented. The algorithm works by coupling a set of local feature detectors with a statistical model of the mutual distances between facial features it is invariant with respect to translation, rotation (in the plane), and scale and can handle partial occlusions of the face. On a challenging database with complicated and varied backgrounds, the algorithm achieved a correct localization rate of 95% in images where the face appeared quasi-frontally.","Layout,
Face detection,
Facial features,
Computer vision,
Face recognition,
Detectors,
Filters,
Computer science,
Mutual coupling,
Image databases"
Positron emission mammography (PEM): a promising technique for detecting breast cancer,"We are developing a high specificity technique for detecting the increased metabolic rate of breast tumours. The glucose analog FDG is known to concentrate in breast tumours rendering them easily detectable in conventional PET scans. Since PET is a relatively expensive imaging technique it has not been used routinely in the detection of breast cancer. Positron emission mammography (PEM) will provide a highly efficient high spatial resolution and low cost positron imaging system whose metabolic images are co-registered with conventional mammography. Coincidences between two BGO blocks cut into 2/spl times/2 mm squares coupled to two 7.5 cm square imaging PMTs are detected and back-projected to form real-time multiple plane images. The design is about 20 times more sensitive than a conventional multi-slice PET body scanner, so much less radio-pharmaceutical can be used, reducing the patient dose and cost per scan. Prototype detectors have been made and extensive measurements done. The device is expected to have an in-plane spatial resolution about 2 mm FWHM. Besides the application as a secondary screening tool the device may be beneficial in measuring a tumour's response to radio-therapy or chemo-therapy, as well as aiding the surgeon in optimizing the removal of malignant tissue.",
Emergent synchrony in locally coupled neural oscillators,"The discovery of long range synchronous oscillations in the visual cortex has triggered much interest in understanding the underlying neural mechanisms and in exploring possible applications of neural oscillations. Many neural models thus proposed end up relying on global connections, leading to the question of whether lateral connections alone can produce remote synchronization. With a formulation different from frequently used phase models, we find that locally coupled neural oscillators can yield global synchrony. The model employs a previously suggested mechanism that the efficacy of the connections is allowed to change on a fast time scale. Based on the known connectivity of the visual cortex, the model outputs closely resemble the experimental findings. Furthermore, we illustrate the potential of locally connected oscillator networks in perceptual grouping and pattern segmentation, which seems missing in globally connected ones.",
Skip-Over: algorithms and complexity for overloaded systems that allow skips,"In applications ranging from video reception to telecommunications and packet communication to aircraft control, tasks enter periodically and have fixed response time constraints, but missing a deadline is acceptable, provided most deadlines are met. We call such tasks ""occasionally skippable"". We look at the problem of uniprocessor scheduling of occasionally skippable periodic tasks in an environment having periodic tasks. We show that making optimal use of skips is NP-hard. We then look at two algorithms called Skip-Over Algorithms (one a variant of earliest deadline first and one of rate monotonic scheduling) that exploit skips. We give schedulability bounds for both.","Quality of service,
Processor scheduling,
Scheduling algorithm,
Computer science,
Application software,
Control systems,
Telecommunication control,
Delay,
Costs,
Terminology"
Allocation and scheduling of precedence-related periodic tasks,"This paper discusses a static algorithm for allocating and scheduling components of periodic tasks across sites in distributed systems. Besides dealing with the periodicity constraints, (which have been the sole concern of many previous algorithms), this algorithm handles precedence, communication, as well as replication requirements of subtasks of the tasks. The algorithm determines the allocation of subtasks of periodic tasks to sites, the scheduled start times of subtasks allocated to a site, and the schedule for communication along the communication channel(s). Simulation results show that the heuristics and search techniques incorporated in the algorithm are very effective.","Scheduling algorithm,
Real time systems,
Orbital robotics,
Robotics and automation,
Space stations,
Production facilities,
Command and control systems,
Processor scheduling,
Information science,
Computer science"
Genetic design of optimum linear and nonlinear QRS detectors,"Describes an approach to the design of optimum QRS detectors. The authors report on detectors including a linear or nonlinear polynomial filter, which enhances and rectifies the QRS complex, and a simple, adaptive maxima detector. The parameters of the filter and the detector, and the samples to be processed are selected by a genetic algorithm which minimizes the detection errors made on a set of reference ECG signals. Three different architectures and the experimental results achieved on the MIT-BIH Arrhythmia Database are described.","Detectors,
Filters,
Polynomials,
Genetic algorithms,
Electrocardiography,
Face detection,
Databases,
Councils,
Computer science,
Sampling methods"
Moving furniture with teams of autonomous robots,"The authors wish to organize furniture in a room with a team of robots that can push objects. The authors show how coordinated pushing by robots can change the pose (position and orientation) of objects and then they ask whether planning, global control, and explicit communication are necessary for cooperatively changing the pose of objects. The authors answer in the negative and present, as witnesses, four cooperative manipulation protocols that use different amounts of state, sensing, and communication. The authors analyze these protocols in the information invariant framework. The authors formalize the notion of resource tradeoffs for robot protocols and give the tradeoffs for the specific protocols discussed here.","Protocols,
Orbital robotics,
Robot sensing systems,
Robotic assembly,
Solid modeling,
Computer science,
Laboratories,
Mobile robots,
Shape,
Parallel robots"
Cooperative mobile robotics: antecedents and directions,"There has been increased research interest in systems composed of multiple autonomous mobile robots exhibiting collective behavior. Groups of mobile robots are constructed, with an aim to studying such issues as group architecture, resource conflict, origin of cooperation, learning, and geometric problems. As yet, few applications of collective robotics have been reported, and supporting theory is still in its formative stages. In this paper, the authors give a critical survey of existing works and discuss open problems in this field, emphasizing the various theoretical issues that arise in the study of cooperative robotics. The authors describe the intellectual heritages that have guided early research, as well as possible additions to the set of existing motivations.","Mobile robots,
Cognitive robotics,
Intelligent robots,
Power generation economics,
Environmental economics,
Learning,
Artificial intelligence,
Computer science,
Computer architecture,
Systems biology"
Probabilistic performance guarantee for real-time tasks with varying computation times,"Describes how the scheduling algorithms and schedulability analysis methods developed for periodic tasks can be extended to provide performance guarantees to semi-periodic tasks. Like periodic tasks, the requests in a semi-periodic task are released regularly. However, their computation times vary widely. We focus on systems where the total maximum utilization of the tasks on each processor is larger than one. Hence, according to the existing schedulability conditions for periodic tasks, we cannot guarantee that the semi-periodic tasks are schedulable, even though their total average utilization is very small. We describe two methods of providing probabilistic schedulability guarantees to the semi-periodic tasks. The first method, called probabilistic time-demand analysis, is a modification of the exact schedulability test for periodic tasks. The second method, called the transform-task method, transforms each task into a periodic task followed by a sporadic task. The transform-task method can provide an absolute guarantee to requests with shorter computation times and a probabilistic guarantee to the longer requests.",
Distributed management by delegation,"This paper introduces a novel approach to distributed computing based on delegation-agents, and describes its applications to decentralize network management. Delegation agents are programs that can be dispatched to remote processes, dynamically linked and executed under local or remote control. Unlike scripted agents, delegation agent programs may be written in arbitrary languages, interpreted or compiled. They can thus be more broadly applied to handle such tasks as real-time monitoring, analysis and control of network resources. Distributed management by delegation (MbD) uses this to manage remote elements and domains. MbD provides a paradigm for distributed, flexible, scalable and robust network management that overcomes the key limitations of current centralized management schemes.",
Training with Noise is Equivalent to Tikhonov Regularization,"It is well known that the addition of noise to the input data of a neural network during training can, in some circumstances, lead to significant improvements in generalization performance. Previous work has shown that such training with noise is equivalent to a form of regularization in which an extra term is added to the error function. However, the regularization term, which involves second derivatives of the error function, is not bounded below, and so can lead to difficulties if used directly in a learning algorithm based on error minimization. In this paper we show that for the purposes of network training, the regularization term can be reduced to a positive semi-definite form that involves only first derivatives of the network mapping. For a sum-of-squares error function, the regularization term belongs to the class of generalized Tikhonov regularizers. Direct minimization of the regularized error function provides a practical alternative to training with noise.",
Contour extraction from cardiac MRI studies using snakes,"The author investigated automatic extraction of left ventricular contours from cardiac magnetic resonance imaging (MRI) studies. The contour extraction algorithms were based on active contour models, or snakes. Based on cardiac MR image characteristics, the author suggested algorithms for extracting contours from these large data sets. The author specifically considered contour propagation methods to make the contours reliable enough despite noise, artifacts, and poor temporal resolution. The emphasis was on reliable contour extraction with a minimum of user interaction. Both spin echo and gradient echo studies were considered. The extracted contours were used for determining quantitative measures for the heart and could also be used for obtaining graphically rendered cardiac surfaces.","Magnetic resonance imaging,
Data mining,
Active contours,
Magnetic noise,
Heart,
Rendering (computer graphics)"
Genocop III: a co-evolutionary algorithm for numerical optimization problems with nonlinear constraints,"During the last two years several methods have been proposed for handling nonlinear constraints by genetic algorithms for numerical optimization problems; most of them were based on penalty functions. However, the performance of these methods is highly problem-dependent; moreover, many methods require additional tuning of several parameters. We present a new optimization system (Genocop III), which is based on concepts of co-evolution and repair algorithms. We present the results of the system on a few selected test problems and discuss some directions for further research.","Vectors,
Constraint optimization,
Nonlinear equations,
Genetic algorithms,
Probability distribution,
Genetic mutations,
Computer science,
System testing,
Upper bound,
Evolutionary computation"
Formal specification and analysis of software architectures using the chemical abstract machine model,"We are exploring an approach to formally specifying and analyzing software architectures that is based on viewing software systems as chemicals whose reactions are controlled by explicitly stated rules. This powerful metaphor was devised in the domain of theoretical computer science by Bana/spl circ/tre and Le Me/spl acute/tayer (1990) and then reformulated as the CHAM (CHemical Abstract Machine) by Berry and Boudol (1992). The CHAM formalism provides a framework for developing operational specifications that does not bias the described system toward any particular computational model. It also encourages the construction and use of modular specifications at different levels of detail. We illustrate the use of the CHAM for architectural description and analysis by applying it to two different architectures for a simple but familiar software system, the multiphase compiler.","Chemical analysis,
Formal specifications,
Computer architecture,
Software architecture,
Software systems,
Computer science,
Power system modeling,
Computational modeling,
Modular construction,
Software engineering"
MRI-guided optical tomography: prospects and computation for a new imaging method,"Living tissue scatters near-infrared light randomly, can this problem be overcome to make NIR optical tomography possible? If so, it could be more accurate and less damaging than other medical imaging techniques. Computational experiments using combined MRI-optical methods show promise.","Tomography,
Optical computing,
Optical imaging,
Optical scattering,
Biomedical optical imaging,
Electromagnetic scattering,
Optical films,
Wavelength measurement,
Particle scattering,
Optical sensors"
Guaranteeing real-time requirements with resource-based calibration of periodic processes,"The paper presents a comprehensive design methodology for guaranteeing end to end requirements of real time systems. Applications are structured as a set of process components connected by asynchronous channels, in which the end points are the system's external inputs and outputs. Timing constraints are then postulated between these inputs and outputs; they express properties such as end to end propagation delay, temporal input sampling correlation, and allowable separation times between updated output values. The automated design method works as follows: First new tasks are created to correlate related inputs, and an optimization algorithm, whose objective is to minimize CPU utilization, transforms the end to end requirements into a set of intermediate rate constraints on the tasks. If the algorithm fails, a restructuring tool attempts to eliminate bottlenecks by transforming the application, which is then resubmitted into the assignment algorithm. The final result is a schedulable set of fully periodic tasks, which collaboratively maintain the end to end constraints.","Calibration,
Timing,
Real time systems,
Design methodology,
Constraint optimization,
Design optimization,
Temperature,
Computer science,
USA Councils,
Performance analysis"
Simplifying quotient determination in high-radix modular multiplication,"Until now the use of high radices to implement modular multiplication has been questioned, because it involves complex determination of quotient digits for the module reduction. This paper presents algorithms that are obtained through rewriting of Montgomery's algorithm. The determination of quotients becomes trivial and the cycle time becomes independent of the choice of radix. It is discussed how the critical path in the loop can be reduced to a single shift-and-add operation. This implies that a true speed up is achieved by choosing higher radices.","Hardware,
Clocks,
Computer science,
Public key,
Councils,
Throughput,
Frequency conversion,
Upper bound"
A modified approach to data cache management,"As processor performance continues to improve, more emphasis must be placed on the performance of the memory system. In this paper, a detailed characterization of data cache behavior for individual load instructions is given. We show that by selectively applying cache line allocation according the characteristics of individual load instructions, overall performance can be improved for both the data cache and the memory system. This approach can improve some aspects of memory performance by as much as 60 percent on existing executables.","Clocks,
Computer science,
Time measurement,
Bandwidth,
Pins,
Velocity measurement,
Sun,
Logic,
Delay,
Prefetching"
Effective analysis for engineering real-time fixed priority schedulers,"There has been considerable activity in recent years in developing analytical techniques for hard real-time systems. Inevitably these techniques make simplifying assumptions so as to reduce the complexity of the problem to be solved. Unfortunately this leads to a gap between theory and engineering practice. The paper presents new analysis that enables the costs of the scheduler (clock overheads, queue manipulations and release delays) to be factored into the standard equations for calculating worst-case response times. As well as predicting the true behavior of realistic systems, the analysis also allows free parameters, such as clock interrupt rate, to be determined.","Distributed computing,
Queueing analysis,
Real time systems,
Delay,
Application software,
Information science,
Clocks,
Aerospace engineering,
Timing,
Traffic control"
Faithful bipartite ROM reciprocal tables,"We describe bipartite reciprocal tables that employ separate table lookup of the positive and negative portions of a borrow-save reciprocal value. The fusion of the parts includes a rounding so the output reciprocals are guaranteed correct to a unit in the last place, and typically provide a round-to-nearest reciprocal for over 90% of input arguments. The output rounding can be accomplished in conjunction with multiplier recoding yielding practically no cost in logic complexity or time in employing bipartite tables. We demonstrate these tables to be 2 to 4 times smaller than conventional 4-bit reciprocal tables. For 10-16 bit reciprocal table lookup the compression grows from a factor of 4 to over 16, making possible the use of larger seed reciprocals than previously considered cost effective.","Read only memory,
Costs,
Iterative algorithms,
Table lookup,
Iron,
Interpolation,
Computer science,
Logic,
Workstations,
Delay"
A multi-body factorization method for motion analysis,"The structure from motion problem has been extensively studied in the field of computer vision. Yet, the bulk of the existing work assumes that the scene contains only a single moving object. The more realistic case where an unknown number of objects move in the scene has received little attention, especially for its theoretical treatment. We present a new method for separating and recovering the motion and shape of multiple independently moving objects in a sequence of images. The method does not require prior knowledge of the number of objects, nor is dependent on any grouping of features into an object at the image level. For this purpose, we introduce a mathematical construct of object shapes, called the shape interaction matrix, which is invariant to both the object motions and the selection of coordinate systems. This invariant structure is computable solely from the observed trajectories of image features without grouping them into individual objects. Once the structure is computed, it allows for segmenting features into objects by the process of transforming it into a canonical form, as well as recovering the shape and motion of each object.","Motion analysis,
Shape,
Image segmentation,
Layout,
Image sequences,
Constraint theory,
Computer science,
Computer vision,
Feature extraction,
Filters"
Nonparametric estimation via empirical risk minimization,"A general notion of universal consistency of nonparametric estimators is introduced that applies to regression estimation, conditional median estimation, curve fitting, pattern recognition, and learning concepts. General methods for proving consistency of estimators based on minimizing the empirical error are shown. In particular, distribution-free almost sure consistency of neural network estimates and generalized linear estimators is established.","Risk management,
Neural networks,
Curve fitting,
Pattern recognition,
Random variables,
Computer errors,
Convergence,
Mathematics,
Computer science,
Training data"
Computing simulations on finite and infinite graphs,"We present algorithms for computing similarity relations of labeled graphs. Similarity relations have applications for the refinement and verification of reactive systems. For finite graphs, we present an O(mn) algorithm for computing the similarity relation of a graph with n vertices and m edges (assuming m/spl ges/n). For effectively presented infinite graphs, we present a symbolic similarity-checking procedure that terminates if a finite similarity relation exists. We show that 2D rectangular automata, which model discrete reactive systems with continuous environments, define effectively presented infinite graphs with finite similarity relations. It follows that the refinement problem and the /spl forall/CTL* model-checking problem are decidable for 2D rectangular automata.","Computational modeling,
Automata,
Computer science,
Application software,
Analytical models,
System analysis and design,
Engineering profession,
Contracts,
Algorithm design and analysis,
State-space methods"
PathFinder: A Negotiation-Based Performance-Driven Router for FPGAs,"Routing FPGAs is a challenging problem because of the relative scarcity of routing resources, both wires and connection points. This can lead either to slow implementations caused by long wiring paths that avoid congestion or a failure to route all signals. This paper presents PathFinder, a router that balances the goals of performance and routability. PathFinder uses an iterative algorithm that converges to a solution in which all signals are routed while achieving close to the optimal performance allowed by the placement. Routability is achieved by forcing signals to negotiate for a resource and thereby determine which signal needs the resource most. Delay is minimized by allowing the more critical signals a greater say in this negotiation. Because PathFinder requires only a directed graph to describe the architecture of routing resources, it adapts readily to a wide variety of FPGA architectures such as Triptych, Xilinx 3000 and mesh-connected arrays of FPGAs. The results of routing ISCAS benchmarks on the Triptych FPGA architecture show an average increase of only 4.5% in critical path delay over the optimum delay for a placement. Routes of ISCAS benchmarks on the Xilinx 3000 architecture show a greater completion rate than commercial tools, as well as 11% faster implementations.",
Automatic registration of CT and MR brain images using correlation of geometrical features,"Describes an automated approach to register CT and MR brain images. Differential operators in scale space are applied to each type of image data, so as to produce feature images depicting ""ridgeness"". The resulting CT and MR feature images show similarities which can be used for matching. No segmentation is needed and the method is devoid of human interaction. The matching is accomplished by hierarchical correlation techniques. Results of 2-D and 3-D matching experiments are presented. The correlation function ensures an accurate match even if the scanned volumes to be matched do not completely overlap, or if some of the features in the images are not similar.",
Combinatorial optimization with use of guided evolutionary simulated annealing,"Feasible approaches to the task of solving NP-complete problems usually entails the incorporation of heuristic procedures so as to increase the efficiency of the methods used. We propose a new technique, which incorporates the idea of simulated annealing into the practice of simulated evolution, in place of arbitrary heuristics. The proposed technique is called guided evolutionary simulated annealing (GESA). We report on the use of GESA approach primarily for combinatorial optimization. In addition, we report the case of function optimization, treating the task as a search problem. The traveling salesman problem is taken as a benchmark problem in the first case. Simulation results are reported. The results show that the GESA approach can discover a very good near optimum solution after examining an extremely small fraction of possible solutions. A very complicated function with many local minima is used in the second case. The results in both cases indicate that the GESA technique is a practicable method which yields consistent and good near optimal solutions, superior to simulated evolution.",
A system for the 3D reconstruction of retracted-septa PET data using the EM algorithm,"We have implemented the EM reconstruction algorithm for volume acquisition from current generation retracted-septa PET scanners. Although the software was designed for a GE Advance scanner, it is easily adaptable to other 3D scanners. The reconstruction software was written for an Intel iPSC/860 parallel computer with 128 compute nodes. Running on 32 processors, the algorithm requires approximately 55 minutes per iteration to reconstruct a 128/spl times/128/spl times/35 image. No projection data compression schemes or other approximations were used in the implementation. Extensive use of EM system matrix (C/sub ij/) symmetries (including the 8-fold in-plane symmetries, 2-fold axial symmetries, and axial parallel line redundancies) reduces the storage cost by a factor of 188. The parallel algorithm operates on distributed projection data which are decomposed by base-symmetry angles. Symmetry operators copy and index the C/sub ij/ chord to the form required for the particular symmetry. The use of asynchronous reads, lookup tables, and optimized image indexing improves computational performance.","Positron emission tomography,
Image reconstruction,
Concurrent computing,
Reconstruction algorithms,
Software design,
Data compression,
Matrix decomposition,
Costs,
Parallel algorithms,
Table lookup"
Robust semiglobal stabilization of minimum-phase input-output linearizable systems via partial state and output feedback,In this paper we consider the problem of robust semiglobal stabilization and/or semiglobal practical stabilization of minimum-phase input-output linearizable systems. The results of this paper significantly extend the recent work of Teel and Praly (1993) on SISO (single-input/single-output) minimum-phase systems in several directions. Among these directions are the development of MIMO (multi-input/multi-output) theory and the relaxation of the restriction on the interaction between nonlinear zero dynamics and the state of the linearizable part of the system.,
Recovery of the 3-D shape of the left ventricle from echocardiographic images,"A computational method is reported which allows the fully automated recovery of the three-dimensional shape of the cardiac left ventricle from a reduced set of apical echo views. Two typically ill-posed problems have been faced: 1) the detection of the left ventricle contours in each view, and 2) the integration of the detected contour points (which form a sparse and partially inconsistent data set) into a single surface representation. The authors' solution to these problems is based on a careful integration of standard computer vision algorithms with neural networks. Boundary detection comprises three steps: edge detection, edge grouping, and edge classification. The first and second steps (which are typical early-vision tasks not involving specific domain-knowledge) have been performed through fast, well-established algorithms of computer vision. The higher level task of left ventricle-edge discrimination, which involves the exploitation of specific knowledge about the left ventricle silhouette, has been performed by feedforward neural networks. Following the most recent results in the field of computer vision, the first step in solving the problem of recovering the ventricle surface has been the adoption of a physically inspired model of it. Basically, the authors have modeled the left ventricle surface as a closed, thin, elastic surface and the data as a set of radial springs acting on it. The recovery process is equivalent to the settling of the surface-plus-springs system into a stable configuration of minimum potential energy. The finite element discretization of this model leads directly to an analog neural-network implementation. The efficiency of such an implementation has been remarkably enhanced through a learning algorithm which embeds specific knowledge about the shape of the left ventricle in the network. Experiments using clinical echographic sequences are described. Four apical views (each with a different rotation of the probe) have been acquired during a heartbeat from a set of seven normal subjects. These images have been utilized to set the various processing modules and test their capabilities.","Shape,
Face detection,
Computer vision,
Neural networks,
Image edge detection,
Feedforward neural networks,
Springs,
Potential energy,
Finite element methods,
Lead"
K-winners-take-all circuit with O(N) complexity,"Presents a k-winners-take-all circuit that is an extension of the winner-take-all circuit by Lazzaro et al. (1989). The problem of selecting the largest k numbers is formulated as a mathematical programming problem whose solution scheme, based on the Lagrange multiplier method, is directly implemented on an analog circuit. The wire length in this circuit grows only linearly with the number of elements, and the circuit is more suitable for real-time processing than the Hopfield networks because the present circuit produces the solution almost instantaneously-in contrast to the Hopfield network, which requires transient convergence to the solution from a precise initial state. The selection resolution in the present circuit is, however, only finite in contrast to the almost infinite resolution in the Hopfield networks.",
Joint scheduling of distributed complex periodic and hard aperiodic tasks in statically scheduled systems,"In this paper we present algorithms for the joint scheduling of periodic and aperiodic tasks in statically scheduled distributed real-time systems. Periodic tasks are precedence constrained, distributed, and communicating over the nodes of the systems. Both soft and hard aperiodic tasks are handled. After a static schedule has been created in a first step, the algorithms determine the amount and distribution of unused resources and leeway in it. These are then used to incorporate aperiodic tasks into the schedule by shifting the periodic tasks' execution, without violating their feasibility. Run-time mechanisms are simple and require only little memory. Processors and communication nodes can be utilized fully. The algorithm performs on optimal online guarantee algorithm for hard aperiodic tasks of O(N). An extensive simulation study exhibits very high guarantee ratios for various load and deadline scenarios, which underlines the efficiency of our method.",
Similarity and affine invariant distances between 2D point sets,"We develop expressions for measuring the distance between 2D point sets, which are invariant to either 2D affine transformations or 2D similarity transformations of the sets, and assuming a known correspondence between the point sets. We discuss the image normalization to be applied to the images before their comparison so that the computed distance is symmetric with respect to the two images. We then give a general (metric) definition of the distance between images, which leads to the same expressions for the similarity and affine cases. This definition avoids ad hoc decisions about normalization. Moreover, it makes it possible to compute the distance between images under different conditions, including cases where the images are treated asymmetrically. We demonstrate these results with real and simulated images.",
DOCTOR: an integrated software fault injection environment for distributed real-time systems,"The paper presents an integrated software fault injection environment (DOCTOR) which is capable of (1) generating synthetic workloads under which system dependability is evaluated, (2) injecting various types of faults with different options, and (3) collecting performance and dependability data. A comprehensive graphical user interface is also provided. The software implemented fault-injection tools supports three types of faults: memory faults, CPU faults, and communication faults. Each injected fault may be permanent, transient or intermittent. A fault-injection plan can be formulated probabilistically, or based on the past event history. The modular organization of tools is particularly designed for distributed architectures. DOCTOR is implemented on a distributed real-time system called HARTS, and it capability has been tested through numerous experiments.","Real time systems,
Hardware,
System testing,
Timing,
Analytical models,
Character generation,
Distributed computing,
Computer science,
Graphical user interfaces,
History"
Application-level document caching in the Internet,"With the increasing demand for document transfer services such as the World Wide Web comes a need for better resource management to reduce the latency of documents in these systems. To address this need, we analyze the potential for document caching at the application level in document transfer services. We have collected traces of actual executions of Mosaic, reflecting over half a million user requests for WWW documents. Using those traces, we study the tradeoffs between caching at three levels in the system, and the potential for use of application-level information in the caching system. Our traces show that while a high hit rate in terms of URLs is achievable, a much lower hit rate is possible in terms of bytes, because most profitably-cached documents are small. We consider the performance of caching when applied at the level of individual user sessions, at the level of individual hosts, and at the level of a collection of hosts on a single LAN. We show that the performance gain achievable by caching at the session level (which is straightforward to implement) is nearly all of that achievable at the LAN level (where caching is more difficult to implement). However, when resource requirements are considered, LAN level caching becomes muck more desirable, since it can achieve a given level of caching performance using a much smaller amount of cache space. Finally, we consider the use of organizational boundary information as an example of the potential for use of application-level information in caching. Our results suggest that distinguishing between documents produced locally and those produced remotely can provide useful leverage in designing caching policies, because of differences in the potential for sharing these two document types among multiple users.","Internet,
Delay,
Network servers,
Local area networks,
Web server,
Web sites,
World Wide Web,
Bandwidth,
File systems,
Computer science"
On generalized fat trees,"We introduce and analyze a new family of multiprocesser interconnection networks, called generalized fat trees, which include as special cases the fat trees used for the connection machine architecture CM-5, pruned butterflies, and various other fat trees proposed in the literature. The generalized fat trees provide a formal unifying concept to design and analyse a fat tree based architecture. The extended generalized fat tree network XGFT(h; m/sub 1/, ..., m/sub h/; w/sub 1/, ..., w/sub h/) of height h has /spl Pi//sub i=1//sup h/ m/sub i/ leaf processors and the inner nodes serve only as switches or routers. Each non-leaf node in level i has m/sub i/ children and each non-root has w/sub i+1/ parent nodes. The generalized fat trees provide regularity, symmetry, recursive scalability, maximal fault-tolerance, logarithmic diameter bisection scalability, and permit simple algorithms for fault tolerant self-routing and broadcasting. These networks are also versatile, since they can efficiently embed rings, meshes and tori, trees, pyramids and hypercubes.","Network topology,
Hypercubes,
Routing,
Fault tolerance,
Broadcasting,
Art,
Very large scale integration,
Image processing,
Computer simulation,
Computer science"
Integrating the timing analysis of pipelining and instruction caching,"Recently designed machines contain pipelines and caches. While both features provide significant performance advantages, they also pose problems for predicting execution time of code segments in real-time systems. Pipeline hazards may result in multicycle delays. Instruction or data memory references may not be found in cache and these misses typically require several cycles to resolve. Whether an instruction will stall due to a pipeline hazard or a cache miss depends on the dynamic sequence of previous instructions executed and memory references performed. Furthermore, these penalties are not independent since delays due to pipeline stalls and cache miss penalties may overlap. This paper describes an approach for bounding the worst-case performance of large code segments on machines that exploit both pipelining and instruction caching. First, a method is used to analyze a program's control flow to statically categorize the caching behavior of each instruction. Next, these categorizations are used in the pipeline analysis of sequences of instructions representing paths within the program. A timing analyzer uses the pipeline path analysis to estimate the worst-case execution performance of each loop and function in the program. Finally, a graphical user interface is invoked that allows a user to request timing predictions on portions of the program.","Timing,
Pipeline processing,
Delay,
Real time systems,
Hazards,
Performance analysis,
Computer science,
Graphical user interfaces,
Contracts,
Registers"
Scaled Euclidean 3D reconstruction based on externally uncalibrated cameras,"Previous work shows that based on five non-coplanar correspondences of two uncalibrated cameras, 3D reconstruction can be achieved under projectile models, or based on four non-coplanar correspondences of two uncalibrated cameras, 3D reconstruction can be achieved under affine models, with three unknown parameters. In this paper, we show that based on four coplanar correspondences of two externally uncalibrated cameras, 3D reconstruction can be achieved in Euclidean space with only one unknown scaling parameter. Moreover, the unknown scale factor is the physical distance from the camera center to the plane formed by the four points in 3D space. If this distance is known a priori, then the 3D structure can be completely recovered. Both simulated and real data experimental results show that our reconstruction algorithm works reasonably robustly.",
Fast Identification of Robust Dependent Path Delay Faults,"Recently, it has been shown in [1] and [2] that in order to verify the correct timing of a manufactured circuit not all of its paths need to be considered for delay testing. In this paper, a theory is developed which puts the work of these papers into a common framework, thus allowing for a better understanding of their relation. In addition, we consider the computational problem of identifying large sets of such not-necessary-to-test paths. Since the approach of [1] can only be applied for small scale circuits, we develop a new algorithm which trades quality of the result against computation time, and allows handling of large circuits with tens of millions of paths. Experimental results show that enormous improvements in running time are only paid for by a small decrease in quality.","Fault diagnosis,
Robustness,
Delay,
Circuit testing,
Circuit faults,
Timing,
Logic testing,
Computer science,
Computer aided manufacturing,
Digital circuits"
Trilinearity of three perspective views and its associated tensor,"It has been established that certain trilinear forms of three perspective views give rise to a tensor of 27 intrinsic coefficients. We show in this paper that a permutation of the the trilinear coefficients produces three homography matrices (projective transformations of planes) of three distinct intrinsic planes, respectively. This, in turn, yields the result that 3D invariants are recovered directly-simply by appropriate arrangement of the tensor's coefficients. On a secondary level, we show new relations between fundamental matrix, epipoles, Euclidean structure and the trilinear tensor. On the practical side, the new results extend the existing envelope of methods of 3D recovery from 2D views-for example, new linear methods that cut through the epipolar geometry, and new methods for computing epipolar geometry using redundancy available across many views.","Tensile stress,
Cameras,
Geometry,
Computer science,
Reconstruction algorithms,
Numerical stability"
Dual priority scheduling,"In this paper, we present a new strategy for scheduling tasks with soft deadlines in real-time systems containing periodic, sporadic and adaptive tasks with hard deadlines. In such systems, much of the spare capacity present is due to sporadic and adaptive tasks not arriving at their maximum rate. Offline methods of identifying spare capacity such as the Deferrable Server or Priority Exchange Algorithm are unable to make this spare capacity available as anything other than a background service opportunity for soft tasks. Further, more recent methods such as dynamic Slack Stealing require computationally expensive re-evaluation of the available slack in order to reclaim such spare capacity. By comparison, the Dual Priority approach presented in this paper provides an efficient and effective means of scheduling soft task in this case.","Processor scheduling,
Real time systems,
Dynamic scheduling,
Computer science,
Stochastic systems,
Timing,
Runtime,
Jitter,
Scheduling algorithm,
Computational modeling"
Chi2: feature selection and discretization of numeric attributes,"Discretization can turn numeric attributes into discrete ones. Feature selection can eliminate some irrelevant attributes. This paper describes Chi2 a simple and general algorithm that uses the /spl chi//sup 2/ statistic to discretize numeric attributes repeatedly until some inconsistencies are found in the data, and achieves feature selection via discretization. The empirical results demonstrate that Chi/sup 2/ is effective in feature selection and discretization of numeric and ordinal attributes.",
Sensorless manipulation using transverse vibrations of a plate,"The existing industrial parts feeders move the parts through a sequence of mechanical filters that reject parts in unwanted orientations. In this paper we develop a new setup that uses a different vibratory mechanism to systematically manipulate parts, by actively orienting and localizing them. The idea is to generate and change dynamic modes for a plate by varying the applied frequency of oscillation. Depending on the node shapes of the plate for these frequencies, the position and orientation of the parts can be controlled. We develop an analysis of the underlying dynamics, and show that it can be used to predict the behavior of objects placed on the vibrating plate. Using this analysis, we propose that the applied frequencies can be automatically sequenced to obtain a ""sensorless"" strategy for manipulating a given object.","Filters,
Frequency,
Sensor systems,
Vibrations,
Computer science,
Aerodynamics,
Shape control,
Assembly,
Costs,
Tactile sensors"
Recursive pattern: a technique for visualizing very large amounts of data,"An important goal of visualization technology is to support the exploration and analysis of very large amounts of data. In this paper, we propose a new visualization technique called a 'recursive pattern', which has been developed for visualizing large amounts of multidimensional data. The technique is based on a generic recursive scheme which generalizes a wide range of pixel-oriented arrangements for displaying large data sets. By instantiating the technique with adequate data- and application-dependent parameters, the user may greatly influence the structure of the resulting visualizations. Since the technique uses one pixel for presenting each data value, the amount of data which can be displayed is only limited by the resolution of current display technology and by the limitations of human perceptibility. Beside describing the basic idea of the 'recursive pattern' technique, we provide several examples of useful parameter settings for the various recursion levels. We further show that our 'recursive pattern' technique is particularly advantageous for the large class of data sets which have a natural order according to one dimension (e.g. time series data). We demonstrate the usefulness of our technique by using a stock market application.","Data visualization,
Humans,
Multidimensional systems,
Computer science,
Pattern analysis,
Displays,
Stock markets,
Visual databases,
Decision making,
Automation"
Arithmetic coding revisited,"During its long gestation in the 1970s and early 1980s, arithmetic coding was widely regarded more as an academic curiosity than a practical coding technique. One factor that helped it gain the popularity it enjoys today was the publication in 1987 of source code for a multi symbol arithmetic coder in Communications of the ACM. Now (1995), our understanding of arithmetic coding has further matured, and it is timely to review the components of that implementation and summarise the improvements that we and other authors have developed since then. We also describe a novel method for performing the underlying calculation needed for arithmetic coding. Accompanying the paper is a ""Mark II"" implementation that incorporates the improvements we suggest. The areas examined include: changes to the coding procedure that reduce the number of multiplications and divisions and permit them to be done to low precision; the increased range of probability approximations and alphabet sizes that can be supported using limited precision calculation; data structures for support of arithmetic coding on large alphabets; the interface between the modelling and coding subsystems; the use of enhanced models to allow high performance compression. For each of these areas, we consider how the new implementation differs from the CACM package.","Arithmetic,
Data structures,
Computer science,
Packaging,
Switches,
Probability,
Statistics,
Internet,
Context modeling,
Frequency"
Decomposition of arbitrarily shaped morphological structuring elements,"For image processing systems that have a limited size of region of support, say 3/spl times/3, direct implementation of morphological operations by a structuring element larger than the prefixed size is impossible. The decomposition of morphological operations by a large structuring element into a sequence of recursive operations, each using a smaller structuring element, enables the implementation of large morphological operations. In this paper, the authors present the decomposition of arbitrarily shaped (convex or concave) structuring elements into 3/spl times/3 elements, optimized with respect to the number of 3/spl times/3 elements. The decomposition is based on the concept of factorization of a structuring element into its prime factors. For a given structuring element, all its corresponding 3/spl times/3 prime concave factors are first determined. From the set of the prime factors, the decomposability of the structuring element is then established, and subsequently the structuring element is decomposed into a smallest possible set of 3/spl times/3 elements. Examples of optimal decomposition and structuring elements that are not decomposable are presented.","Morphological operations,
Image processing,
Shape,
Morphology,
Computer science,
Terminology"
Nonlinear manifold learning for visual speech recognition,"A technique for representing and learning smooth nonlinear manifolds is presented and applied to several lip reading tasks. Given a set of points drawn from a smooth manifold in an abstract feature space, the technique is capable of determining the structure of the surface and of finding the closest manifold point to a given query point. We use this technique to learn the ""space of lips"" in a visual speech recognition task. The learned manifold is used for tracking and extracting the lips, for interpolating between frames in an image sequence and for providing features for recognition. We describe a system based on hidden Markov models and this learned lip manifold that significantly improves the performance of acoustic speech recognizers in degraded environments. We also present preliminary results on a purely visual lip reader.","Speech recognition,
Lips,
Image recognition,
Hidden Markov models,
Loudspeakers,
National electric code,
Computer science,
Degradation,
Machine vision,
Buildings"
MASSIVE: a distributed virtual reality system incorporating spatial trading,"MASSIVE is a distributed virtual reality system. It provides rich facilities to support user interaction and cooperation via text, audio and graphics media and interaction is controlled by a spatial model of interaction. The communications architecture is based on processes communicating via typed connections which have interfaces on both ends and which integrate RPCs, attributes and streams in a common context. A spatial interface trading service, the aura manager, has been developed to support interface trading in a spatial context. The concepts embodied in the aura manager can be useful in other interface trading situations, especially where notions of ""space"" and ""meeting"" may be applied.","Virtual reality,
Context,
Graphics,
Communication system control,
Streaming media,
Context-aware services"
The Helmholtz Machine,"Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.",
Exact two-level minimization of hazard-free logic with multiple-input changes,"This paper describes a new method for exact hazard-free logic-minimization of Boolean functions. Given an incompletely-specified Boolean function, the method produces a minimum-cost sum-of-products implementation which is hazard-free for a given set of multiple-input changes, if such a solution exists. The method is a constrained version of the Quine-McCluskey algorithm. It has been automated and applied to a number of examples. Results are compared with results of a comparable non-hazard-free method (espresso-exact). Overhead due to hazard elimination is shown to be negligible.",
Methods of graph searching for border detection in image sequences with applications to cardiac magnetic resonance imaging,"Automated border detection using graph searching principles has been shown useful for many biomedical imaging applications. Unfortunately, in an often unpredictable subset of images, automated border detection methods may fail. Most current edge detection methods fail to take into account the added information available in a temporal or spatial sequence of images that are commonly available in biomedical image applications. To utilize this information the authors extended their previously reported single frame graph searching method to include data from a sequence. The authors' method transforms the three-dimensional surface definition problem in a sequence of images into a two-dimensional problem so that traditional graph searching algorithms may be used. Additionally, the authors developed a more efficient method of searching the three-dimensional data set using heuristic search techniques which vastly improve execution time by relaxing the optimality criteria. The authors have applied both methods to detect myocardial borders in computer simulated images as well as in short-axis magnetic resonance images of the human heart. Preliminary results show that the new multiple image methods may be more robust in certain circumstances when compared to a single frame method and that the heuristic search techniques may reduce analysis times without compromising robustness.",
Similarity Metric Learning for a Variable-Kernel Classifier,"Nearest-neighbor interpolation algorithms have many useful properties for applications to learning, but they often exhibit poor generalization. In this paper, it is shown that much better generalization can be obtained by using a variable interpolation kernel in combination with conjugate gradient optimization of the similarity metric and kernel size. The resulting method is called variable-kernel similarity metric (VSM) learning. It has been tested on several standard classification data sets, and on these problems it shows better generalization than backpropagation and most other learning methods. The number of parameters that must be determined through optimization are orders of magnitude less than for backpropagation or radial basis function (RBF) networks, which may indicate that the method better captures the essential degrees of variation in learning. Other features of VSM learning are discussed that make it relevant to models for biological learning in the brain.",
A novel pattern generator for near-perfect fault-coverage,"A new design methodology for a pattern generator is proposed, formulated in the context of on-chip BIST. The pattern generator consists of two components: a GLFSR, earlier proposed as a pseudo-random pattern generator, and combinational logic, to snap the outputs of the pseudo-random pattern generator. Using fewer test patterns with only a small area overhead, this combinatorial logic block, for a particular CUT, can be designed to achieve nearly 100% single stuck-at fault coverage. Specifically, where weighted pattern generators only enhance the probability of testing a specified set of hard-to-detect faults, the proposed combinational logic, using a comparable hardware overhead, can guarantee generating the test for those faults. Experimental results demonstrate that under identical conditions, the fault coverage of the proposed pattern generator is significantly higher, compared to the conventional weighted pattern generation techniques. For enhancing effectiveness, this combinational logic mapping technique can also be used to augment any weighted pattern technique. Because LFSRs are special cases of GLFSRs, our design is more general than LFSR-based designs.",
Hard-core distributions for somewhat hard problems,"Consider a decision problem that cannot be 1-/spl delta/ approximated by circuits of a given size in the sense that any such circuit fails to give the correct answer on at least a /spl delta/ fraction of instances. We show that for any such problem there is a specific ""hard core"" set of inputs which is at least a /spl delta/ fraction of all inputs and on which no circuit of a slightly smaller size can get even a small advantage over a random guess. More generally, our argument holds for any non uniform model of computation closed under majorities. We apply this result to get a new proof of the Yao XOR lemma (A.C. Yao, 1982), and to get a related XOR lemma for inputs that are only k wise independent.",
The message flow model for routing in wormhole-routed networks,"In this paper, we introduce a new approach to deadlock-free routing in wormhole-routed networks called the message flow model. This method may be used to develop deterministic, partially-adaptive, and fully-adaptive routing algorithms for wormhole-routed networks with arbitrary topologies. We first establish the necessary and sufficient condition for deadlock free routing, based on the analysis of the message flow on each channel. We then use the model to develop new adaptive routing algorithms for 2D meshes.","Routing,
Intelligent networks,
System recovery,
Network topology,
Sufficient conditions,
Delay,
Computer science,
Multicast algorithms,
Multicast communication,
Mesh networks"
An optimal broadcasting algorithm without message redundancy in star graphs,"Based on the V.E. Mendia and D. Sarkar's algorithm (1992), we propose an optimal and nonredundant distributed broadcasting algorithm in star graphs. For an n-dimensional star graph, our algorithm takes O(n log/sub 2/ n) time and guarantees that all nodes in the star graph receive the message exactly once. Moreover, broadcasting m packets in a pipeline fashion takes O(m log/sub 2/ n+n log/sub 2/ n) time due to the nonredundant property of our broadcasting algorithm.",
Embedding graphs onto the Supercube,"In this paper we consider the Supercube, a new interconnection network derived from the hypercube. The Supercube, introduced by A. Sen (1989), has the same diameter and connectivity as a Hypercube but can be realized for any number of nodes, not only powers of 2. We study the Supercube's ability to execute parallel programs, using graph-embedding techniques. We show that complete binary trees and bidimensional meshes (with a side length power of 2) are spanning subgraphs of the Supercube. We then prove that the Supercube is Hamiltonian and, when the number of nodes is not a power of 2, it contains all cycles of length greater than 3 as subgraphs.",
1995 high level synthesis design repository,"In this paper we briefly describe a set of designs that earn serve as examples for high level synthesis (HLS) systems. The designs vary in complexity from simple behavioral finite state machines to more complex designs such as microprocessors and floating point units. Most of the designs are described in the VHDL language at the behavioral level. We divide the designs into two categories. The first category contains designs that have documentation on the specifications of the designs along with the strategy used to test the individual design models. The second category contains examples used in many HLS papers, but lack comprehensive documentation and/or test vectors.",
Near-optimal critical sink routing tree constructions,"We present critical-sink routing tree (CSRT) constructions which exploit available critical-path information to yield high-performance routing trees. Our CS-Steiner and ""global slack removal"" algorithms together modify traditional Steiner tree constructions to optimize signal delay at identified critical sinks. We further propose an iterative Elmore routing tree (ERT) construction which optimizes Elmore delay directly, as opposed to heuristically abstracting linear or Elmore delay as in previous approaches. Extensive timing simulations on industry IC and MCM interconnect parameters show that our methods yield trees that significantly improve (by averages of up to 67%) over minimum Steiner routings in terms of delays to identified critical sinks. ERTs also serve as generic high-performance routing trees when no critical sink is specified: for 8-sink nets in standard IC (MCM) technology, we improve average sink delay by 19% (62%) and maximum sink delay by 22% (52%) over the minimum Steiner routing. These approaches provide simple, basic advances over existing performance-driven routing tree constructions. Our results are complemented by a detailed analysis of the accuracy and fidelity of the Elmore delay approximation; we also exactly assess the suboptimality of our heuristic tree constructions. In achieving the latter result, we develop a new characterization of Elmore-optimal routing trees, as well as a decomposition theorem for optimal Steiner trees, which are of independent interest.",
Coordinated motion planning for multiple car-like robots using probabilistic roadmaps,"Presents a new approach to the multi-robot path planning problem, where a number of robots are to change their positions through feasible motions in the same static environment. The approach is probabilistically complete. That is, any solvable problem is guaranteed to be solved within a finite amount of time. The method, which is an extension of previous work on probabilistic single-robot planners, is very flexible in the sense that it can easily be applied to different robot types. In this paper the authors apply it to non-holonomic car-like robots, and present experimental results which show that the method is powerful and fast.",
Local invariants for recognition,"Geometric invariants are shape descriptors that remain unchanged under geometric transformations such as projection or changing the viewpoint. A new method of obtaining local projective and affine invariants is developed and implemented for real images. Being local, the Invariants are much less sensitive to occlusion than global invariants. The invariants' computation is based on a canonical method. This consists of defining a canonical coordinate system by the intrinsic properties of the shape, independently of the given coordinate system. Since this canonical system is independent of the original one, it is invariant and all quantities defined in it are invariant. The method was applied without the use of a curve parameter. This was achieved by fitting an implicit polynomial to an arbitrary curve in a vicinity of each curve point. Several configurations are treated: a general curve without any correspondence and curves with known correspondences of one or two feature points or lines. Experimental results for different 2D objects in 3D space are presented.","Shape,
Polynomials,
Object recognition,
Curve fitting,
Image recognition,
Image matching,
Geometry,
Computer science,
Automation"
Unbounded length contexts for PPM,"The prediction by partial matching (PPM) data compression scheme has set the performance standard in lossless compression of text throughout the past decade. The original algorithm was first published in 1984 by Cleary and Witten, and a series of improvements was described by Moffat (1990), culminating in a careful implementation, called PPMC, which has become the benchmark version. This still achieves results superior to virtually all other compression methods, despite many attempts to better it. PPM, is a finite-context statistical modeling technique that can be viewed as blending together several fixed-order context models to predict the next character in the input sequence. Prediction probabilities for each context in the model are calculated from frequency counts which are updated adaptively; and the symbol that actually occurs is encoded relative to its predicted distribution using arithmetic coding. The paper describes a new algorithm, PPM*, which exploits contexts of unbounded length. It reliably achieves compression superior to PPMC, although our current implementation uses considerably greater computational resources (both time and space). The basic PPM compression scheme is described, showing the use of contexts of unbounded length, and how it can be implemented using a tree data structure. Some results are given that demonstrate an improvement of about 6% over the old method.",
Compression domain rendering of time-resolved volume data,"An important challenge in the visualization of three-dimensional volume data is the efficient processing and rendering of time-resolved sequences. Only the use of compression techniques, which allow the reconstruction of the original domain from the compressed one locally, makes it possible to evaluate these sequences in their entirety. In this paper, a new approach for the extraction and visualization of so-called time features from within time-resolved volume data is presented. Based on the asymptotic decay of multiscale representations of spatially localized time evolutions of the data, singular points can be discriminated. Also, the corresponding Lipschitz exponents, which describe the signals' local regularity, can be determined, and can be taken as a measure of the variation in time. The compression ratio and the comprehension of the underlying signal is improved if we first restore the extracted regions which contain the most important information.",
Tone recognition of isolated Cantonese syllables,"Tone identification is essential for the recognition of the Chinese language, specifically far Cantonese which is well known for being very rich in tones. The paper presents an efficient method for tone recognition of isolated Cantonese syllables. Suprasegmental feature parameters are extracted from the voiced portion of a monosyllabic utterance and a three-layer feedforward neural network is used to classify these feature vectors. Using a phonologically complete vocabulary of 234 distinct syllables, the recognition accuracy for single-speaker and multispeaker is given by 89.0% and 87.6% respectively.",
"Efficient access to optical bandwidth wavelength routing on directed fiber trees, rings, and trees of rings","We address efficient access to bandwidth in WDM (wavelength division multiplexing) optical networks. We consider tree topologies, ring topologies, as well as trees of rings. These are topologies of concrete practical relevance for which undirected underlying graph models have been studied before by P. Raghavan and E. Upfal (1993). As opposed to previous studies (A. Aggarwal et al., 1993; R. Pankaj, 1992; P. Raghavan and E. Upfal, 1993), we consider directed graph models. Directedness of fiber links is dictated by physical directedness of optical amplifiers. For trees, we give a polynomial time routing algorithm that satisfies requests of maximum load L/sub max/ per fiber link using no more than 15L/sub max//8/spl les/15OPT/8 optical wavelengths. This improves a 2L/sub max/ scheme that is implicit by P. Raghavan and E. Upfal by extending their undirected methods to our directed model. Alternatively stated, for fixed W wavelength technology, we can load the network up to L,, 8W/15 rather than W/2. In engineering terms, this is a so called ""6.66% increase of bandwidth"" and it is considered substantial. For rings, the approximation factor is 2OPT. For trees of rings, the approximation factor is 15OPT/4. Technically, optical routing requirements give rise to novel coloring paradigms. Our algorithms involve matchings and multicolored alternating cycles, combined with detailed potential and averaging analysis.","Bandwidth,
Wavelength routing,
Tree graphs,
Network topology,
Wavelength division multiplexing,
Stimulated emission,
Optical fiber networks,
Concrete,
Optical fiber amplifiers,
Semiconductor optical amplifiers"
Some improvements to total degree tests,"A low-degree test is a collection of simple, local rules for checking the proximity of an arbitrary function to a low-degree polynomial. Each rule depends on the function's values at a small number of places. If a function satisfies many rules then it is close to a low-degree polynomial. Low-degree tests play an important role in the development of probabilistically checkable proofs. In this paper we present two improvements to the efficiency of low-degree tests. Our first improvement concerns the smallest field size over which a low-degree test can work. We show how to test that a function is a degree d polynomial over prime fields of size only d+2. Our second improvement shows a better efficiency of the low-degree test of Rubinfeld and Sudan (1993) than previously known. We show concrete applications of this improvement via the notion of ""locally checkable codes"". This improvement translates into better tradeoffs on the size versus probe complexity of probabilistically checkable proofs than previously known.",
Elimination of charge-enhancement effects in GaAs FETs with a low-temperature grown GaAs buffer layer,The use of a low temperature grown GaAs (LT GaAs) buffer layer in GaAs FETs is shown via computer simulation and experimental measurement to reduce ion-induced charge collection by two to three orders of magnitude. This reduction in collected charge is associated with the efficient reduction of charge-enhancement mechanisms in the FETs. Error rate calculations indicate that the soft error rate of LT GaAs integrated circuits will be reduced by several orders of magnitude when compared to conventional FET-based GaAs ICs.,
A mechanism of automatic 3D object modeling,"The symbolic representation of 3D objects is the fundamental knowledge for computer systems to understand the environment. This knowledge is usually assumed to exist in a computer but can also be acquired by accumulating spatial features extracted from sensory inputs at different viewing directions. This paper first investigates surface visibility and, then, after introducing mass vector chains (MVC), discusses the relationship between MVC and the spatial closure of object models. An automatic modeling mechanism is established with the observation that the boundary of an object is closed only if the MVC of its model is closed or, alternatively, the tail-to-head vector of an unclosed MVC estimates the visible direction of the missing surfaces. Experimental results and an algorithm are also given at the end.",
A distributed stereocorrelation algorithm,"This paper addresses the problem of parallelizing one of the most computationally intensive imaging tasks, namely, the stereocorrelation operation. Stereocorrelation is a statistical procedure utilized to automatically derive the depth information from a pair of digitized pictures taken from the same scene but at different positions. Thus, the operation automatically generates the third dimension (z-coordinate) of each pixel of the image of the scene. The paper presents a simple but novel distributed stereocorrelation algorithm which uses a data farming approach to balance the work load.",
A fuzzy system for indoor mobile robot navigation,"An autonomous mobile robot has to cope with uncertain, incomplete or approximate information. Moreover, it has to identify sudden perceptual situations to manoeuvre in real time. This paper describes a fuzzy rule based system (FRBS) approach for controlling the movement of an autonomous mobile robot (MORIA). Difficult guiding and controlling properties of the robot are achieved by combining local actions and global strategies within the fuzzy controller. Different behaviors and perceptions are detected with the help of fuzzy rules and stored in fuzzy state variables. These state variables activate different fuzzy rule sets which in turn change the behavior of the fuzzy controller.",
Partial discharge pulse distribution pattern analysis,"Statistical data analysis has been recently requisite for partial discharge (PD) study. This has been enabled by the recent development of microprocessors or the downsizing of computers. It is important to pay more attention to profiles of a packet of PD pulses on the voltage phase angle. Statistical parameters such as skewness and kurtosis have been intensively investigated for PD analysis. More recently, the neural network algorithm has been utilised especially to discriminate PD signals from noise signals for insulation diagnosis.",
Low overhead multiprocessor allocation strategies exploiting system spare capacity for fault detection and location,"Several schemes for detecting faults at the processor level in a multiprocessor system have been discussed in the past. One such scheme (A. Dahbura et al., 1989) works by running secondary versions of jobs on the unused or spare processors of the system and uses the comparison approach (J. Maeng and M. Malek, 1981) to detect faults. We build upon this scheme and propose three new multiprocessor allocation strategies that run a variable number of versions per job. These schemes permit online detection and, in many cases, location of faulty processors in a system with nominal degradation in its delay/throughput performance; these delays are limited chiefly to the delays associated with job preemptions. Two new metrics, the fault detection capability (FDC) and the fault location capability (FLC), are introduced to evaluate these schemes. Extensive simulation results are performed to obtain performance figures for the various schemes. Stochastic Petri net models are also developed to obtain approximate performance results. The results show that these schemes utilize spare capacity more efficiently, thereby improving upon the fault detection and location capabilities of the system.",
The effect of inheritance on the maintainability of object-oriented software: an empirical study,"The empirical study was undertaken as part of a programme of research to explore unsupported claims about the object-oriented paradigm: a series of experiments tested the effect of inheritance on the maintainability of object-oriented software. Subjects were asked to modify object-oriented software with a hierarchy of 3 levels of inheritance depth and equivalent object-based software with no inheritance. The collected timing data showed that subjects maintaining object-oriented software using inheritance performed the modification tasks, on average, approximately 20% quicker than those maintaining equivalent object-based software with no inheritance. An initial inductive analysis revealed that 2 out of 3 subjects performed faster when maintaining the object-oriented software with inheritance. The findings are sufficiently important that attempts to verify the results should be made by independent researchers. Subsequent studies should seek to scale up the findings to the maintenance of more complex software by professional programmers.",
Back gated CMOS on SOIAS for dynamic threshold voltage control,"Simultaneous reduction of supply and threshold voltages for low power design without suffering performance losses will eventually reach the limit of diminishing returns as static power dissipation becomes a significant portion of the total power equation. In order to meet the opposing requirements of high performance and low power, a dynamic threshold voltage control scheme is needed. A novel SOI technology was developed whereby a back-gate was used to control the threshold voltage of the front-gate; this concept was demonstrated on a selectively scaled CMOS process.","Threshold voltage,
Voltage control,
Semiconductor films,
Wafer bonding,
CMOS technology,
Etching,
Substrates,
CMOS process,
Silicon on insulator technology,
Computer science"
An improved method for MRI artifact correction due to translational motion in the imaging plane,"A computer postprocessing technique is developed to remove MRI artifact arising from unknown translational motion in the imaging plane. Based on previous artifact correction methods, the improved technique uses two successive steps to reduce read out and phase-encoding direction artifacts: First, the spectrum shift method is applied to remove read-out axis translational motion. Then, the phase retrieval method is employed to eliminate the remaining subpixel motion of the read-out axis and the entire motion of the phase-encoding axis. In the presence of noise, to protect edge detection (in the spectrum shift method), two high-density gray-level markers are added, one to each side of the imaging object. Experimental results with an actual MR scan confirmed the ability of the method to correct the artifact of an MR image caused by unknown translational motion in the imaging plane.","Magnetic resonance imaging,
Protection,
Image edge detection"
An empirical evaluation of virtual circuit holding time policies in IP-over-ATM networks,"When carrying Internet protocol (IP) traffic over an asynchronous transfer mode (ATM) network, the ATM adaptation layer must determine how long to hold a virtual circuit opened to carry an IP datagram. We present a formal statement of the problem and carry out a detailed empirical examination of various holding time policies taking into account the issue of network pricing. We offer solutions for two natural pricing models, the first being a likely pricing model of future ATM networks, while the second is based on characteristics of current networks. For each pricing model, we study a variety of simple nonadaptive policies as well as easy to implement policies that adapt to the characteristics of the IP traffic. We simulate our policies on actual network traffic, and find that policies based on least recently used (LRU) perform well, although the best adaptive policies provide a significant improvement over LRU.",
Heterogeneous multimedia reasoning,"An innovated system is described which integrates data from heterogeneous multimedia domains. This general-purpose framework uses the concept of a 'mediator' to efficiently support multimedia reasoning. The Media Abstraction Creation System (MACS) implements algorithms for creating or querying media abstractions, for relaxing queries, and for allowing incremental query modifications. We have also developed an integrated framework within which media data can be queried even when those queries need to access other (nonmedia) background data and software. Access to diverse, nonmedia databases is provided by the Heterogeneous Reasoning and Mediator System (Hermes).",
An SEU analysis approach for error propagation in digital VLSI CMOS ASICs,A probabilistic description of error propagation in complex circuits is formulated and solved as a set of linear equations. Comparisons are made with experimental data.,"Error analysis,
Very large scale integration,
Application specific integrated circuits,
Circuit simulation,
Computer errors,
Registers,
Fabrication,
Logic circuits,
Equations,
Discrete event simulation"
Dynamic minimization of OKFDDs,"We present methods for the construction of small Ordered Kronecker Functional Decision Diagrams (OKFDDs). OKFDDs are a generalization of Ordered Binary Decision Diagrams (OBDDs) and Ordered Functional Decision Diagrams (OFDDs) as well. Our approach is based on dynamic variable ordering and decomposition type choice. For changing the decomposition type we use a new method. We briefly discuss the implementation of PUMA, our OKFDD package. The quality of our methods in comparison with sifting and interleaving for OBDDs is demonstrated based on experiments performed with PUMA.",
Voxel based object simplification,"Presents a simple, robust and practical method for object simplification for applications where gradual elimination of high-frequency details is desired. This is accomplished by sampling and low-pass filtering the object into multi-resolution volume buffers and applying the marching cubes algorithm to generate a multi-resolution triangle-mesh hierarchy. Our method simplifies the genus of objects and can also help existing object simplification algorithms achieve better results. At each level of detail, a multi-layered mesh can be used for an optional and efficient antialiased rendering.",
Issues in electrical impedance imaging,"Electrical impedance imaging systems apply currents to the body's surface, measure the corresponding voltages, and use inverse methods to reconstruct the conductivity and permittivity in the interior from these data. Quick, accurate maps of these electrical parameters inside the body could improve the effectiveness of critical medical technologies.",
A new label-based source routing for multi-ring networks,"This paper presents a new source routing technique for ring and multi-ring networks, which uses short address labels. The main objectives for having this new method are that in case of one or more failures a frame will be guaranteed: (1) to be removed from the ring-termination property, and (2) to be copied at most once and only by its destinations-safety property. The scheme is based on dividing the label address space of each ring into subspaces, such that the address subspaces are physically disjoint. More specifically, each ring, in a multi-ring network, is divided into two or more parts such that adjacent address subspaces are disjoint. The route of each frame is described by a sequence of short address labels in the frame's header. The current route of a frame is determined by the first address label in its header, and it can be used for routing over at most one subspace of the ring.",
Comparison between experiment and computer modeling for simple MILO configurations,"The MILO is a crossed-field HPM, high power microwave source which uses its self-generated magnetic field to cut off electron flow to the anode. A detailed comparison of experimental results and a computer simulation has been made for a number of simple axisymmetric MILO structures designed to operate at 1 GHz. The structures were built from demountable components which enabled the number of cavities and their dimensions to be rapidly altered. Measurements were made of the fluctuating magnetic fields at the end of each cavity. The amplitude and depth of RF modulation of the magnetic fields, although repeatable, changed drastically from one configuration to the next; these parameters were compared with predictions from VIPER, a 2-D electromagnetic PIC code. Good quantitative agreement was obtained between experiments and the simulation in most situations, although, late in the current pulse, after about 100 ns, the level of RF began to decay; a phenomenon which became more pronounced as the applied voltage was increased. The decay was attributed to plasma formation on the cavity vanes and subsequent electron emission; this explanation was verified by computer modeling electron emission and by using vanes make from polished stainless steel in place of aluminum vanes.","Magnetic field measurement,
Blades,
Radio frequency,
Electron emission,
Magnetic fields,
Anodes,
Computer simulation,
Electromagnetic measurements,
Amplitude modulation,
Magnetic modulators"
A diffusion model for computer animation of diffuse ink painting,"The multidimensional diffusion model for computer animation of diffuse ink painting opens up a new dimension in painting. In diffuse painting final image is a result of ink diffusion in absorbent paper. A straight-forward diffusion model however is unable to provide very specific features of real diffuse painting. In particular, it can not explain the appearance of certain singularities in intensity of color in the image which are very important features of diffuse ink painting. In our previous work, a model based on physical analysis of paper structure was proposed. Although this model provided an adequate simulation of many diffuse ink painting properties, it was still insufficient to explain the singularities of intensity distribution precisely. Now we solve this problem. A multidimensional diffusion model which we propose proves to provide exactly the same intensity distribution as in real images. The method was applied to animate ink diffusion 'Nijimi' of traditional Japanese ink painting 'Sumie'.",
Cluster validation with generalized Dunn's indices,"Some generalizations of Dunn's (1973) index for validation of crisp clusters are discussed. Numerical examples suggest that this index, which uses the standard measures for inter-set distance and set diameter can be improved by using different functions for either or both of these operators.",
Open combinatorial problems in computational molecular biology,"In the last few years theoretical computer scientists have found new challenges in computational molecular biology. We discuss recent advances and present some open combinatorial problems in different areas of computational molecular biology such as genome rearrangements, DNA physical mapping, DNA sequencing and sequence comparison.",
Redundant binary Booth recoding,"We investigate the efficiencies attainable pursuing Booth recoding directly from redundant binary input with limited carry propagation. As a digit conversion problem we extend the important result that each radix 4 Booth recoded digit can be determined from 5 consecutive input signed bits to obtain that each radix 2/sup k/ Booth recoded digit can be determined from 2k+1 consecutive input signed bits and prove this to be the minimum possible for any k/spl ges/2. Analysis of alternative bit pair encodings of signed bits yields the improved result that each radix 2/sup k/ Booth recoded digit can be determined from only 2k encoded bit pairs employing sign and magnitude bit encoding, a result which does not extend to conventional borrow-save or carry-save redundant binary digit encodings. Radices 4 and 8 gate level designs are illustrated for alternative encodings, with our signed bit design shown to yield smaller depth and fewer gates than existing redundant binary Booth recoding circuits from the literature.",
On-the-fly reading of entire databases,"A common database need is to obtain a global-read, which is a consistent read of an entire database. To avoid terminating normal system activity, and thus improve availability, we propose an on-the-fly algorithm that reads database entities incrementally and allows normal transactions to proceed concurrently. The algorithm assigns each entity a color based on whether the entity has been globally read, and a shade based on how normal transactions have accessed the entity. Serializability of execution histories is ensured by requiring normal transactions to pass both a color test and a shade test before being allowed to commit. Our algorithm improves on a color-only-based scheme from the literature; the color-only scheme does not guarantee serializability.",
Decidability and closure properties of weak Petri net languages in supervisory control,We extend the class of control problems that can be modeled by Petri nets considering the notion of weak terminal behavior. Deterministic weak languages represent closed-loop terminal behaviors that may be enforced by nonblocking Petri net supervisors if controllable. The class of deterministic weak PN languages is not closed under the supremal controllable sublanguage operator.,
Development of a high-resolution PET detector using LSO and wavelength-shifting fibers,"By using wavelength-shifting fibers coupled to thin plates of LSO and to photomultipliers, the authors have demonstrated spatial resolution of 2 mm FWHM for photocapture events. Optimized systems are believed to be capable of better than 1 mm FWHM resolution, without requiring either very small crystals or large numbers of photosensors. With this sensor resolution, annihilation acollinearity dominates system resolution for large PET rings. By radially stacking alternating layers of thin crystals and wavelength-shifting fibers, a direct depth-of-interaction measurement is possible. Energy measurements and coincidence timing may be provided by trigger photomultipliers in a dual-photodetector geometry. Results from computer simulations and component response measurements on proof-of-concept prototypes are presented.",
Quantity control for JPEG image data compression using fuzzy logic algorithm,"The JPEG established the first international standard for continuous-tone still image for both gray scale and color. However, since the JPEG's standard was originally designed for general applications, some modifications must be done for specific applications such as in digital still cameras. We propose a new method to control the bit-rate of the JPEG standard using a fuzzy logic algorithm (FLA). A gain factor is determined to derive an appropriate quantization matrix. Several rules have been prepared to calculate the gain factor. We used 18 test images for simulations and the results show that the mean error is -1.5%, standard deviation is 1.5% and the error range from -3% to 1%. Without the FLA, the error range from -41 to 43%, the mean error is -0.11% and the standard deviation is 24.24%.",
Planning conditional shortest paths through an unknown environment: a framed-quadtree approach,"A conditional shortest path is a collision-free path of shortest distance based on known information on an obstacle-scattered environment at a given time. This paper investigates the problem of finding a conditional L/sub 2/ shortest path through an unknown environment in which path planning is implemented ""on the fly"" as new obstacle information becomes available through external sensors. We propose a novel cell decomposition approach which calculates an L/sub 2/ distance transform through the use of a circular path-planning wave. The proposed method is based on a new data structure, called the framed-quadtree, which combines together the accuracy of grid-based path planning techniques with the efficiency of quadtree-based techniques, hence having the advantages of both. The heart of this method is a linear time algorithm for computing dynamic Voronoi diagrams.",
Design strategies for the final adder in a parallel multiplier,"In this paper we address the problem of adding two n-bit numbers when the bit arrival times are arbitrary (but known in advance). In particular we address a simplified version of the problem where the input arrival times for the i/sup th/ significant bits of both addends are the same, and the arrival times t/sub i/ have a profile of the form: t/sub 0//spl les/t/sub 1//spl les/.../spl les/t/sub k/=t/sub k+1/=...=t/sub p/>t/sub p+1//spl ges/.../spl ges/t/sub n-1/. This profile is important because it matches the signal arrival time profile of the reduced partial products in a parallel multiplier before they are summed in the final adder. In this paper we present a design strategy specific to arrival time profiles generated by partial product reduction trees constructed by optimal application of the Three Dimensional Method presented by V.G. Oklobdzija et al. (1995). This strategy can be used to obtain adders for any arrival time profile that matches the above form, as well as a broad class of arrival time profiles where even greater variation in the input times is allowed. Finally, we show that our designs significantly outperform the standard adder designs for the uniform signal arrival profile, yielding faster adders that (for these profiles) are also simpler and use fewer gates.","Adders,
Time division multiplexing,
Delay,
Circuits,
Computer science,
Very large scale integration,
Joining processes"
Distributed 3-D iterative reconstruction for quantitative SPECT,"We describe a distributed three dimensional (3-D) iterative reconstruction library for quantitative single-photon emission computed tomography (SPECT). This library includes 3-D projector-backprojector pairs (PBPs) and distributed 3-D iterative reconstruction algorithms. The 3-D PBPs accurately and efficiently model various combinations of the image degrading factors including attenuation, detector response and scatter response. These PBPs were validated by comparing projection data computed using the projectors with that from direct Monte Carlo (MC) simulations. The distributed 3-D iterative algorithms spread the projection-backprojection operations for all the projection angles over a heterogeneous network of single or multi-processor computers to reduce the reconstruction time. Based on a master/slave paradigm, these distributed algorithms provide dynamic load balancing and fault tolerance. The distributed algorithms were verified by comparing images reconstructed using both the distributed and non-distributed algorithms. Computation times for distributed 3-D reconstructions running on up to 4 identical processors were reduced by a factor approximately 80-90% times the number of the processors participating, compared to those for non-distributed 3-D reconstructions running on a single processor. When combined with faster affordable computers, this library provides an efficient means for implementing accurate reconstruction and compensation methods to improve quality and quantitative accuracy in SPECT images.",
An efficient task allocation scheme for two-dimensional mesh-connected systems,"Efficient allocation of proper size submeshes to incoming tasks in two-dimensional (2D) mesh-connected processors is very important for achieving the desired high performance. It also needs to guarantee the recognition of the free submeshes with minimum overhead. In this paper we present an efficient task allocation scheme for 2D meshes. By employing a new approach for searching the array, our scheme can find the available submesh without the scanning of the entire 2D array unlike earlier designs. As a result, our scheme can significantly reduce the task allocation time. Comprehensive computer simulation reveals that the average allocation time and waiting delay are much smaller than earlier schemes irrespective of the size of meshes. The hardware overhead is comparable to other schemes.",
On asymptotics of certain sums arising in coding theory,"T. Klove (see ibid., vol.41, p.298-300, 1995) analyzed the average worst case probability of undetected error for linear [n, k; q] codes of length n and dimension k over an alphabet of size q. The following sum: S/sub n/=/spl Sigma//sub i=1//sup n/(/sub i//sup n/)(/sup i///sub n/)/sup i/((1-i)/n)/sup n-i/ arose, which also has applications in coding theory, average case analysis of algorithms, and combinatorics. Klove conjectured an asymptotic expansion of this sum, and we prove its enhanced version. Furthermore, we consider a more challenging sum arising in the upper bound of the average worst case probability of undetected error over systematic codes derived by Massey (1978). Namely S/sub n,k/=/spl Sigma//sub i=1//sup n/(/sub i//sup n-k/)(/sup i///sub n/)/sup i/((1-i)/n)/sup n-i/ for k/spl ges/0. We obtain an asymptotic expansion of S/sub n,k/, and this leads to a conclusion that Massey's bound on the average worst case probability over all systematic codes is better for every k than the corresponding Klove's bound over all codes [n, k; q]. The technique used belongs to the analytical analysis of algorithms and is based on some enumeration of trees, singularity analysis, Lagrange's inversion formula, and Ramanujan's identities. In fact, S/sub n/, turns out to be related to the so-called Ramanujan's Q-function which finds many applications (e.g. hashing with linear probing, the birthday paradox problem, random mappings, caching, memory conflicts, etc.).",
Predicting the response time of OPS5-style production systems,"This paper focuses on the response time analysis problem: the problem of determining a priori the maximal response time or response time upper bound of a given program. We study this problem in the context of OPS5 production systems. The response time of a program is investigated in two respects: the maximal number of rule firings and the maximal number of basic comparisons made by the Rete network during the execution. The execution of a program always terminates in bounded time if this program satisfies one of certain conditions. The present four of these conditions for OPS5 production systems. An algorithm used to compute an upper bound on the number of rule firings is given. Furthermore, to have a better idea on the time required during the execution, we present an algorithm to compute the maximal time required during the match phase in terms of the number of comparisons made by the Rete network.",
Bylands: reverse engineering safety-critical systems,"We address two problems concerned with the maintenance of safety-critical software. Firstly, we analyse the new issues required for the reverse engineering of real-time existing code to extract high level designs. Secondly, we present a possible design abstraction mechanism that can be used for safety-critical software. We use formal transformations both in the reverse engineering of systems involving temporal constraints, and in the definition of the abstract representation. We present a design framework and the results of initial experiments. The contributions are: (1) the requirements analyses for reverse engineering safety-critical systems, (2) the use of very-high-level domain languages, and (3) formal transformations as the unifying technology.",
Lower bounds on arithmetic circuits via partial derivatives,"We describe a new technique for obtaining lower bounds on restricted classes of non-monotone arithmetic circuits. The heart of this technique is a complexity measure for multivariate polynomials, based on the linear span of their partial derivatives. We use the technique to obtain new lower bounds for computing symmetric polynomials and iterated matrix products.",
Design reuse through high-level library mapping,"We present high-level library mapping (HLLM), a technique that permits reuse of complex databook components (specifically ALUs) in architectural synthesis. We describe a dynamic programming formulation of HLLM, demonstrate the versatility of our approach on a variety of libraries and compare HLLM for ALUs with the traditional logic-synthesis approach. Our experiments show that HLLM for ALUs outperforms logic-synthesis in area, delay and runtime, indicating that HLLM is a promising approach for reuse of datapath components in architectural synthesis.",
V-NET: a framework for a versatile network architecture to support real-time communication performance guarantees,"The paper proposes a new network architecture, called the versatile network architecture (V-NET), which provides a framework for flexible support of communication requirements in real-time networks. Applications communicate over the V-NET by using simplex end-to-end network connections which support specific real-time and reliability characteristics tailored to meet the application's specified requirements. V-NET differs from other proposed architectures in that it does not assume one specific packet scheduling algorithm or a specific traffic policing mechanism. Instead, the V-NET has been designed to support a variety of packet scheduling algorithms and traffic policing mechanisms. This flexibility is an important design consideration as a real-time network architecture must accommodate existing and future multimedia applications.",
Software cache coherence for large scale multiprocessors,"Shared memory is an appealing abstraction for parallel programming. It must be implemented with caches in order to perform well, however and caches require a coherence mechanism to ensure that processors reference current data. Hardware coherence mechanisms for large-scale machines are complex and costly, but existing software mechanisms for message-passing machines have not provided a performance-competitive solution. We claim that an intermediate hardware option-memory-mapped network interfaces that support a global physical address space-can provide most of the performance benefits of hardware cache coherence. We present a software coherence protocol that runs on this class of machines and greatly narrows the performance gap between hardware and software coherence. We compare the performance of the protocol to that of existing software and hardware alternatives and evaluate the tradeoffs among various cache-write policies. We also observe that simple program changes can greatly improve performance. For the programs in our test suite and with the changes in place, software coherence is often faster and never more than 55% slower than hardware coherence.",
Counting on computers in DSP education,"Global competition is forcing universities to rethink their approach to undergraduate education. Advanced hardware and software tools make it possible for students to grasp fundamental engineering concepts quicker, ultimately enhancing the undergraduate educational experience. The article highlights the roles played by software in DSP education, including symbolic and numeric processing as well as high-level programming of DSPs. The authors include a survey of some of the more popular software environments and identify important logistical issues.",
Optimal realization of any BPC permutation on K-extra-stage Omega networks,"An N/spl times/N k-Omega network is obtained by adding k more stages in front of an Omega network. An N-permutation defines a bijection between the set of N sources and the set of N destinations. Such a permutation is said to be admissible to a k-Omega if N conflict-free paths, one for each source-destination pair defined by the permutation, can be established simultaneously. When an N-permutation is not admissible, it is desirable to divide the N pairs into a minimum number of groups (passes) such that the conflict-free paths can be established for the pairs id each group. Raghavendra and Varma solved this problem for BPC (Bit Permutation Complement) permutations on an Omega without extra stage. This paper generalizes their result to a k-Omega where k can be any integer between 0 and n-1. An O(NlgN) algorithm is given which realizes any BPC permutation in a minimum number of passes on a k-Omega (0/spl les/k/spl les/n-1).",
An object-oriented nano-kernel for operating system hardware support,"The nano-kernel in the /spl mu/Choices operating system provides hardware support for the operating system. The nano-kernel is a single, modular subsystem that encapsulates the hardware and presents an idealized machine architecture to the rest of the system. Higher levels of the system that implement policy access the nano-kernel through a single interface. Thus the /spl mu/Choices nano-kernel is fully decoupled from higher level abstractions such as virtual memory or process paradigms. Within the nano-kernel, the hardware is modeled as a collection of abstract classes in a hardware support framework that are subclassed for particular hardware platforms. This architecture provides a highly modular and portable design making the system vastly easier to work with than previous versions of Choices. We have implemented a prototype of /spl mu/Choices that runs on UNIX.",
Higher-order unification via explicit substitutions,"Higher-order unification is equational unification for /spl beta//spl eta/-conversion, but it is not first-order equational unification, as substitution has to avoid capture. In this paper higher-order unification is reduced to first-order equational unification in a suitable theory: the /spl lambda//spl sigma/-calculus of explicit substitutions.",
A versatile packet multiplexer for quality-of-service networks,"A novel packet multiplexing technique, called rotating-priority-queues (RPQ), is presented which exploits the tradeoff between high efficiency, i.e., the ability to support many connections with delay bounds, and low complexity. The operations required by the RPQ multiplexer are similar to those of the simple, but inefficient, static-priority (SP) multiplexer. The overhead of RPQ, as compared to SP, consists of a periodic rearrangement (rotation) of the priority queues. It is shown that queue rotations can be implemented by updating a set of pointers. The efficiency of RPQ can be made arbitrarily close to the highly efficient, yet complex, earliest-deadline-first (EDF) multiplexer. Exact expressions for the worst case delays in an RPQ multiplexer are presented and compared to expressions for an EDF multiplexer.",
Illumination invariant object recognition,"Varying illumination is severe problem for existing face recognition algorithms. Altering the light direction from left to right, for example, causes a change of contrast in large face regions and causes most face recognition algorithms to fail. Theoretical results, based on the law of incoherent light superposition, provide the solid ground on which a new illumination invariant recognition algorithm is derived. A face recognition experiment demonstrates that this algorithm indeed shows improved recognition performance even if the conditions, for which the theoretical results were derived, do not hold exactly.","Lighting,
Object recognition,
Light sources,
Face recognition,
Vectors,
Layout,
Computer science,
Laboratories,
Solids,
Pixel"
A RPCL-CLP architecture for financial time series forecasting,"In this paper, we propose a new architecture based on the rival penalized competitive learning algorithm (RPCL) of Xu, Krzyzak and Oja (1993) and combined linear prediction method (CLP). The performance of RPCL-CLP is insensitive to the initial number of cluster nodes selected. Experimental results show that it is robust in long-term prediction for financial time series forecasting.",
Stabilization of inverted pendulum by the genetic algorithm,"The authors consider stabilization of an inverted pendulum which can be controlled by moving a cart in an intelligent way. Here, the authors adopt a PID(proportional plus integral plus derivative) control method to stabilize the pendulum since the PID controller has been extensively used in the industrial world. This controller requires the determination of PID control gains, but it is difficult to select the best gains theoretically. Thus, there have been many approaches to determine them empirically. Most of them are based on experience of operators and knowledge. Here, the authors propose a method using neural networks to tune the PID gains such that human operators tune the gains adaptively according to the environmental condition and systems specification. The tuning method is based on the error backpropagation method (BP method) and hence, it may be trapped in a local minimum. In order to avoid the local minimum problem, the authors use the genetic algorithm to find the initial values of the connection weights of the neural network and initial values of PID gains. The experimental results show the effectiveness of the present approach.",
Server-Directed Collective I/O in Panda,"We present the architecture and implementation results for Panda 2.0, a library for input and output of multidimensional arrays on parallel and sequential platforms. Panda achieves remarkable performance levels on the IBM SP2, showing excellent scalability as data size increases and as the number of nodes increases, and provides throughputs close to the full capacity of the AIX file system on the SP2 we used. We argue that this good performance can be traced to Panda's use of server-directed i/o (a logical-level version of disk-directed i/o [Kotz94b]) to perform array i/o using sequential disk reads and writes, a very high level interface for collective i/o requests, and built-in facilities for arbitrary rearrangements of arrays during i/o. Other advantages of Panda's approach are ease of use, easy application portability, and a reliance on commodity system software.",
Multivariate statistical techniques for parallel performance prediction,Performance prediction can play an important role in improving the efficiency of multicomputers in executing scalable parallel applications. An accurate model of program execution time must include detailed algorithmic and architectural characterizations. The exact values for critical model parameters such as message latency and cache miss penalty can often be difficult to determine. This research uses multivariate data analysis to estimate the values of these coefficients in an analytical model. Representing the coefficients as random variables with a specified mean and variance improves the utility of a performance model. Confidence intervals for predicted execution time can be generated using the standard error values for model parameters. Improvements in the model can also be made by investigating the cause of large variance values for a particular architecture.,
Structured methodology+object-oriented methodology+formal methods: methodology of SOFL,"There is a growing disappointment that formal methods have not been widely adopted in industry. One reason for this is that their application consumes prohibitive amounts of resource. Much research on the integration of available formal methods (e.g. Z, VDM, B-Method) and other structured methodology or object-oriented methodology have been conducted in order to make formal methods more practicable but its success has been very limited. However, little effort has been made to integrate properly formal methods, structured methodologies and object-oriented methodologies in order to take advantage of the desirable features of the three approaches. As an approach to the solution of these problems, we propose a language called SOFL (Structured-Object-Oriented-Formal Language) for system development. It supports the concept that a system be constructed using the structured methodology in the early stages of its development, and using object-oriented methodology at later, more detailed, levels. During the complete system development process, formal methods are applied in a manner that demonstrates their practicability. We first present the SOFL methodology (i.e. the methodology which SOFL supports), and then define the language SOFL by giving its abstract syntax together with an informal semantics. An example of developing a training centre system is used to demonstrate the usability of SOFL. Finally, future research on SOFL is briefly discussed.",
Coding-based replication schemes for distributed systems,"Data is often replicated in distributed systems to improve availability and performance. This replication is expensive in terms of disk storage since the existing schemes generally require full files to be stored at each site. In this paper, we present schemes which significantly reduce the storage requirements in replication based systems. These schemes use the coding method suggested by Rabin to store replicated data. The first scheme that we present is a modification of the simple voting algorithm and its quorum requirements. We then show how some of the extensions of the voting algorithm can also be modified to get storage efficient schemes for managing such replication. We evaluate the availability offered by these schemes and show that the storage space required to achieve certain availability are significantly lower than the conventional schemes with full file replication. Since coding is used, these schemes also provide a high degree of data security.",
Evaluating the effectiveness of process improvements on software development cycle time via system dynamics modelling,"Reducing software development cycle time without sacrificing quality is crucial to the continued success of most software development organizations. Software companies are investing time and money in reengineering processes incorporating improvements aimed at reducing their cycle time. Unfortunately, the impact of process improvements on the cycle time of complex software processes is not well understood. The objective of our research has been to provide decision makers with a model that will enable the prediction of the impact a set of process improvements will have on their software development cycle time. This paper describes our initial results of developing such a model and applying it to assess the impact of software inspections. The model enables decision makers to gain insight and perform controlled experiments to answer ""What if?"" type questions, such as, ""What kind of cycle time reduction can I expect to see if I implement inspections?"" or ""How much time should I spend on inspections?"".",
A comparison of neural network models for wheeze detection,An analysis of the use of neural networks to process lung sounds and identify wheezes is presented. Both raw signal data and Fourier transform data were used to train and test a series of neural networks. The purpose of this study was to compare the performance of the neural networks and their ability to detect wheezes in isolated lung sound segments.,
Discovery of self-replicating structures using a genetic algorithm,"Previous computational models of self-replication in cellular spaces have been manually designed, a very difficult and time-consuming process. This paper introduces the use of genetic algorithms to discover automata rules that govern emergent self-replicating processes. Given dynamically evolving automata, identification of effective fitness functions for self-replicating structures is a difficult task, and we give one solution to this problem. A model consisting of movable automata embedded in a cellular space is introduced and discussed in this context. A genetic algorithm using two fitness criteria was applied to automate rule discovery. After parameter tuning, 6 self-replicating structures consisting of 2, 3 and 4 automata were discovered over a course of 75 genetic algorithm runs. These results indicate that the fitness functions employed are effective and that genetic algorithms can be used to successfully discover rules for self-replicating structures.",
An application of random software testing,"Program testing remains the most commonly used method for verifying the reliability of software products. A popular method for integration testing is to develop a test plan based on the system requirements. We discuss some of the shortcomings of this approach and present our experiences with a random testing approach of a telecommunications project (a secure telephone conferencer). Among the benefits of this approach are improved requirements definition, improved fault detection, a simpler approach to system reliability estimation and reduced life-cycle costs.",
Fast Algorithms for Visualizing Fluid Motion in Steady Flow on Unstructured Grids,,
Broadcasting on the star and pancake interconnection networks,"Broadcasting is an important data communication operation in a parallel computer. In this paper, we first give a short survey on various broadcasting schemes on the star and pancake interconnection networks. We then present a broadcasting algorithm on the star and pancake networks, which can broadcast m messages of fixed length on an n-star or n-pancake in time O(n log n+m), improving the previous best result O(m log n log n). Our result is optimal in view of the /spl Omega/(n log n+m) lower bound for the problem. Moreover our algorithm works for both the star and pancake networks, while the previous O(m log n+n log n) algorithm is only for the star.",
Translation of object-oriented queries to relational queries,Proposes a formal approach for translating OODB queries to equivalent relational queries. The translation is accomplished through the use of relational predicate graphs and OODB predicate graphs. One advantage of using such a graph-based approach is that we can achieve bidirectional translation between relational queries and OODB queries.,
Markov chain algorithms for planar lattice structures,"Consider the following Markov chain, whose states are all domino tilings of a 2n/spl times/2n chessboard: starting from some arbitrary tiling, pick a 2/spl times/2 window uniformly at random. If the four squares appearing in this window are covered by two parallel dominoes, rotate the dominoes in place. Repeat many times. This process is used in practice to generate a random tiling and is a key tool in the study of the combinatorics of tilings and the behavior of dimer systems in statistical physics. Analogous Markov chains are used to randomly generate other structures on various two-dimensional lattices. The paper presents techniques which prove for the first time that, in many interesting cases, a small number of random moves suffice to obtain a uniform distribution.",
Competitive dynamic multiprocessor allocation for parallel applications,"In this paper we use competitive analysis to study preemptive multiprocessor allocation policies for parallel jobs whose execution time is not known to the scheduler at the time of scheduling. The objective is to minimize the makespan (i.e., the completion time of the last job to finish executing). We characterize a parallel job, J/sub i/, by two parameters: its execution time, l/sub i/, and its parallelism, P/sub i/, which may vary over time. The preemption and reallocation of processors can take place at any time. We devise a preemptive policy which achieves the best possible competitive ratio and then derive upper and lower bounds for scheduling N parallel jobs on P processors.",
A performance comparison of adaptive and static load balancing in heterogeneous distributed systems,"This paper focuses on using simulation to compare the performances of adaptive and static load balancing policies in a heterogeneous distributed system model. All the hosts (nodes) an the system are assumed to have the same function but possibly different processing capacities. The overheads and the delays for both job transfer and system state-information exchange are assumed to be nonnegligible. Simulation results show that both adaptive and static policies improve performance dramatically, and that the performance provided by static policies is not much inferior to that provided by adaptive policies. They also show that when overheads are nonnegligibly high at heavy system loads, static policies can provide performance more stable and better than that provided by adaptive policies.",
Reduction of symbolic rules from artificial neural networks using sensitivity analysis,"This paper shows how sensitivity analysis identifies and eliminates redundant conditions from the rules extracted from trained neural networks, by eliminating irrelevant inputs. This leads to a reduction in the number and size of the rules. The reduced rule set accurately and minimally reflect the classification problems presented. Also, the elimination of redundant input units significantly reduces the combinatorics of the rule extraction algorithm. The resultant rule set compares favorably with traditional symbolic machine learning algorithms.",
The Case for Design Using the World Wide Web,"Most information and services required today by designers will soon become available as documents distributed in a wide area hypermedia network. New integration services are required from the design environment, supporting business transactions with design information providers, automatic exchange of design data between independent groups, and integrated support for new forms of collaboration. We discuss design using electronic commerce and other services based on the Internet, and propose a hypermedia system organization for a new generation of CAD systems, conceived to make efficient use of that infrastructure. We also describe our experience as designers of an integrated design and documentation system that interfaces existing design and documentation tools with electronic commerce services based on the World Wide Web.",
Time domain analysis of transmission lines,"A simple numerical method is developed for the determination of the transient response of lossy transmission lines with a general impedance loading to a general voltage excitation. First, the transient response of the lossless transmission line with a load impedance to a voltage excitation is formulated. The steady state response of the line is then derived from its transient response. A recursive relation for determining the transient response is also given. Reflection coefficient at the load is determined by deriving differential equations directly and also by the use of the Thevenin's equivalent circuits. A computer program for the implementation of the numerical method together with several examples of the transient response of lossless/lossy transmission lines with passive/reactive loadings are presented. This numerical method has both practical engineering applications and pedagogical value.",
Realizing a high measure of confidence for defect level analysis of random testing [VLSI],"The defect level in circuit testing is the percentage of circuits, such as chips, which are defective and shipped for use after testing. In this work, it is demonstrated that the defect level of testing a circuit using random patterns should have a probability distribution rather than just a single value. Based on this concept, the confidence degree of a specified defect level for random testing can be derived, and the quality of circuit random testing is thus guaranteed. Results obtained based on random testing can be extended to other test methods, e.g., deterministic testing, pseudo-random testing, or functional testing. Experiments using computer simulation have been conducted for this work, and the results are very encouraging.",
Hybrid scan-conversion of circles,"Conventional algorithms for scan-conversion of circles select one pixel in each iteration. Run-length slice circle algorithms have therefore been suggested. These algorithms determine a run of pixels in each iteration. The speed of scan-conversion is therefore increased due to I/O. A hybrid approach to the scan-conversion of circles is presented. The new approach combines the advantages of the two methods into a hybrid algorithm. Speedup is achieved in the hybrid algorithm not only due to the reduction in the number of I/O operations, but also due to a reduction in the number of arithmetic operations.",
Measures of the potential for load sharing in distributed computing systems,"We are concerned with the problem of determining the potential for load balancing in a distributed computing system. We define a precise measure, called the number of sharable jobs, of this potential in terms of the number of jobs that can usefully be transferred across sites in the system. Properties of this measure are derived, including a general formula for its probability distribution, independent of any particular queuing discipline. A normalized version of the number of sharable jobs, called the job sharing coefficient, is defined. From the general formula, the probability distribution of the number of sharable jobs is computed for three important queuing models and exact expressions are derived in two cases. For queuing models in which an exact expression for the probability distribution of the number of sharable jobs is difficult to obtain, two methods are presented for numerical computation of this distribution. The job sharing coefficient is plotted against traffic intensity for various values of system parameters. Both of these measures are shown to be useful analytic tools for understanding the characteristics of load sharing in distributed systems and can aid in the design of such systems.",
Artificial neural networks in marine propeller design,"Various neural network systems were developed for examining propeller performance data that were derived experimentally. This study is aiming to establish an accurate mapping thus facilitating propeller selection during the design process. Different neural network architectures and learning rates were tested aiming at establishing a near optimum setup. It is evident from the findings so far, that this technology can be used effectively in modeling the performance of a series of marine propellers and thus may be used for propeller selection, and for extrapolation to new designs.",
Accessing Earth system science data and applications through high-bandwidth networks,"We discuss gigabit network applications enabled by ""Mission to Planet Earth"", an international effort to monitor the Earth as a system. We describe the design of a network architecture to support applications developed as part of this program; introduce a new component, public access resource centers (PARCs); and discuss how PARCs would facilitate access by users outside the traditional research community. We also describe how a particular class of users, agriculture users, might benefit from access to data collected as part of the Mission to Planet Earth program and delivered in a value-added form to them by a so-called AgPARC. The suggested architecture requires the deployment of high-bandwidth networks.",
Signature identification via local association of features,"Establishing the identify of a signature by automatic search through a database of signatures is of interest in several areas. This paper describes a system for this purpose. The proposed identification system uses a set of geometric and topologic features to characterize each signature. By considering the spatial distribution of these features, the system maps each signature into two strings of finite symbols. A local associative indexing scheme is then used on these strings to organize the collection of signatures of known identity. When presented with a signature of unknown identity, the system uses the same indexing scheme to retrieve a candidate set of signatures. A verification process is then carried out to find the best match from the candidate set. The performance of the proposed system has been tested with a moderate database. The results obtained indicate that the proposed system is able to identify signatures with great accuracy even when a part of a signature is missing.",
Integrating design and verification environments through a logic supporting hardware diagrams,"Formal methods and verification tools are difficult for designers to use. Research has been concentrated on handling large proofs; meanwhile, insufficient attention has been paid to the reasoning process. We argue that a heterogeneous logic supporting hardware diagrams and sentential logic provides a natural framework for reasoning and for the formal integration of design and verification environments. We present such a logic and demonstrate its flexibility on fragments of a traffic light controller design and verification problem.","Logic design,
Hardware,
Space exploration,
Automatic control,
Process design,
Computer science,
Lighting control,
Formal verification,
Humans,
Design engineering"
Pointing in an auditory interface for blind users,Graphical user interfaces represent a potential barrier for blind computer users. In particular the mouse pointing device is difficult to adapt to non-visual interaction. An adaptation was developed which created an auditory space based on the metaphor of the cursor as microphone. Experiments carried out suggest that-though this auditory interaction will never rival the efficiency of visual interaction-it would be viable for some operations.,
Efficient race detection for message-passing programs with nonblocking sends and receives,"This paper presents an algorithm for performing on-the-fly race detection for parallel message-passing programs. The algorithm reads a trace of the communication events in a message-passing parallel program and either finds a specific race condition or reports that the traced program is race-free. It supports a rich message-passing model, including blocking and non-blocking sends and receives, synchronous and asynchronous sends, receive selectivity by source and/or tag value, and arbitrary amounts of system buffering of messages. It runs in polynomial time and is very efficient for most types of executions. A key feature of the race detection algorithm is its use of several new types of logical clocks for determining ordering relations. It is likely that these logical clocks will also be useful in other settings.",
Interactive navigation inside 3D radiological images,"Many radiological imaging scanners generate high-resolution three-dimensional (3D) images. For complex anatomical regions such as the lungs and heart, these images can be used for interactive navigation inside the anatomy. The 3D image can act as a ""virtual environment"" representing the anatomy, and a computer-based system can be used for navigating through the environment. But such navigation must occur at interactive speeds if it is to be practical. This demands fast volume rendering from arbitrary viewpoints. The authors present an inexpensive fast volume rendering method that can generate sequences of views at interactive speeds. The method forms part of a system that permits dynamic navigation through 3D radiological images. The authors provide pictorial and numerical results for 3D pulmonary analysis.",
Imitation of animal behavior with use of a model of consciousness-behavior relation for a small robot,"This paper proposes an approach to designing behavior and its subjective world of a small robot to behave like an animal. This approach employs a hierarchical model of the relation between consciousness and behavior. The basic idea of this model is that a consciousness appears on a level in the hierarchical structure when an action on an immediately lower level is inhibited for internal or external causes, and that the appearing consciousness drives a chosen higher action. The computer simulation on a Mac shows the behavior of an artificial animal from reflex actions to catching of food. Its instantaneous consciousness that appears due to inhibited behavior is visualized with the behavior on the screen with use of colors according to emotions of the animal.",
A burst-oriented traffic control framework for ATM networks,"ATM networks are intended to accommodate all traffic types, including highly bursty traffic. In this paper, we present a traffic control framework for handling bursty traffic on a burst-by-burst basis. The protocol and associated mechanisms are designed to handle bursts of related cells as units. Hence, cells in a burst are forwarded or dropped as a unit. The framework includes mechanisms to allow a connection to acquire resources at a switch when a burst arrives and to release them when a burst leaves. In addition, techniques for bundling such bursty connections into virtual paths are described. A candidate burst-oriented call admission control scheme is also presented. The performance of the burst-oriented approach is then analyzed.",
An environment for the reverse engineering of executable programs,"Reverse engineering of software systems has traditionally centered upon the generation of high level abstractions or specifications from high level code or databases. We report on a reverse engineering environment for low level executable code: a reverse compilation or decompilation environment that aids in the understanding of the underlying executable program. The reverse compilation process recovers high level code from executable programs at a higher representation level than that produced by disassemblers; in fact, disassembly is part of the first stage in this process. Several tools aid in the process of reverse compilation, these are: loaders, signature generators, library prototype generators, disassemblers, library bindings, and language to language translators. The integration of these tools in the whole process is presented in this paper. The results obtained by the prototype reverse compilation system dcc are encouraging: high level code is regenerated with correct use of expressions and control structures, and the complete elimination of registers and condition codes. An elimination rate of low level instructions of over 75% was reached, representing the overall improvement this decompiler system has made over previous decompilers and disassemblers (where the rate tends to be nil). A sample decompilation program is given.",
Hidden Markov mesh random field: theory and its application to handwritten character recognition,"In recent years, there have been some attempts to extend one-dimensional hidden Markov model (HMM) to two-dimensions. This paper presents a new statistical model for image modeling and recognition under the assumption that images can be represented by a third-order hidden Markov mesh random field (HMMRF) model. We focus on two major problems: image decoding and parameter estimation. A solution to these problems is derived from the scheme based on a maximum, marginal a posteriori probability criterion for the third-order HMMRF model. We also attempt to illustrate how theoretical results of HMMRF models can be applied to the problems of handwritten character recognition.",
A game-theoretic classification of interactive complexity classes,"Game-theoretic characterisations of complexity classes have often proved useful in understanding the power and limitations of these classes. One well-known example tells us that PSPACE can be characterized by two-person, perfect-information games in which the length of a played game is polynomial in the length of the description of the initial position [by Chandra et al., see Journal of the ACM, vol. 28, p. 114-33 (1981)]. In this paper, we investigate the connection between game theory and interactive computation. We formalize the notion of a polynomially definable game system for the language L, which, informally, consists of two arbitrarily powerful players P/sub 1/ and P/sub 2/ and a polynomial-time referee V with a common input w. Player P/sub 1/ claims that w/spl isin/L, and player P/sub 2/ claims that w/spl isin/L; the referee's job is to decide which of these two claims is true. In general, we wish to study the following question: What is the effect of varying the system's game-theoretic properties on the class of languages recognizable by polynomially definable game systems? There are many possible game-theoretic properties that we could investigate in this context. The focus of this paper is the question of what happens when one or both of the players P/sub 1/ and P/sub 2/ have imperfect information or imperfect recall. We use polynomially definable game systems to derive new characterizations of the complexity classes NEXP and coNEXP.",
Empirical study of parallel trace-driven LRU cache simulators,"This paper reports on the performance of four parallel algorithms for simulating an associative cache operating under the LRU (Least-Recently-Used) replacement policy. Three of the algorithms are implemented on the MasPar MP-2. Another algorithm is a parallelization of an efficient serial algorithm on the Intel Paragon. We assess the strengths and weaknesses of these algorithms as a function of problem size and characteristics, and compare their performance on traces derived from execution of three SPEC92 benchmark programs.",
A systolic algorithm and architecture for image thinning,"In this paper, we describe a new special purpose VLSI architecture for image thinning. The architecture is systolic and is based on an algorithm that achieves a high degree of parallelism. The proposed algorithm computes the skeleton of multiple objects in an image in linear time by making 2 scans over the 4-distance transform of the image. The algorithm is mapped onto a linear systolic array of simple processing elements (PEs) and for an N/spl times/N image, the architecture requires N PE's. The entire array can be realized in a single VLSI chip. The proposed hardware can perform thinning on a 512/spl times/512 image in 2.59 msec and on a 256/spl times/256 image in 0.327 msec. Currently, a prototype CMOS VLSI chip implementing the proposed architecture is being designed and built at the University of South Florida.",
Finite state machine decomposition for I/O minimization,"In this paper, we consider the problem of decomposing a Finite State Machine (FSM) into communicating FSMs to minimize the number of inputs/outputs. We propose an FSM decomposition procedure based on partitioning the set of transitions that describes the behavior of an FSM. An extended FM-based partitioning algorithm is applied for transition partitioning. We also devise a state output encoding technique to further reduce the number of interconnections between the FSMs required for communication. Experimental results for MCNC benchmarks show that our algorithm has favorable results over circuit partitioning algorithms on the netlist level.",
An action-resource language for argumentation: the case of softwood lumber negotiation,"We view a group problem as a resource allocation problem among involved parties, and negotiation as the members' act to protect, or better yet, to gain additional resources for themselves. Group members gather together to achieve a common goal. Yet, they are also stakeholders seeking to defend their own interest as well. As such, negotiation can be seen as an effort of all parties seeking to exchange viewpoints, proposing offers and counter-offers with arguments, until a solution is found or a deadlock is considered unsolvable. This paper proposes an action resource model to represent negotiation problems, and an argumentation language to support negotiation processes. The proposed action resource argumentation language is illustrated by an actual negotiation-the softwood lumber negotiation between Canada and the United States.","Computer aided software engineering,
Problem-solving,
Wood industry,
US Department of Commerce,
Water resources,
Protection,
System recovery,
Electronic switching systems,
Government,
Injuries"
Validation of a new SPECT quantification method using computer simulation,"The authors present a new automated method for SPECT quantification and display, and its validation using computer simulations. The computer simulations provide validation for calculating myocardial defect severity. The short axial slices of the phantoms were divided into four walls (Anterior, Septal, Inferior and Lateral). Simulated myocardial defects with various degrees of severity were created in each wall from apical to basal slices. Twenty-nine computerized phantoms each with twelve SPECT short axial slices were quantified and analyzed using the Yale-CQ (Yale Circumferential Quantification) software. Two dimensional (2D) and three dimensional (3D) quantitative profiles and defect scores from the simulated short axial slices were obtained. The calculated defect score correlated highly with simulated defect severity (R=0.99 both in 2D and 3D). Thus, the quantitative defect score generated with the authors' SPECT quantification algorithm can be used as a reliable index for detecting severity of myocardial perfusion defect.",
An incremental learning method with recalling interfered patterns,"This paper presents a new incremental learning method for neural networks. If we train a neural network to memorize novel patterns only by a presentation of the novel patterns, the network will forget patterns that the network had already learnt. This problem is caused by the fact that novel patterns usually interfere in old patterns memorized by the network. In the new method, the network recalls memorized patterns which seem to be interfered by the novel patterns, and then the network learns both novel patterns and recalled patterns.",
Learning in man-machine systems: the measurement of behavioural and cognitive complexity,"A learning experiment was carried out to investigated the development of mental structures. Six subjects were carefully instructed to operate a commercial database management system (DBMS). The long-term knowledge about general DBMS- and computer experience was measured with a questionnaire once at the beginning of the investigation. On three weeks in a row all subjects had to solve the same task twice repeated in an individual session, overall there are six solutions of the same task. At the beginning of each of the three individual sessions the short-term knowledge about the task and the tool was measured with a short questionnaire. For each task solving process all keystrokes were recorded with a time stamp in a logfile. With a special analysing program the logical structure of each empirically observed task solving process was extracted. This logical structure is given as a Petri net. The behavioural complexity (BC) of this net structure can be measured with the McCabe-measure. With some special assumptions the cognitive complexity (CC) can be derived from the empirically gained BC. The main results are: (1) the time structure and BC measure different aspects of the learning process; (2) the time structure is-overall-positively correlated with BC and negatively correlated with CC; and (3) as well the long-term- as the short-term knowledge has an increasing predictive power with the time structure, but not with BC and CC.",
"Discovering attribute relationships, dependencies and rules by using rough sets","The paper reviews the methodology of the application of the theory of rough sets to the problem of knowledge discovery in databases. The methodology is based on the idea of information generalization, to look at the data at various levels of abstraction, followed by the discovery, analysis and simplification of significant data relationships, dependencies, fundamental factors and rules.",
The Milan Civic Network experience and its roots in the town,"The Milan Civic Network (Rete Civica di Milano, RCM for short) is a project of the Department of Computer Science of the University of Milan. The initiative of promoting a Civic Network intends to avail the local community of the skills developed at the department, giving the city a vantage point on the technological communication environment of the twenty-first century. RCM is strongly inspired by Internet values, such as being free and open, and by the principles and goals of community networks. RCM is a BBS based on the SoftArc First Class software, whose server runs on a Apple Macintosh Workgroup Server 95 made available by Apple Computers, while the client software is available both for Mac and Windows platforms. The authors outline the technical details and some special choices we have done and sum up the evolution of the network. They present RCM desktop and its major conferences. Finally, they discuss the features which characterize RCM as a community network rooted in the town.",
Binocular estimation of motion and structure from long sequences using optical flow without correspondence,"We use the left and right monocular motion and structure parameters of two stereo image sequences (direction of translation, relative depth, observer rotation and rotational acceleration) to compute the absolute depth, absolute translation and absolute translational acceleration for each pair of left and right images. Individual translation parameters computed at each frame are integrated over time using a Kalman filter to provide more accuracy and a ""best"" estimate of the absolute translation at each time.",
Continuous purposive sensing and motion for 2D map building,This article describes planning of sensing and motion by a mobile robot to build the map of a 2D unknown environment. The map is represented as a graph that describes the structure of the skeleton of the environment. It is incrementally built by connecting the local structure of the skeleton extracted from sensory data at each position of sensing. The motion of the robot to observe currently unknown region is first roughly planned with the graph-based global map and then precisely planned with a geometric local sensory information.,
Enriching the expressive power of security labels,"Common security models such as Bell-LaPadula focus on the control of access to sensitive data but leave some important systems issues unspecified, such as the implementation of read-only objects, garbage collection, and object upgrade and downgrade paths. Consequently, different implementations of the same security model may have conflicting operational and security semantics. We propose the use of more expressive security labels for specifying these system issues within the security model, so that the semantics of a system design are precisely understood and are independent of implementation details.",
Modeling foreshortening in stereo vision using local spatial frequency,"Many aspects of the real world continue to plague stereo matching systems. One of these is perspective foreshortening, an effect that occurs when a surface is viewed at a sharp angle. Because each stereo camera has a slightly different view, the image of the surface is more compressed and occupies a smaller area in one view. These effects cause problems because most stereo methods compare similarly sized regions (using the same-sized windows in both images), tacitly assuming that objects occupy the same extents in both images. Clearly this condition is violated by perspective foreshortening. We show how to overcome this problem using a local spatial frequency representation. A simple geometric analysis leads to an elegant solution in the frequency domain which, when applied to a Gabor filter-based stereo system, increases the system's maximum matchable surface angle from 30 degrees to over 75 degrees.",
Multi-dimensional interleaving for time-and-memory design optimization,"This paper presents a novel optimization technique for the design of application specific integrated circuits dedicated to perform iterative or recursive time-critical sections of multi-dimensional problems, such as image processing applications. These sections are modeled as cyclic multi-dimensional data flow graphs (MDFGs). This new technique, called multi-dimensional interleaving consists of an expansion and compression of the iteration space while considering memory requirements. It guarantees that all functional elements of a circuitry can be executed simultaneously, and no additional memory queues proportional to the problem size are required. The algorithm runs in O(|E|) time, where E is the set of edges of the MDFG representing the circuit.",
An initial evaluation of the Convex SPP-1000 for earth and space science applications,"The Convex SPP-1000, the most recent SPC, is distinguished by a true global shared memory capability based on the first commercial version of directory based cache coherence mechanisms and SCI protocol. The system was evaluated at NASA/GSFC in the Beta-test environment using three classes of operational experiments targeting earth and space science applications. A multiple program workload tested job-stream level parallelism. Synthetic programs measured overhead costs of barrier, fork-join, and message passing synchronization primitives. An efficient tree-code version of an N-body simulation revealed scaling properties and measured the overall efficiency. This paper presents the results of this study and provides the earliest published evaluation of this new scalable architecture.",
A parallel control computer structure for complex high speed applications,With high speed control applications it is often very difficult to achieve the aspired timing of the control algorithm using just one processor. This is true especially if the system is complex or very large. In this paper we discuss advantages and disadvantages of a new system architecture using many processors of which each one is specialised for one part of the algorithm. A tailorable parallel computer architecture which we developed to cope with the problems of parallelisation of control algorithms is presented. Finally examples from the field of robotics and the performance of the prototype computer is presented.,
Simulation programming languages: an abridged history,"Knowing history can be protective; we have all heard that those who do not are doomed to repeat it. Considering one well regarded expert's estimate of 137 simulation programming languages (SPLs) created by 1981, many perhaps have already duplicated the numerous mistakes of their predecessors. History can also be informative, instructive and entertaining as hopefully this abridged and differently focused approach can illustrate. Questions concerning the causes for so many SPLs, the remarkably similar parallel developments, and the role of the SPLs versus programming languages in general might admit to historical answers. At the least, sharing speculations could prove enlightening and amusing.",
"Defining, computing, and visualizing molecular interfaces","A parallel, analytic approach for defining and computing the inter and intra molecular interfaces in three dimensions is described. The molecular interface surfaces are derived from approximations to the power diagrams over the participating molecular units. For a given molecular interface our approach can generate a family of interface surfaces parametrized by /spl alpha/ and /spl beta/, where /spl alpha/ is the radius of the solvent molecule (also known as the probe radius) and /spl beta/ is the interface radius that defines the size of the molecular interface. Molecular interface surfaces provide biochemists with a powerful tool to study surface complementarity and to efficiently characterize the interactions during a protein substrate docking. The complexity of our algorithm for molecular environments is O(nk log/sup 2/ k), where n is the number of atoms in the participating molecular units and k is the average number of neighboring atoms-a constant, given /spl alpha/ and /spl beta/.",
Approximating the volume of definable sets,"The first part of this paper deals with finite-precision arithmetic. We give an upper bound on the precision that should be used in a Monte-Carlo integration method. Such bounds have been known only for convex sets; our bound applies to almost any ""reasonable"" set. In the second part of the paper, we show how to construct in polynomial time first-order formulas that approximately define the volume of definable sets. This result is based on a VC dimension hypothesis, and is inspired from the well-known complexity-theoretic result ""BPP/spl sube//sub 2/"". Finally, we show how these results can be applied to sets defined by systems of inequalities involving polynomial or exponential functions. In particular, we describe an application to a problem of structural complexity in the Blum-Shub-Smale model of computation over the reals.",
Two-way induction,"General-to-specific learners like ID3 and CN2 perform well when the target concept descriptions are general, but often have difficulties when they are specific or mixed. This problem can be alleviated by combining them with a specific-to-general learning component, resulting in a two-way induction system. In this paper one design for such a component is proposed, as well as two methods for combining the two components. Experiments on artificial domains show the combined learner to match or outperform ""pure"" versions of C4.5 and CN2 across the entire generality spectrum, with the advantage increasing for greater concept specificity. Experiments on 24 real-world domains from the UCI repository confirm the utility of two-way induction: the combined learner achieves higher accuracy than C4.5 in 17 domains (at the 5% significance level in 12), and similar results are obtained with CN2. Closer observation of the system's behavior leads do a better understanding of its ability to correct overly-general rules with specific ones, and shows that there is still room for improvement.",
Pattern recognition by topology free spatio-temporal feature map,"This paper introduces a novel concept that eliminates the need for a topologically ordered feature map required for a pattern classification task. Spatio-temporal feature maps (extensions to Kohonen's self-organizing feature maps) have been shown earlier to provide enhanced classification performances over self-organizing feature maps. However, a topologically ordered feature map is still required as a basis to form a spatio-temporal map. In this paper, it is shown that by picking suitable samples of the input patterns as weights and ensuring that the selected weights are stratified and contained within the convex hull of the input space, the high classification performance of the spatio-temporal feature maps can still be retained. Such a formation of spatio-temporal feature map has no relation to topology preservation concept and the new classification paradigm is, therefore, topology free. The simulation results on 8-class texture and 12-class 3D object feature data sets confirm the high classification performance without the need for computationally expensive training required to obtain topologically ordered feature maps.",
A multi-level approach to the transport of MPEG-coded video over ATM and some experiments,"The CCITT (ITU-IT) has decided that the B-ISDN services of the future will be supported by asynchronous transfer mode (ATM), since it can offer the benefits of statistical multiplexing to bursty traffic, thus bringing down costs. There has been a considerable body of research done on the problem of congestion control in ATM networks, and several different approaches have been investigated. A four-level approach is proposed to the very specific problem of transporting MPEG encoded real-time video over ATM, and it is argued that this is consistent with current research results and directions. Then, using a real video source and the simplest option in the MPEG standard, the intra-frame coding, we experimentally evaluate alternatives for the implementation of the lower three levels, and show that the approach appears feasible.",
Exploratory data mining and analysis using CONQUEST,"Exploratory data mining and analysis requires an extensible environment which provides facilities for the user-friendly expression and rapid execution of ""scientific queries"". The authors present the CONQUEST (parallel query processing system) environment and illustrate its use for exploratory data analysis and data mining of spatio-temporal phenomena from geophysical datasets.",
Electromagnetic scattering of large structures in layered earths using integral equations,"An electromagnetic scattering algorithm for large conductivity structures in stratified media has been developed and is based on the method of system iteration and spatial symmetry reduction using volume electric integral equations. The method of system iteration divides a structure into many substructures and solves the resulting matrix equation using a block iterative method. The block submatrices usually need to be stored on disk in order to save computer core memory. However, this requires a large disk for large structures. If the body is discretized into equal-size cells it is possible to use the spatial symmetry relations of the Green's functions to regenerate the scattering impedance matrix in each iteration, thus avoiding expensive disk storage. Numerical tests show that the system iteration converges much faster than the conventional point-wise Gauss-Seidel iterative method. The numbers of cells do not significantly affect the rate of convergency. Thus the algorithm effectively reduces the solution of the scattering problem to an order of O(N2), instead of O(N3), as with direct solvers.",
Reasoning about Object-Z specifications,This paper presents a method of reasoning about Object-Z specifications. The approach utilises the modularity inherent in Object-Z specifications to simplify proofs. Properties proved for a class in isolation can be used when that class is either inherited by another class or instantiated as part of a system of interacting objects. Proofs using structural induction and the notion of object integrity are discussed.,"Logic,
Computer science,
Large-scale systems,
Guidelines"
Handprinted digit recognition by stroke tracing,"A structural method of recognizing handprinted digits is described, in which the strokes comprising the digits are decomposed into line segments. The lines are matched against stroke templates, beginning at a ""pen down"" point and traced as far as possible. Both direction and length are important to the template matching. Error rates of 3.3% are reported.",
Study of seismic activity in Central Asia applying a parallel distributed paradigm,"Historical data of time, location and magnitude of earthquakes in Central Asia from 25 A.D. to the present are inputted into a neural network and trained using the backpropagation paradigm. The resulting network is utilized to make predictions.",
Learning to change the world: a case study of a mechanical engineering design course,"The National Science Foundation in collaboration with its engineering school coalition partners has been exploring methods for improving undergraduate engineering education. This initiative began in response to a 1992 survey of engineering deans and employers that identified specific weaknesses in engineering education. In response to this survey NSF and its partners have been attempting to remediate the identified weaknesses through specific educational practices. These include providing opportunities for: creative problem formulation and solving experiences; designing in teams; developing written, spoken and graphical communication skills; and using computers as cognitive tools. We know that engineering students need such opportunities to develop the kinds of skills that anoint them to change the world, but we know little about the pedagogic issues involved in developing such skills. This paper reports on a preliminary ethnographic study of a mechanical engineering design classroom. Although not developed in response to the NSF initiative, the course as it has evolved over the years has incorporated most of the the aforementioned remediation recommendations.",
Laboratory setup for teaching and research in computer-based power system protection,"This paper describes the laboratory used for teaching and conducting research in the Department of Electrical Engineering at the University of Saskatchewan, Canada, in the area of computer-based power system protection. The laboratory facilities are used for conducting research and teaching protection related courses. The use of the laboratory is illustrated by describing the implementation of two recent research projects for which laboratory facilities have been used.",
Shape tensors for efficient and learnable indexing,"Multi-point geometry: The geometry of 1 point in N images under perspective projection has been thoroughly investigated, identifying bilinear, trilinear, and quadrilinear relations between the projections of 1 point in 2, 3 and 4 frames respectively. The dual problem-the geometry of N points in 1 image-has been studied mostly in the context of object recognition, often assuming weak perspective or affine projection. We provide here a complete description of this problem. We employ a formalism in which multiframe and multi-point geometries appear in symmetry. Points and projections are interchangeable. We then derive bilinear equations for 6 points (dual to 3-frame geometry), trilinear equations for 7 points (dual to 3-frame geometry), and quadrilinear equations for 8 points (dual to the epipolar geometry). We show that the quadrilinear equations are dependent on the the bilinear and trilinear equations, and we show that adding more points will not generate any new equation. The new equations are used to design new algorithms for the reconstruction of shape from many frames, and for learning invariant relations for indexing into a database.",
Line feature-based recognition using Hausdorff distance,"A point-based (edge pixels) correlational method using the Hausdorff distance to determine if there is any model pattern in a given image was proposed by Huttenlocher et al. (1993). While this approach works well and it is computationally efficient in the presence of model translation in the image, it is significantly time consuming when the model has been rotated and scaled. We propose a line-feature based approach for model based recognition using the Hausdorff distance. This new approach reduces the problem of finding the rotation and scaling to the problem of finding two translations, therefore exploiting the efficiency of the Huttenlocher algorithm. The use of line features separates the rotation, scaling and translation so that each of them can be handled individually. The line features in the original domain are first transformed into a new 2D domain consisting of the orientation and the logarithm of the length of the line. In this way, rotation and scaling in the original domain correspond to a translation in the new domain and the Hausdorff point-based matching is used to find it. Next, the model is rotated and scaled using the result from first matching and second Hausdorff distance matching is performed to determine the model translation. The method performance and sensitivity to segmentation problems are characterized using and experimental protocol with simulated data. It was found that the algorithm performs well, degrading nicely as the segmentation problems increase. The algorithm was also tested with real images.",
Simulation and statistical education,"Discrete event simulation has benefited from statistical analysis but the reverse is not true. Recent advances in computer technology and software development have made it possible to have PC's running specialized simulation languages readily available. The paper discusses how discrete event simulation, developed via specialized simulation languages (e.g., GPSS), can become a useful class resource to teach and motivate statistics students. In addition, simulation helps to present, more effectively, interdisciplinary case studies, to increase group learning and to relieve students and instructors from statistical drudgery. Examples of such GPSS simulation are developed.",
Establishing a framework for comparative analysis of genome sequences,"This paper describes a framework and a high level language tool for comparative analysis of genome sequence alignment. The framework integrates the information derived from multiple sequence alignment and phylogeny to derive new properties about homologous sequences. Multiple sequence alignments are treated as an abstract data type. Abstract operations have been described to manipulate a multiple sequence alignment, and to derive mutation related information from a phylogenetic tree by superimposing parsimony. The framework has been applied to derive constrained columns which exhibit evolutionary pressure to preserve a common property in a column despite mutation. A Prolog tool based on the the framework has been implemented and demonstrated on alignments containing 3000 sequences and 3904 columns.",
A program behavior model and its evaluation,"We investigate a program behavior model that generates address reference strings including the interreference durations. This model views a trace as the sample path of a semi-Markov process. The transition probability and sojourn matrices of that process are statistically inferred. Two methods of evaluating the accuracy of the model are described. We experimentally evaluate the model on the shared data traces of the individual processes of a range of realistic parallel programs executing on a shared memory multiprocessor. The model generates traces that closely approximate the original trace, however, the number of model states needed for accuracy is large.",
Deadlock detection in communicating finite state machines by even reachability analysis,"Fair reachability is a very useful technique in detecting errors of deadlocks and unspecified receptions in networks of communicating finite state machines (CFSMs) consisting of two machines. The paper extends the classical fair reachability technique, which is only applicable to the class of two-machine CFSMs, to the general class of CFSMs. For bounded CFSMs, the extended fair reachability technique reduces by more than one half the total number of reachable global states that have to be searched in verifying freedom from deadlocks. The usefulness of the new reachability technique, called even reachability, is demonstrated through two examples.",
Logic Partition Orderings for Multi-FPGA Systems,"One of the critical issues for multi-FPGA systems is developing software tools for automatically mapping circuits. In this paper we consider one step in this process, partitioning. We describe the task of finding partition orderings, i.e., determining the way in which a circuit should be bipartitioned so as to best map it to a multi-FPGA system. This allows multi-FPGA partitioners to harness standard partitioning techniques. We develop an algorithm for finding partition orderings, which includes a method for increasing parallelism in the process, as well as for including multi-sectioning and multi-way partitioning algorithms. This method is very efficient, and capable of handling most of the current multi-FPGA topologies.",
"Communication, collaboration and cooperation in software development-how should we support group work in software development?","Software development is essentially cooperative work which is collaboratively performed by various roles of persons and tools. Communication among the members of a development team (e.g. conversation) and among the tools is one of the most important characteristics for this collaborative work. To make our software development environment more effective and comfortable, we should observe what communication, collaboration and cooperation are actually made in development processes and what styles are suitable for us. In this paper, several case studies and analytic results in software development are surveyed, and I discuss what kinds of tools are required to seamlessly support group work in software development.",
Theoretical and practical considerations of uncertainty and complexity in automated knowledge acquisition,"Inductive machine learning has become an important approach to automated knowledge acquisition from databases. The disjunctive normal form (DNF), as the common analytic representation of decision trees and decision tables (rules), provides a basis for formal analysis of uncertainty and complexity in inductive learning. A theory for general decision trees is developed based on C. Shannon's (1949) expansion of the discrete DNF, and a probabilistic induction system PIK is further developed for extracting knowledge from real world data. Then we combine formal and practical approaches to study how data characteristics affect the uncertainty and complexity in inductive learning. Three important data characteristics, namely, disjunctiveness, noise and incompleteness, are studied. The combination of leveled pruning, leveled condensing and resampling estimation turns out to be a very powerful method for dealing with highly disjunctive and inadequate data. Finally the PIK system is compared with other recent inductive learning systems on a number of real world domains.",
Divide-and-conquer programming on MIMD computers,We have developed a programming template to implement divide and conquer algorithms on MIMD computers. The template is based on the parallel divide and conquer function of Z.G. Mou and P. Hudak (1988). We explore the programmability and performance of this approach by solving some well known numerical problems on a shared memory multiprocessor and a multicomputer. A byproduct of this work is a new parallel algorithm for solving tridiagonal systems of equations.,
Generalized modular design of testable m-out-of-n code checker,"A testable design of a programmable m-out-of-n code checker is reported in this paper. The checker is modular in nature and can be easily extended by cascading. Basically, a cellular automaton (CA) structure is taken whose combinational logic port (CL-part) is modified in such a way that all the 1's in the initial state (seed) get accumulated in the rightmost cells. This CA is then transformed into an iterative array of combinational logic cells. To make the resulting structure testable for all stuck-at and unidirectional faults, it is partitioned into a number of identical blocks the outputs of which are combined to produce two complementary outputs.",
Views of media objects in multimedia databases,"Multimedia database systems must manage large amounts of graphical, textual, and audio data. To compound the problem, if a multimedia designer creates a new media image from an existing item, current systems store the new object as an independent image. This space is wasted since the data already exists in the system. In order to eliminate this wasted space, views of stored objects should be used instead of replicated data. Views are well known in relational databases to have both benefits and drawbacks. In order to overcome these drawbacks, the paper defines a view schema, which mirrors the media hierarchy; a view dag which determines the base files used to create the view; and view independence, a paradigm for performing operations on views. The criteria for a canonical media algebra are outlined. A canonical media algebra allows multiple editors to instantiate a view. Finally, optimization of specifications is discussed.",
Transient analysis of cell loss control mechanisms in ATM networks,"A new technique is developed to analyze the cell loss behavior of individual connections in finite intervals for ATM networks. The technique is applied to partial buffer sharing (PBS) in deriving formulae for calculating cell loss rate and queue length distributions for each cell class in a single connection in finite intervals. To maximize network utilization, a new and more flexible cell loss control mechanism called threshold-based loss balancing (TLB) is proposed which restricts the violation probability of each cell class. Comparisons of TLB and PBS on selected operating conditions reveal that TLB can accept more connections under the same workload conditions. PBS is shown to be a special case of TLB.",
Effects of computer-mediated communication on group negotiation: an empirical study,"A laboratory experiment was conducted to test the impact of communication medium (computer mediated versus face-to-face) on the performance of three-person groups with an integrative negotiation task. Negotiation process variables this study investigated were: conflict management behavior, judgment accuracy and negotiation time. Negotiation outcome variables considered were joint profit and satisfaction. Results showed that computer-mediated groups used more distributive approach, perceived other group members' priorities less accurately and took more time to reach a group agreement than did groups that used the face-to-face communication medium. It was also found that computer-mediated groups achieved lower joint profit and showed less satisfaction with the communication medium than did face-to-face groups.",
"Decision trees with AND, OR queries","We investigate decision trees in which one is allowed to query threshold functions of subsets of variables. We are mainly interested in the case where only queries of AND and OR are allowed. This model is a generalization of the classical decision tree model. Its complexity (depth) is related to the parallel time that is required to compute Boolean functions in certain CRCW PRAM machines with only one cell of constant size. It is also related to the computation using the Ethernet channel. We prove a tight lower bound of /spl theta/(k log(n/k)) for the required depth of a decision tree for the threshold-k function. As a corollary of the method we also prove a tight lower bound for the ""direct sum"" problem of computing simultaneously k copies of threshold-2 in this model. Next, the size complexity is considered. A relation to depth-three circuits is established and a lower bound is proven. Finally the relation between randomization, nondeterminism and determinism is also investigated, we show separation results between these models.",
Heuristic networks for concurrent pursuit-evasion systems,We consider briefly the linguistic geometry tools and their application to solving 2D optimization problem for autonomous robotic vehicles participating in the aerospace pursuit-evasion game. In this problem we allow the agents to move simultaneously if necessary. A comparison of the searches for aerial and concurrent cases shows that search reduction achieved in the concurrent case is even more dramatic than it was in the serial one.,
On rule checking and learning in an acupuncture diagnosis fuzzy expert system by genetic algorithm,"ADFL is an acupuncture diagnosis expert system based on fuzzy logic, which has been reported by Zhang et al. (1993, 1994). This paper gives a method to learn ADFL rules by genetic algorithms (GAs), and a technique to optimize and check fuzzy membership functions. For any length gene, we present an extended ordinal representation method. A fitness function described with fuzzy rules is explained. The experimental result indicates this method is effective.",
Writing- and communications-across-the-curriculum in the Materials Science and Engineering Department at Virginia Tech,"The Materials Science and Engineering Department (MSE) comprehensive undergraduate writing- and communications-across-the-curriculum program is integrated into eight required MSE courses over students' three years of study in the Department. The program focuses mostly on the development of students' discipline-specific writing skills but also includes instruction in public speaking, interpersonal communications in the workplace, creative scientific thinking, and collaborative writing. In addition, the program includes faculty training and support for integrating writing into technical coursework, developing writing assignments, assisting students in revising their own writing, and grading written assignments. Our writing and communications program is integrated into eight required MSE courses in such a way that the writing and communication tasks in each class do not too often duplicate those addressed in subsequent program classes. We are developing methods for teaching course content using writing and communications as learning tools in such a way that time for technical content instruction is not sacrificed.",
Incremental design of scalable interconnection networks using basic building blocks,"We present an incremental design of scalable interconnection networks using basic building blocks, including both network topologies and routing. We consider wormhole-routed small-scale 2D meshes as basic building blocks. The minimum requirement to expand these networks is a single building block. This implies that the network does not have to maintain the regular 2D mesh topology. We introduce some new topologies: incomplete meshes based on those adaptive routing algorithms designed from the turn model and extended incomplete meshes based on XY routing. We show that the original routing function can be adopted to send a message between any source and destination without using store-and-forward and causing deadlock. The way we construct the network incrementally minimizes the amount of rewiring and keeps high bisection density and short diameter of the network. The design methods can be used to economically and incrementally build expandable and scalable parallel computers.",
Three-dimensional EEG source imaging via maximum entropy method,"Electroencephalographic imaging or the estimation of three-dimensional dipole current sources on the cortical surface from scalp measured potential distribution is a highly underdetermined inverse problem as there are many 'feasible' images which are consistent with the EEG data. Here, the authors explore the maximum entropy approach to obtain better solutions to this problem. This estimation technique selects that image from the possible set of feasible images which is maximally smooth and most likely to give rise to the measured potential field. The authors have also incorporated a noise-rejection mechanism in their scheme that allows them to estimate the noise component of the recorded potentials. Computer simulation demonstrates that the proposed method can reconstruct three-dimensional current distribution where the conventional least-squares minimum-norm method fails. A preliminary testing of this method on evoked potential data measured during visual checkerboard stimulation at 3 Hz yielded sources largely concentrated in the posterior areas of the brain including a region roughly corresponding to the primary visual cortex.",
Comparison of the Hopfield scheme to the hybrid of Lagrange and transformation approaches for solving the traveling salesman problem,"A novel scheme, the hybrid of Lagrange and transformation approaches (Hybrid LT), was proposed by Xu (1994) to solve a combinatorial optimization problem. It separates the constraints into linear-constant-sum constraints and binary constraints. The linear-constant-sum constraints are treated by the Lagrange approach while the binary constraints are transformed into penalty or barrier functions. This paper compares the performance of the Hopfield net and the Hybrid LT based on computer simulations in solving the traveling salesman problem (TSP). The experimental results show that the Hybrid LT is superior to the Hopfield net for greater speed of convergence, higher rate of finding valid solutions and shorter paths found.",
Heuristic strategy for feature matching in parallel curve detection,"Feature matching has been proved very efficient as an approach for curve matching. However, when there are lots of features detected in the two curves, the possible combinations of matching among these features are prohibitive computationally. In this paper, we propose a heuristic algorithm to select the most promising coupling schemes out of these possible patterns. For parallel curve detection, the rationales we employed include proximity measure and compatibility between tangent orientations at these feature points.",
Algorithms for categorizing multiprocessor communication under invalidate and update-based coherence protocols,"Presents simulation algorithms that characterize the main sources of communication generated by parallel applications under both invalidate and update-based cache coherence protocols. The algorithms provide insight into the reference and sharing patterns of parallel programs and into the amount of useless traffic entailed by each coherence protocol. Under an invalidate-based protocol, our algorithms classify the data traffic caused by the different types of cache misses. Under an update-based protocol, our algorithms not only categorize the data traffic, but also classify update transactions with respect to the sharing patterns that caused them. Although our algorithms deal with numerous hardware features, our categorization is widely applicable and can be easily simplified for use in less detailed simulators.",
Reachability problems for cyclic protocols,"We study three reachability problems for cyclic protocols: (1) global state reachability; (2) abstract state reachability; and (3) execution cycle reachability. By combining fair progress and maximal progress during state exploration, we prove that these three problems an all decidable for Q, the class of cyclic protocols with finite fair reachable state spaces. In the course of the investigation, we also show that detection of k-indefiniteness and k-livelock an decidable for Q.",
Technological penetration and cumulative benefits in SMEs,The relative importance of the benefits derived from the adoption of computer-based administrative and production applications depends to a large extent on the level of technological penetration attained by a particular firm. This evolutionary perspective is investigated in an empirical study carried out in manufacturing firms operating in one specific sector of industrial activity.,
The formulation stiffness of forward dynamics algorithms and implications for robot simulation,"The authors identify an important phenomenon they call ""formulation stiffness"" in the numerical simulation of tree-structured multibody systems such as robot manipulators. The numerical simulation problem is usually treated as two separate problems: (i) the forward dynamics problem for computing system accelerations, and (ii) the numerical integration problem far advancing the state in time. The authors show that the interaction of these two problems leads to new conclusions about the overall efficiency of multibody simulation algorithms; in particular the fastest forward dynamics methods are not necessarily best when considered in conjunction with the popular adaptive stepsize integration methods. Specifically, the authors show that the articulated-body method is better suited to deal with certain types of numerical problems than the composite rigid body method. The authors present examples of simulations and discuss the practical implications of these results.",
Requirements monitoring in distributed environments,"We propose requirements monitoring to aid in the maintenance of systems that reside in dynamic, distributed environments. By requirements monitoring we mean the insertion of code into a running system to gather information from which it can be determined whether, and to what degree, that running system is meeting its requirements. Monitoring is a commonly applied technique in support of performance tuning, but the focus therein is primarily on computational performance requirements in short runs of systems. We wish to address systems that operate in a long-lived, ongoing fashion in non-scientific, enterprise applications. We argue that the results of requirements monitoring can be of benefit to the designers, maintainers and users of a system-alerting them when the system is being used in an environment for which it was not designed, and giving them the information they need to direct their redesign of the system. Studies of two commercial systems are used to illustrate and justify our claims.",
Information and process modelling approaches for the engineering of heterogeneous systems,"This paper intends to contribute to efforts for standardising ECBS process and information models and ECBS architectures by exploring these ECBS constituents with respect to the design and development of heterogeneous systems like micro-systems. These highly-integrated systems are usually embedded into larger systems like cars and consist of many different mechanical or electrical elements. This diversity of system parts requires special technology in order to combine the different engineering disciplines involved during the design of such systems. Thus, a process model including the development of all system pacts and an information model able to keep descriptions of the different parts in a uniform way are necessary. They are developed in this paper. In addition to these modelling approaches the paper shows how the system architecture influences the design decisions and which tool support is required.",
High performance transaction systems on the SB-PRAM,"The SB-PRAM is a shared memory parallel machine under construction in Saarbrucken. With the help of simulations we have evaluated the performance of transaction systems on this machine. We use the well known, DEBIT/CREDIT benchmark as workload. According to the simulations the machine reaches 6000 transactions per second with, 256 data disks, 32 log disks and 128 processors, although each processor has only 8 MIPS. With a partial run time analysis we support this surprisingly high transaction rate.",
Applying machine learning to subject classification and subject description for information retrieval,"This paper describes an experiment in applying a standard supervised machine learning algorithm (C4.5) to the problem of developing subject classification rules for documents. This algorithm is found to produce surprisingly concise models of document classifications. While the models are highly accurate on the training sets, evaluation over test sets or through cross-validation shows a significant decrease in classification accuracy. Given the difficult nature of the experimental task, however, the results of this investigation are promising and merit further study. An additional algorithm, 1R, is shown to be highly effective in generating lists of candidate terms for subject descriptions.",
The frequency-agile radar: A multifunctional approach to remote sensing of the ionosphere,"We introduce a new kind of diagnostic sensor that combines multifunctional measurement capabilities for ionospheric research. Multifunctionality is realized through agility in frequency selection over an extended band (1.5 to 50 MHz), system modularity, complete system control by software written in C, and a user-friendly computer interface. This sensor, which we call the frequency-agile radar (FAR), incorporates dual radar channels and an arbitrary waveform synthesizer that allows creative design of sophisticated waveforms as a means of increasing its sensitivity to weak signals while minimizing loss in radar resolution. The sensitivity of the FAR is determined by two sets of power amplifier modules: four 4-kW solid-state broadband amplifiers, and four 30-kW vacuum tube amplifiers. FAR control is by an AT-bus personal computer with on-line processing by a programmable array processor. The FAR does not simply house the separate functions of most radio sensors in use today, it provides convenient and flexible access to those functions as elements to be used in any combination. Some of the first new results obtained with the FAR during recent field campaigns are presented to illustrate its versatility. These include (1) the first detection of anomalous high-frequency (HF) reflections from a barium ion cloud, (2) the first evidence of unexpectedly large drifts and a shear north of the equatorial electrojet, (3) the first HF radar signature of a developing equatorial plasma bubble, and (4) the first measurements by a portable radar of altitude-extended, quasi-periodic backscatter from midlatitude sporadic E. We also mention the potential of the FAR for atmospheric remote sensing.",
Neural nets for decoding error-correcting codes,"Error-correcting codes are used in a variety of areas from computers to communications. Ideally, one simply looks at a received message which may contain errors, and decodes it into the error-free message. Unfortunately, this decoding process can be quite complicated and might not exploit the maximum error correction capabilities of the code. While for the simple Hamming code the decoding is direct, for more complicated codes decoding may be an NP-complete problem. Further the optimal decoding depends on the statistical properties of the error source, which might be very complex. For these reasons we are proposing the use of neural networks as decoders. In this paper, we report some experimental results which show that neural networks can be used to practically solve the general decoding problem mentioned above",
Recursive morphological sieve method for searching pictorial point symbols on maps,"Point symbols on a map represent interesting and significant positional data, such as airports, ski areas and natural parks. Identification of point symbols and finding their positions are important for automatic map understanding. Point symbols on maps may partially overlap on each other. More often they are touched or connected by line symbols, and their background consists of colored and/or textured regions. In this situation, a straightforward morphological template matching method is not feasible. A new partial ordering method for aligning pictorial point symbols which is based on the relationship of K-tolerance covering and a new general method for searching pictorial point symbols on maps, the recursive morphological sieve method, are described. The main ideas of this new method include (1) finding the coverings of the kernels of a symbol instead of the symbol itself, (2) morphologically preprocessing maps to form multi-level sieves, (3) using the sieves to screen out symbol kernels, and (4) renewing sieves during a recursive screening process. This method may also be used to recognize pictorial point symbols on other types of documental files.",
An open visual model for object-oriented operating systems,"Flexibility and user-customizability are amongst the potential benefits of reorganizing and restructuring operating systems using object-orientation. However, operating systems are often designed as black boxes whose internals cannot easily be examined or tailored by the user or application. As a black box, the complexity of an operating system appears to grow as additional features and options are added, We present an ""open"" visual model for object-oriented operating systems that supports browsing, manipulation, and programming. The model exploits the reification of all operating system mechanisms and policies as classified objects to provide interactive visualization, evaluation, and configuration of specialized services and features. The model supports exploration of the dynamic interaction of subsystems within the operating system including comparisons between the abstract architectural properties of the system like its design patterns and specific specializations and customizations of the implementation. This paper describes various visualization experiments and actual experiences of using an implementation of our approach in the Choices object-oriented operating system.",
Improving self-timed pipeline ring performance through the addition of buffer loops,"While self-timed pipelines have overcome the problem of clock skew which degrades the performance of synchronous pipelines, the improvement is not without its cost; self-timed pipelines are forced to spend a larger amount of time on communication than their synchronous counterparts. This has led researchers to search for ways to improve the performance of self-timed pipelines by changing the communication scheme in an effort to reduce the communication delay. Our approach divides the total communication time into two parts: data communication delay and pace handshaking overhead. By adding buffer loops to each stage of a self-timed pipeline, we can reduce the pace handshaking overhead; thus decreasing the total time spent on communication. One important result of this design innovation has been the simplification of analysis needed to find the best initial system configuration. With our design, the same initial system configuration may be chosen regardless of computation time and variations in computation time. In addition, our design has a lower average cycle time than the traditional self-timed pipeline, which leads to an increase in performance.",
The Emperor Has No Clothes: What HPC Users Need to Say and HPC Vendors Need to Hear,"A decade ago, high-performance computing (HPC) was about to ""come of age"" and we were convinced it would have significant impact throughout the computing industry. Instead, the HPC community has remained small and elitist. The rate at which technical applications have been ported to parallel and distributed platforms is distressingly slow, given that the availability of key applications is precisely the mechanism needed to drive the growth of the community. When major software vendors state publicly that their products will never be parallelized - as some have in recent months - it's time for us to take a hard look at reality. Marketing and PR claims to the contrary, HPC is not a success story. Although our capabilities continue to expand, we have not found a way to make HPC improve our productivity.",
A computerised Braille transcriptor for the visually handicapped,"Presents the computerisation of two existing Braille equipment useful for producing texts for the visually handicapped. When used with the Braille press this system can directly produce metallic plates from English texts entered into the computer by persons uninitiated in Braille. Automatic machine translation in both grade 1 and grade 2 has been achieved. Software techniques followed to effect the translations have been discussed. The prototype of the system is operational at the Ramakrishna Mission Blind Boys' Academy, Narendrapur, West Bengal.",
Adaptable mobile systems,"Integrated resource management for small, handheld mobile computers is addressed. The approach is based on the concept of quality of service (QoS). The focus is on providing for detection and adaption, not on transparency. The large and rapid changes in the services and resources available to mobile computers cannot be made transparent, neither to the user nor to system or application software. On the contrary, detection of changes and adaptation to new service and resource levels is necessary in all software layers in order to maximize the potential of the machine.",
Fuzzy partitions of the sample space and fuzzy parameter hypotheses,"In this paper, a generalization of parameter hypotheses tests used in statistical inference is presented. This leads to fuzzy hypotheses defined as imprecise predictions about the unknown parameter of a family of distribution functions in question, rather than exactly and sharply given assertions about its location in the parameter space. The notion of fuzzy partitions is discussed in the first part of this paper and details regarding their structure in relation to possibility theoretical interpretations are investigated subsequently. In the second part the authors introduce the notions of fuzzy tests and disclose the impacts of an ill defined counter hypothesis (i.e., hypotheses, partly supporting the same subset of the parameter space) on the reliability of such a fuzzy test. An example demonstrates the consistency of the notion of fuzzy tests with the classical crisp case in statistical inference.",
Contention resolution with bounded delay,"When distributed processes contend for a shared resource, we need a good distributed contention resolution protocol, e.g., for multiple-access channels (ALOHA, Ethernet), PRAM emulation, and optical routing. Under a stochastic model of request generation from n synchronous processes, Raghavan & Upfal (1995) have shown a protocol which is stable for a positive request rate; their main result is that for every resource request, its expected delay (time to get serviced) is O(log n). Assuming that the initial clock times of the processes are within a known bound of each other, we present a stable protocol, wherein the expected delay for each request is O(1). We derive this by showing an analogous result for can infinite number of processes, assuming that all processes agree on the time.",
Metrics and techniques for automatic partitioning and assignment of object-based concurrent programs,"The software crises is defined as the inability to meet the demands for new software systems, due to the slow rate at which systems can be developed. To address the crisis, object-based design and implementation techniques and domain models have been developed. However, object-based techniques do not address an additional problem that plagues systems engineers-the effective utilization of distributed and parallel hardware platforms. This problem is partly addressed by program partitioning languages that allow engineers to specify how software components should be partitioned and assigned to the nodes of concurrent computers. However, very little has been done to automate the tasks of partitioning and assignment at the task and object level of granularity. Thus, this paper describes automated techniques for distributed/parallel configuration of object-based applications, and demonstrates the technique on Ada programs. The granularity of partitioning is at the level of the Ada program unit (a program unit is an object, a class, a task, a package (possibly a generic template) or a subprogram). The partitioning is performed by constructing a call-rendezvous graph (CRG) for an application program. The nodes of the graph represent the program units, and the edges denote call and task interaction/rendezvous relationships. The CRG is augmented with edge weights depicting inter-program-unit communication relationships and concurrency relationships, resulting in a weighted CRG (WCRC). The partitioning algorithm repeatedly ""cuts"" edges of the WCRG with the goal of producing a set of partitions among which (1) there is a small amount of communication and (2) there is a large degree of potential for concurrent execution. Following the partitioning of the WCRG into tightly coupled clusters, a random neural network is employed to assign clusters to physical processors.",
Experiences about ERASMUS: an interchange project,"Describes what the ERASMUS project is, our experiences of it, and a brief statistical survey related to our participation as a host and home institution. The ERASMUS program started in June 1987 and it aims at supporting, promoting and stimulating cooperation among European universities. The universities participating in ERASMUS are financed to exchange teaching staff, exchange students and develop joint curricula. This program awards grants to students who wish to fulfil part of the requirements for obtaining their degree in a university other than that which they are currently attending. European universities are encouraged to form groups called ICPs (Inter-university Cooperation Programmes) in order to facilitate the coordination and organization job. Each institution is member of a group in which all the members are of the same kind (referred to the study orientation). In the University School of Computer Science of the Universidad Politecnica de Madrid, we consider that ERASMUS brings an excellent opportunity for students to get to know other cultures, work with different groups of people and learn how a ""professional"" job is done in another country. This is why we try to focus our participation on these three points and why we have chosen the final dissertation project as the subject that students must attend.",
Robust parallel resource management in shared memory multiprocessor systems,"Parallel machines are being increasingly used for applications that require both quick response time and high reliability. This poses a challenge in programming these systems since it must be ensured that there is sufficient redundancy to cope with failures and that, at the same time, redundant components are used effectively during failure free periods to enhance the performance. Among the issues, resource management in such systems is highly critical to the robustness and efficiency of the system. A good resource management algorithm should allow the system to continue its operation even in the presence of a significant number of processor failures. Also, the incorporation of fault tolerance should not incur too much overhead. In this paper, we develop two robust resource management algorithms which simultaneously achieve the twin objectives of low overhead and high reliability.",
Real-time networking over HIPPI,"HIPPI provides a very-high-speed communication medium, which is very well suited for a large number of bandwidth-demanding distributed applications. Unfortunately, its circuit-switched nature makes it very difficult to provide real-time guarantees when connections contend for network resources. We present a time-division-multiplex access scheme designed to give timing guarantees to high-speed connections. We describe the problem of scheduling the access to a HIPPI network, and show that, although the problem is very unlikely to be computationally tractable, very simple heuristics give high network utilizations for moderately-sized networks. We present the RMP/RMCP protocol, our implementation of the scheme described in this paper on the XUNET-West HIPPI testbed.",
Weighted selection on coarse-grain hypercubes,"Given n weighted records distributed evenly among a p-processor hypercube, p/spl les/n, we present efficient parallel algorithms for solving the weighted selection and related problems in the coarse-grain weak-hypercube model. A special case of the weighted selection problem, in which all the weights are equal, is known as the (unweighted) selection or order statistics problem. Our algorithms seek to minimize separately the time complexity for local computation and that for global communication on coarse-grain hypercubes. Depending on different ratios of n/p, we present techniques that lead to efficient hypercube algorithms for separate relative ranges of n and p. Our weighted selection algorithms match the local computation time lower bound of the selection problems on hypercubes for almost all the ratios of n/p. More importantly, the communication time bounds of our algorithm are better even than those of the previously best known hypercube solutions for the unweighted case in the corresponding relative ranges of n and p. Our algorithms are based on practical hypercube subroutines and make use of a variety of new schemes.",
B(its) T(o) The U(ser): A Communication Benchmark,,
Ontario Hydro experience in the identification and mitigation of potential failures in safety critical software systems,"Ontario Hydro has had experience in designing and qualifying safety critical software used in the reactor shutdown systems of its nuclear generating stations. During software design, an analysis of system level hazards and potential hardware failure effects provide input to determining what safeguards will be needed. One form of safeguard, called software self checks, continually monitor the health of the computer on line. The design of self checks usually is a trade off between the amount of computing resources required, the software complexity, and the level of safeguarding provided. As part of the software verification activity, a software hazards analysis is performed, which identifies any failure modes that could lead to the software causing an unsafe state, and which recommends changes to mitigate that potential. These recommendations may involve a re-structuring of the software to be more resistant to failure, or the introduction of other safeguarding measures. This paper discusses how Ontario Hydro has implemented these aspects of software design and verification into safety critical software used in reactor shutdown systems.",
Registration of anatomic landmarks during respiration using ultraviolet and structured lighting,A novel registration technique is presented that combines ultraviolet and structured light imaging to register anatomic landmarks. The use of these contrasting modalities overcomes the problem of markers interfering with the structured light image. The notion of hierarchical registration is also introduced. This involves multiple levels of registration resolution. The effectiveness of these techniques is demonstrated in a study of fiducial movement during respiration. Applications to breast volume measurement and burn injury assessment are considered.,
A numerical journey to the Earth's interior,"Slow convection in the mantle of the Earth profoundly affects the crust. Geophysicists use finite-difference, finite-element, and spectral methods to model this highly viscous, complex fluid flow. The author discusses the Earth's structure and composition, and how they relate to mantle convection.",
Generalized quantifiers and 0-1 laws,"We study 0-1 laws for extensions of first-order logic by Lindstrom quantifiers. We state sufficient conditions on a quantifier Q expressing a graph property, for the logic FO[Q]-the extension of first-order logic by means of the quantifier Q-to have a 0-1 law. We use these conditions to show, in particular, that FO[Rig], where Rig is the quantifier expressing rigidity, has a 0-1 law. We also show that FO[Ham], where Ham is the quantifier expressing Hamiltonicity, does not have a 0-1 law. Blass and Harary pose the question whether there is a logic which is powerful enough to express Hamiltonicity or rigidity and which has a 0-1 law. It is a consequence of our results that there is no such regular logic (in the sense of abstract model theory) in the case of Hamiltonicity, but there is one in the case of rigidity. We also consider sequences of vectorized quantifiers, and show that the extensions of first-order logic obtained by adding such sequences generated by quantifiers that are closed under substructures have 0-1 laws.",
Investigation of IF-THEN rule bases by methods of mathematical logic,"The paper deals with the calculus of IF-THEN rules originated by L.A. Zadeh. Using the concept of model and semantic entailment borrowed from mathematical logic, we make more precise the questions formulated by Zadeh in his fundamental papers.",
Using the Hopfield model with mean field annealing to solve the routing problem in packet-switched communication networks,The performance of the Hopfield neural network with mean field annealing for finding optimal or near-optimal solutions to the routing problem in communication network is investigated. The proposed neural network uses mean field annealing to eliminate the constraint terms in the energy function. Unlike other systems which use penalty constraint terms there is no need to tune constraint parameters and the neural network should avoid the problems of scaling. It also avoids the need to pre-determine the minimum number of hops corresponding to the optimal route. We have obtained very encouraging simulation results for the nine node grid network and fourteen node NFSNET-backbone network.,
"The navigation system for an expendable fiber cable ROV ""UROV""","The Japan Marine Science and Technology Center (JAMSTEC) have developed a free-swimming expendable fiber cable ROV ""UROV"" for under sea investigation that was built in co-operation between Fukui Prefectural Fisheries Experimental Station and JAMSTEC in 1992. Sea trials proved that it is able to swim over 1000 meters near the seafloor during two hours operation within the battery limits. Normally, the navigation for UROV is done manually by operators, using its position from navigation computer based on not only an acoustic underwater positioning, but also a GPS receiver and a compass on the board. In the navigation computer an auto-pilot mode has been installed to control UROV automatically along a preset route instead of the joystick, if the Radio Technical Commission for Maritime (RTCM) 104 message from a reference station placed at a known location is available (Differential GPS mode). This paper describes the navigation system for UROV.",
Utilization of hierarchical structure stochastic automata for the back propagation method with momentum,"Backpropagation (BP) is one of the most popular learning algorithms for multilayer networks, but it has limitations. Various modified BP methods have therefore been proposed. The BP method with momentum may be one of the most popular such modified algorithms. It has been reported that the BP method with momentum has been applied quite successfully to many practical problems. However, despite its effectiveness, this method involves the following serious problem: ""Its learning performance depends heavily upon the selection of the value of momentum factor."" Unfortunately, it seems that there has not so far been proposed an intelligent algorithm for determining an appropriate value of the momentum factor. In this paper, we suggest that hierarchical structure stochastic automata are quite helpful for finding an appropriate value of the momentum factor of the BP method with momentum. Several computer simulation results confirm our idea.",
Operating systems and communication protocols,"Modern operating system designs should not only offer suitable system call interfaces to the end-user application programs, but should also be able to accommodate special purpose sub-systems, such as networking software, executing in a privileged/kernel address space. Decisions involved in networking software design and implementation require a careful analysis of the features offered by the target operating environment including available communications hardware and, in particular, the host's operating system. This paper discusses two important issues in networking software design and implementation. The first one relates to the treatment of inter-layer communication (ILC) among multiple protocol entities. If is suggested that the ILC model offers flexibility of design and reuseability if treated as a form of inter-process communication. The second issue is that of the individual layer configurability. Operating system and networking software sub-system startup procedures are inter-dependent and require a new, parametric-driven approach to configuration and generation of executable system files.",
Towards the empirical design of massively parallel arrays for spatially mapped applications,"Although SIMD arrays have been built since the 1960's, they have undergone few empirical studies. The underlying problems-which have included the lack of a unified architectural framework and the computational intractability of simulating large PE arrays-are addressed through the use of trace compilation, a novel approach to trace driven simulation. The results indicate the benefits of adding another level to current SIMD array memory designs. Also, surprising results were obtained about performance effects of varying cache associativity and block size. Together, they indicate that while SIMD array programs have sufficient locality to make PE caches worthwhile, the type of locality may differ fundamentally from that of serial machine and multiprocessor programs. Other results demonstrate the limitations of increasing the datapath width and inter PE communication bandwidth without corresponding improvements in other processor features.",
Those 1-credit special project courses: motivating your best freshmen,"Every school has them, in some form or another. The variable-title, variable-credit course is ubiquitous; but, what do we use them for? Often they become a vehicle for teaching material not yet having a permanent course number. In some cases, professors use them for obtaining inexpensive low-level help for their research, thus providing an undergraduate research opportunity. They are sometimes used to provide a student with an opportunity To overcome a lack of preparation in some area vital to his/her area of study. At Purdue University, the Freshman Engineering Honors Program uses ENGR 195 as a motivational tool. When time and current interests permit the Honors Director offers students the opportunity to participate in special projects that offer experience outside the range of regular coursework available to the student. These projects often enhance the students' resumes as well. During the spring semester of 1994-95, eight students undertook a software design and development project based on an assignment from the Honors Computer Programming class they took the prior semester. These students operated as a software development team, with two groups of four students each taking on a different part of the project. They were supervised by a senior computer engineering student and the Honors Program Director. The project was development of basketball statistics software, and the students were enthused from the start. They were especially excited that they were to complete the development of the software to the point that it could be made available for distribution as a shareware or freeware product, including user manuals and program documentation. The development platform was a realistic (for this problem) 486-based notebook computer. What is needed to provide this type of experience to more students? First, there must exist a problem to be solved, preferably of interest to several students. Second, the means to solve the problem, in terms of hardware, software, and sufficient expertise to begin solving it must be near at hand. Third, there must be an audience, that is, a group of students from which to draw those looking for an additional challenge. Finally, there must be an interested faculty member to serve the students as teacher/mentor in doing the project. Motivation is virtually assured.",
Methods of removing single variable static hazards in Boolean functions,"Two methods are presented in this paper for removing single variable static hazards in Boolean Functions. The first method is a hand calculation method that uses a single variable static hazard cover algorithm. The hand calculation method can be used in the classroom and does not require computer assistance. The second method involves using a single variable static hazard cover software synthesis tool. The second method is more convenient to use outside the classroom on a PC that is available for engineering student use or perhaps on personal computers. To illustrate the process of obtaining single variable static hazard covers, the standard Karnaugh map approach is also used. A commercially available design synthesis tool and schematic capture/simulation timing tool are also used in this paper.",
Software test data generation using the chaining approach,"Software testing, specifically, test data generation is very labor-intensive and expensive. As a result, it accounts for a significant portion of software system development cost. In this paper we present a chaining approach for automated software test data generation. The chaining approach uses data dependence analysis to guide the test data generation process. The experiments have shown that the chaining approach may significantly improve the chances of finding test data as compared to the existing methods of automated test data generation.",
Information Models of VHDL,"The paper discusses issues related to the application of information modelling to the field of Electronic CAD, using VHDL as the basis for discussion. It is shown that an information model of VHDL provides a coherent and uniform description of the VHDL objects at different levels of the language and of the transformations that interrelate these levels. In addition, it captures the time-dependent aspects of the language. Hence, a hierarchy of VHDL information models can exist which encompasses the range from abstraction to detail and can help support CAD applications in a direct manner.",
Performance-Driven Partitioning Using a Replication Graph Approach,"An efficient algorithm is proposed to tackle the performance-driven partitioning problem using retiming and replication. We devise a replication graph to model the composite effect of replication and retiming. With the replication graph, we formulate the problem as an integer linear programming problem. A heuristic algorithm is derived to solve the problem by exploring the dual program of its linear programming relaxation.",
Scheduling replicated critical tasks in faulty networks using evolutionary strategies,Scheduling tasks in distributed systems is a difficult problem. Finding good schedules becomes even more complex when the network is faulty and there are no additional resources available. This paper presents a technique using evolutionary strategies to find task schedules in such systems. Our results indicate that good schedules can be found even when critical tasks must be replicated on distinct processors,
"Distance learning for non-traditional students to study, near home, toward a UNC Charlotte BSET degree","The author considers the problem of whether UNC Charlotte could offer Engineering Technology courses to their Applied Science graduates, who could not afford to come to UNC Charlotte for the full two year Bachelor of Engineering Technology (BSET) degree program. The solution was to offer UNC Charlotte Engineering Technology courses over the North Carolina Information Highway (NCIH). The need for this 'virtual' classroom service to be provided to non-traditional students within North Carolina would provide a win-win situation for the students, UNC Charlotte, and the state of North Carolina. Two courses, one electrical engineering technology and one calculus, are to be offered via distance learning. Multimedia and expert system/knowledge based techniques will be applied where appropriate. The course delivery paradigm will improve over time with: (1) the development of new software to provide immediate display of student answers to course questions using inputs from wireless student response units, (2) inclusion of expert system/knowledge based programs to improve the course quality, (3) operational experience incorporating input from both the students perspective, and the logistics for course presentation, and (4) expansion of the North Carolina Information Highway (NCIH) and the evolution of the National Information Infrastructure.",
An object-oriented design for user interfaces,"The Object-Oriented (OO) design architecture for user interfaces presented in this paper, called IOWARE, is effective, does not violate OO principles, and promotes a high degree of reusability, extendibility, and portability. The innovative features of the design architecture are: [1] the decomposition of interactive applications into atomic and container interactive objects, [2] the use of dynamically manipulable resources to represent all interaction behavior, [3] a generic mechanism that facilitates communication between objects, and [4] a simple and elegant mechanism for composing complex interactive objects. A prototype implementation built on top of Smalltalk is presented.",
A working device model of mercuric iodide X-ray detectors for XRF applications,"Use of mercuric iodide detectors in commercial field portable X-ray fluorescence instruments (XRF) presents many challenges because of the stringent requirements. It is well known that the performance of mercuric iodide for radiation detection is highly variable among detectors. In this study, a consistent device model of mercuric iodide X-ray detectors is developed to interpret a wide range of behaviors observed in fabricating, characterizing, and application of mercuric iodide X-ray detectors. The model is based on the free carrier transport, carrier trapping, electric field distribution, surface effects, dead layers, interaction of radiation with mercuric iodide, and electronic noise. Results from computer simulation based on the model compare well with experimental data. The better understanding of mercuric iodide detectors gained through this device model will help the optimization of detector performance and manufacturing yield.",
An approximate model for the output process of an ATM multiplexer with selective discard mechanism,"We introduce a procedure for modelling the output process of a finite buffer discrete-time queue with selective discard mechanism loaded with a discrete-time batch Markovian arrival process (D-BMAP/sup [H,L]//D/1/K). The procedure matches the output process long-term index of dispersion for counts and correlation of the number of arrivals in consecutive slots with the same statistics of a prioritized two-state Markov modulated Bernoulli process (MMBP{H,L]}. We show through numerical examples that this procedure is accurate. We also introduce a framework for the analysis of queueing networks with prioritized Markov modulated process.",
Minimum coloring random and semi-random graphs in polynomial expected time,"We present new algorithms for k-coloring and minimum (/spl chi/(G)-) coloring random and semi-random k-colorable graphs in polynomial expected time. The random graphs are drawn from the G(n,p,k) model and the semi-random graphs are drawn from the G/sub SB/(n,p,k) model. In both models, an adversary initially splits the n vertices into k color classes, each of size /spl Theta/(n). Then the edges between vertices in different color classes are chosen one by one, according to some probability distribution. The model G/sub SB/(n,p,k) was introduced by A. Blum (1991) and with respect to randomness, it lies between the random model G(n,p,k) where all edges are chosen with equal probability and the worst-case model.",
Multidatabase global query optimization,"Data requests to multidatabases are posed through non-procedural languages such as SQL and, for a global data request, optimization must be performed to achieve good system performance. However, the issues are often complicated in multidatabases, due to additional issues arising because of the autonomy and heterogeneity restrictions of the independent local DBMSs. The data translation problem between various local systems can be observed at the schema level and at the instance level. We have identified the need for a domain translation table in multidatabase query processing, and we discuss methods of implementing it. Some observations about multidatabase inter-site joins are made, and four methods by which multidatabase global queries can be processed are introduced and analytically compared.",
Modeling techniques for curved and tapered branches at different microwave frequencies,"The effects of modeling the curvature and tapering of branches for various microwave frequencies are examined. The forest models that are currently being used represent branches as perfect dielectric cylinders. However actual branches are composed of segments decreasing in diameter along the branch length. Adjacent branch segments also have slightly different angles causing branch curvature. In this study, each branch is represented as a collection of cylindrical segments whose sizes and positions are known. The data for the calculations are obtained from actual branch measurements of red pine and jack pine trees using the ""tree vectorization"" technique which has been developed at the Canada Centre for Remote Sensing. For a given frequency and incidence angle, a curved and tapered branch can be approximated by an equivalent cylindrical branch. The equivalent cylinder can be determined by choosing the size and orientation so that the radiation patterns match closely. For curved branches, the radiation pattern at typical incident angles is calculated by adding the contributions from each branch segment coherently. It is observed that at low frequencies, the parts of the branch with thinner segments do not contribute to scattering. Therefore, the resulting equivalent cylinder is represented by the thicker part of the branch which is closer to the trunk.",
Formal specification of managed objects in an object-oriented network management system using Object-Z,"The specification of managed objects is crucial for the successful design of network management systems. However, a weakness in many current object-oriented methodologies is the absence of formal description of the behavioural aspects of such objects. It is shown that Object-Z can be used to provide readable, precise and extensible specifications which furthermore can be derived quite easily from standard OSI GDMO object templates.",
Faster approximate agreement with multi-writer registers,"We consider the complexity of the wait-free approximate agreement problem in an asynchronous shared memory comprised of only single-bit multi-writer multi-reader registers. For real-valued inputs x/sub 1/,...,x/sub n/ and /spl epsiv/ we show matching upper and lower bounds of /spl Theta/(log(ma.",
An improved approach to fault tolerant rank order filtering on a SIMD mesh processor,"This paper presents an approach for the fault tolerant computation of the rank order filtering on a SIMD (Single Instruction Multiple Data) mesh processes such as the MasPar. The proposed approach improves over a previous approach in two respects: by changing the data dependency in the execution of the rank order filtering, a new algorithm with constant execution time complexity can be designed; and by introducing a dependency for the rank values of faulty PEs as computed by neighboring (fault free) processing elements (PEs), a lower distortion can be achieved for enhancement of the image.",
On the mapping of linear classification trees onto one-hidden-layer neural nets,In this paper we show that the convex regions induced by a decision tree with linear decision function cannot be represented by linear membership functions as suggested in the literature. It appears that a faithful representation is only possible for subregions. We derive explicit expressions for the membership functions of these subregions. This approximation can be used to initialise a one-hidden-layer neural net.,
Scheduling interval orders in parallel,"Interval orders are partial orders defined by having interval representations. It is well known that a transitively oriented digraph G is an interval order iff its (undirected) complement G~ is chordal. We investigate parallel algorithms for the following scheduling problem: given a system consisting of a set /spl Tscr/ of n tasks (each requiring unit execution time) and an interval order < over /spl Tscr/, and given m identical parallel processors, construct an optimal (i.e. minimal length) schedule for (/spl Tscr/,<). Our algorithm is based on a subroutine for computing so-called scheduling distances, i.e. the minimal number of time steps needed to schedule all those tasks succeeding some given task t and preceding some other task t'. For a given interval order with n tasks, these scheduling distances can be computed using n/sup 3/ processors and O(log/sup 2/n) time on a CREW-PRAM. We then give an incremental version of the scheduling distance algorithm, which can be used to compute the empty slots in an optimal schedule. From these, we derive the optimal schedule, using no more resources than for the initial scheduling distance computation and considerably improving on previous work by Sunder and He (1993). The algorithm can also be extended to handle task systems which, in addition to interval order precedence constraints, have individual deadlines and/or release times for the tasks. Our algorithm is the first NC-algorithm for this problem. As another application. It also provides NC-algorithms for some graph problems on interval graphs (which are NP-complete in general).",
Algorithm evolution for face recognition: what makes a picture difficult,"One of the classic problems in computer vision is the face recognition problem. In general, this problem can take on a wide variety of forms, but the most common face recognition problem is ""Who is this a picture of?"" Evolutionary computation has, in the past, been applied indirectly to this problem through techniques like learning neural networks. This paper introduces a genetic programming style approach to learning algorithms that directly investigate face images and are coordinated into a face recognition system. Through a series of experiments, we show that evolved algorithms can accomplish the face recognition task. We also highlight several pitfalls and misconceptions surrounding face recognition as a learning problem.",
On deriving data parallel code from a functional program,"We discuss a translation methodology for transforming a high level algorithmic specification written in ALPHA to an imperative data parallel language. We informally introduce the ALPHA language with the aid of an example and explain how it is adapted for doing static analysis and transformation. An ALPHA program can be naively compiled (P. Quinton et al., 1985) using applicative caching. Our compilation method makes incremental transformations on the abstract syntax tree of can ALPHA program in order to make efficiency and performance improvements over the naive code and optimize it for a given architecture. The compilation steps described include scheduling, alignment, partitioning, allocation, loop nest generation, and code generation and they are illustrated with an example.",
On relating local and global factors: a case study from the game of Go,"Traditional artificial intelligence (AI) approaches to programming the game of Go are based on the translation of local information (modelled by pattern recognition processes) to global symbolic form (modelled by rule-based systems) for access by symbolic reasoning processes. We explore the converse process, seeking to relate local and global factors by integrating global factors into a local representation which can then be accessed by symbolic reasoning processes. We demonstrate one method for such an integration, the interaction of bottom-up and top-down processing. The algorithm we use in our simulation integrates global factors by directly modifying the numeric information contained in the local representation of the board. We use Go as an example of a domain with the characteristic that local and global factors cannot be identified independently of each other. Thus, to form a representation of a Go board requires an interaction between bottom-up processing (to identify local factors) and top-down processing (to identify global factors). In the final section we briefly relate these constraints to other domains.",
Simulating solvent effects in organic chemistry: combining quantum and molecular mechanics,"As computational methods improve, the need for accuracy must still be tempered with practicality. When calculating how molecules interact in solution, treating solute molecules quantum mechanically and the surrounding solvent molecules classically combines accuracy with computational efficiency. Computational methods for modeling solute-solvent interactions generally fall into two categories. A macroscopic approach treats the solvent as a continuous (unstructured) medium characterized by a bulk dielectric constant. Macroscopic methods, while generally faster because solvent molecules are not explicitly represented, are unable to provide specific details of the solute-solvent interaction. The microscopic method, which is the subject of this article, treats the solvent in its discrete molecular form. Most molecular dynamics or Monte Carlo simulations use molecular-mechanical (MM) potentials for both solute and solvent. However, hybrid quantum-mechanical and molecular-mechanical potential functions have recently emerged. We describe this combined approach for simulating chemical reactions in solution.",
Some practical considerations in implementing a pulse-control fuzzy controller,"A new type of fuzzy controller based on a pulse-control technique was proposed previously by the authors. Unlike the more conventional type of fuzzy controller in which the control and measurement actions are essentially carried out concurrently, this controller separates the control phase from the measurement phase. The proposed fuzzy controller has been found to be able to deal with a range of unstable and unintuitive systems. Some practical issues in implementing such a fuzzy controller are described in this paper.",
Modeling for interactive presentation and navigation of time-dependent multimedia information,"For the time-dependent multimedia information systems, more and more powerful functionalities are being required to control the interactive presentation and browsing. However, it is hard to find a, suitable model for supporting all of the concepts such as synchronization, interaction and time-dependent linking of multimedia objects. In this paper, we present a model providing specification methods for the functionalities. We also verify the usefulness of this model by applying it to multimedia information systems.",
Design of a Video Storage System for Interactive Video-on-Demand,,
An auto-invertible neural network for image halftoning and restoration,"In this paper, we apply the so-called Q'tron NN (neural network) paradigm to perform image halftoning and the 'associated' image restoration. These processes are considered to be located at the different sides of the same process. On one side, the process converts a grey-tone image into a binary image, i.e., halftoning. On the other side, the process just performs the inverse, i.e., it restores a binary image to a grey-tone image. One will see that such an auto-associativity regarding the two tightly correlated images is one of the important features of a Q'tron NN. Experimental results are presented to demonstrate that the resulting quality of images is quite satisfactory.",
Capacity Bounds for Simple Recurrent Networks,,
Using speculative execution for fault tolerance in a real-time system,"Achieving fault-tolerance using a primary-backup approach involves overhead of recovery such as activating the backup and propagating execution states, which may affect the timeliness properties of real-time systems. We propose a semi-passive architecture for fault-tolerance and show that speculative execution can enhance overall performance and hence shorten the recovery time in the presence of failure. The compiler is used to detect speculative execution, to insert check-points and to construct the updated messages. Simulation results are reported to show the contribution of speculative execution under the proposed architecture.",
Performance evaluation of connectionless service for ATM networks,"This paper investigates the performance of the direct approach to connectionless service over ATM. The direct approach utilizes connectionless servers, attached to or incorporated in a subset of the ATM switches, to provide a packet switching service. The number of connectionless servers and ATM connections utilized by the direct approach is found analytically and compared to alternative approaches. Furthermore, the effects of two architectural options that significantly impact the performance of connectionless servers, the packet forwarding scheme (packet-based and cell-based) and the flow control scheme (static rate control and a dynamic proportional rate control) are also evaluated.",
Real-Time Control of a Tokamak Plasma Using Neural Networks,"In this paper we present results from the first use of neural networks for real-time control of the high-temperature plasma in a tokamak fusion experiment. The tokamak is currently the principal experimental device for research into the magnetic confinement approach to controlled fusion. In an effort to improve the energy confinement properties of the high-temperature plasma inside tokamaks, recent experiments have focused on the use of noncircular cross-sectional plasma shapes. However, the accurate generation of such plasmas represents a demanding problem involving simultaneous control of several parameters on a time scale as short as a few tens of microseconds. Application of neural networks to this problem requires fast hardware, for which we have developed a fully parallel custom implementation of a multilayer perceptron, based on a hybrid of digital and analogue techniques.",
Patterns of Functional Damage in Neural Network Models of Associative Memory,"Current understanding of the effects of damage on neural networks is rudimentary, even though such understanding could lead to important insights concerning neurological and psychiatric disorders. Motivated by this consideration, we present a simple analytical framework for estimating the functional damage resulting from focal structural lesions to a neural network model. The effects of focal lesions of varying area, shape, and number on the retrieval capacities of a spatially organized associative memory are quantified, leading to specific scaling laws that may be further examined experimentally. It is predicted that multiple focal lesions will impair performance more than a single lesion of the same size, that slit like lesions are more damaging than rounder lesions, and that the same fraction of damage (relative to the total network size) will result in significantly less performance decrease in larger networks. Our study is clinically motivated by the observation that in multi-infarct dementia, the size of metabolically impaired tissue correlates with the level of cognitive impairment more than the size of structural damage. Our results account for the detrimental effect of the number of infarcts rather than their overall size of structural damage, and for the ""multiplicative"" interaction between Alzheimer's disease and multi-infarct dementia.",
The relationship between IT spending and corporate revenue,"The large and continually increasing capital expenditures that firms continue to make in computers and communication systems, coupled with the growing dependence that corporations across virtually all industries have on their IT investment, suggests that executives know what returns they are getting from their spending. In reality, they do not. Moreover, there is a visible controversy in the field which suggests that the return on information technology spending may not be favorable. To gain greater insight into this issue, an analysis was performed to examine the three-year IT spending of a sample of Fortune 500/Service 500 firms. The results of the study show that there is a highly significant positive relation between IT spending and corporate revenue. These findings raise a series of questions meriting additional research.",
Minimal program covering based on the output variables,"This paper discusses how a program can be represented by a binary relation, R, and how to decompose the latter into a set of rectangular relations. Next, we present our methodology based on relational operators and dependence relations, to show how we can use these rectangles to obtain more interesting ones that describe the entire behavior of every variable in the program. The notion of lattice of maximal rectangles is effective in that it permits to have a particular representation of the program which shows all the different parts that constitute the original program. By looking at this lattice structure, we find that the set of the leaves of this lattice, which represent ""pertinent"" rectangles associated to output variables, gives a minimal program covering.",
"Genetic epidemiology, parallel algorithms, and workstation networks","Many interesting problems in genetic epidemiology are formulated as non-linear optimization problems using the Gemini/Almini library of routines. Because of the wide availability of networked workstations, we investigate cost-effectively improving the performance of the Gemini/Almini library by exploiting parallelism with a set of workstations connected via a local area network. Instrumentation of the Gemini/Almini optimization routines reveals significant potential for improving performance via parallelism. Using these instrumentation results, we identify promising targets of parallelism and discuss two preliminary implementations that demonstrate the potential benefits of cost-effective parallel implementations. By applying parallelism to the Almini/Gemini routines, we hope to potentially improve the performance of a large number of genetic epidemiological applications.",
Experience using type theory as a foundation for computer science,"Type theory is an elegant organisation of the fundamental principles of a foundational theory of computing, with theory taken in the sense of a scientific theory as well as a deductive theory. This theory generates a research programme. I examine the elements of this programme and assess progress. A large number of people world wide have been pursuing the type theory aspects of this research programme, so we can survey a large body of work created over a 20 year period for hints of success and failure and challenge. I first look at a few successes. Some of the applications we have attempted have not worked out as expected, and we don't know whether the fault lies with the type theory or elsewhere. I first describe a failure that is clearly not the type theory, but the state of the foundations of computational mathematics. Then we look at problems closer to the structure of modern type theories-problems suggested by the success of classical set theory.",
The Permpar project,"Numerical simulation is one of the areas where highly parallel computers will most likely be used extensively in the near future. However, most of the simulation software systems on the market today have been developed continuously over the past two decades and were designed with sequential computer architectures in mind. On the other hand, the investment in these codes is so large that complete redesign is not possible. The Commission of the European Communities has funded the Europort project with the goal of supporting the migration of existing and industrially relevant software to highly parallel computer architectures. Permpar is one of these projects. The two-year project, begun in January 1994, has adopted the general-purpose finite-element program Permas to meet its software requirements.","Concurrent computing,
Investments,
Computational modeling,
Computer simulation,
Computer architecture,
Software performance,
Data structures,
Processor scheduling,
Distributed decision making,
Computer industry"
Computer processing of radionuclide renogram and use of deconvolution method to calculate renal transit time,"A programme using a matrix algorithm for deconvolution analysis of the renogram has been developed at the All India Institute of Medical Sciences, New Delhi. The programme plots the retention function for each kidney and computes minimum transit time, mean transit time and transit time index for them. Any prolongation of the transit time of the radioactive tracer through the various regions of the kidney becomes an important and reliable diagnostic tool for upper urinary tract obstruction.",
An efficient data compression scheme based on semi-adaptive Huffman coding for moderately large Chinese text files,"This paper presents a data compression scheme for Chinese text files. Due to the skewness of the distribution of Chinese ideograms, the Huffman coding method is adopted. By storing the Huffman tree in the coding table and representing the Huffman tree using the Zaks sequence, the algorithm produces significant improvement on the compression results. The proposed method is evaluated by comparing its performance with three well-known compression algorithms and an algorithm specially designed to compress the coding table. This algorithm should also be applicable to other ideogram-based or oriental language texts. Also, it has the potential to reduce the dictionary size in a bigram or trigram-based semi-adaptive compression scheme for English texts.",
A generalized environmental database for San Diego Bay,"Understanding the fate and effect of materials introduced into marine ecosystems requires broad spatial and temporal perspectives. It may also require the aggregation of measurement data from different scientific disciplines. Because the collection of data on these scales is beyond the scope of most monitoring studies, it is often necessary to share measurements made by different investigators for different objectives. To do so requires the use of primary measurement data recorded in fully documented digital form.",
Enhancing B-ISDN signaling to meet the challenge of multimedia services,"A key B-ISDN functional principle is the support of a wide range of data, video and audio applications in the same network. Moreover, the network must be flexible and adaptable to meet the demands of current and future services. A set of emerging multimedia services is presented in context of the requirements they impose on B-ISDN signaling protocols. With these multimedia services in mind, we extend the existing B-ISDN access signaling protocol with a set of simplified messages, information elements and message flows. These enhancements support multiconnection and multiparty calls, and the Look Ahead and Renegotiation procedures planned for future ITU-T releases. The issues of protocol evolution and backward compatibility were carefully considered while developing the proposed enhancements.",
Automatic analysis of collimator structure for quality assurance,"A non-destructive, automatic and quantitative method was developed in order to characterize the structure of the gamma camera collimators because they remain the weakest part of the system. As the quality of a collimator depends on the regularity of the hole shape, the authors developed software implemented on a microcomputer using the macro programming language provided by the image processing program NIH Image 1.52. A radiograph is first obtained by means of a home made rectilinear scanner and digitized with a film scanner. These data are then analyzed. The hole dimension and septal thickness are determined by applying a matched mask on the digitized image made binary. The hole inclination is estimated by the shift of the center of gravity determined from the gray level image. For foil collimators the results show a big spread; e.g. S.D. are equal to 0.98 and 0.24 mm for 2.3 mm diameter and 0.15 mm septal thickness (manufacturers data) respectively. A cast collimator with the same characteristics has S.D. equal to 0.16 and 0.08 mm. In terms of hole inclination, a value of up to 1.3 degrees was found with foil technology. This method allows to detect automatically the manufacturing defaults of a collimator. Any geometry (parallel, fan beam) may be evaluated with minor changes in the technique.",
A freshman electrical engineering course and laboratory for all engineering majors,"Real and relevant are two adjectives that should apply to any engineering course, but the traditional first course in electrical engineering often appears to the students to provide little reality or relevance. This is particularly true for students in programs such as ours in which other engineering majors, including civil, mechanical, environmental, and computer science are in the course alongside electrical majors. At Northern Arizona University, we have addressed this issue by implementing major changes in the first course in the electrical engineering sequence. In this paper we describe the structure, content, equipment, computer tools, and teaching methods used in both the lecture and laboratory sections of the course.",
IEEE Computational Science and Engineering,,
Radiation balance investigations using METEOSAT data,"The exact knowledge of the components of the radiation balance is very important both in climatology and weather forecasting. The geostationary satellites give a new opportunity for monitoring the radiation balance components because of their excellent temporal resolution. In the present investigation, which started four years ago, the authors' purpose was to develop a relatively simple, fast method to estimate the radiation balance components for the Central European region from METEOSAT digital images. First a method for estimating the solar radiation reaching the surface was developed. Then computer programs for calculating the downward longwave radiation and the net radiation were constructed. In this paper an attempt is made to apply the model to an extended area calculating hourly average values.",
On the systematic design of systolic arrays,"This paper describes a general technique for the design of systolic arrays. Various attempts have been made in the past to design systolic arrays from computational algorithms. However, most of these methods have their limitations. In this paper, a technique based on dependence graphs is presented. Starting from the algorithm itself, steps are outlined which enable the designer to extract parallelism from the program constructs. The method further outlines steps to detect and incorporate pipelining into the yet unrealized systolic architecture. Next, dependence graphs corresponding to the data items, which are now arranged in a parallel and pipelined fashion, are drawn. This is followed by a procedure for 'systolizing' the dependence graphs. Finally, steps are outlined for mapping the systolic dependence graph onto a systolic architecture. The technique is illustrated by designing systolic arrays for computing position velocities and accelerations.",
Dynamics of spatially and temporally decaying arcs for lightning discharge initiation,"Summary form only given. In the field of laser initiated lightning discharge research for electric power network protection, some drastic breakthroughs are necessary to apply this technique commercially, other than the conventional approach only for chasing after the higher laser power. One of the keys seems to be in the experimental results found in the same field. The laser produced plasma in quasi steady high electric field is known to show a strange feature. It has the peak lightning-triggering efficiency at around 100 /spl mu/s after a sub-micro second laser irradiation. From the existing computer simulation, this time scale should be far longer than the recombination and attachment time scale for free electrons in air. In this sense, the dominant constituents for lightening initiation are not free electrons but excited gases in certain conditions. We had set up an experiment with a transversally accelerated arc. This uses a modified version of a rail-gun type system but in high pressure air. This produces a rapidly moving high current arc armature with 10 kA peak and 10 ms current decay time. This time scale is slow enough to be considered as a quasi-DC condition for this application. The time scale of experimentally obtained hot gas decay is about 100 us and the laser pulse duration is in the order of 10 ns. The paper focuses on the mechanism and efficiency of charged particle removal from this plasma with experimental evidence. These results are summarised with the detailed comparison with the simulation and unresolved questions shown.",
Manufacturing system management: decision support tool design through the cooperative object concept use,This paper deals with a computer aided management support tool developed on the basis of hierarchical production management models. Levels specific interactions and main behaviours are modelled through object oriented Petri nets (OOPNs) and computer science protocols such as communication and client/server protocols. Organisational scheme support consists of integrated levels hierarchy and is further extended to lateral co-ordination processes integrated in a pre-established hierarchy. Conception issues and further extensions concerning horizontal organisational schemes are also pointed out.,
IEEE Computational Science and Engineering,,
Index mapping approach of deriving the PM DFT algorithms,"The DFT is of fundamental importance in many areas of science and engineering due to its efficient evaluation of frequently used operations such as signal analysis, convolution, and correlation. It has been shown that radix-2 DFT (discrete Fourier transform) algorithms can be designed based on vector representation of data providing several advantages. These algorithms provide more efficient solution for the problem of DFT computation. An index mapping approach is used to derive these algorithms. This approach makes the derivation simpler and provides a better insight into the functioning of the algorithms. The signal flow graph of the algorithm is also presented.",
Robust Control Basis for Coordinating Multiple Manipulators,,
Robust on-line parameter identification with general knowledge on level of information noise: continuous and discrete cases,"A robust on-line parameter identification problem is posed and solved for systems with general knowledge of the level of the inherent information noise. Both continuous-time and discrete-time cases are considered in this paper. For the former case, the knowledge can be the bound on either the magnitude or the finite-time L/sup p/ norm, p/spl isin/[1, /spl infin/), of the noise. Whereas for the latter case, it can be the bound on either the magnitude or the finite-index l/sup p/ norm, p/spl isin/[1, /spl infin/), of the noise. Based on the knowledge, a switching type algorithm is proposed to estimate the parameters of the system from the available input-output data. In spite of the existence of the information noise, this on-line algorithm guarantees that the estimation error is monotonically decreasing, and the parameter estimate is convergent to a steady state value under a mild condition.",
The Markovian Language Network: steps toward a connectionist architecture for grammar induction,"A novel, modular connectionist architecture for grammar induction is proposed, the ""Markovian Language Network"" (MLN). Based on Zellig Harris's (1989) theory of syntax, MLN would seem to be a plausible biological and psychological model. Modules for extraction of Markov information and for categorization are interlinked and are used for multiple purposes. MLN has not yet been fully implemented, but simple experiments have been conducted with a hybrid system. MLN is seen as a first step toward a complete connectionist implementation of Harrisian linguistics.",
A Bayesian approach to variable screening for modeling the IC fabrication process,"We describe a technique for determining the set of input variables that are significant with respect to a set of fabrication process output variables. Our technique uses a Bayesian analysis approach to focus only on the ""active"" factors while ignoring those that are statistically insignificant. Macromodels are then built using those ""active"" factors. This approach results in reduced simulation models with high accuracy.",
Experimental investigation of high performance cognitive and interactive text filtering,"Text filtering has become increasingly important as the volume of networked information has exploded in recent years. This paper reviews recent progress in that field and reports on the development of a testbed for experimental investigation of cognitive and interactive text selection based on a history of user evaluations. An interactive filtering system model is presented and a new cognitive filtering technique which the authors call the Gaussian User Model is described. Because development of analytic measures of text selection effectiveness has proven intractable, the authors have modified the Cornell SMART text retrieval system to create a flexible text filtering testbed for experimental determination of filtering effectiveness. The paper concludes with a description of the design of this testbed system.",
Toward a pre-disciplinary introductory design sequence,"One of our greatest challenges in education is to prepare our students for the realities of the workplace they will be entering. In an economy characterized by global competition and continuous change, students need career-sustaining skills in addition to hard knowledge. They must know how to synthesize and integrate information; to work together in teams; to be creative; to attack open, multidisciplinary problems; to communicate the answers obtained; and to bring ideas into fruition. Recent critiques of engineering education, state the need a slightly different way. They claim that engineering education has focused too much of its energy on engineering science at the expense of engineering design. The author considers how Georgia Tech's EduTech Institute is spearheading an interdisciplinary effort to address these needs. Key to the effort is a two-quarter pre-disciplinary introduction to design. The introductory courses are designed to lay the foundations for teaming principles and strategies behind good design, reasoning involved in doing design, and cognitive and social skills a designer needs. Keeping in mind that good design requires effective collaboration and effective use of computer technology, collaboration and use of software is integral to the courses.",
Evolving the HPCCI. Recommendations from the NRC/CSTB 1995 Report,"Early in 1994 Congress asked the National Research Council to assess the goals, management, and progress of the High-performance Computing and Communications Initiative (HPCCI) and to determine how it needs to evolve to meet the challenges of the nation's information infrastructure. The NRC's Computer Science and Telecommunications Board convened a committee of 12 experts, which offered its assessment of the HPCCI in a recent NRC monograph. The results of this assessment are summarised: Evolving the HPCCI to support the nation's information infrastructure. Using Grand Challenge problems as a vehicle, the HPCCI has been generally successful in developing a better computing and computational infrastructure and increasing researcher-developer-user synergy. Having helped bring parallel computing into the mainstream, the HPCCI should continue to focus its considerable expertise on new computing and communications technologies, particularly those that support the improvement of the nation's information infrastructure.",
Implementation of Boolean minimization in an abductive framework,"This work represents the final phase in establishing a firm relationship between the Boolean minimization problem (BMP) and the abduction problem (AP). Specifically, it demonstrates how the BMP could be solved using an algorithm based on the parsimonious covering model, which in turn models a class of APs. The implementation of the algorithm that solves the BMP in an abductive framework provides a much needed empirical validation of the theoretical properties of the algorithm, in addition to providing a different method for computing the minimized form for a Boolean function.",
Data partitioning for load-balance and communication bandwidth preservation,"Preservation of locality of reference is the most critical issue for performance in high performance architectures, especially scalable architectures with electronic interconnection networks. We briefly discuss some of the issues in this paper and give results from the use of spectral bisection on irregular grid computations on the Connection Machine systems.",
An intelligent interface for diagram navigation,"Display and navigation of diagrams play an important role in visual information processing, especially for visualising large scale information systems. Based on our analysis of various diagram display techniques, we have chosen a suitable display technique for the purpose of diagram navigation. We have aimed at automatic navigation of large diagrams. By reasoning and making inferences, based on the user's clue set provided visually, a diagram is traversed following some navigation path. By combining diagram display and automatic navigation, a prototype system has been developed.",
DeleGate: towards a conference diplomat's multidimensional workstation,"Diplomats and other experts are sent as delegations to inter-governmental conferences to implement their governments' policies. Individual delegates in a multilateral diplomatic conference environment have needs for simultaneous any-time/any-place types of information and communication that have not been met by either traditional GSS studies in a given time/place environment or existing groupware. Technological developments like groupware standardization and emerging PCMCIA connectivity are making possible new visions of potential improvements to support of inter-organizational group processes. In terms of a diplomatic conference this may mean having the host of a conference provide a network platform (server with collaborative software) according to an internationally accepted standard with participating diplomats having access through their own workstations and interfaces. This paper demonstrates the need for more comprehensive, user-centered, multi-methodological IS research and groupware development to assist in navigating the seas of the collaboration archipelago.",
"Adaptive error correcting codes based on cooperative play of the game of ""twenty questions with a liar""","Summary form only given. The existence of a noiseless, delayless feedback channel permits the transmitter to detect transmission errors at the time they occur. Such a feedback channel does not increase channel capacity, but it does permit the use of adaptive codes with significantly enhanced error correction capabilities. It is well known that codes of this type can be based on cooperative play of the game of ""twenty questions with a liar"". However, it is perhaps not so well appreciated that it is practicable to implement codes of this type on general purpose computers. We describe a simple and fast implementation in which the transmitter makes reference to a fully worked out game tree. In the worst case, storage requirements grow exponentially with block length. For many cases of interest, storage requirements are not excessive. For example, a 4-error correcting code with 8 information bits and 13 check bits requires only 103.2 kilobytes of storage. By contrast, an 8-error correcting code with 6 information bits and 25 check bits requires 21.2 megabytes of storage. However, no nonadaptive code is capable of correcting as many as 8 errors when 6 information bits are encoded in a block of length 31.",
IBM/NC State joint Smalltalk program,"When faced with a shortage of Smalltalk programming skills, the IBM Lab at Research Triangle Park, NC, USA worked with North Carolina State University to find a solution. What resulted was a joint program for Smalltalk object-oriented education centered around sound engineering and project work. That program now running for over two years has been continually improved by evaluating each semester's results and making appropriate changes. IBM's direct involvement as an industry sponsor included the development of course materials, classroom instruction and project mentoring. This involvement provided IBM engineers first-hand experiences with the students and allowed them to work effectively with NC State to adjust the program. This approach has proved successful in overcoming this technology's steep learning curve, providing students with a foundation of object-oriented skills. The program has created an effective combination of an industry's skill needs and the university's goal of providing a solid engineering education. We believe it could serve as a model for other companies that have specific education and skill requirements.",
Sensitivity of AIRSAR data to changes in component biomass for BOREAS jack pine stands,"As part of the BOREAS experiment, the relationship between the components of stand biomass and the radar backscattering coefficients are examined. Four jack pine stands with broadly varying biomass statistics are compared to AIRSAR P-band data. Even age stands have been chosen with biomass values ranging from 10 to 200 t/ha. Stand statistics, architecture, dielectric constants and forest surface characteristics have been measured and the biomass of trunks, branches and needles have been computed for each stand. A two layer distorted Born forest model (single scatter) has been used to compute the like polarized backscatter at P-band. The results are in general agreement with the AIRSAR data. The sensitivity of backscatter to changes in biomass is evaluated by using the forest model. Analysis shows the P-band backscatter is most sensitive to changes in trunk biomass, however certain anomalous parameter ranges do exist.",
Finding bilateral symmetry axes in almost-symmetric images,"Although the human visual system is adept at finding axes of bilateral symmetry in image data, the prevailing computational method for finding symmetry axes, proposed by Marola (1989), was found to be unreliable for large immunohistological images. A modification of the Marola method which exploits the current understanding of how the human visual system processes symmetry is proposed and tested. This modification involves using only the portion of the data within 1 degree of the visual angle of the symmetry axis. Symmetry axes found using the new method were judged by inspection to be better than those found using Marola's method.",
n-dimensional processor arrays with optical dBuses,"dBus-array(k,n) is an n-dimensional processor array of k/sup n/ nodes connected via k/sup n-1/ dBuses. A dBus is a unidirectional bus which receives signals from a set of n nodes (input set), and transmits signals to a different set of n nodes (output set). Two optical implementations of the dBus-array(k,n) are discussed. One implementation uses the wavelength division multiplexing as in the wavelength division multiple access channel hypercube WMCH (P.W. Dowd, 1992). WMCH(k,n) and dBus-array(k,n) have the same diameter and about the same average internode distance, while the dBus-array requires only one tunable transmitter/receiver per node. Compared to n tunable transmitters/receivers per node for the WMCH. The other implementation uses one fixed wavelength transmitter/receiver per node and the dilated slipped banyan switching network (DSB) (R.A. Thompson, 1991) to combine the time division and wavelength division multiplexing (G. Lin et al., 1994).",
