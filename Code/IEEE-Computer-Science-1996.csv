Title,Abstract,Keywords
Probabilistic roadmaps for path planning in high-dimensional configuration spaces,"A new motion planning method for robots in static workspaces is presented. This method proceeds in two phases: a learning phase and a query phase. In the learning phase, a probabilistic roadmap is constructed and stored as a graph whose nodes correspond to collision-free configurations and whose edges correspond to feasible paths between these configurations. These paths are computed using a simple and fast local planner. In the query phase, any given start and goal configurations of the robot are connected to two nodes of the roadmap; the roadmap is then searched for a path joining these two nodes. The method is general and easy to implement. It can be applied to virtually any type of holonomic robot. It requires selecting certain parameters (e.g., the duration of the learning phase) whose values depend on the scene, that is the robot and its workspace. But these values turn out to be relatively easy to choose, Increased efficiency can also be achieved by tailoring some components of the method (e.g., the local planner) to the considered robots. In this paper the method is applied to planar articulated robots with many degrees of freedom. Experimental results show that path planning can be done in a fraction of a second on a contemporary workstation (/spl ap/150 MIPS), after learning for relatively short periods of time (a few dozen seconds).","Path planning,
Robots,
Orbital robotics,
Motion planning,
Joining processes,
Computer science,
Layout,
Workstations,
Laboratories"
Efficient fair queuing using deficit round-robin,"Fair queuing is a technique that allows each flow passing through a network device to have a fair share of network resources. Previous schemes for fair queuing that achieved nearly perfect fairness were expensive to implement; specifically, the work required to process a packet in these schemes was O(log(n)), where n is the number of active flows. This is expensive at high speeds. On the other hand, cheaper approximations of fair queuing reported in the literature exhibit unfair behavior. In this paper, we describe a new approximation of fair queuing, that we call deficit round-robin. Our scheme achieves nearly perfect fairness in terms of throughput, requires only O(1) work to process a packet, and is simple enough to implement in hardware. Deficit round-robin is also applicable to other scheduling problems where servicing cannot be broken up into smaller units (such as load balancing) and to distributed queues.","Circuits,
Throughput,
Processor scheduling,
Load management,
Resource management,
Operating systems,
Process control,
Insulation,
Computer networks,
Computer science"
A sense of self for Unix processes,"A method for anomaly detection is introduced in which ""normal"" is defined by short-range correlations in a process' system calls. Initial experiments suggest that the definition is stable during normal behaviour for standard UNIX programs. Further; it is able to detect several common intrusions involving sendmail and 1pr. This work is part of a research program aimed at building computer security systems that incorporate the mechanisms and algorithms used by natural immune systems.","Immune system,
Computer security,
Protection,
Computer science,
Operating systems,
Cryptography,
Skin,
Biomembranes,
Robustness,
Software engineering"
The theory of hybrid automata,"We summarize several recent results about hybrid automata. Our goal is to demonstrate that concepts from the theory of discrete concurrent systems can give insights into partly continuous systems, and that methods for the verification of finite-state systems can be used to analyze certain systems with uncountable state spaces.",
This site can’t be reached,,
VLSI module placement based on rectangle-packing by the sequence-pair,"The earliest and the most critical stage in VLSI layout design is the placement. The background is the rectangle packing problem: given a set of rectangular modules of arbitrary sizes, place them without overlap on a plane within a rectangle of minimum area. Since the variety of the packing is uncountably infinite, the key issue for successful optimization is the introduction of a finite solution space which includes an optimal solution. This paper proposes such a solution space where each packing is represented by a pair of module name sequences, called a sequence-pair. Searching this space by simulated annealing, hundreds of modules have been packed efficiently as demonstrated. For applications to VLSI layout, we attack the biggest MCNC benchmark ami49 with a conventional wiring area estimation method, and obtain a highly promising placement.",
Single event upset at ground level,Ground level upsets have been observed in computer systems containing large amounts of random access memory (RAM). Atmospheric neutrons are most likely the major cause of the upsets based on measured data using the Weapons Neutron Research (WNR) neutron beam.,"Single event upset,
Neutrons,
Aerospace electronics,
Computer errors,
Random access memory,
Particle beams,
Read-write memory,
Alpha particles,
Packaging,
Biomedical measurements"
Priority encoding transmission,"We introduce a new method, called priority encoding transmission, for sending messages over lossy packet-based networks. When a message is to be transmitted, the user specifies a priority value for each part of the message. Based on the priorities, the system encodes the message into packets for transmission and sends them to (possibly multiple) receivers. The priority value of each part of the message determines the fraction of encoding packets sufficient to recover that part. Thus even if some of the encoding packets are lost en-route, each receiver is still able to recover the parts of the message for which a sufficient fraction of the encoding packets are received. For any set of priorities for a message, we define a natural quantity called the girth of the priorities. We develop systems for implementing any given set of priorities such that the total length of the encoding packets is equal to the girth. On the other hand, we give an information-theoretic lower bound that shows that for any set of priorities the total length of the encoding packets must be at least the girth. Thus the system we introduce is optimal in terms of the total encoding length. This work has immediate applications to multimedia and high-speed networks applications, especially in those with bursty sources and multiple receivers with heterogeneous capabilities. Implementations of the system show promise of being practical.","Encoding,
Computer science,
Propagation losses,
High-speed networks,
Reed-Solomon codes,
USA Councils,
Buffer overflow"
Evolutionary Algorithms for Constrained Parameter Optimization Problems,"Evolutionary computation techniques have received a great deal of attention regarding their potential as optimization techniques for complex numerical functions. However, they have not produced a significant breakthrough in the area of nonlinear programming due to the fact that they have not addressed the issue of constraints in a systematic way. Only recently have several methods been proposed for handling nonlinear constraints by evolutionary algorithms for numerical optimization problems; however, these methods have several drawbacks, and the experimental results on many test cases have been disappointing. In this paper we (1) discuss difficulties connected with solving the general nonlinear programming problem; (2) survey several approaches that have emerged in the evolutionary computation community; and (3) provide a set of 11 interesting test cases that may serve as a handy reference for future methods.",
The PIM architecture for wide-area multicast routing,"The purpose of multicast routing is to reduce the communication costs for applications that send the same data to multiple recipients. Existing multicast routing mechanisms were intended for use within regions where a group is widely represented or bandwidth is universally plentiful. When group members, and senders to those group members, are distributed sparsely across a wide area, these schemes are not efficient; data packets or membership report information are occasionally sent over many links that do not lead to receivers or senders, respectively. We have developed a multicast routing architecture that efficiently establishes distribution trees across wide area internets, where many groups will be sparsely represented. Efficiency is measured in terms of the router state, control message processing, and data packet processing, required across the entire network in order to deliver data packets to the members of the group. Our protocol independent multicast (PIM) architecture: (a) maintains the traditional IP multicast service model of receiver-initiated membership, (b) supports both shared and source-specific (shortest-path) distribution trees, (c) is not dependent on a specific unicast routing protocol, and (d) uses soft-state mechanisms to adapt to underlying network conditions and group dynamics. The robustness, flexibility, and scaling properties of this architecture make it well-suited to large heterogeneous internetworks.","Multicast protocols,
Routing protocols,
Unicast,
Internet,
Jacobian matrices,
Bandwidth,
Process control,
Computer science,
Costs,
Robustness"
Object matching using deformable templates,"We propose a general object localization and retrieval scheme based on object shape using deformable templates. Prior knowledge of an object shape is described by a prototype template which consists of the representative contour/edges, and a set of probabilistic deformation transformations on the template. A Bayesian scheme, which is based on this prior knowledge and the edge information in the input image, is employed to find a match between the deformed template and objects in the image. Computational efficiency is achieved via a coarse-to-fine implementation of the matching algorithm. Our method has been applied to retrieve objects with a variety of shapes from images with complex background. The proposed scheme is invariant to location, rotation, and moderate scale changes of the template.","Shape,
Prototypes,
Bayesian methods,
Image retrieval,
Image databases,
Information retrieval,
Image segmentation,
Object recognition,
Computer science,
Computational efficiency"
Property-based software engineering measurement,"Little theory exists in the field of software system measurement. Concepts such as complexity, coupling, cohesion or even size are very often subject to interpretation and appear to have inconsistent definitions in the literature. As a consequence, there is little guidance provided to the analyst attempting to define proper measures for specific problems. Many controversies in the literature are simply misunderstandings and stem from the fact that some people talk about different measurement concepts under the same label (complexity is the most common case). There is a need to define unambiguously the most important measurement concepts used in the measurement of software products. One way of doing so is to define precisely what mathematical properties characterize these concepts, regardless of the specific software artifacts to which these concepts are applied. Such a mathematical framework could generate a consensus in the software engineering community and provide a means for better communication among researchers, better guidelines for analysts, and better evaluation methods for commercial static analyzers for practitioners. We propose a mathematical framework which is generic, because it is not specific to any particular software artifact, and rigorous, because it is based on precise mathematical concepts. We use this framework to propose definitions of several important measurement concepts (size, length, complexity, cohesion, coupling). It does not intend to be complete or fully objective; other frameworks could have been proposed and different choices could have been made. However, we believe that the formalisms and properties we introduce are convenient and intuitive. This framework contributes constructively to a firmer theoretical ground of software measurement.",
Analyzing regression test selection techniques,"Regression testing is a necessary but expensive maintenance activity aimed at showing that code has not been adversely affected by changes. Regression test selection techniques reuse tests from an existing test suite to test a modified program. Many regression test selection techniques have been proposed, however, it is difficult to compare and evaluate these techniques because they have different goals. This paper outlines the issues relevant to regression test selection techniques, and uses these issues as the basis for a framework within which to evaluate the techniques. The paper illustrates the application of the framework by using it to evaluate existing regression test selection techniques. The evaluation reveals the strengths and weaknesses of existing techniques, and highlights some problems that future work in this area should address.",
Exploiting Choice: Instruction Fetch and Issue on an Implementable Simultaneous Multithreading Processor,"Simultaneous multithreading is a technique that permits multiple independent threads to issue multiple instructions each cycle. In previous work we demonstrated the performance potential of simultaneous multithreading, based on a somewhat idealized model. In this paper we show that the throughput gains from simultaneous multithreading can be achieved without extensive changes to a conventional wide-issue superscalar, either in hardware structures or sizes. We present an architecture for simultaneous multithreading that achieves three goals: (1) it minimizes the architectural impact on the conventional superscalar design, (2) it has minimal performance impact on a single thread executing alone, and (3) it achieves significant throughput gains when running multiple threads. Our simultaneous multithreading architecture achieves a throughput of 5.4 instructions per cycle, a 2.5-fold improvement over an unmodified superscalar with similar hardware resources. This speedup is enhanced by an advantage of multithreading previously unexploited in other architectures: the ability to favor for fetch and issue those threads most efficiently using the processor each cycle, thereby providing the ""best"" instructions to the processor.",
A row-action alternative to the EM algorithm for maximizing likelihood in emission tomography,"The maximum likelihood (ML) approach to estimating the radioactive distribution in the body cross section has become very popular among researchers in emission computed tomography (ECT) since it has been shown to provide very good images compared to those produced with the conventional filtered backprojection (FBP) algorithm. The expectation maximization (EM) algorithm is an often-used iterative approach for maximizing the Poisson likelihood in ECT because of its attractive theoretical and practical properties. Its major disadvantage is that, due to its slow rate of convergence, a large amount of computation is often required to achieve an acceptable image. Here, the authors present a row-action maximum likelihood algorithm (RAMLA) as an alternative to the EM algorithm for maximizing the Poisson likelihood in ECT. The authors deduce the convergence properties of this algorithm and demonstrate by way of computer simulations that the early iterates of RAMLA increase the Poisson likelihood in ECT at an order of magnitude faster that the standard EM algorithm. Specifically, the authors show that, from the point of view of measuring total radionuclide uptake in simulated brain phantoms, iterations 1, 2, 3, and 4 of RAMLA perform at least as well as iterations 45, 60, 70, and 80, respectively, of EM. Moreover, the authors show that iterations 1, 2, 3, and 4 of RAMLA achieve comparable likelihood values as iterations 45, 60, 70, and 80, respectively, of EM. The authors also present a modified version of a recent fast ordered subsets EM (OS-EM) algorithm and show that RAMLA is a special case of this modified OS-EM. Furthermore, the authors show that their modification converges to a ML solution whereas the standard OS-EM does not.",
Generalized communicators in the Message Passing Interface,"We propose extensions to the Message Passing Interface (MPI) that generalize the MPI communicator concept to allow multiple communication endpoints per process, dynamic creation of endpoints, and the transfer of endpoints between processes. The generalized communicator construct can be used to express a wide range of interesting communication structures, including collective communication operations involving multiple threads per process, communications between dynamically created threads, and object-oriented applications in which communications are directed to specific objects. Furthermore, this enriched functionality can be provided in a manner that preserves backward compatibility with MPI. We describe the proposed extensions, illustrate their use with examples, and discuss implementation issues.","Message passing,
Yarn,
Liquid crystal on silicon,
Mathematics,
Computer science,
Laboratories,
Programming profession,
Interference,
Communication system control,
Impedance"
A real-time face tracker,"The authors present a real-time face tracker. The system has achieved a rate of 30+ frames/second using an HP-9000 workstation with a frame grabber and a Canon VC-Cl camera. It can track a person's face while the person moves freely (e.g., walks, jumps, sits down and stands up) in a room. Three types of models have been employed in developing the system. First, they present a stochastic model to characterize skin color distributions of human faces. The information provided by the model is sufficient for tracking a human face in various poses and views. This model is adaptable to different people and different lighting conditions in real-time. Second, a motion model is used to estimate image motion and to predict the search window. Third, a camera model is used to predict and compensate for camera motion. The system can be applied to teleconferencing and many HCI applications including lip reading and gaze tracking. The principle in developing this system can be extended to other tracking problems such as tracking the human hand.",
Validity-guided (re)clustering with applications to image segmentation,"When clustering algorithms are applied to image segmentation, the goal is to solve a classification problem. However, these algorithms do not directly optimize classification duality. As a result, they are susceptible to two problems: 1) the criterion they optimize may not be a good estimator of ""true"" classification quality, and 2) they often admit many (suboptimal) solutions. This paper introduces an algorithm that uses cluster validity to mitigate problems 1 and 2. The validity-guided (re)clustering (VGC) algorithm uses cluster-validity information to guide a fuzzy (re)clustering process toward better solutions. It starts with a partition generated by a soft or fuzzy clustering algorithm. Then it iteratively alters the partition by applying (novel) split-and-merge operations to the clusters. Partition modifications that result in improved partition validity are retained. VGC is tested on both synthetic and real-world data. For magnetic resonance image (MRI) segmentation, evaluations by radiologists show that VGC outperforms the (unsupervised) fuzzy c-means algorithm, and VGC's performance approaches that of the (supervised) k-nearest-neighbors algorithm.","Image segmentation,
Clustering algorithms,
Partitioning algorithms,
Iterative algorithms,
Computer science,
Fuzzy logic,
Magnetic resonance,
Magnetic resonance imaging,
Training data,
Algorithm design and analysis"
A multiple active contour model for cardiac boundary detection on echocardiographic sequences,"Tracing of left-ventricular epicardial and endocardial borders on echocardiographic sequences is essential for quantification of cardiac function. The authors designed a method based on an extension of active contour models to detect both epicardial and endocardial borders on short-axis cardiac sequences spanning the entire cardiac cycle. They validated the results by comparing the computer-generated boundaries to the boundaries manually outlined by four expert observers on 44 clinical data sets. The mean boundary distance between the computer-generated boundaries and the manually outlined boundaries was 2.80 mm (/spl sigma/=1.28 mm) for the epicardium and 3.61 (/spl sigma/=1.68 mm) for the endocardium. These distances were comparable to interobserver distances, which had a mean of 3.79 mm (/spl sigma/=1.53 mm) for epicardial borders and 2.67 mm (/spl sigma/=0.88 mm) for endocardial borders. The correlation coefficient between the areas enclosed by the computer-generated boundaries and the average manually outlined boundaries was 0.95 for epicardium and 0.91 for endocardium. The algorithm is fairly insensitive to the choice of the initial curve. Thus, the authors have developed an effective and robust algorithm to extract left-ventricular boundaries from echocardiographic sequences.","Active contours,
Signal to noise ratio,
Information resources,
Design methodology,
Robustness,
Thickness measurement,
Biomedical imaging,
Ultrasonic imaging,
Cardiology,
Radiology"
Quantum-inspired genetic algorithms,"A novel evolutionary computing method-quantum inspired genetic algorithms-is introduced, where concepts and principles of quantum mechanics are used to inform and inspire more efficient evolutionary computing methods. The basic terminology of quantum mechanics is introduced before a comparison is made between a classical genetic algorithm and a quantum inspired method for the travelling salesperson problem. It is informally shown that the quantum inspired genetic algorithm performs better than the classical counterpart for a small domain. The paper concludes with some speculative comments concerning the relationship between quantum inspired genetic algorithms and various complexity classes.","Genetic algorithms,
Quantum computing,
Orbits,
Electrons,
Energy states,
Quantum mechanics,
Vectors,
Wave functions,
Computational modeling,
Computer science"
Maintenance of discovered association rules in large databases: an incremental updating technique,"An incremental updating technique is developed for maintenance of the association rules discovered by database mining. There have been many studies on efficient discovery of association rules in large databases. However, it is nontrivial to maintain such discovered rules in large databases because a database may allow frequent or occasional updates and such updates may not only invalidate some existing strong association rules but also turn some weak rules into strong ones. An incremental updating technique is proposed for efficient maintenance of discovered association rules when new transaction data are added to a transaction database.","Association rules,
Transaction databases,
Data mining,
Computer science,
Councils,
Algorithm design and analysis,
Economic forecasting,
Data engineering,
Maintenance,
Itemsets"
Trace cache: a low latency approach to high bandwidth instruction fetching,"As the issue width of superscalar processors is increased, instruction fetch bandwidth requirements will also increase. It will become necessary to fetch multiple basic blocks per cycle. Conventional instruction caches hinder this effort because long instruction sequences are not always in contiguous cache locations. We propose supplementing the conventional instruction cache with a trace cache. This structure caches traces of the dynamic instruction stream, so instructions that are otherwise noncontiguous appear contiguous. For the Instruction Benchmark Suite (IBS) and SPEC92 integer benchmarks, a 4 kilobyte trace cache improves performance on average by 28% over conventional sequential fetching. Further it is shown that the trace cache's efficient, low latency approach enables it to outperform more complex mechanisms that work solely out of the instruction cache.","Delay,
Bandwidth,
Throughput,
Engines,
Pipelines,
Registers,
Costs,
Computer science,
Decoding,
Feedback"
Practical considerations for 3-D image reconstruction using spherically symmetric volume elements,"Spherically symmetric volume elements with smooth tapering of the values near their boundaries are alternatives to the more conventional voxels for the construction of volume images in the computer. Their use, instead of voxels, introduces additional parameters which enable the user to control the shape of the volume element (blob) and consequently to control the characteristics of the images produced by iterative methods for reconstruction from projection data. For images composed of blobs, efficient algorithms have been designed for the projection and discrete back-projection operations, which are the crucial parts of iterative reconstruction methods. The authors have investigated the relationship between the values of the blob parameters and the properties of images represented by the blobs. Experiments show that using blobs in iterative reconstruction methods leads to substantial improvement in the reconstruction performance, based on visual quality and on quantitative measures, in comparison with the voxel case. The images reconstructed using appropriately chosen blobs are characterized by less image noise for both noiseless data and noisy data, without loss of image resolution.","Image reconstruction,
Shape control,
Iterative methods,
Reconstruction algorithms,
Smoothing methods,
Iterative algorithms,
Biomedical image processing,
Radiology,
Algorithm design and analysis,
Image resolution"
Histogram refinement for content-based image retrieval,"Color histograms are widely used for content-based image retrieval. Their advantages are efficiency, and insensitivity to small changes in camera viewpoint. However, a histogram is a coarse characterization of an image, and so images with very different appearances can have similar histograms. We describe a technique for comparing images called histogram refinement, which imposes additional constraints on histogram based matching. Histogram refinement splits the pixels in a given bucket into several classes, based upon some local property. Within a given bucket, only pixels in the same class are compared. We describe a split histogram called a color coherence vector (CCV), which partitions each histogram bucket based on spatial coherence. CCVs can be computed at over 5 images per second on a standard workstation. A database with 15,000 images can be queried using CCVs in under 2 seconds. We demonstrate that histogram refinement can be used to distinguish images whose color histograms are indistinguishable.",
Directional processing of color images: theory and experimental results,"The processing of color image data using directional information is studied. The class of vector directional filters (VDF), which was introduced by the authors in a previous work, is further considered. The analogy of VDF to the spherical median is shown, and their relation to the spatial median is examined. Moreover, their statistical and deterministic properties are studied, which demonstrate their appropriateness in image processing. VDF result in optimal estimates of the image vectors in the directional sense; this is very important in the case of color images, where the vectors' direction signifies the chromaticity of a given color. Issues regarding the practical implementation of VDF are also considered. In addition, efficient filtering schemes based on VDF are proposed, which include adaptive and/or double-window structures. Experimental and comparative results in image filtering show very good performance measures when the error is measured in the L*a*b* space. L*a*b* is known as a space where equal color differences result in equal distances, and therefore, it is very close to the human perception of colors. Moreover, an indication of the chromaticity error is obtained by measuring the error on the Maxwell triangle; the results demonstrate that VDF are very accurate chromaticity estimators.",
A space-sweep approach to true multi-image matching,"The problem of determining feature correspondences across multiple views is considered. The term ""true multi-image"" matching is introduced to describe techniques that make full and efficient use of the geometric relationships between multiple images and the scene. A true multi-image technique must generalize to any number of images, be of linear algorithmic complexity in the number of images, and use all the images in an equal manner. A new space-sweep approach to true multi-image matching is presented that simultaneously determines 2D feature correspondences and the 3D positions of feature points in the scene. The method is illustrated on a seven-image matching example from the aerial image domain.","Layout,
Computer science,
Uniform resource locators,
Stereo image processing,
Image reconstruction,
Cameras,
Lenses,
Shape,
Tracking,
Contracts"
Learning long-term dependencies in NARX recurrent neural networks,"It has previously been shown that gradient-descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies, i.e. those problems for which the desired output depends on inputs presented at times far in the past. We show that the long-term dependencies problem is lessened for a class of architectures called nonlinear autoregressive models with exogenous (NARX) recurrent neural networks, which have powerful representational capabilities. We have previously reported that gradient descent learning can be more effective in NARX networks than in recurrent neural network architectures that have ""hidden states"" on problems including grammatical inference and nonlinear system identification. Typically, the network converges much faster and generalizes better than other networks. The results in this paper are consistent with this phenomenon. We present some experimental results which show that NARX networks can often retain information for two to three times as long as conventional recurrent neural networks. We show that although NARX networks do not circumvent the problem of long-term dependencies, they can greatly improve performance on long-term dependency problems. We also describe in detail some of the assumptions regarding what it means to latch information robustly and suggest possible ways to loosen these assumptions.","Intelligent networks,
Recurrent neural networks,
National electric code,
Robustness,
Power system modeling,
Ear,
System identification,
Neural networks,
Nonlinear dynamical systems,
Computer science"
Adifor 2.0: automatic differentiation of Fortran 77 programs,"Numerical codes that calculate not only a result, but also the derivatives of the variables with respect to each other, facilitate sensitivity analysis, inverse problem solving, and optimization. The paper considers how Adifor 2.0, which won the 1995 Wilkinson Prize for Numerical Software, can automatically differentiate complicated Fortran code much faster than a programmer can do it by hand. The Adifor system has three main components: the AdiFor preprocessor, the ADIntrinsics exception-handling system, and the SparsLinC library.",
Detection of stellate distortions in mammograms,"Malignant densities in mammograms have an irregular appearance and frequently are surrounded by a radiating pattern of linear spicules. In this paper a method is described to detect such stellate patterns. This method is based on statistical analysis of a map of pixel orientations. If an increase of pixels pointing to a region is found, this region is marked as suspicious, especially if such an increase is found in many directions. Orientations of the image intensity map are determined at each pixel using a multiscale approach. At a given scale, accurate line-based orientation estimates are obtained from the output of three-directional, second-order, Gaussian derivative operators. The orientation at the scale at which these operators have maximum response is selected. If a line-like structure is present at a given site, this method provides an estimate of the orientation of this structure, whereas in other cases the image noise will generate a random orientation. The pixel orientation map is used to construct two operators which are sensitive to radial patterns of straight lines. Combination of the output of these operators using a classifier allows for detection of stellate patterns. Different classification methods have been compared and results obtained on a common database are presented. Around 90% of the malignant cases were detected at rate of one false positive (FP) per image.","Breast cancer,
Tellurium,
Cancer detection,
Computer errors,
Hospitals,
Radiology,
Statistical analysis,
Pixel,
Noise generators,
Image generation"
Classification of mass and normal breast tissue: a convolution neural network classifier with spatial domain and texture images,"The authors investigated the classification of regions of interest (ROI's) on mammograms as either mass or normal tissue using a convolution neural network (CNN). A CNN is a backpropagation neural network with two-dimensional (2-D) weight kernels that operate on images. A generalized, fast and stable implementation of the CNN was developed. The input images to the CNN were obtained from the ROI's using two techniques. The first technique employed averaging and subsampling. The second technique employed texture feature extraction methods applied to small subregions inside the ROI. Features computed over different subregions were arranged as texture images, which were subsequently used as CNN inputs. The effects of CNN architecture and texture feature parameters on classification accuracy were studied. Receiver operating characteristic (ROC) methodology was used to evaluate the classification accuracy. A data set consisting of 168 ROIs containing biopsy-proven masses and 504 ROI's containing normal breast tissue was extracted from 168 mammograms by radiologists experienced in mammography. This data set was used for training and testing the CNN. With the best combination of CNN architecture and texture feature parameters, the area under the test ROC curve reached 0.87, which corresponded to a true-positive fraction of 90% at a false positive fraction of 31%. The authors' results demonstrate the feasibility of using a CNN for classification of masses and normal tissue on mammograms.",
Module placement on BSG-structure and IC layout applications,"A new method of packing rectangles (modules) is presented with applications to IC layout design. It is based on the bounded-sliceline grid (BSG) structure. The BSG dissects the plane into rooms associated with binary relations ""right-to""and ""above"" such that any two rooms are uniquely in either relation. A packing is obtained through an assignment of modules on the BSG. Followed by physical realization BSG-PACK. A simulated annealing searches for a goon packing of all packings by changing the assignments. Experiments showed that hundreds of rectangles are easily packed in a small rectangle area (chip) with quite good quality in area efficiency. A wide adaptability is demonstrated specific to IC layout design. Remarkable examples are: the chip is not necessarily rectangle, L-shaped modules and modules which are allowed to partially overlap each other can be handled.",
Genetic algorithm and graph partitioning,"Hybrid genetic algorithms (GAs) for the graph partitioning problem are described. The algorithms include a fast local improvement heuristic. One of the novel features of these algorithms is the schema preprocessing phase that improves GAs' space searching capability, which in turn improves the performance of GAs. Experimental tests on graph problems with published solutions showed that the new genetic algorithms performed comparable to or better than the multistart Kernighan-Lin algorithm and the simulated annealing algorithm. Analyses of some special classes of graphs are also provided showing the usefulness of schema preprocessing and supporting the experimental results.",
NULL Convention Logic/sup TM/: a complete and consistent logic for asynchronous digital circuit synthesis,"NULL Convention Logic (NCL) is a symbolically complete logic which expresses process completely in terms of the logic itself and inherently and conveniently expresses asynchronous digital circuits. The traditional form of Boolean logic is not symbolically complete in the sense that it requires the participation of a fundamentally different form of expression, time in the form of the clock, which has to be very carefully coordinated with the logic part of the expression to completely and effectively express a process. We introduce NULL Convention Logic in relation to Boolean logic as a four value logic, and as a three value logic and finally as two value logic quite different from traditional Boolean logic. We then show how systems can be constructed entirely in terms of NULL Convention Logic.","Logic circuits,
Digital circuits,
Circuit synthesis,
Boolean functions,
Computer science,
Clocks,
Computational modeling,
Physics computing,
Chemical processes,
Biochemistry"
Packet loss correlation in the MBone multicast network,"The success of multicast applications such as Internet teleconferencing illustrates the tremendous potential of applications built upon wide-area multicast communication services. A critical issue for such multicast applications and the higher layer protocols required to support them is the manner in which packet losses occur within the multicast network. We present and analyze packet loss data collected on multicast-capable hosts at 17 geographically distinct locations in Europe and the US and connected via the MBone. We experimentally and quantitatively examine the spatial and temporal correlation in packet loss among participants in a multicast session. Our results show that there is some spatial correlation in loss among the multicast sites. However, the shared loss in the backbone of the MBone is, for the most part, low. We find a fairly significant amount of of burst loss (consecutive losses) at most sites. In every dataset, at least one receiver experienced a long loss burst greater than 8 seconds (100 consecutive packets). A predominance of solitary loss was observed in all cases, but periodic losses of length approximately 0.6 seconds and at 30 second intervals were seen by some receivers.","Intelligent networks,
Multicast protocols,
Spine,
Application software,
Web and internet services,
Multicast communication,
Data analysis,
Europe,
Error correction,
Computer science"
Characterizing reference locality in the WWW,"The authors propose models for both temporal and spatial locality of reference in streams of requests arriving at Web servers. They show that simple models based on document popularity alone are insufficient for capturing either temporal or spatial locality. Instead, they rely on an equivalent, but numerical, representation of a reference stream: a stack distance trace. They show that temporal locality can be characterized by the marginal distribution of the stack distance trace, and propose models for typical distributions and compare their cache performance to the traces. They also show that spatial locality in a reference stream can be characterized using the notion of self-similarity. Self-similarity describes long-range correlations in the data set, which is a property that previous researchers have found hard to incorporate into synthetic reference strings. They show that stack distance strings appear to be strongly self-similar, and provide measurements of the degree of self-similarity in the traces. Finally, they discuss methods for generating synthetic Web traces that exhibit the properties of temporal and spatial locality measured in the data.","World Wide Web,
Prefetching,
Web server,
Length measurement,
Application software,
Web sites,
Computer science"
Interactive multiuser VEs in the DIVE system,"Multiuser virtual environments (VEs) raise challenging research questions concerning how users interact with objects, applications, and other users, and how distributed VEs behave when the number of users increases. The Distributed Interactive Virtual Environment (DIVE) is a software platform for multiuser VEs that has served as a toolkit for many distributed VE applications. It emphasizes networking and human-computer interaction and supports autonomous behavior-driven objects, collision detection, and audio and 3D navigation.",
Intelligent access to digital video: Informedia project,"Carnegie Mellon's Informedia Digital Video Library project will establish a large, on-line digital video library featuring full-content and knowledge-based search and retrieval. Intelligent, automatic mechanisms will be developed to populate the library. Search and retrieval from digital video, audio, and text libraries will take place via desktop computer over local, metropolitan, and wide-area networks. The project's approach applies several techniques for content-based searching and video-sequence retrieval. Content is conveyed in both the narrative (speech and language) and the image. Only by the collaborative interaction of image, speech, and natural language understanding technology is it possible to successfully populate, segment, index, and search diverse video collections with satisfactory recall and precision. This collaborative interaction approach uniquely compensates for problems of interpretation and search in error-ridden and ambiguous data sets. The authors have focused the work on two corpuses. One is science documentaries and lectures, the other is broadcast news content with partial closed-captions. Further work will continue to improve the accuracy and performance of the underlying processing as well as explore performance issues related to Web-based access and interoperability with other digital video resources.","Software libraries,
Natural languages,
Information retrieval,
Educational institutions,
Image segmentation,
Collaboration,
Content based retrieval,
Computer errors,
Automatic speech recognition,
Information analysis"
Efficient mining of association rules in distributed databases,"Many sequential algorithms have been proposed for the mining of association rules. However, very little work has been done in mining association rules in distributed databases. A direct application of sequential algorithms to distributed databases is not effective, because it requires a large amount of communication overhead. In this study, an efficient algorithm called DMA (Distributed Mining of Association rules), is proposed. It generates a small number of candidate sets and requires only O(n) messages for support-count exchange for each candidate set, where n is the number of sites in a distributed database. The algorithm has been implemented on an experimental testbed, and its performance is studied. The results show that DMA has superior performance, when compared with the direct application of a popular sequential algorithm, in distributed databases.","Data mining,
Association rules,
Distributed databases,
Partitioning algorithms,
Transaction databases,
Computer science,
Testing,
Distributed algorithms,
Economic forecasting,
Warehousing"
Fuzzy systems with defuzzification are universal approximators,"In this paper, we consider a fundamental theoretical question: Is it always possible to design a fuzzy system capable of approximating any real continuous function on a compact set with arbitrary accuracy? Moreover, we research whether the answer to the above question is positive when we restrict to a fixed (but arbitrary) type of fuzzy reasoning and to a subclass of fuzzy relations. This result can be viewed as an existence theorem of an optimal fuzzy system for a wide variety of problems.","Fuzzy systems,
Fuzzy reasoning,
Control systems,
Fuzzy logic,
Fuzzy control,
Hybrid intelligent systems,
Decision making,
Expert systems,
Computer science,
Artificial intelligence"
Subspace methods for robot vision,"In contrast to the traditional approach, visual recognition is formulated as one of matching appearance rather than shape. For any given robot vision task, all possible appearance variations define its visual workspace. A set of images is obtained by coarsely sampling the workspace. The image set is compressed to obtain a low-dimensional subspace, called the eigenspace, in which the visual workspace is represented as a continuous appearance manifold. Given an unknown input image, the recognition system first projects the image to eigenspace. The parameters of the vision task are recognized based on the exact location of the projection on the appearance manifold. An efficient algorithm for finding the closest manifold point is described. The proposed appearance representation has several applications in robot vision. As examples, a precise visual positioning system, a real-time visual tracking system, and a real-time temporal inspection system are described.",
Completeness and consistency in hierarchical state-based requirements,"This paper describes methods for automatically analyzing formal, state-based requirements specifications for some aspects of completeness and consistency. The approach uses a low-level functional formalism, simplifying the analysis process. State-space explosion problems are eliminated by applying the analysis at a high level of abstraction; i.e., instead of generating a reachability graph for analysis, the analysis is performed directly on the model. The method scales up to large systems by decomposing the specification into smaller, analyzable parts and then using functional composition rules to ensure that verified properties hold for the entire specification. The analysis algorithms and tools have been validated on TCAS II, a complex, airborne, collision-avoidance system required on all commercial aircraft with more than 30 passengers that fly in U.S. Airspace.","Performance analysis,
Error correction,
Robustness,
Aircraft,
Software safety,
Computer science,
Explosions,
Algorithm design and analysis,
Software systems,
Timing"
Input selection for ANFIS learning,"We present a quick and straightfoward way of input selection for neuro-fuzzy modeling using adaptive neuro-fuzzy inference systems (ANFIS). The method is tested on two real-world problems: the nonlinear regression problem of automobile MPG (miles per gallon) prediction, and the nonlinear system identification using the Box and Jenkins gas furnace data.",
Real-time tracking of image regions with changes in geometry and illumination,"Historically, SSD or correlation-based visual tracking algorithms have been sensitive to changes in illumination and shading across the target region. This paper describes methods for implementing SSD tracking that is both insensitive to illumination variations and computationally efficient. We first describe a vector-space formulation of the tracking problem, showing how to recover geometric deformations. We then show that the same vector space formulation can be used to account for changes in illumination. We combine geometry and illumination into an algorithm that tracks large image regions on live video sequences using no more computation than would be required to trade with no accommodation for illumination changes. We present experimental results which compare the performance of SSD tracking with and without illumination compensation.","Lighting,
Target tracking,
Computational geometry,
Iterative algorithms,
Computed tomography,
Optical computing,
Optical distortion,
Motion analysis,
Motion estimation,
Computer science"
A randomized roadmap method for path and manipulation planning,"This paper presents a new randomized roadmap method for motion planning for many DOF robots that can be used to obtain high quality roadmaps even when C-space is crowded. The main novelty in the authors' approach is that roadmap candidate points are chosen on C-obstacle surfaces. As a consequence, the roadmap is likely to contain difficult paths, such as those traversing long, narrow passages in C-space. The approach can be used for both collision-free path planning and for manipulation planning of contact tasks. Experimental results with a planar articulated 6 DOF robot show that, after preprocessing, difficult path planning operations can often be carried out in less than a second.","Path planning,
Motion planning,
Robots,
Robotics and automation,
Joining processes,
Computer science,
Application software,
Virtual reality,
Legged locomotion"
Neural network-based face detection,"We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training the networks, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with other state-of-the-art face detection systems are presented; our system has better performance in terms of detection and false-positive rates.","Neural networks,
Face detection,
Filters,
Computer science,
Gray-scale,
Pattern recognition,
Prototypes,
Face recognition,
Detectors,
Displays"
On Convergence Properties of the EM Algorithm for Gaussian Mixtures,"We build up the mathematical connection between the “Expectation-Maximization” (EM) algorithm and gradient-based approaches for maximum likelihood learning of finite gaussian mixtures. We show that the EM step in parameter space is obtained from the gradient via a projection matrix P, and we provide an explicit expression for the matrix. We then analyze the convergence of EM in terms of special properties of P and provide new results analyzing the effect that P has on the likelihood surface. Based on these mathematical results, we present a comparative discussion of the advantages and disadvantages of EM and other algorithms for the learning of gaussian mixture models.",
A framework for specification-based testing,"Test templates and a test template framework are introduced as useful concepts in specification-based testing. The framework can be defined using any model-based specification notation and used to derive tests from model-based specifications-in this paper, it is demonstrated using the Z notation. The framework formally defines test data sets and their relation to the operations in a specification and to other test data sets, providing structure to the testing process. Flexibility is preserved, so that many testing strategies can be used. Important application areas of the framework are discussed, including refinement of test data, regression testing, and test oracles.","Software testing,
Application software,
Formal specifications,
Computer science,
Life testing,
Object oriented modeling,
Computer Society,
Programming,
Performance evaluation,
Software design"
A fast distributed algorithm for mining association rules,"With the existence of many large transaction databases, the huge amounts of data, the high scalability of distributed systems, and the easy partitioning and distribution of a centralized database, it is important to investigate efficient methods for distributed mining of association rules. The study discloses some interesting relationships between locally large and globally large item sets and proposes an interesting distributed association rule mining algorithm, FDM (fast distributed mining of association rules), which generates a small number of candidate sets and substantially reduces the number of messages to be passed at mining association rules. A performance study shows that FDM has a superior performance over the direct application of a typical sequential algorithm. Further performance enhancement leads to a few variations of the algorithm.",
The sub-rating channel assignment strategy for PCS hand-offs,"A new personal communications services (PCS) hand-off scheme is proposed. This scheme provides for hand-off to radio ports on which there is no free channel by ""sub-rating"" an existing connection. With sub-rating, an occupied full-rate channel is temporarily divided into two half-rate channels: one to serve the existing call and the other to serve the hand-off request. The blocking probabilities (combined forced terminations of existing calls and blocking of new call attempts) of this new scheme compare favorably with the standard scheme (nonprioritizing) and the previously proposed prioritizing schemes. The costs for this scheme are presented and discussed, as well as the additional procedural complexity of implementing on-the-fly sub-rating and the impact of continuing the conversation on a lower rate channel (which may lower speech quality of increase battery drain). Analytical models and simulations investigating the traffic impacts are presented, as are the results that show that even in the highest offered load considered a 3-min conversation in the busy hour experiences less than half a second of sub-rated conversation on average and only about 3% of the calls experience more than 5.12 s of sub-rated conversation. This scheme can increase capacity by 8-35% for systems with 1% call incompletion probability.","Personal communication networks,
Analytical models,
Speech,
Batteries,
Capacity planning,
Communication switching,
Telephony,
Frequency,
Computer science,
Network servers"
On optimal call admission control in cellular networks,"Two important quality-of-service (QoS) measures for current cellular networks are the fractions of new and handoff ""calls"" that are blocked due to unavailability of ""channels"" (radio and/or computing resources). Based on these QoS measures, we derive optimal admission control policies for three problems: minimizing a linear objective function of the new and handoff call blocking probabilities (MINOBJ), minimizing the new call blocking probability with a hard constraint on the handoff call blocking probability (MINBLOCK) and minimizing the number of channels with hard constraints on both of the blocking probabilities (MINC). We show that the well-known guard channel policy is optimal for the MLNOBJ problem, while a new fractional guard channel policy is optimal for the MINBLOCK and MINC problems. The guard channel policy reserves a set of channels for handoff calls while the fractional guard channel policy effectively reserves a non-integral number of guard channels for handoff calls by rejecting new calls with a probability that depends on the current channel occupancy. It is also shown that the fractional policy results in significant savings (20-50%) in the new call blocking probability for the MINBLOCK problem and provides some, though small, gains over the integral guard channel policy for the MINC problem. Further, we also develop computationally inexpensive algorithms for the determination of the parameters for the optimal policies.","Call admission control,
Intelligent networks,
Land mobile radio cellular systems,
Personal communication networks,
Computer science,
Quality of service,
Current measurement,
Computer networks,
Optimal control,
Strain control"
"The role of experimentation in software engineering: past, current, and future",Software engineering needs to follow the model of other physical sciences and develop an experimental paradigm for the field. This paper proposes the approach towards developing an experimental component of such a paradigm. The approach is based upon a quality improvement paradigm that addresses the role of experimentation and process improvement in the content of industrial development. The paper outlines a classification scheme for characterizing such experiments.,"Software engineering,
Problem-solving,
Predictive models,
Humans,
Encapsulation,
Feedback,
Curing,
Physics computing,
Computer science,
Costs"
Probabilistic approximation of metric spaces and its algorithmic applications,"This paper provides a novel technique for the analysis of randomized algorithms for optimization problems on metric spaces, by relating the randomized performance ratio for any, metric space to the randomized performance ratio for a set of ""simple"" metric spaces. We define a notion of a set of metric spaces that probabilistically-approximates another metric space. We prove that any metric space can be probabilistically-approximated by hierarchically well-separated trees (HST) with a polylogarithmic distortion. These metric spaces are ""simple"" as being: (1) tree metrics; (2) natural for applying a divide-and-conquer algorithmic approach. The technique presented is of particular interest in the context of on-line computation. A large number of on-line algorithmic problems, including metrical task systems, server problems, distributed paging, and dynamic storage rearrangement are defined in terms of some metric space. Typically for these problems, there are linear lower bounds on the competitive ratio of deterministic algorithms. Although randomization against an oblivious adversary has the potential of overcoming these high ratios, very little progress has been made in the analysis. We demonstrate the use of our technique by obtaining substantially improved results for two different on-line problems.","Extraterrestrial measurements,
Approximation algorithms,
Computer science,
Contracts,
Distributed computing,
Probability distribution,
Performance analysis,
Algorithm design and analysis,
Polynomials,
Mathematics"
Slicing object-oriented software,"Describes the construction of system dependence graphs for object-oriented software on which efficient slicing algorithms can be applied. We construct these system dependence graphs for individual classes, groups of interacting classes and complete object-oriented programs. For an incomplete system consisting of a single class or a number of interacting classes, we construct a procedure dependence graph that simulates all possible calls to public methods in the class. For a complete system, we construct a procedure dependence graph from the main program in the system. Using these system dependence graphs, we show how to compute slices for individual classes, groups of interacting classes and complete programs. One advantage of our approach is that the system dependence graphs can be constructed incrementally because representations of classes can be reused. Another advantage of our approach is that slices can be computed for incomplete object-oriented programs such as classes or class libraries. We present our results for C++, but our techniques can be applied to other statically typed object-oriented languages such as Ada-95.",
An automatic technique for finding and localizing externally attached markers in CT and MR volume images of the head,"An image processing technique is presented for finding and localizing the centroids of cylindrical markers externally attached to the human head in computed tomography (CT) and magnetic resonance (MR) image volumes. The centroids can be used as control points for image registration. The technique, which is fast, automatic, and knowledge-based, has two major steps. First, it searches the entire image volume to find one voxel inside each marker-like object. The authors call this voxel a ""candidate"" voxel, and they call the object a candidate marker. Second, it classifies the voxels in a region surrounding the candidate voxel as marker or nonmarker voxels using knowledge-based rules and calculates an intensity-weighted centroid for each true marker. The authors call this final centroid the ""fiducial"" point of the marker. The technique was developed on 42 scans of six patients-one CT and six MR scans per patient. There are four markers attached to each patient for a total of 168 marker images. For the CT images the false marker rate was zero. For MR the false marker rate was 1.4% (Two out of 144 markers). To evaluate the accuracy of the fiducial points, CT-MR registration was performed after correcting the MR images for geometrical distortion. The fiducial registration accuracy averaged 0.4 mm and was better than 0.6 mm for each of the eighteen image pairs.","Computed tomography,
Surgery,
Magnetic heads,
Magnetic resonance,
X-ray imaging,
Computer science,
Image processing,
Humans,
Automatic control,
Image registration"
Exploring estimator bias-variance tradeoffs using the uniform CR bound,"We introduce a plane, which we call the delta-sigma plane, that is indexed by the norm of the estimator bias gradient and the variance of the estimator. The norm of the bias gradient is related to the maximum variation in the estimator bias function over a neighborhood of parameter space. Using a uniform Cramer-Rao (CR) bound on estimator variance, a delta-sigma tradeoff curve is specified that defines an ""unachievable region"" of the delta-sigma plane for a specified statistical model. In order to place an estimator on this plane for comparison with the delta-sigma tradeoff curve, the estimator variance, bias gradient, and bias gradient norm must be evaluated. We present a simple and accurate method for experimentally determining the bias gradient norm based on applying a bootstrap estimator to a sample mean constructed from the gradient of the log-likelihood. We demonstrate the methods developed in this paper for linear Gaussian and nonlinear Poisson inverse problems.","Chromium,
Computer science,
Inverse problems,
Vectors,
Random variables,
Fluctuations,
Image sampling,
Spectral analysis,
Smoothing methods"
Glyphs for visualizing uncertainty in vector fields,"Environmental data have inherent uncertainty which is often ignored in visualization. Meteorological stations and doppler radars, including their time series averages, have a wealth of uncertainty information that traditional vector visualization methods such as meteorological wind barbs and arrow glyphs simply ignore. We have developed a new vector glyph to visualize uncertainty in winds and ocean currents. Our approach is to include uncertainty in direction and magnitude, as well as the mean direction and length, in vector glyph plots. Our glyph shows the variation in uncertainty, and provides fair comparisons of data from instruments, models, and time averages of varying certainty. We also define visualizations that incorporate uncertainty in an unambiguous manner as verity visualization. We use both quantitative and qualitative methods to compare our glyphs to traditional ones. Subjective comparison tests with experts are provided, as well as objective tests, where the information density of our new glyphs and traditional glyphs are compared. The design of the glyph and numerous examples using environmental data are given. We show enhanced visualizations, data together with their uncertainty information, that may improve understanding of environmental vector field data quality.","Data visualization,
Graphics,
Displays,
Interpolation,
Sea measurements,
Measurement uncertainty,
Pipelines,
Computer science,
Extraterrestrial measurements,
Ink"
Acting under uncertainty: discrete Bayesian models for mobile-robot navigation,"Discrete Bayesian models have been used to model uncertainty for mobile-robot navigation, but the question of how actions should be chosen remains largely unexplored. This paper presents the optimal solution to the problem, formulated as a partially observable Markov decision process. Since solving for the optimal control policy is intractable, in general, it goes on to explore a variety of heuristic control strategies. The control strategies are compared experimentally, both in simulation and in runs on a robot.",
Collaboratories: doing science on the Internet,"The success of many complex scientific investigations hinges on bringing the capabilities of diverse individuals from multiple institutions together with state-of-the-art instrumentation. Computer scientists working with domain specialists have made progress on several fronts to create and integrate the tools required for Internet-based scientific collaboration. However, both technical and sociological challenges remain. The tools of computer-supported cooperative work are now being applied to such collaborations. Through immersive electronic interaction, team members distributed across a widespread area can collaborate, using the newest instruments and computing resources. The paper discusses some collaboratory prototypes and considers the sociology of collaboration.","Collaboration,
Internet,
Collaborative work,
Instruments,
Collaborative tools,
Fasteners,
Charge carrier processes,
Distributed computing,
Prototypes,
Sociology"
Area-time-power tradeoffs in parallel adders,"In this paper, several classes of parallel, synchronous adders are surveyed based on their power, delay and area characteristics. The adders studied include the linear time ripple carry and Manchester carry chain adders, the square-root time carry skip and carry select adders, the logarithmic time carry lookahead adder and its variations, and the constant time signed-digit and carry-save adders. Most of the research in the last few decades has concentrated on reducing the delay of addition. With the rising popularity of portable computers, however, the emphasis is on both high speed and low power operation. In this paper we adopt a uniform static CMOS layout methodology whereby short circuit power mininization is used as the optimization criterion. The relative merits of the different adders are evaluated by performing a detailed transistor-level simulation of the adders using HSPICE. Among the two's complement adders, a variation of the carry lookahead adder, called ELM, was found to have the best power-delay product. Based on the results of our experiments, a large adder design space is formulated from which an architect can choose an adder with the desired characteristics.","Adders,
Circuits,
Digital signal processing,
Cost function,
Computer science,
Added delay,
Portable computers,
Minimization methods,
Performance evaluation,
Computational modeling"
"An immunological approach to change detection: algorithms, analysis and implications","We present new results on a distributable change-detection method inspired by the natural immune system. A weakness in the original algorithm was the exponential cost of generating detectors. Two detector-generating algorithms are introduced which run in linear time. The algorithms are analyzed, heuristics are given for setting parameters based on the analysis, and the presence of holes in detector space is examined. The analysis provider a basis for assessing the practicality of the algorithms in specific settings, and some of the implications are discussed.","Change detection algorithms,
Algorithm design and analysis,
Detectors,
Protection,
Object detection,
Computer science,
Immune system,
Costs,
Heuristic algorithms,
Computer security"
A computational algorithm for minimizing total variation in image restoration,"A reliable and efficient computational algorithm for restoring blurred and noisy images is proposed. The restoration process is based on the minimal total variation principle introduced by Rudin et al. For discrete images, the proposed algorithm minimizes a piecewise linear l/sub 1/ function (a measure of total variation) subject to a single 2-norm inequality constraint (a measure of data fit). The algorithm starts by finding a feasible point for the inequality constraint using a (partial) conjugate gradient method. This corresponds to a deblurring process. Noise and other artifacts are removed by a subsequent total variation minimization process. The use of the linear l/sub 1/ objective function for the total variation measurement leads to a simpler computational algorithm. Both the steepest descent and an affine scaling Newton method are considered to solve this constrained piecewise linear l/sub 1/ minimization problem. The resulting algorithm, when viewed as an image restoration and enhancement process, has the feature that it can be used in an adaptive/interactive manner in situations when knowledge of the noise variance is either unavailable or unreliable. Numerical examples are presented to demonstrate the effectiveness of the proposed iterative image restoration and enhancement process.",
Learning Bayesian network structures by searching for the best ordering with genetic algorithms,"Presents a new methodology for inducing Bayesian network structures from a database of cases. The methodology is based on searching for the best ordering of the system variables by means of genetic algorithms. Since this problem of finding an optimal ordering of variables resembles the traveling salesman problem, the authors use genetic operators that were developed for the latter problem. The quality of a variable ordering is evaluated with the structure-learning algorithm K2. The authors present empirical results that were obtained with a simulation of the ALARM network.","Bayesian methods,
Genetic algorithms,
Databases,
Traveling salesman problems,
Random variables,
Probability distribution,
Graphical models,
Computer science,
Artificial intelligence,
Humans"
Word spotting: a new approach to indexing handwriting,"There are many historical manuscripts written in a single hand which it would be useful to index. Examples include the W.B. DuBois collection at the University of Massachusetts and the early Presidential libraries at the Library of Congress. Since Optical Character Recognition (OCR) does not work well on handwriting, an alternative scheme based on matching the images of the words is proposed for indexing such texts. The current paper deals with the matching aspects of this process. Two different techniques for matching words are discussed. The first method matches words assuming that the transformation between the words may be modelled by a translation (shift). The second method matches words assuming that the transformation between the words may be modelled by an affine transform. Experiments are shown demonstrating the feasibility of the approach for indexing handwriting. The method should also be applicable to retrieving previously stored material from personal digital assistants (PDAs).",
Image metamorphosis with scattered feature constraints,"This paper describes an image metamorphosis technique to handle scattered feature constraints specified with points, polylines, and splines. Solutions to the following three problems are presented: feature specification, warp generation, and transition control. We demonstrate the use of snakes to reduce the burden of feature specification. Next, we propose the use of multilevel free-form deformations (MFFD) to compute C/sup 2/-continuous and one-to-one mapping functions among the specified features. The resulting technique, based on B-spline approximation, is simpler and faster than previous warp generation methods. Furthermore, it produces smooth image transformations without undesirable ripples and foldovers. Finally, we simplify the MFFD algorithm to derive transition functions to control geometry and color blending. Implementation details are furnished and comparisons among various metamorphosis techniques are presented.","Scattering,
Interpolation,
Computer science,
Animation,
Spline,
Geometry,
Visual effects,
TV,
Digital images,
Color"
A trace-based approach for modeling wireless channel behavior,"The loss behavior of wireless networks has become the focus of many recent research efforts. Although it is generally agreed that wireless communications experience higher error rates than wireline, the nature of these lossy links is not fully understood. This paper describes an effort to characterize the loss behavior of the AT&T WaveLAN, a popular in-building wireless interface. Using a trace-based approach, packet loss information is recorded, analyzed, and validated. Our results indicate that WaveLAN experiences an average packet error rate of 2 to 3 percent. Further analysis reveals that these errors are not independent, making it hard to model them with a simple two-state Markov chain. We derive another model based on the distributions of the error and error-free length of the packet streams. For validation, we modulate both the error models and the traces in a simulator. Trace-driven simulations yield an average TCP throughput of about 5 percent less than simulations using our best error model.","Error analysis,
Computer errors,
Testing,
Wireless networks,
Computational modeling,
Computer science,
Throughput,
Wireless LAN,
Protocols,
Receivers"
A formal framework for on-line software version change,"The usual way of installing a new version of a software system is to shut down the running program and then install the new version. This necessitates a sometimes unacceptable delay during which service is denied to the users of the software. An online software replacement system replaces parts of the software while it is in execution, thus eliminating the shutdown. While a number of implementations of online version change systems have been described in the literature, little investigation has been done on its theoretical aspects. We describe a formal framework for studying online software version change. We give a general definition of validity of an online change, show that it is in general undecidable and then develop sufficient conditions for ensuring validity for a procedural language.","Software systems,
Delay,
Senior members,
Sufficient conditions,
Data analysis,
Software development management,
Computer science,
Electronic switching systems"
Genetic algorithms with dynamic niche sharing for multimodal function optimization,"Genetic algorithms utilize populations of individual hypotheses that converge over time to a single optimum, even within a multimodal domain. This paper examines methods that enable genetic algorithms to identify multiple optima within multimodal domains by maintaining population members within the niches defined by the multiple optima. A new mechanism, dynamic niche sharing, is developed that is able to efficiently identify and search multiple niches (peaks) in a multimodal domain. Dynamic niche sharing is shown to perform better than two other methods for multiple optima identification, standard sharing and deterministic crowding.",
An unbiased parametric imaging algorithm for nonuniformly sampled biomedical system parameter estimation,"An unbiased algorithm of generalized linear least squares (GLLS) for parameter estimation of nonuniformly sampled biomedical systems is proposed. The basic theory and detailed derivation of the algorithm are given. This algorithm removes the initial values required and computational burden of nonlinear least regression and achieves a comparable estimation quality in terms of the estimates' bias and standard deviation. Therefore, this algorithm is particular useful in image-wide (pixel-by-pixel based) parameter estimation, e.g., to generate parametric images from tracer dynamic studies with positron emission tomography. An example is presented to demonstrate the performance of this new technique. This algorithm is also generally applicable to other continuous system parameter estimation.","Biomedical imaging,
Parameter estimation,
Positron emission tomography,
Least squares approximation,
Pixel,
Biomedical measurements,
Integrated circuit modeling,
High-resolution imaging,
Steady-state,
Computer science"
General decidability theorems for infinite-state systems,"Over the last few years there has been an increasing research effort directed towards the automatic verification of infinite state systems. This paper is concerned with identifying general mathematical structures which can serve as sufficient conditions for achieving decidability. We present decidability results for a class of systems (called well-structured systems), which consist of a finite control part operating on an infinite data domain. The results assume that the data domain is equipped with a well-ordered and well-founded preorder such that the transition relation is ""monotonic"" (is a simulation) with respect to the preorder. We show that the following properties are decidable for well-structured systems: reachability; eventuality; and simulation. We also describe how these general principles subsume several decidability results from the literature about timed automata, relational automata, Petri nets, and lossy channel systems.",
Predictive sequential associative cache,"In this paper we propose a cache design that provides the same miss rate as a two-way set associative cache, but with an access time closer to a direct-mapped cache. As with other designs, a traditional direct-mapped cache is conceptually partitioned into multiple banks, and the blocks in each set are probed, or examined, sequentially. Other designs either probe the set in a fixed order or add extra delay in the access path for all accesses. We use prediction sources to guide the cache examination, reducing the amount of searching and thus the average access latency. A variety of accurate prediction sources are considered, with some being available in early pipeline stages. We feel that our design offers the same or better performance and is easier to implement than previous designs.",
Robust classification of hand postures against complex backgrounds,"A system for the classification of hand postures against complex backgrounds in grey-level images is presented. The system employs elastic graph matching, which has already been successfully employed for the recognition of faces. Our system reaches 86.2% correct classification on our gallery of 239 images of ten postures against complex backgrounds. The system is robust with respect to certain variations in size of hand and shape of posture.","Robustness,
Gabor filters,
Kernel,
Shape,
Humans,
Intelligent robots,
Neurons,
Computer science,
User interfaces,
Keyboards"
A parametric model for fusing heterogeneous fuzzy data,"Presented is a model that integrates three data types (numbers, intervals, and linguistic assessments). Data of these three types come from a variety of sensors. One objective of sensor-fusion models is to provide a common framework for data integration, processing, and interpretation. That is what our model does. We use a small set of artificial data to illustrate how problems as diverse as feature analysis, clustering, cluster validity, and prototype classifier design can all be formulated and attacked with standard methods once the data are converted to the generalized coordinates of our model. The effects of reparameterization on computational outputs are discussed. Numerical examples illustrate that the proposed model affords a natural way to approach problems which involve mixed data types.","Parametric statistics,
Magnetic sensors,
Fuzzy sets,
Humans,
Image sensors,
Biosensors,
Sensor phenomena and characterization,
Sensor systems,
Computer science,
Radar cross section"
Observer: an approach for query processing in global information systems based on interoperation across pre-existing ontologies,"The huge number of autonomous and heterogeneous data repositories accessible on the ""global information infrastructure"" makes it impossible for users to be aware of the locations structure/organization, query languages and semantics of the data in various repositories. There is a critical need to complement current browsing, navigational and information retrieval techniques with a strategy that focuses on information content and semantics. In any strategy that focuses on information content, the most critical problem is that of different vocabularies used to describe similar information across domains. We discuss a scalable approach for vocabulary sharing. The objects in the repositories are represented as intensional descriptions by preexisting ontologies expressed in Description Logics characterizing information in different domains. User queries are rewritten by using interontology relationships to obtain semantics preserving translations across the ontologies.","Query processing,
Information systems,
Ontologies,
Navigation,
Database languages,
Information retrieval,
Vocabulary,
Logic,
Information filtering,
Computer science"
On the expected number of failures detected by subdomain testing and random testing,"We investigate the efficacy of subdomain testing and random testing using the expected number of failures detected (the E-measure) as a measure of effectiveness. Simple as it is, the E-measure does provide a great deal of useful information about the fault detecting capability of testing strategies. With the E-measure, we obtain new characterizations of subdomain testing, including several new conditions that determine whether subdomain testing is more or less effective than random testing. Previously, the efficacy of subdomain testing strategies has been analyzed using the probability of detecting at least one failure (the P-measure) for the special case of disjoint subdomains only. On the contrary, our analysis makes use of the E-measure and considers also the general case in which subdomains may or may not overlap. Furthermore, we discover important relations between the two different measures. From these relations, we also derive corresponding characterizations of subdomain testing in terms of the P-measure.",
Data mining for path traversal patterns in a web environment,"In this paper, we explore a new data mining capability which involved mining path traversal patterns in a distributed information providing environment like world-wide-web. First, we convert the original sequence of log data into a set of maximal forward references and filter out the effect of some backward references which are mainly made for ease of traveling. Second, we derive algorithms to determine the frequent traversal patterns, i.e., large reference sequences, from the maximal forward references obtained. Two algorithms are devised for determining large reference sequences: one is based on some hashing and pruning techniques, and the other is further improved with the option of determining large reference sequences in batch so as to reduce the number of database scans required. Performance of these two methods is comparatively analyzed.",
Identification of dynamic comprehension processes during large scale maintenance,We present results of observing professional maintenance engineers working with industrial code at actual maintenance tasks. Protocol analysis is used to explore how code understanding might differ for small versus large scale code. The experiment confirms that cognition processes work at all levels of abstraction simultaneously as programmers build a mental model of the code. Analysis focused on dynamic properties and processes of code understanding. Cognition processes emerged at three levels of aggregation representing lower and higher level strategies of understanding. They show differences in what triggers them and how they achieve their goals. Results are useful for defining information which maintenance engineers need for their work and for documentation and development standards.,"Large-scale systems,
Cognition,
Protocols,
Software maintenance,
Programming profession,
Switches,
Maintenance engineering,
Cognitive science,
Documentation,
Standards development"
Querying the World Wide Web,"The World Wide Web is a large, heterogeneous, distributed collection of documents connected by hypertext links. The most common technology currently used for searching the Web depends on sending information retrieval requests to ""index servers"". One problem with this is that these queries cannot exploit the structure and topology of the document network. The authors propose a query language, WebSQL, that takes advantage of multiple index servers without requiring users to know about them, and that integrates textual retrieval with structure and topology-based queries. They give a formal semantics for WebSQL using a calculus based on a novel ""virtual graph"" model of a document network. They propose a new theory of query cost based on the idea of ""query locality,"" that is, how much of the network must be visited to answer a particular query. Finally, they describe a prototype implementation of WebSQL written in Java.","Web sites,
Navigation,
Computer science,
Network servers,
Network topology,
Costs,
Database languages,
Calculus,
Java,
Search engines"
Virtual structures for high-precision cooperative mobile robotic control,"A key problem in cooperative robotics is the maintenance of a geometric configuration during movement. To address this problem, the concept of a virtual structure is introduced. Control methods are developed to force an ensemble of robots to behave as if they were particles embedded in a rigid structure. The method was tested both using simulation and experimentation with a set of three robots. Results are presented which demonstrate that this approach is capable of achieving high precision movement which is fault tolerant and exhibits graceful degradation of performance. In addition, this algorithm does not require leader selection as in other cooperative robotic strategies. Finally, the method is highly flexible in the kinds of geometric formations that can be maintained.","Mobile robots,
Robot control,
Robot kinematics,
Motion control,
Transportation,
Degradation,
Computer science,
Force control,
Testing,
Orbital robotics"
Spatio-temporal indexing for large multimedia applications,"Multimedia applications usually involve a large number of multimedia objects (texts, images, sounds, etc.). Spatial and temporal relationships among these objects should be efficiently supported and retrieved within a multimedia authoring tool. In this paper, we present several spatial, temporal and spatio-temporal relationships of interest, and propose efficient indexing schemes, based on multidimensional (spatial) data structures, for large multimedia applications that involve thousands of objects. Evaluation models of the proposed schemes are also presented, as well as hints for the selection of the most appropriate one, according to the multimedia author's requirements.","Indexing,
Computer science,
Acoustical engineering,
Shape,
Application software,
Data structures,
Geographic Information Systems,
Design automation,
Very large scale integration,
Multimedia databases"
A rapid supervised learning neural network for function interpolation and approximation,"This paper presents a neural-network architecture and an instant learning algorithm that rapidly decides the weights of the designed single-hidden layer neural network. For an n-dimensional N-pattern training set, with a constant bias, a maximum of N-r-1 hidden nodes is required to learn the mapping within a given precision (where r is the rank, usually the dimension, of the input patterns). For off-line training, the proposed network and algorithm is able to achieve ""one-shot"" training as opposed to most iterative training algorithms in the literature. An online training algorithm is also presented. Similar to most of the backpropagation type of learning algorithms, the given algorithm also interpolates the training data. To eliminate outlier data which may appear in some erroneous training data, a robust weighted least squares method is proposed. The robust weighted least squares learning algorithm can eliminate outlier samples and the algorithm approximates the training data rather than interpolates them. The advantage of the designed network architecture is also mathematically proved. Several experiments show very promising results.",
Cosmic and terrestrial single-event radiation effects in dynamic random access memories,"A review of the literature on single-event radiation effects (SEE) on MOS integrated-circuit dynamic random access memories (DRAMs) is presented. The sources of single-event (SE) radiation particles, causes of circuit information loss, experimental observations of SE information upset, technological developments for error mitigation, and relationships of developmental trends to SE vulnerability are discussed.","Radiation effects,
Random access memory,
Space technology,
Computer errors,
Error correction,
DRAM chips,
Ionizing radiation,
Electronic circuits,
Alpha particles,
Single event upset"
Learning task-dependent distributed representations by backpropagation through structure,"While neural networks are very successfully applied to the processing of fixed-length vectors and variable-length sequences, the current state of the art does not allow the efficient processing of structured objects of arbitrary shape (like logical terms, trees or graphs). We present a connectionist architecture together with a novel supervised learning scheme which is capable of solving inductive inference tasks on complex symbolic structures of arbitrary size. The most general structures that can be handled are labeled directed acyclic graphs. The major difference of our approach compared to others is that the structure-representations are exclusively tuned for the intended inference task. Our method is applied to tasks consisting in the classification of logical terms. These range from the detection of a certain subterm to the satisfaction of a specific unification pattern. Compared to previously known approaches we obtained superior results in that domain.","Backpropagation,
Computer science,
Tree graphs,
Supervised learning,
Data structures,
Labeling,
Information processing,
Neural networks,
Shape,
Detectors"
Feasibility analysis of fault-tolerant real-time task sets,"Many safety critical real-time systems, employ fault tolerant strategies in order to provide predictable performance in the presence of failures. One technique commonly employed is time redundancy using retry/re-execution of tasks. This can in turn affect the correctness of the system by causing deadlines to be missed. This paper provides exact schedulability tests for fault tolerant task sets under specified failure hypothesis.",
Advanced transaction models in workflow contexts,"In recent years, numerous transaction models have been proposed to address the problems posed by advanced database applications, but only a few of these models are being used in commercial products. In this paper, we make the case that such models may be too centered around databases to be useful in real environments. Advanced applications raise a variety of issues that are not addressed at all by transaction models. These same issues, however, are the basis for existing workflow systems, which are having considerable success as commercial products in spite of not having a solid theoretical foundation. We explore some of these issues and show that, in many aspects, workflow models are a superset of transaction models and have the added advantage of incorporating a variety of ideas that have so far remained outside the scope of traditional transaction processing.","Context modeling,
Transaction databases,
Computer science,
Business,
Intelligent networks,
Information systems,
Solids,
Workflow management software"
Combined MRI-PET scanner: a Monte Carlo evaluation of the improvements in PET resolution due to the effects of a static homogeneous magnetic field,"Positron emission tomography (PET) relies upon the detection of photons resulting from the annihilation of positrons emitted by a radiopharmaceutical. The combination of images obtained with PET and magnetic resonance imaging (MRI) have begun to greatly enhance the study of many physiological processes. A combined MRI-PET scanner could alleviate much of the spatial and temporal coregistration difficulties currently encountered in utilizing images from these complementary imaging modalities. In addition, the resolution of the PET scanner could be improved by the effects of the magnetic field. In this computer study, the utilization of a strong static homogeneous magnetic field to increase PET resolution by reducing the effects of positron range and photon noncollinearity was investigated, The results reveal that significant enhancement of resolution can be attained, For example, an approximately 27% increase in resolution is predicted for a PET scanner incorporating a 10-Tesla magnetic field. Most of this gain in resolution is due to magnetic confinement of the emitted positrons. Although the magnetic field does mix some positronium states resulting in slightly less photon noncollinearity, this reduction does not significantly affect resolution. Photon noncollinearity remains as the fundamental limiting factor of large PET scanner resolution.","Monte Carlo methods,
Positron emission tomography,
Magnetic resonance imaging,
Spatial resolution,
Energy resolution,
Magnetic cores,
Magnetic fields,
Physics,
Image resolution,
Single photon emission computed tomography"
Algorithms for address assignment in DSP code generation,"This paper presents DSP code optimization techniques, which originate from dedicated memory address generation hardware. We define a generic model of DSP address generation units. Based on this model we present efficient heuristics for computing memory layouts for program variables, which optimize utilization of parallel address generation units. Improvements and generalizations of previous work are described, and the efficacy of the proposed algorithms is demonstrated through experimental evaluation.","Digital signal processing,
Program processors,
Hardware,
Concurrent computing,
Semiconductor optical amplifiers,
Registers,
Computer science,
Software algorithms,
Algorithm design and analysis,
Very large scale integration"
Correction of computed tomography motion artifacts using pixel-specific back-projection,"Cardiac and respiratory motion can cause artifacts in computed tomography scans of the chest. The authors describe a new method for reducing these artifacts called pixel-specific back-projection (PSBP). PSBP reduces artifacts caused by in-plane motion by reconstructing each pixel in a frame of reference that moves with the in-plane motion in the volume being scanned. The motion of the frame of reference is specified by constructing maps that describe the motion of each pixel in the image at the time each projection was measured; these maps are based on measurements of the in-plane motion. PSBP has been tested in computer simulations and with volunteer data. In computer simulations, PSBP removed the structured artifacts caused by motion. In scans of two volunteers, PSBP reduced doubling and streaking in chest scans to a level that made the images clinically useful. PSBP corrections of liver scans were less satisfactory because the motion of the liver is predominantly superior-inferior (S-I). PSBP uses a unique set of motion parameters to describe the motion at each point in the chest as opposed to requiring that the motion be described by a single set of parameters. Therefore, PSBP may be more useful in correcting clinical scans than are other correction techniques previously described.","Computed tomography,
Image reconstruction,
Motion measurement,
Biomedical imaging,
Time measurement,
Computer simulation,
Liver,
Biomedical engineering,
Pixel,
Testing"
Octree-based decimation of marching cubes surfaces,"The marching cubes (MC) algorithm is a method for generating isosurfaces. It also generates an excessively large number of triangles to represent an isosurface; this increases the rendering time. This paper presents a decimation method to reduce the number of triangles generated. Decimation is carried out before creating a large number of triangles. Four major steps comprise the algorithm: surface tracking, merging, crack patching and triangulation. Surface tracking is an enhanced implementation of the MC algorithm. Starting from a seed point, the surface tracker visits only those cells likely to compose part of the desired isosurface. The cells making up the extracted surface are stored in an octree that is further processed. A bottom-up approach is taken in merging the cells containing a relatively flat approximating surface. The finer surface details are maintained. Cells are merged as long as the error due to such an operation is within a user-specified error parameter, or a cell acquires more than one connected surface component in it. A crack patching method is described that forces edges of smaller cells to lie along those of the larger neighboring cells. The overall saving in the number of triangles depends both on the specified error value and the nature of the data. Use of the hierarchical octree data structure also presents the potential of incremental representation of surfaces. We can generate a highly smoothed surface representation which can be progressively refined as the user-specified error value is decreased.","Surface cracks,
Isosurfaces,
Biomedical engineering,
Biomedical computing,
Information science,
Merging,
Data visualization,
Frequency,
Magnetic resonance imaging,
Head"
From scientific software libraries to problem-solving environments,"As more scientists and engineers adopt computation as a primary tool, they will want more problem-solving help from easy-to-use, comprehensive software systems. A workshop discussed the long path to this vision of scientific software's future, and the roadblocks in the way. In order to understand the findings of the workshop, the paper presents some background on software libraries and problem solving environments.","Software libraries,
Problem-solving,
Software prototyping,
Distributed computing,
Parallel processing,
Aircraft manufacture,
Scalability,
Product development,
Flexible manufacturing systems,
Prototypes"
Vector order statistics operators as color edge detectors,"Color edge detection is approached in this paper using vector order statistics. Based on the R-ordering method, a class of color edge detectors is defined. These detectors function as vector operators as opposed to component-wise operators. Specific edge detectors can be obtained as special cases of this class. Various such detectors are defined and analyzed. Experimental results show the noise robustness of the vector order statistics operators. A quantitative evaluation and comparison to other color edge detectors favors our approach. Edge detection results obtained from real color images demonstrate the effectiveness of the proposed approach in real applications.","Statistics,
Image edge detection,
Detectors,
Color,
Colored noise,
Computer science,
Space technology,
Noise robustness,
Lighting,
Reflectivity"
Choosing effective colours for data visualization,"We describe a technique for choosing multiple colours for use during data visualization. Our goal is a systematic method for maximizing the total number of colours available for use, while still allowing an observer to rapidly and accurately search a display for any one of the given colours. Previous research suggests that we need to consider three separate effects during colour selection: colour distance, linear separation, and colour category. We describe a simple method for measuring and controlling all of these effects. Our method was tested by performing a set of target identification studies; we analysed the ability of thirty eight observers to find a colour target in displays that contained differently coloured background elements. Results showed our method can be used to select a group of colours that will provide good differentiation between data elements during data visualization.","Data visualization,
Computer displays,
Computer graphics,
Computer science,
Ergonomics,
Guidelines,
Military computing,
Testing,
Performance evaluation,
Performance analysis"
Respiratory compensation in projection imaging using a magnification and displacement model,"Respiratory motion during the collection of computed tomography (CT) projections generates structured artifacts and a loss of resolution that can render the scans unusable. This motion is problematic in scans of those patients who cannot suspend respiration, such as the very young or intubated patients. Here, the authors present an algorithm that can be used to reduce motion artifacts in CT scans caused by respiration. An approximate model for the effect of respiration is that the object cross section under interrogation experiences time-varying magnification and displacement along two axes. Using this model an exact filtered backprojection algorithm is derived for the case of parallel projections. The result is extended to generate an approximate reconstruction formula for fan-beam projections. Computer simulations and scans of phantoms on a commercial CT scanner validate the new reconstruction algorithms for parallel and fan-beam projections. Significant reduction in respiratory artifacts is demonstrated clinically when the motion model is satisfied. The method can be applied to projection data used in CT, single photon emission computed tomography (SPECT), positron emission tomography (PET), and magnetic resonance imaging (MRI).","Computed tomography,
X-ray imaging,
Magnetic resonance imaging,
Data acquisition,
Image reconstruction,
Reconstruction algorithms,
Biomedical imaging,
Radiology,
Computer simulation,
Imaging phantoms"
A case study in applying a systematic method for COTS selection,"This paper describes a case study that used and evaluated key aspects of a method developed for systematic reusable off-the-shelf software selection. The paper presents a summary of the common problems in reusable off-the-shelf software selection, describes the method used and provides details about the case study carried out. The case study indicated that the evaluated aspects of the method are feasible, improve the quality and efficiency of reusable software selection and the decision makers have more confidence in the evaluation results, compared to traditional approaches. Furthermore, the case study also showed that the choice of evaluation data analysis method can influence the evaluation results.","Computer aided software engineering,
Software reusability,
Programming,
Embedded software,
Data analysis,
Decision making,
Information technology,
Computer science,
Educational institutions,
World Wide Web"
"Skin and bones: multi-layer, locally affine, optical flow and regularization with transparency","This paper describes a new method for estimating optical flow that strikes a balance between the flexibility of local dense computations and the robustness and accuracy of global parameterized flow models. An affine model of image motion is used within local image patches while a spatial smoothness constraint on the affine flow parameters of neighboring patches enforces continuity of the motion. We refer to this as a ""Skin and Bones"" model in which the affine patches can be thought of as rigid ""bones"" connected by a flexible ""skin"". Since local image patches may contain multiple motions we use a layered representation for the affine bones. To regularize this layered motion representation we develop a new framework for regularization with transparency.","Skin,
Bones,
Image motion analysis,
Motion estimation,
Robustness,
Computer science,
Optical computing,
Area measurement,
Fluid flow measurement,
Tiles"
Isosurfacing in span space with utmost efficiency (ISSUE),"We present efficient sequential and parallel algorithms for isosurface extraction. Based on the Span Space data representation, new data subdivision and searching methods are described. We also present a parallel implementation with an emphasis on load balancing. The performance of our sequential algorithm to locate the cell elements intersected by isosurfaces is faster than the Kd tree searching method originally used for the Span Space algorithm. The parallel algorithm can achieve high load balancing for massively parallel machines with distributed memory architectures.","Isosurfaces,
Data visualization,
Workstations,
Laboratories,
Computer science,
Cities and towns,
Data mining,
High performance computing,
Large-scale systems,
Computer graphics"
A new methodology for gray-scale character segmentation and recognition,"Generally speaking, through the binarization of gray-scale images, useful information for the segmentation of touched or overlapped characters may be lost in many cases. If we analyze gray-scale images, however, specific topographic features and the variation of intensities can be observed in the character boundaries. In this paper, we propose a new methodology for character segmentation and recognition which makes the best use of the characteristics of gray-scale images. In the proposed methodology, the character segmentation regions are determined by using projection profiles and topographic features extracted from the gray-scale images. Then a nonlinear character segmentation path in each character segmentation region is found by using multi-stage graph search algorithm. Finally, in order to confirm the nonlinear character segmentation paths and recognition results, a recognition-based segmentation method is adopted. Through the experiments with various kinds of printed documents, it is convinced that the proposed methodology is very effective for the segmentation and recognition of touched and overlapped characters.",
SWEB: towards a scalable World Wide Web server on multicomputers,"We investigate the issues involved in developing a scalable World Wide Web (WWW) server on a cluster of workstations and parallel machines. The objective is to strengthen the processing capabilities of such a server by utilizing the power of multicomputers to match huge demands in simultaneous access requests from the Internet. We have implemented a system called SWEB on a distributed memory machine, the Meiko CS-2, and networked workstations. The scheduling component of the system actively monitors the usages of CPU, I/O channels and the interconnection network to effectively distribute HTTP requests across processing units to exploit task and I/O parallelism. We present the experimental results on the performance of this system.","Web sites,
Web server,
Workstations,
World Wide Web,
Parallel machines,
Network servers,
Internet,
Central Processing Unit,
Multiprocessor interconnection networks"
Technology mapping for TLU FPGAs based on decomposition of binary decision diagrams,"This paper proposes an efficient algorithm for technology mapping targeting table look-up (TLU) blocks. It is capable of minimizing either the number of TLUs used or the depth of the produced circuit. Our approach consists of two steps. First a network of super nodes, is created. Next a Boolean function of each super node with an appropriate don't care set is decomposed into a network of TLUs. To minimize the circuit's depth, several rules are applied on the critical portion of the mapped circuit.","Field programmable gate arrays,
Data structures,
Boolean functions,
Circuits,
Programmable logic arrays,
Table lookup,
Computer science,
Routing,
Logic design,
Network synthesis"
Automated analysis of nerve-cell images using active contour models,"The number of nerve fibers (axons) in a nerve, the axon size, and shape can all be important neuroanatomical features in understanding different aspects of nerves in the brain. However, the number of axons in a nerve is typically in the order of tens of thousands and a study of a particular aspect of the nerve often involves many nerves. Potentially meaningful studies are often prohibited by the huge number involved when manual measurements have to be employed. A method that automates the analysis of axons from electron-micrographic images is presented. It begins with a rough identification of all the axon centers by use of an elliptical Hough transform procedure. Boundaries of each axons are then extracted based on active contour model, or snakes, approach where physical properties of the axons and the given image data are used in an optimization scheme to guide the snakes to converge to axon boundaries for accurate sheath measurement. However, false axon detection is still common due to poor image quality and the presence of other irrelevant cell features, thus a conflict resolution scheme is developed to eliminate false axons to further improve the performance of detection. The developed method has been tested on a number of nerve images and its results are presented.","Image analysis,
Active contours,
Nerve fibers,
Neurons,
Computer science,
Brain modeling,
Active shape model,
Data mining,
Image converters,
Image quality"
Approximate queries and representations for large data sequences,"Many new database application domains such as experimental sciences and medicine are characterized by large sequences as their main form of data. Using approximate representation can significantly reduce the required storage and search space. A good choice of representation, can support a broad new class of approximate queries, needed in there domains. These queries are concerned with application dependent features of the data as opposed to the actual sampled points. We introduce a new notion of generalized approximate queries and a general divide and conquer approach that supports them. This approach uses families of real-valued functions as an approximate representation. We present an algorithm for realizing our technique, and the results of applying it to medical cardiology data.","Databases,
Solids,
Shape,
Contracts,
Computer science,
Application software,
Cardiology,
Tiles,
Subcontracting,
Pattern matching"
Recursive implementation of erosions and dilations along discrete lines at arbitrary angles,"Van Herk (1992) has shown that the erosion/dilation operator with a linear structuring element of an arbitrary length can be implemented in only three min/max operations per pixel. In this paper, the algorithm is generalized to erosions and dilations along discrete lines at arbitrary angles. We also address the padding problem; so that the operation can be performed in place without copying the pixels to and from an intermediate buffer. Applications to image filtering and to radial decompositions of discs are presented.","Gray-scale,
Filtering algorithms,
Electronic mail,
Minimax techniques,
Structural discs,
Morphology,
Morphological operations,
Mathematics,
Information science,
Australia"
Using computer models to understand the roles of tissue structure and membrane dynamics in arrhythmogenesis,"The merging of hypotheses and techniques from physics, mathematics, biomedical engineering, cardiology, and computer science is helping to form increasingly more realistic computer models of the heart. These models complement experimental and clinical studies that seek to elucidate the mechanisms of arrhythmogenesis and improve pharmacological and electrical therapies. This paper reviews the current state of the art of computer models for investigating normal and abnormal conduction in cardiac muscle. A brief introduction to the mathematical foundations of continuous (monodomain and bidomain) and discrete tissue structure models and to ionic current based and FitzHugh-Nagumo membrane models is presented. The paper summarizes some of the recent contributions in validating tissue structure models, modeling unidirectional block and reentry in a 1-D loop, and applying generic spiral wave theory to cardiac arrhythmias.","Biomedical computing,
Mathematical model,
Merging,
Mathematics,
Biomedical engineering,
Cardiology,
Computer science,
Physics computing,
Heart,
Medical treatment"
Digital mammography: mixed feature neural network with spectral entropy decision for detection of microcalcifications,"A computationally efficient mixed feature based neural network (MFNN) is proposed for the detection of microcalcification clusters (MCCs) in digitized mammograms. The MFNN employs features computed in both the spatial and spectral domain and uses spectral entropy as a decision parameter. Backpropagation with Kalman filtering (KF) is employed to allow more efficient network training as required for evaluation of different features, input images, and related error analysis. A previously reported, wavelet-based image-enhancement method is also employed to enhance microcalcification clusters for improved detection. The relative performance of the MFNN for both the raw and enhanced images is evaluated using a common image database of 30 digitized mammograms, with 20 images containing 21 biopsy proven MCCs and ten normal cases. The computed sensitivity (true positive (TP) detection rate) was 90.1% with an average low false positive (FP) detection of 0.71 MCCs/image for the enhanced images using a modified k-fold validation error estimation technique. The corresponding computed sensitivity for the raw images was reduced to 81.4% and with 0.59 FP's MCCs/image. A relative comparison to an earlier neural network (NN) design, using only spatially related features, suggests the importance of the addition of spectral domain features when the raw image data is analyzed.","Mammography,
Neural networks,
Error analysis,
Computer networks,
Entropy,
Backpropagation,
Kalman filters,
Filtering,
Image databases,
Biopsy"
Evaluating aggregate operations over imprecise data,"Imprecise data in databases were originally denoted as null values, which represent the meaning of ""values unknown at present."" More generally, a partial value corresponds to a finite set of possible values for an attribute in which exactly one of the values is the ""true"" value. We define a set of extended aggregate operations, namely sum, average, count, maximum, and minimum, which can be applied to an attribute containing partial values. Two types of aggregate operators are considered: scalar aggregates and aggregate functions. We study the properties of the aggregate operations and develop efficient algorithms for count, maximum and minimum. However, for sum and average, we point out that in general it takes exponential time complexity to do the computations.","Aggregates,
Uncertainty,
Computer Society,
Graph theory,
Fuzzy set theory,
Database systems,
Algebra,
Null value,
Computer science,
Information management"
A linear time erasure-resilient code with nearly optimal recovery,"We develop an efficient scheme that produces an encoding of a given message such that the message can be decoded from any portion of the encoding that is approximately equal to the length of the message. More precisely, an (n,c,l,r)-erasure-resilient code consists of an encoding algorithm and a decoding algorithm with the following properties. The encoding algorithm produces a set of l-bit packets of total length cn from an n-bit message. The decoding algorithm is able to recover the message from any set of packets whose total length is r, i.e., from any set of r/l packets. We describe erasure-resilient codes where both the encoding and decoding algorithms run in linear time and where r is only slightly larger than n.","Encoding,
Decoding,
Reed-Solomon codes,
Redundancy,
Protection,
Multicast algorithms,
Computer science,
Algorithm design and analysis,
Propagation losses,
Error correction codes"
Fault-tolerant quantum computation,"It has recently been realized that use of the properties of quantum mechanics might speed up certain computations dramatically. Interest in quantum computation has since been growing. One of the main difficulties in realizing quantum computation is that decoherence tends to destroy the information in a superposition of states in a quantum computer making long computations impossible. A further difficulty is that inaccuracies in quantum state transformations throughout the computation accumulate, rendering long computations unreliable. However, these obstacles may not be as formidable as originally believed. For any quantum computation with t gates, we show how to build a polynomial size quantum circuit that tolerates O(1/log/sup c/t) amounts of inaccuracy and decoherence per gate, for some constant c; the previous bound was O(1/t). We do this by showing that operations can be performed on quantum data encoded by quantum error-correcting codes without decoding this data.","Quantum computing,
Fault tolerance,
Quantum mechanics,
Mechanical factors,
Polynomials,
Computational modeling,
Circuits,
Error correction codes,
Decoding,
Interference"
Theory of fault-based predicate testing for computer programs,"Predicates appear in both the specification and implementation of a program. One approach to software testing, referred to as predicate testing, is to require certain types of tests for a predicate. In this paper, three fault-based testing criteria are defined for compound predicates, which are predicates with one or more AND/OR operators. BOR (boolean operator) testing requires a set of tests to guarantee the detection of (single or multiple) boolean operator faults, including incorrect AND/OR operators and missing/extra NOT operators. BRO (boolean and relational operator) testing requires a set of tests to guarantee the detection of boolean operator faults and relational operator faults (i.e., incorrect relational operators). BRE (boolean and relational expression) testing requires a set of tests to guarantee the detection of boolean operator faults, relational operator faults, and a type of fault involving arithmetical expressions. It is shown that for a compound predicate with n, n>0, AND/OR operators, at most n+2 constraints are needed for BOR testing and at most 2*n+3 constraints for BRO or BRE testing, where each constraint specifies a restriction on the value of each boolean variable or relational expression in the predicate. Algorithms for generating a minimum set of constraints for BOR, BRO, and BRE testing of a compound predicate are given, and the feasibility problem for the generated constraints is discussed. For boolean expressions that contain multiple occurrences of some boolean variables, how to combine BOR testing with the meaningful impact strategy (Weyuker et al., 1994) is described.","Software testing,
Fault detection,
Materials testing,
Software engineering,
Computer science,
Digital arithmetic"
ATM rate based congestion control using a Smith predictor: an EPRCA implementation,"Presents a feedback control algorithm for ATM congestion control in which source rates are adjusted according to VC queue lengths at intermediate nodes along the path. The goal is to ""fill in"" the residual bandwidth, without exceeding a specified queue threshold. In order to obtain this, we propose a simple and classical proportional controller, plus a Smith predictor to overcome instabilities due to large propagation delays, as well as to avoid cell loss. We propose an effective EPRCA implementation in which each source computes its input rate based on the maximum VC queue length along the path. Theoretical and experimental results show that high throughput is achieved even with queue sizes independent of the round trip delay.","Communication system traffic control,
Traffic control,
Stability,
Delay,
Queueing analysis,
Virtual colonoscopy,
Bandwidth,
Computer science,
Feedback control,
Proportional control"
A mew ASR approach based on independent processing and recombination of partial frequency bands,"In the framework of hidden Markov models (HMM) or hybrid HMM/artificial neural network (ANN) systems, we present a new approach towards automatic speech recognition (ASR). The general idea is to split the whole frequency band (represented in terms of critical bands) into a few sub-bands on which different recognizers are independently applied and then recombined at a certain speech unit level to yield global scores and a global recognition decision. The preliminary results presented in this paper show that such an approach, even using quite simple recombination strategies, can yield at least comparable performance on clean speech while providing better robustness in the case of noisy speech.","Automatic speech recognition,
Acoustic noise,
Hidden Markov models,
Speech recognition,
Decoding,
Error analysis,
Frequency synchronization,
System testing,
Computer science,
Neural networks"
Perturb and simplify: multilevel Boolean network optimizer,"In this paper, we present logic optimization techniques for multilevel combinational networks. Our techniques apply a sequence of perturbations which result in simplification of the circuit. The perturbation and simplification is achieved through wires/gates addition and removal which are guided by the Automatic Test Pattern Generation (ATPG) based reasoning. The main operations of our approaches are incremental transformations of the circuit (such as adding wires/gates and changing gate's functionality) to remove some particular wire, At each iteration, a summary information of such wires/gates addition and removal is precomputed first. Then, a transformation is chosen to remove several wires at once. We have performed experiments on MCNC benchmarks and compared the results to those of misII and RAMBO. Experimental results are very encouraging.","Wire,
Logic,
Automatic test pattern generation,
Circuit synthesis,
Network synthesis,
Boolean functions,
Kernel,
Data analysis,
Computer science"
Linearity testing in characteristic two,"Let Dist(f,g)=Pr/sub u/[f(u)/spl ne/g(u)] denote the relative distance between functions f,g mapping from a group G to a group H, and let Dist(f) denote the minimum, over all linear functions (homomorphisms) g, of Dist(f,g). Given a function f:G/spl rarr/H we let Err(f)=Pr/sub u,/spl upsi//[f(u)+f(/spl upsi/)/spl ne/f(u+/spl upsi/)] denote the rejection probability of the Blum-Luby-Rubinfeld (1993) linearity test. Linearity testing is the study of the relationship between Err(f) and Dist(f), and in particular lower bounds on Err(f) in terms of Dist(f). We discuss when the underlying groups are G=GF(2)/sup n/ and H=GF(2). In this case, the collection of linear functions describe a Hadamard code of block length 2/sup n/ and for an arbitrary function f mapping GF(2)/sup n/ to GF(2) the distance Dist(l) measures its distance to a Hadamard code. Err(f) is a parameter that is ""easy to measure"" and linearity testing studies the relationship of this parameter to the distance of f. The code and corresponding test are used in the construction of efficient probabilistically checkable proofs and thence in the derivation of hardness of approximation. Improved analyses translate into better nonapproximability results. We present a description of the relationship between Err(f) and Dist(f) which is nearly complete in all its aspects, and entirely complete in some. We present functions L,U:[0,1]/spl rarr/[0,1] such that for all x /spl isin/ [0,1] we have L(x)/spl les/Err(f)/spl les/U(x) whenever Dist(f)=x, with the upper bound being tight on the whole range, and the lower bound tight on a large part of the range and close on the rest. Part of our strengthening is obtained by showing a new connection between linearity testing and Fourier analysis.","Linearity,
Testing,
Computer science,
Error correction,
Error correction codes,
Length measurement,
Upper bound,
Computer errors,
Scholarships,
Mathematics"
"The Tenet real-time protocol suite: design, implementation, and experiences","Many future applications will require guarantees on network performance, such as bounds on throughput, delay, delay jitter, and reliability. To address this need, the authors have designed, simulated, and implemented a suite of network protocols to support real-time channels (network connections with mathematically provable performance guarantees). The protocols, which constitute the prototype Tenet real-time protocol suite (Suite 1), run on a packet-switching internetwork and can coexist with the popular Internet protocol suite. The authors rely on the use of connection-oriented communication, per-channel admission control, channel rate control, and priority scheduling. This protocol suite is the first set of transport and network-layer communication protocols that can transfer real-time streams with guaranteed quality in packet-switching internetworks. The authors have performed a number of experiments and demonstrations on multiple platforms using continuous-media loads (particularly video). The results show that the approach is both feasible and practical to build, and that it can successfully provide performance guarantees to real-time applications. The paper describes the design and implementation of, the suite, the experiments performed, and some of the lessons learned.","Computer science,
Transport protocols,
Internet,
IP networks,
Throughput,
Jitter,
Prototypes,
Admission control,
Channel rate control,
Scheduling"
Polynomial time approximation schemes for Euclidean TSP and other geometric problems,"We present a polynomial time approximation scheme for Euclidean TSP in /spl Rfr//sup 2/. Given any n nodes in the plane and /spl epsiv/>0, the scheme finds a (1+/spl epsiv/)-approximation to the optimum traveling salesman tour in time n/sup 0(1//spl epsiv/)/. When the nodes are in /spl Rfr//sup d/, the running time increases to n(O/spl tilde/(log/sup d-2/n)//spl epsiv//sup d-1/) The previous best approximation algorithm for the problem (due to Christofides (1976)) achieves a 3/2-approximation in polynomial time. We also give similar approximation schemes for a host of other Euclidean problems, including Steiner Tree, k-TSP, Minimum degree-k, spanning tree, k-MST, etc. (This list may get longer; our techniques are fairly general.) The previous best approximation algorithms for all these problems achieved a constant-factor approximation. All our algorithms also work, with almost no modification, when distance is measured using any geometric norm (such as l/sub p/ for p/spl ges/1 or other Minkowski norms).",
An abstract-device interface for implementing portable parallel-I/O interfaces,"We propose a strategy for implementing parallel I/O interfaces portably and efficiently. We have defined an abstract device interface for parallel I/O, called ADIO. Any parallel I/O API can be implemented on multiple file systems by implementing the API portably on top of ADIO, and implementing only ADIO on different file systems. This approach simplifies the task of implementing an API and yet exploits the specific high performance features of individual file systems. We have used ADIO to implement the Intel PFS interface and subsets of MPI-IO and IBM PIOFS interfaces on PFS, PIOFS, Unix, and NFS file systems. Our performance studies indicate that the overhead of using ADIO as an implementation strategy is very low.","File systems,
Libraries,
Computer science,
Laboratories,
Concurrent computing,
Application software,
Ground penetrating radar,
Geophysical measurement techniques,
Standards development,
Runtime"
Knowledge-based interpretation of MR brain images,"The authors have developed a method for fully automated segmentation and labeling of 17 neuroanatomic structures such as thalamus, caudate nucleus, ventricular system, etc. in magnetic resonance (MR) brain images. The authors' method is based on a hypothesize-and-verify principle and uses a genetic algorithm (GA) optimization technique to generate and evaluate image interpretation hypotheses in a feedback loop. The authors' method was trained in 20 individual T1-weighted MR images. Observer-defined contours of neuroanatomic structures were used as a priori knowledge. The method's performance was validated in eight MR images by comparison to observer-defined independent standards. The GA-based image interpretation method correctly interpreted neuroanatomic structures in all images from the test set. Computer-identified and observer-defined neuroanatomic structure areas correlated very well (r=0.99, y=0,95x-2.1). Border positioning errors were small, with a root mean square (rms) border positioning error of 1.5/spl plusmn/0.6 pixels. The authors' GA-based image interpretation method represents a novel approach to image interpretation and has been shown to produce accurate labeling of neuroanatomic structures in a set of MR brain images.","Brain,
Labeling,
Computer errors,
Image segmentation,
Magnetic resonance,
Genetic algorithms,
Optimization methods,
Image generation,
Feedback loop,
Testing"
Java security: from HotJava to Netscape and beyond,"The introduction of Java applets has taken the World Wide Web by storm. Information servers can customize the presentation of their content with server-supplied code which executes inside the Web browser. We examine the Java language and both the HotJava and Netscape browsers which support it, and find a significant number of flaws which compromise their security. These flaws arise for several reasons, including implementation errors, unintended interactions between browser features, differences between the Java language and bytecode semantics, and weaknesses in the design of the language and the bytecode format. On a deeper level, these flaws arise because of weaknesses in the design methodology used in creating Java and the browsers. In addition to the flaws, we discuss the underlying tension between the openness desired by Web application writers and the security needs of their users, and we suggest how both might be accommodated.","Java,
Sun,
Information security,
Web sites,
HTML,
Navigation,
Memory management,
Computer science,
Storms,
Web server"
Synchronization protocols in distributed real-time systems,"In many distributed real-time systems, the workload can be modeled as a set of periodic tasks, each of which consists of a chain of subtasks executing on different processors. Synchronization protocols are used to govern the release of subtasks so that the precedence constraints among subtasks are satisfied and the schedulability of the resultant system is analyzable. Tasks have different worst-case and average end-to-end response times when different protocols are used. In this paper, we consider distributed real-time systems with independent, periodic tasks and fixed-priority scheduling algorithms. We propose three synchronization protocols and conduct simulation to compare their performance with respect to the two timing aspects.","Protocols,
Real time systems,
Processor scheduling,
Delay,
Monitoring,
Sun,
Computer science,
Computational modeling,
Timing,
Displays"
Optimal image sampling schedule: a new effective way to reduce dynamic image storage space and functional image processing time,"An optimal image sampling schedule for tracer dynamic studies with positron emission tomography (PET) is proposed. This schedule incorporates the characteristics of PET measurement and uses a new cost function and the D-optimal criterion. A detailed case study of the estimation of the local cerebral metabolic rate of glucose (LCMRGLc) using the tracer fluorodeoxyglucose (FDG) and the four-parameter FDG model is presented. As the sampling schedule designed requires only four dynamic images, the storage space and data processing time are greatly reduced, while the precision of the parameter estimates is almost the same as that achieved with a commonly used schedule. The effects of intersubject and intrasubject parameter variations on parameter estimation with the use of this optimal sampling schedule are investigated by computer simulation. The simulation results show that the estimation of parameters is sufficiently robust with respect to these intersubject and intrasubject variations. The optimal sampling schedule is quite suitable therefore for PET regional parameter estimation, as well as for image-wide parameter estimation, for different subjects.","Image sampling,
Parameter estimation,
Positron emission tomography,
Dynamic scheduling,
Processor scheduling,
Image storage,
Cost function,
Sugar,
Data processing,
Computer simulation"
Tracking and learning graphs and pose on image sequences of faces,"We demonstrate a system capable of tracking in real world image sequences, landmarks such as eyes, mouth, or chin on a face. In the standard version, knowledge previously collected about faces is used for finding the landmarks in the first frame. In a second version, the system is able to track the face without any prior knowledge about faces and is thus applicable to other object classes. By using Gabor filters as visual features, and by both avoiding limiting assumptions and many parameters our tracking tool is simple and easy to use. As a first application the tracking results are used to estimate the pose of a face.","Image sequences,
Face recognition,
Eyes,
Gabor filters,
Head,
Robustness,
Military computing,
Object recognition,
Frequency,
Computer science"
Reactive modules,"We present a formal model for concurrent systems. The model represents synchronous and asynchronous components in a uniform framework that supports compositional (assume-guarantee) and hierarchical (stepwise refinement) reasoning. While synchronous models are based on a notion of atomic computation step, and asynchronous models remove that notion by introducing stuttering, our model is based on a flexible notion of what constitutes a computation step: by applying an abstraction operator to a system, arbitrarily many consecutive steps can be collapsed into a single step. The abstraction operator, which may turn an asynchronous system into a synchronous one, allows us to describe systems at various levels of temporal detail. For describing systems at various levels of spatial detail, we use a hiding operator that may turn a synchronous system into an asynchronous one. We illustrate the model with diverse examples from synchronous circuits, asynchronous shared-memory programs, and synchronous message passing.","Scalability,
Adders,
Computer aided instruction,
Contracts,
Circuits,
Message passing,
Hardware,
Engineering profession,
Wires,
Delay effects"
Quantitative measures of change based on feature organization: eigenvalues and eigenvectors,"We propose four measures of image organizational change which can be used to monitor construction activity. The measures are based on the thesis that the progress of construction will see a change in the individual image feature attributes as well as an evolution in the relationships among these features. This change in the relationship is captured by the eigenvalues and eigenvectors of the relation graph embodying the organization among the image features. We demonstrate the ability of the measures to differentiate between no development, the onset of construction, and full development, on the available real test image set.","Eigenvalues and eigenfunctions,
Image segmentation,
Computer science,
Buildings,
Roads,
Airplanes,
Statistics,
Image edge detection,
Motion estimation,
Face recognition"
Worst-case quadratic loss bounds for prediction using linear functions and gradient descent,"Studies the performance of gradient descent (GD) when applied to the problem of online linear prediction in arbitrary inner product spaces. We prove worst-case bounds on the sum of the squared prediction errors under various assumptions concerning the amount of a priori information about the sequence to predict. The algorithms we use are variants and extensions of online GD. Whereas our algorithms always predict using linear functions as hypotheses, none of our results requires the data to be linearly related. In fact, the bounds proved on the total prediction loss are typically expressed as a function of the total loss of the best fixed linear predictor with bounded norm. All the upper bounds are tight to within constants. Matching lower bounds are provided in some cases. Finally, we apply our results to the problem of online prediction for classes of smooth functions.","Prediction algorithms,
Predictive models,
Computer science,
Upper bound,
Algorithm design and analysis"
The Rocky 7 Mars rover prototype,"This paper provides a system overview of a new Mars rover prototype, Rocky 7. We describe all system aspects: mechanical and electrical design, computer and software infrastructure, algorithms for navigation and manipulation, science data acquisition, and outdoor rover testing. In each area, the improved or added functionality is explained in a context of its path to flight, and need within the constraints of desired science missions.","Mars,
Prototypes,
Software prototyping,
Software design,
Algorithm design and analysis,
Software algorithms,
Navigation,
Data acquisition,
Software testing,
System testing"
A parallel cellular tool for interactive modeling and simulation,"The paper discusses Camel, an interactive parallel programming environment based on cellular automata. With Camel users can develop high-performance applications in science and engineering. Examples in geology, traffic planning, image processing, and genetic algorithms show its usefulness.","Automata,
Concurrent computing,
Lattices,
Cells (biology),
Parallel processing,
Differential equations,
Content addressable storage,
Modeling,
Automatic programming,
Parallel programming"
A practical strategy for testing pair-wise coverage of network interfaces,"Distributed systems consist of a number of network elements that interact with each other. As the number of network elements and interchangeable components for each network element increases, the trade-off that the system tester faces is the thoroughness of test configuration coverage vs. limited resources of time and expense that are available. An approach to resolving this trade-off is to determine a set of test configurations that test each pair-wise combination of network components. This goal gives a well-defined level of test coverage, with a reduced number of system configurations. To select such a set of test configurations, we show how to apply the method of orthogonal Latin squares, from the design of balanced statistical experiments. Since the theoretical treatment assumes constraints that may not be satisfied in practice, we then show how to adapt this approach to realistic application constraints.","Network interfaces,
System testing,
Constraint theory,
Software testing,
Protocols,
Automatic testing,
Software engineering,
Computer science,
Electronic mail,
Manufacturing"
"Partitioned ROBDDs-a compact, canonical and efficiently manipulable representation for Boolean functions","We present a new representation for Boolean functions called Partitioned ROBDDs. In this representation we divide the Boolean space into 'k' partitions and represent a function over each partition as a separate ROBDD. We show that partitioned-ROBDDs are canonical and can be efficiently manipulated. Further they can be exponentially more compact than monolithic ROBDDs and even free BDDs. Moreover, at any given time, only one partition needs to be manipulated which further increases the space efficiency. In addition to showing the utility of partitioned-ROBDDs on special classes of functions, we provide automatic techniques for their construction. We show that for large circuits our techniques are more efficient in space as well as time over monolithic ROBDDs. Using these techniques, some complex industrial circuits could be verified for the first time.","Data structures,
Boolean functions,
Polynomials,
Computer industry,
Construction industry,
Computer science,
Formal verification,
Sequential circuits,
Binary decision diagrams"
Engineering Multiversion Neural-Net Systems,"In this paper we address the problem of constructing reliable neural-net implementations, given the assumption that any particular implementation will not be totally correct. The approach taken in this paper is to organize the inevitable errors so as to minimize their impact in the context of a multiversion system, i.e., the system functionality is reproduced in multiple versions, which together will constitute the neural-net system. The unique characteristics of neural computing are exploited in order to engineer reliable systems in the form of diverse, multiversion systems that are used together with a ""decision strategy"" (such as majority vote). Theoretical notions of ""methodological diversity"" contributing to the improvement of system performance are implemented and tested. An important aspect of the engineering of an optimal system is to overproduce the components and then choose an optimal subset. Three general techniques for choosing final system components are implemented and evaluated. Several different approaches to the effective engineering of complex multiversion systems designs are realized and evaluated to determine overall reliability as well as reliability of the overall system in comparison to the lesser reliability of component substructures.",
Teaching and learning in cyberspace,"From both a technological and educational perspective, cyber education creates a multitude of challenges for students and instructors. Both novice and experienced computer users alike must master the use of Internet tools quickly, while also working to overcome conceptual misunderstandings about the technology and its root metaphors. The technology also makes commenting on student documents cumbersome but does have the benefit of creating a digitized record of students' writing processes, while also allowing for the online publication of students' work. Other benefits include more active learning and better interactive collaboration. Preliminary assessments further indicate that, despite critics' concerns about the rigor and quality of distance learning, for a variety of technical and social reasons, student work is equal to and sometimes better than that of on-campus students.","Internet,
Distance learning,
Educational technology,
Computer aided instruction,
Protocols,
Computer science education,
Writing,
Collaborative work,
Web sites,
Electronic mail"
Picture similarity retrieval using the 2D projection interval representation,"Spatial relationships are important ingredients for expressing constraints in retrieval systems for pictorial or multimedia databases. We have proposed a unified representation for spatial relationships, 2D Projection Interval Relationships (2D-PIR), that integrates both directional and topological relationships. We develop techniques for similarity retrieval based on the 2D-PIR representation, including a method for dealing with rotated and reflected images.","Image retrieval,
Information retrieval,
Multimedia databases,
Content based retrieval,
Shape,
Computer science,
Agricultural engineering,
Image databases,
Spatial databases,
Engines"
"Key Roles of Information Granulation and Fuzzy Logic in Human Reasoning, Concept Formulation and Computing with Words","In our quest for machines which could be called intelligent, we are developing a better understanding of how humans reason, form concepts and make rational decisions in an environment of uncertainty and imprecision. What is becoming increasingly clear is that information granulation plays an essential role in human cognition and that it underlies the remarkable human ability to employ words rather than numbers in the solution of imprecisely defined problems.","Fuzzy logic,
Humans"
Uncertainty management issues in the object-oriented data model,"This paper fully develops a previous approach by George et al. (1993) to modeling uncertainty in class hierarchies. The model utilizes fuzzy logic to generalize equality to similarity which permitted impreciseness in data to be represented by uncertainty in classification. In this paper, the data model is formally defined and a nonredundancy preserving primitive operator, the merge, is described. It is proven that nonredundancy is always preserved in the model. An object algebra is proposed, and transformations that preserve query equality are discussed.","Uncertainty,
Data models,
Object oriented modeling,
Knowledge representation,
Database systems,
Application software,
Very large scale integration,
Computer aided software engineering,
Database languages,
Computer science"
Reduction of false positives in lung nodule detection using a two-level neural classification,"The authors have developed a neural-digital computer-aided diagnosis system, based on a parameterized two-level convolution neural network (CNN) architecture and on a special multilabel output encoding procedure. The developed architecture was trained, tested, and evaluated specifically on the problem of diagnosis of lung cancer nodules found on digitized chest radiographs. The system performs automatic ""suspect"" localization, feature extraction, and diagnosis of a particular pattern-class aimed at a high degree of ""true-positive fraction"" detection and low ""false-positive fraction"" detection. In this paper, the authors aim at the presentation of the two-level neural classification method in reducing false-positives in their system. They employed receiver operating characteristics (ROC) method with the area under the ROC curve (A/sub z/) as the performance index to evaluate all the simulation results. The two-level CNN showed superior performance (A/sub z/=0.93) to the single-level CNN (A/sub z/=0.85). The proposed two-level CNN architecture is proven to be promising and to be extensible, problem-independent, and therefore, applicable to other medical or difficult diagnostic tasks in two-dimensional (2-D) image environments.","Lungs,
Cellular neural networks,
Computer architecture,
Cancer,
Computer aided diagnosis,
Convolution,
Neural networks,
Encoding,
Testing,
Diagnostic radiography"
A declarative language for querying and restructuring the Web,"World Wide Web is a hypertext based, distributed information system that provides access to vast amounts of information in the Internet. A fundamental problem with the Web is the difficulty of retrieving specific information of interest to the user, from the enormous number of resources that are available. We develop a simple logic called WebLog that is capable of retrieving information from HTML (Hypertext Markup Language) documents in the Web. WebLog is inspired by SchemaLog, a logic for multidatabase interoperability. We demonstrate the suitability of WebLog for: querying and restructuring Web information; exploiting partial knowledge users might have on the information being queried; and dealing with the dynamic nature of information in the Web. We illustrate the simplicity and power of WebLog using a variety of applications involving real life information in the Web.","Information retrieval,
Databases,
HTML,
Web sites,
Computer science,
Logic,
Markup languages,
Web search,
Distributed information systems,
Internet"
Monitoring compliance of a software system with its high-level design models,"As a complex software system evolves, its implementation tends to diverge from the intended or documented design models. Such undesirable deviation makes the system hard to understand, modify and maintain. This paper presents a hybrid computer-assisted approach for confirming that the implementation of a system maintains its expected design models and rules. Our approach closely integrates logic-based static analysis and dynamic visualization, providing multiple code views and perspectives. We show that the hybrid technique helps determine design-implementation congruence at various levels of abstraction: concrete rules like coding guidelines, architectural models like design patterns or connectors, and subjective design principles like low coupling and high cohesion. The utility of our approach has been demonstrated in the development of /spl mu/Choices, a new multimedia operating system which inherits many design decisions and guidelines learned from experience in the construction and maintenance of its predecessor, Choices.","Software systems,
Guidelines,
Operating systems,
Computerized monitoring,
Visualization,
Concrete,
Computer science,
World Wide Web,
Multimedia systems,
Utility programs"
A parallel computing approach to creating engineering concept spaces for semantic retrieval: the Illinois Digital Library Initiative project,"This research presents preliminary results generated from the semantic retrieval research component of the Illinois Digital Library Initiative (DLI) project. Using a variation of the automatic thesaurus generation techniques, to which we refer to as the concept space approach, we aimed to create graphs of domain-specific concepts (terms) and their weighted co-occurrence relationships for all major engineering domains. Merging these concept spaces and providing traversal paths across different concept spaces could potentially help alleviate the vocabulary (difference) problem evident in large-scale information retrieval. In order to address the scalability issue related to large-scale information retrieval and analysis for the current Illinois DLI project, we conducted experiments using the concept space approach on parallel supercomputers. Our test collection included computer science and electrical engineering abstracts extracted from the INSPEC database. The concept space approach called for extensive textual and statistical analysis (a form of knowledge discovery) based on automatic indexing and co-occurrence analysis algorithms, both previously tested in the biology domain. Initial testing results using a 512-node CM-5 and a 16-processor SGI Power Challenge were promising.","Parallel processing,
Large-scale systems,
Information retrieval,
Testing,
Software libraries,
Thesauri,
Merging,
Vocabulary,
Scalability,
Information analysis"
Biological and cognitive foundations of intelligent sensor fusion,"This paper reviews the literature from the biological and cognitive sciences in sensory integration and derives principles for use in constructing intelligent sensor fusion systems. In particular, it presents psychophysical and neurophysical studies on how sensor fusion is accomplished and cognitive models of associated activities, including optimization of sensing configurations, improvement of sensing quality, and filtering of noise. The sensor fusion effects architecture for robot navigation is also presented as one example of how these insights from the biological and computer science can be applied to robotic sensor fusion. Experimental results demonstrates the utility of the biological and cognitive insights, especially that of fusion modes. Other representative architectures for robotic sensor fusion are contrasted with the biological and cognitive principles.","Intelligent sensors,
Sensor fusion,
Robot sensing systems,
Cognitive robotics,
Biology,
Computer architecture,
Psychology,
Biological system modeling,
Filtering,
Navigation"
"Legion-a view from 50,000 feet","The coming of giga-bit networks makes possible the realization of a single nationwide virtual computer comprised of a variety of geographically distributed high-performance machines and workstations. To realize the potential that the physical infrastructure provides, software must be developed that is easy to use, supports a large degree of parallelism in the application code, and manages the complexity of the underlying physical system for the user. Legion is a metasystem project at the University of Virginia designed to provide users with a transparent interface to the available resources, both at the programming interface level as well as at the user level. Legion addresses issues such as parallelism, fault-tolerance, security, autonomy, heterogeneity, resource management and access transparency in a multi-language environment. In this paper, we present a high-level overview of Legion, its vision, objectives, a brief sketch of how some of those objectives will be met, and the current status of the project.","Application software,
Computer science,
Parallel processing,
Resource management,
Pervasive computing,
US Department of Energy,
Solids,
Computer networks,
Distributed computing,
Workstations"
A virtual classroom approach to teaching circuit analysis,"We have implemented several innovative uses of computers and computer networks to develop a new pedagogy for the delivery of university engineering courses. These uses of computers and networks are creating efficiencies in the learning process, and students have found this interactive learning environment to be a significant improvement upon a traditional engineering course. Student performance and retention, as well as faculty productivity, are increased in this innovative teaching and learning environment.","Circuit analysis,
Circuit analysis computing,
Computer networks,
Springs,
Equations,
Feedback,
Senior members,
Productivity,
Computer science education,
Educational products"
A comparison of exact and quasi-static methods for evaluating grounding systems at high frequencies,"Recently, it has been suggested that traditional quasi-static methods for evaluating power network grounding systems are not valid at high frequencies. However, the conditions under which exact full wave theory must be used have not been established. In this paper, exact full wave and quasi-static methods are used to evaluate touch and step potentials of a simple ground stake. It is shown that a sufficient condition for quasi-static theory to be valid is if the size of the buried electrode is less than one-tenth of a wavelength in the earth.",
Nonparametric estimation and classification using radial basis function nets and empirical risk minimization,"Studies convergence properties of radial basis function (RBF) networks for a large class of basis functions, and reviews the methods and results related to this topic. The authors obtain the network parameters through empirical risk minimization. The authors show the optimal nets to be consistent in the problem of nonlinear function approximation and in nonparametric classification. For the classification problem the authors consider two approaches: the selection of the RBF classifier via nonlinear function estimation and the direct method of minimizing the empirical error probability. The tools used in the analysis include distribution-free nonasymptotic probability inequalities and covering numbers for classes of functions.","Function approximation,
Risk management,
Convergence,
Shape,
Kernel,
Computer science,
Error probability,
Neural networks,
Multi-layer neural network,
Multilayer perceptrons"
Improving release-consistent shared virtual memory using automatic update,"Shared virtual memory is a software technique to provide shared memory on a network of computers without special hardware support. Although several relaxed consistency models and implementations are quite effective, there is still a considerable performance gap between the ""software-only"" approach and the hardware approach that uses directory-based caches. Automatic update is a simple communication mechanism, implemented in the SHRIMP multicomputer, that forwards local writes to remote memory transparently. In this paper we propose a new lazy release consistency based protocol, called Automatic Update Release Consistency (AURC), that uses automatic update to propagate and merge shared memory modifications. We compare the performance of this protocol against a software-only LRC implementation on several Splash-2 applications and show that the AURC approach can substantially improve the performance of LRC. For 16 processors, the average speedup has increased from 5.9 under LRC to 8.3 under AURC.","Hardware,
Protocols,
Costs,
Workstations,
Network interfaces,
Software maintenance,
Coherence,
Computer science,
Computer networks,
Application software"
Repairs to GLVQ: a new family of competitive learning schemes,"First, we identify an algorithmic defect of the generalized learning vector quantization (GLVQ) scheme that causes it to behave erratically for a certain scaling of the input data. We show that GLVQ can behave incorrectly because its learning rates are reciprocally dependent on the sum of squares of distances from an input vector to the node weight vectors. Finally, we propose a new family of models-the GLVQ-F family-that remedies the problem. We derive competitive learning algorithms for each member of the GLVQ-F model and prove that they are invariant to all scalings of the data. We show that GLVQ-F offers a wide range of learning models since it reduces to LVQ as its weighting exponent (a parameter of the algorithm) approaches one from above. As this parameter increases, GLVQ-F then transitions to a model in which either all nodes may be excited according to their (inverse) distances from an input or in which the winner is excited while losers are penalized. And as this parameter increases without limit, GLVQ-F updates all nodes equally. We illustrate the failure of GLVQ and success of GLVQ-F with the IRIS data.","Prototypes,
Vector quantization,
Computer science,
Iris,
Clustering algorithms,
Terrorism,
Machine intelligence,
Mathematics,
Computer networks"
Making views self-maintainable for data warehousing,"A data warehouse stores materialized views over data from one or more sources in order to provide fast access to the integrated data, regardless of the availability of the data sources. Warehouse views need to be maintained in response to changes to the base data in the sources. Except for very simple views, maintaining a warehouse view requires access to data that is not available in the view itself. Hence, to maintain the view, one either has to query the data sources or store auxiliary data in the warehouse. The authors show that by using key and referential integrity constraints, one often can maintain a select-project-join view without going to the data sources or replicating the base relations in their entirety in the warehouse. They derive a set of auxiliary views such that the warehouse view and the auxiliary views together are self-maintainable-they can be maintained without going to the data sources or replicating all base data. In addition, their technique can be applied to simplify traditional materialized view maintenance by exploiting key and referential integrity constraints.","Warehousing,
Data warehouses,
Computer science,
Databases,
Laboratories,
Contracts,
Costs"
SMD: visual steering of molecular dynamics for protein design,"SMD, a system for interactively steering molecular dynamics calculations of protein molecules, includes computation, visualization, and communication components. Biochemists can ""tug"" molecules into different shapes by specifying external forces in the graphical interface, which are added to internal forces representing atomic bonds and nonbonded interactions. SMD provides a new tool for biochemists to use in exploring the structure of proposed designs, as well as in more general applications such as exploring the molecular dynamics model itself. Its primary use is in modeling single large biomolecules in a bath of water acting as the solvent.","Proteins,
Visualization,
Shape,
Molecular biophysics,
Solvents"
On the diagnosis of programmable interconnect systems: Theory and application,"This paper considers the diagnosis of field programmable interconnect systems (FPIS) in which programmable grids made of switches are included. For this type of interconnects, the number of times the grid must be programmed and the programming sequence of the switches an two of the most important figures of merit for full diagnosis (defection and location with no aliasing and confounding). A hierarchical approach to diagnosis is proposed and fully characterized. The application of this technique to commercially available FPIS such as FPGAs, is discussed. It is shown that the proposed diagnostic technique can be applied to the general purpose interconnect of the FPGAs in the 3000 family by Xilinx.","Testing,
Switches,
Field programmable gate arrays,
Bridges,
Application software,
Programmable logic arrays,
Fault detection,
Computer science,
Logic programming,
Programmable logic devices"
Modeling of profile effects for inductive helicon plasma sources,"A computer code for modeling existing and new helicon sources for materials processing has been developed. The Nagoya type-III, helical, and Stix coil antennas have been modeled to study and examine plasma density and temperature profile effects on power absorption of a small fraction (n/sub fe//n/sub e//spl ap/5%) of fast electrons (T/sub e-fast//spl ap/40 eV) which provide ionization of the neutral gas in the experiment, and bulk (T/sub e-slow//spl ap/3 eV) electron distributions in an argon gas. The ""ANTENA"" computer code, originally written by McVey (1984) to study ion cyclotron waves, was modified and used to study and model helicon sources. A collisional model that includes radial density and temperature profiles was added to the code to study the effect of collisions on the heating mechanisms. The competing effects of collisional and Landau damping heating mechanisms have been investigated in detail, and results indicate that collisions play an important role in the plasma absorption profile at high densities (n/sub c//spl ges/10/sup 13/ cm/sup -3/). The radio frequency wave absorption profiles are sensitive to the plasma density and temperature profiles. The partial-turn helix antenna, that solely excites the m=+1 azimuthal mode, is found to be more efficient in coupling the power to an assumed plasma profile than the Nagoya type-III. The Stix coil is also found to be promising due to its on-axis peaking of the wave heating fields.","Plasma sources,
Plasma temperature,
Plasma density,
Heating,
Coils,
Helical antennas,
Electrons,
Plasma waves,
Electromagnetic wave absorption,
Materials processing"
Fast OFDD-based minimization of fixed polarity Reed-Muller expressions,"We present methods to minimize fixed polarity Reed-Muller expressions (FPRMs), i.e., two-level fixed polarity AND/EXOR canonical representations of Boolean functions, using ordered functional decision diagrams (OFDDs). We investigate the close relation between both representations and use efficient algorithms on OFDDs for exact and heuristic minimization of FPRMs. In contrast to previously published methods, our algorithm can also handle circuits with several outputs. Experimental results on large benchmarks are given to show the efficiency of our approach.","Circuit synthesis,
Circuit testing,
Boolean functions,
Minimization methods,
Field programmable gate arrays,
Computer science,
Logic testing,
Data structures,
Very large scale integration,
Arithmetic"
Cube-4-a scalable architecture for real-time volume rendering,"We present Cube-4, a special-purpose volume rendering architecture that is capable of rendering high-resolution (e.g., 1024/sup 3/) datasets at 30 frames per second. The underlying algorithm, called slice-parallel ray-casting, uses tri-linear interpolation of samples between data slices for parallel and perspective projections. The architecture uses a distributed interleaved memory, several parallel processing pipelines, and an innovative parallel data flow scheme that requires no global communication, except at the pixel level. This leads to local, fixed bandwidth interconnections and has the benefits of high memory bandwidth, real-time data input, modularity, and scalability. We have simulated the architecture and have implemented a working prototype of the complete hardware on a configurable custom hardware machine. Our results indicate true real-time performance for high-resolution datasets and linear scalability of performance with the number of processing pipelines.","Rendering (computer graphics),
Hardware,
Computer architecture,
Pipelines,
Bandwidth,
Solid modeling,
Computer graphics,
Data visualization,
Costs,
Computer science"
The Strobe algorithms for multi-source warehouse consistency,"A warehouse is a data repository containing integrated information for efficient querying and analysis. Maintaining the consistency of warehouse data is challenging, especially if the data sources are autonomous and views of the data at the warehouse span multiple sources. Transactions containing multiple updates at one or more sources, e.g., batch updates, complicate the consistency problem. The authors identify and discuss three fundamental transaction processing scenarios for data warehousing. They define four levels of consistency for warehouse data and present a new family of algorithms, the Strobe family, that maintain consistency as the warehouse is updated, under the various warehousing scenarios. All of the algorithms are incremental and can handle a continuous and overlapping stream of updates from the sources. Their implementation shows that the algorithms are practical and realistic choices for a wide variety of update scenarios.","Warehousing,
Data warehouses,
Computer science,
Information analysis,
Performance analysis,
Laboratories,
Contracts,
Query processing"
Connectivity and sparse wavelength conversion in wavelength-routing networks,"Wavelength-routing networks offer the advantages of wavelength re-use and scalability over broadcast-and-select networks and are therefore suitable for wide area networks (WANs). We study the effects of topological connectivity and wavelength conversion in circuit-switched all-optical wavelength-routing networks. An approximate blocking analysis of such network is performed. We first propose an improved framework for the analysis of networks with arbitrary topology. We introduce a simple model for networks with a variable number of converters and analyze the effect of wavelength converter density on blocking probability. We then apply this framework to two sparse network topologies, the ring and the mesh-torus, and obtain the blocking performance. The results show that, in most cases, only a fraction of the network nodes need to be equipped with wavelength conversion capability for good performance. Finally, the tradeoff between physical connectivity, wavelength conversion, and the number of available wavelengths is studied through networks with random topologies.","Intelligent networks,
Optical wavelength conversion,
Network topology,
Wavelength conversion,
Wavelength division multiplexing,
Switching circuits,
Computer science,
Performance analysis,
Circuit topology,
Optical fiber networks"
Efficient algorithms for array redistribution,"Dynamic redistribution of arrays is required very often in programs on distributed presents efficient algorithms for redistribution between different cyclic(k) distributions, as defined in High Performance Fortran. We first propose special optimized algorithms for a cyclic(x) to cyclic(y) redistribution when x is a multiple of y, or y is a multiple of x. We then propose two algorithms, called the GCD method and the LCM method, for the general cyclic(x) to cyclic(y) redistribution when there is no particular relation between x and y. We have implemented these algorithms on the Intel Touchstone Delta, and find that they perform well for different array sizes and number of processors.","Concurrent computing,
Distributed computing,
Computer Society,
High performance computing,
Runtime library,
Random access memory,
Arithmetic,
Degradation,
Mathematics,
Computer science"
Design of an optimal loosely coupled heterogeneous multiprocessor system,This paper presents an approach for mapping tasks optimal to hardware and software components in order to design a real-time system. The tasks are derived from an algorithm and are represented by a task-graph. The performance of the algorithm on the resulting real-time system will meet the specified timing constraints. Some of the hardware components are programmable and others are application specific hardware processors. We propose a powerful MILP (Mixed Integer Linear Program) model with and without functional pipelining. The efficiency of the method is demonstrated with practical examples.,"Multiprocessing systems,
Hardware,
Timing,
Real time systems,
Application software,
Cost function,
Pipeline processing,
Mathematics,
Computer science,
Software algorithms"
Hidden signatures in images,An image authentication technique by embedding each image with a signature so as to discourage unauthorized copying is proposed. The proposed technique could actually survive several kinds of image processing and the JPEG lossy compression.,"Discrete cosine transforms,
Image coding,
Image resolution,
Image processing,
Frequency,
Pixel,
Multimedia communication,
Computer science,
Authentication,
Electronic publishing"
Lucas: a system for modeling land-use change,"Changes in land use depend on both natural processes and human behavior, complicating the job of natural resource managers. Building Lucas (Land-Use Change Analysis System) to model and predict land-use change required expertise from ecologists, economists, sociologists and computer scientists. The C++ object-oriented language allowed a flexible, modular approach.","Biosphere,
Biological system modeling,
Project management,
Humans,
Resource management,
Data engineering,
Workstations,
Environmental management,
Pattern analysis,
Environmental factors"
"A sensor-based solution to the ""next best view"" problem","Acquiring the complete surface geometry of an object using a range scanner invariably requires that multiple range images be taken of it from different viewpoints. An algorithm is presented which solves the ""next best view"" (NBV) problem: determine the next position for the range scanner given its previous scans of the object. As part of a complete surface acquisition system the scanner's next position should cause it not only to sample more of the object's surface but to resample part of the object already scanned to allow for the registration and integration of the new data with the previous scans. A novel representation, positional space, is presented which facilitates a solution to the NBV problem by representing what must be and what can be scanned in a unified data structure. The expensive operation of determining the visibility of part of the viewing volume is computed only once, not for each potential position of the scanner thus breaking the computational burden of choosing the NBV from a large number of positions. No assumptions are made about the geometry or topology of the object. The algorithm is self-terminating will scan all visible surfaces of an object and can be directed to resample surfaces which were scanned with low confidence. In addition, the algorithm will work with nearly any range camera and scanning setup. A completely automated surface acquisition system featuring the proposed NBV algorithm is demonstrated on a real object.","Topology,
Laboratories,
Information science,
Computational geometry,
Information geometry,
Data structures,
Cameras,
Subcontracting,
Tires,
Indium tin oxide"
Profiling and reducing processing overheads in TCP/IP,"This paper presents detailed measurements of processing overheads for the Ultrix 4.2a implementation of TCP/IP network software running on a DECstation 5000/200. The performance results were used to uncover throughput and latency bottlenecks. We present a scheme for improving throughput when sending large messages by avoiding most checksum computations in a relatively safe manner. We also show that for the implementation we studied, reducing latency (when sending small messages) is a more difficult problem because processing overheads are spread over many operations; gaining a significant savings would require the optimization of many different mechanisms. This is especially important because, when processing a realistic workload, we have found that nondata-touching operations consume more time in aggregate than data-touching operations.","TCPIP,
Throughput,
Local area networks,
Software measurement,
Delay,
Telecommunication traffic,
Wide area networks,
Computer science,
Size measurement,
Intelligent networks"
Detection of eye locations in unconstrained visual images,"This paper describes a computational approach for accurately determining the location of human eyes in unconstrained monoscopic gray level images. The proposed method is based on exploiting the flow field characteristics that arise due to the presence of a dark iris surrounded by a light sclera. A novel aspect of the proposed method lies in its use of both spatial and temporal information to detect the location of the eyes. The spatial processing utilizes flow field information to select a pool of potential candidate locations for the eyes. Temporal processing uses the principle of continuity to filter out the actual location of the eyes from the pool of potential candidates. Extensions for gaze angle determination, and the tracking of human point-of-regard are indicated.","Eyes,
Humans,
Psychology,
Head,
Neural networks,
Data mining,
Cameras,
Computer science,
Iris,
Filters"
"Quantum information processing: cryptography, computation, and teleportation","Present information technology is based on the laws of classical physics. However, advances in quantum physics have stimulated interest in its potential impact on such technology. This article is an introductory review of three aspects of quantum information processing, cryptography, computation, and teleportation. The author serves up hors d'oeuvres on the relevant parts of quantum physics and the sorts of quantum systems which might form the building blocks for quantum processors. Quantum cryptography utilizes states of individual quantum systems for the transfer of conventional classical bits of information. The impossibility of measuring quantum systems without disturbing them guarantees the detection of eavesdropping and hence secure information transfer is possible. In a sense, teleportation is the inverse of cryptography, using more robust classical bits to faithfully transfer a quantum state through a noisy environment. Quantum computation utilizes the evolving quantum state of a complex system, which consists of many interacting individuals. If such a machine could be built, it would be capable of solving some problems which are intractable on any conventional computer; he illustrates this with Shor's (see Proc. 35th IEEE Symposium on Foundations of Computer Science, p.124, 1994) quantum factoring algorithm. Details are given of the current experimental achievements, proposals, and prospects for the future and of the patents granted to date.","Quantum computing,
Information processing,
Cryptography,
Physics,
Teleportation,
Information technology,
Robustness,
Working environment noise,
Computer science,
Proposals"
The Augmint multiprocessor simulation toolkit for Intel x86 architectures,"Most publicly available simulation tools only simulate RISC architectures. These tools cannot capture the instruction mix and memory reference patterns of CISC architectures. We present an overview of Augmint, an execution driven multiprocessor simulation toolkit that fills this gap by supporting Intel x86 architectures. Augmint also supports trace driven simulation for uniprocessors as well as multiprocessors, with minor effort on the part of simulator developers. Augmint runs m4 macro extended C and C++ applications such as those in the SPLASH and SPLASH-2 benchmark suites. Augmint supports a thread based programming model with shared global address space and private stack space. Augmint supports a simulator interface compatible with that of the MINT simulation toolkit for MIPS architectures, thus allowing the reuse of most architecture simulators written for MINT. Augmint simulations run on x8d based uniprocessor systems under Unix or Windows NT. The source code of Augmint is publicly available from http://www.csrd.uiuc.edu/iacoma/augmint.","Computational modeling,
Computer architecture,
Computer simulation,
Hardware,
Software prototyping,
Analytical models,
Trademarks,
Computer science,
Memory architecture,
Prototypes"
Single-source unsplittable flow,"The max-flow min-cut theorem of Ford and Fulkerson is based on an even more foundational result, namely Menger's theorem on graph connectivity Menger's theorem provides a good characterization for the following single-source disjoint paths problem: given a graph G, with a source vertex s and terminals t/sub 1/,...,t/sub k/, decide whether there exist edge-disjoint s-t/sub i/ paths for i=1,...,k. We consider a natural, NP-hard generalization of this problem, which we call the single-source unsplittable flow problem. We are given a source and terminals as before; but now each terminal t/sub i/ has a demand p/sub i//spl les/1, and each edge e of G has a capacity c/sub e//spl ges/1. The problem is to decide whether one can choose a single s-t/sub i/ path for each i, so that the resulting set of paths respects the capacity constraints-the total amount of demand routed across any edge e must be bounded by the capacity c/sub e/. The main results of this paper are constant-factor approximation algorithms for three natural optimization versions of this problem, in arbitrary directed and undirected graphs. The development of these algorithms requires a number of new techniques for rounding fractional solutions to network flow problems; for two of the three problems we consider, there were no previous techniques capable of providing an approximation in the general case, and for the third, the randomized rounding algorithm of Raghavan and Thompson provides a logarithmic approximation. Our techniques are also of interest from the perspective of a family of NP-hard load balancing and machine scheduling problems that can be reduced to the single-source unsplittable flow problem.","Routing,
Approximation algorithms,
Computer science,
Constraint optimization,
Load management,
Processor scheduling,
Admission control,
Scheduling algorithm,
Postal services,
Graph theory"
"Genetic Algorithms, Selection Schemes, and the Varying Effects of Noise","This paper analyzes the effect of noise on different selection mechanisms for genetic algorithms (GAs). Models for several selection schemes are developed that successfully predict the convergence characteristics of GAs within noisy environments. The selection schemes modeled in this paper include proportionate selection, tournament selection, (μ, λ) selection, and linear ranking selection. An allele-wise model for convergence in the presence of noise is developed for the OneMax domain, and then extended to more complex domains where the building blocks are uniformly scaled. These models are shown to accurately predict the convergence rate of GAs for a wide range of noise levels.","uniformly scaled building blocks,
Genetic algorithms,
selection,
noise,
selection intensity,
tournament selection,
linear ranking,
stochastic universal selection,
proportionate selection"
Call admission and resource reservation for multicast sessions,"Many multicast applications, including audio and video, require quality of service (QoS) guarantees from the network. Hence, multicast admission control and resource reservation procedures will be needed. In this paper we present a general framework for admission control and resource reservation for multicast sessions. Within this framework, efficient and practical algorithms that aim to efficiently utilize network resources are developed. The problem of admission control is decomposed into several subproblems that include: the division of end-to-end QoS requirements into local QoS requirements, the mapping of local QoS requirements into resource requirements, and the reclaiming of the resources allocated in excess. These are solved independently of each other yielding a set of mechanisms and policies that can be used to provide admission control and resource reservation for multicast connection establishment. The resource allocation algorithms we consider specifically accommodate receiver heterogeneity (in both end-to-end and per-hop QoS requirements) by reserving necessary and sufficient resources for a multicast session. An application of these algorithms in the context of packetized voice multicast connections over the Mbone is provided to illustrate their applicability.","Quality of service,
Admission control,
Multicast algorithms,
Resource management,
Internet,
Routing,
Computer science,
Application software,
Delay,
Mathematical model"
Hardware assisted volume rendering of unstructured grids by incremental slicing,"Some of the more important research results in computational science rely on the use of simulation methods that operate on unstructured grids. However, these grids, composed of a set of polyhedra, introduce exceptional problems with respect to data visualization. Volume rendering techniques, originally developed to handle rectangular grids, show significant promise for general use with unstructured grids as well. The main disadvantage of this approach, compared to isosurfaces, particles or other visualization tools is its non-interactive performance. We describe an efficient method for rendering unstructured grids that is based on incremental slicing and hardware polygon rendering. For a given view direction, the grid vertices are transformed to image space using available graphics hardware. We then incrementally compute the 2D polygon-meshes that result from letting a set of planes, parallel to the screen plane, intersect (slice) the transformed grid. Finally, we use the graphics hardware to render (interpolate-fill) the polygon-meshes and composite them in visibility order. We show that, in addition to being faster than existing methods, our approach also provides adaptive control and progressive image generation. The adaptive method provides user control to ensure that the contribution of every cell is included in the final image or to limit the number of cells that are missed.","Hardware,
Rendering (computer graphics),
Grid computing,
Data visualization,
Graphics,
Adaptive control,
Computational modeling,
Isosurfaces,
Concurrent computing,
Image generation"
"Analysis, evaluation, and comparison of algorithms for scheduling task graphs on parallel processors","In this paper, we survey algorithms that allocate a parallel program represented by an edge-weighted directed acyclic graph (DAG), also called a task graph or macro-dataflow graph, to a set of homogeneous processors, with the objective of minimizing the completion time. We analyze 21 such algorithms and classify them into four groups. The first group includes algorithms that schedule the DAG to a bounded number of processors directly. These algorithms are called the bounded number of processors (BNP) scheduling algorithms. The algorithms in the second group schedule the DAG to an unbounded number of clusters and are called the unbounded number of clusters (UNC) scheduling algorithms. The algorithms in the third group schedule the DAG using task duplication and are called the task duplication based (TDB) scheduling algorithms. The algorithms in the fourth group perform allocation and mapping on arbitrary processor network topologies. These algorithms are called the arbitrary processor network (APN) scheduling algorithms. The design philosophies and principles behind these algorithms are discussed, and the performance of all of the algorithms is evaluated and compared against each other on a unified basis by using various scheduling parameters.","Algorithm design and analysis,
Scheduling algorithm,
Processor scheduling,
Clustering algorithms,
Computer science,
Network topology,
Software algorithms,
Parallel processing,
Concurrent computing,
Polynomials"
Optimal linear transformation for MRI feature extraction,"This paper presents development and application of a feature extraction method for magnetic resonance imaging (MRI), without explicit calculation of tissue parameters. A three-dimensional (3-D) feature space representation of the data is generated in which normal tissues are clustered around prespecified target positions and abnormalities are clustered elsewhere. This is accomplished by a linear minimum mean square error transformation of categorical data to target positions. From the 3-D histogram (cluster plot) of the transformed data, clusters are identified and regions of interest (ROI's) for normal and abnormal tissues are defined. These ROI's are used to estimate signature (prototype) vectors for each tissue type which in turn are used to segment the MRI scene. The proposed feature space is compared to those generated by tissue-parameter-weighted images, principal component images, and angle images, demonstrating its superiority for feature extraction and scene segmentation. Its relationship with discriminant analysis is discussed. The method and its performance are illustrated using a computer simulation and MRI images of an egg phantom and a human brain.","Magnetic resonance imaging,
Feature extraction,
Image segmentation,
Layout,
Mean square error methods,
Histograms,
Prototypes,
Vectors,
Image generation,
Computer simulation"
Quality-of-service guarantee in high-speed multimedia wireless networks,"The next generation of wireless networks (i.e. high-speed wireless networks) will provide enough resources to support multimedia traffic. In such systems, it is necessary to provide quality-of-service (QOS) guarantees for the multimedia applications. We propose a connection setup algorithm which provides the QOS guarantees for the multimedia traffic. The proposed algorithm provides the QOS guarantees by reserving resources in the surrounding cells. It is shown through extensive simulations that the proposed algorithm provides smaller connection dropping probability than existing algorithms. It is also shown that better bandwidth efficiency is obtained by using the information regarding the user's movement pattern.","Quality of service,
Intelligent networks,
Wireless networks,
Telecommunication traffic,
Traffic control,
Communication system traffic control,
Resource management,
Air traffic control,
Admission control,
Computer science"
Pattern rejection,"The efficiency of pattern recognition is particularly crucial in two scenarios; whenever there are a large number of classes to discriminate, and, whenever recognition must be performed a large number of times. We propose a single technique, namely, pattern rejection, that greatly enhances efficiency in both cases. A rejector is a generalization of a classifier, that quickly eliminates a large fraction of the candidate classes or inputs. This allows a recognition algorithm to dedicate its efforts to a much smaller number of possibilities. Importantly, a collection of rejectors may be combined to form a composite rejector, which is shown to be far more effective than any of its individual components. A simple algorithm is proposed for the construction of each of the component rejectors. Its generality is established through close relationships with the Karhunen-Loeve expansion and Fisher's discriminant analysis. Composite rejectors were constructed for two representative applications, namely, appearance matching based object recognition and local feature detection. The results demonstrate substantial efficiency improvements over existing approaches, most notably Fisher's discriminant analysis.","Computer vision,
Pattern recognition,
Image matching,
Object recognition,
Computer applications,
Computer science,
Object detection,
Pattern analysis,
Algorithm design and analysis,
Face recognition"
Reducing address bus transition for low power memory mapping,"We present low power techniques for mapping arrays in behavioral specifications to physical memory, specifically for memory intensive behaviors that exhibit regularity in their memory access patterns. Our approach exploits this regularity in memory accesses by reducing the number of transitions on the memory address bus. We study the impact of different strategies for mapping arrays in behaviors to physical memory, on power dissipation during memory accesses. We describe a heuristic for selecting a memory mapping strategy to achieve low power, and present an evaluation of the architecture that implements the mapping techniques to study the transition count overhead. Experiments on several image processing benchmarks indicate power savings of upto 63% through reduced transition activity on the memory address bus.","Power dissipation,
Decoding,
Energy consumption,
Digital signal processing,
CMOS logic circuits,
Minimization,
Computer science,
Image processing,
Signal processing algorithms,
Compression algorithms"
The noise amplification index for optimal pose selection in robot calibration,"This paper presents a new observability index to quantify the selection of best pose set in robot calibration. This noise amplification index is considerably more sensitive to calibration error than previously published observability indices. Support for the proposed index as provided analytically and geometrically, and also through comparison against previous indices by a simulation for a 3-link planar robot and by an experiment for a 3-DOF redundant parallel-drive robot.","Calibration,
Robot sensing systems,
Observability,
Kinematics,
Jacobian matrices,
Equations,
Parallel robots,
Integrated circuit noise,
Mechanical engineering,
Computer science"
Techniques for non-linear magnification transformations,"This paper presents efficient methods for implementing general non-linear magnification transformations. Techniques are provided for: combining linear and non-linear magnifications, constraining the domain of magnifications, combining multiple transformations, and smoothly interpolating between magnified and normal views. In addition, piecewise linear methods are introduced which allow greater efficiency and expressiveness than their continuous counterparts.","Image resolution,
Spatial resolution,
Visualization,
Computer science,
Piecewise linear techniques,
Computer displays,
Computer applications,
Application software,
Computer graphics,
Layout"
Transistor sizing for low power CMOS circuits,"A direct approach to transistor sizing for minimizing the power consumption of a CMOS circuit under a delay constraint is presented. In contrast to the existing assumption that the power consumption of a static CMOS circuit is proportional to the active area of the circuit, it is shown that the power consumption is a convex function of the active area. Analytical formulation for the power dissipation of a circuit in terms of the transistor size is derived which includes both the capacitive and the short circuit power dissipation. SPICE circuit simulation results are presented to confirm the correctness of the analytical model. Based on the intuitions drawn from the analytical model, heuristics for initial transistor sizing on critical and noncritical paths for minimum power consumption are developed. Further, fast heuristics to perform transistor sizing in CMOS circuits for minimizing power consumption while meeting the given delay constraints are presented.","Energy consumption,
Delay,
Power dissipation,
Analytical models,
SPICE,
Circuit simulation"
Extending existing dependency theory to temporal databases,"Normal forms play a central role in the design of relational databases. Several normal forms for temporal relational databases have been proposed. These definitions are particular to specific temporal data models, which are numerous and incompatible. The paper attempts to rectify this situation. We define a consistent framework of temporal equivalents of the important conventional database design concepts: functional dependencies, primary keys, and third and Boyce-Codd normal forms. This framework is enabled by making a clear distinction between the logical concept of a temporal relation and its physical representation. As a result, the role played by temporal normal forms during temporal database design closely parallels that of normal forms during conventional database design. These new normal forms apply equally well to all temporal data models that have timeslice operators, including those employing tuple timestamping, backlogs, and attribute value timestamping. As a basis for our research, we conduct a thorough examination of existing proposals for temporal dependencies, keys, and normal forms. To demonstrate the generality of our approach, we outline how normal forms and dependency theory can also be applied to spatial and spatiotemporal databases.","Data models,
Relational databases,
Spatial databases,
Transaction databases,
Computer science,
Proposals,
Spatiotemporal phenomena,
Solids,
Guidelines,
Application software"
"Efficient streamline, streamribbon, and streamtube constructions on unstructured grids","Streamline construction is one of the most fundamental techniques for visualizing steady flow fields. Streamribbons and streamtubes are extensions for visualizing the rotation and the expansion of the flow. The paper presents efficient algorithms for constructing streamlines, streamribbons, and streamtubes on unstructured grids. A specialized Runge-Kutta method is developed to speed up the tracing of streamlines. Explicit solutions are derived for calculating the angular rotation rates of streamribbons and the radii of streamtubes. In order to simplify mathematical formulations and reduce computational costs, all calculations are carried out in the canonical coordinate system instead of the physical coordinate system. The resulting speed up in overall performance helps explore large flow fields.","Visualization,
Vectors,
Differential equations,
Computational efficiency,
Computer science,
Cities and towns,
Postal services,
NASA,
Computer applications,
Convergence"
Finding obstacle-avoiding shortest paths using implicit connection graphs,"We introduce a framework for a class of algorithms solving shortest path related problems, such as the one-to-one shortest path problem, the one-to-many shortest paths problem and the minimum spanning tree problem, in the presence of obstacles. For these algorithms, the search space is restricted to a sparse strong connection graph that is implicitly represented and its searched portion is constructed incrementally on-the-fly during search. The time and space requirements of these algorithms essentially depend on actual search behavior. Therefore, additional techniques or heuristics can be incorporated into search procedure to further improve the performance of the algorithms. These algorithms are suitable for large VLSI design applications with many obstacles.","Very large scale integration,
Shortest path problem,
Tree graphs,
Algorithm design and analysis,
Robots,
Information systems,
Circuits,
Wires,
Computer science,
Heuristic algorithms"
Visage: a user interface environment for exploring information,"Visage is a prototype user interface environment for exploring and analyzing information. It represents an approach to coordinating multiple visualizations, analysis and presentation tools in data-intensive domains. Visage is based on an information-centric approach to user interface design which strives to eliminate impediments to direct user access to information objects across applications and visualizations. Visage consists of a set of data manipulation operations, an intelligent system for generating a wide variety of data visualizations (SAGE) and a briefing tool that supports the conversion of visual displays used during exploration into interactive presentation slides. This paper presents the user interface components and styles of interaction that are central to Visage's information-centric approach.","User interfaces,
Data visualization,
Information analysis,
Displays,
Filters,
Computer science,
Prototypes,
Impedance,
Intelligent systems,
Data analysis"
A global router with a theoretical bound on the optimal solution,"The global routing problem is formulated as a multiterminal, multicommodity flow problem with integer flows, An E-optimal 2-terminal multicommodity flow algorithm with fractional flows is extended to handle multiterminal commodities, Our adaptation of this network flow algorithm seeks to maximize overall routability by minimizing edge congestion as opposed to conventional techniques which usually seek to minimize wire length. We show that under certain conditions, our approach derives an approximate optimal solution. We apply a randomized rounding procedure to derive an integer solution from the fractional multicommodity flow solution. Experimental results demonstrate that this network flow algorithm can be realistically used to route industrial sized circuits with reduced congestion.","Routing,
Linear programming,
Iterative algorithms,
Wire,
Pins,
Integrated circuit interconnections,
Vectors,
Joining processes,
Computer science,
Cost function"
Simplified and improved resolution lower bounds,"We give simple new lower bounds on the lengths of resolution proofs for the pigeonhole principle and for randomly generated formulas. For random formulas, our bounds significantly extend the range of formula sizes for which non-trivial lower bounds are known. For example, we show that with probability approaching 1, any resolution refutation of a randomly chosen 3-CNF formula with at most n/sup 6/5-/spl epsiv// clauses requires exponential size. Previous bounds applied only when the number of clauses was at most linear in the number of variables. For the pigeonhole principle our bound is a small improvement over previous bounds. Our proofs are more elementary than previous arguments, and establish a connection between resolution proof size and maximum clause size.","Testing,
Computer science"
Learning to grasp using visual information,"A scheme for learning to grasp objects using visual information is presented. The learning problem is divided into two separate subproblems: choosing grasping points and predicting the quality of a given grasp. For each grasp we store location parameters that code the locations of the grasping points, quality parameters that are relevant features for the assessment of grasp quality, and the associated grade. The location parameters, using a special coding which is not object specific, are used to locate grasping points on new target objects. A function from the quality parameters to the grade is learned from examples. Grasp quality for novel situations can be generalized and estimated using the learned function. An experimental setup using an AdeptOne manipulator was developed to test this scheme. The system had demonstrated an ability to grasp a relatively wide variety of objects, and its performance had significantly improved with practice following a small number of trials. The knowledge learned for a set of objects was successfully generalized to new objects.","Computer science,
Robots,
Learning systems,
Tellurium,
Libraries,
Target recognition,
Decision trees,
Testing,
Psychology,
Fingers"
Large-scale project management is risk management,"Because large-scale software projects increasingly affect the public good, the ""normal science"" paradigm is proving insufficient to model their complexity and potential consequences. The ""postnormal science"" paradigm offers a better fit, using a robust management approach predicated on a risk-taking ethic.","Large-scale systems,
Project management,
Risk management,
FAA,
Delay,
Integrated circuit modeling,
Automation,
Contracts,
US Government,
Flexible manufacturing systems"
The relative importance of concurrent writers and weak consistency models,"This paper presents a detailed comparison of the relative importance of allowing concurrent writers versus the choice of the underlying consistency model. Our comparison is based on single- and multiple-writer versions of a lazy release consistent (LRC) protocol, and a single-writer sequentially consistent protocol, all implemented in the CVM software distributed shared memory system. We find that in our environment, which we believe to be representative of distributed systems today and in the near future, the consistency model has a much higher impact on overall performance than the choice of whether to allow concurrent writers. The multiple writer LRC protocol performs an average of 9% better than the single writer LRC protocol, but 34% better than the single-writer sequentially consistent protocol. Set against this, MW-LRC required an average of 72% memory overhead, compared to 10% overhead for the single-writer protocoIs.","Access protocols,
Hardware,
Computer science,
Educational institutions"
Grid-clustering: an efficient hierarchical clustering method for very large data sets,"Clustering is a common technique for the analysis of large images. In this paper a new approach to hierarchical clustering of very large data sets is presented. The GRIDCLUS algorithm uses a multidimensional grid data structure to organize the value space surrounding the pattern values, rather than to organize the patterns themselves. The patterns are grouped into blocks and clustered with respect to the blocks by a topological neighbor search algorithm. The runtime behavior of the algorithm outperforms all conventional hierarchical methods. A comparison of execution times to those of other commonly used clustering algorithms, and a heuristic runtime analysis are presented.","Clustering methods,
Clustering algorithms,
Partitioning algorithms,
Multidimensional systems,
Data structures,
Runtime,
Computer science,
Information systems,
Heuristic algorithms,
Algorithm design and analysis"
A genetic algorithm with disruptive selection,"Genetic algorithms are a class of adaptive search techniques based on the principles of population genetics. The metaphor underlying genetic algorithms is that of natural evolution. Applying the ""survival-of-the-fittest"" principle, traditional genetic algorithms allocate more trials to above-average schemata. However, increasing the sampling rate of schemata that are above average does not guarantee convergence to a global optimum; the global optimum could be a relatively isolated peak or located in schemata that have large variance in performance. In this paper we propose a novel selection method, disruptive selection. This method adopts a nonmonotonic fitness function that is quite different from traditional monotonic fitness functions. Unlike traditional genetic algorithms, this method favors both superior and inferior individuals. Experimental results show that GAs using the proposed method easily find the optimal solution of a function that is hard for traditional GAs to optimize. We also present convergence analysis to estimate the occurrence ratio of the optima of a deceptive function after a certain number of generations of a genetic algorithm. Experimental results show that GAs using disruptive selection in some occasions find the optima more quickly and reliably than GAs using directional selection. These results suggest that disruptive selection can be useful in solving problems that have large variance within schemata and problems that are GA-deceptive.","Genetic algorithms,
Genetic mutations,
Algorithm design and analysis,
Sampling methods,
Convergence,
Optimization methods,
Councils,
Computer science,
Design optimization,
Traveling salesman problems"
A new range-sensor based globally convergent navigation algorithm for mobile robots,"We present TangentBug, a new range-sensor based navigation algorithm for two degrees-of-freedom mobile robots. The algorithm combines local reactive planning with globally convergent behaviour. For the local planning, TangentBug uses the range data to compute a locally shortest path based on a novel structure, termed the local tangent graph (LTG). The robot uses the LTG for choosing the locally optimal direction while moving towards the target. The robot also uses the LTG in its other motion mode, where it follows an obstacle boundary. In this mode the robot uses the LTG for making local short-cuts and testing a leaving condition which allows the robot to resume its motion towards the target. We analyze the convergence and performance properties of TangentBug. We also present simulation results, showing that TangentBug consistently performs better than the classical VisBug algorithm. Moreover, TangentBug produces paths that in simple environments approach the globally optimal path as the sensor's maximal detection range increases.","Navigation,
Mobile robots,
Robot sensing systems,
Motion planning,
Convergence,
Path planning,
Mobile computing,
Computer science,
Testing,
Resumes"
Unsupervised learning of probabilistic models for robot navigation,"Navigation methods for office delivery robots need to take various sources of uncertainty into account in order to get robust performance. In previous work, we developed a reliable navigation technique that uses partially observable Markov models to represent metric, actuator and sensor uncertainties. This paper describes an algorithm that adjusts the probabilities of the initial Markov model by passively observing the robot's interactions with its environment. The learned probabilities more accurately reflect the actual uncertainties in the environment, which ultimately leads to improved navigation performance. The algorithm, an extension of the Baum-Welch algorithm, learns without a teacher and addresses the issues of limited memory and the cost of collecting training data. Empirical results show that the algorithm learns good Markov models with a small amount of training data.","Unsupervised learning,
Navigation,
Uncertainty,
Training data,
Robot sensing systems,
Educational robots,
Robustness,
Actuators,
Computer science,
Costs"
Continuity and synchronization in MPEG,"The requirement of continuous retrieval, and the presence of multiple media streams whose display must proceed in a mutually synchronized manner are the distinguishing features that are unique to digital multimedia. In the emerging international multimedia encoding standard MPEG, continuity and synchronization are handled at different layers of the multimedia stream. The authors discuss how they are specified and propose techniques for their implementation within a distributed multimedia environment.",
On the semantics of interactive visualizations,"Interactive techniques are powerful tools for manipulating visualizations to analyze, communicate and acquire information. This is especially true for large data sets or complex 3D visualizations. Although many new types of interaction have been introduced recently, very little work has been done on understanding what their components are, how they are related and how they can be combined. This paper begins to address these issues with a framework for classifying interactive visualizations. Our goal is a framework that will enable us to develop toolkits for assembling visualization interfaces both interactively and automatically.",
Unsupervised vector image segmentation by a tree structure-ICM algorithm,"In recent years, many image segmentation approaches have been based on Markov random fields (MRFs). The main assumption of the MRF approaches is that the class parameters are known or can be obtained from training data. In this paper the authors propose a novel method that relaxes this assumption and allows for simultaneous parameter estimation and vector image segmentation. The method is based on a tree structure (TS) algorithm which is combined with Besag's iterated conditional modes (ICM) procedure. The TS algorithm provides a mechanism for choosing initial cluster centers needed for initialization of the ICM. The authors' method has been tested on various one-dimensional (1-D) and multidimensional medical images and shows excellent performance. In this paper the authors also address the problem of cluster validation. They propose a new maximum a posteriori (MAP) criterion for determination of the number of classes and compare its performance to other approaches by computer simulations.",
The design of whole-program analysis tools,"Building efficient tools for understanding large software systems is difficult. Many existing program understanding tools build control flow and data flow representations of the program a priori, and therefore may require prohibitive space and time when analyzing large systems. Since much of these representations may be unused during an analysis, we construct representations on demand, not in advance. Furthermore, some representations, such as the abstract syntax tree, may be used infrequently during an analysis. We discard these representations and recompute them as needed, reducing the overall space required. Finally, we permit the user to selectively trade off time for precision and to customize the termination of these costly analyses in order to provide finer user control. We revised the traditional software architecture for compilers to provide these features without unnecessarily complicating the analyses themselves. These techniques have been successfully applied in the design of a program slicer for the Comprehensive Health Care System (CHCS), a million line hospital management system written in the MUMPS programming language.",
Forcing behavioral subtyping through specification inheritance,"A common change to object-oriented software is to add a new type of data that is a subtype of some existing type in the program. However, due to message passing, unchanged pearls of the program may now call operations of the new type. To avoid reverification of unchanged code, such operations should have specifications that are related to the specifications of the appropriate operations in their supertypes. This paper presents a specification technique that uses inheritance of specifications to force the appropriate behavior on the subtype objects. This technique is simple, requires little effort by the specifier, and avoids reverification of unchanged code. We present two notions of such behavioral subtyping, one of which is new. We show how to use these techniques to specify examples in C++.",
Detection of software modules with high debug code churn in a very large legacy system,"Society has become so dependent on reliable telecommunications, that failures can risk loss of emergency service, business disruptions, or isolation from friends. Consequently, telecommunications software is required to have high reliability. Many previous studies define the classification fault prone in terms of fault counts. This study defines fault prone as exceeding a threshold of debug code churn, defined as the number of lines added or changed due to bug fixes. Previous studies have characterized reuse history with simple categories. This study quantified new functionality with lines of code. The paper analyzes two consecutive releases of a large legacy software system for telecommunications. We applied discriminant analysis to identify fault prone modules based on 16 static software product metrics and the amount of code changed during development. Modules from one release were used as a fit data set and modules from the subsequent release were used as a test data set. In contrast, comparable prior studies of legacy systems split the data to simulate two releases. We validated the model with a realistic simulation of utilization of the fitted model with the test data set. Model results could be used to give extra attention to fault prone modules and thus, reduce the risk of unexpected problems.",
Knowledge discovery from telecommunication network alarm databases,"A telecommunication network produces daily large amounts of alarm data. The data contains hidden valuable knowledge about the behavior of the network. This knowledge can be used in filtering redundant alarms, locating problems in, the network, and possibly in predicting severe faults. We describe the TASA (Telecommunication Network Alarm Sequence Analyzer) system for discovering and browsing knowledge from large alarm databases. The system is built on the basis of viewing knowledge discovery as an interactive and iterative process, containing data collection, pattern discovery, rule postprocessing, etc. The system uses a novel framework for locating frequently occurring episodes from sequential data. The TASA system offers a variety of selection and ordering criteria for episodes, and supports iterative retrieval from the discovered knowledge. This means that a large part of the iterative nature of the KDD process can be replaced by iteration in the rule postprocessing stage. The user interface is based on dynamically generated HTML. The system is in experimental use, and the results are encouraging: some of the discovered knowledge is being integrated into the alarm handling software of telecommunication operators.",
I/O requirements of scientific applications: an evolutionary view,"The modest I/O configurations and file system limitations of many current high-performance systems preclude solution of problems with large I/O needs. I/O hardware and file system parallelism is the key to achieving high performance. We analyze the I/O behavior of several versions of two scientific applications on the Intel Paragon XP/S. The versions involve incremental application code enhancements across multiple releases of the operating system. Studying the evolution of I/O access patterns underscores the interplay between application access patterns and file system features. Our results show that both small and large request sizes are common, that at present, application developers must manually aggregate small requests to obtain high disk transfer rates, that concurrent file accesses are frequent, and that appropriate matching of the application access pattern and the file system access mode can significantly increase application I/O performance. Based on these results, we describe a set of file system design principles.",
A new approach to eigenstructure assignment by output feedback,"The eigenstructure assignment problem for a linear time invariant multi-input-multi-output system using output feedback is considered. A new approach is developed which identifies the eigenspaces for the desired set of all the closed-loop eigenvalues. For the assignment of this desired set, necessary and sufficient conditions are established. These conditions contain two coupled Sylvester matrix equations, one of which is proven to be a reduced-order square Sylvester matrix equation. This results in an efficient analytical procedure, numerically superior to known techniques, for the determination of the output feedback gain-matrix.",
"Speculative data dissemination and service to reduce server load, network traffic and service time in distributed information systems","We present two server-initiated protocols to improve the performance of distributed information systems (e.g. WWW). Our first protocol is a hierarchical data dissemination mechanism that allows information to propagate from its producers to servers that are closer to its consumers. This dissemination reduces network traffic and balances load amongst servers by exploiting geographic and temporal locality of reference properties exhibited in client access patterns. Our second protocol relies on ""speculative service"", whereby a request for a document as serviced by sending, in addition to the document requested, a number of other documents that the server speculates will be requested in the near future. This speculation reduces service time by exploiting the spatial locality of reference property. We present results of trace-driven simulations that quantify the attainable performance gains for both protocols.",
A novel word clustering algorithm based on latent semantic analysis,"A new approach is proposed for the clustering of words in a given vocabulary. The method is based on a paradigm first formulated in the context of information retrieval, called latent semantic analysis. This paradigm leads to a parsimonious vector representation of each word in a suitable vector space, where familiar clustering techniques can be applied. The distance measure selected in this space arises naturally from the problem formulation. Preliminary experiments indicate that, the clusters produced are intuitively satisfactory. Because these clusters are semantic in nature, this approach may prove useful as a complement to conventional class-based statistical language modeling techniques.",
Education via advanced technologies,"Advanced technologies like the World Wide Web offer interesting opportunities for improving higher education. A study done at the Massachusetts Institute of Technology (MIT) focused on these matters and made several specific recommendations. Since this study was completed, a new center was established at MIT to coordinate and promote the use of advanced technologies in education, and a high-level council on educational technology was formed. After briefly peering into the future, this paper describes the study and the new center.",
Dynamic logic in four-phase micropipelines,"Micropipelines are self-timed pipelines with characteristics that suggest they may be applicable to low-power circuits. They were originally designed with two-phase control, but four-phase control appears to offer benefits for CMOS implementations. In low-power applications static circuit behaviour is desirable since it allows activity to cease (and hence power to be saved) without loss of state. However, dynamic circuits offer the benefits of increased speed and lower switched capacitance. Therefore low-power designs often employ dynamic logic with additional latches or charge-retention circuits to give pseudo-static behaviour. These additions increase the cost and power consumption of the dynamic circuits, thereby compromising their potential advantages. Circuits are proposed in this paper that allow dynamic logic to operate efficiently within a four-phase micropipeline framework without the above-mentioned encumbrances whilst still retaining externally static behaviour.","Protocols,
Latches,
Pipelines,
Logic circuits,
Costs,
Computer science,
Application software,
Switching circuits,
Capacitance,
CMOS logic circuits"
Automatic Incremental State Saving,,
Shifting perspectives on organizational memory: from storage to active remembering,"This paper provides a critique of current conceptions of ""organizational memory"" as presented in a number of recent studies. It briefly reviews some of the rich and varied contributions from both administrative studies and information systems concerning this topic, while at the same time noting the vagueness of the term as it is commonly used. What is of interest is the pervasiveness and perseverance of this nebulous concept across a wide range of disciplinary endeavours. The paper provides an important reformulation of one aspect of ""memory"" that is implicit if not explicit in most current views (i.e. the notion of memory as a passive store), arguing instead for an active, constructive view of ""remembering"" that has a long, if forgotten history within psychology and other fields. Some implications of such an approach are discussed, paying particular attention to the need for empirical studies of ""memories in use"" and the need to focus on the active construction of common information spaces from information repositories and expanding the domain of discourse to include sociological as well as psychological perspectives on concepts such as memory, learning, remembering, talking, etc. in the content of organizations. This reformulation of the issues surrounding organizational memory has significant implications for the kinds of computer support for this phenomenon which might be possible or feasible.",
"A computer-aided, total quality approach to manufacturing education in engineering","This paper describes an ongoing study in improving entry-level engineering education through the deployment of new teaching and learning tools. We introduce a computer-aided interactive multimedia manufacturing courseware. To improve manufacturing education we need to change not only the process of teaching and learning, but also provide new tools and technology that promote efficient learning and make it widely available and continuously improving. To address this manufacturing education challenge, we are presently designing a program based on a new computer-aided education paradigm that embodies total quality management (TQM) and critical thinking (CT) concepts. An interactive multimedia manufacturing courseware lies at the heart of this new computer-aided education paradigm. The manufacturing engineering multimedia courseware (MEMC) includes: on-line lectures, audiovideo education tools, interactive computer software, on-line assignment and exams, information about faculty, and on-line evaluation tools to obtain users' feedback to enhance teaching. It also makes access available to related academia, industry, and government research and education information through the World Wide Web. In this paper, we briefly review the status of engineering education in the United States and describe the appropriateness of unifying the concepts of TQM and CT. Additionally, we provide details of how these concepts can be used in an educational model.",
Business rule extraction from legacy code,"Business rules are operational rules that business organizations follow to perform various activities. Over time, business rules evolve and the software that implemented them are also changed. As the encompassing software becomes large and aged the business rules embedded are difficult to extract and understand. Furthermore, the encompassing software is changed without changing the corresponding documents, so the business organization often trusts the code more than any other documents. It is possible to use a generic tool to extract business rules, but this can be an expensive exercise. The paper proposes a tailored solution approach to the business rule extraction problem, which combines variable classifications, program slicing, and hierarchical abstraction among other maintenance techniques. The proposed approach has been implemented as a system and successfully experimented with a number of industrial programs. The prototype has been demonstrated at several industrial software maintenance sites since June 1995.",
A framework for resource-constrained rate-optimal software pipelining,"The rapid advances in high-performance computer architecture and compilation techniques provide both challenges and opportunities to exploit the rich solution space of software pipelined loop schedules. In this paper, we develop a framework to construct a software pipelined loop schedule which runs on the given architecture (with a fixed number of processor resources) at the maximum possible iteration rate (a la rate-optimal) while minimizing the number of buffers-a close approximation to minimizing the number of registers. The main contributions of this paper are: First, we demonstrate that such problem can be described by a simple mathematical formulation with precise optimization objectives under a periodic linear scheduling framework. The mathematical formulation provides a clear picture which permits one to visualize the overall solution space (for rate-optimal schedules) under different sets of constraints. Secondly, we show that a precise mathematical formulation and its solution does make a significant performance difference. We evaluated the performance of our method against three leading contemporary heuristic methods. Experimental results show that the method described in this paper performed significantly better than these methods. The techniques proposed in this paper are useful in two different ways: 1) As a compiler option which can be used in generating faster schedules for performance-critical loops (if the interested users are willing to trade the cost of longer compile time with faster runtime). 2) As a framework for compiler writers to evaluate and improve other heuristics-based approaches by providing quantitative information as to where and how much their heuristic methods could be further improved.",
Spectral partitioning works: planar graphs and finite element meshes,"Spectral partitioning methods use the Fiedler vector-the eigenvector of the second-smallest eigenvalue of the Laplacian matrix-to find a small separator of a graph. These methods are important components of many scientific numerical algorithms and have been demonstrated by experiment to work extremely well. In this paper, we show that spectral partitioning methods work well on bounded-degree planar graphs and finite element meshes-the classes of graphs to which they are usually applied. While active spectral bisection does not necessarily work, we prove that spectral partitioning techniques can be used to produce separators whose ratio of vertices removed to edges cut is O(/spl radic/n) for bounded-degree planar graphs and two-dimensional meshes and O(n/sup 1/d/) for well-shaped d-dimensional meshes. The heart of our analysis is an upper bound on the second-smallest eigenvalues of the Laplacian matrices of these graphs: we prove a bound of O(1/n) for bounded-degree planar graphs and O(1/n/sup 2/d/) for well-shaped d-dimensional meshes.",
A method to represent multiple-output switching functions by using multi-valued decision diagrams,"Multiple-output switching functions can be simulated by multiple-valued decision diagrams (MDDs) at a significant reduction in computation time. analyze the following approaches to the representation problem: shared multiple-valued decision diagrams (SMDDs), multi-terminal multiple-valued decision diagrams (MTMDDs), and shared multi-terminal multiple-valued decision diagrams(SMTMDDs). For example, we show that SMDDs fend to be compact, while SMTMDDs tend to be fast. We present an algorithm for grouping input variables and output functions in the MDDs.",
A prototype rotating slat collimator for single photon emission computed tomography,"A collimator consisting of a series of highly attenuating parallel slats has been constructed and used in conjunction with a gamma-camera to approximately measure planar projections of a given radionuclide distribution. The enlarged solid angle of acceptance afforded by the slat collimator gave rise to an increased geometric efficiency of between 12 and 28 times that observed with a low-energy high-resolution (LEHR) parallel-hole collimator. When the slats rotated over the face of the detector and the camera gantry turned about the object, sufficient projections were acquired to reconstruct a three-dimensional (3-D) image using the inversion of the 3-D radon transform. The noise behavior of an algorithm for implementing this inversion was studied analytically and the resulting relationship has been verified by computer simulation. The substantially improved geometric efficiency of the slat collimator translated to improvements in reconstructed signal-to-noise ratio (SNR) by, at best, up to a factor of 2.0 with respect to standard parallel-hole collimation. The spatial resolution achieved with the slat collimator was comparable to that obtained with a LEHR collimator and no significant differences were observed in terms of scatter response. Accurate image quantification was hindered by the spatially variant response of the slat collimator.",
Mathematical notation in formal specification: too difficult for the masses?,"The phrase ""not much mathematics required"" can imply a variety of skill levels. When this phrase is applied to computer scientists, software engineers, and clients in the area of formal specification, the word ""much"" can be widely misinterpreted with disastrous consequences. A small experiment in reading specifications revealed that students already trained in discrete mathematics and the specification notation performed very poorly; much worse than could reasonably be expected if formal methods proponents are to be believed.",
On designing an experiment to evaluate a reverse engineering tool,"The Rigi reverse engineering system is designed to analyze and summarize the structure of large software systems. Two contrasting approaches are available for visualizing software structures in the Rigi graph editor. The first approach displays the structures through multiple, individual windows. The second approach, simple hierarchical multi-perspective (SHriMP) views, employs fisheye views of nested graphs. The paper describes the design of an experiment to evaluate these alternative user interfaces. Various results from a preliminary pilot study to test the experiment design are reported.",
Highly fault-tolerant parallel computation,"We re-introduce the coded model of fault-tolerant computation in which the input and output of a computational device are treated as words in an error-correcting code. A computational device correctly computes a function in the coded model if its input and output, once decoded, are a valid input and output of the function. In the coded model, it is reasonable to hope to simulate all computational devices by devices whose size is greater by a constant factor but which are exponentially reliable even if each of their components can fail with some constant probability. We consider fine-grained parallel computations in which each processor has a constant probability of producing the wrong output at each time step. We show that any parallel computation that runs for time t on w processors can be performed reliably on a faulty machine in the coded model using wlog/sup 0(1/)w processors and time tlog/sup 0(1)/w. The failure probability of the computation will be at most t/spl middot/exp(-w/sup 1/4 /). The codes used to communicate with our fault-tolerant machines are generalized Reed-Solomon codes and can thus be encoded and decoded in O(nlog/sup 0(1)/n) sequential time and are independent of the machine they are used to communicate with. We also show how coded computation can be used to self-correct many linear functions in parallel with arbitrarily small overhead.",
Competitive robot mapping with homogeneous markers,"We consider the robot exploration problem of graph maps with homogeneous markers. The graph consists of nodes and edges, where the robot can navigate from one node to another through an edge connecting these two nodes. However, the robot may not distinguish one node (or edge) from another in the unknown graph. All the nodes (edges) look the same. However, at each node, the robot can observe a consistent local relative orientation of its incident edges, that is, a cyclic order of edges incident to the node. To assist the robot's task of mapping the environment, it can put homogeneous marks on nodes or edges which can be recognized later. The total number of edges traversed when constructing a map of the graph is often used as a performance measure for robot strategies. However, since the graph is unknown, a strategy may be efficient in one situation but not in others. Thus, there is a conceptual question about what is an optimal strategy. In this paper, we apply the competitive analysis method for robot explorations. In particular, we compare the cost for constructing a map with the cost for verifying the same map; their ratio is the competitive ratio. A strategy is optimal if it minimizes the worst-case ratio of the total number of edges traversed when constructing a map of the graph to the optimum number of edges traversed in verifying the correctness of a given map of the same graph. If this competitive ratio is bounded above by a constant, we say the strategy is competitive.","Robots,
Cognitive robotics,
Navigation,
Costs,
Joining processes,
Computer science,
Computer errors,
Solid modeling,
Decision making"
Ecological interface design for a power plant feedwater subsystem,"Virtually every major vendor of nuclear power plants (NPPs) is currently working on the design of advanced control rooms for their next generation of plants. Ecological interface design (EID) is a theoretical framework that has been developed to provide guidance in the design of these advanced control rooms. Previous research has applied and evaluated EID in the context of a small-scale, but representative, thermal-hydraulic process simulation. The purpose of this research was to apply the principles of EID to a larger-scale system that is more representative of the complexity of a NPP. A power plant feedwater subsystem was selected as the focus of the study. This paper briefly discusses the principles of EID, describes the design of an EID interface for a power plant feedwater subsystem, and identifies some lessons learned. The main findings of this study are: (1) a proof of concept showing that the principles of EID can be meaningfully applied to a larger-scale design problem representative of those found in the nuclear industry, (2) EID needs to be supplemented by more specific interface design principles, and (3) it is possible to effectively integrate EID with these other design principles. Therefore, EID seems to be a viable candidate for the design of advanced computer interfaces for NPPs.","Power generation,
Displays,
Biological system modeling,
Context modeling,
Prototypes,
Computer interfaces,
Computer industry,
Design engineering,
Power engineering and energy,
Proposals"
Software requirements specification database based on requirements frame model,"The author proposes a method to build a relational database of software requirements specifications (SRSs) from textual SRSs automatically. The author has been developing a requirements model called requirements frame and a text-base requirements language based on the model in order to improve the quality of SRSs. Since requirements frame can be transformed into a relational data model, each of the requirement sentences can be regarded as a tuple of a relational table. The author has been developing both a query language and a relational database management system for on SRS database. One of the features of the SRS DB system is to give an answer with a requirement sentence as an example/counter-example. This feature contributes to verification of the SRS from a developer's own viewpoint. Another feature is detecting changed requirements in modification of the SRS. This feature contributes to effective maintenance of the SRS.",
Effects of tapering on gyrotron backward-wave oscillators,"Computer modeling has been utilized to guide gyrotron backward-wave oscillator (gyro-BWO) experiments at the University of Michigan over a wide range of tapered interaction regions and tapered magnetic fields. E-GUN code is used to examine beam and diode characteristics, while MAGIC is used to analyze the dynamics of the problem, such as particle kinematics and microwave power production. Several innovative techniques are used to create matching boundary conditions for a backward propagating wave. MAGIC simulations predict optimum performance of the gyro-BWO operating in a TE/sub 01/ mode within a combination of uniform interaction region and a tapered axial magnetic field which increases 7.5% in the direction of beam propagation. Experiments have been performed to investigate the effects of tapering magnetic fields and tapered interaction region radii on the high-power microwave emission from the gyro-BWO over the frequency range from 4.0 to 6.0 GHz. These experiments were performed on the Michigan Electron Long-Pulse Accelerator (MELBA) with parameters: V=-0.7 to -0.9 MV, I/sub diode/=1-10 kA, I/sub tube/=1-4 kA, T/sub e-beam/=0.4-1.0 /spl mu/s. Tapered interaction regions of 37%, 23%, 9.4%, and 6.4% were built and tested to determine their effect on microwave power, pulselength, and inferred energy compared to the uniform interaction region. Magnetic tapering trim coils with a range of -10.6%",
ECO: Efficient Collective Operations for communication on heterogeneous networks,"PVM and other distributed computing systems have enabled the use of networks of workstations for parallel computation, but their approach of treating all networks as collections of point-to-point connections does not promote efficient communication-particularly collective communication. The Efficient Collective Operations (ECO) package contains programs which solve this problem by analyzing the network and establishing efficient communication patterns. This paper describes ECO and gives performance results of using ECO to implement the collective communication in CHARMM, a widely used macromolecular dynamics package. ECO substantially improves the performance of CHARMM on a heterogeneous network. ECO's approach gives a programmer the ability to use the available networks to their full potential without acquiring any knowledge of the network structure.",
An analytic framework for specifying and analyzing imprecise requirements,"There are at least three challenges with requirements analysis. First, it needs to bridge informal requirements, which are often vague and imprecise, to formal specification methods. Second, requirements often conflict with each other. Third, existing formal requirement specification methodologies are limited in supporting trade-off analysis between conflicting requirements and identifying the impact of a requirement change to the rest of the system. In this paper, an analytic framework is developed for the specification and analysis of imprecise requirements. In this framework, the elasticity of imprecise requirements is captured using fuzzy logic and the relationships between requirements are formally classified into four categories: conflicting, cooperative, mutually exclusive and irrelevant. This formal foundation facilitates the inference of relationships between requirements for detecting implicit conflicts, to assess the relative priorities of requirements for resolving conflicts, and to assess the effect of a requirement change.","Formal specifications,
Elasticity,
Programming,
Computer science,
Bridges,
Fuzzy logic,
Software systems,
Cost function,
Application software,
Documentation"
The regularity lemma and approximation schemes for dense problems,"There are two main contributions of the present paper. In the first, we use the constructive version of the Regularity Lemma to give directly simple polynomial time approximation schemes for several graph ""subdivision"" problems in dense graphs including the Max Cut problem, the Graph Bisection problem, the Min l-way cut problem and Graph Separator problem. Arora, Karger and Karpinski (1992) gave the first PTASs for these problems whose running time is O(n/sup o(1/e2)/). Our PTASs have running time where the exponent of n is a constant independent of e. The central point here is that the Regularity Lemma provides an explanation of why these Max-SNP hard problems turn out to be easy in dense graphs. We also give a simple PTAS for dense versions of a special case of the Quadratic Assignment Problem (QAP).",
Assertion-oriented automated test data generation,"Assertions are recognized as a powerful tool for automatic run time detection of software errors. However, existing testing methods do not use assertions to generate test cases. We present a novel approach of automated test data generation in which assertions are used to generate test cases. In this approach the goal is to identify test cases on which an assertion is violated. If such a test is found then this test uncovers an error in the program. The problem of finding program input on which an assertion is violated may be reduced to the problem of finding program input on which a selected statement is executed. As a result, the existing methods of automated test data generation for white box testing may be used to generate tests to violate assertions. The experiments have shown that this approach may significantly improve the chances of finding software errors as compared to the existing methods of test generation.",
Test generation for global delay faults,This paper describes test generation for delay faults caused by global process disturbances. The correlations between path delays is used to reduce the number of paths that must be tested. We build macro models of path delays as a function of process parameters to reduce test generation time. The test generation problem is formulated as a nonlinear optimization using a set of candidate paths supplied by a path generator. Results are given for the ISCAS85 benchmarks.,
Making trust explicit in distributed commerce transactions,"In a distributed environment where nodes are independently motivated, many transactions or commercial exchanges may be stymied due to a lack of trust between the participants. The addition of trusted intermediaries may facilitate some exchanges, but others are still problematic. We introduce a language for specifying these commercial exchange problems, and sequencing graphs, a formalism for determining whether a given exchange may occur We also present an algorithm for generating a feasible execution sequence of pairwise exchanges between parties (when it exists). Indemnities may be offered to facilitate previously infeasible transactions. We show when and how they enable commercial transactions.",
Fuzzy parameter adaptation in optimization: some neural net training examples,"Parameters of certain neural net training algorithms and classification procedures are often chosen or adapted using heuristics that contain fuzzy descriptors. Such heuristics, quantified into a fuzzy inference engine, can take the human out of the loop and provide for faster convergence or improved performance. Other applications, outside of neural nets, are also possible.",
Deferred updates and data placement in distributed databases,"Commercial distributed database systems generally support an optional protocol that provides loose consistency of replicas, allowing replicas to be inconsistent for some time. In such a protocol, each replicated data item is assigned a primary copy site. Typically, a transaction updates only the primary copies of data items, with updates to other copies deferred until after the transaction commits. After a transaction commits, its updates to primary copies are sent transactionally to the other sites containing secondary copies. We investigate the transaction model underlying the above protocol. We show that global serializability in such a system is a property of the placement of primary and secondary copies of replicated data items. We present a polynomial time algorithm to assign primary sites to data items so that the resulting topology ensures serializability.",
Pursuing a petaflop: point designs for 100 TF computers using PIM technologies,"This paper is a summary of a proposal submitted to the NSF 100 Tera Flops Point Design Study. Its main thesis is that the use of Processing-In-Memory (PIM) technology can provide an extremely dense and highly efficient base on which such computing systems can be constructed the paper describes a strawman organization of one potential PIM chip, along with how multiple such chips might be organized into a real system, what the software supporting such a system might look like, and several applications which we will be attempting to place onto such a system.",
Improving the performance of coordinated checkpointers on networks of workstations using RAID techniques,"Coordinated checkpointing systems are popular and general-purpose tools for implementing process migration, coarse-grained job swapping, and fault-tolerance on networks of workstations. Though simple in concept, there are several design decisions concerning the placement of checkpoint files that can impact the performance and functionality of coordinated checkpointers. Although several such checkpointers have been implemented for popular programming platforms like PVM and MPI, none have taken this issue into consideration. This paper addresses the issue of checkpoint placement and its impact on the performance and functionality of coordinated checkpointing systems. Several strategies, both old and new, are described and implemented on a network of SPARC-5 workstations running PVM. These strategies range from very simple to more complex borrowing heavily from ideas in RAID (Redundant Arrays of Inexpensive Disks) fault-tolerance. The results of this paper will serve as a guide so that future implementations of coordinated checkpointing can allow their users to achieve the combination of performance and functionality that is right for their applications.",
Dimensionality of illumination in appearance matching,"Appearance matching was recently demonstrated as a robust and efficient approach to 3D object recognition and pose estimation. Each object is represented as a continuous appearance manifold in a low-dimensional subspace parametrized by object pose and illumination direction. Here, the structural properties of appearance manifolds are analyzed with the aim of making appearance representation efficient in off-line computation, storage requirements, and online recognition time. In particular, the effect of illumination on the structure of the appearance manifold is studied. It is shown that for an ideal diffused surface of arbitrary texture, the appearance manifold is linear and three dimensional. This enables the construction of the entire illumination manifold from just three images of the object taken using linearly independent light sources. This result is shown to hold even for illumination by multiple light sources and for concave surfaces that exhibit inter-reflections. Finally, a simple but efficient algorithm is presented that uses just three manifold points for recognizing images taken under novel illuminations.",
Structure and motion of curved 3D objects from monocular silhouettes,"The silhouette of a smooth 3D object observed by a moving camera changes over time. Past work has shown how surface geometry can be recovered using the deformation of the silhouette when the camera motion is known. This paper addresses the problem of estimating both the full Euclidean surface structure and the camera motion from a dense set of silhouettes captured under orthographic or scaled orthographic projection. The approach relies on a viewpoint-invariant representation of curves swept by viewpoint-dependent features such as bitangents, inflections and contour points with parallel tangents. Feature points, which form stereo frontier points between non-consecutive images, are matched using this representation. The camera's angular velocity is computed from constraints derived from this correspondence along with the image velocity of these features. From the angular velocity, the epipolar geometry is ascertained, and infinitesimal motion frontier points can be detected. In turn, the motion of these frontier points constrains the translation component of camera motion. Finally, the surface is reconstructed using established techniques once the camera motion has been estimated.",
The evolution of systems analysis and control: a personal perspective,"The foundations of systems analysis and control as we know them today were laid for the most part at MIT's Radiation Laboratory during World War II and a period thereafter. Most of the founders-both in the United States and abroad-are no longer with us. As one who had the privilege of knowing Wiener, Bode, Nyquist, Guillemin, Gordon Brown, Sam Mason, John Coales, Aizerman, Pontryagin, Letov, Bellman, and many others, I present some personal perceptions and reminiscences in this paper. However, in view of the vastness of the subject, I touch upon only a small subset of the issues and events that were at the center of attention.",
Is parallelism for you?,"This article offers practical, basic rules of thumb that can help you predict if parallelism might be worthwhile, given your application and the effort you want to invest. The techniques presented for estimating likely performance gains are drawn from the experiences of hundreds of computational scientists and engineers at national labs, universities, and research facilities. The information is more anecdotal than experimental, but it reflects the very real problems that must be overcome if parallel programming is to yield useful benefits.","Parallel processing,
Application software,
Thumb,
Frequency estimation,
Testing,
Sea measurements,
Investments,
Concurrent computing,
Performance gain,
Educational institutions"
Modular fixture design for generalized polyhedra,"The term fixturing refers to the task of immobilizing a workpiece for the purpose of performing operations such as assembly and machining. As such, fixturing is of fundamental importance to industrial manufacturing. Fixtures can either be fabricated from scratch, or assembled from a toolkit of modular components, the latter approach is termed modular fixturing. In this paper, we present a complete algorithm to automatically design fixtures for the fixture vice toolkit. The algorithm enumerates fixture configurations, consisting of peg positions, workpiece pose, and jaw separation, for a given generalized polyhedral workpiece. A generalized polyhedral prismatic workpiece is defined to have a generalized polygonal silhouette, with a boundary composed of linear edges and circular area.",
Universal stability results for greedy contention-resolution protocols,"In this paper we analyze the behavior of communication networks in which packets are generated dynamically at the nodes and routed in discrete time steps across the edges. We focus on a basic adversarial model of packet generation and path determination for which the time-averaged injection rate of packets requiring the use of any edge is limited to be less than 1. A crucial issue that arises in such a setting is that of stability-will the number of packets in the system remain bounded, as the system runs for an arbitrarily long period of time? Among other things, we show: (i) There exist simple greedy protocols that are stable for all networks. (ii) There exist other commonly-used protocols (such as FIFO) and networks (such as arrays and hypercubes) that are not stable. (iii) The n-node ring is stable for all greedy routing protocols (with maximum queue-size and packet delay that is linear in n). (iv) There exists a simple distributed randomized greedy protocol that is stable for all networks and requires only polynomial queue size. Our results resolve several questions posed by Borodin et al. and provide the first examples of (i) a protocol that is stable for all networks, and (ii) a protocol that is not stable for all networks.",
Checking subsystem safety properties in compositional reachability analysis,"The software architecture of a distributed program can be represented by an hierarchical composition of subsystems, with interacting processes at the leaves of the hierarchy. Compositional reachability analysis has been proposed as a promising automated method to derive the overall behavior of a distributed program in stages, based on its architecture. The method is particularly suitable for the analysis of programs which are subject to evolutionary change. When a program evolves, only behavior of those subsystems affected by the change need be re-evaluated. The method however has a limitation. The properties available for analysis are constrained by the set of actions that remain globally observable. The properties of subsystems, may not be analyzed. We extend the method to check safety properties of subsystems which may contain actions that are not globally observable. These safety properties can still be checked in the framework of compositional reachability analysis. The extension is supported by augmenting finite-state machines with a special undefined state /spl pi/. The state is used to capture possible violation of the safety properties specified by software developers. The concepts are illustrated using a gas station system as a case study.",
Experience assessing an architectural approach to large-scale systematic reuse,"Systematic reuse of large-scale software components promises rapid, low cost development of high-quality software through the straightforward integration of existing software assets. To date this promise remains largely unrealized, owing to technical, managerial, cultural, and legal barriers, One important technical barrier is architectural mismatch. Recently, several component integration architectures have been developed that purport to promote large-scale reuse. Microsoft's OLE technology and associated applications are representative of this trend. To understand the potential of these architectures to enable large-scale reuse, we evaluated OLE by using it to develop a novel fault-tree analysis tool. Although difficulties remain, the approach appears to overcome architectural impediments that have hindered some previous large-scale reuse attempts, to be practical for use in many domains, and to represent significant progress towards realizing the promise of barge-scale systematic reuse.",
NPN fuzzy sets and NPN qualitative algebra: a computational framework for bipolar cognitive modeling and multiagent decision analysis,"An NPN (Negative-Positive-Neutral) fuzzy set theory and an NPN qualitative algebra (Q-algebra) are proposed which form a computational framework for bipolar cognitive modeling and multiagent decision analysis. First a 6-valued NPN logic is introduced which extends the usual 4-valued Q-algebra (S,/spl ap/,/spl oplus/,/spl otimes/) and S={+,-,0,?} by adding one more level of specification; and then a real-valued NPN fuzzy logic is introduced which extends the 6-valued model to the real space {/spl forall/(x,y)|(x,y)/spl isin/[-1,0]/spl times/[0,1]} and adds infinite levels of specifications, As a generalization, a fuzzy set theory is presented that allows /spl beta/-level fuzzy number-based NPN variables (x,y) to be substituted into (S,/spl ap/,/spl oplus/,/spl otimes/) where /spl otimes/ stands for any NPN T-norm; /spl oplus/ stands for disjunction (V) or union (/spl cup/), and /spl beta/ is the number of /spl alpha/-cuts.",
Evaluation of integrated system-level checks for on-line error detection,"This paper evaluates the capabilities of an integrated system level error detection technique using fault and error injection. This technique is comprised of two software level mechanisms for concurrent error detection, control flow checking using assertions (CCA) and data error checking using application specific data checks. Over 300,000 faults and errors were injected and the analysis of the results reveals that the CCA detects 95% of all the errors while the data checks are able to detect subtle errors that go undetected by the CCA technique. Latency measurements also shelved that the CCA technique is faster than the data checks in detecting the error. When both techniques were incorporated, the system was able to detect over 98% of all injected errors.","Fault detection,
Computer errors,
Error correction,
Hardware,
Delay,
Control systems,
Application software,
Computer science,
Computer applications,
Data flow computing"
Optimizing light collection from thin scintillators used in a beta-ray camera for surgical use,"The authors are developing a 1-2 cm/sup 2/ area camera for imaging the distribution of beta-emitting radiopharmaceuticals at the surface of tissue exposed during surgery. The front end consists of a very thin continuous or segmented scintillator sensitive to betas (positrons or electrons) of a few hundred keV, yet insensitive to gamma rays. The light from the scintillator is piped through clear fibers to the photon detector (PD). This approach requires that a sufficient number of scintillation photons be transported from the scintillator, through the fibers to the PD. The scintillator, reflector, surface treatments, geometry, fiber light guides, and optical couplings must be optimized. The authors report here on efforts made to optimize the light collection from <3 mm thick plastic and CaF/sub 2/(Eu) scintillators into clear fibers using experimental measurements and computer simulations. The authors measured that with a 1.25 cm diameter, 0.5 mm thick optimized CaF/sub 2/(Eu) disk coupled to a 5 cm long bundle of clear optical fibers, on average, /spl sim/250 photoelectrons are produced at a PMT photocathode for a /sup 204/Tl beta flood source (E/sub max/=763 keV). This corresponds to a sufficient number of photoelectrons for <1 mm resolution imaging capabilities for the proposed camera.",
Zero knowledge and the chromatic number,"We present a new technique, inspired by zero-knowledge proof systems, for proving lower bounds on approximating the chromatic number of a graph. To illustrate this technique we present simple reductions from max-3-coloring and max-3-sat, showing that it is hard to approximate the chromatic number within /spl Omega/(N/sup /spl delta//), for some /spl delta/>0. We then apply our technique in conjunction with the probabilistically checkable proofs of Bellare, Goldreich and Sudan (1995), and of Hastad (1996), and show that it is hard to approximate the chromatic number to within /spl Omega/(N/sup 1-/spl epsiv//) for any E>0, assuming NP/spl sub/ ZPP. Here, ZPP denotes the class of languages decidable by a random expected polynomial-time algorithm that makes no errors. Our result matches (up to low order terms) the known gap for approximating the size of the largest independent set. Previous 0(N/sup /spl delta//) gaps for approximating the chromatic number (such as those by Lund and Yannakakis (1994), and by Furer (1995)) did not match the gap for independent set, and do not extend beyond /spl Omega/(N/sup 1/2-/spl epsiv//).",
On breaking a Huffman code,"We examine the problem of deciphering a file that has been Huffman coded, but not otherwise encrypted. We find that a Huffman code can be surprisingly difficult to cryptanalyze. We present a detailed analysis of the situation for a three-symbol source alphabet and present some results for general finite alphabets.",
"Fast, flexible syntactic pattern matching and processing","Program understanding can be assisted by tools that match patterns in the program source. Lexical pattern matchers provide excellent performance and ease of use, but have a limited vocabulary. Syntactic matchers provide more precision, but may sacrifice performance, retargetability, ease of use, or generality. To achieve more of the benefits of both models, we extend the pattern syntax of AWK to support matching of abstract syntax trees, as demonstrated in a tool called TAWK. Its pattern syntax is language-independent, based on abstract tree patterns. As in AWK, patterns can have associated actions, which in TAWK are written in C for generality, familiarity, and performance. The use of C is simplified by high-level libraries and dynamic linking. To allow processing of program files containing non-syntactic constructs, mechanisms have been designed that allow transparent matching in a syntactic fashion. So far TAWK has been retargeted to the MUMPS and C programming languages. We survey and apply prototypical approaches to concretely demonstrate the tradeoffs. Our results indicate that TAWK can be used to quickly and easily perform a variety of common software engineering tasks, and the extensions to accommodate non-syntactic features significantly extend the generality of syntactic matchers.",
Analysis of Selection Algorithms: A Markov Chain Approach,"A Markov chain framework is developed for analyzing a wide variety of selection techniques used in genetic algorithms (GAs) and evolution strategies (ESs). Specifically, we consider linear ranking selection, probabilistic binary tournament selection, deterministic s-ary (s = 3,4, …) tournament selection, fitness-proportionate selection, selection in Whitley's GENITOR, selection in (μ, λ)-ES, selection in (μ + λ)-ES, (μ, λ)-linear ranking selection in GAs, (μ + λ)-linear ranking selection in GAs, and selection in Eshelman's CHC algorithm. The analysis enables us to compare and contrast the various selection algorithms with respect to several performance measures based on the probability of takeover. Our analysis is exact—we do not make any assumptions or approximations. Finite population sizes are considered. Our approach is perfectly general, and following the methods of this paper, it is possible to analyze any selection strategy in evolutionary algorithms.",
Automating image processing for scientific data analysis of a large image database,"Describes the Multimission VICAR Planner (MVP): an AI planning system which uses knowledge about image processing steps and their requirements to construct executable image processing scripts to support high-level science requests made to the Jet Propulsion Laboratory (JPL) Multimission Image Processing Subsystem (MIPS). This article describes a general AI planning approach to automation and application of the approach to a specific area of image processing for planetary science applications involving radiometric correction, color triplet reconstruction, and mosaicing in which the MVP system significantly reduces the amount of effort required by image processing experts to fill a typical request.",
Shielding considerations for satellite microelectronics,"Shielding for space microelectronics needs to provide an acceptable dose rate with minimum shield mass. The analysis presented here shows that the best approach is, in general, to use a graded-Z shield, with a high-Z layer sandwiched between two low-Z materials. A graded-Z shield is shown to reduce the electron dose rate by more than sixty percent over a single-material shield of the same areal density. For protons, the optimal shield would consist of a single, low-Z material layer. However, it is shown that a graded-Z shield is nearly as effective as a single-material shield, as long as a low-Z layer is located adjacent to the microelectronics. A specific shield design depends upon the details of the radiation environment, system model, design margins/levels, compatibility of shield materials, etc. Therefore, we present here general principles for designing effective shields and describe how the computer codes are used for this application.",
"Data mining: machine learning, statistics, and databases","Knowledge discovery in databases and data mining aim at semiautomatic tools for the analysis of large data sets. We give an overview of the area and present some of the research issues, especially from the database angle.",
Architectural retiming: pipelining latency-constrained circuits,"This paper presents a new optimization technique called architectural retiming which is able to improve the performance of many latency-constrained circuits. Architectural retiming achieves this by increasing the number of registers on the latency-constrained path while preserving the functionality and latency of the circuit. This is done using the concept of a negative register, which can be implemented using precomputation and prediction. We use the name architectural retiming since it both reschedules operations in time and modifies the structure of the circuit to preserve its functionality. We illustrate the use of architectural retiming on two realistic examples and present performance improvement results for a number of sample circuits.",
Implementation and performance of a parallel file system for high performance distributed applications,"Dedicated cluster parallel computers (DCPCs) are emerging as low-cost high performance environments for many important applications in science and engineering. A significant class of applications that perform well on a DCPC are coarse-grain applications that involve large amounts of file I/O. Current research in parallel file systems for distributed systems is providing a mechanism for adapting these applications to the DCPC environment. We present the Parallel Virtual File System (PVFS), a system that provides disk striping across multiple nodes in a distributed parallel computer and file partitioning among tasks in a parallel program. PVFS is unique among similar systems in that it uses a stream-based approach that represents each file access with a single set of request parameters and decouples the number of network messages from details of the file striping and partitioning. PVFS also provides support for efficient collective file accesses and allows overlapping file partitions. We present results of early performance experiments that show PVFS achieves excellent speedups in accessing moderately sized file segments.",
Using multimedia to teach the theory of digital multimedia signals,"A new digital signal processing (DSP) course designed to build students' intuition about signals and systems is presented. The course makes extensive use of multimedia demonstrations to relate real-world signals and discrete-time systems to their mathematical descriptions. In addition to the numerous in-class demonstrations, weekly laboratory exercises have been developed to further strengthen the ties between DSP theory and real-world signals and systems. Many of the in-class demonstrations have been captured on the World Wide Web (WWW), so the students can explore them outside of class. In the near future, the WWW-based demonstrations will be packaged on a CD-ROM as an integral part of a new text.",
Controlling synchro-drive robots with the dynamic window approach to collision avoidance,"This paper proposes the dynamic window approach to reactive collision avoidance for mobile robots equipped with synchro-drives. The approach is derived directly from the motion dynamics of the robot and is therefore particularly well-suited for robots operating at high speed. It differs from previous approaches in that the search for commands controlling the translational and rotational velocity of the robot is carried out directly in the space of velocities. The advantage of our approach is that it correctly and in a rigorous way incorporates the dynamics of the robot. This is done by reducing the search space to the dynamic window, which consists of the velocities reachable within a short time interval. Within the dynamic window the approach only considers admissible velocities yielding a trajectory on which the robot is able to stop safely. Among these velocities the combination of translational and rotational velocity is chosen by maximizing an objective function. The objective function includes a measure of progress towards a goal location, the forward velocity of the robot, and the distance to the next obstacle on the trajectory. In extensive experiments the approach presented here has been found to safely control our mobile robot RHINO with speeds of up to 95 cm/sec, in populated and dynamic environments.",
Admission control for hard real-time connections in ATM LANs,"A CAC algorithm must efficiently determine if a new connection can be admitted by verifying that its QoS requirements can be met without violating those of previously admitted connections. In hard real-time systems, the QoS requirements are specified in terms of end-to-end cell deadlines and no cell loss due to buffer overflow. A CAC algorithm must account for interdependencies among connections caused by statistical multiplexing of cells in ATM networks. Arbitrarity of network topology may lead to cyclic dependencies among various connections. We present an efficient CAC algorithm that addresses the above issues. The algorithm uses a traffic descriptor called the maximum traffic rate function to effectively compute bounds on end-to-end delays of connections and buffer requirements within the network. Our work differs from most previous work in that it does not require traffic restoration inside the network.",
Processing natural language software requirement specifications,"Ambiguity in requirement specifications causes numerous problems; for example in defining customer/supplier contracts, ensuring the integrity of safety-critical systems, and analysing the implications of system change requests. A direct appeal to formal specification has not solved these problems, partly because of the restrictiveness and back of habitability of formal languages. An alternative approach, described in the paper, is to use natural language processing (NLP) techniques to aid the development of formal descriptions from requirements expressed in controlled natural language. While many problems in NLP remain unsolved, we show that suitable extensions to existing tools provide a useful platform for detecting and resolving ambiguities. Our system is demonstrated through a case-study on a simple requirements specification.",
Fingerprint enhancement,"Fingerprint images vary in quality. In order to ensure that the performance of an automatic fingerprint identification system (AFIS) will be robust with respect to the quality of input fingerprint images, it is essential to incorporate a fingerprint enhancement module in the AFIS system. We introduce a new fingerprint enhancement algorithm which decomposes the input fingerprint image into a set of filtered images. From the filtered images, the orientation field is estimated and a quality mask which distinguishes the recoverable and unrecoverable corrupted regions in the input image is generated. The input fingerprint image is adaptively enhanced in the recoverable regions. The performance of our algorithm has been evaluated on an online fingerprint verification system using the MSU fingerprint database containing over 600 fingerprint images. Experimental results show that our enhancement algorithm improves the performance of the online fingerprint verification system and makes it more robust with respect to the quality of input fingerprint images.",
Bounding completion times of jobs with arbitrary release times and variable execution times,"In many real-time systems, the workload can be characterized as a set of jobs with linear precedence constraints among them. Jobs often have variable execution times and arbitrary release times. We describe three algorithms that can be used to compute upper bounds on completion times of such jobs scheduled on a priority-driven basis. The algorithms have different performance and complexity. Simulation was performed to compare their performance.",
Principles of conservative parallel simulation,This tutorial describes considerations in writing parallelized discrete-event simulations. We identify key principles behind various synchronization methods tailored to simulate complex systems.,"Discrete event simulation,
Computational modeling,
Packaging,
Educational institutions,
Writing,
Computer simulation,
Concurrent computing,
Petri nets,
Computer science,
System recovery"
Dynamic video playout smoothing method for multimedia applications,"Multimedia applications including video data require the smoothing of video playout to prevent potential playout discontinuity. In this paper, we propose a dynamic video playout smoothing method, called the video smoother, which dynamically adopts various playout rates in an attempt to compensate for high delay variance of networks. Specifically, if the number of frames in the buffer exceeds a given threshold (TH), the smoother employs a maximum playout rate. Otherwise, the smoother uses proportionally reduced rates in an effort to eliminate playout pauses resulting from the emptiness of the playout buffer. To determine THs under various loads, we present an analytic model assuming the Poisson process arrival correspondent with a network with a traffic shaper. Based on the analytic results, we establish a paradigm of determining THs and playout rates for achieving different playout qualities under various loads of networks. Finally, to demonstrate the viability of the video smoother, we have implemented a prototyping system including a multimedia teleconferencing application and the video smoother performing as a part of the transport layer. The prototyping results show that the video smoother achieves smooth playout incurring only unnoticeable delays.",
CCD mosaic technique for large-field digital mammography,"The authors present a novel technique for large-field digital mammography. The instrument uses a mosaic of electronic digital imaging [charge coupled device (CCD)] arrays, novel area scanning, and a radiation exposure and scatter reducing mechanism. The imaging arrays are mounted on a carrier platform in a checker-board pattern mosaic. To fill in the gaps between array-active areas the platform Is repositioned three times and four X-ray exposures are made. The multiple image areas are then recombined by a digital computer to produce a composite image of the entire region. To reduce X-ray scatter and exposure, a lead aperture plate is interposed between X-ray source and patient. The aperture plate has a mosaic of square holes in alignment with the imaging array pattern and the plate is repositioned in synchronism with the carrier platform. The authors discuss proof-of-concept testing demonstrating technical feasibility of their approach. The instrument should be suitable for incorporation into standard mammography units. Unique features of the new technique are: large field coverage (18/spl times/24 cm); high spatial resolution (14-17 lp/mm); scatter rejection; and excellent contrast characteristics and lesion detectability under clinical conditions.",
Load balancing and density dependent jump Markov processes,"We provide a new approach for analyzing both static and dynamic randomized load balancing strategies. We demonstrate the approach by providing the first analysis of the following model: customers arrive as a Poisson stream of rate /spl lambda//sub n/, /spl lambda/<1, at a collection of n servers. Each customer chooses some constant d servers independently and uniformly at random from the n servers, and waits for service at the one with the fewest customers. Customers are served according to the first-in first-out (FIFO) protocol, and the service time for a customer is exponentially distributed with mean 1. We call this problem the supermarket model. We wish to know how the system behaves, and in particular we are interested in the expected time a customer spends in the system in equilibrium. The model provides a good abstraction of a simple, efficient load balancing scheme in the setting where jobs arrive at a large system of parallel processors. This model appears more realistic than similar models studied previously, in that it is both dynamic and open: that is, customers arrive over time, and the number of customers is not fixed.",
Design of stability-guaranteed fuzzy logic controller for nuclear steam generators,"A fuzzy logic controller (FLC) and a fuzzy logic filter (FLF), which have a special type of fuzzifier, inference engine, and defuzzifier, are applied to the water level control of a nuclear steam generator (S/G). It is shown that arbitrary two-input, single-output linear controllers can be adequately expressed by this FLC. A procedure to construct stability-guaranteed FLC rules is proposed. It contains the following steps: 1) the stable sector of linear feedback gains is obtained from the suboptimal concept based on LQR theory and the Lyapunov's stability criteria: 2) the stable sector of linear gains is mapped into two linear rule tables that are used as limits for the FLC rules; and 3) the construction of an FLC rule table is done by choosing certain rules that lie between these limits. This type of FLC guarantees asymptotic stability of the control system. The FLF generates a feedforward signal of S/G feedwater from the steam flow measurement using a fuzzy concept. Through computer simulation, it is found that the FLC with the FLF works better than a well-tuned PID controller with variable gains to reduce swell/shrink phenomena, especially for the water level deviation and abrupt steam flow disturbances that are typical in the existing power plants.",
Using partial orders for trace theoretic verification of asynchronous circuits,"In this paper, we propose a method to generate the reduced state spaces in which the trace theoretic verification method of asynchronous circuits works correctly and efficiently. The state space reduction is based on the stubborn set method and similar ideas, but they have been extended so that the conformance checking works correctly in the reduced state space. Our state reduction algorithm also guarantees that a kind of simple liveness properties are correctly checked. Some experimental results show the efficiency of the proposed method.",
Characterization and parameterized random generation of digital circuits,"The development of new Field-Programmed, Mask-Programmed and Laser-Programmed Gate Array architectures is hampered by the lack of realistic test circuits that exercise both the architectures and their automatic placement and routing algorithms. In this paper, we present a method and a tool for generating parameterized and realistic random circuits. To obtain the realism, we propose a set of graph-theoretic characteristics that describe a physical netlist, and have built a tool that can measure these characteristics on existing circuits. The generation tool uses the characteristics as constraints in the random circuit generation. To validate the quality of the generated netlists, parameters that are not specified in the generation are compared with those of real circuits, and with those of ""random"" graphs.","Character generation,
Digital circuits,
Circuit testing,
Benchmark testing,
Permission,
Shape measurement,
Computer architecture,
Delay,
Pins,
Computer science"
Neural network analysis of flow cytometry immunophenotype data,"Acute leukemia is one of the leading malignancies in the United States with a mortality rate strongly influenced by the phenotype. This phenotype is based on detection of cell associated antigens normally expressed during leucopoietic differentiation. In this regard, leukemia classified as lymphoid or myeloid by phenotype is also classified as a candidate for the corresponding chemotherapy protocol. Additionally, the subtype of leukemia based on the degree of differentiation and cell maturity influence prognosis, response to treatment, and median survival times. In this paper, we analyze immunophenotype flow cytometry data toward categorization of leukemia into subcategories based on lineage and differentiation antigen expression. Twenty-eight inputs (derived from the mean fluorescence intensity of up to 27 antibodies, and an additional binary input denoting the past diagnosis of leukemia) are used as input to a neural classifier to categorize a total of 170 cases into the lineage and differentiation categories of leukemia. The neural classifier consisted of a feed forward network trained using back propagation. A complexity regulation term (weight decay) was used to improve the generalization performance of the neural classifier. A training error of 0.0% and a generalization error of 10.3% was obtained for categorization based on lineage, while a training error of 0.0% and a generalization error of 10.0% was obtained for categorization based on differentiation. These results indicate that objective classification of multifaceted phenotypes in leukemia can be achieved for analyzing multiparameter data in flow cytometry and further categorization into the prognostic subtypes.",
Written language recognition based on texture analysis,"Numerous techniques have been reported for optical character recognition (OCR). Almost all such techniques make an implicit assumption that the language of the document to be processed is known. We attempt to eliminate this assumption by presenting a novel algorithm for automatic written language recognition. Given that different languages are often visually distinctive in written form, we take a global approach based on texture analysis, where each language is regarded as a different texture. In principle this allows us to apply any standard texture recognition algorithm for the task. Experiments with six languages clearly demonstrate the great potential of the proposed global approach.",
A new digital signal processing technique for applications in nuclear spectroscopy,"A multichannel pulse-height-analysis system based on digital signal processing techniques was developed and tested for nuclear spectroscopy. The system digitizes gaussian-shaped pulses with widths greater than 2 /spl mu/s. Input pulses are completely processed in real time on a single PC-AT bus board that replaces the standard analog pulse-height analyzer boards. Using a modified sliding-scale method, differential nonlinearity has been reduced to 1 to 2% for 4096 channels. A 30% increase in pulse throughput rate at 1 kHz has been achieved by reducing overall system dead time. Other potential advantages will be explored in future work.",
Electric fields and currents induced in organs of the human body when exposed to ELF and VLF electromagnetic fields,"Formulas for the transverse components of the electric and magnetic fields of the traveling-wave currents of three different types of three-wire, three-phase high-voltage power lines and of a typical VLF transmitter are given. From them, exposure situations for the human body are chosen which permit the analytical determination of the total current induced in that body. With this, the fraction of the total axial current, the axial current density, and the axial electric field in each organ of the body are obtained at any desired cross section. The dimensions and conductivity of these organs must be known. The electric field so obtained is the average macroscopic field in which the cells in each organ are immersed when the whole body is exposed to a known incident field. It corresponds in vivo to the electric field used in vitro to expose cells in tissues.",
Bayesian fused classification of medical images,"In many applications in computer vision and signal processing, it is necessary to assimilate data from multiple sources. This is a particularly important issue in medical imaging, where information on a patient may be available from a number of different modalities. As a result, there has been much recent research interest in this area. The authors suggest an additional Bayesian method which generates a segmented classification concurrently with improving reconstructions of a set of registered images. A synthetic example is used to demonstrate the subjectives and benefits of this proposed approach. Two medical applications, one fusing computed tomography (CT) and single photon emission computed tomography (SPECT) brain scans, and the other magnetic resonance (MR) images at two different resolutions, are considered.",
Color stereo vision using hierarchical block matching and active color illumination,"Stereo is a well-known technique for obtaining depth information from digital images. Nevertheless, this technique still suffers from a lack in accuracy and/or long computation time needed to match stereo images. A new hierarchical algorithm using an image pyramid for obtaining dense depth maps from color stereo images is presented. We show that matching results of high quality are obtained when using the new hierarchical chromatic block matching algorithm. Most stereo matching algorithms can not compute correct dense depth maps in homogenous image regions. This paper shows that using an active color illumination will considerably improve the quality of the matching results. We present results for synthetic and for real images.","Stereo vision,
Lighting,
Pixel,
Layout,
Color,
Computer science,
Digital images,
Computer vision,
Image reconstruction,
Surface reconstruction"
Truth vs. knowledge: the difference between what a component does and what we know it does,"Conventional doctrine holds that specifications are sufficient, complete, static, and homogeneous. For system level specifications, especially for software architectures, conventional doctrine often fails to hold. This can happen when properties other than functionality are critical, when not all properties of interest can be identified in advance, or when the specifications are expensive to create. That is, the conventional doctrine often fails for practical software components. Specifications for real software must be incremental, extensible, and heterogeneous. To support such specifications, our notations and tools must be able to extend and manipulate structured specifications. In the UniCon architecture description language, we introduce credentials, a property list form of specification that supports evolving heterogeneous specifications and their use with system building and analysis tools.",
How many clusters?: A Ying-Yang machine based theory for a classical open problem in pattern recognition,"Determination of the number of clusters in the classical mean square error (MSE) clustering analysis (e.g., by the well known k-mean algorithm) and determination of the number of Gaussians in a finite Gaussian mixture (e.g., by the EM algorithm) are well known model selection problems that take important roles in unsupervised pattern recognition. The problem has remained open for decades since there is no appropriate theory for solving it except for some heuristic techniques. This paper presents a theory for solving this problem based on the Ying-Yang machine-a Bayesian-Kullback learning scheme for unified learnings (Xu, 1995, 1996). By this theory, we obtain the criteria for selecting the correct number of clusters in the MSE clustering or in a Gaussian mixture. In addition, an automatic procedure is designed for a fast implementation of the selection. Experimental results are provided to demonstrate our success.",
NetSovle: A Network Server for Solving Computational Science Problems,"This paper presents a new system, called NetSolve, that allows users to access computational resources, such as hardware and software, distributed across the network. The development of NetSolve was motivated by the need for an easy-to-use, efficient mechanism for using computational resources remotely. Ease of use is obtained as a result of different interfaces, some of which require no programming effort from the user. Good performance is ensured by a load-balancing policy that enables NetSolve to use the computational resources available as efficiently as possible. NetSolve offers the ability to look for computational resources on a network, choose the best one available, solve a problem (with retry for fault-tolerance), and return the answer to the user.",
A sensitivity analysis algorithm for pruning feedforward neural networks,"A pruning algorithm, based on sensitivity analysis, is presented in this paper. We show that the sensitivity analysis technique efficiently prunes both input and hidden layers. Results of the application of the pruning algorithm to various N-bit parity problems agree with well-known published results.",
Evaluation of checkpoint mechanisms for massively parallel machines,"Massively parallel machines typically contain thousands of processor units and therefore are more likely to suffer system breakdown because of component failures. This paper studies efficient diskless checkpointing mechanisms for SIMD massively parallel machines. Three checkpointing schemes: mirror checkpointing, parity checkpointing, and partial parity checkpointing are compared in terms of their checkpoint performance and storage overheads, based on empirical measurements. Mirror checkpointing and parity checkpointing schemes have been successfully implemented and tested on a DECmpp 12000 machine, without hardware or OS modifications. It has been shown that mirror checkpointing is an order of magnitude faster than parity checkpointing, but takes twice as much storage overhead. Partial parity checkpointing, although significantly reduces the storage overhead, could lead to unpredictable execution performance. This paper also examines the detailed storage/performance tradeoffs for partial parity checkpointing through manual instrumentation, and describes the implementation experience from these experiments.",
Optical flow recognition from the power spectrum of a single blurred image,"In this paper a new technique for calculation of the optical flow is presented. When there is motion in the observed scene, an image taken will be motion blurred (to a degree depending on the exposure time). Up to now most of the algorithms for estimating the motion in a scene ignored motion blur and treated it as noise. On the contrary, motion blur is structured information and in certain cases can be used to infer the velocities locally. This new approach uses the information of the motion blur in the frequency domain to extract the orientation and the magnitude of the velocity-optical flow.",
Neural networks with adaptive spline activation function,"In this paper a new neural network architecture, based on an adaptive activation function, called generalized sigmoidal neural network (GSNN), is proposed. The activation functions are usually sigmoidal but other functions, also depending on some free parameters, have been studied and applied. Most approaches tend to use relatively simple functions (as adaptive sigmoids), primarily due to computational complexity and difficulties hardware realization. The proposed adaptive activation function, built as a piecewise approximation with suitable cubic splines, can have arbitrary shape and allows to reduce the overall size of the neural networks, trading connection complexity with activation function complexity.",
Two ranking schemes for efficient computation on the star interconnection network,A node ranking scheme provides the necessary structural view for developing algorithms on a network. We present two ranking schemes for the star interconnection network both of which allow constant time order preserving communication. The first scheme is based on a hierarchical view of the star network. It enables one to efficiently implement order preserving ASCEND/DESCEND class of algorithms. This class includes several important algorithms such as the Fast Fourier Transform (FFT) and matrix multiplication. The other ranking scheme gives a flexible pipelined view of the star interconnection network and provides a suitable framework for implementation of pipelined algorithms.,
Endeavors: a process system integration infrastructure,"As software projects evolve, possibly differing in size, complexity, scope and purpose, the development processes that support the project must evolve to reflect these changes. For a distributed project, maintaining proper communications, coordinating between project stakeholders, and maintaining managerial control become increasingly important and, unfortunately, increasingly difficult. The Endeavors system is an open, distributed process modeling and execution infrastructure that addresses communication, coordination, and control issues. Complex processes may require: (a) distribution of people and processes; event based and intermediate format integration of external tools; a low entry barrier through ease of use and incremental adoption; ability to customize and reuse objects, tools, and policies; and dynamic change of runtime processes, objects, and behaviors. Endeavors' solution architecture achieves these goals through application of five key design strategies: (1) maintaining multiple object model layers; (2) implementing the architecture as a set of highly componentized, lightweight, transportable, concurrent elements; (3) providing customization capabilities for each layer of the object model; (4) using a reflexive object model to support dynamic change; and (5) allowing dynamic loading and changing of objects including loading of executable handlers, new object types, and extensions. We discuss these goals and design strategies, describe the architecture, and describe the current status of the project and its relevance to its own development.",
A physical interpretation for the single-event-gate-rupture cross-section of n-channel power MOSFETs,"The single-event-gate-rupture cross-section is measured as a function of drain-source and gate-source bias for some n-channel power MOSFETs. The experimental techniques are explained, and the results are interpreted with the help of two-dimensional computer modeling.",
An algorithm for representing functions of many variables by superpositions of functions of one variable and addition,"A computer algorithm is given for representing functions of many variables by superpositions of functions of one variable and addition. By this algorithm, nonseparable functions are represented in separable forms automatically by computer.",
Developing a Testing Maturity Model for software test process evaluation and improvement,"Testing is a critical component of a mature software development process. It is one of the most challenging and costly process activities, and in its fullest definition if provides strong support for the development of high qualify software. Existing maturity models do not adequately address testing issues, nor has the concept of a mature testing process been well defined. We are developing a Testing Maturity Model (TMM) to address these issues. The TMM will contain a set of maturity levels through which an organization can progress towards testing process maturity, a set of recommended practices at each level of maturity that can be put into place, and an assessment model that will allow software development organizations to evaluate and improve their testing processes. In this paper we discuss our approach to TMM development, the major features of the TMM, its ties to the Capability Maturity Model, and our initial plans for a Testers' Tool Workbench that will support testing process maturity growth.",
Automated design of part feeders using a genetic algorithm,"We describe a genetic algorithm approach to the automated design of vibratory bowl part feeders. Our approach gives us near-optimal designs in much less time than previously published optimal, brute-force search methods. We have implemented our approach in an automated part feeder design system, and we present preliminary results generated by our system.",
Advancing Interactive Visualization and Computational Steering,,Visualization
An evaluation of software test environment architectures,"Software test environments (STEs) provide a means of automating the test process and integrating testing tools to support required testing capabilities across the test process. Specifically, STEs may support test planning, test management, test measurement, test failure analysis, test development and test execution. The software architecture of an STE describes the allocation of the environment's functions to specific implementation structures. An STE's architecture can facilitate or impede modifications such as changes to processing algorithms, data representation or functionality. Performance and reusability are also subject to architecturally imposed constraints. Evaluation of an STE's architecture can provide insight into modifiability, extensibility, portability and reusability of the STE. This paper proposes a reference architecture for STEs. Its analytical value is demonstrated by using SAAM (Software Architectural Analysis Method) to compare three software test environments: PROTest II (PROLOG Test Environment, Version II), TAOS (Testing with Analysis and Oracle Support), and CITE (CONVEX Integrated Test Environment).",
Recognition of the multi-specularity objects using the eigen-window,"This paper describes a method for recognizing partially occluded specularity objects for bin-picking tasks using the eigen-space analysis. Although effective in recognizing an isolated object, as was shown by Murase and Nayar, the current method can not be applied to partially occluded objects that are typical in bin-picking tasks. The analysis also requires that the object is centered in an image before recognition. These limitations of the eigen-space analysis are due to the fact that the whole appearance of an object is utilized as a template for the analysis. We propose a new method, referred to as the ""eigen-window"" method, that stores multiple partial appearances of an object in the eigen-space. Such partial appearances require a large number of memory space. To reduce the memory requirement by avoiding redundant windows and to select only effective windows to be stored, a similarity measure among windows is developed. Using a pose clustering method among windows, the method determines the pose of an object. We have implemented the method and verify the validity of the method.",
Doing FLIPS: flexible interactive presentation synchronization,"Multimedia presentation technology has enormous potential for a myriad of applications including academic classrooms, industrial training, and business presentations. As presentation technology advances, it is possible to incorporate a wider range of media including variable duration media such as simulations and animations. At the same time, users are able to take more control over presentations by controlling the rate and selection of media being played. To make full use of these advances, multimedia systems must support flexible presentations that incorporate many variations in the way they are played. This paper identifies three requirements for flexible presentations and derives four requirements for synchronization of flexible presentations. The paper presents flexible interactive presentation synchronization (FLIPS), a model for specifying coarse synchronization for flexible presentations. FLIPS supports a wide range of temporal synchronization specifications. It also provides algorithms for attaining a consistent and coherent presentation state in response to user interaction (e.g. skipping to a different slide or selection) and other state-changing events. Applications of the FLIPS model are discussed.",
Combination of multiple classifiers using local accuracy estimates,"Combination of multiple classifiers (CMC) has recently drawn attention as a method of improving classification accuracy. This paper presents a method for combining classifiers that use estimates of each individual classifier's local accuracy in small regions of feature space surrounding an unknown test sample. Only the output of the most locally accurate classifier is considered. We address issues of (1) optimization of individual classifiers, and (2) the effect of varying the sensitivity of the individual classifiers on the CMC algorithm. Our algorithm performs better on data from a real problem in mammogram image analysis than do other recently proposed CMC techniques.",
Exploiting physical constraints: heap formation through behavioral error in a group of robots,"In this paper we describe a collective heap building process by a group of robots. Instead of predefining ""cognitive"" capacities, we exploit the physical structure of the robots and the self-organizing properties of group processes. The robots' control program effectively contains just one behavioral rule to avoid detected obstacles. Due to the constrained sensory input, the robots collide with objects that are exactly in front of them. In this way, objects are pushed and clusters are formed. To study the dynamics of the cluster process we conducted experiments in which the number of objects and robots were varied. We found that a limited amount of mutual interference is crucial for the fusion of clusters; hence large, single heaps did never emerge in trials with just one robot. However, with more than 4 robots the heap building process slows down due to increased mutual avoidance movements.",
N-tuple features for OCR revisited,"N-tuple features for optical character recognition have received only scattered attention since the 1960s. Our main purpose is to show that advances in computer technology and computer science compel renewed interest. N-tuple features are useful for printed character classification because they indicate the presence or absence of a given rigid configuration of n black and white pixels in a pattern. Desirable n-tuples fit each pattern of a specified (positive) training set of characters in at least p different shift positions, and fail to fit each pattern of a specified (negative) training set by at least n-q pixels in each shift position. We prove that the problem of finding a distinguishing n-tuple is NP-complete, by examining a natural subproblem with binary strings called the missing configuration problem. The NP-completeness result notwithstanding, distinguishing n-tuples are found automatically in a few seconds on contemporary workstations. We exhibit a practical search algorithm for generating, from a small training set, a collection of n-tuples with low class-conditional correlation and with specified design parameters n, p, and q. The generator, which is available on the Internet, is empirically shown to be effective through a comparison with a benchmark generator. We show experimentally that the design parameters provide a useful tradeoff between distinguishing power and generation time, and also between the conditional probabilities for the positive and negative classes. We explore the feature probabilities obtainable for various dichotomies, and show that the design parameters control the feature probabilities.",
Scheduling soft real-time jobs over dual non-real-time servers,"In this paper, we consider soft real-time systems with redundant off-the-shelf processing components (e.g., CPU, disk, network), and show how applications can exploit the redundancy to improve the system's ability of meeting response time goals (soft deadlines). We consider two scheduling policies, one that evenly distributes load (Balance), and one that partitions load according to job slackness (Chop). We evaluate the effectiveness of these policies through analysis and simulation. Our results show that by intelligently distributing jobs by their slackness amount the servers, Chop can significantly improve real-time performance.",
Circuit-switched broadcasting in torus and mesh networks,"We consider the problem of broadcasting on torus and mesh networks using circuit-switched, half-duplex, and link-bound communication. In this paper, we obtain an optimal broadcasting algorithm that uses pd time steps for a d-dimensional torus with (2d+1)/sup p/ nodes in each side of the torus. Using this algorithm, we show that a broadcasting on a d-dimensional mesh with the same size can be done in pd+p+d-1 time steps.",
Balanced spanning trees in complete and incomplete star graphs,"Efficiently solving the personalized broadcast problem in an interconnection network typically relies on finding an appropriate spanning tree in the network. In this paper, we show how to construct in a complete star graph an asymptotically balanced spanning tree, and in an incomplete star graph a near-balanced spanning tree. In both cases, the tree is shown to have the minimum height. In the literature, this problem has only been considered for the complete star graph, and the constructed tree is about 4/3 times taller than the one proposed in this paper.","Tree graphs,
Broadcasting,
Computer Society,
Multiprocessor interconnection networks,
Hypercubes,
Computer science,
Intelligent networks,
Parallel architectures,
Parallel processing,
Information management"
Software package requirements and procurement,"The paper outlines the problems of specifying requirements and deploying these requirements in the procurement of software packages. Despite the fact that software construction de novo is the exception rather than the rule, little or no support for the task of formulating requirements to support assessment and selection among existing software packages has been developed. We analyse the problems arising in this process and review related work. We outline the key components of a programme of research in this area.",
Understanding Application Performance on Shared Virtual Memory Systems,"Many researchers have proposed interesting protocols for shared virtual memory (SVM) systems, and demonstrated performance improvements on parallel programs. However, there is still no clear understanding of the performance potential of SVM systems for different classes of applications. This paper begins to fill this gap, by studying the performance of a range of applications in detail and understanding it in light of application characteristics.We first develop a brief classification of the inherent data sharing patterns in the applications, and how they interact with system granularities to yield the communication patterns relevant to SVM systems. We then use detailed simulation to compare the performance of two SVM approaches---Lazy Released Consistency (LRC) and Automatic Update Release Consistency (AURC)---with each other and with an all-hardware CC-NUMA approach. We examine how performance is affected by problem size, machine size, key system parameters, and the use of less optimized program implementations. We find that SVM can indeed perform quite well for systems of at leant up to 32 processors for several nontrivial applications. However, performance is much more variable across applications than on CC-NUMA systems, and the problem sizes needed to obtain good parallel performance are substantially larger. The hardware-assisted AURC system tends to perform significantly better than the all-software LRC under our system assumptions, particularly when realistic cache hierarchies are used.","Support vector machines,
Support vector machine classification,
Hardware,
Protocols,
Application software,
Computer science,
Gold,
Displays"
Spatial-Query-by-Sketch,"Today's methods for interacting with geographic information systems (GISs) and geographic databases are primarily aspatial, as they require users to deal with geographic data primarily through alphanumeric command languages. Spatial querying by typing a command in some spatial query language or by selecting the same syntax from pull-down menus is a tedious process, because it often requires extensive training in the use of the particular query language, and forces users to translate a spatial image they may have in their mind into a non-spatial language. To overcome this conceptual gap, we propose Spatial-Query-by-Sketch, a sketch-based GIS user interface that focuses on specifying spatial relations by drawing them. This query style more directly supports human spatial thinking, which is critical because users frequently have an image-like representation in their minds when they query about spatial configurations. This paper introduces the fundamental concepts of Spatial-Query-by-Sketch, provides examples of typical interactions and discusses query processing strategies by relaxing the constraints drawn in terms of a qualitative model.",
The new data acquisition system at GSI,"The new general purpose data acquisition system developed at GSI is currently installed at about 30 experiments at GSI and other sites. It is based on the LynxOS operating system. Several CPUs, i.e. GSI developed CAMAC computer boards, VME processor boards, and Aleph Event Builders, are connected by a memory mapped bus, i.e. a VSB or VME bus. It can be configured easily for various hardware setups and is highly optimized for fast trigger rates and data throughput. The first and successful experiment taking data with the system was the SHIP production run for elements 110 and 111",
The women of ENIAC,"A group of young women college graduates involved with the ENIAC (Electronic Numerical Integrator And Computer) are identified. As a result of their education, intelligence, as well as their being at the right place and at the right time, these young women were able to perform important computer work. Many learned to use effectively ""the machine that changed the world"" to assist in solving some of the important scientific problems of the time. Ten of them report on their background and experiences. It is now appropriate that these women be given recognition for what they did as ""pioneers"" of the age of computing.",
Game semantics and abstract machines,"The interaction processes at work by M. Hyland and L. Ong (1994) (HO) and S. Abramsky et al. (1994) (AJM) new game semantics are two preexisting paradigmatic implementations of linear head reduction: respectively Krivine's abstract machine and Girard's interaction abstract machine. There is a simple and natural embedding of AJM-games to HO-games, mapping strategies to strategies and reducing AJM definability (or full abstraction) property to HO's one.",
Derivation of data intensive algorithms by formal transformation: the Schnorr-Waite graph marking algorithm,"Considers a particular class of algorithms which present certain difficulties to formal verification. These are algorithms which use a single data structure for two or more purposes, which combine program control information with other data structures or which are developed as a combination of a basic idea with an implementation technique. Our approach is based on applying proven semantics-preserving transformation rules in a wide spectrum language. Starting with a set theoretical specification of ""reachability"", we are able to derive iterative and recursive graph marking algorithms using the ""pointer switching"" idea of Schorr and Waite (1967). There have been several proofs of correctness of the Schorr-Waite algorithm, and a small number of transformational developments of the algorithm. The great advantage of our approach is that we can derive the algorithm from its specification using only general-purpose transformational rules, without the need for complicated induction arguments. Our approach applies equally well to several more complex algorithms which make use of the pointer switching strategy, including a hybrid algorithm which uses a fixed length stack, switching to the pointer switching strategy when the stack runs out.",
Developing and implementing interactive multimedia in education,"This paper presents first hand experience with the development and implementation of interactive multimedia instructional materials for an engineering economy course attended by 1000 engineering students over three years at Virginia Tech. The interactive software was developed from a National Science Foundation grant aimed at enhancing the undergraduate engineering curriculum through increased emphasis on design and economic principles. The purpose of this paper is to describe the type of multimedia instructional materials that were developed, how they were implemented, and, finally, to present an evaluation of the results from a survey in which the benefits of multimedia-based support materials were assessed.","Environmental economics,
Animation,
Packaging,
Education,
Systems engineering and theory,
Engineering students,
Knowledge engineering,
Acoustical engineering,
Computer graphics"
Tries for approximate string matching,"Tries offer text searches with costs which are independent of the size of the document being searched, and so are important for large documents requiring spelling checkers, case insensitivity, and limited approximate regular secondary storage. Approximate searches, in which the search pattern differs from the document by k substitutions, transpositions, insertions or deletions, have hitherto been carried out only at costs linear in the size of the document. We present a trie based method whose cost is independent of document size. Our experiments show that this new method significantly outperforms the nearest competitor for k=0 and k=1, which are arguably the most important cases. The linear cost (in k) of the other methods begins to catch up, for our small files, only at k=2. For larger files, complexity arguments indicate that tries will outperform the linear methods for larger values of k. The indexes combine suffixes and so are compact in storage. When the text itself does not need to be stored, as in a spelling checker, we even obtain negative overhead: 50% compression. We discuss a variety of applications and extensions, including best match (for spelling checkers), case insensitivity, and limited approximate regular expression matching.",
Induction machine stator fault on-line diagnosis based on LabVIEW environment,"The benefits of machine condition monitoring have been widely recognized as superior with respect to other alternative maintenance approaches. Condition monitoring is an operational strategy for machine integrity assessment, fault identification and life extension. The cost-benefit ratio will be reduced in progress owing to the commercial diagnostic environment availability. This paper presents the implementation of a diagnostic procedure to detect induction machine stator faults based on the LabVIEW environment. The diagnosis is performed by using a suitable neural network, trained by a faulted machine simulator, that has, as input variables, the negative sequence current component and the variation of the positive sequence current component between actual and healthy machine conditions. The statement of a trigger threshold that discerns between true faults and intrinsic dissymetry of the machine is discussed as well.",
Wavelet-based methods for the nonlinear inverse scattering problem using the extended Born approximation,"In this paper, we present an approach to the nonlinear inverse scattering problem using the extended Born approximation (EBA) on the basis of methods from the fields of multiscale and statistical signal processing. By posing the problem directly in the wavelet transform domain, regularization is provided through the use of a multiscale prior statistical model. Using the maximum a posteriori (MAP) framework, we introduce the relative Cramér-Rao bound (RCRB) as a tool for analyzing the level of detail in a reconstruction supported by a data set as a function of the physics, the source-receiver geometry, and the nature of our prior information. The MAP estimate is determined using a novel implementation of the Levenberg-Marquardt algorithm in which the RCRB is used to achieve a substantial reduction in the effective dimensionality of the inversion problem with minimal degradation in performance. Additional reduction in complexity is achieved by taking advantage of the sparse structure of the matrices defining the EBA in scale space. An inverse electrical conductivity problem arising in geophysical prospecting applications provides the vehicle for demonstrating the analysis and algorithmic techniques developed in this paper.",
Recovery of SHGCs from a single intensity view,"Generalized cylinders are a flexible, loosely-defined class of parametric shapes capable of modeling many real-world objects. Straight homogeneous generalized cylinders are an important subclass of generalized cylinders, whose cross-sections are scaled versions of a reference curve. Although there has been considerable research into recovering the shape of SHGCs from their contour, this work has almost exclusively involved methods that couple contour and heuristic constraints. A rigorous approach to the problem of recovering solid parametric shape from a single intensity view should involve at least two stages: (1) deriving the contour constraints, and (2) determining if additional image constraints, e.g., intensity, can be used to uniquely determine the 3D object shape. In this paper, the authors follow the approach just described. This methodology is also important for the recovery of object classes like tubes, where contour and heuristic constraints are shown to be insufficient for shape recovery. First, the authors prove that SHGC contours generated under orthography have exactly two degrees of freedom. Next, the authors show that the remaining free parameters can be resolved using reflectance-based constraints, without knowledge of the number of light sources, their positions, intensities, the amount of ambient light; or the surface albedo. Finally, the reflectance-based recovery algorithm is demonstrated on both synthetic and real SHGC images.",
Predictable communication protocol processing in real-time Mach,"Scheduling of many different kinds of activities takes place in distributed real time and multimedia systems. It includes scheduling of computations, window services, filesystem management, I/O services and communication protocol processing. We investigate the problem of scheduling communication protocol processing in real time systems. Communication protocol processing takes a relatively substantial amount of time and if not structured correctly, unpredictable priority inversion and undesirable timing behavior can result to applications communicating with other processors but are otherwise scheduled correctly. We describe the protocol processing architecture in the RT-Mach operating system, which allows the timing of protocol processing to be under strict application control. An added benefit is also obtained in the form of higher performance. This scheduling architecture is consistent with the: other RT-Mach scheduling mechanisms including fixed priority scheduling and processor reservation. The benefits of this protocol architecture are demonstrated both under synthetic workloads and in a realistic distributed videoconferencing system we have implemented in RT-Mach. End to end delays for both audio and video are as predicted even with other threads competing for the CPU and the network.",
A federated model for scheduling in wide-area systems,"A model for scheduling in wide area systems is described. The model is federated and utilizes a collection of local site schedulers that control the use of their resources. The wide area scheduler consults the local site schedulers to obtain candidate machine schedules. A set of issues and challenges inherent to wide area scheduling are also described and the proposed model is shown to address many of these problems. A distributed algorithm for wide area scheduling is presented and relies upon information made available about the resource needs of user jobs. The wide area scheduler will be implemented in Legion, a wide area computing system developed at the University of Virginia.",
Reducing cache invalidation overheads in wormhole routed DSMs using multidestination message passing,"Current generation distributed shared memory (DSM) systems use point-to-point (unicast) messages for cache invalidations. This incurs a large number of control messages, heavy network traffic, and high occupancy at home nodes. This paper introduces a new approach to reduce these overheads by using multidestination-based reservation and gather worms for distributing invalidation requests and collecting acknowledgments. Different grouping schemes to generate multidestination worms on networks supporting deterministic (e-cube) or adaptive (turn-model) routing are investigated to implement the fully-mapped cache-coherence protocol. For different applications on a 2D mesh system, our simulation results indicate that up to 15% reduction in overall execution time can be achieved by using multidestination messages.",
Wizard: a database inference analysis and detection system,"The database inference problem is a well-known problem in database security and information system security in general. In order to prevent an adversary from inferring classified information from combinations of unclassified information, a database inference analyst must be able to detect and prevent possible inferences. Detecting database inference problems at database design time provides great power in reducing problems over the lifetime of a database. We have developed and constructed a system called Wizard to analyze databases for their inference problems. The system takes as input a database schema, its constituent instances (if available) and additional human-supplied domain information, and provides a set of associations between entities and/or activities that can be grouped by their potential severity of inference vulnerability. A knowledge acquisition process called microanalysis permits semantic knowledge of a database to be incorporated into the analysis using conceptual graphs. These graphs are then analyzed with respect to inference-relevant domains we call facets using tools we have developed. We can determine inference problems within single facets as well as some inference problems between two or more facets. The architecture of the system is meant to be general so that further refinements of inference information subdomains can be easily incorporated into the system.",
Robust and highly customizable recognition of online handwritten Japanese characters,"This paper describes a new online handwritten character recognition system which is composed of coarse classification, linear-time elastic matching, structured character pattern representation and context post-processing. It has marked 90 to 95% correct recognition rates without learning to a large database of on-line handwritten Japanese text. The recognition time is about 0.3 sec./input character on an i486 DX2/66 MHz processor. The system is not only robust to pattern distortions but also highly customizable for personal use. Upon the request of learning an input pattern, it investigates which subpattern (radical) or the pattern as a whole is non-standard, registers the (sub)pattern and extends the effect of the registration to all the character categories whose shapes include it.",
Quantifying spectral characteristics of fricatives,"In a search for spectral parameters that can be used to distinguish and to model fricatives, spectral moments, dynamic amplitude, and slope above maximum amplitude were computed for a fricative corpus including sustained fricatives at different effort levels, and fricatives in vowel context. Moments varied significantly by frequency range used in computation. M3 appeared to vary the least across fricative, contrasting with Forrest et al.'s 1988 study. Dynamic amplitude separated sibilants and non-sibilants, as predicted; slope above the maximum amplitude varied significantly with effort level.",
View-invariant regions and mobile robot self-localization,"This paper addresses the problem of mobile robot self-localization given a polygonal map and a set of observed edge segments. The standard approach uses interpretation tree search with pruning heuristics to match observed edges to map edges. Our approach introduces a preprocessing step in which the map is decomposed into view-invariant regions (VIRs). The VIR decomposition captures information about map edge visibility, and can be used for a variety of robot navigation tasks.",
Adaptive source routing in multistage interconnection networks,"We describe the adaptive source routing (ASR) method which is a first attempt to combine adaptive routing and source routing methods. In ASR, the adaptivity of each packet is determined at the source processor. Every packet can be routed in a fully adaptive or partially adaptive or non-adaptive manner, all within the same network at the same time. We evaluate and compare performance of the proposed adaptive source routing networks and oblivious routing networks by simulations. We also describe a route generation algorithm that determines maximally adaptive routes in multistage networks.",
CALIC-a context based adaptive lossless image codec,"We propose a context-based, adaptive, lossless image codec (CALIC). CALIC obtains higher lossless compression of continuous-tone images than other techniques reported in the literature. This high coding efficiency is accomplished with relatively low time and space complexities. CALIC puts heavy emphasis on image data modeling. A unique feature of CALIC is the use of a large number of modeling contexts to condition a non-linear predictor and make it adaptive to varying source statistics. The non-linear predictor adapts via an error feedback mechanism. In this adaptation process, CALIC only estimates the expectation of prediction errors conditioned on a large number of contexts rather than estimating a large number of conditional error probabilities. The former estimation technique can afford a large number of modeling contexts without suffering from the sparse context problem. The low time and space complexities of CALIC are attributed to efficient techniques for forming and quantizing modeling contexts.",
Dynamic bandwidth allocation policies,"When traffic of connectionless best effort protocols such as IP is carried over connection oriented protocols with guaranteed bandwidth, such as CBR connection in ATM, the interface layer between the protocols (i.e., AAL-the ATM adaption layer) needs to specify the bandwidth requirement and the duration of the bandwidth reservation. The purpose of this paper is to develop policies for deciding and for adjusting the amount of bandwidth requested for a best effort connection over such networks. Our aim is to develop such policies that achieve a good trade off between latency and utilization. The performances of the different policies are compared by an empirical evaluation.",
Similarity queries in image databases,"Query-by-content image database will be based on similarity, rather than on matching, where similarity is a measure that is defined and meaningful for every pair of images in the image space. Since it is the human user that, in the end, has to be satisfied with the results of the query, it is natural to base the similarity measure that we will use on the characteristics of human similarity assessment. In the first part of this paper, we review some of these characteristics and define a similarity measure based on them. Another problem that similarity-based databases will have to face is how to combine different queries into a single complex query. We present a solution based on three operators that are the analogous of the and, or, and not operators one uses in traditional databases. These operators are powerful enough to express queries of unlimited complexity, yet have a very intuitive behavior, making easy for the user to specify a query tailored to a particular need.",
A form-based dialogue manager for spoken language applications,"A popular approach to dialogue management is based on a finite state model, where user utterances trigger transitions between the dialogue states, and these states, in turn, determine the system's response. The paper describes an alternative dialogue planning algorithm based on the notion of filling in an electronic form, or ""E-form"". Each slot has associated prompts that guide the user through the dialogue, and a priority that determines the order in which the system tries to acquire information. These slots can be optional or mandatory. However, the user is not restricted to follow the system's lead, and is free to ignore the prompts and take the initiative in the dialogue. The E-form based dialogue planner has been used in an application to search a database of used car advertisements. The goal is to assist the user in selecting, from this database, a small list of cars which match their constraints. For a large number of dialogues collected from over 600 naive users, we found over 70% compliance in answering specific system prompts.",
A fast algorithm for area minimization of slicing floorplans,"The traditional algorithm for area minimization of slicing floorplans due to Stockmeyer has time and space complexity O(n/sup 2/) in the worst case. For more than a decade, it has been considered the best possible. This paper presents a new algorithm of worst-case time and space complexity O(n log n), where n is the total number of realizations for the basic blocks, regardless whether the slicing is balanced or not. We also show R(n log n) is the lower bound on the time complexity of any area minimization algorithm. Therefore, the new algorithm not only finds the optimal realization, but also has the optimal running time.",
Toward work-centered digital information services,"Work-centered digital information services are a set of library-like services meant to address work group needs. Workplace users especially need to access legacy documents and external collections. They also frequently want to retrieve information (rather than documents per se), and they require that digital information systems be integrated into established work practices. Realizing work-centered digital information systems requires a broad technical agenda. Three types of analysis-document image, natural language, and computer vision, are necessary to facilitate information extraction. Users also need new user interface paradigms and authoring tools to better access multimedia information, as well as improved protocols for client-program interaction with repositories (collections). Moreover, entirely new types of documents must be developed to exploit these capabilities. The system developed by the authors follows a client-server architecture, in which the servers are repositories implemented as databases supporting user-defined functions and user-defined access methods. The repositories also serve as indexing servers. The authors are creating a prototype set of information services called the California Environmental Digital Information System, which includes a diverse collection of environmental data.",
Landmark-based autonomous navigation in sewerage pipes,"We describe a method for an autonomous mobile robot to navigate through a system of sewerage pipes. Landmarks signalling positions in the pipe system have to be detected and classified, where classification is allowed do be unreliable. Self localization is interpreted as a partially observable Markov decision problem and solved accordingly. The method is implemented and used on a prototype robot platform operating in a dry sewage pipe test network.",
Parallel multilevel graph partitioning,"In this paper we present a parallel formulation of a graph partitioning and sparse matrix ordering algorithm that is based an a multilevel algorithm we developed recently. Our parallel algorithm achieves a speedup of up to 56 on a 128-processor Cray T3D for moderate size problems, further reducing its already moderate serial run-time. Graphs with over 200,000 vertices can be partitioned in 128 parts, on a 128-processor Gray T3D in less than 3 seconds. This is at least an order of magnitude better than any previously reported run times on 128-processors for obtaining an 128-partition. This also makes it possible to use our parallel graph partitioning algorithm to partition meshes dynamically in adaptive computations. Furthermore, the quality of the produced partitions and orderings are comparable to those produced by the serial multilevel algorithm that has been shown to substantially outperform both spectral partitioning and multiple minimum degree.",
Carnegie Mellon's software development studio: a five year retrospective,"The Software Development Studio is the centerpiece of Carnegie Mellon's Master of Software Engineering Curriculum. It represents 40 per cent of the course units students spend in the program. The Studio has continuously evolved since its prototype in the spring and summer of 1990. The lessons learned about organization, projects, and other issues are the subject of the paper. The use of a well established development process, a matrix organization, and one on one mentoring give the highest return on investment. This is being written as the seventh class of students prepare to enter the Carnegie Mellon University (CMU) Master of Software Engineering (MSE) Program. The program is a joint effort of the School of Computer Science and the Software Engineering Institute at CMU. The curriculum for the MSE has continuously evolved since its inception, though there have been no major changes for two years. The concept based core curriculum, representing 30 per cent of the units required for the degree, is documented by D. Garlan et al. (1995). The prototype offering of the Studio course, 40 per cent of the units students take, is described by J.E. Tomayko (1991). The paper presents the lessons learned in the five 16 month Studio offerings since then. Twelve projects, some of which were continuations of previous projects, staffed by just under 70 students, form the database from which the information presented is drawn.",
A 3-approximation for the minimum tree spanning k vertices,In this paper we give a 3-approximation algorithm for the problem of finding a minimum tree spanning any k-vertices in a graph. Our algorithm extends to a 3-approximation algorithm for the minimum tour that visits any k-vertices.,
Extended application of suffix trees to data compression,"A practical scheme for maintaining an index for a sliding window in optimal time and space, by use of a suffix tree, is presented. The index supports location of the longest matching substring in time proportional to the length of the match. The total time for build and update operations is proportional to the size of the input. The algorithm, which is simple and straightforward, is presented in detail. The most prominent lossless data compression scheme, when considering compression performance, is prediction by partial matching with unbounded context lengths (PPM). However, previously presented algorithms are hardly practical, considering their extensive use of computational resources. We show that our scheme can be applied to PPM-style compression, obtaining an algorithm that runs in linear time, and in space bounded by an arbitrarily chosen window size. Application to Ziv-Lempel (1977) compression methods is straightforward and the resulting algorithm runs in linear time.",
Modeling the effects of contention on the performance of heterogeneous applications,"Fast networks have made it possible to coordinate distributed heterogeneous CPU, memory and storage resources to provide a powerful platform for executing high-performance applications. However, the performance of these applications on such systems is highly dependent on the allocation and efficient coordination of application tasks. A key component for a performance-efficient allocation strategy is a predictive model which provides a realistic estimate of application performance under varying resource loads. In this paper, we present a model for predicting the effects of contention on application behavior in heterogeneous systems. In particular, our model calculates the slowdown imposed on communication and computation for non-dedicated two-machine heterogeneous platforms. We describe the model for the Sun/CM2 and Sun/Paragon coupled heterogeneous systems. We present experiments on production systems with emulated contention which show the predicted communication and computation costs to be within 15% on average of the actual costs.",
Identifying contact formations from sensory patterns and its applicability to robot programming by demonstration,"This paper presents a pattern recognition approach to identifying contact formations from force sensor signals. The approach is sensor-based and does not use geometric models of the workpieces. The design of a fuzzy classifier is described, when membership functions are generated automatically from training data. The technique is demonstrated using supervised learning. Test results are included for experiments using both rigid and non-rigid workpieces. The technique is discussed in the context of robot programming by human demonstration.",
Scalable backoff language models,"When a trigram backoff language model is created from a large body of text, trigrams and bigrams that occur few times in the training text are often excluded from the model in order to decrease the model size. Generally, the elimination of n-grams with very low counts is believed to not significantly affect model performance. This project investigates the degradation of a trigram backoff model's perplexity and word error rates as bigram and trigram cutoffs are increased. The advantage of reduction in model size is compared to the increase in word error rate and perplexity scores. More importantly, this project also investigates alternative ways of excluding bigrams and trigrams from a backoff language model, using criteria other than the number of times an n-gram occurs in the training text. Specifically, a difference method has been investigated where the difference in the logs of the original and backed off trigram and bigram probabilities is used as a basis for n-gram exclusion from the model. We show that excluding trigrams and bigrams based on a weighted version of this difference method results in better perplexity and word error rate performance than excluding trigrams and bigrams based on counts alone.",
Fault tolerant data structures,"The authors consider the tolerance of data structures to memory faults. They observe that many pointer-based data structures (e.g. linked lists, trees, etc.) are highly nonresilient to faults. A single fault in a linked list or tree may result in the loss of the entire set of data. They present a formal framework for studying the fault tolerance properties of pointer-based data structures, and provide fault tolerant versions of the stack, the linked list, and the dictionary tree.",
An evidential reasoning approach to attribute value conflict resolution in database integration,"Resolving domain incompatibility among independently developed databases often involves uncertain information. DeMichiel (1989) showed that uncertain information can be generated by the mapping of conflicting attributes to a common domain, based on some domain knowledge. We show that uncertain information can also arise when the database integration process requires information not directly represented in the component databases, but can be obtained through some summary of data. We therefore propose an extended relational model based on Dempster-Shafer theory of evidence to incorporate such uncertain knowledge about the source databases. The extended relation uses evidence sets to represent uncertainty in information, which allow probabilities to be attached to subsets of possible domain values. We also develop a full set of extended relational operations over the extended relations. In particular, an extended union operation has been formalized to combine two extended relations using Dempster's rule of combination. The closure and boundedness properties of our proposed extended operations are formulated. We also illustrate the use of extended operations by some query examples.",
The program understanding problem: analysis and a heuristic approach,"Program understanding is the process of making sense of a complex source code. This process has been considered as computationally difficult and conceptually complex. So far no formal complexity results have been presented, and conceptual models differ from one researcher to the next. We formally prove that program understanding is NP hard. Furthermore, we show that even a much simpler subproblem remains NP hard. However we do not despair by this result, but rather offer an attractive problem solving model for the program understanding problem. Our model is built on a framework for solving constraint satisfaction problems, or CSPs, which are known to have interesting heuristic solutions. Specifically, we can represent and heuristically address previous and new heuristic approaches to the program understanding problem with both existing and specially designed constraint propagation and search algorithms.",
Electrical machines and drives: present and future,"The paper discusses the present and future of electrical machines and variable-speed drives. Although at present improved DC drives are emerging the market for DC drives is expected to fall and AC drives will dominate the market. Different manufacturers have released various universal drives, and it is expected that other similar drives will also emerge. There will be an increase in vector-controlled drives (speed-sensorless implementations as well), permanent magnet synchronous motor drives and different types of reluctance motor drives (switched reluctance motor, synchronous reluctance motor). Various drives, including vector and direct torque controlled drives are also discussed and compared. It is also shown that there will be a revolution in the packaging of drives and also in the application of various intelligent control techniques, (fuzzy, neural, fuzzy-neural etc.) techniques. Drive efficiency will be a critical issue. Trends for minimum configuration DSP controlled drives are also discussed. Results of a survey on the future of drives are also presented. This involves a large number of manufacturers.",
Parallel Multilevel k-way Partitioning Scheme for Irregular Graphs,"In this paper we present a parallel formulation of a multilevel k-way graph partitioning algorithm. The multilevel k-way partitioning algorithm reduces the size of the graph by collapsing vertices and edges (coarsening phase), finds a k-way partition of the smaller graph, and then it constructs a k-way partition for the original graph by projecting and refining the partition to successively finer graphs (uncoarsening phase). A key innovative feature of our parallel formulation is that it utilizes graph coloring to effectively parallelize both the coarsening and the refinement during the uncoarsening phase. Our algorithm is able to achieve a high degree of concurrency, while maintaining the high quality partitions produced by the serial algorithm. We test our scheme on a large number of graphs from finite element methods, and transportation domains. Our parallel formulation on Cray T3D, produces high quality 128-way partitions on 128 processors in a little over two seconds, for graphs with a million vertices. Thus our parallel algorithm makes it possible to perform dynamic graph partition in adaptive computations without compromising quality.",
The Hirlam project [meteorology],"In 1985 the national meteorological institutes of Denmark, Finland, Iceland, The Netherlands, Norway, and Sweden joined forces to develop and maintain a numerical short range weather forecasting system. Later Ireland and Spain joined the project, and the Hirlam (high resolution limited area modeling) project is now in its third phase. Version 2 of the Hirlam system, built during the second phase of the project, is now used in routine weather forecasting at almost all participating institutes. Modern numerical weather forecasting systems have three basic components: an analysis unit, a forecast model, and a postprocessor. Analysis provides the initial conditions for the model, based on recent observations and other sources of information. The forecast model spatially and temporally discretizes and then integrates the classical equations-Newton's second law, mass conservation, and thermodynamics. In the postprocessing step, the relevant weather phenomena (for example, wind speed at 10 m height) are calculated from the model variables. The article outlines the analysis unit, forecast model, and postprocessor of the Hirlam system, as well as its implementation on massively parallel processing systems.",
Wavelets: What next?,"The author looks ahead to see what the future can bring to wavelet research. He tries to find a common denominator for ""wavelets"" and identifies promising research directions and challenging problems.",
A risk and control-oriented study of the practices of spreadsheet application developers,"Australian spreadsheet application developers and their development practices in the field were surveyed. The developer population was mainly of graduate level but otherwise varied. Their development practices exhibited a high level of risk with a very low level of managerial, IT department or auditor control. Few of the developers surveyed were aware of a spreadsheet control policy within their organisation and even less had a documented copy available to them. The applications in the study were of significant status and most were developed in relatively uncontrolled environments. Most applications were large and of moderate or high importance. The majority involved corporate rather than purely private data and the output of nearly one third was distributed beyond the organisation where it was developed. The developers usage of design, formula, input, output, review testing, documentation and security controls is reported together with developer opinions as to each control's appropriateness for their particular application. The significance to the management of end-user computing of tolerating a high level of risk is discussed and the need for an end-user spreadsheet control model is established. Suitable metrics to measure spreadsheet complexity, importance and developer expertise are required.",
A set theory for soft computing: a unified view of fuzzy sets via neighborboods,"The notion of fuzzy is context dependent, so for each context very often there is a fuzzy theory. Present papers use the notion of neighborhood systems to unify them. A neighborhood system is an association that assigns to each datum a list of data (a neighborhood). Rough sets and topological spaces are special cases. A ""real world"" fuzzy set should allow small amount of perturbation, so it should have an elastic membership function. Mathematically, such an elastic membership function can be expressed by a highly structured subset of membership function space. Structured sets can be singletons, equivalence classes, neighborhoods, or their fuzzified versions. This paper proposed that fuzzy sets should be abstractly defined by such structures and are termed soft sets (sofsets). Based on such structures, W-sofset, F-sofset, P-sofset, B-sofset, C-sofset, N-sofset, FP-sofset, and FF-sofsets have been identified. In this sequence, a predecessor is always a special case of a successor. Each type represents some implicit form of classical fuzzy theory. It is hoped that such a unified view will provide a useful set theory for soft computing.",
"Series expansions for the incomplete Lipschitz-Hankel integral Ye0(a, z)","Three series expansions are derived for the incomplete Lipschitz-Hankel integral Ye0(a, z) for complex-valued a and z. Two novel expansions are obtained by using contour integration techniques to evaluate the inverse Laplace transform representation for Ye0(a, z). A third expansion is obtained by replacing the Neumann function by its Neumann series representation and integrating the resulting terms. An algorithm is outlined which chooses the most efficient expansion for given values of a and z. Comparisons of numerical results for these series expansions with those obtained by using numerical integration routines show that the expansions are very efficient and yield accurate results even for values of a and z for which numerical integration fails to converge. The integral representations for Ye0(a, z) obtained in this paper are combined with previously obtained integral representations for Je0(a, z) to derive integral representations for He(1)0 (a, z) and He(2)0(a, z). Recurrence relations can be used to efficiently compute higher-order incomplete Lipschitz-Hankel integrals and to find integral representations and series expansions for these special functions and many other related functions.",
An efficient approach to simultaneous transistor and interconnect sizing,"In this paper, we study the simultaneous transistor and interconnect sizing (STIS) problem. We define a class of optimization problems as CH-posynomial programs and reveal a general dominance property for all CH-posynomial programs. We show that the STIS problems under a number of transistor delay models are CH-posynomial programs and propose an efficient and near-optimal STIS algorithm based on the dominance property. When used to solve the simultaneous driver/buffer and wire sizing problem for real designs, it reduces the maximum delay by up to 16.1%, and more significantly, reduces the power consumption by a factor of 1.63/spl times/, when compared with the original designs. When used to solve the transistor sizing problem, it achieves a smooth area-delay trade-off. Moreover, the algorithm optimizes a clock net of 367 drivers/buffers and 59304 /spl mu/m-long wire in 120 seconds, and a 32-bit adder with 1026 transistors in 66 seconds on a SPARC-5 workstation.",
PVR: high-performance volume rendering,"Traditional volume rendering methods are too slow to provide interactive visualization, especially for large 3D data sets. The PVR (parallel volume rendering) system implements parallel volume rendering techniques that speed up the visualization process. Moreover, it helps computational scientists, engineers, and physicians to more effectively apply volume rendering to visualization tasks. The authors describe the PVR system that they have developed in a collaboration between the State University of New York at Stony Brook and Sandia National Laboratories. PVR is an attempt to provide an easy-to-use portable system for high performance visualization with the speed required for interactivity and steering. The current version of PVR consists of about 25000 lines of C and Tcl/Tk code. It has been used at Stony Brook, Sandia, and Brookhaven National Labs to visualize large data sets for over a year.",
Backpropagation and recurrent neural networks in financial analysis of multiple stock market returns,"Proposes a new methodology to aid in designing a portfolio of investment over multiple stock markets. It is our hypothesis that financial stock market trends may be predicted better over a set of markets instead of any one single market. A selection criterion is proposed in this paper to make this choice effectively. This criterion is based upon the observed backpropagation and recurrent neural networks' prediction accuracy, and the overall change recorded in the previous year. The results obtained when using data for four consecutive years over five international stock markets supports our claim. Backpropagation networks use gradient descent to learn spatial relationships. On the other hand, recurrent networks are capable of capturing spatiotemporal information from training data. This paper analyzes application of recurrent networks to the stock market return prediction problem in contrast with backpropagation networks. On the basis of the results observed during these experiments it follows that the effect of learning temporal information was not substantial on the prediction accuracy for the stock market returns.",
On-line adaptation of the SCHMM parameters based on the segmental quasi-Bayes learning for speech recognition,On-line quasi-Bayes adaptation of the mixture coefficients and mean vectors in semicontinuous hidden Markov model (SCHMM) is studied. The viability of the proposed algorithm is confirmed and the related practical issues are addressed in a specific application of on-line speaker adaptation using a 26-word English alphabet vocabulary.,
Application of higher-order vector basis functions to surface integral equation formulations,"Higher-order vector basis functions are described that provide a linear normal and quadratic tangential representation of a vector quantity on rectangular or triangular cells. These functions are used to represent the surface current density on a variety of scatterers, including perfectly conducting plates, spheres, and cone-spheres. Results are compared with lower-order rooftop basis functions that provide a constant normal and linear tangential vector representation. Results suggest that the higher-order basis functions provide improved accuracy for a given number of unknowns but that special functions incorporating edge singularities are needed for a robust treatment of scatterers with corners or edges.",
Document layout structure extraction using bounding boxes of different entitles,"The paper presents an efficient technique for document page layout structure extraction and classification by analyzing the spatial configuration of the bounding boxes of different entities on the given image. The algorithm segments an image into a list of homogeneous zones. The classification algorithm labels each zone as test, table, line-drawing, halftone, ruling, or noise. The text lines and words are extracted within text zones and neighboring text lines are merged to form text blocks. The tabular structure is further decomposed into row and column items. Finally, the document layout hierarchy is produced from these extracted entities.",
Recognition of the multi specularity objects for bin-picking task,"This paper describes a method for recognizing partially occluded objects for bin-picking tasks using the eigen-space analysis. Although effective in recognizing an isolated object, as was shown by Murase and Nayar (1995), the current method can not be applied to piratically occluded objects that are typical in bin-picking tasks. The analysis also requires that the object is centered in an image before recognition. These limitations of the eigen-space analysis are due to the fact that the whole appearance of an object is utilized as a template for the analysis. We propose a new method, referred to as the ""eigen-window"" method, that stores multiple partial appearances of an object in the eigen-space. Such partial appearances require a large number of memory space. To reduce the memory requirement by avoiding redundant windows and to select only effective windows to be stored, a similarity measure among windows is developed. Using a pose clustering method among windows, the method determines the pose of an object and the object type of itself. We have implemented the method and verify the validity of the method.",
Do mediated contexts differ in information richness? A comparison of collocated and dispersed meetings,"Examines the question of whether or not media differ in the perceptions they generate among users with respect to social presence, communication effectiveness and the communication interface. The study observes these factors over a four-session period among 33 groups performing a collaborative writing task. Groups were assigned to one of three treatments: face-to-face, distributed synchronous and distributed asynchronous. No significant differences were observed between distributed and face-to-face conditions in their patterns of change over time. Results do indicate, however, that mediated contexts differed in terms of perceived social presence, communication effectiveness and the communication interface, when considered on a session-by-session basis.",
Two adaptive hybrid cache coherency protocols,"We present and evaluate adaptive, hybrid cache coherence protocols for bus-based, shared-memory multiprocessors. Such protocols are motivated by the observation that sharing patterns vary substantially between different programs and even cache blocks within the same program. Performance measurements across a range of parallel applications indicate that the adaptive protocols we present perform well compared to both write-invalidate and write-update protocols.",
Quicksort on a linear array with a reconfigurable pipelined bus system,"Based on the current fiber optic technology, a new computational model, called a linear array with a reconfigurable pipelined bus system (LARPBS), is proposed in this paper. A parallel quicksort algorithm is implemented on the model, and its time complexity is analyzed. For a set of N numbers, the quicksort algorithm reported in this paper runs in O(log/sub 2/N) average time on a linear array with a reconfigurable pipelined bus system of size N. Besides proposing a new algorithm on the model, some basic data movement operations involved in the algorithm are discussed. We believe that these operations can be used to design other parallel algorithms on the same model. Future research in this area is also identified in this paper.",
A gray image compression using a Hilbert scan,"Hilbert curve is one of the space-filling curves published by Peano. There are several applications using this curve such as image processing, computer hologram, etc. In this paper, we concentrate on a lossy compression technique for a gray image using the Hilbert curve. The merit of this curve is to pass through all points in a quadrant, and it always moves to the neighbor quadrant. Our method is based on this neighborhood property, by a simple segmentation of the scanned one-dimensional data using a zero order interpolation. From our experiments, we have confirmed that in spite of the simple computation in comparison to JPEG, acceptable quality images can be obtained at bit-rates above 0.6 bit/pixel.",
HDL optimization using timed decision tables,"System-level presynthesis refers to the optimization of an input HDL description that produces an optimized HDL description suitable for subsequent synthesis tasks. In this paper, we present optimization of control flow in behavioral HDL descriptions using external Don't Care conditions. The optimizations are carried out using a tabular model of system functionality, called Timed Decision Tables or TDTs. TDT based optimization presented here have been implemented in a program called PUMPKIN. Optimization results from several examples show a reduction of 3-88% in the size of synthesized hardware circuits depending upon the external Don't Care information supplied by the user.",
On effective execution of nonuniform DOACROSS loops,"It is extremely difficult to parallelize DOACROSS loops with nonuniform loop-carried dependences. In this paper, we present a static scheduling scheme with an accompanying synchronization strategy that can execute such DOACROSS loops effectively and efficiently. Our approach uses one of the parallelization techniques called Dependence Uniformization, which finds a small set of uniform dependence vectors to cover all possible nonuniform dependences in a DOACROSS loop. It differs from the previous schemes in that we demonstrate a better way to select the uniform dependence vectors. When used with the Static Strip Scheduling scheme, the proposed uniform dependence vector set allows us to enforce dependences with more locality, which reduces the requirement of explicit synchronization considerably while retaining most of the parallelism. This paper describes the uniform dependence vectors selection strategy and the static strip scheduling scheme. The performance analysis and examples are also presented.",
Ten misconceptions about minimalism,"We describe ten common misconceptions about the minimalist approach to documentation design. For each, we analyze how the misconception arises from plausible interpretations of minimalist principles and heuristics. We then clarify how each misconception deviates from minimalism, as we understand it. Analysis and discussion of creative elaborations of minimalism-including ""misconceptions""-can promote a sharper concept of what minimalism is.",
Fail-aware failure detectors,"In existing asynchronous distributed systems it is impossible to implement failure detectors which are perfect, i.e. they only suspect crashed processes and eventually suspect all crashed processes. Some recent research has however proposed that any ""reasonable"" failure detector for solving the election problem must be perfect. We address this problem by introducing two new classes of fail-aware failure detectors that are (1) implementable in existing asynchronous distributed systems, (2) not necessarily perfect, and (3) can be used to solve the election problem. In particular we show that there exists a fail-aware failure detector that allows to solve the election problem and which is strictly weaker than a perfect failure detector.",
Structural gate decomposition for depth-optimal technology mapping in LUT-based FPGA design,"In this paper, we study the problem of decomposing gates in fanin-unbounded or K-bounded networks such that the K-input LUT mapping solutions computed by a depth-optimal mapper have minimum depth. We show (1) any decomposition leads to a smaller or equal mapping depth regardless the decomposition algorithm used, and (2) the problem is NP-hard for unbounded networks when K/spl ges/3 and remains NP-hard for K-bounded networks when K/spl ges/5. We propose a gate decomposition algorithm, named DOGMA, which combines level-driven node packing technique (Chortle-d) and the network flow based optimal labeling technique (FlowMap). Experimental results show that networks decomposed by DOGMA allow depth-optimal technology mappers to improve the mapping solutions by up to 11% in depth and up to 35% in area comparing to the mapping results of networks decomposed by other existing decomposition algorithms.",
Deadlock detection and recovery in flexible production systems with multiple capacity resources,"Flexible production systems exhibit a high degree of resource sharing which can lead to deadlock situations. Since deadlocks are highly undesirable, it is necessary to adopt recovery or avoidance policies to resolve such situations. This paper introduces a graph-theoretic approach for deadlock detection/recovery in production systems with multiple capacity resources. The method uses a digraph that, owing to its transparent meaning, characterizes the deadlock occurrence efficiently. A case study illustrates the method that appears also suitable for real-time applications.",
A practical approach to collision detection between general objects,"Since collision detection is becoming a bottleneck for real-world applications involving increasingly complex geometries, more efficient ways of detecting collisions are called for. Hierarchies of detail based on spheres seem a powerful approach to overcome this problem. A representation for non-convex objects with curved surfaces is presented, which is independent of the number of features used for the polyhedral model of the object. A new algorithm for collision detection between many moving objects is described. Experiments with two different robots show the efficiency of the method.",
A pre-emptive transaction scheduling protocol for controlling priority inversion,"Hard real-time database systems (RTDBS) must provide a guarantee that real-time transactions meet their deadlines. To preserve data consistency, hard RTDBS require concurrency control protocols to synchronize transactions to access the shared data. Transaction blocking enforced by concurrency control protocols leads to the priority inversion problem that violates the principle of priority-based scheduling and degrades system schedulability. Moreover, this blocking delay due to priority inversion can be unbounded, which is unacceptable in hard real-time applications. Some priority ceiling protocols have been proposed to control priority inversion. However, they suffer from the problem of unnecessary transaction blockings due to their conservatism of scheduling transactions to access the shared data. We propose a new transaction scheduling protocol that exploits the semantics of transaction operations to enhance transaction preemptability. It can avoid some unnecessary transaction blockings and provide better worst-case scheduling conditions for a transaction set compared to other protocols.",
Games and full abstraction for FPC,"We present a new category of games, /spl Gscr/, and build from it a cartesian closed category I and its extensional quotient /spl epsi/. /spl epsi/ represents an improvement over existing categories of games in that it has sums as well as products, function spaces and recursive types. A model of the language FPC, a sequential functional language with just this type structure, in /spl epsi/ is described and shown to be fully abstract.",
The odd-even input-queueing ATM switch: performance evaluation,"This paper introduces and studies the performance of an N/spl times/N space-division, single-stage ATM switch with dual input-queueing. Each input port has two separate FIFO queues, an ""odd"" and an ""even"" queue. An incoming cell is stored at the input at either of two FIFOs according its output port destination (output ports are also labeled as ""odd"" or ""even""). Hence we call this scheme the odd-even switch. We compare the odd-even switch to an ordinary input-buffered switch and we find that it can achieve a considerably higher throughput. This is due to the fact that the head-of-line effect is less problematic under the odd-even scheme. We present results for various traffic models. Finally, we compare the odd-even strategy to the look-ahead (input ""window"") scheme.",
Implementing concepts from the Personal Software Process in an industrial setting,"The Personal Software Process (PSP) has been taught at a number of universities with impressive results. If is also of interest to industry as a means for training their software engineers. While there are published reports on the teaching of PSP in classroom settings (at universities and industry), little systematic study has been conducted on the implementation of PSP in industry. Also, largely anecdotal evidence exists as to its effectiveness with real programming tasks. Effectiveness is measured in terms of the number of trained engineers who actually use PSP in their daily work, and improvements in productivity and defect removal. We report on a study of the implementation of some PSP concepts in a commercial organization. The empirical enquiry method that we employed was action research. Our results identify the problems that were encountered during the four major activities of an implementation of PSP: planning, training, evaluation, and leveraging. We describe how these problems were addressed, and the general lessons learned from the implementation. An overall transfer of PSP training rate of 46.5% was achieved. For the engineers in our study, those who applied all of the taught PSP concepts on-the-job improved their defect detection capabilities.",
Fast fault-tolerant concurrent access to shared objects,"The authors consider a synchronous model of distributed computation in which n nodes communicate via point-to-point messages, subject to the following constraints: (i) in a single ""step"", a node can only send or receive O(logn) words, and (ii) communication is unreliable in that a constant fraction of all messages are lost at each step due to node and/or link failures. They design and analyze a simple local protocol for providing fast concurrent access to shared objects in this faulty network environment. In the protocol, clients use a hashing-based method to access shared objects. When a large number of clients attempt to read a given object at the same time, the object is rapidly replicated to an appropriate number of servers. Once the necessary level of replication has been achieved, each remaining request for the object is serviced within O(1) expected steps. The protocol has practical potential for supporting high levels of concurrency in distributed file systems over wide area networks.",
New single-error-correcting codes,A matrix construction of nonlinear error-correcting codes is considered. It is shown how this construction and some related theorems can be applied to old codes to get new codes with minimum distance 3. In total 13 new binary single-error-correcting codes of length at most 511 are obtained.,
A distributed hypermedia link service,"Open hypermedia is an established model within the hypermedia research community; it provides an approach to engineering flexible and maintainable hypermedia systems based on the concept of a hypermedia link service. However the World Wide Web, which provides an important solution for distributed hypermedia, is typically used as a closed hypermedia system: it does not exploit the link service concept. This paper presents the design of a system which applies the open hypermedia philosophy to the World Wide Web, whereby the link service component is abstracted out into a third party service known as the distributed link service (DLS). A scalable architecture for the DLS is discussed and experience with a prototype is described.",
A new algorithm for implementation of design functions by available devices,"In CAD systems, it is often required to implement desired behaviors by some available device. The selection of the device that can implement the behavior, and the required interfacing, is usually done by human experts. The interface consists of transformations that may have to be performed on the inputs and outputs of the device. This paper describes an approach to automatically derive the specifications of the device's interface. For sequential circuits, the behaviors are often represented as FSMs, and hence the task is to determine whether the FSM of the desired function can be contained in the FSM of the device, subject to the transformations of the interface. A related objective, addressed in this paper, is to model the behaviors of complex devices in a way that facilitates the analysis.",
Generating fuzzy rules from data,"This paper introduces an effective method of developing fuzzy rules from continuous valued data. The fuzzy rules may be used for control applications without tuning. The fuzzy rules are created by exploiting the properties of decision trees, as embodied by the C4.5 decision tree learning system. A crisp decision tree is created by creating a discrete set of fuzzy output classes and providing a set of training examples to C4.5. Fuzzy rules are then extracted from the decision tree. The fuzzy rule learning system has been applied to chemical plant start-up control and the Box-Jenkins gas furnace prediction problem. Comparisons are made to fuzzy rule sets created by others for these problems. The learned rules are able to provide smooth control.",
Fred: an architecture for a self-timed decoupled computer,"Decoupled computer architectures provide an effective means of exploiting instruction level parallelism. Self-timed micropipeline systems are inherently decoupled due to the elastic nature of the basic FIFO structure, and may be ideally suited for constructing decoupled computer architectures. Fred is a self-timed decoupled, pipelined computer architecture based on micropipelines. We present the architecture of Fred, with specific details on a micropipelined implementation that includes support for multiple functional units and out-of-order instruction completion due to the self-timed decoupling.",
Efficient execution of parallel applications in multiprogrammed multiprocessor systems,"Existing techniques for sharing the processing resources in multiprogrammed shared-memory multiprocessors, such as time-sharing, space-sharing and gang-scheduling, typically sacrifice the performance of individual parallel applications to improve overall system utilization. We present a new processor allocation technique that dynamically adjusts the number of processors an application is allowed to use for the execution of each parallel section of code based on the current system load. This approach exploits the maximum parallelism possible for each application without overloading the system. We implement our scheme on a Silicon Graphics Challenge multiprocessor system and evaluate its performance using applications from the Perfect Club benchmark suite and synthetic benchmarks. Our approach shows significant improvements over traditional time-sharing and gang-scheduling. It has a performance comparable to, or slightly better than, static space-sharing, but our strategy is more robust since, unlike static space-sharing, it does not require a priori knowledge of the applications' parallelism characteristics.",
Using remote memory to avoid disk thrashing: a simulation study,"The increasing use of high-bandwidth and low-latency networks make possible the use of remote (network) memory as an alternative to disk means of storing an application's data, because remote-to-local memory transfers over a modern interconnection network are faster than traditional disk-to-memory transfers. We explore the possibility of using the remote memory as (i) a (faster-than-disk) backing store, (ii) an extension of main memory accessed using single (remote) memory references, and (iii) as a combination of both. We use execution driven simulation to investigate the performance impact the use of remote memory has on several real programs. We conclude that even for today's low throughput networks, using remote memory as a place for storing (some) of an application's data may result in significant performance improvements, which will continue to get higher as the disparity between disk transfer rates and network transfer rates continues to increase.",
Experiments on dextrous manipulation without prior object models,"In this paper we present a kinematic method for 6-degree-of-freedom manipulation of rigid objects using a dextrous robotic hand. Our method does not require prior models of the objects to be manipulated; all the information needed can be obtained directly from the hand's sensors. The method allows arbitrary (within the robot's physical limits) translations and rotations of the object, and its low computational cost makes real-time performance easy to achieve. We present experimental results of manipulation using the Utah/MIT hand, a 16-DOF dextrous manipulator, and show that with the addition of a Cartesian controller a high degree of accuracy can be attained.",
Mechatronics in the Netherlands,"This article assesses the present situation of mechatronics in the Netherlands. After a short historical survey, it describes the postgraduate ""mechatronic designer course"", introduced in 1991. It deals with the principles of this course and how these principles have been implemented. Also, the activities of the Dutch government in cooperation with the industrial mechatronics community to enhance the awareness of mechatronics, especially directed toward small and medium-sized enterprises (SMEs) is described.",
Dynamic tuning of parameterized defuzzification methods applied to automatic control and diagnosis,"Tuning parameterized defuzzification methods by dynamically adjusting parameters values according to dynamic changes in the environment have proven successful in optimizing fuzzy systems. This paper illustrates the techniques involved by discussing examples of dynamically tuning of one type of parameterized defuzzification method, those based on the basic defuzzification distribution (BADD) method. An application of dynamic tuning of parameterized defuzzification in optimizing automatic control and diagnostic systems which use fuzzy reasoning underline the potential offered by this technique in coping with complex tasks.",
An adaptive data compression method based on context sorting,"Every symbol in the data can be predicted by taking its immediately preceding symbols, or context, into account. This paper proposes a new adaptive data compression method based on a technique of context sorting. The aim of context sorting is to sort a set of contexts in order to find previous contexts similar to the current one. The proposed method predicts the next symbol by ranking the previous context-symbol pairs in order of context similarity. The codeword for the next symbol represents the rank of the symbol in this ordered sequence. The compression performance is evaluated both analytically and empirically. Although the proposed method uses no probability distribution to make a prediction, it has comparable compression performance to the best known data compression utilities.",
Proposed power electronics curriculum,"In this paper, an attempt is made to present undergraduate and graduate curricula in the field of power electronics. This includes detailed class contents for power electronics I & II and associated laboratory assignments for the first course. This work is motivated by the current efforts under way at the University of Central Florida (USA) to develop the power program in the Electrical and Computer Engineering Department. Based on the recent NSF Workshop on power electronics education, this paper also sheds some light into the educational aspects of the field of power electronics. Finally, an outline for the hardware laboratory as supporting facility for effective power electronics education is also addressed.",
Silicon drift photodiode array detectors,"Silicon drift photodiodes have been constructed in a two-dimensional array format for the purpose of position sensitive detection. The arrays are dimensioned 4/spl times/4, with pixel size 3 mm /spl times/3 mm, and are intended to be used with discrete-channel readout electronics. This aspect of the design limits the overall array dimension in future prototype versions to approximately 10/spl times/10, but allows for higher total count rates than multiplexed or serial-read arrays. The photodiode arrays use the floating cathode ring design described by others. The design reported here, however, differs in that these devices have been specifically processed to maximize the detection efficiency of visible light. This feature makes these detectors attractive for use in position-sensitive scintillator readout. Measurements are presented using these photodiodes coupled to common scintillator crystals to detect gamma-rays. Measurements are also shown using these photodiodes to directly detect low energy X-rays. Typical room temperature noise (ENC) measured in the prototype arrays is 60 electrons rms.",
Estimating the Cost of Throttled Execution in Time Warp,,
An innovative course emphasizing real-time digital signal processing applications,"This paper presents an innovative undergraduate real-time digital signal processing (DSP) course, Digital Signal Processors, which emphases hands-on experiments and practical applications. It enables students to experiment with sophisticated DSP applications to augment the theoretical, conceptual, and analytical material provided in their first DSP course, Digital Filter Design. The inclusion of both software and hardware developmental experiences permits undergraduates to undertake a wide range of real-time DSP projects in their required Senior Engineering Design course. This paper briefly introduces representative examples of some challenging DSP applications, such as acoustic echo cancellation, active noise control, and image compression.",
Residual fault density prediction using regression methods,"Regression methods are used to model residual fault density in terms of several product and testing process measures. Process measures considered include discovered fault density, test set size and various coverage measures such as block, decision and all-uses coverage. Product measures considered include lines of code as well as block, decision and all-uses counts. The relative importance of these product/process measures for predicting residual fault density is assessed for a specific data set. Only selected testing process measures, in particular discovered fault density and decision coverage, are important predictors in this case while all product measures considered are important. These results are based on consideration of a substantial family of models, specifically, the family of quadratic response surface models with two-way interaction. Model selection is based on ""leave one out at a time"" cross-validation using the predicted residual sum of squares (PRESS) criterion.",
AND/EXOR-based synthesis of testable KFDD-circuits with small depth,"Decision Diagrams are used in design automation for efficient representation of Boolean functions. It is also possible to directly derive circuits from Decision Diagrams. In this paper we present an approach to synthesize circuits from a very general class of Decision Diagrams, the ordered Kronecker Functional Decision Diagrams. These Decision Diagrams make use of Davio decompositions which are based on exclusive-or operations and therefore allow the use of EXOR gates in the synthesized circuits. We investigate area, depth, and testability of these circuits and compare them to circuit designs generated by other synthesis tools. Experimental results show that the presented approach is suitable to overcome the trade-off between depth and testability at the price of reasonable area overhead.",
What should computer scientists teach to physical scientists and engineers? 1.,"To help clarify the issues involved in deciding what computing skills to teach to physical scientists and engineers, the article presents a thought experiment. Imagine that every new graduate student in science and engineering at your institution, or every new employee in your company's R&D division, has to take an intensive one week computing course. What would you want that course to cover? Should it concentrate on algorithms and data structures, such as multigrid methods and adaptively refined meshes? Should it introduce students to one or two commonly used packages, such as Matlab and SAS? Or should it try to teach students the craft of programming, giving examples to show why modularity is important and how design cycles work. The author chose one week as the length of our idealized course because it is long enough to permit discussion of several topics, but short enough to force stringent prioritization.",
Using knowledge-based transformations to reverse-engineer COBOL programs,"We describe a program restructuring tool under development. The tool is constructed using program transformations executed by the TAMPR program transformation system. We discuss the knowledge embodied in the transformations and how they restructure an example COBOL program developed in the mid-1970s. While the tool needs to be extended further to produce a robust commercial product, early use for restructuring COBOL programs demonstrates the power and flexibility of this transformational approach.",
History consideration in reconstructing polyhedral surfaces from parallel slices,"We introduce an algorithm for reconstructing a solid model given a series of planar cross sections. The main contribution of this work is the use of knowledge obtained during the interpolation of neighboring layers while attempting to interpolate a particular layer. This knowledge is used to reconstruct a surface in which consecutive layers are connected smoothly. In most previous work, each layer is interpolated independently of what happened or will happen in the other layers. We also discuss various objective functions which aim to optimize the reconstruction, and present an evaluation of the different objective functions by using various criteria.",
"An MPI library which uses polling, interrupts and remote copying for the Fujitsu AP1000+","A complete implementation of MPI for the Fujitsu AP1000+ is presented. The library can employ a number of different mechanisms in implementing the send and receive message passing operations. The method of detecting the arrival of new messages can be realized through interrupt-driven and polling techniques. Transferring message data is achieved by either sending the message data directly to the receiver ""in-place"", or using a rendezvous method which allows the use of a fast noncopying nonblocking remote-fetching operation. The MPI library exhibits good performance compared to the native message passing library, and allows the user to decide at runtime which mechanisms will be used in order to achieve the best performance on a per-application basis.",
Processing of 3D DIC microscopy images for data visualization,"Differential interference contrast (DIC) microscopy is a popular method for studying the three dimensional structure of living cells. Currently, no volume rendering tools exist which support visualisation of 3D DIC data. The authors develop a transformation method which removes the differential appearance of DIC imager, producing data which is suitable for volume rendering, and compare this method to an edge-enhancing approach based on a variance filter.",
Solving randomly generated fuzzy constraint networks using evolutionary/systematic hill-climbing,"This paper introduces an evolutionary/systematic hybrid which combines the concept of evolutionary hill-climbing search with the systematic search concept of arc revision to form a hybrid that quickly find solutions to fuzzy constraint satisfaction problems. This hybrid outperforms a modified version of a well known hill-climber, the iterative descent method, on a test suite of 500 randomly generated fuzzy constraint networks.",
Scalable interfaces to support program comprehension,"Studies of how programmers understand code suggest that programmers approach the understanding task in both bottom-up and top-town ways, depending on the context. We present a tool, VIPR, that provides a unified visual representation of both high-level and low-level constructs and a smooth transition between the two levels through smoothly animated zooming and focus and context ('fisheyeing') techniques. VlPR is currently being used to visualize Tcl programs, but the technique is generally applicable to programs written in any imperative programming language.",
An HMMRF-based statistical approach for off-line handwritten character recognition,"We propose a new methodology for off-line handwritten character recognition using a 2D hidden Markov mesh random field (HMMRF)-based statistical approach. In the HMMRF model for character recognition, the inputs to the model are assumed to be sequences of discrete symbols chosen from a finite alphabet. In the proposed methodology, the grey-level input image is first divided into nonoverlapping blocks with same size. Then, each block is encoded into a discrete symbol based on the local features of the block by using the vector quantizer. The HMMRF-based statistical approach necessitates two phases: the decoding phase and the training phase. In both phases we use the lookahead scheme based on a maximum, marginal a posteriori probability criterion for a third-order HMMRF model. In order to verify the performance of the proposed methodology for off-line handwritten character recognition, a large-set handwritten Hangul database was used. Experimental results revealed the viability of the HMMRF-based statistical approach on the task of off-line handwritten character recognition.",
FPGA-based high performance page layout segmentation,"A page layout segmentation algorithm for locating text, background and halftone areas is presented. The algorithm has been implemented on Splash 2-an FPGA-based array processor. The speed as determined by the Xilinx synthesis tools projects an application speed of 5 MHz. For documents of size 1,024/spl times/1,024 pixels, a significant speedup of two orders of magnitude compared to a SparcStation 20 has been achieved.",
Fuzzy inference neural network for fuzzy model tuning,"In fuzzy modeling, it is relatively easy to manually define rough fuzzy rules for a target system by intuition. It is, however, time-consuming and difficult to fine-tune them to improve their behavior. This paper describes a tuning method for fuzzy models which is applicable regardless of the form of fuzzy rules and the used defuzzification method. For this purpose, this paper proposes a fuzzy neural network model which can embody fuzzy models. The proposed model provides the functions to perform fuzzy inference and to tune the parameters for the shape of antecedent linguistic terms, the relative importance degrees of rules, and the relative importance degrees of antecedent linguistic terms in rules. In addition, to show its applicability, we perform some experiments and present the results.",
Application of controller area network to mobile robots,A mobile robot hardware architecture generally consists of a number of sensors and actuators connected to a central processing unit. This type of architecture is time consuming and leads to an unreliable system. The purpose of this paper is to present a decentralized architecture made possible by recent advances in electronics. This architecture is based on the controller area network (CAN) bus and is shown to be very suitable and advantageous in mobile robotics.,
An efficient encoding algorithm for image compression hardware based on cellular automata,Analytical study of three neighbourhood two state per cell Cellular Automata (CA) behaviour is a comparatively recent phenomenon. A wide variety of applications have been developed utilizing the elegant structure of group CA. In this paper some of the characterizations of the state transition behaviour of non-group CA are reported. These results are subsequently utilized to develop an efficient parallel scheme for image compression. The experimental results confirms compression ratio of the CA based scheme is of the same order as that of JPEG coding.,
Tensor codes for the rank metric,"Linear spaces of n/spl times/n/spl times/n tensors over finite fields are investigated where the rank of every nonzero tensor in the space is bounded from below by a prescribed number /spl mu/. Such linear spaces can recover any n/spl times/n/spl times/n error tensor of rank /spl les/ (/spl mu/-1)/2, and, as such, they can be used to correct three-way crisscross errors. Bounds on the dimensions of such spaces are given for /spl mu//spl les/2n+1, and constructions are provided for /spl mu//spl les/2n-1 with redundancy which is linear in n. These constructions can be generalized to spaces of n/spl times/n/spl times/.../spl times/n hyper-arrays.",
Power-oriented graphs for modeling electrical machines,"The paper shows how to obtain the dynamic models of electrical machines by using a modeling technique named ""power-oriented graphs"". In particular, the dynamic model of a three-phase synchronous motor (brushless) is derived. The power-oriented graphs technique uses the ""power interaction"" between sub-systems as key element for modeling real systems. The block schemes obtained with this modeling technique are simple, modular and easy to use.",
On Extending More Parallelism to Serial Simulators,,
Computer based waveform synthetizer for switching regulators characterization,"This paper introduces a virtual instrument which acts as a programmable multisine waveform generator suitable for switching regulators' characterization. Inserting this programmable generator in a standardized computer-based instrumentation system, regulator performances such as stability and audiosusceptibility are obtained. A PWM buck switching regulator is used to compare the measured and predicted values.",
Finite array scan impedance Gibbsian models,"Oscillations in scan impedance over a finite array have been shown previously; here they are modeled by a Gibbs-type standing wave. The wave period derived from the transform of a single pulse with phase shift is .5λ/(1 - sin θ0), which closely fits simulated array data. Gibbsian models containing sine integrals provide fair to excellent matches to computer-simulated dipole arrays, with and without screen, and for E and H plane scans. The E plane scan results are new, as are those for the H plane scan with screen.",
Fusion of short-axis and long-axis cardiac MR images,"A method is introduced for fusing the short-axis and long-axis cardiac MR images into an isotropic volume image. A volume image obtained by this method contains the left ventricular (LV) cavity in one piece, facilitating measurement of its shape and volume. The main goal in this image fusion is to reconstruct the LV cavity in volume form and in high resolution. The accuracy of the method is measured using a synthetic image. Examples of image fusion using real images are also presented.",
Teaching parallel processing using free resources,"Parallel processing can be taught effectively even though parallel hardware is not available. Free software is available to support the three major paradigms of parallel computing. Parallaxis is a software simulator that is based on the SIMD model. PVM and MPI allow one to treat a network of workstations as a message passing MIMD multicomputer with distributed memory. A locally developed shared memory simulator supports the MIMD model of computing with a common shared memory. While none of these products is superior to an actual parallel computer, each can be used in a variety of courses to give students experience with parallel algorithms.",
Teaching microcontrollers,"This presentation provides a brief look at the process used to teach microcontrollers to junior/seniors in EE and CpE. Each pair of students is issued a development hardware kit and related software. All software development is perfected on the student's required personal computer. The style presented permits teaching of several microcontrollers i.e. Motorola 68HC11, Intel 8051 and Microchip PIC family. Example laboratories are suggested.",
Using PVS to analyze hierarchical state-based requirements for completeness and consistency,"Previously, we have defined procedures for analyzing hierarchical state based requirements specifications for two properties: (1) completeness with respect to a set of criteria related to robustness (a response is specified for every possible input and input sequence) and (2) consistency (the specification is free from conflicting requirements and undesired nondeterminism) (M.P.E. Heimdahl and N.G. Leveson, 1995; 1996). We implemented the analysis procedures in a prototype tool and evaluated their effectiveness and efficiency on a large real world requirements specification expressed in an hierarchical state based language called RSML (Requirements State Machine Language). Although our approach has been largely successful, there are some drawbacks with the current implementation that must be addressed. Our prototype implementation uses Binary Decision Diagrams (BDDs) to perform the analysis. Unfortunately, since BDDs treat predicates and functions as uninterpreted and thus fail to capture their semantics, the use of BDDs can lead to large numbers of spurious (false) error reports. We are currently investigating how the Prototype Verification System (PVS) and its theorem proving component can help us increase the accuracy of our analysis. PVS is a verification system that provides an interactive environment for writing formal specifications and checking formal proofs. The paper discusses the problems with spurious error reports and describes our experiences using the Prototype Verification System to increase the accuracy of our analysis results.",
Modular neural networks evolved by genetic programming,"We present an evolvable model of modular neural networks which are rich in autonomy and creativity. In order to build an artificial neural network which is rich in autonomy and creativity, we have adopted the ideas and methodologies of artificial life. The paper describes the concepts and methodologies for the evolvable model of modular neural networks, which will be able not only to develop new functionality spontaneously but also to grow and evolve its own structure autonomously. Although the ultimate goal of this model is to design the control system for such behavior based robots as Khepera, we have attempted to apply the mechanism to a visual categorization task with handwritten digits. The evolutionary mechanism has shown a strong possibility to generate useful network architectures from an initial set of randomly connected networks.",
The extended cube connected cycles: an efficient interconnection for massively parallel systems,"The hypercube structure is a very widely used interconnection topology because of its appealing topological properties. For massively parallel systems with thousands of processors, the hypercube suffers from a high node fanout which makes such systems impractical and infeasible. In this paper, we introduce an interconnection network called The Extended Cube Connected Cycles (ECCC) which is suitable for massively parallel systems. In this topology the processor fanout is fixed to four. Other attractive properties of the ECCC include a diameter of logarithmic order and a small average interprocessor communication distance which imply fast data transfer. The paper presents two algorithms for data communication in the ECCC. The first algorithm is for node-to-node communication and the second is for node-to-all broadcasting. Both algorithms take O(log N) time units, where N is the total number of processors in the system. In addition, the paper shows that a wide class of problems, the divide and conquer class, is easily and efficiently solvable on the ECCC topology. The solution of a divide and conquer problem of size N requires O(log N) time units.",
Redesign of the ASDEX Upgrade plasma position and shape controller,"In the ASDEX Upgrade tokamak, position and selected shape parameters are controlled by a parallel computer. Years of operational experience went into its redesign. Mapping the functionality of the algorithm into modules of code provides a sound base for extension to full shape control. A rule-based coordination layer guides controller action depending on discharge programme and plasma and internal states",
Synthesis of a set of real-valued shift-orthogonal finite-length PN sequences,"The finite-length sequence whose non-periodic autocorrelation function takes zero sidelobes except at left and right shift ends can be called the shift-orthogonal finite-length sequence, and is effective to reduce the intersymbol interference in a direct-sequence spread-spectrum communication system. This paper proposes the synthesis of a set of real-valued shift-orthogonal finite-length sequences which has family size 2/sup /spl nu// or 2/sup /spl nu/+1/ and length 2/sup /spl nu//+1, /spl nu/=1, 2, 3, ... and is suitable for the fast correlation processing. A sequence is synthesized by convolving the /spl nu/+1 original or order-reversed elementary sequences from the sequences of lengths 2 and 5. The fast correlation for the sequence is decomposed to the (/spl nu/+1)-stage correlations.",
TCP over ATM: simulation model and performance results,"While the transmission control protocol (TCP) generally provides robust performance across many network environments, several researchers have identified the poor end-to-end performance achieved by TCP on asynchronous transfer mode (ATM) networks. The performance problems arise for several reasons: (1) a size mismatch between TCP segments and ATM cells; (2) the simple protocols used for segmentation and reassembly in ATM, and for retransmission in TCP; (3) specific ""optimizations"" in TCP, made primarily for low-bandwidth Internet environments which do not work well in high speed ATM networks; (4) features (or misfeatures) in most TCP protocol implementations; and (5) subtle interactions between all of these factors. This paper describes a simulation model that we have constructed to study TCP performance on ATM networks, as well as a set of simulation experiments conducted using the model. The TCP model is detailed enough to recreate many of the performance problems identified by other researchers, as well as to evaluate potential solutions to the performance problems. The TCP model adds to the traffic modeling toolkit available in our existing ATM network simulator, and enables the further study of performance issues in TCP over ATM.",
Higher dimensional transition systems,"We introduce the notion of higher dimensional transition systems as a model of concurrency providing an elementary, set-theoretic formalisation of the idea of higher dimensional transition. We show an embedding of the category of higher dimensional transition systems into that of higher dimensional automata which cuts down to an equivalence when we restrict to non-degenerate automata. Moreover, we prove that the natural notion of bisimulation for such structures is a generalisation of the strong history preserving bisimulation, and provide an abstract categorical account of it via open maps. Finally, we define a notion of unfolding for higher dimensional transition systems and characterise the structures so obtained as a generalisation of event structures.",
Fuzzy logic and fuzzy systems: recent developments and future directions,"Over the last decade or so, several parallel advances have been made in the two distinct disciplines: fuzzy logic and neural networks. As the names imply, the theory of fuzzy logic provides a mathematical framework for the emulation of certain perceptual and linguistic attributes associated with human cognition, whereas the science of neural networks provides new computing morphologies with learning and adaptive capabilities. A marriage between these two distinct disciplines has the potential of producing robotic machines with some sort of cognitive abilities. We briefly examine these two fields: fuzzy logic and neural networks, and explore the possibilities of their integration in the development of cognitive robotic systems.",
Analysis of out of phase reclosing required for the protection of dispersed storage and generation units,"One of the most important protection requirements of a dispersed storage and generation (DSG) unit is to provide protection against islanding and hence to prevent out of phase reclosing with the utility source which may occur following an undetected islanding condition. The possible effect of an out of phase reclosing to a power system containing a DSG unit has been analyzed in this work. In addition, the response of the DSG protection system to an out of phase reclosing created by a remote circuit breaker has also been evaluated. Computer simulation and test studies show that the rate of change of power protection algorithm, which is initially designed for islanding protection also shows a very good performance for reliable detection of out of phase reclosing conditions.",
Foundation Coalition at Texas A&M University: utilizing TQM and OD to manage curricula change,"The Foundation Coalition is developing and implementing significant changes in how first and second year college engineering, mathematics, science and English courses are taught. These efforts incorporate strategies which have been explored at many institutions, such as: integrating content across course boundaries, delivering instruction in active and cooperative environments, and utilizing technology more effectively as a teaching tool. This paper examines the fundamentals of total quality management (TQM) and organisational development (OD) and compares similarities and differences of each principle. TQM principles are particularly useful in assessing the effectiveness of curriculum innovation at a research university. OD principles are important in facilitating paradigm shifts in the attitudes of faculty, staff and students from a traditional curriculum to an innovative integrated curriculum.",
Performance evaluation of selective cell discard schemes in ATM networks,"In transport-layer protocols such as TCP over ATM networks, a packet is discarded when one or more cells are lost in that packet, and the destination node then requires its source to retransmit the corrupted packet. Therefore, once one of the cells constituting a packet is lost, the subsequent cells of the corrupted packet waste network resources. Thus, discarding those cells will enable us to efficiently utilize the network resources and improve the packet loss probability. We focus on tail dropping (TD) and early packet discard (EPD) as selective cell discard schemes which force the switches to discard some of the arriving cells instead of relaying them. We exactly analyze the packet loss probability in a system applying these schemes. Their advantage and limits are then discussed based on numerical results derived through the analysis.",
Generating and coding of fractal graphs by neural network and mathematical morphology methods,"We present an algorithm for generating a class of self-similar (fractal) graphs using simple probabilistic logic neuron networks and show that the graphs can be represented by a set of compressed encoding. An algorithm for quickly finding the coding, i.e., recognizing the corresponding graphs, is given and the coding are shown to be optimal (i.e., of minimal length). The same graphs can also be generated by a mathematical morphology method. These results may possibly have applications in image compression and pattern recognition.",
System-level synthesis of application specific systems using A* search and generalized force-directed heuristics,"This paper presents a system-level approach to the synthesis of multi-task hard real-time applications. The goal is to select a set of off-the-shelf processors with minimal cost while satisfying timing constraints. Our approach has three design phases: resource allocation, assignment, and scheduling. With the observation that the resource allocation is a search for a set of processors which requires the minimum cost, we adopted A* search based technique. For assignment we use a variation of the force-directed technique. Final task scheduling is based on the Earliest Deadline First (EDF) algorithm. Experimental results show that this approach is highly effective on a variety of examples.",
Concurrency-oriented optimization for low-power asynchronous systems,"We introduce new architectural optimizations for low-power asynchronous systems, such as Tangram-based systems of van Berkel et al. (1994). Our goal is to reduce power consumption by improving system concurrency. We introduce two new sequencer designs, with greater concurrency than existing ones, that provide the opportunity for substantial power savings through voltage scaling. To safely accommodate this added concurrency, new latch designs are presented, for both dual-rail and single-rail implementations.",
"A semantic view of classical proofs: type-theoretic, categorical, and denotational characterizations","Classical logic is one of the best examples of a mathematical theory that is truly useful to computer science. Hardware and software engineers apply the theory routinely. Yet from a foundational standpoint, there are aspects of classical logic that are problematic. Unlike intuitionistic logic, classical logic is often held to be non-constructive, and so, is said to admit no proof semantics. To draw an analogy in the proofs-as-programs paradigm, it is as if we understand well the theory of manipulation between equivalent specifications (which we do), but have comparatively little foundational insight of the process of transforming one program to another that implements the same specification. This extended abstract outlines a semantic theory of classical proofs based on a variant of Parigot's /spl lambda//spl mu/-calculus, but presented here as a type theory. After reviewing the conceptual problems in the area and the potential benefits of such a theory, we sketch the key steps of our approach in terms of the questions that we have sought to answer: Syntax: How should one circumscribe a coherent system of classical proofs? Is there a satisfactory Curry-Howard style representation theory? Categorical characterization: What is the ""boolean algebra"" of classical propositional proofs (as opposed to validity)? What manner of categories characterizes classical proofs the same way that cartesian closed categories capture intuitionistic propositional proofs? Complete denotational models: Are there good intensional game models of classical logic canonical for the circumscribed proofs?.",
Image transmission systems through CDMA channels using spreading sequences of variable-period,"Image transmission systems using spread spectrum techniques are discussed. We transmit DCT coefficients of the image through CDMA channels. Within a limited bandwidth, in order to reduce the error rates of significant bits, we assign spreading sequences of longer period to more significant bits than to less ones. This technique is analogous to the Shannon-Fano encoding. It is found that spreading sequences of variable-period are useful in such image transmissions through CDMA channels.",
Real-time multicast routing with optimal network cost,Multicast routing for real-time applications has two important requirements: minimal network cost and shortest network delay. It is always difficult to meet one requirement without compromising the other. This paper presents a real-time multicast routing algorithm which minimizes overall network cost without letting the delay from source to any destination exceed a time constraint. The multicast routing with optimal overall network cost is an NP-complete problem. Our algorithm is based on the idea of the minimum spanning tree heuristic.,
Design synthesis using adaptive search techniques and multi-criteria decision analysis,Safety and real-time requirements of computer-based life-critical applications dramatically increase the complexity of the issues that need to be addressed during the design process. Typically quantitative analysis of such requirements are undertaken in an ad-hoc manner after the artifact has been produced. We propose a more systematic approach which uses adaptive search and multi-criteria decision analysis techniques to provide analytical support during the design-decision process.,
A fast optimal robust path delay fault testable adder,In this paper we explore the test complexity of the adder function with respect to the robust path delay fault model. A lower bound of /spl omega/(n/sup 2/) for the cardinality of a complete test set for a combinational n-bit adder is proven. This result is valid for any adder design known until now. In addition we present a fast O(/spl radic/n)-time adder that is fully robust path delay fault testable with a test set of size /spl Theta/(n/sup 2/).,
Comments on a widely used capture model for slotted ALOHA,"Previously (see ibid., vol.41, no.9, p.1364, 1993), the capture phenomenon in nonbit-synchronous mobile packet radio networks for binary phase shift keying (BPSK) and differential phase shift keying (DPSK) modulation was investigated. Most of the analysis is correct. Unfortunately, the mathematical expressions for state transition probabilities, adopted from previous work, have errors. Over a decade, these mathematical formulas have been used repeatedly to analyze various capture models with a finite number of users in the traditional slotted ALOHA system. In this note, we present a corrected version of the state transition probabilities.",
A scheduling tool for parallel and distributed systems,"This paper first briefly describes the PARSA (PARallel program Scheduling and Assessment) prototype tool. PARSA is designed to address the efficient partitioning and scheduling of parallel programs on multiprocessor systems. It then presents the scheduling methods that have been implemented in the PARSA prototype and provides a comparative performance evaluation of these schedulers. The PARSA prototype distinguishing features, demonstrated through several examples, include: (1) PARSA simplifies the application development process by eliminating synchronization, scheduling, and machine-dependent concerns; (2) applications developed with PARSA efficiently utilize parallel system resources; (3) PARSA allows development of portable parallel code across a wide range of concurrent systems; (4) applications developed with PARSA can be easily scaled to various sized parallel systems; and (5) PARSA supports fine-tuning of parallel application performance and/or their mappings on the target parallel system.",
A novel approach to optical character recognition based on ring-projection-wavelet-fractal signatures,"In this paper, we present a novel approach to optical character recognition that utilizes ring-projection-wavelet-fractal-signatures. In particular, the proposed approach reduces the dimensionality of a two-dimensional pattern by way of a ring-projection method, and thereafter, performs Daubechies' wavelet transform on the derived one-dimensional pattern to generate a set of wavelet sub-patterns, namely, curves that are non-self intersecting. Further from the resulting non-self intersecting curves, the divider dimensions are readily computed. These divider dimensions constitute a new characteristic vector for the original two-dimensional pattern, defined over the curves' fractal dimensions.",
Automating system-level design: from specification to architecture,In this paper we present a new method to specify digital systems at the system level and to automatically transform a specification into a set of required system components from which in turn a complete system can be constructed using a knowledge-based configuration system. Our approach bases on an object-oriented domain model which captures all knowledge about a certain domain of systems. Throughout the paper we use the domain of RISC processors as an example. We show how the designer specifies systems based on a given domain model and how a specification is automatically mapped into a set of system functions.,
On bufferless routing of variable length messages in leveled networks,"We study the most general communication paradigm on a multiprocessor, wherein each processor has a distinct message (of possibly distinct lengths) for each other processor. We study this paradigm, which we call chatting, on multiprocessors that do not allow messages once dispatched ever to be delayed on their routes. By insisting on oblivious routes for messages, we convert the communication problem to a pure scheduling problem. We introduce the notion of a virtual chatting schedule, and we show how efficient chatting schedules can often be produced from efficient virtual chatting schedules. We present a number of strategies for producing efficient virtual chatting schedules on a variety of network topologies.",
Bridging the gap between formal specification and analysis of communication protocols,"SDL, Estelle and LOTOS are three high-level formal description techniques (FDT) that have been developed and standardized by the international organizations, the CCITT and ISO respectively, for the specification of industry-strength communication protocols. It is crucial to formally verify an FDT protocol specification before its implementation. Most formal analysis and verification techniques for communication protocols, however, have been based on mathematically simple low-level formulations such as finite state machines (FSM) or extension of this model (EFSM). There is a gap between the low-level formal models and the high-level FDTs in terms of expressive power and verifiability, which prohibits the use of existing state/transition based formal analysis methods, such as reachability analysis, for direct use in the formal verification of the protocols specified in the FDTs. This paper proposes a uniform framework as an approach to bridging the gap through systematic reduction of the FDT protocol specifications to a common intermediate model called the structured system of communicating machines (SSCM) which is an EFSM based, powerful, yet rather simply defined formalism. LOTOS is then used to demonstrate the applicability of the approach by developing a complete set of transformation rules and providing the proof for the semantic perseverance of those transformation rules under a well-defined event trace equivalence.",
Multimedia teaching tools for an introductory circuit analysis course,"The paper discusses the use of computer technology as a tool for increasing access to and quality of our electrical engineering program at the California State University, Sacramento. Using readily available software and Internet access, a comprehensive set of lecture materials, student exercises and basic laboratory demonstrations are produced in electronic format in order to promote learning, reinforcement and consistency. While the central focus of this work is on learning, the convenience aspect of this method of delivery has proven to be effective tool for increasing performance as well as enrolment.",
Derivation and presentation of an abstract program space for Ada,"A visualisation concept called ""layering"" promises to deal with large software systems, but has to date been exclusively oriented towards the level of concrete code rather than more design level information. The first step in further progress is to derive with some degree of formalism a space of abstract program constructs that can be viewed by layers. Next, iconography for the elements of the space is derived with a similar formal spirit. Finally, the implementation not only allows simultaneous, linked views of concrete and abstract layers, but can also be extended to accommodate any number of subsequent higher level design views that may be envisaged.",
Tarskian set constraints,"We investigate set constraints over set expressions with Tarskian functional and relational operations. Unlike the Herbrand constructor symbols used in recent set constraint formalisms, the meaning of a Tarskian function symbol is interpreted in an arbitrary first order structure. We show that satisfiability of Tarskian set constraints is decidable in nondeterministic doubly exponential time. We also give complexity results and open problems for various extensions of the language.",
Outline of a roadmap for compiler technology,"Compiler technology has been a major subfield of computer science ever since the first compilers were developed in the late 1950s. Compilers made possible the development of today's efficient and sophisticated software at an affordable cost, thus playing a crucial role in popularizing computers. Though we have learned much over 40 years about compiler development tools, internal compiler organization, parsing techniques, and optimization algorithms, sustained progress in computer usability and performance will require much more research in this area. The paper discusses a few important challenges in compiler technology.",
Small signal GaAs MESFET model parameters extracted from measured S-parameters,"A small signal GaAs FET model is derived based on measured s-parameters. The model parameters have been found using a computer aided optimization program, where the initial value of the circuit elements are determined in part from measured s-parameters at 1 GHz, and in part from DC measurements. By using the optimization program, it is to be noted that the final value of some circuit elements is changed by a negligible amount compared with its initial value. Some other circuit elements which have large changes between their initial and final values, can be readjusted using the second order approximation.",
Combining the detection and correction of speech repairs,"Previous approaches to detecting and correcting speech repairs have for the most part separated these two problems. We present a statistical model of speech repairs that uses information about the possible correction to help decide whether a speech repair actually occurred. By better modeling the interactions between detection and correction, we are able to improve our detection results.",
A handwritten character recognition system using hierarchical displacement extraction algorithm,"A handwritten character recognition system using the hierarchical algorithm to extract displacement between a template pattern and an input pattern is proposed. In the proposed system, the displacement can be computed by Gauss-Seidel iteration derived from Euler-Lagrange equations of the energy functional, which consists of a correspondence error between patterns and a smoothness constraint of the extracted displacement. To extract both global and local deformations included in input patterns, the hierarchical structure is introduced. In computer experiments, the recognition performance is clarified. In addition, the relation between the ability of displacement extraction and the recognition performance when the correspondence error is used as the distance is discussed. Finally, we show that it is possible to improve the recognition performance by using both the correspondence error and the smoothness of the extracted displacement as the distance.",
Layout-driven detection of bridge faults in interconnects,"This paper presents a new approach to fault detection of interconnects; the novelty of the proposed approach is that test generation and scheduling are established using the physical characteristics of the layout of the interconnect under test. This includes critical area extraction and a realistic fault model for a structural methodology. Physical layout information is used to model the adjacencies in an interconnect and possible bridge faults by a novel weighted graph approach. This graph is then analyzed to appropriately schedule the order of test compaction and execution for (early) detection of bridge faults. Generation and compaction of the test vectors are accomplished by calculating node and edge weights of the new adjacency graph as figure of merit. The advantage of the proposed approach is that on average, early detection of faults is possible using a number of tests significantly smaller than with previous approaches. A further advantage is that it represents a realistic alternative to adaptive testing because it avoids costly on-line test generation, while still requiring a small number of vectors.",
HOMAGE: a heterogeneous object-based environment to develop multi-agent systems,"This paper presents an object based programming environment, called HOMAGE, for the development of multiagent systems. This environment offers two different programming levels: object and agent. The object level allows the development of agent models and systems on the basis of two traditional object-oriented programming languages (i.e. C++ and Common Lisp). The agent level allows to specialize the agent models defined at the object level and to develop real multi-agent systems through a multi-agent oriented language, called MAPL++. Moreover, this environment offers a set of communication and distribution libraries to distribute a system on the Internet and to allow the communication between agents through different protocols. The paper includes a brief description of some personal assistants that have been implemented during experimentation with this environment.",
A software package for computer-aided robotics education,"The kinematics of robot manipulators is a corner stone in the study of robotics in general. The computational complexity of the kinematics quite often prevents robotics instructors from using robots of general structure in their illustrative examples and assignments. The software discussed in this article, developed from recent research to support an undergraduate course in robotics, offers computational relief to educators and students in the study of robot manipulators with revolute joints and renders their use as classroom examples possible. Five or six revolute joints robot manipulators of general architecture can be solved easily with the help of this software.",
Exploiting the capabilities of communications co-processors,"Communications coprocessors (CCPs) have become commonplace in modern massively parallel processors (MPPs) and networks of workstations. These coprocessors provide dedicated hardware support for fast communication. In this paper, we study how to exploit the capabilities of CCPs for executing user-level message handlers. We show, in the context of active messages and Split-C, that we can move message handling code to the coprocessor, thus freeing the main processor for computational work. We address the important issues that arise, such as synchronization, and the limited computational power and flexibility of CCPs. We have implemented coprocessor versions of both active messages and Split-C. These implementations, developed on the Meiko CS-2, provide us with an excellent experimental platform to evaluate the benefits of a communications coprocessor architecture.",
Using zpl to develop a parallel chaos router simulator,"This paper reports on our experience in writing a parallel version of a chaos router simulator using the new data driven parallel language ZPL. The simulator is a large program that tests the capabilities of ZPL. The (parallel) ZPL program is compared with the existing serial implementation on two very different architectures: a 16-processor Intel Paragon and a cluster of eight Alpha work stations. On the Paragon, the simulator performs best when simulating medium- to large-sized routers, and on the Alpha cluster, it performs best when simulating large routers. Thus a user can choose the parallel platform best suited to the router size.",
Multiple fault diagnosis in sequential circuits using sensitizing sequence pairs,"The paper presents an approach to multiple fault diagnosis in sequential circuits by using input sequence pairs having sensitizing input pairs. This represents an extension of our previous work dealing with combinational circuits (N. Yanagida et al., 1995). After reviewing our previous method, we introduce an input sequence pair having sensitizing input pairs to diagnose multiple faults in a sequential circuit partitioned into subcircuits. We call such an input sequence pair, the sensitizing sequence pair. Next, we extend the use of the previous method for combinational circuits to sequential circuits. From a relation between a sensitizing path generated by a sensitizing sequence pair and a subcircuit, the proposed method deduces the suspected faults for the subcircuits, one by one, based on the responses observed at primary outputs without probing any internal line. The paper provides the first experimental reports on diagnostic results of the ISCAS circuits by using our diagnostic method for sequential circuits, without probing any internal line, any fault simulation, or fault enumeration.",
Minimizing communication of a recirculating bitonic sorting network,This paper presents the construction of a new recirculating bitonic sorting network which reduces the O(Nlog/sup 2/N) cost complexity of the original bitonic sorting network to O(NlogN) while preserving the well known time complexity of O(log/sup 2/N). Network communication is reduced by one half by leaving the N/2 even-parity keys in the local memory of each comparator.,
Wavelet analysis for diagnostic problems,"In this paper the problem of off-line diagnosis of analog circuits affected by catastrophic multiple faults is considered. Information on the circuit behavior is given by the voltage measurements in a set of available test points. Because of the huge amount of data involved in each voltage acquisition, it is necessary to apply a compression technique to reduce the volume of the data to be given to the diagnostic system. Our diagnostic system consists of a neural network, trained to recognise catastrophic single faults, and used to diagnose multiple faults. The network has as many input nodes as the features extracted by each circuit image and the output nodes are as many as the number of circuit components. In this paper we investigate the suitability of the wavelet analysis to the problem of data compression of measurements for detecting faults in analog circuits. We also present some results obtained with the proposed diagnostic system in diagnosing multiple faults in the test circuit examined and compare these results with those previously obtained.",
State reduction using reversible rules,"We reduce the state explosion problem in automatic verification of finite-state systems by automatically collapsing subgraphs of the state graph into abstract states. The key idea of the method is to identify state generation rules that can be inverted. It can be used for verification of deadlock-freedom, error and invariant checking and stuttering-invariant CTL model checking.",
Binary linear decision tree with genetic algorithm,"A linear decision binary tree structure is proposed in constructing piecewise linear classifiers with the genetic algorithm (GA) being shaped and employed at each nonterminal node to search for a linear decision function optimal in the sense of maximum impurity reduction. The methodology works for both the two-class and multiclass cases. In comparison to several other well known methods, the proposed binary tree-genetic algorithm (BTGA) is demonstrated to produce a much lower cross validation misclassification rate. Finally, a modified BTGA is applied to the important pap smear cell classification. This results in a spectrum for the combination of the highest desirable sensitivity along with the lowest possible false alarm rate. The multiple choices offered by the spectrum for the sensitivity-false alarm rate combination will provide the flexibility needed for the pap smear slide classification.",
Augmented inherited multi-index structure for maintenance of materialized path query views,"Materialized complex object-oriented views are a promising technique for the integration of heterogeneous databases and the development of powerful data warehousing systems. Path query views are virtual classes formed from selection queries that specify a predicate upon the value of an aggregation hierarchy path. The primary difference between previous work regarding OODB indexing and the efficient implementation of materialized path query views addressed in this paper lies in the nature of their usage. For OODB indexing, query usage is the primary purpose of index structures. Because the materialized view data itself can be used to answer queries, the primary use of index structures with regard to materialized path query views is for the incremental maintenance of views in the face of updates. We have developed an augmented inherited multi-index (AIM) strategy that is specifically tailored for the maintenance of materialized path query views. We find that we can improve update performance by augmenting traditional inherited multi-indices with structured representations of the path queries that use them. This enables us to use class hierarchy relationships to prune the number of aggregation paths that must be re-instantiated during update propagation and also to support complex path queries that include cycles.",
Dynamic task scheduling and allocation for 3D torus multicomputer systems,"Multicomputer systems achieve high performance by utilizing a number of computing nodes. Multidimensional meshes have become popular as multicomputer architectures due to their simplicity and efficiency. In this paper we propose an efficient processor allocation scheme for 3D torus based on first-fit approach. The scheme minimizes the allocation time by effectively manipulating the 3D information as 2D information using CST (Coverage Status Table). Comprehensive computer simulation reveals that the allocation time of the proposed scheme is always smaller than the earlier scheme based on best-fit approach, while allowing comparable processor utilization. The difference gets more significant as the input load increases. To investigate the performance of the proposed scheme with different scheduling environment, non-FCFS scheduling policy along with the typical FCFS policy is also studied.",
Visualizing program execution,"The motivation for this work stems from the lack of good visual tools for describing the execution of procedure-level constructs such as procedures, functions, coroutines, iterators, methods and processes. Our proposed solution to this problem is an extension of an old technique called the contour diagram, which was originally used to give semantics for Algol-like languages. Our extensions allow the contour diagram to be used for more modern languages, such as object-oriented languages, logic languages, etc. In this paper, we explain this extended notation, and its use in visualizing the execution of procedural, object-oriented and logic programs. The significance of this extension is that it can serve as a basis for program visualization tools.",
On the expressive power of simply typed and let-polymorphic lambda calculi,"We present a functional framework for descriptive computational complexity, in which the Regular, First-order, Ptime, Pspace, k-Exptime, k-Expspace (k/spl ges/1), and Elementary sets have syntactic characterizations. In this framework, typed lambda terms represent inputs and outputs as well as programs. The lambda calculi describing the above computational complexity classes are simply or let-polymorphically typed with functionalities of fixed order. They consist of: order 0 atomic constants, order 1 equality among these constants, variables, application, and abstraction. Increasing functionality order by one for these languages corresponds to increasing the computational complexity by one alternation. This exact correspondence is established using a semantic evaluation of languages for each fixed order, which is the primary technical contribution of this paper.",
Runtime support for parallelization of data-parallel applications on adaptive and nonuniform computational environments,"In this paper, we discuss the runtime support required for the parallelization of unstructured data-parallel applications on nonuniform and adaptive environments. The approach presented is reasonably general and is applicable to a wide variety of regular as well as irregular applications. We present performance results for the solution of an unstructured mesh on a cluster of heterogeneous workstations.",
A method for extracting outline using the genetic algorithm based on factors for perceptive grouping,"A method for extracting a global outline in a noisy image is proposed. The genetic algorithm is applied to extract a proper outline in an image which includes disconnected outline pixels and noisy pixels. Each individual in a population for the genetic algorithm has a sequence of outline pixel numbers as it genotype. The fitness of each individual is defined using factors represented by a closeness of pixels and a continuity of an outline. Individuals which have higher fitness survive to the next generation. Parents are selected in surviving individuals and their genes information is crosscovered to generate a child. Mutation is applied to improve variation of evolutionary processes. As the experimental results, the global proper outline is extracted using the evolutionary operations and rules in the genetic algorithm.",
Animating real-time reactive systems,"This paper presents an overview of a process model and an object-oriented environment that supports the process model, for the development of complex real-time reactive systems described using the timed reactive object model. The environment includes an animation tool consisting of a graphical user interface, an interpreter and a simulator, as well as an axiom generator and a verification manager. Debugging, simulating the consequences of exercising a computational step, and verifying invariant properties of an evolving design at different stages of the design process are all permitted.",
Effective utilization of disk bandwidth for supporting interactive video-on-demand,This paper proposes a novel data retrieval scheme that achieves effective utilization of disk bandwidth for designing fully interactive video-on-demand systems. The main distinctions of the proposed data retrieval scheme are (1) it requires no extra disk bandwidth to support interactive features such as fast forward search and fast backward search; and (2) it is based on a data placement scheme that can effectively utilize disk bandwidth during normal-speed playback.,
On the optimal design of rational rank selection filters for image restoration,"In this paper, a powerful class of nonlinear filters, called rank conditioned rational rank selection (RCRRS) filters, is proposed to improve the filtering capability of rank conditioned rank selection (RCRS) filters. The approach of rational rank selection is presented in this paper such that they can output appropriate data from a larger set of samples instead of the data from the observation samples.",
Visual language features supporting human-human and human-computer communication,"Fundamental to the design of visual languages are the goals of facilitating communication between people and computers, and between people and other people. The Object Block Programming Environment (OBPE) is a visual design, programming, and simulation tool which emphasizes support for both human-human and human-computer communication. OBPE provides several features to support effective communication: (1) multiple, coordinated views and aspects, (2) customizable graphics, (3) the ""machines with push-buttons"" metaphor and (4) the host-transient pattern. OBPE uses a diagram-based, visual object-oriented language that is intended for quickly designing and programming visual simulations of factories.",
A formal architectural design patterns-based approach to software understanding,"Mastering the complexity of programs and systems, particularly distributed systems, should lead to significant improvements in program and system understanding. We present a formal approach for (distributed) software understanding based on abstraction hierarchies represented by architectural design patterns. This approach allows us to model the distributed software applications through a formal model representing the underlying structure. The representation uses instances of architectural design patterns which can hide details that may be irrelevant in specific situations. The formal models which are produced could be used as a basis for reasoning, code generation, and measuring the ""goodness"" of a design.",
Table transformation tools: why and how,"The paper describes a prototype tool for inverting tabular representations of mathematical functions, providing the motivation for its construction, some discussion of its use, and a sketch of its implementation. Table transformation tools of the type discussed, are intended to be useful in the documentation of complex computer systems.",
Experimental study of extended HIPPI connections over ATM networks,"To extend the widespread use of the high performance parallel interface (HIPPI) as a networking solution for high-speed communications, the 25-meter distance limitation must be solved. Three options available for alleviating the problem of distance limitation are serial-HIPPI, HIPPI/SONET mapping or HIPPI-ATM mapping, and IP routing. Serial-HIPPI, HIPPI/SONET mapping and HIPPI-ATM mapping provide extended HIPPI connectivities at the physical layer, while IP routing forwards data between the HIPPI networks and other networks at the network layer. We study two feasible solutions of this problem, HIPPI tunneling (HIPPI-ATM mapping) and IP routing. We compare these two schemes in terms of network connectivities, protocol overhead, and flow control. The performance evaluation of one implementation of HIPPI tunneling and IP routing is presented. The experimental performance suggests that a high degree of bandwidth utilization was achieved by both HIPPI tunneling and IP routing in this implementation.",
Dynamic scheduling of real-time aperiodic tasks on multiprocessor architectures,"The application of static optimization techniques such as branch-and-bound to real-time task scheduling has been investigated. Few pieces of work, however, have been reported which propose and investigate online optimization techniques for dynamic scheduling of real-time tasks. In such task domains, the difficulty of scheduling is exacerbated by the fact that the cost of scheduling itself contributes directly to the performance of the algorithms and that it cannot be ignored. The paper proposes a class of algorithms that employ novel, on-line optimization techniques to dynamically schedule a set of sporadic real-time tasks. These algorithms explicitly account for the scheduling cost and its effect on the ability to meet deadlines. The paper addresses issues related to real-time task scheduling in the context of a general graph-theoretic framework. Issues related to where and when the task of scheduling is performed are also addressed. We compare two online scheduling strategies, namely an inter-leaving strategy and an overlapping strategy. In the former strategy, scheduling and execution are inter-leaving in time. Each scheduling phase performed by one processor of the system is followed by an execution phase. In the latter strategy, scheduling and execution are overlapping in time. A specified processor, in this strategy, is dedicated to perform scheduling. Results of experiments show that the proposed algorithms perform better than existing approaches, in terms of meeting deadlines and total execution costs, over a large range of workloads.",
Runtime Performance of Parallel Array Assignment: An Empirical Study,"Generating code for the array assignment statement of High Performance Fortran (HPF) in the presence of block-cyclic distributions of data arrays is considered difficult, and several algorithms have been published to solve this problem. We present a comprehensive study of the run-time performance of the code these algorithms generate. We classify these algorithms into several families, identify several issues of interest in the generated code, and present experimental performance data for the various algorithms. We demonstrate that the code generated for block-cyclic distributions runs almost as efficiently as that generated for block or cyclic distributions.",
An Analysis of the “Universal Suffrage” Selection Operator,"The “universal suffrage” selection operator, designed primarily for concept learning inside the system REGAL, is discussed for both overlapping and nonoverlapping populations. Analysis of its behavior is performed by using the “virtual average population” method, a new tool for investigating asymptotic properties of convergence of macroscopic quantities related to the population of a genetic algorithm.",
WWW page design: projects for introduction to design for environmental engineers,"All Humboldt State University Environmental Resources Engineering (ERE) students are required to take ENGR 111: Introduction to Design. In the Spring and Fall semesters of 1995, ENGR 111 students were assigned World Wide Web page design projects. In the Spring of 1995, 45 students designed WWW pages describing the ERE Department. In addition, each of the 11 design teams worked closely with an ERE faculty member and developed a home page for that faculty member. The Fall 1995 ENGR 111 class completed two design projects. 20 students designed WWW pages describing the Schatz Energy Research Center, which focuses on hydrogen fuel cell research. The second half of the class (25 students) designed web pages describing the Arcata Marsh and Wildlife Sanctuary, which is the first constructed wetland in the nation to be used for waste water treatment. This paper describes ENGR 111, its WWW page design projects and various student and departmental benefits of those projects.",
Intelligent forecasting of distribution system loads,"In this paper, for the first time, two algorithmic and one nonalgorithmic models have been developed for forecasting of distribution systems/feeders loads. The two algorithmic models are a 3 dimensional model of daily peak load versus daily peak wind and temperature, and a load versus wind-chill which has the advantage of reducing the analysis from a 3 dimensional model to a 2 dimensional model. The main criterion for the load forecasting study is that the final method used must have an average error of 5% or a curve fit above 0.9. It should be noted that 5% error is acceptable for mid-term load forecasting due to the accuracy of long-term weather forecast. The nonalgorithmic method is the application of neural networks. The reliability of the forecasts using the neural nets, combined with their ability to perform at this level without the aid of an experienced system operator, make neural nets an attractive alternative for load forecasting. Therefore, it has been selected for practical implementation in a power utility.",
MicroPET: a high resolution PET scanner for imaging small animals,"MicroPET is a high resolution positron emission tomography (PET) scanner designed for imaging small laboratory animals. It consists of a ring of 30 position-sensitive scintillation detectors, each with an 8/spl times/8 array of small lutetium oxyonhosilicate (LSO) crystals coupled via optical fibers to a multi-channel photomultiplier tube. The detectors have an intrinsic resolution averaging 1.68 mm, an energy resolution between 15 and 25% and 2.4 ns timing resolution at 511 keV. The detector ring diameter of microPET is 17.2 cm with an imaging field of view of 112 mm transaxially by 18 mm axially. The scanner has no septa and operates exclusively in 3D mode. Reconstructed image resolution 1 cm from the center of the scanner is 2.0 mm and virtually isotropic, yielding a volume resolution of 8 mm/sup 3/ For comparison, the volume resolution of state-of-the-art clinical PET systems is in the range of 50-75 mm/sup 3/. Initial images of phantoms have been acquired and are reported. A computer controlled bed is under construction and will incorporate a small wobble motion to improve spatial sampling. This is projected to further enhance spatial resolution. MicroPET is the first PET scanner to incorporate the new scintillator LSO and to our knowledge is the highest resolution multi-ring PET scanner currently in existence.",
On the search of mobile agents,"In a mobile computing environment that supports mobile agents, a client can send an agent to visit a sequence of servers in the network. To track the location of agents becomes a critical problem in managing a mobile agent service network. This paper studies the agent search problem based on an open architecture proposed by Lien (see Proc. of the First Workshop on Mobile Computing, Hsing-Chu, Taiwan, p.2-9, 1995) and is currently being implemented in the National Chengchi University.",
Support environment for CSCW research-design and implementation of a desktop computer conferencing system,"CSCW (computer supported cooperative work) has been a promising research field in recent years. CSCW is based on interdisciplinary principles, and it promotes the alternation from traditional isolated work to group cooperative work. The paper introduces the basic background of CSCW. A computer conferencing system belongs to real-time CSCW application and is one of the most significant parts in the CSCW research field. The paper analyzes the functional requirement for desktop computer conferencing system. Then the author describes a full schematic software design for a desktop computer conferencing system, and explains the strategies adopted in the design. In this design, the author also advances a client-server model for multimedia synchronization. Finally, the paper presents some prospects on future work.",
"NSF workshop on information technology and undergraduate science, mathematics, engineering, and technology education: challenges and opportunities","Activities at a recent NSF-sponsored workshop are described in this note. Background information and motivation for the workshop are discussed, along with a description of its organization. Initial observations from the workshop are reported, and areas of leadership with respect to the use of information technology in undergraduate science, mathematics, engineering, and technology education are identified.",
Decomposition abstraction in parallel rule languages,"Decomposition abstraction is the process of organizing and specifying decomposition strategies for the exploitation of parallelism available in an application. In this paper we develop and evaluate declarative primitives for rule-based programs that expand opportunities for parallel execution. These primitives make explicit, implicit relations among the data and similarly among the rules. The semantics of the primitives are presented in a general object-based framework such that they may be applied to most rule-based programming languages. We show how the additional information provided by the decomposition primitives can be incorporated into a semantic-based dependency analysis technique. The resulting analysis reveals parallelism at compile time that is very difficult, if not impossible, to discover by traditional syntactic analysis techniques. Simulation results demonstrate scalable and broadly available parallelism.",
T0: A Single-Chip Vector Microprocessor with Reconfigurable Pipelines,"A Single-Chip Fixed-Point Vector Microprocessor Is Described. The Chip Contains A Mips-Ii Risc Core With A 1 Kb Instruction Cache, Dual Eight-Way Parallel Vector Arithmetic Pipelines, A 128-Bit Memory Interface, And An 8-Bit Serial Host Interface. Each Vector Arithmetic Pipeline Contains A Cascade Of Six Functional Units That Can Be Dynamically Reconfigured By Each Instruction. The Resulting Peak Performance Is 4.3 Billion 32-Bit Arithmetic Operations Per Second At A Clock Speed Of 45 MHz.","Microprocessors,
Pipelines,
Arithmetic,
Instruction sets,
Delay,
Reduced instruction set computing,
Registers,
Clocks,
Space technology,
Coprocessors"
Hierarchies of circuit classes that are closed under complement,"We examine three hierarchies of circuit classes and show they are closed under complementation. (1) The class of languages recognized by a family of polynomial size skew circuits with width O(w), are closed under complement. (2) The class of languages recognized by family of polynomial size circuits with width O(w) and polynomial tree-size, are closed under complement. (3) The class of languages recognized by a family of polynomial size, O(log(n)) depth, bounded AND fan-in with OR fan-in f (f/spl ges/log(n)) circuits are closed under complement. These improve upon the results of (i) Immerman (1988) and Szelepcsenyi (1988), who show that /spl Nscr//spl Lscr//spl Oscr//spl Gscr/ is closed under complementation, and (ii) Borodin et al. (1989), who show that /spl Lscr//spl Oscr//spl Gscr//spl Cscr//spl Fscr//spl Lscr/ is closed under complement.",
Defeasible logic graphs for decision support,"Knowledge based systems provide decision support by applying a previously developed representation of knowledge for a particular domain. We describe a method for representing knowledge about any domain using defeasible logic graphs. Because these graphs are based on a defeasible logic of the sort described in (Nute, 1992), they can represent uncertain or incomplete knowledge. We reason about the represented domain by propagating markers in the graph to show which propositions are true, false, or unestablished. We propose to construct an argumentation based software system incorporating defeasible logic graphs. We establish the formal foundations for such a system by showing that the inference mechanism for defeasible logic graphs is sound and complete with respect to defeasible logic.",
Hybrid temporal reasoning for planning and scheduling,"This paper address the problem of representing heterogeneous temporal information in a uniform framework. Metric information relative to intervals is combined with qualitative information in a homogeneous representation based on a temporal constraint network. We illustrate the properties of the new sub-algebra called IDSA (Interval-Distance Sub-Algebra), the algorithms used to propagate temporal information and their complexity.",
"Fuzzy logic, logic programming, and linear logic: towards a new understanding of common sense","Fuzzy logic was originally proposed as a tool for describing human reasoning. Currently, the main area of applications of fuzzy logic is in fuzzy control, where the choice of logic is usually motivated not by logical considerations (i.e. not by what best describes how people actually think), but by purely pragmatic, engineering considerations: what logic would lead to the best control. In this paper, we return to the original meaning of fuzzy logic: a tool for describing human reasoning. We analyze why the existing formalisms are not always adequate, and describe possible modifications of fuzzy logic. Our analysis shows that there are deep similarities between the descriptions of common-sense reasoning in three different fields: fuzzy logic, logic programming and linear logic. Thus, the future formalism for describing human reasoning will probably be a synthesis of these three.",
AlgorithmExplorer: a student centered algorithm animation system,"Algorithm animation can be an effective tool for understanding the behavior of programs. However, most approaches towards algorithm animation have focused on evermore sophisticated graphical depictions of programs, and not on the process of how students can develop and make use of animations in an educational setting. We describe the AlgorithmExplorer, a flexible algorithm animation system targeted towards classroom, laboratory and individual student use. The AlgorithmExplorer provides an open system architecture for integrating student programs, mechanisms for supporting user input, and a three tiered animation command interface that provides rich animation constructs while also supporting a wide range of student programming abilities.",
Redundancy in model representation: a blessing or a curse?,"With the intent of dispelling the prevailing negative connotations associated with redundancy, we argue that redundancy can effect benefits in model specification as opposed to model execution. Sources of redundancy are classified as accidental or intentional, and several examples are given for each. The comparative benefits and detriments are discussed briefly, and for the most interesting source of redundancy that induced by a modeling methodology, we demonstrate that automated elimination of redundancy can actually improve the execution time. Although the set of models investigated is small, these results are encouraging for researchers in modeling methodologies using automated model diagnosis.",
Mission specification for autonomous underwater vehicles,"The user of an autonomous vehicle must have an effective means of expressing a mission for the vehicle. Ideally, the user should only be concerned with expressing what the mission is, with minimal concern about how it will be carried out. One thing that the object-oriented (OO) paradigm is known for is the emphasis being on what is accomplished rather than on how it is accomplished. Various parts of a mission have many things in common, and the object-oriented paradigm also provides inheritance for capturing these commonalities. As such, we believe that an object-oriented approach is ideally suited to mission specification, and have developed a prototype object-oriented mission specification system for an autonomous underwater vehicle. The user specifies several mission phase objects that are combined to form a single mission object. The mission object may then be displayed or executed.",
Integration in Real PCF,"Real PCF is an extension of the programming language PCF with a data type for real numbers. Although a Real PCF definable real number cannot be computed in finitely many steps, it is possible to compute an arbitrarily small rational interval containing the real number in a sufficiently large number of steps. Based on a domain-theoretic approach to integration, we show how to define integration in Real PCF. We propose two approaches to integration in Real PCF. One consists in adding integration as primitive. The other consists in adding a primitive for maximization of functions and then recursively defining integration from maximization. In both cases we have an adequacy theorem for the corresponding extension of Real PCF. Moreover based on previous work on Real PCF definability, we show that Real PCF extended with the maximization operator is universal, which implies that it is also fully abstract.","Differential equations,
Integral equations,
Algebra"
Coregistration of range and optical images using coplanarity and orientation constraints,"A least-squares method simultaneously solves for the model-to-sensor-suite pose and sensor-to-sensor registration. The development is for a sensor-suite containing separate range and optical sensors. To address outliers and, more generally, match finding, a statistical method (median filtering) and a search method (local search) are developed. Sensitivity to Gaussian noise and the choice of initial pose estimates is investigated on synthetic data. Both of the matching methods are demonstrated on real data.",
A framework for inter-ORB request level bridge construction,The paper addresses a problem of building a bridge between different CORBA compliant systems. It presents a framework of the bridge based on the UNO approach whose architecture is easily extendable to more sophisticated in parallelizing level and functionality units. A problem of mapping objects defined in CORBA model is described and a few suggestions to deal with it are presented. As a case study implementation of the bridge for Orbix and DOME is described.,
Performance comparison of desktop multiprocessing and workstation cluster computing,"The paper describes initial findings regarding the performance tradeoffs between cluster computing where the participating processors are independent machines connected by a high speed switch and desktop multiprocessing where the processors reside within a single workstation and share a common memory. While interprocessor communication time has typically been cited as the limiting force on performance in the cluster, bus and memory contention have had similar effects in shared memory systems. The advent of high speed interconnects and improved bus and memory access speeds have enhanced the performance curves of both platforms. We present comparisons of the execution times of three applications with varying levels of data dependencies-numerical integration, matrix multiplication, and Jacobi iteration across three environments: the PVM distributed memory model, the PVM shared memory model, and the Solaris threads package.",
Midpoint line algorithm for high-speed high-accuracy-rotation of images,"Image rotation is a basic operation in most digital image processing systems as well as a necessary component in some visual pattern recognition systems. The speed and/or accuracy of image rotation are significant in most cases. In this paper, a new one-pass method, called the midpoint line algorithm, for realizing high-speed high-accuracy rotation of images is proposed. The new method, based on a novel view of images and their rotation, employs the principle of the midpoint technique for line generation. It mainly uses integer addition, increment by 1, and logic justification, and it significantly reduces the floating-point computations, so that it can be performed very fast without loss of high accuracy. Analysis of computation and accuracy of the method is given in comparison with those of the multipass methods.",
Inertial measurement system for the position control of a flexible robot arm,"A method to measure the position of the end of a robot arm using an inertial measurement system was designed and tested. Traditionally large robots determine their positions in the work space by means of sensors affixed to the joints of the arm segments. This method cannot be used in the case of flexible arms, due to the bending of the arm, which in turn influences the position of the arm's endpoint. It was decided to make use of three angular rate sensors to determine the position of the arm's end point, since these sensors measure the displacement of the arm irrespective of the movement of the joints in the arm. It is thus possible to determine the position of the arm's endpoint, irrespective of bending or movement of the joints. The experimental system described consists of three piezoelectric vibrating gyroscopes affixed on a small two-segment robot arm.",
Under pressure: recommendations for managing a practical course in software engineering,"Carrying out project courses in a university adds a level of difficulty to software engineering, because there usually exist very tight deadlines and the knowledge of the developers is naturally low. Based on the experience with two practical courses conducted at Technische Universitat Munchen, the paper gives some recommendations on how to deal with these problems.",
The geometry of coin-weighing problems,"Given a set of m coins out of a collection of coins of k unknown distinct weights, the authors wish to decide if all the m given coins have the same weight or not using the minimum possible number of weighings in a regular balance beam. Let m(n,k) denote the maximum possible number of coins for which the above problem can be solved in n weighings. They show that m(n,2)=n/sup ( 1/2 +o(1))n/, whereas for all 3/spl les/k/spl les/n+1, m(n,k) is much smaller than m(n,2) and satisfies m(n,k)=/spl Theta/(n log n/log k). The proofs have an interesting geometric flavour; and combine linear algebra techniques with geometric probabilistic and combinatorial arguments.",
Pictorial queries by image similarity,"A method for specifying pictorial queries to an image database is introduced. A pictorial query specification consists of a query image, and a similarity level that specifies the required extent of similarity between the query image and database images that are to be retrieved. The query image is constructed by positioning objects so that the desired locational and spatial constraints hold. Two image similarity factors are considered: (1) contextual similarity: how well does the content of one image match that of another; (2) spatial similarity: the relative locations of the matching symbols in the two images. Algorithms for retrieving all database images that conform to a given pictorial query specification are presented and compared.",
N-version programming: a unified modeling approach,"This paper presents an unified approach aimed at modeling the joint behavior of the N version system and its operational environment. Our objective is to develop reliability model that considers both functional and performance requirements which is particularly important for real-time applications. The model is constructed in two steps. First, the Markov model of N version failure and execution behavior is developed. Next, we develop the user-oriented model of the operational environment. In accounting for dependence we use the idea that the influence of the operational environment on versions failures and execution times induces correlation. The model addresses a number of basic issues and yet yields closed-form solutions that provide considerable insight into how reliability is affected by both versions characteristics and the operational environment.",
An analytic model for estimating the average radar cross section (RCS) for identifying the flickering target,"We develop an analytical model for estimating the average RCS which gives good results in identifying flickering targets. The proposed analytical model is applied, specially in the high frequency operating range, by resolving the shape of the complex flicker targets into a number of simple geometric segments, of defined cross-section. Thus the scattered-field contribution from each of these geometric components is in turn attributed to the combination of various analytic and geometric components, and only the major contributors are retained. For simple bodies, these contributions were combined by taking into account the relative phases between the analytic components. However, for a complex body, there may be a large number of components of the same relative magnitude which contribute to the cross section. In this event, small errors in the estimate of the relative phases between these contributors can lead to large errors in the estimated cross section patterns. These large errors are avoided by developing the proposed model based on calculating the average cross section /spl sigma//sub a/spl nu//. The simulation represents the variation of the average cross section calculated by the proposed analytical technique for the target in question, as a function of the aspect angle and frequency (target frequency signature), illustrating the effect of this analytical technique in estimating an average cross-section approximately equal to the average RCS of the experimental measurement.",
"Impact of World Wide Web, Java, and virtual environments on education in computational science and engineering","The World Wide Web (WWW) on the Internet has been recognized as an effective environment to create new distributed applications that have the potential to bring instruction beyond the bounds of the classroom. The availability of browsers (e.g. Mosaic, Netscape, Hot Java) has enormously simplified the access to the WWW, and there have been numerous initiatives to take advantage of this new technology for teaching. This work will illustrate recent developments of tools for engineering education and technology transfer which take advantage of WWW browsers, Java applets, and virtual reality. We have developed modules based on WWW browsers incorporating educational and research software, including advanced visualization, which find use for multimedia classroom presentations accessible by Internet users, and which can also improve interaction among academic groups and industry. Therefore, the material is suitable for asynchronous distance learning and technology transfer. Examples of interactive WWW applications include device simulation, semiconductor band structure calculation, numerical techniques, and electromagnetics.",
Induction motors with low copper windings and improved performances,"In the paper the fractional windings of the induction motors are analyzed in order to reduce the end-winding length and to clean the air-gap mmf from the point of view of spatial harmonics. The used copper can be reduced by proper arrangement of the windings. The example presented in the paper for a 1.5 slot per pole and phase winding shows the possibility of obtaining improved performances (efficiency, power factor, starting torque, noise level) and/or the lower copper used.",
Using active clients to minimize replication in primary-backup protocols,"We consider a primary-backup approach to provide fault-tolerance service under a model in which the clients play an active role when their service requests are not fulfilled. Each client maintains an ordered list of servers and sends its service requests to the first server in its list. If the server does not respond within it specified timeout period, the client retransmits the request to the next server in its list. Under this model, we construct protocols that tolerate crash failures, send-omission failures, and receive-omission failures. For each type of failure, our protocol is optimal with respect 20 the degree of replication. More precisely, our protocols tolerate up to f server failures using only f+1 servers. In addition, these protocols tolerate an arbitrary number of client failures. Further, the protocols ensure that the service provided by the system is functionally equivalent to that provided by a single failure-free server.",
Wavelet analysis of motor unit action potentials,"In this study the usefulness of the wavelet transforms (WT) Daubechies with 4 and 20 coefficients, Chui, and Battle-Lemarie in analyzing MUAPs recorded from normal subjects and subjects suffering with motor neuron disease and myopathy was investigated. The results of this study are summarised as follows: (i) The orthogonal WT decomposes the MUAP signal into a set of orthogonal basis functions where each coefficient represents an entirely different signal feature describing the energy content in the given time-frequency window. Most of the energy of the MUAP signal is distributed among a small number of well localized (in time) WT coefficients in the region of the main spike. (ii) The WT uses long duration windows for low frequencies, and short duration windows for high frequencies. For MUAP signals, this means that the authors to look to the low frequency coefficients for capturing the average behaviour of the MUAP signal over long durations, and the authors look to the low frequency coefficients for locating MUAP spike changes; (iii) In the case of the Daubechies 4 wavelet an extremely high time-resolution of only four signal samples is provided tracking effectively the transient components of the MUAP signal. (iv) Finally, it is shown that the diagnostic performance of neural network models trained with the Battle-Lemarie wavelet feature set is similar to the empirically determined time domain feature set.",
Alignment of Coexisting Cortical Maps in a Motor Control Model,"How do multiple feature maps that coexist in the same region of cerebral cortex align with each other? We hypothesize that such alignment is governed by temporal correlations: features in one map that are temporally correlated with those in another come to occupy the same spatial locations in cortex over time. To examine the feasibility of this hypothesis and to establish some of its detailed implications, we studied a multilayered, closed-loop computational model of primary sensorimotor cortex. A simulated arm moving in three dimensions formed the external environment for the model cortical regions. Coexisting proprioceptive and motor maps formed and generally aligned in a fashion consistent with the temporal correlation hypothesis. For example, in simulated proprioceptive sensory cortex the map of elements responding strongly to stretch of a particular muscle matched the map of tension sensitivity in antagonist muscles. In simulated primary motor cortex the map of elements responding strongly to increased tension in specific muscles matched the map of output elements for the same muscles. These computational results suggest specific experimental measurements that can support or refute the temporal correlation hypothesis for map alignments.",
Teaching systems integration in an advanced microprocessor applications course,"The New Jersey Institute of Technology's Electrical Engineering Technology program has developed an advanced microprocessor course that teaches systems integration, and develops the skills required to minimize development design time and costs. The course uses the generic technical concepts of microprocessor technology as a vehicle to learn the basic concepts of the system integration process. It also emphasizes the use of available and reusable technologies to develop systems and products as well as the importance of documentation and test planning. This paper describes the content of the course with regard to the basic steps of the systems integration process.",
A fast approach to detect and correct skew documents,"In this paper, a fast approach is proposed to detect and correct the skew documents. In this proposed approach, a coarse skew angle /spl theta//sub e/ is first estimated to decrease the heavily computational burden. Next, the exact skew angle /spl theta/ located in the previously estimated range [/spl theta//sub e/-3, /spl theta//sub e/+3] is then determined by making use of the Hough transform method. In addition to the skew angle detection. The skew image is quickly corrected by decomposing the traditional geometric rotation into a translation and a local rotation process. A mapping table which stores the local rotation information is employed to avoid the large floating-point operations and speed up the rotation process on general-purpose computers. Experimental results are conducted to verify the feasibility and efficiency of our proposed method.",
IEEE Expert,,
User-centered system decomposition: Z-based requirements clustering,"Requirements clustering (RC) provides a differed approach to system decomposition, by enabling a system to be partitioned into user-recognizable components, where each component can be used, almost independently, to satisfy part of the user's needs. Requirements clustering is essential for a software development approach called incremental delivery (ID). A successful clustering of system requirements produces a set of useful, usable, and semi-independent clusters that can be developed and delivered to the customers in increments. The paper presents a requirements clustering process based on ER modeling, scenarios, and the formal specification notation Z.",
On a planar representation of 3D figures commutative with respect to set and morphological operations,"In order to reduce the amount of time required for computing set and morphological operations on 3D figures, we pose and solve the problem of finding such a planar representation of 3D figures, which is commutative with respect to these operations. The proposed representation is a generalization of the well known halftone technique, called ordered dithering, and thereby is a new kind of halftoning which can find applications elsewhere.",
Directory Services: Basis for Mobile Computing with DECT,,
Simulating message-driven programs,"Simulation studies are quite useful for performance prediction on new architectures and for systematic analysis of performance perturbations caused by variations in the machine parameters, such as communication latencies. Trace-driven simulation is necessary to avoid large computational costs over multiple simulation runs. However, trace-driven simulation of nondeterministic programs has turned out to be almost impossible. Simulation of message-driven programs is particularly challenging in this context because they are inherently nondeterministic. Yet message-driven execution is a very effective technique for enhancing performance, particularly in the presence of large or unpredictable communication latencies. We present a methodology for simulating message-driven programs. The information that is necessary to carry out such simulations is identified, and a method for extracting such information from program executions is described.",
Real-time active vision with fault tolerance,"The active vision paradigm couples perception and action at several different levels. The effective use of active vision in complex robotic tasks requires that these levels operate both independently and cooperatively, reliably and in real-time. In this paper, we present a system for real-time active vision with fault tolerance. The system provides a vocabulary of active vision routines along with the means for composing the routines into continuously running perception-action processes. A novel architecture which enables the integration of the perceptive capabilities of real-time active vision with the active capabilities of other robotic devices is presented. We then enhance the architecture with a unified approach to fault tolerance and present results from experiments and simulations.",
New lower bounds for halfspace emptiness,"The author derives a lower bound of /spl Omega/(n/sup 4/3/) for the halfspace emptiness problem: given a set of n points and n hyperplanes in R/sup 5/, is every point above every hyperplane? This matches the best known upper bound to within polylogarithmic factors, and improves the previous best lower bound of /spl Omega/(nlogn). The lower bound applies to partitioning algorithms in which every query region is a polyhedron with a constant number of facets.",
Hardware supports for efficient barrier synchronization on 2-D mesh networks,"In this paper, we consider a hardware scheme for supporting barrier synchronization on scalable systems with a 2D mesh network. Our design takes into account of the program execution path in such systems-from programming interfaces down to routers. The hardware router design will be based on the MPI-1 standard. A distributed algorithm is proposed to construct a collective synchronization tree (CS tree) from the nodes participating in the barrier based upon the CS tree, the status registers in the routers are set up and synchronization messages are transmitted along the paths set by the status registers. Performance evaluations show that our proposed method has better performance for barrier synchronization and is less sensitive to variations in group size and startup delay than previous approaches. However our scheme has the extra overhead of building the CS tree. Thus it is more suitable for parallel iterative computations, in which the same barrier is invoked repetitively.",
Deductive synthesis of numerical simulation programs from networks of algebraic and ordinary differential equations,"Computational science and engineering design can benefit from software tools that facilitate construction of programs for simulating physical systems. Our research adapts the methodology of deductive program synthesis to the problem of synthesizing numerical simulation codes. We have focused on simulators that can be represented as second-order functional programs composed of numerical integration and root-extraction routines. Synthesis of second-order programs appears to present a problem for deductive systems that operate in first order logic. We present a recursive synthesis algorithm that overcomes this difficulty for a class of program synthesis problems. Our system has successfully constructed numerical simulators for computational design of jet-engine nozzles and sailing yachts, among others.",
On programming with view synchrony,"View synchrony has been proposed as a programming paradigm for developing reliable distributed applications. The paradigm is particularly attractive when the underlying computing system is asynchronous and prone to complex failure scenarios including partitions. View synchrony encourages a programming style where groups of processes cooperate closely in order to maintain some form of shared state among them. In this paper we examine the technical problems that arise in shared state management when programming applications using view synchrony. We identify three classes of problems corresponding to state transfer upon group joins, state recreation after total failures and state merging after partition unions. We argue that shared state problems are inherent to any implementation, and without explicit support, attempts to solve them may easily obscure much of the simplicity and elegance of view synchrony. Finally, we propose an extension to the traditional view synchrony model based on the notion of subviews that addresses the problems raised by shared state management.",
Using distributed hypermedia educational systems for distance education,"This paper describes a Distributed Hypermedia Educational System (DHES) that applies the most current distributed hypermedia technologies for instructional delivery. The system provides more realistic learning environment than that are currently available in all the electronic educational systems. It offers the advantages of simulated face-to-face interaction as well as self-learning. The system incorporated most of the essential Internet services such as customized email, file transfer, remote log in to another host and World Wide Web navigator. The system has been field tested and the experience has been most encouraging.",
A multimedia computer assisted learning in pneumatics,Both lecturers and students encountered teaching and learning problem with the abstract concepts in pneumatics encompassing a dynamic nature. The paper describes the development and experience of a multimedia computer assisted learning package for teachers training in pneumatics. The main feature of the package lies in the explanation of the dynamic nature of abstract pneumatic concepts by making use of the animation on the flowing of air particles inside pneumatic components within a circuit from which the circuit operation can be explained clearly. The approach has been found to be very useful to train teachers who are undergoing initial or refresher training and to those who have difficulties in learning abstract concepts in the operation of pneumatic circuits.,
A network flow framework for online dynamic channel allocation,"We present a framework based on network flows for dynamic channel allocation. The framework allows us to gracefully extend previously proposed heuristics, while avoiding the problems associated with the optimal packing schemes. The framework is shown to yield parametrizable algorithms that tradeoff the benefit of channel reassignments with the costs. Channel reassignments are allowed to be performed not just at new call arrivals, but also at various other trigger points like call terminations and channel quality deterioration.",
Tree-lookup for partial sums or: how can I find this stuff quickly?,"Suppose that you need to maintain a large list of data that is to be searched and modified frequently in the course of a computation. You can almost always arrange for both operations to be done efficiently by storing the data in a structure other than a single array. Although there are several elegant ways to do this, we present just one: a simple tree that doesn't require a lot of thinking to implement. It can speed up execution tremendously because instead of searching through a list item after item for each update, possibly examining n items, you need only examine O(log n) items per operation. (More advanced methods can reduce this to log*n at the cost of more complex programming. Here, log*n is the number of iterations of log/sub 2/ that are required to get below 1. For numbers having less than 65K bits, log* is less than 5.).",
SHARP: efficient loop scheduling with data hazard reduction on multiple pipeline DSP systems,"Computation intensive DSP applications usually require a parallel/pipelined processor in order to achieve specific timing requirements. Data hazards are a major obstacle against the high performance of pipelined systems. This paper presents a novel efficient loop scheduling algorithm that reduces data hazards for those DSP applications. Such an algorithm has been embedded in a tool, called SHARP, which schedules a pipelined data flow graph to multiple pipelined units, while hiding the underlying data hazards and minimizing the execution time. This paper reports significant improvement for some well-known benchmarks, showing the efficiency of the scheduling algorithm and the flexibility of the simulation tool.",
Predictor strategies for continuation methods applied to nonlinear circuit analysis,"In designing microwave circuits the use of computer aided design (CAD) techniques is invaluable. Continuation methods using different types of predictors are implemented and their performance is evaluated in two nonlinear circuits (a lumped element one and a microstrip one) analysed by the harmonic balance technique. The comparison is carried out taking into account the error in the predictors and the number of iteration the solver needs to reach the next solution point. In doing so, the possibility of making the predictor adaptive is discussed.",
Control system of PLS 2-GeV electron linac,"A graphic-based realtime control system is developed and used for commissioning and operation of the PLS 2-GeV electron linac. The system has three layers of hierarchy; operator interface computer, supervisory control computer (SCC) for data processing, and device interface computer (DIG) for distributed data acquisition. The operator interface is based on the UNIX system with a graphic-based development system named RTworks. The SCC consists of three subsystems; modulators, magnet power supplies, and beam diagnostics. DICs attached to individual devices are placed in the 200-m linac building. There are altogether 23 VME CPUs and various I/Os in both SCC and DIC levels. The realtime operating system is OS-9. The major role of DICs is to control and to monitor individual devices upon operator's requests. The SCC updates variable database from DICs and conveys commands from the operator to DICs. SCCs and DICs are connected through a local area network to form a client/server system. Low-cost workstations are used for the operator interface. In order to have quick manipulation and easy understanding, we use intensive graphics for the control windows",
Alternate path reasoning in intelligent instrument fault diagnosis for gas chromatography,"Intelligent instrument fault diagnosis is addressed using expert networks, a hybrid technique which blends traditional rule-based expert systems with neural network style training. One of the most difficult aspects of instrument fault diagnosis is developing an appropriate rule base for the expert network. Beginning with an initial set of rules given by experts, a more accurate representation of the reasoning process can be found using example data. A methodology for determining alternate paths of reasoning and incorporating them into the expert network is presented. Our technique presupposes interaction and cooperation with the expert, and is intended to be used with the assistance of the expert to incorporate knowledge discovered from the data into the intelligent diagnosis tool. Tests of this methodology are conducted within the problem domain of fault diagnosis for gas chromatography. Performance statistics indicate the efficacy of automating the introduction of alternate path reasoning into the diagnostic reasoning system.",
Strong interaction fairness via randomization,"We present Multi, a symmetric, fully distributed, randomized algorithm that, with probability 1, schedules multiparty interactions in a strongly fair manner. To our knowledge, Multi is the first algorithm for strong interaction fairness to appear in the literature. Moreover, the expected time taken by Multi to establish an interaction is a constant not depending on the total number of processes in the system. In this sense, Multi guarantees real-time response. Multi makes no assumptions (other than boundedness) about the time it takes processes to communicate. It thus offers an appealing tonic to the impossibility results of Tsay&Bagrodia and Joung concerning strong interaction fairness in an environment, shared-memory or message-passing, in which processes are deterministic and the communication time is nonnegligible. Because strong interaction fairness is as strong a fairness condition that one might actually want to impose in practice, our results indicate that randomization may also prove fruitful for other notions of fairness lacking deterministic realizations and requiring real-time response.",
A popularity-based data allocation scheme for a VOD server,"In the real world, the popularity of each video is different. We propose a new popularity-based data allocation scheme to allocate data units within a cluster such that the corresponding data units of these popular videos are stored in those cylinders at one end of each cluster. Due to a higher spatial locality within these hot cylinders, some data units requested by the users are stored in the same cylinder such that one seek operation, one rotation, and one transfer operation are required to retrieve these data units. Therefore, the time required to retrieve data for these requests can be reduced, thus reducing the system response time as well. Based on our results, the system response time could be reduced by half or even more.",
"Capacity improvement in cellular systems through distributed, C/I-based power control","A distributed, interference-based dynamic channel assignment scheme is assessed and compared to maximum packing. The system takes action based on knowledge of C/I of active calls. Distributed power control is also used in the call admission and handoff process. We study the impact of the distributed C/I based power control on the system performance. We focus on the blocking probability, power level and number of call re-arrangements (intra-cell handoffs). In addition, several variants of the basic assignment algorithm are introduced and their performances are compared. We find that the distributed power control scheme has to be carefully implemented so that the system does not become unstable. In addition, when the uplink/downlink C/I is monitored, the transmitted power is lowered, and the capacity of the system may be improved. The improved performance is due to less variation of average call quality. The improvement in capacity is gained at the expense of re-arrangements, which increase with stricter control.",
/spl lambda//spl tau/-space representation of images and generalized edge detector,"An image clad surface representation based on regularization theory is introduced in this paper. This representation is based on a hybrid model derived from the physical membrane and plate models. The representation, called the /spl lambda//spl tau/-representation, has two dimensions; one dimension represents smoothness or scale while the other represents the continuity of the image or surface. It contains images/surfaces sampled both in scale space and the weighted Sobolev space of continuous functions. Thus, this new representation can be viewed as an extension of the well-known scale space representation. We have experimentally shown that the proposed hybrid model results in improved results compared to the two extreme constituent model, i.e., the membrane and the plate models. Based on this hybrid model, a generalized edge detector (GED) which encompasses most of the well-known edge detectors under a common framework is developed. The existing edge detectors can be obtained from the generalized edge detector by simply specifying the valves of two parameters, one of which controls the shape of the filter (/spl tau/) and the other controls the scale of the filter (/spl lambda/). By sweeping the valves of these two parameters continuously, one can generate an edge representation in the /spl lambda//spl tau/ space, which is very useful for developing a goal-directed edge detection scheme for a specific task. The proposed representation and the edge detector have been evaluated qualitatively and quantitatively on several different types of image data such as intensity, range and stereo images.",
Self adaptation of agent's behavior using GA with n-BDD,"A gene expression n-BDD (binary decision diagram) is proposed in the paper. The gene expression is suitable for agent models to decide agent's behavior using information from environment. We can apply genetic algorithm to evolve behavior of agents. A simulator of a quasi-ecosystem developed using the gene expression is described. In the quasi-ecosystem, a herbivore, a carnivore and plants make a food chain. A herbivore using an n-BDD more rapidly acquire its ability to survive from a carnivore than using finite state automaton.",
New coding techniques for improved bandwidth utilization,"The introduction of parallel models that account for communication between processors has shown that interprocessor bandwidth is often the limiting factor in parallel computing. In this paper, we introduce a new coding technique for transmitting the XOR of carefully selected patterns of bits to be communicated which greatly reduces bandwidth requirements in some settings. This technique has broader applications. For example, we demonstrate that the coding technique has a surprising application to a simple I/O (Input/Output) complexity problem related to finding the transpose of a matrix. Our main results are developed in the PRAM(M) model, a limited bandwidth PRAM model where P processors communicate through a small globally shared memory of M bits. We provide new algorithms for the problems of sorting and permutation routing. For the concurrent read PRAM(M), as P grows with M held constant, our sorting algorithm outperforms any previous algorithm by /spl Omega/(log/sup c/ P) for any constant c. The combination of a known lower bound for sorting in the exclusive read PRAM(M) model and this algorithm implies that the concurrent read PRAM(M) is strictly more powerful than the exclusive read PRAM(M).",
The scientist's infosphere,"The importance of national defense in the US required the development of tailor made computing and communication systems. The military coined the term infosphere to refer to the collection of remote instruments, appliances, computing tools, and people made accessible by these systems from a person's working environment, such as the cockpit of a plane or the bridge of a ship. Because military personnel are often mobile, they must use remote instruments and computing programs and they must collaborate with people at distant sites. It is concluded that within a decade, most scientists' infospheres will reside on portable computing devices. The infosphere will allow the scientist to access and control home appliances, office devices, laboratory instruments, and computing tools and to communicate with colleagues everywhere. Communication bandwidths may vary as the scientist moves from place to place, but the infosphere will increasingly free the scientist from the constraints of physical location. This freedom will change the ways in which scientific research is carried out, science is taught, and scientific results are disseminated.",
Results given by a new evaluation system for placement and routing heuristics,"This paper presents a software system for generation of VLSI layouts. Its main advantages are minimal hardware requirements and its independence of programming languages, hardware platforms and fabrication processes. It is thus designed for use in algorithm development and university backgrounds.",
An integrated model of acoustics and language using semantic classification trees,"We propose multilevel semantic classification trees to combine different information sources for predicting speech events (e.g. word chains, phrases, etc.). Traditionally in speech recognition systems these information sources (acoustic evidence, language model) are calculated independently and combined via Bayes rule. The proposed approach allows one to combine sources of different types it is no longer necessary for each source to yield a probability. Moreover the tree can look at several information sources simultaneously. The approach is demonstrated for the prediction of prosodically marked phrase boundaries, combining information about the spoken word chain, word category information, prosodic parameters, and the result of a neural network predicting the boundary on the basis of acoustic-prosodic features. The recognition rates of up to 90% for the two class problem boundary vs. no boundary are already comparable to results achieved with the above mentioned Bayes rule approach that combines the acoustic classifier with a 5-gram categorical language model. This is remarkable, since so far only a small set of questions combining information from different sources have been implemented.",
Digital comb filter implementation for the II/spl Delta//spl Sigma/ A/D converter,"This paper describes a topology for the design and implementation of decimation comb filters used in the parallel delta-sigma (II/spl Delta//spl Sigma/) A/D converter. The comb filter implementation presented here avoids any multibit digital multiplication and, by translating some of the processing through the downsampler to lower clock speeds, an area efficient and low power implementation is realized. The circuit implementation uses true single phase clocking, antiphase clocking and pipelining to obtain the necessary speed. A prototype of a 22-bit, 418-tap comb filter was designed, fabricated and tested in a 2 /spl mu/m p-well CMOS process for use in an eight-channel, 15-bit II/spl Delta//spl Sigma/ A/D converter operating at a sampling rate of 20 MHz. The testing results showed that the filter worked as expected.",
Large signal transient response of long-wavelength injection lasers,"The large signal transient response of 1.3 /spl mu/m InGaAsP Fabry-Perot type lasers is investigated by assuming that the injection current changes by steps. Single-mode and multimode operation are considered. For a one step current change the amplitude and frequency of the relaxation oscillations increase with the amplitude of the main pulse. Multimode operation shows the relative importance of the photon side modes during the transient. For a two steps current change a very important chirp reduction may be observed when the prepulse amplitude is smaller than the main pulse amplitude. By choosing the right prepulse amplitude and duration an absolute minimum for the chirp amplitude can be obtained. When the prepulse amplitude is larger than the main pulse amplitude there is no detectable change in the amplitude and frequency of the relaxation oscillations. By increasing the prepulse duration from zero up to its turn-on delay time, it is possible to vary the main pulse turn-on delay time between its maximum value and the prepulse turn-on delay time.",
Process improvement for software engineering training,"The training program of a software organization is the combination of activities described in the project training plans and the organizational training plan. The training process is the sequence of activities that are performed whenever a training cycle is enacted typically on an annual basis. This paper examines the components of a prototypical training process, emphasizing the early phases where software engineering organizations often exhibit weaknesses: the training needs analysis phase and the training planning phase. The paper concludes with questions for the readers to consider with regard to their own software engineering training programs.",
UNIX kernel based reference implementation in PCTS,"The protocol conformance test system (PCTS) is the principal facility used to verify the conformance between the communication protocol implementation and its international standard. We first discuss the importance and function of reference implementation (RI) in the PCTS, and give a general abstract model of the RI with the method of object-oriented design (OOD). After introducing the UNIX STREAMS mechanism, we present the basic abstract model for developing the RI in a UNIX kernel, and introduce the IP/X25 RI, which is implemented with this model, as a example.",
Interactive multimedia CD-ROMs for education,"With the growing availability of software for multimedia development and CD-ROM mastering as well as the ever-increasing capabilities of personal computers, there has been significant increase in interest in the development and production of multimedia modules for education. The paper highlights some of the multimedia CD-ROMs developed by the CAEME Center for Multimedia Education and Technology. The trade-offs involved in development and some procedures for enhancing students interactivity are described. More recent research and development efforts by CAEME to use virtual reality technology in education are discussed.",
Integrating design and development in the production of multimedia documents,"Production of multimedia artifacts demands guidance for both engineers developing software and designers generating content. This paper distinguishes conceptually between media, multiple media and multimedia and defines the multimedia document as both communicative object and system, thus identifying integration as a fundamental of production. It proposes media transformations as a means of describing the design activities involved in the generation of media elements containing content. It defines navigable discourse structure as the discourse structure actually realised by the combination of media elements and operations, and it proposes a discourse driven process model. From a practical point of view, the paper outlines two case studies of the production of multimedia demonstrations of software engineering tools. It provides transformation representation rules as a succinct means for recording media transformations. Finally it describes the main components of a practical method, based on the process model, for the production of multimedia documents.",
Reducing P to a sparse set using a constant number of queries collapses P to L,"We prove that there is no sparse hard set for P under logspace computable bounded truth-table reductions unless P=L. In case of reductions computable in NC/sup 1/, the collapse goes down to P=NC/sup 1/. We generalize this result by parameterizing the sparseness condition, the space bound and the number of queries of the reduction, apply the proof technique to NL and L, and extend all these theorems to two-sided error randomized reductions in the multiple access model, for which we also obtain new results for NP.",
SNMP and OSI management information modeling and translation: a case study,"The SNMP and OSI network management standards define two different languages and information models for describing management information. The advocacy of these different approaches is often driven by politics, religion and history as much as by technical considerations. We hope to remain aloof from these partisan arguments and to assess the strengths and weaknesses of the models from the point of view of human managers and of implementors. Comparisons in the literature often focus on performance aspects; load on networks and managed systems, memory requirements, etc. The main focus of our paper is on the information models themselves. Our approach is to explore how the same management information would be modeled within the OSI and SNMP standards. We then assess the results from the point of view of the ""naturalness"" of the modeling, its extensibility and its likely impact on the efficiency of management interactions. In some networking environments-especially in telecommunications-the OSI and SNMP standards must co-exist. The Network Management Forum (NMF) has defined a series of guidelines concerning coexistence and these include a methodology for translating between information models. We also attempt to assess the effectiveness of the NMF translation rules.",
Uncoated SAW delay lines as thermal gas detector,"A SAW delay line, without any absorbent film between the interdigital transducers, has been developed as a thermal gas detector. The change of the thermal conductivity of the ambient atmosphere produces a change of substrate temperature thereby causing a SAW phase variation (SAW response) at the output of the device. Different effects contribute to the SAW response such as the thermal conductivity /spl Delta//spl lambda/, flow rate /spl Delta/u, dynamic viscosity /spl Delta//spl mu/, and the density /spl Delta//spl rho/ of the test ambient and they are numerically analysed for each test gas. Uncoated SAW delay lines operating at 41-263 MHz are implemented on SiO/sub 2/, LiNbO/sub 3/, Bi/sub 12/GeO/sub 20/, and Bi/sub 12/SiO/sub 20/ substrates. The gases under test (H/sub 2/, He, Ar, CH/sub 4/, NH/sub 3/, N/sub 2/, O/sub 2/, dry air) are used within concentrations of 0.1-100% and flow rates of 50-2000 mL/min at 20-150/spl deg/C and atmospheric pressure. The SAW response /spl Delta//spl phi///spl phi/, is measured as function of /spl Delta//spl lambda/, /spl Delta/u, operating temperature, and gas concentration. The SAW prototype has a good sensitivity: /spl Delta//spl phi///spl phi//spl equiv/15 ppm to 0.7% CH/sub 4/ in N/sub 2/ for a Bi/sub 12/SiO/sub 20/ delay line heated at 120/spl deg/C; /spl Delta//spl phi///spl phi//spl equiv/15 ppm to 0.4% NH/sub 3/ in N/sub 2/ for a YZ-LiNbO/sub 3/ delay line heated at 120/spl deg/C. Some selective SAW gas responses are discussed.",
Behavioral views for software requirements engineering,"The paper introduces the concept of software behavioural views, and presents a formal notation for their specification and composition. The objective is software behavioral requirements specification independent of design and implementation. The paper claims that behavioral views can reduce the complexity of software behavioral requirements specification. To establish this claim, the paper introduces a notation, called Viewcharts, which is based on David Harel's (1987) Statecharts. Viewcharts extends Statecharts to include behavioral views and their compositions, limits the scope of broadcast communications and, consequently, reduces the complexity of scale that Statecharts faces in behavioral specification of large systems.",
A form dropout system,"This paper describes a system for form dropout when the filled-in characters or symbols are either touching or crossing the form frames and the form model is unknown. Since some of the character strokes are either touching or crossing the form frames, we need to address the following three issues: (i) localization of form frames; (ii) separation between characters and form frames, and (ii) reconstruction of broken strokes introduced during separation. The form frame is automatically located by finding long straight lines based on a data structure, called block adjacency graph. Form frame removal and character reconstruction are implemented in this graph. When the same process is applied to a blank form, followed by the procedure of connected component extraction and clustering, a form structure-based template is automatically generated which includes form model, skew angle and preprinted data areas. Given the form template, our system can extract both handwritten and machine-typed filled-in data. Experimental results on three different types of forms demonstrate the performance of our system.",
CAL support for complex CASE tutorials. Demonstrating software engineering concepts through CASE,"The steep curve inherent in learning to use CASE has been often noted. Integrating CASE in the software engineering curriculum requires substantial effort and resources to produce and support tutorials suitable for university students. As integrated CASE (I-CASE) products are more extensive and rigorous than consumer productivity tools, the magnitude of the learning task can be underestimated. CASE documentation must be adapted for educational use. This paper reports the development, implementation and refinement of a computer-assisted learning (CAL) tutorial resource to support software engineering education in the use of commercial I-CASE software. Beginning with a discussion of learning and unit objectives, this paper addresses tutorial development strategy, subject rationale and presentation sequence, resource requirements, recommendations and observations. Examples of the student interface, tutorial screens and text are illustrated.",
Results of modeling and experimental measurements for the design of a neutron surface moisture measurement sensor,Computer modeling and experimental testing have been performed to produce a design for a neutron-moderation-based surface moisture sensor. Results indicate that this sensor should be capable of providing information about the moisture concentration profile in the top 15 cm of material. Tests show that the sensor should be capable of operating in the expected high temperature and gamma exposure rate fields within the Hanford Site tanks. A field-ready sensor and an in-tank deployment system were fabricated for application of this technology to the Hanford Site tank farms.,
A bulk-synchronous parallel library implementation for the BBN butterfly GP1000,"One of the fundamental goals of parallel computing is to develop a framework that will support portable and efficient application programs. The Bulk-Synchronous Parallel (BSP) model was proposed to help achieve this goal. The BSP model is intended to be a ""unifying model""-it addresses both software and hardware issues by allowing theoretical analysis to coexist with practical physical implementations. For several years the BSP model has been supported mainly by theoretical results. Recent experiments, however, have begun to demonstrate the practicality of the model for real architectures running real applications. The goal of this paper is to describe the methodology used to construct an efficient BSP library on the BBN Butterfly GP1000. Our results are relevant for BSP library implementations on shared-memory systems in general and for NUMA (nonuniform m-memory-access) machines in particular.",
